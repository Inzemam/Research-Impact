detailsdistribution, posting, or copying of this pdf is strictly prohibited without written permission of the national academies press. (request permission) unless otherwise indicated, all materials in this pdf are copyrighted by the national academy of sciences.copyright © national academy of sciences. all rights reserved.the national academies pressvisit the national academies press at nap.edu and login or register to get:œ œ 10% off the price of print titlesœ special offers and discountsget this bookfind related titlesthis pdf is available at sharecontributorshttp://nap.edu/167managing microcomputers in large organizations151 pages | 6 x 9 | paperbackisbn 9780309034920 | doi 10.17226/167board on telecommunications and computer applications, national research councilmanaging microcomputers in large organizationscopyright national academy of sciences. all rights reserved.managingmicrocomputers inlarge organizationsboard on telecommunicationsand computer applicationscommission on engineeringand technical systemsnational research councilnational academy presswashington, d.c. 1985imanaging microcomputers in large organizationscopyright national academy of sciences. all rights reserved.national academy press 2101 constitution ave., nw washington, dc20418notice: the project that is the subject of this report was approved by the governing board of thenational research council, whose members are drawn from the councils of the national academyof sciences, the national academy of engineering, and the institute of medicine. the members ofthe committee responsible for the forum were chosen for their special competences and with regardfor appropriate balance.this report has been reviewed by a group other than the authors according to proceduresapproved by a report review committee consisting of members of the national academy of sciences, the national academy of engineering, and the institute of medicine.the national research council was established by the national academy of sciences in 1916 toassociate the broad community of science and technology with the academy's purposes of furthering knowledge and of advising the federal government. the council operates in accordance withgeneral policies determined by the academy under the authority of its congressional charter of1863, which establishes the academy as a private, nonprofit, selfgoverning membership corporation. the council has become the principal operating agency of both the national academy ofsciences and the national academy of engineering in the conduct of their services to the government, the public, and the scientific and engineering communities. it is administered jointly by bothacademies and the institute of medicine. the national academy of engineering and the institute ofmedicine were established in 1964 and 1970, respectively, under the charter of the nationalacademy of sciences.library of congress cataloging in publication databoard on telecommunications and computer applications, commission on engineering and technical systems, national research council.managing microcomputers in large organizations.includes index.1. businessšdata processingšmanagementšcongresses. 2. office practicešautomationšmanagementšcongresses. 3. microcomputersšcongresses. i. national research council (u.s.)board on telecommunications and computer applications.hf5548.2.m297 1985 658.054 8422617isbn 0309034922copyright ©1985 by the national academy of sciencesno part of this book may be reproduced by any mechanical, photographic, or electronic process, orin the form of a photographic recording, nor may it be stored in a retrieval system, transmitted orotherwise copied for public or private use, without written permission from the publisher, except forthe purposes of official use by the united states government.printed in the united states of americaii´managing microcomputers in large organizationscopyright national academy of sciences. all rights reserved.preface and acknowledgmentsmost people in the business of information management had beenexpecting microcomputers for years. yet their arrival in the christmas season of1981 took many by surprise. it was another example of technology being inadvance of our ability to use it and to manage it. since that time, furthertechnological advances have made microcomputers more powerful, moreeconomical, and simpler to use. these socalled personal computers havespawned a revolution in the way information is gathered and exchanged.for large organizations the revolution means a basic change in therelationship of end users to central computing facilities. until recently end usersdepended on data processing specialists to create and operate their programs.with new development in personal computers and software, however, end usersare growing more and more independent of the specialists: many professionalswith no prior experience in data processing have introduced personal computersinto their working lives.the proliferation of microcomputers has overwhelmed many organizationsand in the process created two serious problems for management: how do wecontrol the headlong transition from centralized to decentralized computationwithout stifling the creativity of the end user? and how do we manage the useof microcomputers to enhance productivity and make the organization's totalcomputing capability costeffective?preface and acknowledgmentsiiimanaging microcomputers in large organizationscopyright national academy of sciences. all rights reserved.the national research council's board on telecommunications andcomputer applications held a forum in late 1983 to address these and relatedconcerns. the meeting featured experts from two broad areas of experience:senior executives from the private and public sectors who have directed the useof computers in their own companies or in the federal government, andtechnology innovators who are directly responsible for the increasing popularityof personal computers.this book is the product of that meeting. written by and for executives, itprobes these questions: where is microcomputer technology going? what arethe implications of these directions for large organizations? what are theemerging issues critical to top management? and how are selected largeorganizations dealing with these issues?many people shared in the creation of this book. in particular, i wish tothank the members of our steering committee (see page viii) and thecontributing authors. staff members of the board on telecommunications andcomputer applications who organized the forumšjerome d. rosenberg,senior staff officer and forum director; and lois a. leak, administrative secretaryšalso deserve special thanks, as does paula kaufmann, who edited thetranscript of the meeting.i also wish to recognize and thank our sponsors: arthur young andcompany, the tennessee valley authority, the u.s. department of defense, theu.s. general services administration, and the u.s. veterans administration.francis a. mcdonough*chairman* francis a. mcdonough is deputy assistant administrator of the office ofinformation resources management, u.s. general services administration. hechampioned the development of the federal government's managed innovation program,which is fully described in chapter 9.preface and acknowledgmentsivmanaging microcomputers in large organizationscopyright national academy of sciences. all rights reserved.contents steering committee viii overview vision and value: getting the most out of microcomputersjohn m. thompson 3 the organizational issuesjohn diebold 11i small computer technologyšwhere we are and wherewe're headed introductionwilliam h. leary iii 17 faster, smaller, cheaper: trends in microcomputer technologythomas h. willmott 19 trends in personal computer softwaremitchell kapor 28 personal computer networksrobert m. metcalfe 36contentsvmanaging microcomputers in large organizationscopyright national academy of sciences. all rights reserved.ii small computers in large organizationsšthe implications introductionhannah i. blank 43 managing uncontrollable growthjohn h. bennett 45 managed innovation: controlling enduser computing in thefederal governmentray kline 52 personal computers and the office of the futurejames h. bair 60iii managing microcomputersšthe issues introductionwilliam c. rosser 69 a perspective for the chief executive officeralastair i. omand 71 managing microcomputers and enduser computing: somecritical issuesroger l. sisson 81 regaining control through centralized actionthomas d. conrad 93iv managing microcomputersšcase studies introductionrhoda w. canter 99 productivity through automationjohn j. alexander, jr. 101contentsvimanaging microcomputers in large organizationscopyright national academy of sciences. all rights reserved. managing microcomputers in state and local governmentfred dugger 115 the user eramartin b. zimmerman 124 personal computing, not personal computersnorman m. epstein 130 control through persuasionallan z. loren 135 index 141contentsviimanaging microcomputers in large organizationscopyright national academy of sciences. all rights reserved.steering committee managingmicrocomputers in large organizationsfrancis a. mcdonough (chairman), general services administrationjohn h. bennett, united technologies corporationhannah i. blank, chase manhattan bankrhoda w. canter, arthur young and companye. floyd kvamme, kleiner perkins caufield & byerswilliam h. leary iii, office of the assistant secretary of defensewilliam c. rosser, gartner group, inc.arthur h. schneyman, mobil corporationmichael e. treacy, massachusetts institute of technology viiimanaging microcomputers in large organizationscopyright national academy of sciences. all rights reserved.overview1managing microcomputers in large organizationscopyright national academy of sciences. all rights reserved.2managing microcomputers in large organizationscopyright national academy of sciences. all rights reserved.vision and value getting the most out ofmicrocomputersjohn m. thompson*i believe we are now in the second wave of the computer revolution. thefirst wave focused on the use of information technology to replace people; nowwe are more concerned with supporting people. we are moving from theautomation of structured tasks of the first 10 to 15 years of the computerrevolution into the support of unstructured tasks, the support of managerialactivity.much of the discussion in this book concerning enduser computing andmanagerial support uses words and ideas that have been around for years. now,however, different media and technologies are available. in the late 1960s wesaw timesharing; in the 1970s, minicomputers; in the early 1980s, theinformation centersša sort of inhouse timesharing service bureau. now in themid1980s, we are seeing an influx of personal computers (pcs), endusercomputing, and networks. there is a lot of talk about the need to integrate all ofthese technologies to support people in their workplace.john diebold sets the stage for this discussion when he talks about themillions of computerliterate people in the workplace for whommicrocomputers can unleash imaginations and creativity. he also reminds us ofthe proliferation of technological alternatives* john m. thompson is vicepresident of index systems, inc., cambridge,massachusetts.vision and value getting the most out of microcomputers 3managing microcomputers in large organizationscopyright national academy of sciences. all rights reserved.with a rather catchy phrase: ﬁoption shock.ﬂ the theme of unleashing creativityrecurs throughout this book. in the industrial revolution we invented machinesto provide leverage to human muscle; perhaps now we are inventing machinesthat can provide leverage to the human mind. to carry the analogy a littlefurther, in the nineteenth century we provided support to the bluecollar worker;in the twentieth century we have been providing support to the whitecollarworker. some time early in the twentyfirst century we must find a way to bringit all together.the essays in part i examine where microcomputer technology has comefrom and where it appears to be headed. thomas willmott discusses what hecalls the ﬁtechnology pushﬂ in his description of the first two phases in theevolution of the personal computer. in the first phase the personal computer issimply an individual workstation. it is characterized by a cottage industry ofsoftware producers supplying the workstation and trying to make it useful. nowwe are moving into a second phase, in which microcomputers become part of anetwork. this phase raises many kinds of different problems that are echoed inthis report: where is the data? how do i manage it? how do i exercise control?do i control? willmott, as well as other contributors to this volume, evokes theimage of some bright scientists making machines faster, cheaper, and smallerand then pushing them into our organizations, saying, ﬁthere! you figure outwhat to do with them!ﬂthe notion of ﬁtechnology pushﬂ is reinforced by mitchell kapor, whowrites about the ﬁheroic geniusesﬂ producing software. software producers arenot driven by a careful analysis of market need, says kapor. instead, they aredriven by the need simply to figure out what to do with the personal computer.because nobody really knows what to do with these computers, somebody getsa good idea, tries it, and perhaps it works in the marketplace.kapor predicts that soon the general drive will be toward software systemsthat integrate five major areas of microcomputer usešspreadsheet, data base,word processing, graphics, and communications. new developments will alloweasy movement between windows and create open systems that can interactwith other hardware and applications software systemsšin which each of thefive elements is not compromised by being part of an integrated system. thedesign objectives of such systems is to produce individual applications that arejust as good as the availablevision and value getting the most out of microcomputers 4managing microcomputers in large organizationscopyright national academy of sciences. all rights reserved.standalone versions. in addition, the ﬁhomeﬂ of the data would be absolutelytransparent to the user. for those of us who have used any form of a personalcomputing system, be it a personal computer or distributed from a timesharingsystem, this all sounds very powerful. in fact, combining the power of thetechnology in both hardware and software seems a little like having a ferrari onthe island of grenada. right now pcs make a great deal of power available, butthere are not many places for us to use that power yet.robert metcalfe predicts widespread use of local area networks withperipheral sharing, information access, and personal communications. somehave predicted that the 1980s will be remembered as the critical decade inwhich everyone became interconnected. if this is the case, it is apparent that thecommunications applications of microcomputers will become increasinglyimportant. we can expect communications to emerge as one of the criticalissues of the mid1980s.part ii of this report probes the implications of this technology push. johnbennett discusses its meaning for the managers of information systems (is). inshort, the is manager must improve service or lose. bennett describes some ofthe tools now available to the manager to improve the productivity of the isdepartment. he also describes the reactions of is managers to the rapid growthin microcomputer use occurring in their companies: proliferation inevitablyraises the issue of control, specifically control of data. to those who ask thequestion ﬁwhy control?ﬂ the answer heard most often is: ﬁbecause in two years'time the data that is recognized as a critical resource of this corporation will notbe in my machine, but will be on everybody else's desk. my chief executive isgoing to turn to me and say, ‚how in the world did you ever get us into thismess?'ﬂbennett also points out that a major part of the job of the is manager is toopen the gates and facilitate access to corporate data by a variety of differentsystems, including microcomputers. related to this is the need for is managersto take responsibility for educating management. although it is a significantdeparture from their traditional role of information processing, more and morethe responsibility of is managers to educate is being termed critical to theirsuccess. bennett outlines a very successful program at united technologies toeducate 1,000 senior managers.finally, bennett raises the issue of security as one of the majorvision and value getting the most out of microcomputers 5managing microcomputers in large organizationscopyright national academy of sciences. all rights reserved.worries of the is manager. horror stories abound in this area. i know one ismanager who put his career in jeopardy by walking around his executive suiteand picking up floppy disks left on the top of file cabinets. when he took themback to his office, he found that he had all of the corporation's recent andprojected financial data, the latest competitive analysis, and some sensitive dataabout personnel salaries for the leading 100 people in the corporation. peoplewere leaving sensitive data around on floppy disks that they would never leavearound on a piece of paper. this represents a whole different set of securityissues from what we have dealt with for the last 20 years in managinginformation systems, and we have to learn the differences fast. by 1990 theworkstation will probably be as common as the telephone. managing thischange has significant implications for organizations.ray kline reports on what the federal government is doing in its managedinnovation program. his ﬁstickandcarrotﬂ metaphor is particularly appropriateto describe the controlsupport system of the government's program. while theimposition of standards and restrictions on users is necessary, users can beoffered in return more support for their work. such support includes easierprocurement procedures, and new tools for education.james bair examines some of the implications of small computertechnology for managerial support. he describes a major shift away from a veryspecialized awareness in information technology to a mass awareness in oursociety and the implications of this shift within corporations and betweencorporations and society.bair also underlines the importance of communications and the need forthe microcomputer business to support communications. this idea has beenechoed by many others in the field of executive support. in the early days wecoined the term ﬁdecision support system.ﬂ in the late 1960s and early 1970s wehad a rather naive notion that decisions were the executive activity that neededsupport. later research pointed out, however, that executives spend far moretime communicating than making decisions. instead of decision support systemswe must talk about communications support systems, or perhaps learningsupport systems, or management support systems, or just plain support systems.the real task is to bridge the gap between technology and people byunderstanding what people do that needs supporting.jim bair gives us another insight in this area. he says he hasvision and value getting the most out of microcomputers 6managing microcomputers in large organizationscopyright national academy of sciences. all rights reserved.spent a great deal of time trying to figure out how to measure productivity, onlyto discover that nobody wants to measure it anymore. i suspect that many of ushave as an objective the improvement of ﬁproductivity,ﬂ something we have notdefined and cannot measure. we all intuitively understand that using thetechnology to improve productivity is a worthy objective. but what does thatmean? how do you know when you've done it? how does it apply to ourorganizations? there is an increased reluctance to hone in on what productivityis. at the same time there is a great move to double it!part iii explores some specific and vital management issues. alastair omandšone of the few senior is people who report to the executive committee of avery large corporationšbelieves there are two major issues raised bymicrocomputers in organizations that the chief executive officer (ceo) shouldreally worry about. the first is data management and its associated problems ofaccessibility, compatibility, and security. for this he recommends putting aperson in a position of organizational responsibility for the corporation's data.the ceo's second worry is the shift in skill setsšanother variation on thepersistent theme of education. other contributors deal with this theme from theperspective of the effects of microcomputers on the entire culture of theorganization, but the point made by all of the contributors is that topmanagement, and chief executives in particular, must worry about how thistechnology will change their organizations. their users must become developersand managers. they must keep up with shifting technical trends and watch forthe changing role of information systems in their organizations.other important issues, according to omand, should be left to the linemanagement of the organization to handle. product proliferation, or ﬁoptionshock,ﬂ as john diebold calls it, causes concern about ﬁconnectivity.ﬂ will webe able to interconnect everyone? will we be able to supply adequate technicalsupport? will we have huge duplication of effort between people who do nottalk to each other in the organization? omand recommends that we leave theseproblems to the users, keeping them in charge, letting them take advantage oftechnological change.acquisition practice raises another set of concerns. there are questionsabout cost justification and how to achieve good cost savings, what interface tohave with vendors, buying equipment that becomes obsolete very fast, andtrouble with license restrictionsvision and value getting the most out of microcomputers 7managing microcomputers in large organizationscopyright national academy of sciences. all rights reserved.on software. for all of these very real issues omand recommends that the chiefexecutive officer make use of existing mechanisms like the purchasingdepartment and the control mechanisms through the budget.roger sisson views the discussion on microcomputers as really adiscussion about enduser computing. he suggests that putting motivated peopletogether with good tools and relevant data will produce what he callsﬁdistributed creativity.ﬂ in other words, looking at data seems to facilitateinnovative thinking.distributed creativity is not a term that would have fared very well in theearly 1970s. a decade later, however, we are getting used to this sort ofconcept. it is tied in with the theme of unleashing creativity and the notion thatsomething very fundamental is now going on that we do not really understand.it has been brought on by new tools, data, and the technology push.part iv presents a number of case studies that suggest the possibilities fordealingšon a daytoday basisšwith the challenges of the microcomputerrevolution. norman epstein sees control as the overriding concern formanagement. from this comes his concept of personal computing rather thanpersonal computers. other contributors to this section suggest alternativeapproaches to the dual questions of control and support. in most cases thevehicle chosen is less important than what actually happens in support of people.several essays in this volume refer to gibson and nolan's stages of growthcurve, which can also be called a technology learning curve. gibson and nolanfirst published their article on the four stages of growth at a time when we wereapproaching maturity in the earliestšdata processingštechnology learningcurve. we are now on another technology learning curve involving personalcomputing.we hear much talk about the first and third stages on this curve: gettingstarted, and getting control. we do not find many people worrying about thesecond stage, getting value. perhaps this is because value is a difficult wordthat, like productivity, defies definition and measurement. but the question willnot go away. why are the technology pushers doing this to us, and what valuecan we get out of it for whom?i believe value can be derived for at least three levels of people inorganizations, but for each level opportunities are different. as the leader of theorganization, the senior executive can set thevision and value getting the most out of microcomputers 8managing microcomputers in large organizationscopyright national academy of sciences. all rights reserved.culture and change the organization. at another level, the functional managersneed to develop an intuition about the businessšabout what works and whatdoesn't work. by and large, they are the people who achieve the operatingresults of the organization. at a third level, there are analysts who are clearlythe prime target for the new tools of the technology. how are we to derive valuefor these three defined users of microcomputer technology? do we rely on somesort of invisible hand? should we merely give them the tools and say, ﬁgo dosomething useful with this elegant technology?ﬂ to what extent should wefocus on data management, on security, on controls, on education concerningwhat the tool can do?we are left with many unanswered questions. however, i would beamazed if they were answered at this stage because i do not think that we yetknow where we are going with the technology. and if you don't know whereyou are going you can get there any way you like.these papers reflect the candor of informed experts in the field who arenot afraid to admit that ﬁwe don't know where we are going.ﬂ there's an oldzen saying: ﬁin the mind of the beginner, there are infinite possibilities; in themind of the expert, there are very few.ﬂ right now, we should beware ofexperts who claim to have all the answers; at this stage we really can't knowwhat we are getting into.perhaps one word we need to hear more often is ﬁvision.ﬂ organizationsmust have a strategic purpose for this technology. they must head toward somegoal. as a whole, the papers in this volume may present what appears to be adichotomy between two broad themes. on the one hand, there is the very cleartheme of using automation to increase productivity. this goal raises veryfamiliar management issues of planning, organization, and control. theresponse is relatively straight forward. we can apply the tools of planning andmanagement that we have learned over the last several decades, and they oughtto stand us in good stead. we will manage our way through it, if that is wherewe are going.on the other hand, some of these authors suggest that our goal has to dowith unleashing capability, with leveraging a person's mind, with innovationand creativity. if this is the goal i am not at all sure we know how to manage.perhaps it is appropriate at this point to hand out tools and say, ﬁyou figure outwhat to do with them.ﬂ perhaps this is a time of intelligent experimentation, avision and value getting the most out of microcomputers 9managing microcomputers in large organizationscopyright national academy of sciences. all rights reserved.ﬁseed time,ﬂ a period of learning. or perhaps both approaches should befollowed together to see what happens if we unleash the creativity of a millioncomputerliterate people.most importantly, i believe we must pause for a moment to examine whatwe are trying to accomplish with this technology in our individualorganizations, that is, where we want to go. clearly, something very importantand very fundamental is going on that we do not yet fully understand. it is anextraordinarily interesting, stimulating, and exciting time. i am onlyuncomfortable when i hear people predicting their future instead of choosing it.this is my argument with the concept of technology push. we must be sure toconfront the dichotomy between productivity and creativity. do we want one orthe other? or do we want both? and what does that mean? we must choosewhat we want, not just predict what we might get. depending on our choice,there are fundamentally different implications for different management styles,the changing role of the is manager, data, security, value, and how to manage inan era of intelligent experimentation.the attributes of success that many of the contributors to this volumediscuss have some common themes. there is the concept of marketing to users.there is the concept of users and the need to provide education and othersupport systems for them. there are the operational issues of security and datamanagement. there are the finance issues and questions of cost justification.there are some issues of administrative responsibility. further, there are humanresource issues: how do we educate people and what kind of people do we needfor this new era?these are, in fact, the major headings for a business plan: marketing,operations, finance, administration, human resources. but this is a plan for abusiness within the business. it is the business of making microcomputers orany personal computing technology work to support the people within theorganization. like any business, its primary objective is getting business value.this must be the ultimate objective for us all.vision and value getting the most out of microcomputers 10managing microcomputers in large organizationscopyright national academy of sciences. all rights reserved.the organizational issuesjohn diebold*from a management perspective, three important points can be made aboutthe phenomenon of microcomputer proliferation we are now witnessing. thefirst is that we stand at a milestone. the everincreasing computer literacy ofour society has altered the way one thinks about microprocessors, as well as therole of information technology in any large organization. daily there are frontpage items in the newspapers indicating that the role of the computer in societyhas changed from what it was only two or three years ago. the 414sštheinfamous milwaukeearea ﬁhackersﬂ who take their name from their telephonearea codešare a marvelous symptom of this change.the difference is due not simply to larger numbers of micros. the situationis different because we have reached a critical mass in society. there are nowmillions of computerliterate people who are doing new kinds of things withthis tool. this is a very different environment in which to place the role ofinformation processing.the second point, and a crucial issue, is whether an organization shouldadopt a control mentality toward microprocessors or whether it should take avery different approach. the alternative* john diebold is chairman and founder of the diebold group, inc., new york, newyork.the organizational issues11managing microcomputers in large organizationscopyright national academy of sciences. all rights reserved.approach encourages a much more permissive environment so that individualsmay develop many more uses and roles for micros within the organization.until recently, information processing meant an essentiallyinstitutionalized structure in which the individual had little choice in what he orshe did in minutetominute applicationsšwhat the psychologists of theworkplace call nondiscretionary work. this is changing. information processingis making labor very discretionary. such a shift raises a number of questions.what is an organization's management policy on microprocessors? do theybecome a symbol of this transformation, the unleashing of imaginations, theextending of an employee's capabilities? or are they in essence controlled?these two approaches represent fundamentally different management styles andphilosophies, and organizations must analyze them thoroughly before adoptingone or the other.we should not assume that the same approaches and managementphilosophy applied to information technology up to this point can or must applyin the future. most of the literature, as well as most management approaches,treats information processing as a support function. in many cases it is still asupport function, but in more and more organizations it is becoming a lineactivity. it is essential to recognize this change. no longer is it a question ofthinking about an orderly or a costjustified approach to information processing.that is not how personal computers (pcs) are justified or bought. they arebought through every conceivable means for all kinds of applications. only aminimum number of these purchases are justified according to the sort of highlydeveloped methodology that exists for conventional data processing.microcomputers are also being purchased in gigantic quantities, with farreaching implications. the standalone personal computer is a transitoryphenomenon, but it happens to dominate at the moment. the most recentsurveys indicate that only about onefifth of micros now in use are connected tolarger systems. this obviously will change, and it will become the norm to putthese micros into many kinds of informal as well as formal networks.we must also recognize that the microprocessor joins the game while thereare five loose pieces on the organizational playing board. these pieces and theirrelationship to management decisionsthe organizational issues12managing microcomputers in large organizationscopyright national academy of sciences. all rights reserved.about microcomputers are the third key point i would emphasize.one of these loose pieces is the traditionally managed data processingactivity. this activity normally reports at a middletohighlevel managementstructure and has a quite highly developed methodology and control mechanism.the second loose piece is the communications function, which normallyhas reported at a lower level because there haven't been very many options.now, however, the communications area is experiencing what some are callingﬁoption shockﬂ in response to the vast number of communications optionsavailable. as a result, an enormous burden of network design, previously borneby the principal outside supplier, at&t, has been placed on the large user. theentire communications function is changing very rapidly.the third piece on the board is the office automation function. this areahas typically reported at a lower level than the data processing activity and hasbeen oriented towards clerical savings. in reality, however, office automationshould support professional activities. this is beginning to occur.a fourth piece is the area of manufacturing systems, where major changesin the whole approach to manufacturing are occurring as a result of informationtechnology. although several organizational models exist in this area, dieboldresearch program studies have shown that it has normally reported separately.now, however, manufacturing systems are starting to be consolidated withother information processing functions.the fifth piece is the largest and most importantšthe end user. the enduser level is the point at which many microcomputers are now being acquired.for example, one of our clients recently bought 10,000 pcs, with an escalatorclause to go to 30,000. in fact, most large organizations are purchasing microsby the thousands, but they are doing so in smaller increments.until now only 3 percent of the data processing education budget has beenspent on end users. as a result, employees are currently spending a lot of timeeducating themselves. obviously this situation must change dramatically. enduser education is where the bulk of the future data processing (dp) trainingbudget will have to be spent. all of the traditional statistics on dp spendingignore this enduser area. thus, the actual level of investment of an organizationfor data processing is quite different than thethe organizational issues13managing microcomputers in large organizationscopyright national academy of sciences. all rights reserved.traditional budget figures might show, because it is with the end user thatdevelopments are occurring.these five organizational pieces have been floating, and organizationshave tried to deal with them in different ways. although the right approachdepends on the organization and its mission the role of information technologyin that mission is itself changing because the parameters of competition arechanging. industry lines are shifting. these shifts are most apparent in thefinancial service fields, in publishing and broadcasting, and now in retailing ascomputers move into the home.changes are also beginning to occur in the parameters of competitionwithin individual industries. this means that management must think less aboutmanagement information systems and more about business informationsystems. as information flows from the raw data stage through all the stages tothe end user, previously discrete activities become intertwined.what i mean to suggest is that today, all is in flux, and it is against thischanging and developing background that management must think about therole of microcomputers.the organizational issues14managing microcomputers in large organizationscopyright national academy of sciences. all rights reserved.part ismall computer technologywhere we are and where we'reheaded15managing microcomputers in large organizationscopyright national academy of sciences. all rights reserved.16managing microcomputers in large organizationscopyright national academy of sciences. all rights reserved.introductionwilliam h. leary iii*there are few, if any, precedents for the microcomputer explosion. it hassurprised most of us and is now recognized as a major force of change in smalland large organizations alike. microcomputers are spreading rapidly into everywalk of life. unfortunately, such growth makes it difficult to identify cleartrends and other underlying forces. the authors of the next three chapters areuniquely qualified to assess the status of and trends in microcomputertechnology.thomas willmott of international data corporation begins with a broadreview that gathers the threads of hardware, software, and communications intoa fabric that explains the direction of the microcomputer/personal computerindustry. he represents that special group of industry observers who can help usunderstand what is happening.the other authors, mitchell kapor and robert metcalfe, are leaders amongthe small number of visionary geniuses in the microcomputer industry. kapor,whose software products include the enormously successful 123 as well asvisitrend and visiplot, shares his observations on where microcomputersoftware is heading and looks at the apparent conflict between the successful* william h. leary iii is deputy director, information resources managementsystems, office of the assistant secretary of defense.introduction17managing microcomputers in large organizationscopyright national academy of sciences. all rights reserved.software entrepreneur and the organizational needs of the software companiesas they grow larger.robert metcalfe of 3com corporation is the wellknown inventor of theethernet local area network technology and a major figure in microcomputercommunications. he reviews the status and future of the increasingly importantnetworking potential of microcomputers.together, these three chapters offer an important perspective on theproblems and potential of microcomputer technology.introduction18managing microcomputers in large organizationscopyright national academy of sciences. all rights reserved.faster, smaller, cheaper trends inmicrocomputer technologythomas h. willmott*at international data corporation we do market research and competitiveanalysis for both vendors and users of information processing equipment.therefore, when i look at where the technology has been and where it is going,i see it from two viewpoints: where the industry is moving and how well usersare adapting to the changes. but from both viewpoints the movement can besummarized in three words: faster, smaller, and cheaper.as we look at the past and future of microcomputers it may be helpful tothink in terms of a microcomputer's life cycle. i would suggest that there arethree phases to this cycle. the first is hardware introduction, during which timeusers have the opportunity to review hardware capabilities. the second phase isthe response to that original debutšwhether it attracts peripheral vendors ofhardware and stimulates software development that provides a wide and richwork environment. no corporation, not even ibm, can stand alone in thismarketplace. all need the diversity of the expansion board vendors, thesoftware vendors, the whole new industry that has emerged over the last fewyears. thus, in the second phase it is critical to sense whether the new machine is* thomas h. willmott is director of user programs and a personal computer analystfor international data corporation, framingham, massachusetts.faster, smaller, cheaper trends in microcomputer technology19managing microcomputers in large organizationscopyright national academy of sciences. all rights reserved.attracting the needed kind of support. if it is, the third phase may extend wellbeyond the technologically useful life of the machine. it may not be the smartestprocessor on the block, but because of its installed base of software and itsrange of capabilities it is sufficient to the task at hand for the two, three, or fouryears needed to amortize the equipment.if we think in historical terms, it's obvious that what we have seen so far inpersonal computers and microcomputers is merely an introduction to what willbe coming. in the early 1960s mainframe computers were placed behind plateglass windows and handled by men and women in white coats. approaching thedata processing facility was like going to the mountain, which mohammed hadto do, even if he was the president of the corporation. in its first stage ofdevelopment the personal computer, too, was seen as a curiosity. end usersfound themselves relatively unprepared in technological terms to deal withmicrocomputers in any meaningful way. the industry had to translate thebuzzwords for the enduser group; the technology, though smaller, was stillrather exotic.in this first stage we dealt with the personal computer as an individualworkstation, as an independent processing unit rather than as part of a largerorganizational framework. looked upon perhaps as a toy by the mis(management information systems) department, it was viewed with suspicion,as something that wasn't really part of the computer resources facility. we alsohad to deal with the whole area of shared peripherals, secondary to whateverunit was on the desk. a winchester disk that cost $2,500, for example, wasdifficult to justify for a microcomputer that sold for $1,995. thus we ended upwith rolling resourcesšprinters on a cart, which could be moved around adepartment and shared among a number of people. this was a rudimentaryapproach to what would in time become a sophisticated dataprocessingenvironment.during the first stage we also had to explain system software to our endusers, as well as come to grips with it in terms of management decisions. oneproblem was that a singleuser operating system was completely foreign to thedata processing environments in which our top managers worked. they wereused to much more sophisticated operating systems. since end users hadabsolutely no experience with system software, we had to teach them how touse it and explain its role in terms of system responsibility. wefaster, smaller, cheaper trends in microcomputer technology20managing microcomputers in large organizationscopyright national academy of sciences. all rights reserved.had to deal with many poor hardware and software decisions made byuneducated end users who purchased equipment for their departments. wefound ourselves buying hardware that could not do the job it was assigned to dobecause application software was dreadfully lacking.jean piaget, a psychologist who specialized in cognitive development, hada theory about individuals being thrown off equilibrium and then assimilatinginformation on a new subject until they reached equilibrium again. in its firststage of development the microcomputer created a similar disequilibriumenvironment, and only now are end users and managers coming to grips withthe issues raised by the microcomputer as a small, cheap workstation.during this first stage we also had to deal with how application softwarewas going to be defined and developed. would it focus on taskspecific,horizontal packages such as database managers, word processors, andspreadsheets? or would the thrust be for vertical markets, where an applicationcould be developed that solved a total business problem? in terms of horizontalpackages, we began to move from singlefunction to integrated software. in thevertical market there was less progress because there were fewer opportunitiesboth from a research and development standpoint and from a financial, businessstandpoint.hardware was still a precious resource in this first stage. memory was agoverning factor. the 8bit cp/m operating system and a number of softwaregraphics packages based on 8bit technology were designed to do somersaultswithin a small space because memory and disk storage capacities were at apremium. stageone hardware was supported by a cottage industry of softwareprogrammers who appeared highly suspect to large companies and federalagencies. one of these new companies, with perhaps 10 to 12 employees, wouldhave the hottest package in the world but a balance sheet that would make yourpersonal checkbook look proud. we were not used to doing business with thesekinds of firms.as we moved toward the end of stage one we began to take a look at whatelse we could do with a microcomputer. we had figured out what it could do forus locally; now we began to look at the larger environment: the corporation as awhole, the federal agency, the state government computing resource.in the second stage of microcomputer development, which isfaster, smaller, cheaper trends in microcomputer technology21managing microcomputers in large organizationscopyright national academy of sciences. all rights reserved.where we are today, we are more likely to see the microcomputer as a windowon a greater distributed resource system. today we look out from ourmicrocomputers and talk either to a mainframe, a stripfile on a minicomputer,or to a larger communications database technology. we may be tying into otherorganizations or services, into other resources in our own corporations, orperhaps into a local group of microcomputers, where we can share files in amore sophisticated fashion than ever before. today the microcomputer isassociated more with communications and with computing on a grand scalethan it is with the idea of a standalone workstation. processing capabilities arestill important at the local level, but concurrent processes are more important.these include multiple windows on the same terminal which allow the user tobe active in a job that runs on a mainframe and also to merge that data into localfiles and manipulate it at local memory levels.the key issue we are now facing is microtomainframe communication.we are trying to answer questions about how the microcomputer fits into a totalcomputing capability. where are the remote data? are they available to the enduser? shall they be made available? how often will they be updated? how doesall this affect decision making? the microcomputer is coming of age. in thissecond stage of development the communications capability will increase ratherthan decrease; the demands for even broadband communications links maywell be in place within the next two years.the whole area of microprocessor technology, peripheral chips, and smallcircuit technology in general has brought additional pressures to bear on themis or adp (automated data processing) planning branch. microcomputers areonly one symptom of the total technological wave that is rolling over us. we arebeing forced to make management changes as well, not only in the area ofmicrocomputer planning and acquisition training, but in our whole adpplanning and telecommunications staff. we have a communications group onone hand, and an adp planning function on the other. microprocessortechnology and microcomputers have forced an organizational change in therelation between the two types of jobs in the communications area.microprocessor technology has in one way made capacity planningobsolete, not in terms of the skills or people required, but simply becausemovement is no longer from one huge machine to another. smaller steps arenow involved. we are more concernedfaster, smaller, cheaper trends in microcomputer technology22managing microcomputers in large organizationscopyright national academy of sciences. all rights reserved.with how to develop application software across an entire computing resource.clearly, we have more options in the area of capacity planning than we hadseveral years ago when the only computing facility for a new application was alarge mainframe or perhaps a distributed mini with a dedicated voicegrade line.today we have an entire range of processing capabilities throughout ourorganizations, ranging from small desktop equipment to much larger machines.the variety of options offered by the new technology means that traditional jobfunctions have to change to maintain efficiency within the organization.integrated circuit technology is the driving force of equipmentdevelopment occurring in the second stage. and we almost have a secondgeneration of the distributed resource, which will include small, local loops oftechnology (a loop of intelligent workstations or a loop of apple computers, forexample); front ends to large mainframes; twin minis, which handle strip filesfor management decision making; and remote communications to othernetworks. based on effective management decisions concerning cost/benefitanalysis, these are the directions in which we will be moving in the mid1980sin terms of a totally distributed information resource. it is absolutely critical thatorganizations be concerned not only with the technology but also with how thetechnology will serve our business, raise our profit levels, and help our service.i suggest that the key issue to keep in mind when making decisions aboutthe management of computers is that what we are doing with each new level oftechnology is serving a wider audience of potential users. when we had fivepeople in white coats overseeing the major computer in the computer center, themanagement problem was relatively easy. we delivered data to the center,pumped it through, and if everything went according to plan, the machine gaveus a printout that we could distribute on a rolling cart. today there is still theneed for that kind of application; batch applications for payroll and receivablesand datacrunching operations have not gone away. however, as the technologybecomes cheaper, we can serve more people with different types of equipmentand software.if we take a look at what has been happening from the vendors' standpointwe find that in the early 1980s the 6502 and the z80 chips were the keyfoundations for microcomputer development. software developers were movingthe fastest in these areas and, therefore, they attracted hardware peripheralvendors, additionalfaster, smaller, cheaper trends in microcomputer technology23managing microcomputers in large organizationscopyright national academy of sciences. all rights reserved.software development, and user involvement in the applications developmentprocess at the site. these two early microprocessors have now been relegated tothe sidelines. their addressbased limitations have caused people in thebusiness graphics market, for example, to shift to larger 16and 32bit types ofhardware. thus, to purchase the older equipment today may be a bad businessdecision, though there may be nothing wrong with continuing to use iteffectively if we already have it. because of market dynamics, the purchase ofboth hardware and software is an important business decision. we don'tnecessarily want our programmers to have to solve all our application needs. ofcourse, if we have embedded systems or unique applications, we will want to beable to quickly develop new applications that meet our specifications. but wealso want to be able to make use of new applications in the field.it now appears that the intel 16/32bit chips (the 8088 and perhaps laterthe 188) may provide some stability in the area of the generalpurpose businesstool like the ibm pc. however, the apple mcintosh is coming in at a relativelyinexpensive level using the 68000 from motorola, and others will be fast on itsheels as a third generation of application software is developed.we can anticipate some exciting things for the future. these include objectoriented architecturesšdealing in objects rather than continuous lines of codešthat move from one object to another and have the kind of transfer talked aboutin the same breath with artificial intelligence. we will also have additionalcapabilities on the chip itself, which will give another level of sophistication.however eagerly we may anticipate such future developments, it is notnecessarily best to wait for them to happen. there are many things anorganization can do now to improve its productivity, even beyond an individualworkstation application to make that device earn its keep. to be concernedabout technological obsolescence is a good idea from the standpoint of longrange planning, but it shouldn't prevent us from getting new work done now.in the near term one of the major technological impacts will be in the areaof peripheral chips. this is a key area in terms of the smaller, cheaper, fastersyndrome. the ability to put all kinds of sophisticated programming locally atthe chip level and integrate it into the workstation gives the user a smaller,cleaner, and morefaster, smaller, cheaper trends in microcomputer technology24managing microcomputers in large organizationscopyright national academy of sciences. all rights reserved.powerful device. this is a characteristic of the second stage. for users stage twomeans looking out across a larger network of capabilities; for the vendor itmeans delivering more power at lower cost, with peripheral technology as wellas dedicated distributed resources. modem chips are one example of peripheralchip development; they give the capability to put a modem on an expansionboard and simply have a wire attachment coming out of the personal computer.i think peripheral chip technology will make the whole area of word processingprotocol conversion a nonissue in two or three years, when we will be able toencode all of a system's character formats and control codes at the chip levelšall transparent to the end user.what is apparent in this discussion of stagetwo developments is thatvendors have not and managers of computer user environments should notunderestimate the demand both for sophisticated devices with interfaces that areeasy to use and for powerful microprocessor systems. this demand changes thecharacter of the capacity planning capability that i defined earlier. where willwe deliver the application of software? how will we develop it? where shouldit be located in our network of processing capabilities? the question is not howmuch we can get, but what good use can we put it to? and further, how muchraw processing power do we need at the desk? this last question is part of thelarger questionšwhere do we need power in our organization?it is likely that in just a few years, we will walk into a large mainframecomputer facility and see 40 people wandering around a processor the size of afile cabinet. a brief look at what is presently available in terms of offloadingpower from the mainframe and the minicomputer suggests some of thepossibilities for the future:lisa. though i hate rodents and think the mouse is overrated and takesaway from keyboarding capabilities, lisa suggests the potential of multiplewindows. the ability to process numerous applications at one time and to movefrom one process to another as we would with pieces of paper on our desk isclearly in our future.apollo. apollo represents the workstation of the future. (one is alreadyavailable for $9,000.) with tremendous horsepower, and capable of relativelysimplified artificial intelligence research, apollo was chosen as the lead systemat the yale artificialfaster, smaller, cheaper trends in microcomputer technology25managing microcomputers in large organizationscopyright national academy of sciences. all rights reserved.intelligence labs on the basis of cost performance, multiple concurrentprocesses, and a domain operating system in which a number of units in thenetwork share their own resources. rather than having a mainframe, one keycomputer facility, apollo will have its intelligence distributed around thenetwork, with additional capacity available at any time if needed. apollorepresents a new philosophy. it defines domain as being able to reach out, notonly to share files but to gain processing power from other workstations.synapse. another company that has been involved in sharing processingpower is synapse. with each additional user a microprocessor is added into abackplaying system. the controlling logic of the system distributes theresources of the microprocessors in the network so that the user is not limited tothe power at the individual workstation.from a historical perspective, development has clearly been frommainframe to microcomputer to the concept of the workstation as a windowonto a distributed resources network. there has been development in otherdirections as well:wang. a recent offering from wang gives a sense of the other capabilitiesthat will become available. the new wang pic, a jazzedup version of itspersonal computer, works with a chargecoupled device technology to digitize apiece of paper. the user places the piece of paper containing graphics on theworkstation platform and presses a button on the keyboard; the bitmap of theimage is blown into a bitmap on the screen and is integrated into the textcapability. thus we have the integration of text and graphics at a verysophisticated level as well as the ability to store this in digital form on a drive.one of the key issues to keep in mind as we move more and more towardgraphics and high resolution bitmap displays is the necessity for highcommunications speeds to refresh the screens at appropriate intervals. anotherelement seen in some offices today is the hp7475a, a 2or 3pin plotteravailable for under $1,000 and capable of doing excellent business graphics inwork environments as a part of the local pc station.cynthia peripherals. a subsidiary of honeywell, cynthia peripherals hasbeen involved in winchester disk technology, especially removable disktechnologies. winchester technology had provided a very large volume storagecapabilityš10 megabytesfaster, smaller, cheaper trends in microcomputer technology26managing microcomputers in large organizationscopyright national academy of sciences. all rights reserved.or more. when those disk platters were full, however, the user had to buy a newone, an investment of $2,500 or more. we now have removable media. thecover of the cynthia peripherals device flops down and the user can pull out a10megabyte cartridge. this is perfect for things like electronic mail where filesmultiply like crazy. now we can issue a 10megabyte disk that has archivalcapabilities.with such devices we have continued to offload resources from themainframe and even from the minicomputer. the result is that we now have anumber of interesting capabilities at the local level. such technology clearlyrequires a sophisticated management environment. we are moving toward a stagešprobably near the end of the decadešin which data will be available in databanks at remote locations, artificial intelligence will be incorporated at thehardware level, and logic chips and wafer technology will permit the incrediblecapability of a 100 million instructions per second. from the user's perspectivethe key issue in this new stage is that the microcomputer becomes an extensionof oneself, a transparent tool for the worker.faster, smaller, cheaper trends in microcomputer technology27managing microcomputers in large organizationscopyright national academy of sciences. all rights reserved.trends in personal computer softwaremitchell kapor*the personal computer industry has, relatively speaking, no history. i'vebeen involved with pcs since 1978, and that makes me an oldtimer. inbusiness personal computers became legitimate only a few years ago, withvisicalc on the apple ii. the first hardware legitimacy for personal computersin business came even more recently, with the ibm pc. thus, to forecast whatthe industry may look like, what the products and their uses will be, is in mymind like looking at a first grader and determining what success in whatprofession that child is going to have.several characteristics of current product development and the computerfield in general make the future uncertain. first it is important to note that thepersonal computer software products that have been most successful in themarketplacešvisicalc, dbase, wordstar, and 123 are a fewšwere conceivedof, inspired by, and developed through the efforts of single individuals or, atmost, teams of two people working in isolation. in other words, those productscame about through the heroic efforts of individual geniuses. productsdeveloped through a more structured approach, which included a marketingrequirements document and a formal development teamšthat is, products withgood methodologyšhave* mitchell kapor is president of lotus development corporation, cambridge,massachusetts.trends in personal computer software28managing microcomputers in large organizationscopyright national academy of sciences. all rights reserved.not yet achieved the same degree of success in the marketplace. two examplesare vision and lisa; much is made of the hundreds of workyears that went intotheir development.this is a significant fact for us at lotus because we worry about how as anorganization we are going to depend less on the efforts of individual geniuses. itis also a concern from a buyer's standpoint. if i were a data processing managerthinking about making a multimilliondollar commitment to personal computersin my company, it would be important to deal with software vendors that had atrack record in the marketplace. at present this is an unsolved problem.to add fuel to the fire, the successful products i mentionedš123, dbase,wordstar, and visicalcšwere all originally developed in assembly language.to my knowledge, the currently released versions still run in that language,which, for a number of reasons, gives much better performance. when you'redealing with limited resources performance in terms of speed and power iscrucial, but such software is not very portable or responsive to the change inhardware environments. successful commercial products for personalcomputers in a higherlevel language, such as c or pascal, have not yet beendeveloped and in my view probably will not be accomplished with today's 16bit technology. we will have to wait for 32bit technology and go through ourinvestigation again. in the 16bit world you can make a better productšbetterin the sense of user acceptancešif you write it in assembly language.portability and other assets are the tradeoffs to make a product people will behappy with.another peculiarity of these successful products is that they were notdeveloped out of any clear sense of market need. that is almost heresy. in thecase of 123, for example, we looked at what was coming and said, ﬁaha, 16bit technology is much better than 8bit. we can do some important things.ﬂ welooked at visicalc and the other products and said, ﬁit would be great if visicalchad a graphing command in it.ﬂ that was our market research.this peculiar situation may stem from the fact that no one can be taughthow to design a personal computer software product. it is possible to takecourses, get degrees, go to seminars, and put together a huge bookshelf on howto write a compiler, how to do topdown structure systems design, and how towrite transaction processingoriented applications. there is training and a bodyof knowledge; there are experts and some wellrecognized principlestrends in personal computer software29managing microcomputers in large organizationscopyright national academy of sciences. all rights reserved.and practices. but none of these things tells you how to write a goodproductivity application for a personal computer. writing such an application isan art as well as an exercise in trial and error; software designers are likemedieval artisans. there is also a subjective and psychological element in thedevelopment of software products that please users. it involves recognizingwhat will feel right and what will work for the user. it's a big factor because endusers are not programmers and are not necessarily comfortable with thetechnology.thus, software design is not the structured, orderly, systematic, rational,and controllable process that one would like to present to potential investors ina company's software technology. i assume that the training and thedevelopment of a design methodology will come in time, but it will be a longprocess.i also know that more structured market research not only is possible but isalready being done. in general, however, successful products still come aboutbecause someone has the germ of an idea and someone, either the same personor a different person, has the technological capability to begin working on thatidea. they go off somewhere for six to nine months and come back with aproduct. that should make anyone nervous. for software companies such aslotus or any of its competitors, longterm prospects depend on moving toanother stage of development and implementation and even inspiration. wehave a long way to go.besides the strongly individual and unpredictable nature of softwaredesign and development thus far, two other characteristics of the industry makeforecasting difficult. the first is that software companies do not control theirown destinies. major software products are hardwaredriven, which todaymeans ibmdriven. the area of microtomainframe communications is a goodexample of the kind of problem that affects lotus and many other softwarecompanies. the market need is obviously there. pcs have to be hooked up, andwe can see many possible approaches. local area networks are one. once youhave 5, 10, 50, or 100 pcs handing off floppy disks from one user to the othersoon becomes tiresome. if these users were connected in a local area network,they could share files and do electronic mail. in fact, organizations are alreadyasking for these capabilities.the software company, however, understandably wants to be cautious inimplementing something that may have to be thrown out once ibm or anothercompany announces something new.trends in personal computer software30managing microcomputers in large organizationscopyright national academy of sciences. all rights reserved.major hardware shifts upset everyone, and longterm prediction is quitedifficult at this point. i can't say what we will be doing three years from nowbecause i honestly don't know. i can only talk about what i think will happen inthe next year.a third characteristic that affects software forecasting is that nobody reallyunderstands what to do with a personal computer. we have this wonderful pieceof technology for which creative people invent uses. the general public buysthe tool because it does something for them on a daytoday basis. but for ourtarget market of managers and professionals i don't think the applications ofpersonal computer technology will occupy more than 5 to 10 percent of theirtime. in the area of artificial intelligence, for example, i have given speechesabout terrific products that could make managers and professionals moreproductive. but i don't know what shape such products might actually take orwhat their impact on end users or organizations might be because the creativespark for matching up the technology to market need is simply not there. youcan't just hire a consultant to tell you, ﬁbuild this product.ﬂ i have never seen asuccessful microcomputer product developed that way and i am quitepessimistic about such efforts.so we are back to our small teams of heroic geniuses sitting in the corner.even if they have a pipeline to a national corporateuser group, such as we haveat lotus, they simply cannot ask, ﬁwhat do you want?ﬂ the technology isevolving so rapidly, both in hardware and software, that no one can fullyarticulate what is needed. an organization can only say, ﬁwell, we think weneed this, this, and this.ﬂ but could any organization have said several years agothat what it specifically needed was visicalc? absolutely not. the idea forvisicalc came out of creator dan brickland's head, was realized as a product,and now has several million quite happy electronic spreadsheet users. visicalccould not have been developed by going out and doing market research.i expect the industry's longterm future to be very exciting, with innovativeand useful applications being created and personal computers continuing tohave a major impact in organizations. but i am quite uncertain about the natureof that impact and the kinds of developments that may occur beyond the next 12months. therefore, if it were my job to figure out what to do with personalcomputer technology in an organization, i would adopt a fairly cautious longterm approach. in the near term, however, wetrends in personal computer software31managing microcomputers in large organizationscopyright national academy of sciences. all rights reserved.can say a number of things about what is likely to happen with software.recently some new buzzwords have emerged that offer some clues. one ofthem is ﬁintegrated software.ﬂ what this says to me is that the ground rules areshifting and that the market expects ibm pcs or xts to have available on themsome multifunction application. further, this multifunction application shouldaddress itself to a generic set of needs of users in organizations. there are fivesuch broad application areas: spreadsheet, database, word processing, graphics,and communications. these needs are not new, but there is a much sharperfocus and emphasis on new products coming out in these areas. previously wehad individual products such as visicalc and visiplot; today we have integratedofferings from lotus, visicorp, apple with its lisa machine, context with itsmba package, and about 15 other companies.it is apparent that, at least as a concept, integrated software is a bigwinitem. if you can deliver an integrated product that does several things well at anacceptable speed, that has a common command structure or user interface, andthat has some ability to share data, you are removing a lot of the fragmentationthat existed previously with individual products. if by learning one set ofcommands you can access database data in a spreadsheet, make graphs from aspreadsheet, and incorporate pieces of the database into your word processor,you have a much more versatile product.there seems to be general agreement that the five applicationsšspreadsheet, database, word processing, graphics, and communicationšwhilenot exhaustive, answer basic productivity needs. beyond that, there is adiversity of approaches being taken toward integrated software. no one has yetdelivered the perfect product, which, i think, would have to perform the fiveindividual applications quite well. in other words, if you give people aspreadsheet application so they can do their forecasting or their planning it hasto be at least as good as standalone spreadsheets. similarly, a word processingapplication that will be used for everything from dashing off a quick memo todoing a complete report or document should not be a substantial compromisebelow a standalone word processor.the inevitability of some compromise is one of the many criticisms ofintegrated software packages, but i believe the geniuses will solve thatparticular problem, as they have others. they willtrends in personal computer software32managing microcomputers in large organizationscopyright national academy of sciences. all rights reserved.do so because you cannot expect people to accept a product that forces them togive up features, convenience, power, or anything else they are used to. theywon't buy it, they won't use it, and it will sit on the shelf.simply meeting application needs, however, is insufficient. there isfurther agreement, i think, that integrated products as application have to beopenended so that they can connect in different ways to other applications, tovertical market products, to mainframes, to minicomputers, to voice. but thereis no general agreement about the best type of open system. we will continue tosee diversity until the market votes with its dollars.i would like to focus on the prospects for two broad approaches tointegrated software. the first is the integration of the applications themselves.recently, what i call the windowmouse type of system, both in the apple lisaand the vision product, received a great deal of attention. quite a few otherproducts adopted that metaphor. with this system it is possible to createdifferent windows or physical regions on a screen, each of which can have adifferent application in it. as a user i can set up my system so that i have myword processing in one window and my database in another. i can blow up asingle window and work with a fullscreen spreadsheet, then shrink it backdown. and i control the entire application using a pointing device such as amouse. the general enthusiasm for the window approach, particularly with themouse, appears to be leveling off. there has even been some mouse backlashclaiming that perhaps keyboards are not so bad after all, especially for creatinga document.for me, the main problem of the window approach is that there aredifferent applications in each window. these are like little islands connected bybridges over which data move. let me give a concrete example using lisacalc,which is a spreadsheet: to do forecasts and graphs, you have to cut the data outof the window and put it onto a clipboard. it then gets pasted into the graphingprogram and makes a graph. to change a number on the spreadsheet or to getdifferent summary numbers, you must move back to the spreadsheet window,enter the new number, cut the data out and paste it back in again. this turns outto be a laborious and cumbersome process.what people really want is to make some new numbers, hit a key, and seea new graph come up on the screen. no one has yet discovered how to do this ina window system. each window (application) has its own separate datastructure sharing a commontrends in personal computer software33managing microcomputers in large organizationscopyright national academy of sciences. all rights reserved.interface so that things can be moved around. but from the user's point of view,why is this necessary? why can't the user have a unitary data structure that canbe called up as a document, or a spreadsheet, or a database? although thismight be a much more satisfactory approach for the users, it has yet to berealized. so it is back into the little locked room for the geniuses to design aviable product based on user need.the second approach to integrated software and, i think, the big coup, hasto do with the integration of data rather than the integration of the userinterface. things should be simple, straightforward, consistent, and intuitive forusers. a common user interface certainly goes a long way toward that goal, butuntil data is completely transparent to the user, many people will shy away fromusing the products.this integration of data is involved not only in the personal computer butis also important in the microtomainframe link. we are in the midst of a greatdeal of very painful progress in that area. recent developments are certainlymaking it much easier and more convenient to move data from a mainframe to apersonal computer, but we are still at a rather primitive stage in terms of howwe actually put the data inside of a pc. the process will not be transparent untilthe next generation in development, whenever that occurs.there is also a long way to go in the area of communications. i predict thatit will be people in organizations willing to invest and devote more of their ownresources and cleverness who will develop usable systems. and i expect it willtake a least another couple of years to see this happen.i have no doubt that we are going to see good integrated applications andintegrated data structure. but i think it is equally important that future softwareproducts be as openended as possible and have different qualities or types ofopenendedness. in the near future, i think it is safe to say that organizationsmay have their own staffs of application developers. they will want to createcustom applications for their own corporate needs or they may wish to createapplications in some vertical market and sell them. organizations may wanteither to develop an application from scratch or some higherlevel language thattalks to integrated applications so users can get the benefit of the spreadsheet,the graphing, and the database regardless of the specific application.trends in personal computer software34managing microcomputers in large organizationscopyright national academy of sciences. all rights reserved.beyond development from scratch, which is expensive, time consuming,and hard to control, a productivity application can be made openended so thatan expert user (not a programmer) within the organization can create a modeltemplate application. this application can then be used as a standard within theorganization. to do this, however, expert users must have the necessary tools.the first, stumbling effort lotus made in this direction was to include amacrolanguage with 123 that contained the rudiments of a formalprogramming language, control structures, variables assignments, and userinterface primitives. it is clumsy and anything but elegant, but it is being used incorporations far more than we ever could have predicted. in fact, some of ourcorporate accounts have devoted 5 and 10 workyears of development in thisarea and their results are being distributed to hundreds, perhaps thousands, ofusers within their own organizations worldwide.a third type of openendedness allows the individual application to talk toand work with other offtheshelf retail applications. i predict that in the futureusage patterns may well be focused on one or two core standardized productswithin the organization. at the same time there will be legitimate needs forperhaps as many as two dozen other more specific applications that can also bepurchased off the shelf. therefore, it is critical that a core productivityapplication be able to be integrated with those other applications. there are avariety of approaches that will be coming out to achieve this.in sum, openendedness is not a simple thing. there are communities ofusers with different needs, vertical market needs, internal needs, the desire tocreate salable addon applications to a core product. i think it will take a coupleof years to sort out what the most desired aspects of openendedness are and todetermine what will become the accepted standard.blessing or curse, we live in interesting times. despite all the uncertaintiesi am quite optimistic about the value of delivering personal computer softwareand in particular integrated software. i see an absolute shift in these deliveriestoward the corporate marketplace. as this shift is taking place, i think softwarecompanies are maturing as entrepreneurs and are learning to understand the realneeds of organizations. with a modest amount of patience on both sides,organizations and computer companies are going to achieve their mutual goal ofdelivering good information processing to the broadest community of users.trends in personal computer software35managing microcomputers in large organizationscopyright national academy of sciences. all rights reserved.personal computer networks.robert m. metcalfe*there are three kinds of people in the world: the technologists who believethat technology can, will, and should turn the world upside down and who areengaged principally in revolution; the users who believe that everything ismoving too quickly, wish it were 1965 again, and whose principal activity iscounterinsurgency; and those who recognize that technology will reach usefulapplication more slowly than most technologists would like and more quicklythan most users would like. the progress of this technology is measured not byany absolute timeframe, but on the basis of how successful we are in matchingthe superstructure of technology to the infrastructure of organizations.local networking, for example, is one technology whose superstructure isnow being matched, successfully, to the infrastructures of various organizations.it is a technology bent on revolution: in my view, the next decade can becharacterized from a computing standpoint as the decade of the localnetworking of personal computers. this represents the third major phase in thehistory of computers and in the application of computing technology. not longago computing meant batch processing on mainframesšthe* robert m. metcalfe is founder, chairman, and vicepresident of strategies andprojects, 3com corporation, mountain view, california.personal computer networks 36managing microcomputers in large organizationscopyright national academy of sciences. all rights reserved.first phase. then computing became timesharing on minicomputersšthesecond phase. now we are moving more and more into local networking ofpersonal computers. each phase broadens the market for computing and thenumber of applications for computing and brings more and more computing to agreater number of people.networking adds a sixth category to mitchell kapor's five applicationsšword processing, spreadsheet, database, graphics, and communications. whatare variously called local computer networks, local area networks, or morefrequently just networking answer the need for a system to connect hundreds ofcomputers on separate desks. there are approximately 200 varieties of localnetworks now being sold, of which ethernet is only one.there are two approaches to this third stage of computer development:what i call the ethercentric view and the boxcentric view. it is easy to identifywhich is which. ask people to draw their computer systems on the blackboard.if they begin by drawing a box they are boxcentric. if they begin by drawing asweeping line across the board they are ethercentric and have adopted therevolutionary view that communication is central. computers are viewed as thearray of resources around communication; they are seen less as arithmeticdevices and more as communication tools.with the arrival of personal computersšparticularly the ibm pcšinlarge enough numbers and with sufficient power to be usefully connected athigh speed in local networks, the pressures for the development of networkingbecame overwhelming. now that the ibm pc is here, many believe that allprogress can stop. i do not subscribe to that point of view; i do believe thatmany other personal computers will continue the trend toward increasedcomputer power on the desk. but the important crossover has already occurredbetween the cost of providing multimegabit, highspeed local networking, asrepresented by ethernet, and the density and power of personal computers. thismeans that it is now important enough and cheap enough for a small portion ofthe industry to get involved in networking personal computers. so the ceilingtiles are coming down again and more cables are going in.for what are these local networks of personal computers being used? firstthere are the basics, such as peripheral sharing. with more and more personalcomputers economics becomes increasingly important. sharing resources,principally printers and disks, is the first step. we start out by trying to do whatwe arepersonal computer networks 37managing microcomputers in large organizationscopyright national academy of sciences. all rights reserved.already doing more cheaply. we then try to do things we haven't done before,which begins the second phase of local networking.this second phase involves the use of local networks to give access topersonal computers or to give personal computers access to information. mostof the information currently computerized is on mainframes. thus, a highpriority item is to give personal computers access to mainframes. that is whythe ibm pc 3270, which allows pcs to act as terminals and get information thatalready exists on mainframes, is so significant. as less and less of ourinformation exists on mainframes, however, the priority will shift to personalcomputers communicating with each other. now that personal computers exist,there is a trend to bring data into the local workgroup where they are generatedand used. in the future, i believe we will see as much as 80 percent of thecomputerized data being kept not on a corporate mainframe but on adepartmental or even a workgroupshared file system.the third and final use of local networks of personal computers is as toolsfor communications. this brings up the subject of electronic mail.in 1970 the appeal of electronic mail was that you didn't have to movepaper around anymorešyou could move electrons. and the transmission ofelectrons was much less expensive than the transmission of cellulose. itappeared that electronic mail was concerned with transmission, movinginformation, and the economies of moving electronic information. however, wequickly realized that although the cost of sending the information might be lowwe were spending 15 dollars to prepare the document we were sending.electronic mail was synonymous with its preparation, and it became verypreparationintensive.we also realized that we were spending a lot of time moving electronicmail from desk to desk manually after computers prepared and transmitted it.the fact is, most people don't want to send messages from one post office toanother, they want to send them from one desk to another. electronic mail thenbecame a distribution problemšspecifically the development and maintenanceof distribution lists. we are now in the distribution phase of this trend inpersonal communication. the focus of current progress is the creation ofdistribution mechanisms for electronic mail. and the computer industry ismoving into the next phase.because local networks of personal computers are so effective atgenerating and delivering electronic messages, electronic mailpersonal computer networks 38managing microcomputers in large organizationscopyright national academy of sciences. all rights reserved.has become a filing and database problem. we now receive so many importantmessages we can't afford to throw away the 10 megabytes that can be quicklyconsumed by our electronic correspondence. in the future, electronic mail willinvolve even more dataintensive modesšfor example, voice integration in themessaging system, eliminating the telephone tag but maintaining voice, andeventually formsbase messaging in which the contents of the message ismachine processable.where does software development fit into the trend toward localnetworking of personal computers? i would put software into five broadcategories in terms of its relation to networking. in some of these categoriesactual software does not yet exist. the first category i would callﬁunnetworkedﬂ software, and there is little of it left. unnetworked software runsonly on a personal computer and cannot in any way be networked, eitherbecause there is no transparent networking available or because the softwareimplementors have not used the standard operating system available. very fewof these software packages would interest organizations.the second category can be called transparently networked software. thisis software that uses the operating system cleanly, and networking facilities thathave been developed for that operating system can be used transparently bypreexisting software. most of the software that is available today, including123, can be and is transparently networked.the third category i refer to as networkdelivered software. the use of thenetwork to deliver the software is a substitute for the floppy disks that most ofus have come to think of as a delivery mechanism for software. examples arevisicalc and visiword that can now be distributed without diskettes through3com's local network. these facilities can be bought for a large group of users.supply users with the appropriate number of manuals, and they can get thatsoftware over the network much more quickly than they can off a floppy disk.and, they do not have to worry about storing the floppy disk. those of us whowere involved in the struggle to eliminate punch cards during the 1970s are nowin the process of eliminating floppy disks.the fourth category of software i call multiaccess (or multiuser) networkedsoftware. the best example of this would be a dbase ii personal computerdatabase package that would allow users on multiple pcs to be concurrentlyaccessing and updating a sharedpersonal computer networks 39managing microcomputers in large organizationscopyright national academy of sciences. all rights reserved.database through the network. this is not the same as transparent networkingusing the current dbase, which is a singleuser system that does not provide forshared access to the same database. with such a system, there is the danger ofmultiple accesses damaging each other. thus, the obvious next step is makingavailable multiaccess database software using local networks to bring a numberof pc users to the same database for concurrent access and update.the fifth and final category i call networkintegrated software. just as in123, which integrates database, word processing, and graphics into a uniformuser interface, and just as in vision, where the entire user interface has beenintegrated along with a variety of applications, we can think of integratingnetworking into other applications for volume management. the movement ofdata from one place to another is a part of the natural use of integratedapplications.as far as i know, only the first three software categories now exist:unnetworked, transparently networked, and networkdelivered. but the othertwo are coming. in fact, the objective of the fifth category of software is toeliminate itself. in other words, when networking software exists, we won'thave to talk about it as a separate category because it will be lost in theintegration and become part of the applications that users actually need. no oneactually needs networking, they need the applications it makes possible. theseapplications will be the focus of the coming decade.personal computer networks 40managing microcomputers in large organizationscopyright national academy of sciences. all rights reserved.part iismall computers in largeorganizations the implications 41managing microcomputers in large organizationscopyright national academy of sciences. all rights reserved. 42managing microcomputers in large organizationscopyright national academy of sciences. all rights reserved.introductionhannah i. blank*the implications of small computer technology for large organizations,whether corporate, government, or academic, are farreaching. they includetechnical, organizational, and, to some extent, societal issues.the technical implications may be the least difficult to handle: data processing (dp) professionals will need new skills, such as interfacingmicros with minis or mainframes and generating applications from genericpackages with programmer tools different from what they are accustomedto using. questions of architecture will have to be solved. where does themicrocomputer fit in the overall computer architecture of a givenorganization? data integrity and security are highvisibility concerns magnified by theubiquity of the micro and the ease with which diskettes can be copied andtransported. small computer technology has generated a new set ofimplications for an organization and its management: new job descriptions are emerging. functions that did not exist before areto some extent replacing existing functions. higher skills may be required in the same job functions. this may appearthreatening to some individuals, while others may regard it as anopportunity. one example is the use of the microcomputer* hannah i. blank is vicepresident of the domestic institutional bank, chasemanhattan bank, new york, new york.introduction43managing microcomputers in large organizationscopyright national academy of sciences. all rights reserved.by the secretary not only for word processing but for functions related tospreadsheets, data management, and even graphics. some may respondenthusiastically to learning new skills; others may be intimidated. different loci of power are being created in the organization, and turf issuesabound. motivated by their own productivity needs, people outside the dataprocessing field, such as financial analysts, are to some extent directing theuse of micros. this can pose a threat to the control that dp managementhas regarded as its right and responsibility. a whole range of societalimplications and concerns are inevitable as microcomputers become a partof our everyday lives: unlike the mainframe and the minicomputer, whose use was confined to aprofessional class with specialized skills, the microcomputer is infiltratingthe lives of a great many people. it is accessible and usable at some level ofcomplexity by virtually everyone. on the job this will increase the demandfor micros, for training, and for mobility. it also means that some computerliteracy will in all likelihood be acquired at home. the microcomputer adds fuel to the flames over ﬁhomework,ﬂ an emotionalissue for women and minorities. homework has positive benefits forwomen with children who do not want to leave home for a full workingday; for the disabled who cannot leave home; and for individuals withmany interests who wish to work only part time. posed against thesebenefits are the opportunities for employee exploitation on the part of theemployers. the cost and transportability of the micro magnify thepossibilities.the next three chapters offer a closer look at some of the specific problemsthat are arising as microcomputers become accepted tools in large organizations.john bennett, of united technologies corporation, considers theimplications of microcomputer growth for both systems departments andgeneral management. ray kline, of the general services administration,explains some of the problems the federal government faces and the steps it istaking to respond to technological change while maintaining managementcontrol. finally, james h. bair, of hewlettpackard, explores the evolution ofmicrocomputers and their potential roles in the office of the future.introduction44managing microcomputers in large organizationscopyright national academy of sciences. all rights reserved.managing uncontrollable growthjohn h. bennett*the explosion in the acquisition and use of minicomputers andmicrocomputers is by now quite familiar. over the past 10 years i haveobserved this growth from the point of view of a large, driversified,multinational corporation, united technologies corporation (utc).united technologies is a fortune 100 corporation whose companiesmanufacture many kinds of high technology products. these include pratt &whitney aircraft engines, sikorsky helicopters, carrier airconditioningsystems, otis elevators, mostek semiconductors, and a broad range of electricaland electronic devices and controls. all of these companies are experiencing thecurrent microcomputer revolution. since utc allows its divisions a high degreeof autonomy in management, each has reacted in its own way, both to thepressures of technological change and to the guidelines provided by thecorporate office. thus, utc offers a unique opportunity to study a variety ofcompanies, determine what the major common problems are, and see whatmanagerial responses seem to be most successful.most organizations using computer technology experience growth in thenumber of terminals used. while this growth rate is* john h. bennett is corporate director of data processing for united technologiescorporation, hartford, connecticut.managing uncontrollable growth45managing microcomputers in large organizationscopyright national academy of sciences. all rights reserved.steady, it is by and large related to the rate of installation of systems on a centralcomputer. thus, it is nominally under control. workstation growth is a differentstory.proceeding from a smaller base, word processors, personal computers,engineering design stations, and intelligent remote jobentry stations aregrowing at a much faster rate. many of these applications require little or noimmediate support from an organization's main data center. the systems areinexpensive enough to get over or around the hurdles of cost justification thatcorporate bureaucracies have placed in the path of computer acquisition. theirgrowth is thus difficult to control. however, management of the process isobviously necessary for a couple of reasons. many applications will eventuallyrequire data access and data integrity; others clearly should be carried out on acentral computer.the reasons for this rapid growth are threefold: service, economics, andcontrol. most computer service organizations or departments have a backlog ofup to two years of costjustified applications. customers see minicomputers andmicrocomputers as a shortcut around this bottleneck. their cost is relativelyeasy to justify, and users have at least the appearance of being in control of theirown destiny.the impact of these developments on computer service organizations hasbeen devastating. the first symptoms of change took the systems departmentsby surprise. user departments presented requests for specialpurpose,minicomputerbased terminal systems for special applications. the firstrequests seemed innocuous enough: a computer for a laboratory; a dedicatedtest application; a standalone, computeraided drafting application. suchprojects were outside the normal range of competence or responsibility of mostdata processing departments, and they were clearly limited in scope. or werethey?at united technologies over 500 of the digital equipment corporationpdp 11s have now been brought in for these ﬁlimitedﬂ applications. there areover 50 vaxs with their full network of supported terminals, virtually all ofwhich are beyond the direct control of the systems department. the same dramais about to be played out in the area of manufacturing support.the first microcomputers were brought into my corporation by theirowners, who were convinced of their utility but could not get approval topurchase them. in one division i found that an employee had set up his ownmicro in a conference room and providedmanaging uncontrollable growth46managing microcomputers in large organizationscopyright national academy of sciences. all rights reserved.a signup sheet so that other employees could schedule time to use it. needlessto say, i felt obligated to mention to management that an apple computer is fattoo inexpensive to have expensive professional employees queueing up for it.now, of course, the problem has changed to one of moderating the rate ofacquisition. for the first time computer service organizations are beingthreatened by their own technologies. the warnings are clear: improve serviceor lose control. a twoyear backlog is no longer viable. the ibm solution is notthe only acceptable one. and corporate data can no longer be accessible only toinformation specialists.the twoyear backlog is, of course, the result of a rational attempt toestablish priorities for a large workload. an unfortunate consequence is thatsmall jobs, important only to one person or one department, never seem to getdone. a number of options are available to computer service organizations(csos) to attack this backlog. system generators and application generators areone obvious choice. these tools can make a programmer up to five times moreeffective. such tools also tend to be selfdocumenting, making latermaintenance easier.another toolrelated problem has been the willingness of somedepartments to sacrifice service to the user in favor of operational convenience.many cso managers who have pointed to ibm or some other preferredvendor's solution to a problem as if it were the only solution are now waking upto find their customers installing another vendor's equipment and runningproprietary software. and in the process their own responsibilities have beenreduced to downloading data files to the new system. more enlighteneddepartments have tried to provide modern tools for users to get at their data.some csos have even taken the offensive and initiated educationprograms to show users what systems can do for them, including whatapproaches are likely to be successful and what the limitations are. theseprograms may be addressed to a specific group, such as a manufacturing unit, orto management in general. such efforts improve the odds that whateverapproach a user decides onšmini, micro, or main computeršwill bereasonable and will have a high chance for success.microcomputer growth has produced a number of problems for generalmanagement as well as for systems departments. the problem of computersecurity, both external and internal, is now amanaging uncontrollable growth47managing microcomputers in large organizationscopyright national academy of sciences. all rights reserved.concern at the highest levels of corporate management. with more managersand executives using terminals and personal computers, the problems of dataaccess and corresponding data security can no longer be avoided. utc is onlyone of many companies considering new guidelines for computer security.cost is another management problem. in more and more corporatenegotiations over system support, service wins out over compatibility. terminalbased systems are more expensive than their batch report counterparts, whetherthey are mini, micro, or main computer based. at utc, for example, the cost ofhardware is growing both in absolute dollars and as a percentage of the dataprocessing budget, despite the advancing technology.united technologies corporation has used a number of approaches tomanage the latest stage of the computer revolution. the most obvious is the useof report writers and other highlevel languages. these allow data processingprofessionals to respond to requests for service that require occasional reportsdrawn from existing data bases or the calculation of specific results on aninfrequent basis. some of these tools are simple enough to be taught to endusers as well.along with the personal computer, these new tools are the major focus ofthe information centers that are currently springing up throughout unitedtechnologies. the information center at the essex group headquarters, forexample, has achieved national acclaim. it uses terminals, microcomputers, anduser friendly software to help its customers perform a variety of tasks that mightotherwise have to be done by data processing.another response to service problems is to tackle the backlog directly byincreasing programmer productivity. there is increased interest today in the useof automated programming aids such as program generators. in the area ofprogram or system generators utc has been somewhat conservative. theraytheon product, readicode, is used at one company; the burroughs product,link, is used at another; a third unit using a homegrown applicationsgenerator is probably the best of the three. i cannot overemphasize the power ofa welldesigned application generator. one of utc's smaller divisions installeda broad range of financial and administrative systems over a twoandonehalfyear period. during that time the systems development group never includedmore than twoandonehalf employees. all applications were fully customized,not packages. the latest was amanaging uncontrollable growth48managing microcomputers in large organizationscopyright national academy of sciences. all rights reserved.complex purchasing system (10,000 lines of cobol code) begun in may andturned over to the users for testing in august.utc is perhaps unique in the area of management education. in 1982 thecompany decided to invest in a major effort to educate its executives in thecapabilities of computers. the method chosen was to develop a trainingprogram for executives. the resulting program, implemented in late 1982, iscalled the executive personal computing workshop. this workshop, a threeday session conducted at the united technologies research center, is restrictedto senior managers and executives. it offers a handson approach using the ibmpersonal computer and the context mba software. following the program,participants receive a personal computer for their ﬁbusiness use.ﬂexecutives in this program are taught to use an array of standard tools:automated spreadsheets, word processing, business graphics, andcommunications. no jobspecific applications or programming techniques aretaught. although the threeday time period does not permit inclusion ofdatabase applications, a selfstudy database module has been developed for laterstudy. considerable effort was spent developing this program, which includescustom training materials rather than manuals. individual workstations areconnected to the instructor's station through a switching network that allowsparticipants to view the instructor's screen on their own screens by flipping aswitch. the program has been well accepted and is one of the most popular thecorporation has ever run. it suggests the type of imagination that dataprocessing organizations will need in the future if they are to regain control oftheir enterprise.the current upheaval has some predictable longerterm effects as well.first, it is safe to assume that by the end of the decade virtually every whitecollar worker, and many others, will have or have access to some form ofworkstation. the intelligence built onto that station will vary with user need.similarly, whether it is connected to a minicomputer or a mainframe willdepend on the application. second, this rapid growth in workstations willimpose an enormous training requirement on organizations. already companiesare springing up to meet this need, and vendors are unbundling their training toreap additional profit. even computer service organizations will not be immuneto this training requirement due to the acquisition of new productivity tools.what will happen to computer service departments in the future?managing uncontrollable growth49managing microcomputers in large organizationscopyright national academy of sciences. all rights reserved.clearly there will be far less need for mundane programming skills given theuse of productivity tools both by the systems department and by customers.although computers will be much more widely distributed than they are today,systems departments will neither wither away nor will their leaders becomesuper information executives. on the contrary, these departments will return totheir original function of providing professional services consisting of computersupport, file management, systems analysis, and program generation. they willbe smaller, highly professional organizations.finally, if our organizations have an apparent destiny, we must decide howwe can best lead them toward it. there are two sides to this question, which canbe illustrated by two stories.the first story is about the traveling salesman who, after searching severalhours for his destination, finally encounters a farmer. when asked how to reachthe destination, the farmer sadly replies, ﬁyou can't get there from here.ﬂluckily, for most of us, the situation is not that bad. for some companies,however, that statement is all too true. faced with fragmented systemsresponsibilities, internal bickering over resources, and no central control ormanagement commitment, the only way they can deal with the coming changesis to restructure, to become different organizations.the second story comes from lewis carroll's classic, alice's adventures inwonderland. alice, who is lost, asks one of the consultants of her day, thecheshire cat, for directions:ﬁwould you tell me, please, which way i ought to go from here?ﬂﬁthat depends a good deal on where you want to get to,ﬂ said the cat.ﬁi don't much care wherešﬂ said alice.ﬁthen it doesn't matter which way you go,ﬂ said the cat.ﬁšso long as i get somewhere,ﬂ added alice, as an explanation.ﬁoh, you're sure to do that,ﬂ said the cat, ﬁif you only walk long enough.ﬂthis exchange reflects the more normal situation in many companies nowtrying to make decisions about which way to go with computer technology. thepossible paths are management approaches. the goalsšwhere we want to gošare strategic directions. we must choose: do we want to be on the leading edge of the technology and possiblyachieve a strategic advantage over our competitors? if so, the cost is high,and so is the risk of costly blunders. we havemanaging uncontrollable growth50managing microcomputers in large organizationscopyright national academy of sciences. all rights reserved.to develop many of our own tools. costbenefit analysis as a managementtool is virtually useless. are we willing to stay a step behind the leading edge? if so, we are lesslikely to achieve a competitive advantage. however, the cost is less and sois the risk. most applications can be economically justified. but somedecisions will still have to be justified on strategic grounds. do we want to stay well back from the leading edge, implementing onlyproven and therefore costeffective systems changes? if so, we will haveminimum cost and minimum risk of failure. we are, however, at maximumrisk of finding ourselves at a competitive disadvantage.i cannot recommend which path to take. each company must make its ownchoice, and it should choose a path consistent with its management approach inother areas. but most importantly, it should not pick a path by default and itshould not let its technicians do the choosing.managing uncontrollable growth51managing microcomputers in large organizationscopyright national academy of sciences. all rights reserved.managed innovation controlling endusercomputing in the federal governmentray kline*in 1982 the general services administration (gsa), responding to thelongawaited microcomputer revolution, convened a small group ofknowledgeable people in government to help determine what the federalgovernment's policies should be in this emerging area. the group met every twoor three weeks to look at various aspects of the problem and to see what mightbe done. the quandary it faced was change versus controlšthe government'sneed to be responsive to technological change and the need to maintainmanagement control.in the area of computer technology, of course, change is essential andinevitable. decades ago the federal government was in the vanguard ofautomated data processing and related technologies. today it lags behind.studies during the ford administration (1974œ1976) placed government about10 years behind the private sector. by 1983 the grace commission reported thatthe gap was only 6.7 years. it is possible that by 1990 we will have caught upwith or even surpassed the private sector. however, the driving force to initiatechange is not merely to keep up with the private sector but to improve theefficiency of government and to increase the productivity of the federal worker.president reagan has impressed upon his entire cabinet the need to cutgovernment* ray kline is acting administrator of the u.s. general services administration.managed innovation controlling enduser computing in thefederal government52managing microcomputers in large organizationscopyright national academy of sciences. all rights reserved.spending, to increase productivity dramatically, and to perform the functions ofgovernment at a lower cost with fewer people.at the same time there is the need to maintain control. and in the federalbureaucracy controls are imposed somewhat differently than in the privatesector. the government is not, as one often reads, just another large corporation.there is a general accounting office (gao), congressional legislation, a boardof directors (congress) not always in agreement with the chief executive officer(the president). all of these players must be considered when developingeffective controls for microcomputers in the federal government.compounding the problem, the gao has issued an audit report stating thata runaway condition exists in the use of personal computers in the federalbureaucracy. this situation has arisen even sooner than originally predicted.federal employees aren't waiting for guidance. they are bringing in their ownequipment and putting it to work. in my trips around the country i have seen thegrowing number of computers, and although the numbers vary in differentdepartments no one can deny that there is a lot of activity. in the budget forfiscal year 1985, for example, new automation requirements are in the hundredsof millions of dollars. these requirements serve to indicate what employeeswant to do with personal computers as well as with some of the larger systems.some degree of control in this process is imperative.with these two considerations in mindšthe need to make change and theneed to control itšgsa's microcomputer assessment group performed its workand in june 1983 released a report, managing enduser computing in thefederal government. the assessment group identified three components of enduser computing: mode, users, and technologies (table 1).from a management perspective the report concludes that the integratedmode requires the most attention. in reviewing users and technologies theassessment group observed that in the past heavy emphasis was placed onclerical applications. now the technology has useful applications throughout thework force. therefore, although the group focused on personal computers, othertechnologies were considered, along with the broad range of potential users.the group considered two possible courses of action to deal with thegrowth of personal computers in the federal government.managed innovation controlling enduser computing in thefederal government53managing microcomputers in large organizationscopyright national academy of sciences. all rights reserved.table 1 components of enduser computingmodetechnologiesusersstandaloneword processingclericalsintegratedoptical character recognitionoffice professionalselectronic mailscientistsmicrographicsfacsimilepersonal computersdictation systemsterminals and networksdocument storage/retrievalgraphicsdistribution data processingone was to institute a moratorium on the purchase of computers untilthings could be sorted out. the other was to allow activity to continue and try toprovide an interim environment for learning. needless to say, judgment ran infavor of the latter.the group then began to look at what kinds and levels of controls andsupports would be helpful in creating this learning environment. in the processmembers of gsa's advisory board, composed mainly of vicepresidents ofprivate sector companies, were asked about the controls they imposed on theirown enduser computing. approximately half said they had tight controls. theother half said they had rather loose controls, and some wished they could beginagain and include more control features.the assessment group began to shape a new microcomputer managementenvironment around the idea of sticks and carrots. control was the stick;support or encouragement was the carrot. sticks include the kinds of standardsthat should be imposed on people and organizations and the levels at whichsuch restrictions apply. the group identified three main types of carrotsšprocurement vehicles, support structures, and education tools. in each area thegroup concluded that more and better carrots were needed. for example, therewas room for much improvement and streamlining of the federal procurementsystem. support structures in departments and agencies were not at the levelsthey should be. improvement was needed in both managerial supportmanaged innovation controlling enduser computing in thefederal government54managing microcomputers in large organizationscopyright national academy of sciences. all rights reserved.of changes and technical support to new users. more and better education toolswere required at all levels. the most obvious needs in the education area werefor better training plans, reference tools such as buyer's guides, and aclearinghouse for information on what other people were doing.from a broad perspective, what was needed during this transitionallearning period was an environment for growth at a pace in keeping with eachorganization's ability to effectively apply a new technology. the assessmentgroup evolved several broad goals to underlie its managementrecommendations for this transition period: create a friendly but wellmanaged environment. don't scare people with too many control systems. encourage the use of technology to improve productivity.improved productivity must be the end result because, without the privatesector's bottom line, productivity is the only way to judge whether efforts aretruly costeffective. beware of the runaway train syndrome, which could resultone day in a pile of unproductive equipment and a situation that is out of control.the group called its approach the ﬁmanaged innovation program.ﬂ itsobjective was to meet the needs of a oneor twoyear transition period. duringthis time microcomputer users could develop an understanding of what is goingon in the marketplace, and the market itself might stabilize somewhat. there aretwo parts to the managed innovation program: a set of 13 governmentwideinitiatives for which gsa is responsible and 12 individual agency initiatives.these initiatives are summarized in table 2.governmentwide initiatives fall into four categories of purpose: policy,agency assistance, education, and inhouse learning. these served as commondenominators for the recommendations and suggestions made by the assessmentgroup.the group had to evolve policy not for the long term, but for the transitionperiod. thus, it recognized the need for a factfinding facility to stay currentwith the activities of different departments and agencies, many of which werequite advanced. local datanetwork policy was another area that neededconsideration. an interagency group is now working to develop a deeperunderstanding in this area that may evolve into policy guidelines. a final policyarea involved revision of the regulations and themanaged innovation controlling enduser computing in thefederal government55managing microcomputers in large organizationscopyright national academy of sciences. all rights reserved.table 2 gsa's managed innovation program initiativesšgovernmentwide and within agenciesgovernmentwideindividual agencypurposeinitiativepurposeinitiativepolicy1. develop a factfinding facility.policy and planning1. establish a general policy.2. develop a local data network policy.2. develop a strategic plan.3. develop agencywide data rules.3. revise regulations and guidelines.4. consider a telecommunications network.agency assistance4. develop a buyer's guide.5. use standards to promote compatibility.5. develop a procedures cookbook.6. provide procurement vehicles for equipment andsoftware.review6. establish an evaluation program.7. develop a nationwide cluster maintenance contract.process7. develop concise justification procedures.8. establish review and approval procedures.education8. organize a conference for agency executives.use assistance9. identify classes of uses and users.10. encourage formation of tribes.9. establish a forum for line managers in agencies.11. establish support structures.10. promote a joint literacy plan.education12. develop a computerliteracy plan.inhouse learning11. investigate questions of records management.12. conduct a data and equipment compatibility project.13. perform an enduser pilot project.managed innovation controlling enduser computing in thefederal government56managing microcomputers in large organizationscopyright national academy of sciences. all rights reserved.guidelines themselves to make them more responsive to actual needs.governmentwide initiatives should help people get started. the managedinnovation program calls for agency assistance in two main areas. one involvesobtaining and maintaining equipment, providing procurement vehicles forequipment and software and developing a nationwide cluster maintenancecontract to streamline procurement modes. the other involves helping the userby providing buying and procedural guides.governmentwide initiatives have already begun in the area of education. in1983 senior executives from federal agencies met to discuss problems and makesuggestions about how gsa could improve its central managementperformance and about what individual agencies could do.in the area of inhouse learning there have been some notableachievements within individual agencies, such as the department ofagriculture's graduate school program, the gsa interagency training center,and other departmental efforts. new initiatives are needed. gsa conducted itsown enduser pilot project, one of the recommended initiatives in the inhouselearning category.initiatives of individual agencies form the second part of the managedinnovation program. although specifics will differ according to the needs ofeach agency or department, initiatives can be grouped in four broad areas:policy and planning review, process, use assistance, and education.in the policy and planning area the top management of each agency mustassume responsibility. further, the organization needs to understand topmanagement's position on microcomputers to move through the transition phaseand into the future. strategic planning for microcomputers should address bothvertical (topdown) and horizontal perspectives on the organization.planning must also take into account the need for agencywide dataregulations and periodic reviews to determine that resources are being usedproperly. management must make sure that the public's money is being wellspent. agencies must move away from the earlier management mentality, whichwas dominated by the large, mainframe, automatic data processing departments.they must avoid the trap of believing that the only way to control the endusercomputing environment is to employ the kind of controls imposed on the largesystems in the 1960s. in its report the assessment group recommends thatcomputing be placed in themanaged innovation controlling enduser computing in thefederal government57managing microcomputers in large organizationscopyright national academy of sciences. all rights reserved.hands of line managers, giving them the resources they need to do theirparticular jobs.in the areas of user assistance and education recommended initiativesinclude encouraging people with common problems to get together, providingways to help them, and developing computerliteracy plans that meet individualand agency needs.after the assessment group completed its report gsa began to apply itsprinciples through an enduser pilot project. top management informed gsa's30,000 employees around the country that it was interested in their ideas on enduser computing. to evaluate the enduser computing proposals that resulted, theagency formed a twoperson review panel made up of one member whounderstood the world of enduser computing and the other who understood thebreadth of gsa's activities. proposals had to include justifications notexceeding one page, a brief description of the application, and an estimate ofthe gain in productivity. gsa received over 100 proposals, and about half wereapproved. the people involved in the proposals that have been implemented areconvinced of the value of these tools to the performance of their jobs. to helpthem use the approved applications, gsa set up a threemember technicalsupport group. the agency also set up a steering committee and authorizedsome broader applications throughout the organization, such as automatingmany of the general counsel's activities and tying in with the white house'ssystem of electronic mail.the enduser pilot project has resulted in some real improvements. therehas been a definite gain in productivity. reports are coming out in a fraction ofthe time that had previously been required. capabilities now exist for ﬁwhatifﬂanalysis that were not previously available. finally, the quality and accuracy ofthe work product have improved.perhaps just as important, the project also identified areas that need moreattention. proposal ideas came, by and large, from people directly involved withenduser applications, and reflected their considerable knowledge. but theproposals also reflected the fact that people had obviously been working andlearning pretty much on their own. they pointed up the need for a strong,effective training component, including a survey course of what an endusercomputer can do and a handson training session. the agency concluded that, interms of training time, there should be at least a twotoone ratio of handsontraining versus lecture. tomanaged innovation controlling enduser computing in thefederal government58managing microcomputers in large organizationscopyright national academy of sciences. all rights reserved.limit training to broad brush exposure is not enough. all 50 people involved inthe pilot project stressed the need for much more initial handson training inorder to achieve competency in a much shorter period of time.in the area of technical support gsa found it indispensable to have an inhouse group of people who are not vendors to advise and help users byanswering questions, many of them procedural, on how to get things going.such inhouse capability eliminates the problem of a vendor walking off afterdelivering equipment or software and leaving users to their own devices asproblems arise.in the area of procurement gsa found some isolated cases in whichcomputers were being chosen on the basis of price alone, with no considerationfor training and support components. in certain cases the hardware came fromvendors hundreds of miles away, making onsite support extremely difficult.such experiences emphasized the importance of procurement procedures thatfocus not merely on the hardware price alone, but include the total package ofhardware, software, training, and technical support.gsa is now applying the knowledge gained from the pilot project to itsmicrocomputer management system. the agency has also moved intorequirements contracting to cover its enduser computing needs. gsa learnedthrough this enduser computing experiment that there are four or fiveapplications that cover 85 percent of the needs. the requirements package beingprovided for the acquisition of hardware will meet the needs of thosepredominant applications.gsa has made considerable progress in dealing with the problems of enduser computing in the federal government. however, there is still much to learn.looking to the future, the agency expects the current initiatives of the managedinnovation program to evolve into a management pattern that will benefit thefederal government and ultimately the taxpayer in a costeffective way.managed innovation controlling enduser computing in thefederal government59managing microcomputers in large organizationscopyright national academy of sciences. all rights reserved.personal computers and the office of thefuturejames h. bair*a historical perspective is helpful in understanding where small computertechnology is headed. in the beginning, back in 1945, vannevar bush proposedthat computers could serve as an extension to human memory. that idea tookshape at stanford research institute in the early 1960s as an augmented humanintellect system. this system in turn evolved into augmented knowledgeworkshops and eventually into the concept of office automation, a system thatwas actually demonstrated at the national computer conference in 1967.the ﬁoffice of the future,ﬂ as it was conceived then, never really got off theground. it was generally superseded by personal computers. but some of theissues that were typical then are still important today and will continue to beissues as we move toward integrating the personal computer into themainstream of digital technology.one way to look at evolutionšin this case, the evolution of digitaltechnologyšis in terms of the ﬁshare of mindﬂ that a technology commands ata given time. in fact, it is not really the technology but some manifestation of itthat has the share of mind.prior to 1960 scientific and military computing was the dominantmanifestation. then, management information systems* james h. bair is manager of advanced systems for the information systems group,hewlettpackard co., cupertino, california.personal computers and the office of the future60managing microcomputers in large organizationscopyright national academy of sciences. all rights reserved.(mis) began to take hold. huge budgets were set aside and large corporateorganizations were built for data processing. the total audience, however, wasstill quite small and specializedšthose people directly involved withcomputing technology. in the late 1960s, however, ibm introduced wordprocessing, a concept that became known to a much larger group of people thanmis had been. organizations such as the international word processingassociation were born.even though word processing focused on the mechanization of typing,primarily a clerical activity, management and other people saw it as a new wayto use digital technology in a new place, the office. mis by contrast, had beenconfined to a very centralized location.this development follows alvin toffler's concept of the evolution ofinnovations. a ﬁwaveﬂ of innovation begins when traditional ways of thinkingabout something are unfrozen and new ideas are introduced and adopted. they,in turn, reach a peak, and there is a refreezing as what was once unfamiliarbecomes ordinary. thus, word processing is as commonplace now astypewriters were 20 years ago.the wave began again in the area of office automation, but with a slightlydifferent slant. in the late 1970s, the first office automation conference washeld. spurred by reports in the press and a lot of advertising, a new applicationof digital technologyšmicrocomputersšcame into public awareness. thoughwidely known, its use remained relatively specialized until the generalpopulation involved itself with the introduction of the personal computer.i now see the wave of office automation and personal computers droppingoff and another wave coming. this next wave has already started with expertsystems, knowledge systems, and robotics. artificial intelligence will begin totake hold. this wave will have a major impact, perhaps even more with bluecollar workers than with whitecollar workers. as this wave drops off, i believewe will be left with integrated digital systems to support both industry and theconsumer sector.besides toffler's innovation waves there is another way of looking at theevolution of microcomputers, and this is from the perspective of culturalchange. as anyone who has tried to get approval for a personal computer in theoffice can attest, the data processing/management information systems (dp/mis) organization occupies a powerful sphere of control in the corporatepersonal computers and the office of the future61managing microcomputers in large organizationscopyright national academy of sciences. all rights reserved.structure. but we can also view this dp/mis organization more broadly as asphere of culture. as such it has its own language, and its members share acommon awareness that includes experience, materials, and tools. traditionally,users have been left outside this culture.these users, who may be financial analysts, lawyers, or executives, makeup a very different sphere of overlapping and interacting cultures. today theyare challenging the authority of the dp/mis culture. by 1983 the massconsumer had acquired 2.5 million personal computers for games, education,and use in personal business. getting a personal computer was the thing to do.perhaps one of the most important aspects of all this personal computeractivity was that it involved individuals making decisions about computeracquisitions. as a result there are now about 900,000 personal computers inbusinesses. all of these are under individual control, except for the 150,000 thatwere installed through dp/mis departments.there is a builtin conflict here, and i think it may prove difficult tointegrate these two dissimilar cultures. the corporate side is trying to provideand control tools to get a job done for a justifiable cost, and to offer measurablebenefits to the company. control of pcs offers the multiple benefits ofcommunication, compatibility, service, and economies of scale.on the other side there is the psychology of the personal computer user:ﬁit's not much, but it's my own.ﬂ this is a very powerful notion and can providesome real benefits to the corporation. for a marginal cost users are getting whatthey always wantedša highly responsive environment. what is needed is tofind some way of combining the advantages that come with corporate controland the motivation that comes with individual choice.i have studied productivity and developed a methodology for productivitymeasurement, but now i find that no one wants to measure it. the feeling is,ﬁoh, improved productivity will just come. let's get some pcs in here.ﬂ onereason for this response is that we know more about improving individualefficiency than we do about improving corporate productivity. the problem istranslating knowledge of the former into the latter. to do this, individuals mustreinvest the time saved into some other activity. in other words, corporationsdon't want shorter workdays, they want individuals to invest the time saved byusing personal computerspersonal computers and the office of the future62managing microcomputers in large organizationscopyright national academy of sciences. all rights reserved.in something that will benefit the corporation. reinvestment of time is onemeasure of the productivity that comes from individual use of personalcomputers.the acquisition of personal computers raises many issues, both forindividuals and corporations. i think the most important ones are architectural.these exist at four different levels in terms of users and the location of data andresources: the individual, the department, the corporation, and the public. theindividual needs desktop personal filing, telephone management, andscheduling applications. at the department level, the applications changedramatically and may include accounting tasks and records management.at the corporate level data processing applications will involve generalledgers, personnel, and inventory. for the public new databases are availablethrough services like videotex and the source, giving access to the dow jonesindustrial average, the new york times, and other information sources.so many different kinds of data and applications raise many questions.where do you put the application? if you provide the person who generates adocument with word processing, what happens to the document at thedepartmental level? how do you create some way of moving data to differentlevels and still keep track of it? data being transmitted across the corporationposes even more complications. suppose you are developing an annual report ofa new marketing program. it is initially developed by individuals; it is thenapproved by departments; finally, it is implemented by the corporation. it has tomove and be managed. the view of floppy disks as a means of moving andmanaging information is almost as silly as using punchcards. we need ways toget programs to talk to each other. at present, getting a personal computer toaccess a corporate database provides nothing more than a glass teletype. gettinginformation to move between windows is left up to the user and even inmachines like the apple lisa is very difficult. there are additional problems ofcommunication protocols for moving information.it becomes apparent that some resolution of these architectural issues isnecessary. one of the most important steps we can take is to move away fromthe notion of the omnipotent and omnipresent ibm personal computer tosomething that gives much more capability. we need to be heading toward theconcept of the personalpersonal computers and the office of the future63managing microcomputers in large organizationscopyright national academy of sciences. all rights reserved.workstation environment. the xerox star 8010 is a prototype of such a concept.not many people would think of it as a personal computer. it exists in a networkenvironment, even though it has up to 25 megabytes of hard disk. with all itsshortcomings, it represents the direction in which we are headed: personalcomputers that don't look or behave like personal computers anymore.one of the critical things to consider in terms of productivity and the futureof personal computers is what people actually do in offices. a study of almost700 people in 7 corporations indicated that as much as 75 percent of a person'stime, especially an executive's time, is spent in communication. the onlyvariable was the individual style of communicating. personal computers mustsupport this function.computer messaging is the embodiment of the way terminals can talk toeach other and enable people to send messages back and forth very rapidly.electronic mail has been around for a long time, but we haven't heard anythingabout how telephony, or even teleconferencing, is going be integrated intoworkstation environments. one of the reasons people meet is to share visualinformation. this can be done from desktop to desktop. two people can look atthe same data on machines located anywhere in the world as long as they can beconnected through data networks. thus, there is no reason they can not interactthrough an audio link and a data link to carry on true teleconferencing over longdistances. intelligent workstationsšthe personal computers of the futurešwillbe used to support this kind of communication network. in diagrammatic form,this network might look like figure 1.all kinds of usersšmanagers, professionals, administrators, clericalworkers, and specialistsšare connected in this network, through a linkup ofintelligent workstations, minicomputers, and maxicomputers. the networkforms a gateway to other organizations and ties into other support mechanisms.for example, the instant a message is sent from this kind of intelligentworkstation, it is picked up by support software that automatically packages it,puts it in an ﬁenvelope,ﬂ interacts with the network, sends it out, andautomatically delivers it if the recipient is logged on at that moment. in otherwords, the message can be delivered instantly.personal computers and the office of the future64managing microcomputers in large organizationscopyright national academy of sciences. all rights reserved.figure 1 office automation for the organization of the future.personal computers and the office of the future65managing microcomputers in large organizationscopyright national academy of sciences. all rights reserved.this office of the future is the logical outgrowth of the continuingevolution of computer technology. today's pcs represent the crest of a wavethat has brought popular understanding and public embracing of computertechnology. that wave is beginning to recede. it will be replaced by anotherwave that has already begun to formšthe development of integrated digitalsystems that more fully support the communication and the sharing ofinformation between people.personal computers and the office of the future66managing microcomputers in large organizationscopyright national academy of sciences. all rights reserved.part iiimanaging microcomputers theissues67managing microcomputers in large organizationscopyright national academy of sciences. all rights reserved.68managing microcomputers in large organizationscopyright national academy of sciences. all rights reserved.introductionwilliam c. rosser*the headline of an advertisement in the september 1983 issue of scientificamerican asked, ﬁcan a computer make you cry?ﬂ the intent of the ad wasnot frivolous. it was placed by a company that described itself as an associationof electronic artists united in a common goalšto fulfill the potential of thepersonal computer. its goal is to take the computer beyond its use as aconventional facilitator of unimaginative tasks and use what it calls thewondrous nature of the personal computer to learn more about ourselves. thecompany sees the personal computer as more than a processor. it can also be acommunicator, a communications medium, an interactive tool. it can illustrateour own interest, as human beings, in imagination rather than in predictableprocedures, in learning by direct experience rather than by rote ormemorization. finally, the company sees the computer as a transmitter ofthoughts and feelings.perhaps the question, ﬁcan a computer make you cry?ﬂ is not yet veryrelevant to large organizations. but there are human as well as technologicalaspects to the personal computer that need attention. after all, personalcomputers are microcomputers used by individuals. it is this personal side thathas particular relevance* william c. rosser is vicepresident and director of small systems for the gartnergroup, inc., stamford, connecticut.introduction69managing microcomputers in large organizationscopyright national academy of sciences. all rights reserved.for management. because of their potential impact on a significant portion ofemployees, microcomputers may be a uniquely important issue for the largeorganization and its management.considering the rate of growth and popular enthusiasm for personalcomputers, along with the potentially serious problems they raise, the biggestrisk management can take is to wait and do nothing. if the personal computer isan agent for inevitable change, how do we prepare for it? management has aresponsibility to address the potential improvement in the performance of theorganization's primary assetšpeople.alastair omand identifies a whole range of important issues raised by thepresence of microcomputers in large organizations. only a few of these, hesays, deserve the attention of top management. the rest are better handled atother organizational levels.for contributor roger sisson, the most important issues are how endusercomputing affects the quality of decisions and how microcomputers can be usedto facilitate better and more creative decision making in organizations.thomas conrad offers a personal perspective on a set of control relatedissues that are especially critical for very large organizations such as themilitary: procurement, standardization, and centralization.introduction70managing microcomputers in large organizationscopyright national academy of sciences. all rights reserved.a perspective for the chief executiveofficeralastair i. omand*what is so different about the personal computer that it causesmanagement such discomfort? why is it so hard to figure out which issuesdeserve top management's attention?first, the personal computer marketplace is extremely dynamic. changesin the technology are coming rapidly, apparently too fast for management toknow what to buy. almost daily the trade journals announce newmicrocomputer products, but nobody knows which are best for the long term.some of the claims are misleading, and prices are always changing. it is a muchdifferent pace from that of the traditional world of large computers.second, there is no longer an obvious central point for control. centralmanagement information systems (mis) departments have traditionallycontrolled the development and operation of computer systems, as well as thedata used by them. end users have worked through programmers or mis liaisonpeople to get their work done. with personal computers, raw computing poweris now available on the desktop of the user, who can do many if not all of thethings formerly done by the mis group.further, this computing power is available to the user at relatively* alastair i. omand is executive in charge, general motors information systems andcommunications activity, warren, michigan.a perspective for the chief executive officer71managing microcomputers in large organizationscopyright national academy of sciences. all rights reserved.low cost. this is significant not only because it signals an economic shift awayfrom the large mainframes, but because many users can now approve thepurchase of computers on their own authority. as a result, the marketing tacticsof computer vendors have changed. formerly, they marketed to the mis peoplein the organization. microcomputer vendors now are skirting the misdepartment and aiming directly at end users.before we can deal with the issues surrounding personal computers, weneed to define personal computing and understand where it fits in the overallcomputing activities of an organization. personal computing is one of threefundamentally different modes of computing in a corporate environment:largescale computing services represent the traditional mode, in whichapplication systems are managed by mis professionals rather than by end users.the users provide the data needed, and the mis organization provides theapplication programs and processing services to produce the output. goodexamples of such applications are payroll processing and product warrantysystems. these kinds of applications will continue. the largescale computingenvironment typically uses large mainframe computers managed in a restrictedfacility by the mis department.some organizations in the corporation have adopted an alternativedepartmental computing mode, often implemented on locally installedminicomputers. departmental computing is characterized by shared use ofcomputer resources among members of a department. an example would be aproduction scheduling system for a manufacturing plant.then there is the personal computing mode, in which the individualperforms tasks unique to the specific job. personal computing automates thefunction of the individual, not that of the department, the division, or thecorporation.my definition of personal computing is the use of a computer by anindividual to help prepare his or her work product. this definition is importantbecause most microcomputers today are designed for personal computing, thatis, for individual users. thus, the focus is not on payroll processing or shareddatabases, but on improving the productivity of the individual.in discussing what the microcomputer management issues really are, weneed to draw a clear line between personal computing and departmentalcomputing. departmental computing systemsa perspective for the chief executive officer72managing microcomputers in large organizationscopyright national academy of sciences. all rights reserved.require a significantly higher level of control since they affect the jobs of manypeople and may be the repository for corporate records. the danger is thatpersonal computer activities may grow into departmental systems without thenecessary controls.the most frequently discussed issues concerning the use of personalcomputers in business may be grouped into five categories: data management; product proliferation; acquisition practices; level of user maturity; the shift in skill sets.data management means assuring that the information needs of thecorporation are met by the activities that collect data, produce data, or maintainrecords. this includes providing data access and security. three specificconcerns about data management have been raised relative to the use ofpersonal computers.the first concern is accessibility of data. the traditional mis solution toproviding data needed by several people was to store it in a central database, inthe computers of the central data processing shop. with personal computers,however, data can reside in many places, creating difficulties for access.knowledge of data location becomes a major problem.the second concern is consistency and compatibility of data. withoutcentral control of data there is a concern that users will employ different namesand formats for the same data. similarly, they might use the same names fordifferent data, which could lead to a comparison of apples and oranges. toreconcile such diversities could require major conversion efforts and causedelays in responding to business needs.the third concern is data security. traditionally, security was ensured bythe mis department with its expertise, elaborate controls, and physically securefacilities. with personal computers responsibility for security is put in the handsof people who may have little or no experience with it. moreover, with personalcomputers there is no longer a single logical place to concentrate securityexpertise and efforts.a second set of issues relates to the proliferation of personal computerproducts from many different vendors. this proliferation raises concerns aboutconnectivity. how do we get ibma perspective for the chief executive officer73managing microcomputers in large organizationscopyright national academy of sciences. all rights reserved.personal computers to talk to wang or dec equipment? how do we get datafrom one personal computer to another without rekeying?there are also concerns regarding technical support. how can a largecorporation reasonably expect to provide technical support for all of theproducts used? must we limit ourselves to only a few vendors? how manyvendors will we have to deal with for service or resolution of problems?finally, product proliferation raises concerns about duplication ofdevelopment efforts. will the same systems have to be developed many times torun on different equipment?the third set of issues involves acquisition practices. in this area, there isconcern that large corporations will miss opportunities for significant costsavings unless a central group coordinates the quantity purchase of personalcomputers to attain price leverage.there is also concern about vendor interface. end users are dealing directlywith vendors, a responsibility formerly managed by knowledgeable misprofessionals. users cannot possibly be well informed on all of the availableproducts and their technical complexities. purchases may not be appropriatelyjustified since many users do not really know why they need a personalcomputer. in some cases a terminal might do just as well.obsolescence is another concern in the area of acquisitions. the rapidadvances in microcomputer technology may require new computers to bepurchased within a few years. finally, there is concern about the obligationscreated by license restrictions where control of software piracy may be difficultor impossible. will the corporation inadvertently incur legal liabilities?the fourth set of issues involves the organization's level of experience inusing computers. this is illustrated by the ﬁstages of growthﬂ model developedby dick nolan. nolan claims that mis activities have evolved through a seriesof stages. the first is initiation, in which computing is first introduced. this isfollowed by a period of contagion, in which computer applications proliferatethroughout the organization in many functional areas. eventually, the need forcontrols is recognized. the third stage is thus marked by centralization of manysupport functions and control procedures in mis departments. the fourth andlater stages are characterized by increased stability and the development ofinformation management disciplines.a perspective for the chief executive officer74managing microcomputers in large organizationscopyright national academy of sciences. all rights reserved.the mis activities of most companies today are in the third stage, markedby central controls. however, the growth of personal computing and enduserexperience is generally back in stage one, initiation, or entering stage two,contagion. we are still in the process of learning what personal computing is allabout, where the benefits are, and how it is going to work. at the same timedevelopments are spurring rapid increases in use.this disparity of stages is a source of concern about the growth of personalcomputing. a major concern in stages one and two, initiation and contagion,lies in the unpredictability of enduser development projects. without adequatecontrols in place there are risks that systems will not achieve expected benefitsand will not be developed on time and within budget. there are risks thatinflexible systems may result. without proper development techniques forpersonal computers the systems developed by end users will be difficult tounderstand, difficult for others to modify, and unresponsive to changingbusiness needs.in the traditional mis environment these risks were addressed in thecontrol stage by a variety of systemsdevelopment disciplines and reviewprocedures. welldefined programming guidelines were developed. but no suchroadmaps exist to guide users today in developing applications for personalcomputers.the last set of issues relates to the shift in skills required by widespreaduse of personal computers. first, users will become developers, a functionformerly in the domain of the mis community. yet many users today do nothave the skills to manage the development process properly.second, end users must learn to manage computers and deal with the risksinvolved in their use. they will be responsible for backup, data integrity,security, and error detection. in the past the mis organization has taken care ofmost of these tasks.third, end users must learn to deal with shifting technical trends. you can'tinstall new systems every time a new gimmick hits the market, but you don'twant to miss major opportunities because you are locked into obsoleteequipment.the growth in personal computers means a change in the roles of mispersonnel as well. this began with the development of userfriendly languages,information centers, and timesharing and has accelerated with the use ofpersonal computers. many mis professionals who have spent their careers asdevelopers will move to user departments; others will become consultants ora perspective for the chief executive officer75managing microcomputers in large organizationscopyright national academy of sciences. all rights reserved.educators. both alternatives represent new career paths for mis people.all of these issues are important. but which ones should legitimatelyconcern the chief executive officer (ceo) of a company? where should topmanagement focus its efforts? and who is best suited to deal with the remainingissues?i believe that some of these issues should not, and in some cases cannot, beaddressed through top management control measures. they can be adequatelyresolved through normal business practices or by further developments intechnology. from my perspective, the ceo should focus on the first and lastissues outlined abovešdata management and the shift in skill sets. the otherthree categoriesšproduct proliferation, acquisition practices, and level of usermaturityšare best dealt with at lower levels in the organization.in terms of the first of these lowerlevel concerns, it is not at all clear thatwe should try to control product proliferation at this stage. the diversity ofproducts is a good indicator that the potential uses and the best approaches forpersonal computers are still evolving.the technology will continue to evolve as well. although some de factostandards have emerged they may be only temporary. the ibm pc hasobviously created some of these standards, but that product represents only oneof several plateaus in the evolution of personal computer technology. the nextplateau probably belongs to systems running unix, the at & t operatingsystem, and supporting more sophisticated user interfaces derived from theapple lisa.unfortunately, even though we may be able to identify some of the likelyfuture standards, today's products do not yet support them. the best approach,therefore, is to take advantage of currently available products to meet currentneeds. the connectivity issue can be addressed where necessary, but settingstandards for centralized development and support isn't worth the effort.centralized developers cannot possibly keep up. besides, vendors will providetechnical support and maintenance to maintain their reputation and survive inthe marketplace. the choice of products should be driven primarily by thepersonal needs of the individuals who are to use them. the current basis ofpersonal computer purchases should be specific needs and opportunities, notanticipated potential.a perspective for the chief executive officer76managing microcomputers in large organizationscopyright national academy of sciences. all rights reserved.mis people can help users sort out alternatives and evaluate theapplications. but they should not be given too much authority. keep the users incharge.acquisition practices for computers are important, as they are for anyexpenditure. but the primary responsibility in this area rests with anorganization's purchasing staff. they should be looking for equivalent productsat lower prices and they should recognize and pursue opportunities for volumediscounts. in fact, this is much easier to do for microcomputers than it ever wasfor data processing equipment. there are many interchangeable products,particularly peripherals like printers and display monitors. purchasing peoplealso have the responsibility for seeing that license agreements are reasonable.they should be allowed to do their jobs.similarly, the investment decision should be controlled by traditionalfinancial procedures. budgets and appropriation procedures are no lessapplicable to personal computers than to any other equipment bought to run abusiness.for the most part major investments are not involved. thus, the risks areminimal. most personal computers purchased now should have paid forthemselves by the time they are clearly obsolete. savings opportunitiesshouldn't be missed because it might be possible to make a better decision later.there will continue to be uncertainties.the disparity in the level of user experience between mis departments andusers of personal computers is undeniable. while mis organizations are busycontrolling their resources and tuning their operations, the wholemicrocomputer marketplace is in the contagion stage. this needn't cause greatconcern. it should be exploited, not controlled.when data processing activities were in the contagion stage milliondollarinvestments in equipment and development projects were involved. the riskswere high that projects would not meet expectations on time, within budget.this is not the case with personal computers. with smallscope systems theinvestment is small. the business impact is focused on the job of a singleindividual. the development investment may be simply one individual'slearning curve, not the expenditure of years of effort by fulltime technicalexperts.finally, auditors should be allowed to do their job. their responsibility isto assure that good business practices are followed. ifa perspective for the chief executive officer77managing microcomputers in large organizationscopyright national academy of sciences. all rights reserved.there is concern that such practices might be ignored with the use of personalcomputers, auditors should be called in.when many of the personal computer issues are handled at appropriate,lower organizational levels, the ceo can focus attention on the more criticalissues.data management is one of the most complex of these critical concerns. tounderstand how the growth of personal computers affects data management it ishelpful to identify the functions: business processes, particularly those that involve multiple departments,should be streamlined through business systems analysis. interdepartmental interfaces must be clearly defined and supported withcommunications facilities to assure data availability. sources of business data must be defined to assure that the proper controlsare exercised and necessary records are kept. data security must be assured through the identification of risks and thedefinition of control requirements and precautions.these functions focus primarily on mainstream business systems ratherthan on personal computing. they involve multiple departments rather than theindividual. but there is little doubt that personal computers, like their users, willbecome participants in these larger systems. personal computers will be used toprocess corporate data; they will thus influence the development of largersystems that go beyond the scope of personal computing.the ceo can take three steps to ensure that data management is properlyaddressed in the area of personal computing as well as in other areas of theorganization.the first step is to place organizational responsibility for datamanagement. data management is a coordination and control function. itrequires the participation of all departments, but it also requires a focal point ofresponsibility. general motors is encouraging both divisions and corporatestaffs to create a department called business information management. itsactivities include the mis functions, but with a new mission. rather thancontrolling the use of computer technology, its function is to help otherdepartments exploit the technology while providing the coordination andcontrol necessary for data management.the next step for the ceo is to initiate the definition of appropriatepolicies and practices, mainly in the area of data securitya perspective for the chief executive officer78managing microcomputers in large organizationscopyright national academy of sciences. all rights reserved.and integrity. the data management organization should lead this effort, but itrequires the participation of other parts of the organization as well. thesemeasures must address the new risks associated with personal computers.personal computers are located in offices instead of data centers, and they storelarge amounts of data on diskettes that are easy to transport. these and relatedfactors create new security risks that must be faced.finally, the ceo should initiate policies and practices that control thescope of personal computing. such policies should be reinforced by datamanagement efforts. it is fairly easy for a personal computing effort to growinto a departmental system. a red flag must go up when an individual startsmaintaining departmental records on a personal computer. although variousindividuals need copies of corporate data, the organization must not depend onthem and their personal computers as the sole source of such data.the second critical area for ceo concern involves personnel. the growthof personal computing is just one aspect of the increasing involvement of endusers in the application of computers. as a result, the responsibilities and manyof the skills required for computer application development must migrate fromthe mis organization to the end user.for this migration to be effective the ceo must be prepared to make asignificant investment in education. this will take the form of classes, seminars,and the cost of a learning curve for users, all of which translates into dollars.end users must be trained to manage the new responsibilities that comewith personal computing. first they must gain an adequate understanding of thegeneral concepts of computer and telecommunications technology. then theymust learn about microcomputers in particular. they must be skilled in propertechniques for systems design and security.mis professionals will also require education. some will be relocated inuser departments and will need a greater understanding of the user's business.many others will refocus their careers from systems developers to consultantsfor end users and must develop consulting skills.the education effort will also need to extend beyond users of computers.people in purchasing departments must have an increased understanding oftechnology in general and will need to develop selection criteria for variousvendor products. this willa perspective for the chief executive officer79managing microcomputers in large organizationscopyright national academy of sciences. all rights reserved.require a much greater knowledge of personal computers than they have today.auditors must have a greater awareness of the latest microcomputer technologyas well. they will need to be aware of the risks involved in using thesemachines and be able to assess alternative measures.all of these educational needs will cost money, and management shouldexpect to spend it to get the most benefit from the new technologies. our workforce must get smart on the risks and the opportunities if the investment inpersonal computers is to yield the desired return.in short, here is my advice to organizations trying to cope with thepersonal computer invasion. don't try to stop it. don't try to standardize thetechnology. don't limit the user's innovation and initiative. instead, focus yourattention on the critical success factors. manage your data resources to assurethat the right people continue to have access to the right data. educate users sothey can take advantage of the technology, properly manage it, and deal withthe attendant risks.dissertations have been written about the disparity of investment inproductivity between production workers and knowledge workers. personalcomputing is one of the most promising opportunities to increase knowledgeworker productivity. it is management's responsibility to make sure that peoplehave every chance to exploit it.a perspective for the chief executive officer80managing microcomputers in large organizationscopyright national academy of sciences. all rights reserved.managing microcomputers and endusercomputing some critical issuesroger l. sisson*much of u.s. industry needs help. proper use of data and computer powermight provide part of the needed assistance. an example of the kind of help i'mreferring to comes from the steel industry, one of those most in need. withaccess to a computer and the right data, a cost analyst at one company weworked with provided significant information about his company's productprofits.this analyst had spent part of his career in the sales department. soon afterhe was assigned to cost analysis duties, he was given access to a terminal, apowerful fourthgeneration language system, and datašparticularly invoice andcost data.until this point no one had been able to explain variations in companyprofits. as a result of his work in sales, however, the analyst knew that costsand profits were reported only on a product line basis, and that one lineaccounted for 60 percent of the company's business. with his computeraidedtools he was able, in a few days, to analyze the profitability of individualproducts in the major line. this had many benefits: it explained the monthtomonth variation in profits; it pinpointed areas where cost reductions would payoff; and it provided guidance to sales. this analysis* roger l. sisson is president of sisson, michaelis associates, inc., swarthmore,pennsylvania, and lecturer at the wharton school of business.managing microcomputers and enduser computing some criticalissues81managing microcomputers in large organizationscopyright national academy of sciences. all rights reserved.will not turn the entire steel industry from loss to profit, but it is helping onecompany come closer to profitability.such analysis is becoming routine, an example of the pay off use ofcomputers by nonspecialists. but this use of computers by nondata processingprofessionals in large organizations raises some important issues.two issues are critical: the quality of decisions that result when decisionanalysis is facilitated by microcomputers or other computerbased aids, and thesecurity of data and programs from theft or manipulation for unauthorizedpurposes. there are also less critical, but still important, administrative issues.before looking at these issues we need to understand what we mean whenwe talk about this exploding phenomenon of enduser computing. computing inthis context means accessing and performing analyses on data to supportdecision making. the data usually come from existing files but may be enteredby the analyst. computing also covers the case in which the user is installing asmall, local, complete system that includes entering and updating data andproducing standard reports as well as analytic outputs.the end user is a nondata processing professional who is performing thesort of computing just described. in other words, the main mission of the enduser is some function other than data processing. he or she may be performingmarket research, financial analysis, contract administration, or budgetpreparation.in considering end users and their use of computers in relationship to theorganization, we may for the moment ignore the difference between terminalsand microcomputers as the end user's tool. whatever the tool, the principal issueis: does the availability of computer power facilitate better decisions?over the last 30 years the data processing profession has developedimportant standards, guidelines, and procedures designed to facilitate the use ofthe computer, to make program maintenance easier and less expensive, and toinsure the integrity and security of data. do the hundreds of thousands of newend users have to come up with their own sets of standards and proceduresthrough long and sometimes bitter experience? or can data processingprofessionals help end users? these questions suggest a third important enduser computing issue that, along with quality of decisions and security, deservesattention. this is the issuemanaging microcomputers and enduser computing some criticalissues82managing microcomputers in large organizationscopyright national academy of sciences. all rights reserved.of administration of information processes and resources. let us look at each ofthese issues in some detail.as background to a discussion of the effect of enduser computing ondecision quality, i would like to introduce the notion of distributed creativity.this is my name for the much discussed concept that organizational successdepends on innovation and a striving for excellence at all levels of theorganization.in the book in search of excellence (harper and row, new york: 1982)peters and waterman remark that in the excellent firms, ﬁquality and servicewere invariable hallmarks. to get them, of course, everyone's cooperation isrequired, not just mighty labors from the top 200ﬂ (p. 24). elsewhere in thebook, they offer an example: ﬁ3m has been described as ‚so intent oninnovation that its essential atmosphere seems not like that of a largecorporation but rather a loose network of laboratories and cubbyholes populatedby feverish inventors and dauntless entrepreneurs who let their imaginations flyin all directions'–. they encourage private risk taking, and support good triesﬂ(p. 15).in more philosophical terms, one goal of a large organization is to strike abalance between authority and anarchy. somewhere between these twoextremes lies a freedom to create modified by controls to insure that theorganization is cohesive and has direction. the best means of accomplishingthis is the major management theory question of this decade.one way of answering the question goes like this: wisdom comes fromexperience and usually correlates with level in the organization. we assume thechief executive officer is wise. therefore, important decisions should be madeat the top. we should centralize the decision making and the analytic support.actual experience, however, has shown that the ﬁtopﬂ is too far from theaction. there one loses touch with customers, vendors, operation, shops, andlabs. to restate peters and waterman, the best process is to transmit the wisdomthrough the culture and to place most of the decision making where the actionis. if the culture transmits wise values and constraints, decisions made at lowerlevels will be based on the best information in a context that guides thedecisions with experience and wisdom of top management. enduser computingcan help both to provide information and to transmit some aspects of thecultural wisdom. how does it do this?managing microcomputers and enduser computing some criticalissues83managing microcomputers in large organizationscopyright national academy of sciences. all rights reserved.creative leaps of imagination are not inductive. they do not derivelogically from the data. but creativity does not occur in an environment devoidof data either. the process of studying data seems to facilitate innovativethinking. since the end user often cannot predict which data are relevant, accessis needed to a variety of files with data about the environment, the history andcurrent status of operations, policies and rules, and resource availability. theend user may be performing any of a number of management analysis tasks:looking for exceptions, for opportunities, for measures of current status andprogress, for alternatives, or for estimates of the consequences of adopting analternative. an innovative insight may come at any point during any of thesestudies.there are of course costs and risks in providing computer resources tomany end users. is the cost worth the benefits from improved decisions? i thinkthe benefits significantly outweigh the risks and costs. this judgment is basedon two beliefs: first, that distributed creativity is valuable and is facilitated byenduser computing, and second, that our culture is particularly able to becreative if given the right tools.especially in the united states, we learn from childhood to do thingsourselves. at great national expense we prefer automobiles to mass transit sothat we can go where we want when we want. we have even learned how todrive trucks to move our belongings ourselves rather than use a truckingservice. the ultimate demonstration of distributed creativity in our culture is theprevalence of entrepreneurs and the proliferation of small enterprises.with this cultural heritage it is not surprising that managers and analystswant to handle their own decision support and the computing that goes with it.the rise of personal computing was predictable given our culture of individualinitiative, even if most data processing professionals, blinded by their ownexpertise, did not forecast it.but a desire to do things on one's own is not enough. you can't rent a driveityourself truck if the only trucks available are 18wheelers requiring specialtraining and hard work to drive. and you won't rent a truck if the closest rentaloffice is several hundred miles away.until a couple of years ago computers were like 18wheelersšmanaging microcomputers and enduser computing some criticalissues84managing microcomputers in large organizationscopyright national academy of sciences. all rights reserved.they were inaccessible. communication facilities were not extensive; terminalswere slow and awkward. operating systems were unfriendly, making it nearlyimpossible, even for professionals, to get into the system. and if you did it costan arm and a leg.six things have occurred recently to make enduser computing a reality, tomake it easy for the user to rent (connect to) the systems, to drive, to read themaps (use software), and to pay for the gas (the computer resources): inexpensive online computer resources became available through efficientmainframes and reasonably easytouse video terminals or throughmicrocomputers. operating systems are now userfriendly. fourthand fifthgeneration language systems are now available, allowingend users to communicate with computer systems without knowingprogramming languages. a large number of people are either knowledgeable about computers or atleast not afraid of them. decisionmakers have a desire, and usually a real need, to processinformation rather than fly by the seat of their pants. end users recognize that the data they need is available somewhere in theorganization's computer files.these breakthroughs mean we can now ﬁdo it ourselves.ﬂ endusercomputing is here. with 8.3 million terminals and 2.4 million micros in theunited states, the 300,000 data processing professionals are outnumbered byend users 33 to 1.now that we have the tools, will their use result in better decisions?although i have seen no formal studies on changes in the quality of decisionswhen extensive computer facilities are available to the decisionmaker, there issome evidence. a survey now in progress of enduser support groups(information centers) suggests that enduser computing does make contributionsto the bottom line. alloway and quillard (mis quarterly 7(2):1983, 27œ41) alsofound, as a result of a survey, that managers certainly want decisionsupportcomputer facilities.according to peters and waterman, abernathy, clark, and kantrow(industrial renaissance. basic books, new york: 1983, p. 21), and othermanagement researchers, encouraging innovation at all levels is vital to anorganization's continued success. ifmanaging microcomputers and enduser computing some criticalissues85managing microcomputers in large organizationscopyright national academy of sciences. all rights reserved.this is so, the availability of dataanalyzing capabilities and good endusercomputing support should in many cases improve the decisionmaking process.there is also a flip side to using computers in decision making. acomputer, whether a micro or a mainframe, is a very powerful tool. as with anypowerful tool it is possible to use it to make powerful mistakes, inadvertently orfraudulently. one of the potentially dangerous mistakes is a bad decision,because it may have farreaching consequences. therefore, it is important toknow how decisions are being made by end users, to know that they are beingmade in reasonable ways and with the organization's objectives in mind. thisconcern was illustrated by an official of the bank of america who said to me,ﬁwe wonder how all those loan officers are deciding on loans with their ibmpcs and visicalcs.ﬂthe freedom of the individual to innovate, given the right tools, is uniquein the west and most prevalent in the united states. thus, i believe that the costof not providing modern information tools to end users is much greater than thecost of a few bad decisions made more easily by the use of these powerful tools.our successful organizations have always depended on their employees toimprove them, to innovate, and to advance their welfare. one characteristic ofthese organizations is that they provide nearly everyone with the tools needed tobe creative. a period of increasing international competition is not the time tohold back.once we agree that good enduser computing resources do lead to betterdecisions, we can go on to consider the difference between enduser supportfrom terminalbased mainframes and from microcomputers. (mainframe heremeans any large computer or minicomputer controlled by a data processingstaff.) the mainframe/micro choice must be viewed from the benefits side.which will do more to improve the quality of decision making? i think thechoice today depends on the specific system. there are three determining factors: access to relevant data may be difficult on a standalone micro, but if goodcommunications are available the micro is no different from a terminal. a mainframe system with very good response time and an array ofeffective, userfriendly software might provide better support of decisionmaking than a micro. but many mainframemanaging microcomputers and enduser computing some criticalissues86managing microcomputers in large organizationscopyright national academy of sciences. all rights reserved.timesharing systems do not meet those objectives. they are slow, and goodsoftware for end users does not exist or is not made available. in thesecases the end user will find the micro preferable. mainframes are expensive because of the multiuser overhead and thecommunications costs.if we recognize that any micro can act as a terminal and do much more,and that micros are not much more expensive than a terminal, microcomputerswill probably be the preferred solution in terms of benefits.three additional developments will clinch the micro's advantage overterminals: the capacity of micros is increasing; the friendliness of microsoftware is improving; and the facilities to transfer data between micro andmainframe, preferably in a way transparent to the user, are becoming available.soon the micro, combined with communications to the sources of data inthe mainframe, will be the dominant solution. (this combination ofmicroprocessor and communication, accompanied by good software, is termedby some a ﬁworkstation.ﬂ)in addition to these policy and technical considerations, we cannot forgetthe individuality factor in our culture and how it will affect the use of micros.taking away or refusing to allow someone to have a micro will be like takingaway or refusing someone the use of an automobile. that is unthinkable in acountry where we have a hard time preventing even drunks from usingautomobiles.enduser computing, largely supported by micros, is obviously valuable. ifthere is some reason to slow down or suppress the use of micros it must bebecause of other issues, such as security or administrative decisions.security is a critical issue: making a system secure may also limit itsflexibility and usefulness. such limitations prevent or slow down thedistribution of capabilities that aid creativity. therefore, the goal is to make thesecurity procedures easy to use yet relatively impenetrable. a single key is nothard to use. a lock requiring two people and two keys gets to be a nuisance.when i was designing systems i had little concern for safety. the auditorswere there to worry about that, and their specific suggestions would beimplemented. security, however, was not allowed to get in the way of ease ofoperation. today i thinkmanaging microcomputers and enduser computing some criticalissues87managing microcomputers in large organizationscopyright national academy of sciences. all rights reserved.greater concern is warranted. when thousands of people are accessing sensitivecorporate or agency data, the probability is that someone will removecompetitively sensitive data on a floppy disk. a floppy disk is convenient; it iseasy to transfer data to it; it can hold hundreds of pages of data. a person canwalk out of a building with it unobserved. the contents need not be identifiableexternally. how does a security guard know what is on one?there is also the wellpublicized problem of illegal access by telephone.since solutions to this kind of unauthorized access are known, the problem fallsinto the administrative area. it should be possible to detect and trapunauthorized dialin access.access to a computer system is possible not only directly through thenormal logon process, but also by tapping communications links and byremoval of media such as floppy disks. to eliminate loss through these meanssome form of encryption is required to protect the data even when the media arecompromised, or to make decoding so expensive that potential thieves aredissuaded.these measures are expensive, as are any security measures. to justifythem the potential loss from a security breach must be greater than the cost ofthe security measures. the organizations we have worked with limit theirsecurity measures to password systems. some depend only on the logonpassword; others have protection at the file level, so the end user must knowtwo or three passwords. a few have additional password protection at theoperating systems level. i know of no commercial firm that uses encryption inrelation to the kind of enduser computing we are discussing. ultimately, mostorganizations depend on a trustworthy work force.microcomputers are removed from the direct control of the mainframe,and therefore are not guided by data processing standards. does the use ofmicros create additional security problems? to answer this we must distinguishbetween external and internal breaches of security. in terms of external attemptsto invade the system, it seems to me that micros do not add problems.physically locking the micro and any removable media may be necessary insome cases. however, data downloaded to a micro is not at greater risk thandata printed out, or data put on a floppy disk, or microfiche, or any otherremovable media. in relation to internal security leaks, the micro, as a powerfulinformation processor, may provide opportunities for access and removalmanaging microcomputers and enduser computing some criticalissues88managing microcomputers in large organizationscopyright national academy of sciences. all rights reserved.not previously available. internal security is ultimately still a matter of auditingand personnel policies, not technology.the final set of issues are those that, i believe, can be resolved in mostorganizations by normal administrative procedures and proper computeruserinterfacing. these issues are the effects of bad data, data integrity, the problemof transferring enduserdeveloped programs, and the question of who pays foradded computer capacity, the wasting of resources, and the proliferation oflanguages.poor data. are decisions supported by computerbased analysis deficientbecause of poor data? this may be an important question where data fromoutside sources are used. the quality of outside data must be checked just as thequality of received parts or materials is checked. (in a few places it may beadvisable to check the quality of the internal data as well.) data quality reallyinvolves two questions. is the decisionmaker better off with no data or with datathat may have some errors? if flawed data are used will the power of thecomputerbased analytic tools in some way amplify the errors?there are several responses to this problem. obviously, care should betaken to prevent bad data whether micros, terminals, or adding machines arebeing used. in each case the end user is more likely to know the data andtherefore be better able to catch errors than corporate staff or others. in fact, thepower of the computer may help the end user find errors by performing variousconsistency and trend analyses. thus, end user access to data may have anadditional benefit of making the data more accurate. further, the kind ofdecisionsupport analysis we are discussing may be insensitive to occasionaldata errors. of course, management should always be aware thatrecommendations and analytic results may be based on poor data. endusercomputing should not aggravate this situation if administrative procedures arefollowed to keep the data as clean as possible and to remind the staff to checkthe data and the reasonableness of the results.data integrity. can the end user affect the operational data in corporatefiles? if so, can the integrity of the data be assured? can the changes made byan end user be audited?the only way to insure the integrity of data is to maintain all the controlsthat data processing has painfully learned: good editing, effective audit logs andprocedures, and good recovery processes.managing microcomputers and enduser computing some criticalissues89managing microcomputers in large organizationscopyright national academy of sciences. all rights reserved.there is no reason to relax these for the benefit of end users. the only waymaster files should be updated is by the normal, wellcontrolled transactionprocessing system.transfer of data and programs. what about files that a user maintainsindependent of the mainframe systems? should the data and programsdeveloped by an end user be considered private, scratch pad material or shouldsome mechanism be installed to allow sharing of this material? if the end user isworking on a mainframe the data and programs are at least accessible, even ifthey are not properly structured and documented. but data and programsdeveloped on a micro may be as inaccessible as those on paper. someorganizations are in fact taking the attitude that the data and programsdeveloped by an end user on a micro are the same as material on paper. if theuser leaves, the new occupant must develop his or her own routine and data.peers who need to do similar processing must develop their own tools, perhapswith the informal help of the first end user. the results of the end user'sanalyses are submitted in reports (with appropriate appendixes) in the usualway. the data and routines not included in the appendixes are no longer ofinterest; they are throwaway materials.if the organization decides that it does want to capture programs or datathat are on a micro and judged to be valuable, administrative procedures mustbe instituted just as they are for data processing professionals. these will not beeasy to enforce, however, because there are many decentralized end users whohave little interest in data processing's problems. the only other alternative is toprohibit the development of ﬁsystemsﬂ on micros and allow such developmentonly in a mainframe context.resource use and chargeback. if we provide many end users with theopportunity to use computers, and especially if we make it easy for them witheffective languages and other software, the demand for computer capacity willsoar. the issue is whether additional capacity is a good investment. if end usersare helping to make decisions that significantly increase the effectiveness of theorganization, the service it provides, or its profits, the cost of the computerresources may be well justified.but only management can justify this resource use. to do this, it shouldknow what the end user is doing and how that work contributes to theorganization. the data processing people, who normally have to justify theincrease in capacity, do not know themanaging microcomputers and enduser computing some criticalissues90managing microcomputers in large organizationscopyright national academy of sciences. all rights reserved.benefits side of the justification. therefore, chargeback of all computer costs ismandatory, and every company we have worked with or surveyed that supportsenduser computing has a chargeback of real budget dollars. the budgettransfer from chargeback is prima facie evidence for the data processingdepartment that the computer use has value; the customer paid for it. if those intop management do not agree, they have to discuss it with the end users.wasted resources. if people are given relatively free access to computerswill they waste the resources? waste can occur in several ways: trying to solve problems by random trial and error (with or without models); computing rather than thinking, researching, or getting out and talking withcustomers, employees, and other individuals; using poorly written, inefficient routines; trying to write programs instead of using packaged software or higherlevellanguages.unlike scrap in a metals production process, the scrap in an informationprocess is invisible. it is not easy to identify waste, so it may become prevalentand expensive.people waste information tools all the time. who keeps track of pencil andpaper wastage? is a few minutes of computer time spent on an inefficientroutine worse than throwing away some sheets of paper from a bad draft? toanswer, one has to look at cost and benefit. for example, a little waste in aprocess that costs a couple hundred dollars but may be saving 10 times that inreduced costs or improved sales is not worth concern.choice of languages. effective enduser computing that leads to betterdecisions requires languages that end users can handle. a major issue is theselection of the proper languages and supporting software systems. this is atechnical topic, but it should not be overlooked as a management issue. it isimportant also to standardize languages throughout the organization to prevent atower of babel. every group we have contacted has settled on a few basiclanguage tools: for reporting, for statistical analysis, for financial planning, forelectronic mail, and sometimes for word processing. (in all cases there was oneset of standard software for the mainframe and one set for micros.)managing microcomputers and enduser computing some criticalissues91managing microcomputers in large organizationscopyright national academy of sciences. all rights reserved.if my thesis about distributed creativity is correct, enduser computing,with micros where justified, is appropriate. through a good enduser supportgroup, sometimes called an information center, local managers can receiveinformation support of all types, not only with hardware but with software andtraining. however, some controls should be attached to these support efforts: use passwords properly. change them frequently. tie them to people andterminals, not groups. control the use of floppy disks and prevent unauthorized removal of data. isolate the user from the operating system with a friendly menu andcommanddriven front end. don't let end users update main files except through batch processes withgood edits and controls or through online systems with formal edit, audit,and recovery procedures. decide between a throwaway or a share policy for userdeveloped,decisionaiding programs. if sharing is chosen, build the infrastructure tosupport it. provide good user training. users tend to follow the instructions given, andgood training can help reduce waste, correct inefficient routines, andpromote data security and integrity. use a real money chargeback system and let the end users' manager worryabout waste. allocate disk space carefullyšit is relatively expensive.limiting disk space constrains the extent of data and thus indirectlycontrols how large and complex the user's systems can be.distributed creativity is an important national asset. it can help therevitalization of industry and the recovery of the u.s. position in internationalcompetition. enduser computing, supported by micros or terminals and by agood support staff, promotes distributed creativity. microcomputers inparticular can encourage innovation and good decision making at all levels ofthe organization. no longer just an executive gadget or a local data processor,micros have become, i believe, an important link in rebuilding our nationalstrength.managing microcomputers and enduser computing some criticalissues92managing microcomputers in large organizationscopyright national academy of sciences. all rights reserved.regaining control through centralizedactionthomas d. conrad*the issues involved in managing microcomputers can be summed up in afew words: to standardize or not, to facilitate or not, to control or not, to wait ornot. we face aspects of these issues daily in the air force.standardization presents a special problem to the military because ofrotation policies. as useroperators move from one assignment to another, theyare exposed to different systems, equipment, and database management systems(dbmss). as a result, we have continual training and logistics problems. howdo we resupply a microcomputer in egypt, grenada, korea, the philippines, orokinawa? how do we handle backward compatibility? how do we handle theportability of data as we move around the world? how do we handle theportability of the hardware itself?procurement raises another set of issues. should it be centralized ordecentralized? do we purchase computers with capital funds or with operationalfunds? this is not an insignificant problem in the military services. do we buyor lease? this issue is being debated in congress. do we use a lowestcostacquisitions policy or do we consider technical merit along with cost? for yearsthe military services have been prodded into awarding contracts* thomas d. conrad is former deputy assistant secretary, information systemsmanagement, office of the assistant secretary of the air force.regaining control through centralized action93managing microcomputers in large organizationscopyright national academy of sciences. all rights reserved.to the lowest bidder. are hardware and software acquisitions single or separateprocurements?how should the whole proposal and contract award process work? shoulda request for a proposal (rfp) be on a requirement basis, in effect designing thecomputer, or should specifications be strictly functional? should thedevelopment of specifications and the evaluation of proposals be centralized ordecentralized? how should we handle multiple awards? do we choose inhouseor contract maintenance?all of these issues grew out of some initial observations i made when ifirst came to washington. i realized there was a proliferation of all kinds ofmicrocomputers in the air force. in fact, many were not really businesscomputers at all; they were more like home computers, but they were beingused in critical areas of our national defense. sometimes they were purchasedwith capital funds, sometimes with operational funds, sometimes with slushfunds, and sometimes with private funds. personal computers, owned byindividuals, were being used operationally. when those individuals rotated out,they took their computers with them. such practices did not seem very wisefrom a business viewpoint, particularly when that business was national defense.my first step was to place a moratorium on the purchase ofmicrocomputers. for almost a year no approval was given to purchase anymicrocomputers. besides catching everyone's attention, this step stimulatedcooperation in the expeditious development of specifications and a procurementstrategy.my next action was to convene an advisory council. the group of about 33appointees met in november 1982. it was not the usual committee, because ihad decisionmaking power. thus, council members who had the mostinfluential or convincing arguments would actually determine the direction wewould take.we began by developing a requirements contract for microcomputers. nominimum or maximum quantities were specified. nor did the contract requirethat any money be available for purchases. instead, it simply said that if anymicros at all were purchased over the period of the oneyear contract (with twooneyear options for renewal on the part of the air force) they would bepurchased under the terms of the contract, from whomever won the award. (infact, when we placed the rfp on the street and even when we awarded thecontract we had no assurance that weregaining control through centralized action94managing microcomputers in large organizationscopyright national academy of sciences. all rights reserved.would buy any computers, because we had no money whatsoever behind thecontract.)further, the contract set only minimum specificationsšfor example, thesize of the disk and the size of the screenšand therefore was not a functionalrfp. it was coordinated with and approved through the general servicesadministration (gsa) and became a joint acquisition of the air force, navy,and marine corps.we received requests for the rfp from 330 different companies ororganizations. about two weeks after the release of the rfp we held thepreproposal vendor conference, which was attended by 155 people. wereceived 32 proposals, a record for the air force computer acquisition center.of those 32 proposals, we eliminated 17 that were deemed unsuitable.i had stated publicly that it was not the air force's intent to award thecontract solely on the basis of the lowest bid. instead, the award was to be basedon three integrated considerations: cost, technical excellence, and postsalecontractor support. the rfp stated that cost would carry a weight of 40 to 60percent. the exact weight was not made public and, in fact, was not evendetermined until after we had initially evaluated the proposals. technicalexcellence would have a weight of 30 to 50 percent, and contractor supportcounted for 10 to 20 percent.the final award would be determined by an objective, detailed scoring ofpoints weighted for technical excellence, based on an established range set up ina scoring model. excellence points were given for those items offered that wereabove and beyond the minimum specifications. for example, we specified a 5megabyte hard disk as an optional item. those vendors who offered a largercapacity hard disk earned excellence points. we also gave excellence points fora separate keyboard, for the capacity of floppy disks, for different databasesystems, and for certain spreadsheets. it was possible for a vendor to get asmany as 1,000 extra points for technical excellence.to select a tentative winning proposal, we integrated a value of low cost,technical excellence points, and contractor support. the integration exercisewas interesting because we had never before brought together cost and technicalexcellence points. one possible way to do this was to take the vendor's bid anddivide by the total technical excellence points given. we could then put a valueregaining control through centralized action95managing microcomputers in large organizationscopyright national academy of sciences. all rights reserved.on these points, and whoever had the lowest cost per point would receive thework.the fallacy of this method was that it gave no consideration to the vendorwho had no technical excellence points. what was the cost of his points if hehad none? such a bidder obviously had an acceptable system because he hadsurvived the competition, but with zero points for technical excellence he couldnot win the award. to solve this problem we established a base value for thebasic offering. every system that met the minimum requirement was worth $18million. we subtracted $18 million from every bid, divided the remainder bythe technical excellence points, arrived at a value per point, and integrated thatvalue with the cost.best and final offers brought prices down substantially, and a tentativewinner was selected. following the selection, a live test demonstration resultedin a rescoring and reduction of points that caused us to reevaluate and go to thenext apparent winner. that vendor came out of the live test with more technicalexcellence points than he had originally.in october 1983 we awarded the contract to zenith data systems for theirz100 base systems at a cost substantially below what was available in themarket or through gsa (probably close to a 55 percent discount on the retailprice). in my estimation this ﬁexerciseﬂ saved the air force and thereforetaxpayers some $36 million. as its originator, i am quite pleased with thiscentralized action and its result.regaining control through centralized action96managing microcomputers in large organizationscopyright national academy of sciences. all rights reserved.part ivmanaging microcomputerscase studies97managing microcomputers in large organizationscopyright national academy of sciences. all rights reserved.98managing microcomputers in large organizationscopyright national academy of sciences. all rights reserved.introductionrhoda w. canter*if grappling with microcomputer technology trends, their implications, andthe management issues involved is difficult on a theoretical level, developingand implementing strategies uniquely adapted to individual organizations is aneven more awesome task. for every theoretical issue there are multiplepractical questions to answer: how much control is needed over microcomputers, where and when is itneeded, and through what means can it be applied most effectively? should microcomputers be interconnected or connected withminicomputers and mainframes? what is the most systematic approach todefining appropriate uses of the different technologies? what are theimplications for the organizational structure and skills of the informationprocessing community? how can the most effective microcomputer applications be identified andwhat are the implications for the functional management community? what pace of implementation will best suit the needs, desires, andcapabilities of potential users of microcomputer? should management responsibilities for information processing bereassigned and, if so, how?* rhoda w. canter is a principal of arthur young and company, washington, d.c.introduction99managing microcomputers in large organizationscopyright national academy of sciences. all rights reserved. how can an organization establish education and training goals forintroducing new technologies? what programs will ensure that these goalsare achieved effectively? how should concerns about data administration be addressed? what are theorganization's information resources? where are these resources? who isresponsible for them? how can security of information resources beachieved? how can microcomputer components be acquired, maintained, andoperated most effectively?the list only scratches the surface.difficult as these challenges are, managers in large organizations mustcome to grips with them. the five case studies that follow present a variety oflarge organizations and their efforts to manage emerging microcomputertechnologies. the organizations represent several major segments of industryšmanufacturing, insurance and financial institutions, state government, and themilitary services. they also represent a broad spectrum of management styles,ranging from overt central control to control through persuasion to a laissezfaire approach. in each case senior management has addressed universal issuesand made particular decisions with respect to successful management in a givenenvironment.the balance and movement between management and practice in the realmof technology is intricate. through these five case studies of successes, failures,and lessons learned we catch glimpses of a common process. the processencompasses analysis of the organization's culture; setting strategic goals inconcert with that culture; planning, organizing, and controlling to meet strategicgoals; and marketing the organization's approach to the entire work force. thisprocess, distilled from the maze of particulars, can contribute significantly tomanagement theory and, in turn, to management practice in large organizationseverywhere.introduction100managing microcomputers in large organizationscopyright national academy of sciences. all rights reserved.productivity through automationjohn j. alexander, jr.*automation is widely viewed as a key way to increase productivity. atreynolds metals company the use of automation to improve productivity is thefocus of the company's planning and development efforts for the 1980s. itsspecific objective is to use automation to double the productivity of its salariedwork force by 1990. a brief profile of the company will give some idea of theextent of this effort.reynolds is a fortune 100 producer of primary and recycled aluminum.operations are conducted by 12 divisions that explore, mine, ship, and refinebauxite; reduce alumina to primary aluminum; recycle scrap aluminum; andfabricate aluminum into a wide range of products. an international divisionmanufactures and sells overseas. about onethird of our 30,000 employees aresalaried and work in 50 plants, 25 sales offices, and the corporate headquartersin richmond, virginia.management, of course, is the key word in such an effort. and just like themanagement of any other corporate function, managing information processingis itself a process. specific tasks required to carry out the process can beidentified. in addition to the conventional maxim ﬁplan, organize, and control,ﬂi would add ﬁstrategize, rationalize, and market.ﬂ* john j. alexander, jr. is senior vicepresident of management information systems,mci telecommunications corporation, washington, d.c.; and former corporate directorof administration, reynolds metals company.productivity through automation101managing microcomputers in large organizationscopyright national academy of sciences. all rights reserved.reynolds' planning process for automation was developed during 1978 and1979. the planning cycle begins in the spring when an update and anautomation planning manual is issued, telling all division and plant automationmanagers what they need to do to develop an automation plan for their unit.corporate management provides some structure and asks managers to detailtheir plans and programs.the deadline for responses is october and coincides with the completionof the company's annual business planning cycle. reynolds encourages but doesnot require interaction between business and automation planning. the analysisof the results of one cycle consumes the octobertomay period and providesinput and structure for the next cycle. thus, planning is a continuous process.the first five planning cycles are summarized below. 1979œ80 planning cycle. in this first planning cycle hardware wasstandardized and data was identified as an issue. a study wascommissioned that provided an understanding of logical and physical viewsof data and the tools for analysis. 1980œ81 planning cycle. the corporate staff proposed a network, which hasbeen installed, and the salaried employee was identified as a target formajor productivity improvement. 1981œ82 planning cycle. the system/38 was added to the approvedhardware list, experimentation with personal computers was encouraged,and ibm's business systems planning (bsp) technology was introduced. 1982œ83 planning cycle. the ibm displaywriter and personal computerwere added to the approved hardware line, and a study of informationarchitecture was begun. 1983œ84 planning cycle. the need for additional central computer capacityto keep balance with the growth of distributed processing was determined.from the early planning cycles we concluded that although the annualplanning process is essential, looking at information requirements for the shortterm is not sufficient. we had to take a much longer look, and that required astrategic goal. in 1980 we articulated such a longerterm planning goal.this goal was to provide all salaried employees with the automation toolsthey need to manage the information necessary to perform their jobs and doubletheir productivity by 1990. in definingproductivity through automation102managing microcomputers in large organizationscopyright national academy of sciences. all rights reserved.this goal we also described the rationale behind it, projected the rate at which itcould be achieved, identified the major roadblocks to accomplishing it,developed a marketing approach to delivering the necessary automation tools,and limned the kind of changes in the organizational culture that would have tooccur to achieve the goal.although this is a very ambitious goal, the company believes it must beaccomplished if reynolds is to be productive and profitable over the nextdecade. seeking a rationale, we looked at what our salaried employees do andrecognized that information plays a very large role in their jobs. they receiveinformation in the form of memos or reports and store some of it, mostly in fivedrawer file cabinets. they use salaried secretaries to retrieve it or go after itthemselves, look at it, manipulate it, reformat it, analyze it, and send it on tosomebody else. automation can provide some support for every one of theseinformation processing operations. it is not going to reduce the human factor tozero, but it can increase productivity.based on our projections about rates of change in cost of automationcomponents and people, we projected that 20 percent per year reduction in thecost of automation components is sustainable for this decade. in the past fiveyears, people have increased in cost an average of 10 percent per year atreynolds.in some areas we have carefully tracked what a work group was doingbefore automation was applied and what was required to accomplish the sametasks after automation was applied. in these cases we can demonstrateimprovements in productivity ranging from 100 to over 500 percent.the major roadblock to automation lies in taking the conventionalapproach to developing automation solutions to business problems. thisapproach calls for customdesigned systems to solve the informationrequirements of an individual or a group of individuals. at reynolds what wecall a system typically takes about three years to develop and directly affectsabout 15 people. with a limited set of resources, 150 people at reynolds buildand maintain systems. half maintain the old systems and half build new systems.using the 1980 work force as a base we assumed that the 1,000 individualsthen using automation were representative of the total. extrapolating thesystems work done from 1977 through 1979 to the entire work force yielded abacklog of approximatelyproductivity through automation103managing microcomputers in large organizationscopyright national academy of sciences. all rights reserved.three decades needed to automate all 11,000 salaried employees. we concludedthat we needed a new approach.the alternative we developed is a marketing approach to providinginformation services within a major corporation. marketing provides theanswers to three critical questions: who is our customer? what is our product?how is the product distributed?we began by recognizing that our ﬁcustomerﬂ is the individual employeewho uses a computer or terminal. we classified these individuals by need andidentified common informationhandling requirements. this allowed us toorganize the market of employees into a handful of segments and to developdelivery vehicles for automation services to reach each segment.when analyzing such a market, it is helpful to consider whether it consistsof all employees; various segments of employees like managers, professionals,clerks and secretaries, and other individuals defined as office workers; or allsalaried employees. reynolds settled on the last category as the mostappropriate for its automation program.if the salaried employee is the customer, what is the product? i believethere are only two broad categories of automation services: custom systems andstandard vehicles.information professionals have been building custom data processingsystems for 30 years. each one has been a specific solution to a functional orindividual need, with all the attendant problems of limited life, costlymaintenance, and general dissatisfaction with the disparities between what thecustomer wanted and what he or she got.an alternative to customdesigned systems is a standard vehicle, which is ageneral solution to an information problem presented in a sufficiently friendlyfashion for the individual to assume personal responsibility for it. thespreadsheet approach to presenting and analyzing financial data as embodied invisicalc is a classic example of a standard vehicle. of course, standard vehicleshave their own problems of documentation, standardization, and control.to get a custom solution a user specifies requirements to an informationsystem analyst, who proceeds to build a custom product. this can be calledsystems development. by planning for the conception, birth, growth, maturity,and death of the product over a life cycle, systems development explicitlyrecognizes the obsolescence of the custom solution. by contrast, with a standardproductivity through automation104managing microcomputers in large organizationscopyright national academy of sciences. all rights reserved.vehicle the user is educated into a solution without having to master themysteries of electronic data processing. the educating is done by a professionalwho becomes the coach, informs the user of corporate standards, and helpsselect the best technology to satisfy the user's information needs.at reynolds we started with the assumption that eventually every salariedemployee will have a terminal. every terminal will be tied to a network, andthat network can access any computer. although this is not something we canafford to do today, at least we have defined the problem, we know how to solveit, and it will become cheap enough to implement in the future. it may costtwice as much today to deliver a given service automatically as it does todeliver it manually, but if our cost projections hold true, crossover occurs in twoyears, after which automation is less expensive.users will bear the cost of the terminal and a pro rata cost for using thenetwork. we assume they will justify those devices based on productivityimprovements of the staff. other possible justifications include better service ornew kinds of service that could not be provided in any other fashion. but thefocus is on productivity.to determine what kinds of automation would be needed we began byidentifying six delivery vehicles for automated information processing: wordprocessing, electronic mail, data inquiry, transaction processing, technicalcomputation, and business analysis. we then took the personnel system'sfunction codes for all salaried employees and mapped them against the sixdelivery vehicles to identify the size of the market for each vehicle.secretaries can obviously use word processing, but the market segment forthis vehicle is much larger. lawyers are wordsmiths; programmers and systemsanalysts are in the business of language translation; and public relations andpurchasing departments already have word processing capabilities. wecurrently have over 400 terminals providing word processing support.essentially, what they do is format and edit words, but these capabilities caneasily be expanded. add technical computation and business analysis, and notjust words but an entire report can be formated. wrap an electronic envelopearound a memo or a report, plug it into electronic mail, and it can be deliveredto the recipient.only managers are considered an appropriate market for electronic mail.among the things managers do is send and receive aproductivity through automation105managing microcomputers in large organizationscopyright national academy of sciences. all rights reserved.lot of memos. electronic mail appears to be a fine vehicle for sending,receiving, and following up on memos. currently reynolds has over two dozenterminals that are sending and receiving memos by electronic mail. thisincludes a few intended primarily for international communications. automaticfollowup, scheduling, and calendaring routines are available.data inquiry services are ﬁmarketedﬂ to employees who need access topublic information or large data banks. in 1980 we had only a handful ofterminals justified for this purpose. to determine who needed what kinds ofdata we brought in a data dictionary, which allows users to define theinformation they need to access. to meet these needs we have bought twolanguageprocessing systems: inquire and intellect.inquire is a textprocessing language with very powerful boolean logiccapabilities. our research librarians find it an effective tool for abstracting,storing, and retrieving information about company reports and projects. thesame language is being developed for use by the legal department to storeabstracts of legal contracts and maintain an index of where those contracts arephysically stored.intellect is a parsing program that analyzes an english sentence typedon a computer terminal and generates a call to a specific database. the firstapplication of intellect has been in support of our human resourcedevelopment (hrd) search activity.in addition to database and querylanguage facilities, the system/38, withits own capabilities, is providing outstanding service and accounts for thelargest segment of our recent growth in the area of data inquiry. further, we arejust becoming comfortable with the database tool on the personal computer.i expect that as we become proficient in defining data for these kinds ofsystems anyone who has a terminal will be able to use data inquiry to get accessto needed information. data inquiry will largely supplant printed reports as thevehicle for distributing information from automated systems.transaction processing is our largest delivery vehicle. in three years thisapplication has grown 50 percent to over 900 terminals that support clerks whoenter and process transactions. all terminals now exercise systems built toprocess transactions in a conventional manner. we have defined a requirementto handle the other 5,000 forms that the company is currently processingmanually.productivity through automation106managing microcomputers in large organizationscopyright national academy of sciences. all rights reserved.we are now seeking a universal form processor that will allow a user to call upon a terminal any form used in the company, fill in the blanks, and send it byelectronic mail to the individual or computer that has to review it, approve it, orprocess it. if this capability is acquired, we believe it would be used by almosteveryone.technical computation is used by our technicians, engineers, andscientists. timesharing has been the delivery vehicle, and the applications aremainly specialized routines the company has purchased or the users havewritten for themselves.business analysis was the final delivery vehicle we identified. in manycompanies business analysts, planners, accountants, and others have long usedautomation as a vehicle for improving their productivity. at the beginning of1980, however, reynolds had no terminals to support such individuals. thissituation has changed with the availability of color graphics and analytic toolpackages. a spreadsheet computer program such as visicalc, which runs on apersonal computer, is very popular; we wish it were available now on largeribm machines. when it is we believe the number of terminals used by businessanalysts will grow even more rapidly.how is the investment in various automation delivery vehicles justified? ihave already explained that in order to determine the market for each of the sixdelivery vehicles we mapped the number of people in each functional job codeagainst their potential to use a primary delivery vehicle. the resulting matchupwas also used for justification purposes.obviously, we expect most users to employ more than one deliveryvehicle. for example, secretaries who primarily use word processing alsoprocess transactions and inquire into databases. but the justification for thisinvestment requires an improvement in productivity that is usually assumed tobe measurable only by the primary focus. once a terminal is installed forprimary access, of course, other delivery vehicles can be made available.when we matched job functions with delivery vehicles we found thatterminals for about 57 percent of the salaried employees would be justified bytransaction processing. the second largest justified use (15 percent) was fortechnical computation. this is because reynolds is a manufacturing andtechnology company and has a significant cadre of engineers and scientists.three percent of the staff was identified as managers, and should have devicesproductivity through automation107managing microcomputers in large organizationscopyright national academy of sciences. all rights reserved.justified for electronic mail. the balance was about evenly split between wordprocessing (10 percent), database (9 percent), and business analysis (6 percent).quite a different picture emerged when we looked at the actual use ofdelivery vehicles compared to the justified (or projected) use. (see figure 2.)the productivity improvement in one primary area had provided thejustification, but microcomputers are in fact blurring the lines between vehicles,causing changes in job function and rebalancing the workload. what seems tobe happening is that automation is changing the jobs of salaried employees.the individual delivery vehicles have had widely varying growth ratesbetween 1980 and 1983. transaction processing, which began with the largestbase, has been growing steadily at about 15 percent per year. technicalcomputation doubled in 1980 and grew another 60 percent during 1981. wordprocessing more than doubled in each of its first three years and is approaching50 percent penetration of the market. in all, these three vehicles should satisfyover 80 percent of reynold's automation needs when fully implemented.data inquiry, business analysis, and electronic mail began the decade atground zero. in the last two years, both data inquiry and business analysis havebroken out of the pilot/test modes and have achieved market acceptance.electronic mail has not been as successful, and research is now under way tofind out why.with its market reasonably defined and delivery vehicles in place,reynolds addressed the issue of the rates of growth we could manage whilemeeting our endofthedecade objective. as a percentage of our salaried workforce, individuals using automation grew from a base of 6 percent at the start of1979 to 21 percent in january 1983. a compound growth rate of 30 percent peryear from 1983 on would achieve the 1990 objective of 100 percent automation.actual growth has been over 33 percent per year, compounded. as productivityimprovements are achieved the salaried work force needed to handle a givenvolume of business is being reduced. this makes 100 percent automationdelivery by 1990 even more likely.along with the marketing and growth aspects of its longterm plan,reynolds has identified a number of organizational issues that demandattention. these can be broadly divided into technology, management, andcultural issues.computers are available in a dazzling variety of sizes, capabilities,productivity through automation108managing microcomputers in large organizationscopyright national academy of sciences. all rights reserved.figure 2 justified versus actual use of automation delivery vehicles byreynolds metals salaried employees.productivity through automation109managing microcomputers in large organizationscopyright national academy of sciences. all rights reserved.and costs. to deal with this diverse technology reynolds simplified itsperception to three sizes: large, medium, and small. two large amdahls managethe corporate information systems and interconnect all other machines. mediumsized ibm system/38s manage plant and department information needs. theibm displaywriter and personal computers provide individual automationcapability. all of these machines are connected through a network. althoughsome form of each delivery vehicle capability is currently available on all userdevices, some are better than others. the longterm objective is for all userdevices to provide high levels of all six automation functions.to successfully change the mode of distributed processing in a company(that is, to change from manual to automated processing throughout), the threekey variables of hardware, software, and management must be organized insome way. the question isšhow? if we simplify the possible operatingenvironment for each of these variables to either totally centralized or totallydecentralized, a number of combinations are possible. these can be viewed asthe eight corners of a cube (figure 3).for example, centralized electronic data processing (edp) functions bestwhen all three variables (hardware, software, and management) are centralized.at the opposite extreme is the personal computer, with all three variables totallydecentralized.at the back plane of the cube in figure 3, where management iscentralized, three combinations are possible. if only software is relaxed(decentralized), development is distributed. alternatively, relaxing hardwareleads to distributed processing. decentralizing both hardware and software withcentralized management requires strong efforts toward standardization.the face plane represents decentralized management. if this occurs whenhardware and software are both centralized, frustration is the usual result. whenhardware remains centralized and software is relaxed, timesharing and remotejob entry are the result. centralizing hardware and relaxing software areequivalent to purchasing packages.most organizations consist of some mix of all or most of the eightextremes of automated processing. the important point is for organizations toknow where they stand and where they are likely to be in the future.the culture of an organization poses a final set of concerns for automationefforts. although it is unplotted in figure 3, a cultureproductivity through automation110managing microcomputers in large organizationscopyright national academy of sciences. all rights reserved.figure 3 operating space of automated information processing.is present in every company. the culture of an organization must changeas it moves through the hardwaresoftwaremanagement cube. in general, ibelieve that automation professionals need to reorient themselves from customsystems to standard solutions. this frees key personnel to become coaches andfocuses a smaller cadre on technology planning, evaluation, and selection.other employees need to accept automation education, devices, and networksthat give access to information. this allows individuals to accept responsibilityfor their own information requirements.a comparison of the cultural implications of automation with those ofhuman resources clarifies the management issue. along with money andmachines, people and information are resources every organization mustmanage. the personnel function is recognized by most organizations as a keyone, and reports are made to the chief executive officer or one step below. wellrun personnelproductivity through automation111managing microcomputers in large organizationscopyright national academy of sciences. all rights reserved.departments provide expertise in recruiting, compensation, development,evaluation, and training. however, management of the human resources is notthe responsibility of the personnel department but of managers at variousdepartment and section levels.information management should be considered in the same light. for toolong it has been delegated to the edp or mis department. yet computers are nomore complex to manage than people. every manager and information workerneeds to understand what information is needed, where it is, who controls it,how good it is, and how to get it. plans to improve information availability, cutcosts, and measure performance should be made annually, just like humanresource development plans.how to make people recognize that information management is part ofevery manager's job is another question. leadership needs to come from theinformation systems ranks. identifying the chairman, president, or business unitvicepresident who will be the role model in the organization is essential.articles in periodicals such as the harvard business review, fortune, andbusiness week, which popularize the role of information management, will alsobe helpful.reynolds has given priority to four technical issues. first, we need a datadictionary that spans the full range of automation devices from personalcomputers to departmental machines to corporate mainframes. the second issueis access. once we know what information is located where, access must beavailable to all who need it. the access issue leads directly to privacy andsecurity issues. information must be available only to those who should have it,and in a fashion that assures individual privacy.the fourth technical issue involves optimal selection of devices andnetworks. presently we analyze stated needs and select the best devices to meetthese needs. we then build networks to connect the devices. a better approachwould be to feed all sources and uses of data to a model that provides the bestcombination and use of computers, terminals, and networks.on the management side of the equation, we have identified a number ofdifficult open issues. one of the major issues we will face, both as individualsand as organizations, is how to implement productivity improvements. if agroup of 10 clerks increases its productivity by 50 percent, a fairlystraightforwardproductivity through automation112managing microcomputers in large organizationscopyright national academy of sciences. all rights reserved.management response would be to allow turnover or reduction in the workforce. however, if a marketing manager or an engineering manager is given adevice that doubles productivity, it is much more difficult to decide how to takeadvantage of it. do you give the marketing manager two markets to manage?do you give the engineering manager both engineering and marketing tomanage?it will be a major challenge for the management of reynolds and othercompanies to find ways to restructure jobs to take advantage of theimprovements in productivity made possible by automation. inevitably, somepeople will be displaced. managers in our company and the country at large aregoing to have to address the issue of retraining these people.a second management issue we are now facing in the course of increasingsecurity for corporate data is the question of who owns what data. reynolds hasbought a sophisticated computer program that requires identifying who ownseach piece of data, who can access it, who can change it, and who can delete it.this raises some interesting questions. for example, does a piece of taxinformation in a division system belong to the tax department or to the division?a third issue involves measuring productivity. economic feasibility hasbeen blithely assumed, but it needs to be tested at every point. on the one hand,reynolds has established a goal to double the productivity of its salaried workforce. on the other, we don't even measure the productivity of our salaried workforce today. we can measure the ratio of the total number of salaried employeesto pounds of aluminum or dollars of sales. on an individual basis, however, wedon't know what productivity is and we don't know how to measure it. i believethis question of how to measure individual productivity is an issue we willwrestle with at least for the rest of the decade.a final management issue involves multiple functions and responsibilities.in the past auditors have taken a lot of comfort in the careful separation ofresponsibility. one person requisitions a purchase, a second approves, a thirdreceives, and a fourth pays the invoice. automation makes it possible for asingle individual to perform more than one function. reynold's management, aswell as its auditors, must consider what kinds of controls to build intoautomated information processing systems. these controlsproductivity through automation113managing microcomputers in large organizationscopyright national academy of sciences. all rights reserved.must provide adequate protection for corporate assets, assure proper processingof financial transactions, and at the same time achieve the necessaryimprovements in productivity.i think the broad conclusions to be drawn from reynolds' efforts are clearand important. first, information processing is just too important a componentof every salaried employee's job to subcontract it to information specialists oranyone else. it needs to be a part of each employee's job description.second, the decreasing cost of automation technology and the increasingcost of people will not only permit but will mandate that every salariedemployee's productivity be improved through the use of automated technologyduring the course of this decade.third, redesigning jobs to take advantage of the productivity potentialprovided by automation is going to present major challenges to management inthis decade.microcomputers are machines. like all machines they have advantagesand disadvantages. to deal with them successfully organizations need anautomation strategy, an understanding of how microcomputers fit, anautomation plan, and a mechanism to monitor performance.reynolds is only one company that has begun this process. i believe thatall firms that successfully automate their information functions will develop acompetitive edge over those that do not.productivity through automation114managing microcomputers in large organizationscopyright national academy of sciences. all rights reserved.managing microcomputers in state andlocal governmentfred dugger*state and local governments, like private sector companies, face a new andformidable challengešhow to harness the technology of powerful, inexpensivemicrocomputers to improve the organization's productivity. the computingpower and low cost of these devices are well known. what is not as well knownis how to manage their prudent use. we must initially identify areas whereproductivity might be improved by using micros. then we must address issuesof product selection, procurement procedures, software acquisition anddevelopment, communications network integration, education, andmaintenance. we must determine how microcomputer technology can enhancerather than confuse our information generating systems. we must distributecomputing power throughout our organizations while maintaining the necessaryinformation flow to top management. and we must accomplish this in anenvironment so dynamic that tomorrow's product announcements may makeyesterday's requests for bids obsolete.this dynamic environment extends to users and potential users as well. itseems that almost everyone has a strong, sometimes* fred dugger is director of the department of data processing, state of nevada(carson city).managing microcomputers in state and local government115managing microcomputers in large organizationscopyright national academy of sciences. all rights reserved.highly emotional opinion about micros and personal computers. people whoown them often consider them a friend; their children almost always do. thesepeople believe that micros have a definite place in their own officeenvironment. they are probably right.other people with little or no direct contact with micros are still intrigued.the media have brought home the fact that ours is a computerized society; ourchildren have frequent access to computers at school; dick cavett, charliechaplin, and captain kirk have shown us how we can generate sophisticatedgraphics for our businesses with a single keystroke. and with the plummetingprice of hardware how can we afford not to install one? there is surely a salesrep at the other end of the telephone who can solve all our problems.then there are the professionals in traditional data processingorganizations who have sweated blood for so many years developing, installing,and maintaining enormous, highly sophisticated systems. these systemsperform largescale accounting operations, transfer millions of dollars dailyaround the country, and provide countless other services in government and theprivate sector. will these organizations lose the power they have acquired, thetalent they have gathered? if they no longer control computer acquisitions, willthe chaos of the early days return in the form of unmaintainable systems;programs written by authors who have long vanished, unstructured, unplannedsystems; communications incompatibilities; a complete lack of documentation?finally, there is the largest group of all, the uneasy whitecollar workerswho believe they will have to embrace this new and strange technology, masterit instantly, and improve productivity measurably while continuing to cope witheverything they are now barely able to accomplish. these people know thatthey will soon have to alter fundamentally and permanently the way in whichthey accomplish their daytoday tasks. they know that change means stress.and they are worried.so what do we do? do we even have a choice? can we opt to refuse thenew technology, to continue with methods that now appear to be at the peak ofefficiency? the answer is no. we have no choice on whether micros will be inour organizations. we do have a choice as to how they will be controlled,acquired, supported and, in general, put to productive use.but how do we measure productive use? the answer is economics, pureand simple. unfortunately, in all too many cases themanaging microcomputers in state and local government116managing microcomputers in large organizationscopyright national academy of sciences. all rights reserved.mystique of computers has successfully evaded costbenefit analyses and returnon investment calculations. there is no reason for this. computers are tools, anddata processing centers are machine shops. the same economics apply. dataprocessing ﬁexpertsﬂ may attempt the standard technique of claimingindependence from accountability due to technical complexity, but this is noexcuse. as with every other capital and operating cost, line managers must beresponsible for the costs associated with implementation and use of micros.it is only fair, however, to make sure that managers understand the truecosts of microcomputers. usually, the major cost of a microcomputer lies in thecost of the people who program and operate it. it is this consumption ofpersonnel resource that is almost always underestimated, particularly if itinvolves any original programming. functions that have been performed intraditional data processing applications are often completely ignored by peopleacquiring new microcomputers. as a result, those who attempt to writeprograms without systems planning, analysis, design, and data definition aredoomed to commit the disastrous errors that plagued the early data processingindustry. one can only hope these disasters will be on a smaller scale.the economics, then, are simple: equipment is cheap, people areexpensive. minimize people requirements. eliminate programming by buyingofftheshelf software. minimize training time by providing a helpful supportstaff. standardize vendors, both hardware and software, for commonapplications. provide centralized maintenance wherever possible. provide aninternal qualified consultant staff to answer the myriad questions on commonmicros and associated software that will arise from firsttime users.in short, make micros easy to use. if they are easy to use they require lessstaff time, and that means less money.what have state and local governments done to facilitate the use ofmicrocomputers? the answer isšmany things. states' responses to themicrocomputer onslaught have been as varied as their existing data processingorganizational structures. those with highly centralized data processingauthorities and procurement agencies quickly developed new policies andprocedures to cover micros. authorization procedures tended to stress proof ofbeneficial use, as well as sources of funding and costbenefit analyses. thoseorganizations that already had approval authoritymanaging microcomputers in state and local government117managing microcomputers in large organizationscopyright national academy of sciences. all rights reserved.for data processing applications and equipment also exercised approvalauthority for microcomputers. standardized forms for requesting micros havebeen implemented in many organizations. in a similar fashion, centralizedpurchasing authorities developed qualified vendor lists, negotiated volumediscount agreements, and published procedures to be used to requestmicrocomputer equipment.some government agencies have modified their organizational structure toaccommodate new demands. the state of kentucky has created a new unitwithin its systems services branch called the microcomputer support unit(msu). this unit has been directed to establish policies concerning theevaluation and use of microcomputers by kentucky state government, and toprovide technical, financial, contractual, and management support for thesepolicies. the unit has developed forms for minimicrocomputer needsassessments and costbenefit analyses. these new forms, along with preexistingones, provide a way to determine the suitability of a proposed microcomputerinstallation. the msu also coordinates all activity with microcomputer vendorson behalf of the state; initiates all hardware and software purchases; andregisters all software, whether developed inhouse or purchased. although theunit does not itself provide software development services, the msu willcoordinate with the information systems department to provide that service ifcustom software is needed.the kentucky microcomputer support unit has the additionalresponsibility of developing and implementing standards and guidelines forhardware and software acquisition. the msu has established a recommendedlist of software packages, specific to the applications desired, and has setstandard communications protocols for micros that communicate withmainframe computers. the unit is developing guidelines to make installationand operation of microcomputers easier, including suggestions on backup andrecovery procedures. the msu is also developing an internal ﬁcomputer store.ﬂthis store will have an inventory of approved microcomputers, peripheraldevices, and software in common use. this equipment may be borrowed bystate agencies for shortterm use and is available for demonstrations and tests.the store is expected to facilitate comparisons of various micros and softwarepackages, and prospective purchasers will be able to solicit advice fromobjective, rather than salesoriented, staff. finally,managing microcomputers in state and local government118managing microcomputers in large organizationscopyright national academy of sciences. all rights reserved.the msu will serve as a clearinghouse for all application programs, provideadvice on career training for microcomputer users, run a hotline service forquestions, and coordinate all equipment maintenance and service.clearly, kentucky is taking an approach that will facilitate selection, use,and maintenance of microcomputers while maintaining procurement andauthorization controls and procedures. the state's investment in staffing themsu will be recovered many times over in the time savings of the personnelsupported. the creation of this organizational structure reflects management'scommitment to the effective use of microcomputers.the state of california has long been noted for its centralized control overacquisition of data processing equipment and procurement procedures.california has formulated a central policy for microcomputer acquisition aswell, but has left procurement authority for such equipment with the variousdata processing entities throughout the state. several of these departments havedeveloped their own guidelines for selecting microcomputers. the health andwelfare department has implemented the computer store concept for itsinternal use. members of this department may make an appointment to visit thestore to discuss their computing requirements with data processing staff. ondisplay and available for trial use are microcomputers that are compatible withdepartment mainframes. quantity discount arrangements have been made withvarious vendors through the california central purchasing authority. thecomputer store has been so popular that it has had to restrict access to only itsown department staff. other departments are watching this approach and areconsidering stores of their own. california is continuing to develop centralizedpolicies for other aspects of microcomputer use, and is currently investigatingways of providing equipment maintenance. the state does not now have acentralized education capability for microcomputer users and is also quiteconcerned about mainframe compatibility.other governments have implemented other methods of control. the stateof illinois reviews each request for microcomputer acquisition against themaster data processing plan for the requesting agency. if the microcomputerperforms a service that supports the plan, and if it is economically beneficial, itreceives approval. if the acquisition does not support the plan, the purchase iseither disapproved or the plan is changed and subsequentmanaging microcomputers in state and local government119managing microcomputers in large organizationscopyright national academy of sciences. all rights reserved.approval is sought. dade county, florida, has placed its microcomputer supportfunction within the data processing organizations in its information center. theinformation center has already been providing support for userfriendlysoftware on large mainframes, attempting to bring handson computer powerdirectly to analytical and managerial personnel without requiring traditionalcomputer programming. the information center concept has achievedwidespread success. dade county believes that the user orientation of theinformation center staff will help establish correct use of microcomputers. thecenter also provides an excellent opportunity to compare directly thecapabilities of applications software running on large mainframes and onpersonal computers.the state of alaska is pursuing a policy that attempts to avoid duplicationof mainframe functions on microcomputers. the state recognizes that there aresubstantial differences in information systems requirements, and that much timeand effort can be wasted trying to shoehorn a large application into a smallcomputer in the name of efficiency. like kentucky, alaska is standardizing offtheshelf software packages for specific applications, thereby improving themobility of experienced staff and reducing training requirements. the state hasalso developed its own training courses and materials. in one rather innovativeprogram alaska is using prelaw students to assist in developing applicationsprograms to support legislative functions. the students receive university creditfor their work.several other states have taken less comprehensive approaches. these areunderstandable if we keep in mind that, in general, state governments moreclosely resemble a collection of independent companies, each with a unique setof goals, rather than a single large company with an overall profit objective. inother words, providing drivers' licenses has little to do with licensing real estatebrokers, except that both must be accomplished at the lowest cost to thetaxpayer and with the highest quality of service. as a result of the disparatefunctions of state agencies, decisions about acquisition and use ofmicrocomputers are frequently left to the individual agency. since mostagencies do not have the resources to provide specialized internal consulting formicrocomputers, those that want to take advantage of the new technology mustfend for themselves. this has sometimes led to a proliferation of microcomputervendors, software products,managing microcomputers in state and local government120managing microcomputers in large organizationscopyright national academy of sciences. all rights reserved.and limited communications capability. it has also consumed a great deal ofpersonnel resource, as people who are unfamiliar with microcomputers visitcomputer stores and attend seminars and expositions to increase their technicalknowledge. perhaps the worst consequence is the frustration anddisillusionment of managers who must struggle with the consequences ofbuying the wrong computer for a task or must use novice programmers to try tobuild customized software.the state of washington's employment security department (esd) offersa good example of a departmentlevel approach to microcomputers. esd hasdeveloped a microcomputerbased system to support its job placement trainingact (jpta) activities. this application involves a local area networking systemthat allows crosscommunication between sites and communication to the esdmainframe. insofar as local area networking is much talked about but littleunderstood these days, several other states are watching the developments inwashington with great interest. some of these states, including nevada, haveentered into agreements with washington to allow transfer of the contractordeveloped software to their own jpta programs. many interesting issues aresurfacing during these transfers, and it appears that the operation of thewashington system and the transfers to other states will provide excellentlearning opportunities.in nevada current planning strategy distinguishes between backbonesystems, which are information systems considered vital to the effectiveoperation of an organization, and decision support systems, which providedigested information to management and program personnel. examples ofbackbone systems include payroll, corporation licensing, gaming tax andlicense fee collection, and motor vehicle registration. decision support systemsinclude caseload projections, revenue projections, and tax impact analysis.professional data processing personnel will continue to develop, implement,and maintain the state's backbone systems, thereby utilizing the most expensiveresource, people, to insure the quality of the most valuable asset, timely andaccurate data. the new technologies of the improved, userfriendly softwareproducts and microcomputers, functioning as professional workstations, willprovide direct computing power to managers and analysts.state governments as a group, functioning through the nationalassociation of state information systems (nasis), havemanaging microcomputers in state and local government121managing microcomputers in large organizationscopyright national academy of sciences. all rights reserved.recognized the magnitude of the management problem they face. they haveaccepted as a primary responsibility the development of policies, procedures,and techniques to make the best use of the new microcomputer technology.they have also recognized that all states have similar, if not identical, problems.in an effort to share ideas and resources and to cooperate in solving commonproblems, nasis has established an information clearinghouse. thisclearinghouse will function as a common repository for states' policies,procedures, plans, productivity techniques, and anything else that might helpachieve excellence in information systems services. although few documentsare currently indexed and stored, the research and education committee ofnasis has placed a major emphasis on acquiring and indexing qualitydocuments from the states. nasis wishes to extend the availability of thesedocuments beyond its membership to other governmental entities and to theprivate sector. mechanisms are now being developed to allow distribution of anindex of available documents, as well as the documents themselves.it is too soon to evaluate the approaches of various state and localgovernments to managing microcomputers within their organizations. based onsome of their early experiences, however, it is possible to make somerecommendations about the use of microcomputers by government and privatesector organizations: recognize that microcomputers are becoming an increasingly importanttool for your professional staff. micros are here to stay. provide management support for a cohesive microcomputer policy in yourorganization. establish an organizational unit that understands microcomputers and canprovide quality advice for internal management. such a unit can cutthrough marketing claims and provide realistic assessments of capabilities. use your established expertise in systems engineering. an organization'sdata processing professionals can greatly facilitate an integrated systemsapproach using the new lowcost hardware. monitor early microcomputer installations closely. capitalize on thesuccesses and learn from the failures. provide for and insist on education for your managementmanaging microcomputers in state and local government122managing microcomputers in large organizationscopyright national academy of sciences. all rights reserved.staff. they don't have to understand how computers work, but they mustunderstand their capabilities and limitations. do not expect microcomputers to replace the large information systemscurrently in place on mainframes. properly managed, micros can providevaluable local support functions while enhancing the quality and quantityof the corporate database. improperly managed, micros can diffuse thedatabase and confuse the accuracy of data.although the challenge of managing the microcomputer invasion appearslarge, the opportunities that micros provide are also enormous. the power andflexibility they bring will permit quantum leaps in the provision of qualityinformation. these thinking robots are amplifying the analytical power ourorganizations possess at a cost undreamed of a decade ago. as we learn how toharness this new technology we will be able to use its powerful capabilities toachieve our goals better, faster, and more economically.managing microcomputers in state and local government123managing microcomputers in large organizationscopyright national academy of sciences. all rights reserved.the user eramartin b. zimmerman*in the late 1970s richard nolan described a sixstage model for theevolution of computers and information systems (harvard business review,marchapril 1978). this model provides a framework for the u.s. army'spresent initiatives in automation. a brief description of the model will helpclarify the motivation behind these initiatives.stage 1: initiation. the first use of computers in most organizations was tosolve welldefined problems and to unburden those involved in repetitivefunctions such as payroll and accounting. for the u.s. army this occurredduring the period 1956œ1962.stage 2: expansion. this stage was characterized by an explosion in theuse of computer technology. hardware was king. little was known of thesoftware problem. computers were placed behind glass walls and theuninitiated were paraded past the mysterious devices in semireligiousceremonies. companies and organizations depended on the computer industryfor total system solutions in what were described as ﬁturnkeyﬂ systemscontracts. this second stage can best be described by its laissezfaire* martin b. zimmerman is deputy assistant, deputy chief of staff for operations andplans, u.s. army, where he serves as technical advisor on automation.the user era124managing microcomputers in large organizationscopyright national academy of sciences. all rights reserved.management and decentralized decision making. the result was ﬁgood newsand bad news.ﬂ the bad news was that decentralized management led to thedevelopment of duplicate functional systems. the army, for example, had 49automated payroll systems in 1968. the good news was that without such anexpansion stage the industry would not have been able to develop the corporateprofessionals required for subsequent phases of growth. the uncontrolled natureof the expansion naturally led to the next stage.stage 3: control. in this stage steering committees were established,budgetary control procedures developed, and central design agencies organized.technological talent was concentrated within the organization, and companiesbelieved they could solve all their own problems. development using inhouseassets was in; contractual support was out. this stage resulted in users'frustration caused by the inability of central development departments to solveall user problems.stage 4: integration. this stage is characterized by increased interactivity,data management, and management initiatives to integrate informationhorizontally. the concept of developing a corporate database and sharinginformation among corporate staff is inherent in stage 4. a return todecentralization also marks this stage, since more systems are userdeveloped.nolan, in fact, believes that stages 1œ3 can be categorized as the ﬁera of thecomputer,ﬂ while stages 4œ6 can be defined as the ﬁera of the user.ﬂstage 5: data ownership. this stage is characterized by the need to assigndata ownership and a focus on solving the natural friction between the dataprocessing professional and the increasingly literate and active user.stage 6: maturity. this final stage occurs when organizations areexperienced enough to design a corporate planning model derived fromcorporate databases.today, most organizations are moving out of stage 3, central control, andinto stage 4, integrationšfrom the end of the computer era to the beginning ofthe user era. three phenomena are stimulating this move.the user era125managing microcomputers in large organizationscopyright national academy of sciences. all rights reserved.the first phenomenon can be called limits to the design activity. thesoftware factories of the late 1960s and early 1970s were established to solvemost if not all software development problems. they have not, however,succeeded as envisioned. as systems developed and proliferated more userswere trained. over time these users requested more and more changes to thebasic systems. such changes, part of the software ﬁmaintenanceﬂ process, haveused up 70 percent of the inhouse programming resources of mostorganizations. unfortunately, this does not mean that 30 percent of thecorporate programming staff will always be available for new functions. ahypothetical example shows why: if an organization began development of asystem with 1,000 programmer/analysts, at the end of the effort 700 wouldremain for maintenance and 300 would be available to develop a secondsystem. this second design activity, when finished would require 210 personnel(70 percent) for maintenance, leaving only 90 programmer/analysts for the thirdsystem. it is obvious from this example that the number of systems any singleorganization can both develop and maintain is finite. and user frustration is theultimate result.the second phenomenon is increased user literacy. for more than 20 yearscomputers represented a mysterious technology, understood only by dataprocessing professionals. today, however, computers are everywhere, andcomputer literacy, aided by software products that permit unique systemdevelopment without the need to understand cobol, fortran, or any otherhigh order language, has greatly increased.the third phenomenon that has spurred movement out of the control stageis the technology explosion. the cost performance curve in logical devices hasled to a sizable expansion in their use. if we continue to acquire computers inthe future at the same rate as we have in the past, we will compound deliveriesat 25 percent annually. theoretically, computer growth requires an equivalentexpansion in trained programmers. today there are 300,000 programmers. byone estimate, approximately 3 million would be needed by 1994 to maintain thecurrent computertoprogrammer ratio. since such expansion is unlikely, greaterdependence must by placed on the user community. fortunately, the softwareindustry has the capability to help solve the problem.the dynamics of change in the information industry and society'scontinuing thirst for technologyaided solutions demand responses fromorganizational management. one user of computerthe user era126managing microcomputers in large organizationscopyright national academy of sciences. all rights reserved.technology, the u.s. department of the army, has identified seven initiatives tohelp deal with information systems:1. establish a single management source. the army is in the process ofcreating a new corporate ﬁofficerﬂ who will provide central control forplanning, programming, policy setting, and developing resources for allfacets of information systems (the information itself, automation, andcommunication facilities).2. establish a single developer for critical nonbattlefield informationsystems. this concept does not conflict with the plan to involve the usercommunity in a major portion of future systems developments. it doesrecognize, however, that some systems are so complex that they canonly be developed by professional programmers. further, systemsdeveloped by the individual user become candidates for organizationalstandardization. the central developer ultimately becomes responsiblefor the ﬁmaintenanceﬂ of systems selected for such standardization.3. emphasize technology. in an attempt to seek homogeneoustechnological solutions, thus minimizing software development costs,the army has acquired or is in the process of acquiring a variety ofhardware and software that will support this effort. it includesstandardized minicomputers and a family of microcomputers; a set ofhardware and supporting software that satisfies the department'sworldwide administrative and mobilization information needs; a broadband local area network within the pentagon to integrate multifunctionalinformation needs; video teleconferencing technology to be installedinitially at 18 locations, with options for additional installations; and adata base management system (dbms) for use on one vendor'sstandard mainframes. the army has also begun efforts to develop asoftware package that will combine the best features of relational,network, and hierarchical dbms techniques.4. improve planning. nolan articulated the need to develop horizontalcorporate database systems in lieu of the present vertical, singlefunctionsystems. to do this requires a unique analytical method. ibm, among alimited number of companies, has developed a procedure calledinformation systems planning (isp) that details the steps required insuch an analysis. the army has adopted the method and is in theprocess of performing isp studies worldwide.5. develop a new technologist. the melding of communicationthe user era127managing microcomputers in large organizationscopyright national academy of sciences. all rights reserved.and automation technologies demands individuals skilled in both fields.the army is engaged in developing programs that will train such persons.6. define the relative roles of users and data processing professionals. theincrease in the ability of users to perform their own data processingfunctions has led to friction between these two groups. yet both need toplay key roles. specifically, the professional should: design, develop,and manage the common user network; develop software that satisfiesthe needs of more than one agency or activity, such as payroll andbudget; develop and enforce standards throughout the organization;establish and staff the organization's information center, which instructsusers on new tools; provide fourthgeneration software tools; and act asthe database administrator, ensuring that every data element has a singleowner who has identified procedures for its use. finally, thisprofessional should develop a procedure to provide visibility for userdeveloped software. the army has developed a software clearinghousethat will function in two ways. users can find out if software alreadyexists that meets their requirements before initiating new development.professionals can use the clearinghouse to identify software productsthat are candidates for standardization.7. establish standards. in the public sector establishing standards is theonly real way to ensure both economic and effective systemsdevelopment. although the computer industry has defined standards incertain areas, it has not formulated standards in other areas for purelybusiness reasons. the army has set its own standards in five broad areas:languages. cobol, fortran, basic and ada (for battlefieldsystems) are established standards. simscript, lisp, c, and pascalare other languages that have been used for specific and obvious functions.data base management systems (dbms). the army has a variety ofdbmss. on most of its large ibm mainframes, datacom d/b isused. for the future the army believes that the solution lies in the creationof a simple but standard data manipulation language. the software behindthe language could be developed to manage databases using hierarchical,network, or relational procedures. with such a solution present databasescould be preserved.the user era128managing microcomputers in large organizationscopyright national academy of sciences. all rights reserved.communication protocols. the army is committed to the defensedata network (ddn) protocols (based on the sevenlayer referencestandard of the international organization for standardization). systemsthat do not satisfy ddn will not be purchased.operating systems. the army will establish as preferred standards cp/m for 8bit micros and both unix (version v) and ms dos for 16/32 bitmicros, with unix preferred.local area networks. the army will initially use ethernetprotocols (csma/cd), but recognizes that it is probably too early in thedevelopment of this technology to commit to a single standard.without doubt, society is in the midst of major change in its use ofcomputer technology. clearly, the seminal force is the availability of ever morepowerful technology at relatively low cost. computer literacy has increased atan exponential rate. the pressure on the software industry for more softwareproducts for the professional has exceeded its capability to respond. however,the industry has made specific useroriented software products available.to shape a program that benefits from these varied forces the army haschosen to provide topdown planning and broad architecture; bottomup, userinitiated development; a procedure for giving visibility to userdevelopedsoftware; a definition of the relative roles of the professional and the user; and aset of preferred standards to increase compatibility and minimize the cost ofsoftware. the specific course of action the army has chosen will clearly beiterative.in nineteenthcentury england a band of workmen known as luddites triedto prevent the use of laborsaving machinery by destroying it. obviously, thiswas not a very successful response to technological change. managing today'scomputer technology also requires the ability to implement changeappropriately. those organizations that remain static will be the luddites oftomorrow.the user era129managing microcomputers in large organizationscopyright national academy of sciences. all rights reserved.personal computing, not personalcomputersnorman m. epstein*at e.f. hutton we do not believe in personal computers, but we do believein personal computing. this is more than a semantic difference. it affects thetechnology applied, the philosophy we adhere to, and the planning that wentinto our system.although we think our approach is very obvious and logical one,apparently others do not. it reminds me of the famous story of louis pasteurand the maggots. in pasteur's day people believed that maggots came fromdecaying meat because every place there was decaying meat there weremaggots. but pasteur, being a scientist, took decaying meat, put it in a large belljar, and covered the bell jar with gauze. he came back three days later andfound the maggots on top of the gauze, proving that maggots came from gauze.in short, the system has to suit you. and if you look around, no one iswearing the same suit. our organization would seem to be a perfect candidatefor personal computers. we have 400 branch offices around the world and acampus in lower manhattan with 6,000 people in 12 buildings. we have 9,000people in the field, 6,000 account executives, and 3,000 people in supportpositions.our goal has been to automate the operation in the field. therefore,* norman m. epstein is executive vicepresident and director of e.f. hutton groupand e.f. hutton and company, inc., new york, new york.personal computing, not personal computers130managing microcomputers in large organizationscopyright national academy of sciences. all rights reserved.our approach might not work for a company with 14,000 people in one building.currently, each of our 6,000 account executives has a single terminal on adesk. these terminals are called branch informationprocessing systemterminals (bips). the terminal comes out of a dualport cpu (centralprocessing unit), and data comes from two different networks, bunker ramoand our own.when our system is complete everyone in the firm except telephoneoperators and porters will have a terminal. everyone. the multipurpose use ofthis terminal is the most important aspect of our system. we have a limitedamount of space on a desk, and we are going to accommodate it by having oneterminal. that one terminal may be supplied by data general, by ibm, bywang, by bunker ramo. but no matter who makes it, each terminal will haveaccess into all the databases. the secondary function of each terminaldetermines the reason for selecting a particular product. for example, thebunker ramo terminal is selected because it is a market data system, the wangterminal because it has certain word processing capabilities, the data generalterminal because it ties into our operations and communications.how are our operations and communications tied together? we start withheadend computers that create the databases and supply information atcorporate headquarters. the next level down consists of about 35 data generaleclipses located at regional offices around the world. why do we have adistributed network around the world? most people involved with personalcomputing know that when you hit the button and have to wait any length oftime for a response, you're in trouble. the reason for the distributed network isvery simple: hit the button, get a response. obviously, when the computer is inthe branch you can talk much faster.this part of the system provides only the data. there is nothing fancyabout the technology, no stateoftheart breakthroughs. we have simplyrecognized that if having data in a central location does not give adequateresponse time economically, we must move the data out.at the next level is the branch information processing system. it is theintegrated architecture of the network. it consists of 400 data generalmv4000s, one in each branch. the mv4000 is larger than the computer thatran all of e.f. hutton's communications a dozen years ago. it is basically a 2megabyte machine, .6personal computing, not personal computers131managing microcomputers in large organizationscopyright national academy of sciences. all rights reserved.mips (megainstructions per second), and 350 million characters of storage.this is an awesome box, and it is sitting in a branch office. the terminal hangsoff this box, with highspeed, dual mode printers and local database for veryrapid response time and for personal computing.at the bottom of the hierarchy, the terminal level, the branch informationprocessing system has the ability to grow whereas the personal computer doesnot. and the system can grow in the same box, for the terminals do not need tochange. they are essentially just a light bulb and a screen. the brains of thatterminal sits in the branch office. we estimate that one mv4000 canaccommodate 30 to 35 terminals. and, in general, we have three terminals toone letter quality printer.these levelsšheadquarters, region, branch, and terminalšare the fabricof our communications and data processing capability, and they are allinterconnected. the host complex controls the distributed information systemand the distributed information system controls the branch processing. userscan reach around it or through it to get to whatever level they want.from my perspective, the frequently asked question of who owns the datais the wrong question. we ought to ask who maintains the data. with oursystem, who ﬁownsﬂ it is irrelevant, since anybody who wants it and isapproved can get it. this system makes data available to anybody anywhere inthe world. each machine is individually addressable and assignable and has itsown name, and each user has his or her own signon code. the end result is thatone computer communications resource is providing information toapproximately 10,000 terminals. basically, the hardware is secondary; primarilywhat we do is provide a terminal, and the person using that terminal has thesmarts of the computer behind it.one of the problems we used to have involved gateways. in the past therewas only one way out of a branch office. that was via a teletype system thatwas part of our network. information was passed from functional areas tocommunications areas and then out. the new system solves that problem.everyone who has a terminal has a gateway out of the office.one of the most important management issues is that beyond a certainpoint you can't pay people more money to do a better job. and people who arecapable of doing a better job won't necessarily stay with a company just formore money. to solve the problempersonal computing, not personal computers132managing microcomputers in large organizationscopyright national academy of sciences. all rights reserved.you have to provide such people with better resources. make the same peoplemore productive. our system does that in a number of areas.my basic philosophy has always been that the point of origin ought to bethe point of entry. if you can accomplish that you can eliminate duplication.this system, obviously, is a natural extension of that philosophy. in terms ofword processing, for example, with multiple printers in the branch we can easilyseparate various functions.in the area of decision and sales support the system provides the realresources of personal computing at the branch level. anything that can be doneon a personal computer can be done on this system. and much more.for e.f. hutton electronic filing of information is part of the electronicmail system and part of the word processing system. it is integrated officeautomation, and our system does it very logically. within drawers we havefolders, within folders, documents. we can file electronically; we can crumpleand uncrumple electronically. we have, in effect, an electronic janitor, and untilhe empties the electronic waste basket, we can retrieve. again, we have all thethings personal computers give, plus additional capabilities.but the administrative support is what sells the system. we can use it as anextension of our operations administration. for example, i can use the system toschedule meetings with people who report to me. i have various options: i candemand a meeting, in which case the computer will cancel any conflictingmeetings. or i can request a meeting, meaning that if these people have opentime, the computer will arrange a meeting around their schedules. there will bedefaults in which i can get only 18 out of 20 people or 4 out of 5. i also haveﬁpublic timeﬂ in which people can schedule their meetings with me.the system extends outward as well. we started ﬁhutton lineﬂ in the stateof florida and expanded it nationally in december 1983. using a personalcomputer at home our customers can get direct access to our files. they can useword processing with their account executives, check portfolios, and perform avariety of other functions if they want, including spreadsheet analysis. in short,they have all the functions that are on the personal computer plus the functionson our computer. thus, the network now extends not only to headquarters, toregional offices, and topersonal computing, not personal computers133managing microcomputers in large organizationscopyright national academy of sciences. all rights reserved.branch offices, but also to our customers. with a million customers, we arelooking for a reasonable response of 5,000 clients by the end of 1984.this system cost 40 million dollars. for us, the costbenefits are really verysimple. e.f. hutton is a firm on the leading edge. as such, we cannot afford notto have such a system; we cannot afford to be second. our alternative is not tobe in business. therefore, it is not a question of if, it is a question of who. wechose data general because its hardware is significantly more powerful thanthe other computers we investigated. we received an operating system with addons and improvements written in to keep us competitive.we estimate that we can justify the cost of the system over a period ofthree to five years, and we anticipate its life cycle will be more than sevenyears. this is unusual because we generally blow our computers before theirleases expire. in fact, i have never kept a computer until term. the companysimply cannot afford to keep computers that are no longer efficient andeconomical.the cost, as far as i'm concerned, is dependent upon the mission. andrealizing the mission of an organization raises the issue of management control.yet, strangely enough, in all the talk about personal computers, we don't hearmuch about managers' ability to control the work. to me, online personalcomputing means giving away or abdicating a great amount of responsibility.and, as a businessman, i must wonder how i can control what i have given away.for this reason i view the personal computer as a dangerous weapon and itreat it as such. i think the first and most important question to ask whenconsidering a personal computing system is, can i control this? if the answer isyes, and i am satisfied with the level of control i can exercise over the system,then i would choose it. if not, i would look for something else. there is a vastarray of options from which to choose. my advice is to take the best and leavethe rest.personal computing, not personal computers134managing microcomputers in large organizationscopyright national academy of sciences. all rights reserved.control through persuasionallan z. loren*president john f. kennedy told the story of the leader in the frenchrevolution who said, ﬁthere go my people. i must find out where they aregoing so i can lead them.ﬂ this story suggests how cigna developed itsapproach for managing the proliferation of computer technology. it is anapproach i call ﬁcontrol through persuasion.ﬂperhaps the main reason this approach has worked so well is that it grewout of the particular character of our organization. cigna is both an oldcompany and a new company. formed by the largest merger in the financialservices industry, cigna is a blending of the old insurance company of northamerica (or ina) and the old connecticut general. it is a diversified financialservices organization with over 40,000 people worldwide, $35 billion in assets,1983 revenues of $12.5 billion, and an aftertax income of $400 million.cigna's employee life and health benefits division is the nation's sixthlargest provider of group insurance products, which include life, medical, anddental insurance. we are among the largest providers of property and casualtyinsurance and risk* allan z. loren is senior vicepresident of cigna's systems division, philadelphia,pennsylvania.control through persuasion135managing microcomputers in large organizationscopyright national academy of sciences. all rights reserved.management services in the world. we have international operations in 147countries and the largest investorowned health maintenance organization in thecountry.ranked as one of the nation's top 10 managers of private pension funds,cigna's employee retirement and savings benefit division provides a widerange of group annuity products and services and diversified investmentvehicles for pension, profitsharing, and employeepaid investment and savingsprograms. in addition, cigna's investment group provides a variety ofinvestment and portfolio management services and is among the nation's topasset management organizations.as a result of these different operations cigna is a melding of distinctand different cultures. even before the merger, however, the cultures of thepredecessor companies were very diverse. some areas have a centralizedvertical management; others have a more bottomup management style.different styles are intermingled among the various operations.several years ago we became aware of a couple of trends related totechnology and to our ﬁcustomers,ﬂ that is, our agents and other employees.first, technology was becoming less expensive, more plentiful, and morewidely available. second, a new type of employee/customer was beginning toemerge, younger than average, highly educated, and accustomed to a widevariety of automation capabilities. these new types began populating cignain all of the company's support functions, including marketing, financialanalysis, actuarial, and accounting. we concluded that to control and managethe upcoming ﬁtechnology explosionﬂ and at the same time satisfy the needs ofthis new employee/customer group, we needed to let customers experiment withtechnology. enforcing firm central control and policy would not be possible ordesirable given the diverse cultural environment and management stylescharacteristic of cigna. at the same time, of course, we didn't want toabdicate responsibility. the philosophy we came up with for managingtechnology might be described as ﬁplanting the seeds and letting the flowersbloom.ﬂto complement this philosophy we established an enduser supportgroup in 1975. this group, comprised of people with a marketing orientation,circulated within the corporation to get close to our new customers and to learnwhat their needs were. more importantly, this group worked to facilitate the useof technologycontrol through persuasion136managing microcomputers in large organizationscopyright national academy of sciences. all rights reserved.within the customer community. in effect, they became part of the customerenvironment.at the same time large central applications groups within the companywere also dealing with this new customer. the support group established closeties with these applications groups and with senior systems management. theclose contact among all these groups enabled us to monitor what was occurringand to suggest the most appropriate technology solutions to our customers'problems. to a large extent the support group is really a controlled distributionforce, but it doesn't appear so. its job is to channel our customers' requirementsinto the mainstream of technology.the first technical challenge the support group identified was the growthof timesharing. the new customers had already started going outside to get theircomputational needs satisfied. we facilitated and encouraged this. we alsowatched what was happening, and guided the customer to certain preferredvendors and applications. then, when we understood what our customers' realneeds were, we created a solutionšan internal timesharing system that we callquest i. the support group took this internal timesharing system and ﬁsoldﬂ itto the customers who had been buying outside timesharing services. there wereseveral advantages to our inhouse system. first, our timesharing service wasmore competitively priced than those outside. second, we had easier access tocorporate databases. third, we provided better service. in effect, we went intocompetition with external timesharing organizations and we were verysuccessful.i must emphasize that we did not mandate the use of this internaltimesharing service. however, it was priced and packaged in such a way thatcustomers could not justify going outside and spending more money for thesame service. this milton friedmanlike ﬁfree marketplaceﬂ philosophyworked quite effectively and as a result most of the outside timesharing waseventually internalized. some outside databases were being used, however,which we could not internalize. in these cases, processing with outside vendorscontinued. about four years after we introduced quest i we came out with anew product called quest ii. it has enhanced graphics and languagecapabilities, offers more software packages, has increased filehandlingcapabilities, and overall, provides more functions.control through persuasion137managing microcomputers in large organizationscopyright national academy of sciences. all rights reserved.when word processing became the ﬁnewﬂ technology, we provided ourcustomers with direction by selecting three vendors with whom we would dobusiness. through consultation and training we channeled these vendor servicesto our employee/customer.as a result of the experiences with timesharing networks and wordprocessing systems an interesting situation developed. the support group wasable to build a strong relationship with its customer. the group was acceptedinto the new customer's organization and was considered part of the customer'steam. as new technology emerged these customers turned to the support groupfor advice and consultation.in addition to forming close ties with the customer community, the supportgroup also maintained its strong links with the applications groups and was ableto provide these groups with insight into changing customer requirements.in the late 1970s and early 1980s, when personal computers started tobecome popular, we made available to customersšon a casual basisšdemonstrations of hardware and software and provided people to answerquestions. this approach evolved into what we now call our ﬁinformationcenters.ﬂmore recently, we established formal information centers in a halfdozenlocations. these centers were created to provide customers with a central placeto get information, demonstrations, guidance, and training on cigna's latesttechnology. through the centers we were able to channel people in the rightdirection. in essence, the information centers served as magnets to draw anddirect our customers.this approach was quite successful. we found that many of our customerswanted help, particularly with the proliferation of hardware and software that isbeing offered. the information centers and our enduser support group, withtheir very close customer working relationships, were able to produce a set ofﬁtechnology handcuffs.ﬂ the customer had become dependent on the group andthe centers for guidance on technology. from our standpoint this was an idealrelationship because we were able to point the customer in the direction wewanted to go.as the 1980s rolled around there was a terrific explosion inmicrocomputers. at cigna we experienced some of the effects of thisexplosion. the information centers, which were well structured by this time,served as real magnets for microcomputercontrol through persuasion138managing microcomputers in large organizationscopyright national academy of sciences. all rights reserved.users. in fact, the centers became overwhelmed by the number of customersusing their services. to deal with this situation, we went from an informal,casual environment to a more structured, institutional one.we also began to enhance our level of support. we required users ofmicrocomputers to participate in formal training programs on hardware andsoftware. a newsletter was created to disseminate the latest information onmicrocomputers. an ﬁapproved productﬂ catalogue was developed to aidcustomers in ordering. because our customers were demanding direction, webegan to issue more standards and became more involved in evaluatingproducts and services. we continued to keep the communication lines open inorder to improve our ability to control and channel the uses of this technology.today the sheer variety of outside software available to our customers is asource of confusion and therefore has become an area of considerable concernfor us. to address this concern we plan to establish a software library. we willlend out software that we find acceptable and provide demonstrations of bothﬁapprovedﬂ and ﬁunapprovedﬂ software in order to show the contrast. to set upthis library we will need to arrange for national contracts on software, thusreducing costs and avoiding multiple distributions. we believe the library willsave money and help reduce the confusion caused by the proliferation ofsoftware. we are also considering developing generic software to run on ourpersonal computers and mainframe. we are finding that much of the softwarebeing acquired is used for fairly similar functions and we think we couldprovide internally written software that accomplishes these same functions.another supportrelated enhancement we are considering is the extensionof the information center concept to include a computer store. at present, ordersare placed with outside vendors who process them and deliver the software andhardware. our strategy is to internalize this service by having our customersorder directly through an information center computer store. a centralizedprocurement activity will not only be convenient for our customers but willallow us to control inventory, offer standard services, and keep a handle on ourcustomers' needs.in addition, the store will improve our visibility in the customercommunity, provide us with more information on maintenance and performanceof equipment, and help us to know our customers'control through persuasion139managing microcomputers in large organizationscopyright national academy of sciences. all rights reserved.upgrading needs. the store will also give us and our customers moreopportunities to exchange information on applications.to summarize, the technique cigna has used to control proliferationincludes having a ﬁfriendlyﬂ and ﬁforgivingﬂ enduser support group whosejob was to serve as objective advisor and work closely with customers. supportgroup members were able to subtly channel customers in the direction wewanted. today they continue to provide and enhance support through twowaycommunications, thereby enabling us both to maintain control and to satisfy ourcustomers' requirements.control through persuasion140managing microcomputers in large organizationscopyright national academy of sciences. all rights reserved.indexaabstracting, 106accounting tasks, 63acquisition practices, 54, 57, 59, 74,7677, 9395, 115, 117120ada, 128administrative support, 133agency initiatives, 5556air force, 9396air force computer acquisition center, 95alaska state government, 120alexander, john j., jr., 101114alice's adventures in wonderland, 50amdahl, 110analytic tool packages, 107apollo, 2526apple inc.lisa, 25, 29, 3233, 63, 76mcintosh, 24applications generators, 4749applications software, 2123, 120shared, 118architectural issues, 24, 63, 129artificial intelligence, 2427, 61assembly languages vs. higherlevel languages, 29audio links, 64audit logs and procedures, 90auditors, role of, 7778, 80, 113authorization procedures, 117119automated data processing (adp) planning, 22automated information processing, 105,111automated programming aids, 48automation, 9, 101114automation professionals, 111bbackbone systems, 121backlog in computer service departments,4648, 103104backup and recovery procedures, 75, 90,118index141managing microcomputers in large organizationscopyright national academy of sciences. all rights reserved.backward compatibility, 93bair, james h., 67, 44, 6066bank of america, 86basic, 128batch processing, 23, 36, 92bennett, john h., 56, 44, 4551bitmaps, 26blank, hannah i., 4344boolean logic capabilities, 106bottomup management, 136boxcentric computer development, 37branch information processing systemterminals (bips), 131132brickland, dan, 31budgetary control procedures, 125bunker ramo network, 131burroughs corp. link, 48bush, vannevar, 60business analysis, 105, 107108business graphics, 26business information managementdepartments, 78business information systems, 14business systems analysis, 78buying and procedural guides, 57cc, 128california state government, 119canter, rhoda w., 99100capacity planning, 2223, 25career training for microcomputer users,see enduser educationcarroll, lewis, 50centralization vs. decentralization, 93,104105, 110111, 119centralized electronic data processing(edp), 110centralized maintenance, 117centralized management, 76, 118, 125,127, 136, 139,see also decentralized managementchargeback systems, 9092children's access to computers, 116cigna's systems division, 135146cobol, 128color graphics, 107communications issues, 56, 13, 64, 78,87, 116, 121,see also networkingcommunications protocols, 129for moving information between windows, 63computer centers, 23computer literacy, 11, 2021, 44, 7576,126, 129,see also enduser educationcomputer life cycles, 134computer messaging, 64computer service organizations (csos),47, 4950computer stores, 118119, 139computertoprogrammer ratio, 126connectivity, 7conrad, thomas d., 70, 9396context, mba software package, 32, 49contractordeveloped software, 121contractor support, postsale, 95control, 62, 125,see also data securityindex142managing microcomputers in large organizationscopyright national academy of sciences. all rights reserved.control measures, management, 76, 134,137control through persuasion, 135140control vs. creativity, 9, 1112corporate databases, 63, 125, 127, 137costbenefit analysis, 26, 117118costjustified applications, 7, 4647, 105,107108cost performance curve in logicaldevices, 126cp/m, 26, 129creativity, 812, 8384, 92culture of organizations, 111custom applications, 3435, 103104, 111,see also softwarecustomer access to data and files,133134, 136cynthia peripherals, 2627ddade county, florida, 120data, portability of, 93data accessibility, 7, 48, 73, 78, 112dataanalyzing capabilities, 86datacom d/b, 128data compatibility, 7, 64, 73datacrunching, 23data dictionary, 112data generaleclipse, 131mv4000s, 131132data integrity, 5, 43, 75, 7879, 8990, 123data inquiry, 105106, 108data maintenance, 132data management, 7, 57, 73, 76, 7879data ownership, 88, 90, 92, 113, 125, 128,132data processing/management informationsystems (dp/mis) organization ,6162data processing planning, 119data processing professional standards,8283, 8990data processing (dp) professionals, roleof, 11, 13, 16, 4344, 121 , 128,see also information processingdata security, 67, 43, 4748, 73, 75,7879, 82, 8789, 92, 112113data sharing, 32,see also integrated softwaredatabases, 3940, 131corporate, 63, 125, 127, 137database administration, 128database management systems (dbms),93, 127128dbase, 2829dbase ii, multiaccess, 3940decentralized management, 83, 110, 125,136,see also centralized managementdecision quality, 8283, 8586, 89decision support systems, 6, 121, 133defense data network (ddn) protocols,129department of agriculture's graduateschool program, 57departmentlevel computing, 63, 7273,79, 112, 121desktop personal filing, 62diebold, john, 3, 7, 1114diebold research program, 13digital equipment corporation pdp 11, 46distributed creativity, 812, 8384, 92distributed processing, 110index143managing microcomputers in large organizationscopyright national academy of sciences. all rights reserved.documentation, 116domain operating system, 26dualport cpu, 129dugger, fred, 115123eeducation, see enduser educatione.f. hutton group, 130134electronic janitor, 133electronic mail, 3839, 64, 105108, 133employeeschanging skills of, 113, 136,see also enduser educationsalaried, 101114employment security department (esd)(washington), 121encryption, 88enduser computing, iii, 3, 8, 5259, 75,8192enduser education, 1314, 5455, 58, 79,92, 115, 117, 119120, 139enduser purchase of microcomputers, 13,24, 4647, 72enduser skill level, 11, 2021, 44, 7576,126, 129enduser support groups, 54, 58, 117,136, 140endusers, role of, 124129endusers,see also userdeveloped software; userfriendly softwareepstein, norman m., 8, 130134error detection, 75essex group headquarters, 48ethernet, 37, 129,see also networkingexecutive personal computing workshop, 49expansion stage, 124125expert systems, 61expert users, 35ffloppy disksdata security, 6, 88, 92elimination of through networks, 39form processors, universal, 107formsbase messaging, 39fortran, 128fourthand fifthgeneration language systems, 85fourthgeneration software, 128free marketplace philosophy, 137ggateways, 132general ledgers, 63general motors, 78general services administration (gsa),5259generic software, 139gibson and nolan's stages of growthcurve, 8government organizations, recommendations for, 122123governmentwide initiatives, 5557grace commission report, 52graphicsbusiness, 26capabilities of internal systems, 137color, 107graphics integration, 26index144managing microcomputers in large organizationscopyright national academy of sciences. all rights reserved.hhardwareacquisition, standardized guidelines for,118,see also acquisition practicescentralization vs. decentralization, 110developmental stages, 21, 124increased power of, 2526portability of, 93hardware peripheral vendors, 2324headend computers, 131health and welfare department (california), 119hewlettpackard hp7475a plotters, 26highlevel languages, 48highresolution bitmap displays, 26highspeed dual mode printers, 132homework, microcomputerrelated issues,44honeywell, 26horizontal corporate database systems, 127human resource development (hrd)search activity, 106ﬁhutton line,ﬂ; 133134iibmbusiness systems planning (bsp) technology, 102displaywriter, 102, 110information systems planning (isp), 127pcs, 24, 3738, 49, 76, 102, 110system/38s, 102, 106, 110illinois state government, 119120implementation of productivity improvements, 112industrial renaissance, 85inhouse maintenance, 94inhouse programming, 126inhouse technical support, 59in search of excellence, 83inexpensive online computer resources, 85inflexible systems, 75information center computer stores, 139information centers, 3, 48, 85, 92, 120,128, 138139information clearinghouse, 122information flow to top management, 115informationhandling requirements, 104information management, 111112information processing, 1112, 106,see also data processinginformation system analysts, 104information systems (is) managers, roleof, 5initiation stage, 124inquire, 106integrated circuit technology, 2225, 27integrated software, 45, 21, 3235, 54integration stage, 125intel corporation microprocessors, 24intellect, 106intelligent workstations, 64,see also workstationsinterdepartmental interfaces, 78index145managing microcomputers in large organizationscopyright national academy of sciences. all rights reserved.internal computer stores, 118119, 139internal marketing, 136137international data corporation, 1927international organization for standardization, 129international word processing association, 61jjob definitions, changing, 4344, 108job placement training act (jpta) activities, 121kkapor, mitchell, 4, 17, 2835, 37kentucky state government, 118119kline, ray, 6, 44, 5259knowledge systems, 61llanguageprocessing systems, 106language proliferation, 89, 9192, 128language systems, fourth and fifth generation, 85languages, assembly vs. higherlevel, 29largescale computing services, 72learning support systems, 6leary, william h., iii, 1718letter quality printers, 132license restrictions on software, 78, 74lisa, 25, 29, 3233, 63, 76lisacalc, 33lisp, 128local area networks, 3640, 105, 111112,115, 121, 129, 131,see also networkinglocal governments, 115123logic chips, 27logon passwords, 88, 92, 132loren, allan z., 135140lotus development corporation 123,2829, 35, 40mmcdonough, francis a., iiiivmacrolanguage, 35main files, update of, 88, 92,see also data ownershipmainframes vs. microcomputers, 2526,8687, 120maintenancecentralized, 115, 117, 119, 139contract, 57, 94inhouse, 94managed innovation program, 5557management, centralized vs. decentralized, 110, 136,see also centralized management;decentralized managementmanagement education, 49, 121123management information systems (mis)departments, role of, 10, 14, 6061,71, 7477, 79, 112management research, 8586index146managing microcomputers in large organizationscopyright national academy of sciences. all rights reserved.management support systems, 3, 6,122123managing enduser computing in thefederal government, 53manufacturing systems, 13market data system, 131market need, software development independent of, 29marketing tactics, 72, 104master data processing plan, 119maturity stage, 125meeting scheduling, 133metcalfe, robert m., 5, 1718, 3640microcomputer support unit (msu) (kentucky), 118119microcomputersfuture of, 1927,see also personal computersguidelines for selecting, 119micromainframe communication, 12, 22,30, 34, 38, 63, 87, 99, 119 , 121, 123micromicro communication, 38, 121minicomputers, 3mis departments, see management information systems departmentsmis quarterly, 85model template application, 35modem chips, 25motorola 68000, 24mouse, 25ms dos, 129multiaccess networked software, 3940,127,see also databases, corporatemultiple concurrent processes, 26multiple functions and responsibilities ofmanagement, 113multiple windows, 22, 25nnational association of state informationsystems (nasis), 121122national contracts on software, 139nationwide cluster maintenance contracts,57networkdelivered software, 3940network design, 13networking, 23, 55, 128local area, 3640, 64, 105, 111112, 115,121, 129, 131,see also communications issuesnevada state government, 121nolan, richard, 8, 74, 124125, 127oobjectoriented architecture, 24obsolescence, 24, 7475office automation, 13, 6066offtheshelf software, 35, 117, 120121omand, alastair i., 78, 70, 7180online computer resources, inexpensive,85openended software systems, 3335operating systems, 129user isolation from, 92organizational issues, 9, 108109, 127,135140operating systems, 129index147managing microcomputers in large organizationscopyright national academy of sciences. all rights reserved.pparsing programs, 106pascal, 128passwords, 88, 92payroll processing, 72peripheral chips, 22, 2425peripheral sharing, 20, 3738peripheral vendors, role of in microcomputer development, 1920personal computer networks, 3640,see also networkspersonal computer purchases by endusers, 13, 24, 4647, 72personal computer software, trends in,2835personal computers in business, proliferation of, 3, 12, 20, 62, 7374,see also microcomputerspersonal computing, 7273, 84personnel departments, 63personnel education, 79piaget, jean, 21planning cycle for automation, 102plotters (2or 3), 26policy development, 122portability of hardware and data, 93postsale contractor support, 95printers, 132privacy, 112private sector organizations, recommendations for, 122123procedure development, 122procurement procedures, 54, 57, 59, 74,7677, 9395, 115, 117120 , 139,see also purchasing departmentsproduct warranty systems, 72productivity, as cost justification, 55, 105productivity, individual, 72, 132133productivity, measurement of, 7, 62, 113,116117productivity improvements, implementation of, 112productivity improvements planning,101114, 122productivity vs. creativity, 10program generators, 48programmer productivity, 48, 117programs, see softwareproliferation of hardware and software,74, 76, 89, 110, 135proliferation of languages, 89, 9192, 128purchase of computers by end users, 13,24, 4647, 72, 94purchasing departments, role of, 7980, 118qquality of decisions, 8283, 8596, 89quantity discounts, 119querylanguage facilities, 106quest i and ii, 137rraytheon readicode, 48records management, 63recovery procedures, 75, 90, 118remote data, 22remote job entry, 110report writers, 48request for a proposal (rfp), 9495research and education committee ofnasis, 122research librarians, 106index148managing microcomputers in large organizationscopyright national academy of sciences. all rights reserved.resource use, 9091response time, 8687retraining, 113, 136return on investment (roi) calculations,117review procedures, 75reynolds metals company, 101114robotics, 61rolling resources, 20rosser, william c., 6970rotation policies, 93ssales support, 133security, see data securityselfdocumenting tools, 47semiconductor technology, 2225, 27signon codes, 88, 92, 132simscript, 128singleuser operating systems, 20sisson, roger l., 8, 70, 8192skill sets, 7, 7576softwarebasic applications, 32custom applications, 3435, 103104, 111integrated, 45, 21, 3235, 54licensing restrictions on, 78, 74most successful, 2829offtheshelf, 35, 117, 120121userfriendly, 8687, 120121, 129software acquisition policies, 115, 118, 120software developmentcentralized vs. decentralized, 110,126127enduser, 8990, 92, 125, 128129focus of, 1921, 37hardware controlled, 3031individual geniuses vs. formal development teams, 2830software library, 139software piracy, 74software proliferation, 120121, 126, 139software vendors, 2835spreadsheets, 2829, 31, 39, 104, 107, 133stages of evolution of computers andinformation systems, 124125standalone computing, 54standardization, see centralization vs.decentralization; management; maintenance,standardized maintenance, 117standards, establishment of, 128stanford research institute, 60state government policies, 115123synapse, 26systems development, 75, 79, 104, 122,125systems generators, 47ttechnical computation, 105, 107108technical excellence, 9596technologists, role of, 127128teleconferencing, 64telephone access, illegal, 88telephone management, 62telephony, 64index149managing microcomputers in large organizationscopyright national academy of sciences. all rights reserved.terminalsmultipurpose use of, 131proliferation of, 4546, 48, 131text integration, 26textprocessing language, 106the source, 63thompson, john m., 3103com's local network, 39timesharing, 3, 37, 87, 107, 110, 137,see also networkingtoffler, alvin, 61topdown planning, 129training of endusers, see enduser educationtransaction processing, 105108transparently networked software, 39uunited states army, 124129united technologies corporation (utc),4551universal form processors, 107unix, 76, 129unplanned systems, 116update of main files, 92use, actual vs. justified, 108109userdeveloped software, 8990, 92, 125,128129userfriendly operating systems, 85userfriendly software, 8687, 120121,129user literacy, 11, 2021, 44, 7576, 126,129user networks, see communicationsissues; networkingusers, see endusersvvax, 46vendor interface, 74vendor proliferation, 117, 120121vendor role in technical support and maintenance, 76vendors, qualified, 118videotex, 63video teleconferencing technology, 127visicalc, 2829, 31, 39, 104, 107visicorp, 32vision, 29, 40visiword, network delivered, 39visual information sharing, 64voice integration, 39wwafer technology, 27wang, 26washington state government, 121wasted resources, invisibility of, 91willmott, thomas h., 4, 17, 1927winchester disk technology, 2627windows, 3334, 63windowmouse systems, 3334word processing, 105, 107108, 133, 138effect of peripheral chip technology on,25introduction of, 61word processing capabilities, 131wordstar, 2829workstations, 6, 20, 46, 49, 64, 87index150managing microcomputers in large organizationscopyright national academy of sciences. all rights reserved.xxerox star 8010, 64yyale artificial intelligence labs, 2526zz 80 software developers, 2324zenith data systems microprocessors, 96zimmerman, martin b., 124129index151
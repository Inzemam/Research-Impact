detailsdistribution, posting, or copying of this pdf is strictly prohibited without written permission of the national academies press. (request permission) unless otherwise indicated, all materials in this pdf are copyrighted by the national academy of sciences.copyright © national academy of sciences. all rights reserved.the national academies pressvisit the national academies press at nap.edu and login or register to get:œ œ 10% off the price of print titlesœ special offers and discountsget this bookfind related titlesthis pdf is available at sharecontributorshttp://nap.edu/9823the internet's coming of age256 pages | 6 x 9 | hardbackisbn 9780309384858 | doi 10.17226/9823committee on the internet in the evolving information infrastructure, computerscience and telecommunications board, commission on physical sciences,mathematics, and applications, national research councilthe internet's coming of agecopyright national academy of sciences. all rights reserved.the internet's coming of agecopyright national academy of sciences. all rights reserved.committee on the internet in theevolving information infrastructurecomputer science and telecommunications boardcommission on physical sciences, mathematics, and applicationsnational research councilnational academy presswashington, d.c.the coming of agethe internet's coming of agecopyright national academy of sciences. all rights reserved.notice: the project that is the subject of this report was approved by the governing board of the national research council, whose members are drawn fromthe councils of the national academy of sciences, the national academy of engineering, and the institute of medicine. the members of the committee responsiblefor the report were chosen for their special competences and with regard forappropriate balance.support for this project was provided by the national science foundation undercontract no. ani9714374. any opinions, findings, conclusions, or recommendations expressed in this material are those of the authors and do not necessarilyreflect the views of the sponsor.library of congress cataloginginpublication datathe internetõs coming of age / committee on the internet in the evolvinginformation infrastructure, computer science and telecommunicationsboard, commission on physical sciences, mathematics, and applications,national research council.p. cm.isbn 03090699201. internet. i. national research council (u.s.). committee on theinternet in the evolving information infrastructure. ii. title.tk5105.875.i57 i5435 2000004.67õ8ñdc2100012242additional copies of this report are available from:national academy press2101 constitution ave., nwbox 285washington, dc 2005580062462422023343313 (in the washington metropolitan area)http://www.nap.educopyright 2001 by the national academy of sciences. all rights reserved.printed in the united states of americathe internet's coming of agecopyright national academy of sciences. all rights reserved.the national academy of sciences is a private, nonprofit, selfperpetuating society of distinguished scholars engaged in scientific and engineering research, dedicated to the furtherance of science and technology and to their use for the generalwelfare. upon the authority of the charter granted to it by the congress in 1863,the academy has a mandate that requires it to advise the federal government onscientific and technical matters. dr. bruce m. alberts is president of the nationalacademy of sciences.the national academy of engineering was established in 1964, under the charterof the national academy of sciences, as a parallel organization of outstandingengineers. it is autonomous in its administration and in the selection of its members, sharing with the national academy of sciences the responsibility for advising the federal government. the national academy of engineering also sponsorsengineering programs aimed at meeting national needs, encourages educationand research, and recognizes the superior achievements of engineers. dr. williama. wulf is president of the national academy of engineering.the institute of medicine was established in 1970 by the national academy ofsciences to secure the services of eminent members of appropriate professions inthe examination of policy matters pertaining to the health of the public. theinstitute acts under the responsibility given to the national academy of sciencesby its congressional charter to be an adviser to the federal government and, uponits own initiative, to identify issues of medical care, research, and education.dr. kenneth i. shine is president of the institute of medicine.the national research council was organized by the national academy of sciences in 1916 to associate the broad community of science and technology withthe academyõs purposes of furthering knowledge and advising the federal government. functioning in accordance with general policies determined by theacademy, the council has become the principal operating agency of both thenational academy of sciences and the national academy of engineering in providing services to the government, the public, and the scientific and engineeringcommunities. the council is administered jointly by both academies and theinstitute of medicine. dr. bruce m. alberts and dr. william a. wulf are chairmanand vice chairman, respectively, of the national research council.national academy of sciencesnational academy of engineeringinstitute of medicinenational research councilthe internet's coming of agecopyright national academy of sciences. all rights reserved.the internet's coming of agecopyright national academy of sciences. all rights reserved.committee on the internetin the evolving information infrastructureeric schmidt, novell inc., chairterrence mcgarty, the telmarc group, vice chairanthony s. acampora, university of california at san diegowalter s. baer, rand corporationfred baker, cisco systemsandrew blau, flanerie worksdeborah estrin, university of california at los angeleschristian huitema, microsoftedward jung, intellectual venturesdavid a. kettler, bellsouthjohn c. klensin, at&tmilo medin, excite@homecraig partridge, bbn technologiesdaniel schutzer, citibankspecial advisordavid d. clark, massachusetts institute of technologystaffjon eisenberg, study directormarjory s. blumenthal, directorsuzanne ossa, senior project assistantdavid padgham, research assistantvthe internet's coming of agecopyright national academy of sciences. all rights reserved.computer science and telecommunications boarddavid d. clark, massachusetts institute of technology, chairdavid borth, motorola labsjames chiddix, time warner cablejohn m. cioffi, stanford universityelaine cohen, university of utahw. bruce croft, university of massachusetts, amherstsusan l. graham, university of california at berkeleyjudith hempel, university of california at san franciscojeffrey m. jaffe, ibm corporationanna karlin, university of washingtonmichael katz, university of california at berkeleybutler w. lampson, microsoft corporationedward d. lazowska, university of washingtondavid liddle, u.s. venture partnerstom m. mitchell, whizbang! labs, inc.donald norman, unext.comdavid a. patterson, university of california at berkeleyhenry (hank) perritt, chicagokent college of lawcharles simonyi, microsoft corporationburton smith, tera computer companyterry smith, university of california at santa barbaralee sproull, new york universitymarjory s. blumenthal, directorherbert s. lin, senior scientistjerry r. sheehan, senior program officeralan s. inouye, senior program officerjon eisenberg, senior program officergail pritchard, program officerlynette millett, program officerjanet briscoe, administrative officerdavid c. drake, project assistantdaniel d. llata, senior project assistantmargaret marsh, senior project assistantdavid padgham, research assistantmickelle rodgers rodriguez, senior project assistantsuzanne ossa, senior project assistantbrandye williams, office assistantvithe internet's coming of agecopyright national academy of sciences. all rights reserved.commission on physical sciences,mathematics, and applicationspeter m. banks, xr ventures, llc, cochairwilliam h. press, los alamos national laboratory, cochairwilliam f. ballhaus, jr., the aerospace corporationshirley chiang, university of california at davismarshall h. cohen, california institute of technologyronald g. douglas, texas a&m universitysamuel h. fuller, analog devices, inc.michael f. goodchild, university of california at santa barbaramartha p. haynes, cornell universitywesley t. huntress, jr., carnegie institutioncarol m. jantzen, westinghouse savannah river companypaul g. kaminski, technovation, inc.kenneth h. keller, university of minnesotajohn r. kreick, sanders, a lockheed martin company (retired)marsha i. lester, university of pennsylvaniaw. carl lineberger, university of coloradodusa m. mcduff, state university of new york at stony brookjanet norwood, former commissioner, bureau of labor statisticsm. elisabeth pat…cornell, stanford universitynicholas p. samios, brookhaven national laboratoryrobert j. spinrad, xerox parc (retired)james f. hinchman, acting executive directorviithe internet's coming of agecopyright national academy of sciences. all rights reserved.the internet's coming of agecopyright national academy of sciences. all rights reserved.in 1967, the presidentõs science advisory committeeõs panel on computers in higher education opened its report by noting that òafter growing wildly for years, the field of computing now appears to be approaching its infancy.ó1 this comment preceded by about 2 years the initialdeployment of internet nodes, but while computing developed and penetrated society in many ways over the succeeding decades, the internetgrew more slowly until its commercialization in 1995, which led to anexplosion of growth that continues today. extending the 1967 advisorycommitteeõs analogy, one might today view the internet as having reachedits adolescence. how it will grow upñand how its maturation can befosteredñis the subject of this report.motivated by two concernsñwhat would help the internet mature tomeet everrising expectations and how might that maturation beachievedñthe national science foundation asked the computer scienceand telecommunications board (cstb) to undertake a study of theinternet and the key challenges that will shape its maturation, focusing onthe core technologies of the internet. in response to this request, cstbassembled the committee on the internet in the evolving informationinfrastructure. this committee, made up of experts in technology and1presidentõs science advisory committee. 1967. computers in higher education. whitehouse, washington, d.c., february, p. 1.ixthe internet's coming of agecopyright national academy of sciences. all rights reserved.xprefacepolicy, received briefings and conducted deliberations over a period ofalmost 2 years. those 2 years were particularly turbulent. they witnessed enormous growth and diversification of the industries and nonprofit entities associated with the internet and turned it into a causec”l‘bre: the internet seems to be everyoneõs business today.no matter how rapid the changes, the committee process does notlend itself to the reaching of consensus in òinternet time.ó like mostcstb committees, this one, too, needed time to learn, deliberate, andconvergeñand also to see which trends endured and which seemed transient. many parts of the report required little updating in the course ofthis work, as they speak to basic design or technical principles that haveremained durable. but some elements in the landscape changed over thepast year, and the committee strove to update its analysis accordingly. itsought to focus on guiding principles rather than on the more rapidlyshifting details. the resulting integrated discussion and analysis is intended to help inform technical design, development, deployment, operation, and management decisions relevant to the evolving network of networks. it is also intended to guide policy makers as they seek to reconcilethe internetõs unique features with the existing body of policies and practices that are touched by the internet, such as telecommunications regulation.the internetõs coming of age is the latest report in an influential seriesabout the internet issued by cstb. the first report, toward a nationalresearch network (1988), validated the concept of a comprehensive network to support communications among researchers across the country(and around the world), and it leveraged federal funding to supportinternet research, development, and deployment in the late 1980s. thesecond, realizing the information future: the internet and beyond (1994),addressed the internetõs transition from a network complex aimed at theresearch, education, and library communities to a complex serving allsegments of the economy and society, noting potential impacts on thosepioneering communities and covering a range of issues, from pricing tointellectual property protection, that would impinge on the commercialinternet. it explained the makeup of the internet, relating its essentialtechnology to the proliferation of uses and communications modes thataccelerated in the 1990s. the third, the unpredictable certainty: informationinfrastructure through 2000 (1996), examined the different industrieswhose investments would be key to the internetõs growth and the evolution of user interests in internet capabilitiesñthe chemistry of supply anddemand that would shape what the internet looks like. this new reportgives readers an inside look at todayõs thriving commercial internet, identhe internet's coming of agecopyright national academy of sciences. all rights reserved.prefacexitifying short and longterm technical challenges as well as approaches tointernetrelated public policy.the committee wishes to thank the various members of the cstb staffwho helped to make this report happen. in particular, jon eisenberg, thestaff officer responsible for this project, has played a central role throughout the entire project, coordinating all of the various elements of the report. the committee would also like to thank suzanne ossa for her assistance in organizing committee meetings and preparing the report. davidpadgham contributed significantly to the editing and research done forthis report. liz fikre was instrumental in editing the final manuscript.david d. clark, chaircomputer science and telecommunications boardthe internet's coming of agecopyright national academy of sciences. all rights reserved.the internet's coming of agecopyright national academy of sciences. all rights reserved.this report has been reviewed in draft form by individuals chosen fortheir diverse perspectives and technical expertise, in accordance with procedures approved by the nrcõs report review committee. the purposeof this independent review is to provide candid and critical commentsthat will assist the institution in making its published report as sound aspossible and to ensure that the report meets institutional standards forobjectivity, evidence, and responsiveness to the study charge. the reviewcomments and draft manuscript remain confidential to protect the integrity of the deliberative process. we wish to thank the following individuals for their review of this report:geoff baehr, sun microsystems, inc.,edward balkovich, bell atlantic,scott bradner, harvard university,hanswerner braun, university of california at san diego,charles n. brownstein, corporation for national research initiatives,brian e. carpenter, ibm,william j. dally, stanford university,joseph farrell, university of california at berkeley,robert m. frieden, pennsylvania state university,reed e. hundt, mckinsey & company,geoffrey huston, telstra internet,stephen t. kent, bbn corporation,hal varian, university of california at berkeley, andkevin werbach, release 1.0.xiiithe internet's coming of agecopyright national academy of sciences. all rights reserved.although the reviewers listed above provided many constructivecomments and suggestions, they were not asked to endorse the conclusions or recommendations, nor did they see the final draft of the reportbefore its release. the review of this report was overseen by william h.press, los alamos national laboratory, appointed by the nrcõs commission on physical sciences, mathematics, and applications, who wasresponsible for making certain that an independent examination of thisreport was carried out in accordance with institutional procedures andthat all review comments were carefully considered. responsibility forthe final content of this report rests entirely with the authoring committeeand the institution.xivacknowledgment of reviewersthe internet's coming of agecopyright national academy of sciences. all rights reserved.˝overview and recommendations11introduction and context29what is the internet?, 29success by designñabstract features and principles, 34the internetõs òhourglassó architecture, 36the robustness principle, 39scalable, distributed, and adaptive design, 40from internet technology to internet marketplace, 41internet organizations, 43key trends in internet development, 44growth in backbone capacity, 45growth and diversification of the isp market, 46upgrading the local access infrastructure, 46growing role for wireless services, 49voice and data services, 50rise in the use of singlepurpose devices, 50future evolution and success, 512scaling up the internet and making itmore reliable and robust53building a better internet, 53scaling, 54scaling of capacity, 55xvthe internet's coming of agecopyright national academy of sciences. all rights reserved.xvicontentsscaling of protocols and algorithms, 56scaling of the internetõs naming systems, 58scaling up the address space, 64managing addresses, 65routing table scaling and address aggregation, 66running out of addresses?, 71network address translation, 76ipv6: a potential solution to addressing and configuration, 77deploying an ipv6 solution, 79reliability and robustness, 81designing for robustness and reliability, 82vulnerability of the internet to attack, 84more adaptive routing, 89putting it together, 90application reliability and robustness, 92robustness and auxiliary servers, 93toward greater reliability and robustness: reportingoutages and failures, 94quality of service, 983keeping the internet the internet:interconnection, openness, and transparency107interconnection: maintaining endtoend service throughmultiple providers, 107structure of the internet service provider industry, 109interconnection mechanisms and agreements, 112considerations affecting decisions to enter intopeering agreements, 118evolution of interconnection models, 121monitoring internet interconnection, 123openness and innovation, 124critical open standards in the internetñthe hourglassarchitecture, 126the internet as a platform for application innovation, 131evolution of internet standards setting, 132endtoend transparency, 138addressing issues, 139nonuniform treatment of bits, 142market and business influences on openness, 145keeping the internet open, 149the internet's coming of agecopyright national academy of sciences. all rights reserved.contentsxvii4collisions between existing industries andemerging internet industries: telephonyas a case study151introduction, 151what is ip telephony?, 152new and evolving architectures for telephony, 154ip telephony architectures, 155the evolving architecture of the pstn, 159architectural contrasts between ip telephony andtodayõs pstn, 161scenarios for future evolution, 162interoperation between ip telephony and the pstn, 165addressing and number portability, 167signaling and control and service creation, 168robustness, 169implications of ip telephony for telephony regulation, 170looking forward: the internet and other industry sectors, 1755implications for broad public policy177introduction, 177privacy, anonymity, and identity, 180privacy, 180anonymity, 190identity, 194authentication on the internet, 199taxation of internetbased commerce, 205universal service, 209appendix: biographies of committee members217index225the internet's coming of agecopyright national academy of sciences. all rights reserved.the internet's coming of agecopyright national academy of sciences. all rights reserved.1overviewthe rhetoric of the internet revolution surrounds us. the transformation of a research network used by a few tens of thousands of researchersinto a global communications infrastructure vital to many aspects of life iscelebrated as folk history and pointed to as the basis for a new economicorder. electronic commerce has transformed the way in which manyindividual consumers, companies, and governments buy and sell products and services. email, chat rooms, and other forms of communicationhave become common in the workplace and many homes. the internetprovides nearinstant access to a wide range of multimedia content andhas become an important channel for software distribution.where is the internet going, and how is it getting there? all indications are that the internet revolutionñgiven its impact, òrevolutionóseems the appropriate labelñis not nearly over. just during the course ofthe authoring committeeõs work, there were a number of developmentsthat are likely to have longlasting impact; salient among them are thewidening deployment of broadband residential internet service and thebeginnings of commercial deployment of mobile wireless devices thathave internet connectivity. other recent developments include the advent of new interconnection models and businesses and the widespreaduse of new content delivery mechanisms designed as overlays to theinternet. meanwhile, innovation continues in the applications and services that run over the internet, exemplified by the rise of interactive chatand games and various forms of internetbased telephony. napster andthe internet's coming of agecopyright national academy of sciences. all rights reserved.2the internetõs coming of ageits kin, which enable decentralized, peertopeer distribution of information, are challenging conventional business models and stimulating yetmore applications and new businesses. the unprecedented speed atwhich software can be distributed over the internet means that dissemination of an innovation is not limited by the production and distributionof a physical artifact.further complicating the picture is uncertainty about which developments will prove to be transient and which will have a lasting impact.while the world wide web has indeed had a great impact, mid1990shype about òpushó technologies proved unfounded, given their comparatively limited impact on either internet users or businesses. just afew years ago, experts and pundits predicted that congestion of theinternet backbone was an imminent peril, a forecast that proved incorrect,thanks to improved backbone speeds. such uncertainty means that theplanning processñfor businesses, policy makers, and others focused onthe internet and its usesñcan easily be overtaken by events and that theimportance of specific events is hard to appraise, especially in the shortterm. this uncertainty was a confounding factor in the project that culminated in this reportñtechnical issues can be resolved in multiple ways ina dynamic environment, and the consequent diversity of opinion sometimes makes it hard to reach consensus.the middle of a revolution is a difficult point from which to gaugelongterm outcomes. inherent uncertainty clashes with growing politicalpressures on policy makers to respond to apparent trends and to the sideeffects of internet activities. the actions of the businesses that provideinternet services, content, and applications fill the daily news. increasingly, these businesses are the subject of public scrutiny and governmental inquiry into the implications of their actions, which range from mergers involving internet service providers to practices surrounding personalinformation gathered from people visiting web sites. the public debateabout the internet often reveals significant gaps in understanding of theinternet, and those gaps can compromise the decisions and investmentsthat should be made in order to gain the most from what the internet hasto offer.to shed light on appropriate actions and responses to the internetrevolution, this report, written by a committee with an indepth understanding of the internetõs technologies and its core businesses, undertakesan assessment of the internet along several lines:¥reviewing the fundamental technical design principles that havehelped shape the internetõs success;¥considering the state of the art as internet technology continues tothe internet's coming of agecopyright national academy of sciences. all rights reserved.overview and recommendations3evolve, with an eye toward identifying technical issues that meritattention;¥exploring operational and management issues that require attention by those who develop, operate, and use the internet; and¥developing guiding principles for governments to use as they confront the internetrelated issues that arise in different spheres.with these tasks in mind, the committeeõs assessment focuses on fivethemes: (1) the internetõs basic design features; (2) its scalability, reliability, and robustness; (3) interconnection and openness; (4) collisions between the internet and other communicationsbased industries, particularly those that long predate the internet; and (5) broader social policyissues. this chapter covers the key points made in the main text and goeson to recommend where investment will be required to head off futureproblems and to maximize the economic and social benefits that can flowfrom the use of the internet. it concludes by articulating some guidingprinciples for those who formulate internet policy and regulation.success by designthe internet is a composite of tens of thousands of individually ownedand operated networks that are interconnected, providing the user withthe illusion that they are a single network. a customer who purchasesinternet service is actually purchasing service from a particular internetservice provider (isp) connected to this network of networks. the isp inturn enters into business arrangements for connectivity with other serviceproviders to ensure that the customerõs data can move smoothly amongthe various parts of the internet. the networks that make up the internetare composed of communications links, which carry data from one pointto another, and routers, which direct the communications flow betweenlinks and thus, ultimately, from senders to receivers. communicationslinks to users may employ different communications media, from telephone lines to cables originally deployed for use in cable television systems to satellite and other wireless circuits. internal to networks, especially larger networks, are linksñtypically optical fiber cablesñthat cancarry relatively large amounts of traffic. the largest of these links arecommonly said to make up the internetõs òbackbone,ó although this definition is not precise and even the backbone is not monolithic.the networks that compose the internet share a common architecture(how the components of the networks interrelate) and protocols (standards governing the interchange of data) that enable communicationwithin and among them. the architecture and protocols are shaped bythe internet's coming of agecopyright national academy of sciences. all rights reserved.4the internetõs coming of agefundamental design principles adopted by the early builders of theinternet, including the following:¥òhourglassó architecture. the internet is designed to operate overdifferent underlying communications technologies, including those yet tobe introduced, and to support multiple and evolving applications andservices. it does not impede or restrict particular applications (althoughusers and isps may make optimizations reflecting the requirements ofparticular applications or classes of applications). such an architectureenables people to write applications that run over it without knowingdetails about the configuration of the networks over which they run andwithout involving the network operators. this critical separation between the network technology and the higherlevel services throughwhich users actually interact with the internet can be visualized as anhourglass, in which the narrow waist represents the basic network serviceprovided by the internet and the wider regions above and below represent the applications and underlying communications technologies,respectively.¥endtoend architecture. edgebased innovation derives from anearly fundamental design decision that the internet should have an endtoend architecture. the network, which provides a communications fabric connecting the many computers at its ends, offers a very basic level ofservice, data transport, while the intelligence, the information processingneeded to provide applications, is located in or close to the devices attached to the edge of the network.¥scalability. the internetõs design enables it to support a growingamount of communicationsñgrowth in the number of users and attacheddevices and growth in the volume of communications per device and intotal, properties referred to as òscale.ó nonetheless, as is discussed below, the internet currently faces and will continue to face scaling challenges that will require significant effort by those who design and operate it.¥distributed design and decentralized control. control of the network(from the standpoint of, for instance, how data packets are routed throughthe internet) is distributed except for a few key functions, namely, theallocation of address blocks and the management of toplevel domainnames in the domain name system. no single entity (organization,corporation, or government body) controls the internet in its entirety.these design principles mean that the internet is open from the standpoint of users, service providers, and network providers, and as a resultit has been open to change in the associated industry base as well as inthe technologies they supply and use. a wide range of applications andthe internet's coming of agecopyright national academy of sciences. all rights reserved.overview and recommendations5services, some leveraging the commonality of the internet protocol (ip)and others also leveraging standards layered on top of ip, most notablyemail and the web interface, have flourished. observations about thesedesign principles have already begun to be introduced into regulatoryproceedings, and the merit of sustaining them is recognized by principals in the internet technical community, including the members of thiscommittee.sustaining the growth of the internetthe power of the internetõs basic design is reflected in its ability tosustain vigorous growth in three dimensionsñthe number of users (anddevices) connected, the amount of data that each user or device typicallytransmits, and the number of ways in which people use the network.while its rapid growth rate makes it difficult to determine the extent towhich the internet has become entwined in daily life, all indications arethat the internet plays a vital role that will only continue to expand.widely understood to be a place to òlive,ó work, and play, the internethas reached missioncritical status for many individuals, businesses, organizations, and applications.to meet the expected demands, the internet will have to continue toscale up into the foreseeable future. while the fundamental design principles have so far proven durable in the face of growth, sustainedgrowthñincluding support for faster communications and the ability formore devices to connect to the networkñwill pose challenges. but withgrowth come needs beyond simple support for more or faster connectivity. making the internet and its constituent components more reliableand robust and less vulnerable to system or component failures andattacks is also of increasing importance. a comprehensive, detailed compilation of all the challenges posed by the growth of the internet wouldeasily fill an entire report; in this report, the committee describes severalof the challenges in some detail, aiming to provide sufficient informationto allow experts and nonexperts alike to understand their essentialfeatures.scaling challengesscaling challenges at all levels, from the internetõs core to the applications that run over the internet, will require continuing, persistent attention by infrastructure operators, equipment vendors, application developers, and researchers. the research and development that underlie theinternet coreõs growth and the processes by which new protocols aredeveloped, deployed, and modified in response to shortcomings havethe internet's coming of agecopyright national academy of sciences. all rights reserved.6the internetõs coming of agegenerally been satisfactory. the challenges described below are ones thatespecially need continued or heightened attention by researchers andinternet operators.past experience with application protocols that scale poorly, in combination with an appreciation of the ease and rapidity with which newapplication protocols can come into widespread use as a consequence ofthe internetõs open architecture, gives rise to expectations of future scaling surprises. like the earlier versions of the web protocol http, newinternet applications are not necessarily well designed for widespreaduse, and some of them will encounter performance challenges as theinternet continues to grow. reengineering popular applications so theycontinue to work well as the scale of their use expands is likely to be anongoing challenge.the internetõs domain name system (dns) also faces scaling challenges. two sources of pressureñthe flat structure of much of the namesystem and the registration of millions of namesñreflect market demands. they are generating a growing load and concentrating it on asmall number of servers. possible solutions include alternative serverarchitectures that can cope better with the load or new naming architectures (in place of or on top of the dns) that spread the load over a largernumber of servers.there are also scaling challenges that are less immediate, such asthose associated with routingñthe mechanisms by which the internetpasses around information about system addresses and locations. in fact,some of the addressing issues discussed in this report stem from routingissues. the internet routing infrastructure threatens to become overwhelmed by the volume and complexity of information being distributedand perhaps by the volume of information that each router is required tomaintain. indeed, some believe that the current system that enables routers to decide where to send data packets as they move through the network will require a fundamental rethinking.scaling up the address spacethe internetõs basic protocol, ip, was designed to provide onlyroughly 4.3 billion unique identifiers, a limitation that is becoming increasingly problematic as the number of computers attached to theinternet continues to grow. the seriousness and urgency attached to apotential or actual address shortage depend largely on oneõs vantagepoint. overall, only roughly onefourth of the total pool of internet addresses is observed to be in use today, but about half of this pool has beendelegated by the regional registriesñthe handful of organizations thatassign addresses according to global regionñto isps and other organizathe internet's coming of agecopyright national academy of sciences. all rights reserved.overview and recommendations7tions. large blocks of addresses are held by organizations, includingisps, government, research and educational institutions, and businessesthat claimed them in the early days of the internet. the balance of thedelegated addresses is allocated in smaller blocks by the regional addressregistrars to isps or other organizations.unlike many internet scaling problems, where the challenge is to finda new solution, concerns about address scarcity have led to simultaneousmoves down two different paths. one response has been the creation of areplacement to the current protocol, ipv4. called ipv6, this new solutionprovides billions of billions of unique addresses. support for ipv6 hasbeen included in a number of hardware and software products and tools,and strategies supporting a transition to ipv6 have been developed. butthe costs of moving to ipv6, reflecting the large number of componentsthat would have to be modified, have dampened enthusiasm for it, and ithas seen only limited deployment to date. the low deployment rate, inturn, diminishes the incentives for switching.the other response has been the installation in many networks, including those of both customers and isps, of a workaround technologyknown as network address translation (nat), which allows individualcomputers in a group to be assigned private addresses even as they sharea single internet address. this response offers some advantages, such aseasier management of addresses on local area networks, but has significant architectural shortcomings. where true endtoend connectivity isless important, such as for isps supporting users who engage mainly inbasic web browsing, nat may prove to be an adequate workaround, atleast in the short term. on the other hand, if support is desired for peertopeer applications or users that run servers, then nat, with its trickyworkarounds, is a much less attractive solution. widespread use of natsalso brings new complications: when nats are connected to nats, basicconnectivity and the proper operation of some protocols can be inhibited.nat is also unattractive where it is desired to deploy large numbers ofinternetconnected devices with globally unique identifiers. in light ofrecent activity and in anticipation of continued growth in the mobileinternet device market, where it is projected that the number of deviceswill exceed the available address space, there has been renewed interestin ipv6. indeed, the developers of socalled thirdgeneration (3g) wireless services have, at this stage, committed to using ipv6.at present, many concerns stem less from a shortage of addressesthan from the cost or hassle associated with obtaining an allocation in aclimate where regional address registrars and isps are motivated to befrugal as they hand out addresses. address assignments reflect needsthat the requesting organization has been able to substantiate on the basisof current use or credible projections that it can make of future needs;the internet's coming of agecopyright national academy of sciences. all rights reserved.8the internetõs coming of agethey also reflect the overall availability of addresses at the time that theassignment is made. thus, organizations and regions that have alreadybeen allocated greater numbers of internet addresses and thus do not facea looming shortage are less likely to find ipv6 attractive, particularly inthe short run. in contrast, organizations that are building new networksand seeking to greatly expand the number of users (and thus ip addresses)face costs (fees and effort expended in justifying their requests) to obtainan allocation from a regional address registry or their isp, and they aremore likely to advocate ipv6 deployment. disparities among organizations and geographical regions in address allocation, which tend to favorthose who made earliest use of the internet, also mean that address scarcity may be perceived as an equity issue associated with perceived disparities in control over the internet.while there has been no crisis thus far, there is still considerable riskassociated with exhaustion of the ipv4 address space. in the short term,the costs and problems associated with address scarcity will not be imposed uniformly. if there is no migration to ipv6, address scarcity will bea serious problem for a subset of internet users in the short term and amore pervasive problem in the long term. the number of computersattached to the internet can be expected to continue to grow, reflectingboth more users and more devices per user. this growth will be mostpronounced and will come soonest in regions and countries where theinternet has made the fewest inroads today, where the number of potential users is large and penetration is expected to be great, and whereproviders are seeking to deploy very large numbers of devices with fullinternet connectivity, such as would be the case if there were an explosionin the development and sales of internetcapable appliances for the homeand/or the 3g mobile phones discussed above. a key question is justhow far off the òlong termó is, when the impacts of scarcity will be widelyand deeply felt. the answer depends on many factors that are difficult toproject. and even with a substantial commitment to an eventual switchover to ipv6, the use of nat and natlike ipv4toipv6 translators willadversely affect the endtoend transparency of the internet in the meantime.robustness and reliabilitythere is widespread acknowledgment that it is important to make theinternet as a wholeñas well as its constituent networks and individualcomponentsñmore robust and reliable. reactions to a series of distributed denialofservice attacks in 2000 illustrate the extent to which problems are viewed with concern by government officials and the public.some challenges, including the need to fix known problems, are wellthe internet's coming of agecopyright national academy of sciences. all rights reserved.overview and recommendations9understood today, but more information is needed to comprehend thefull spectrum of risks and vulnerabilities. because the internet is composed of thousands of distinct networks run by different isps and because isps typically do not publicly report outages, much less their cause,little is known about the primary causes of internet failures. indeed, littleis known about how often there are major failures that affect a largenumber of customers. in the absence of this sort of information, it is verydifficult to start a program to improve the internetõs robustness and reliability.even with better information on risks and vulnerabilities, a betterunderstanding of the underlying technologies for reliable and robust networks is needed to design and implement fixes, especially in the face ofless predictable applications and traffic running over the internet. abright spot is that those who develop security technologies and practiceshave learned much about how the internetõs components can be attackedand have been working with vigor on techniques to make the internet lessvulnerable to attackers, efforts that can also suggest ways of improvingthe internetõs robustness to inadvertent failures. a number of technologies have been developed to improve robustnessñto secure internet systems, detect and prevent intrusion, and authenticate transactions. implementation of these measures, however, has tended to lag behind the stateof the art, and an array of management actions will be needed to betteralign practices with the technology.quality of servicethe internetõs besteffort quality of service (qos) has been successfulin supporting a wide range of applications running over the internet. thedebate over whether mechanisms supporting other forms of qos areneeded is a longstanding one within the internet community. it hasshifted from an original focus on mechanisms that would support multimedia applications over the internet to mechanisms that would support abroader spectrum of potential uses, from enhancing the performance ofparticular classes of applications over constrained network links to providing isps with mechanisms for valuestratifying their customers. thereis significant disagreement among experts (including the experts on thiscommittee) on how effective qos mechanisms would be and on the relative priorities that should be attached to, on the one hand, investing inadditional bandwidth and, on the other, deploying qos mechanisms. akey feature of this debate is differing opinion on the extent to which arising tide of capacity in the internet will alleviate most performanceproblems. contributing to the debate is incomplete knowledge of thecauses of performance problems within the besteffort network and thethe internet's coming of agecopyright national academy of sciences. all rights reserved.10the internetõs coming of ageactual benefits that would be obtained by deploying various qos mechanisms within operational networks. another open issue is whether thereis a role for internet qos on links that are inherently constrained (e.g.,wireless) or on links where adding capacity may be much more expensivethan adding capacity within the internet backbone (e.g., the links betweenlocal area networks or residences and isps).service quality is a weaklink phenomenon. providing endtoendqos requires isps to agree as a group on multiple technical and economicparameters, including technical standards for signaling, the semantics ofhow to classify traffic and what priorities the categories should be assigned, and the addition of qos considerations to their interconnectionbusiness agreements. the reality of todayõs internet is that endtoendenhancement of qos is a dim prospect. it may be that localized deployment of qos, such as on the links between a customerõs local area network and its isp, is a useful alternative to endtoend qos, but the effectiveness of this approach and the circumstances under which it wouldprove useful are both poorly understood, as is whether such piecemealdeployment could contribute to a balkanization of the internet.qos deployment has also been the subject of interest and speculationby outside observers. one view is that qos would be an enabler of newapplications and business models while another is that the introductionof qos capabilities into the internet would undermine the equal treatment of all communications across the network, irrespective of source ordestination. mechanisms that enable disparate treatment of customerinternet traffic have led to concerns that they could be used to providepreferential support for particular customers or content providers (e.g.,those having business relationships with the isp). what users actuallyexperience will depend on multiple factors: what the technology makespossible, the design of marketing plans, preferences that customers express, and what capabilities isps opt to implement in their networksñwhich will depend in part on their determination of how effective particular qos mechanisms would be.additional insights into the role of qos mechanisms in the internetwill come through several avenues: better understanding of the factorsthat contribute to network performance, including the limits to performance that can be obtained using besteffort service; better understanding of the effectiveness of qos approaches in particular circumstances;and greater experience with qos in operational settings.keeping the internet interconnected and openone of the internetõs hallmarks has been its openness. this opennessappears in a variety of distinct although related ways, including opennessthe internet's coming of agecopyright national academy of sciences. all rights reserved.overview and recommendations11to new entrants and openness to innovation. keeping the internet openhas a number of goals, including continuing innovation in internet service, preserving access to the full set of content and services that are madeavailable over the internet, and fostering competition as a means of ensuring innovation, access, and affordability.access to the local loopthe first key openness issue is access to facilities in the local loop (thefinal communications hop into the premises), especially perceived advantages for those who already own linksñtoday, the incumbent local telecommunications carriers and cable operators. in the local loop, opennessissues are frequently linked to the term òopen access,ó which refers to theability of residential or smalloffice customers to have a choice of alternative isps and to have access to content and services that are made available over the internet even when they are not supported directly by thecustomerõs isp (i.e., when there is no business arrangement between theisp and the provider of the content or service). because the local loop isthe point of entry for many internet users, outcomes here can have significant consequences for the shape of the internet as a whole. it is unclearwhether issues of open access will be resolved in the near term throughregulatory action (e.g., new unbundling requirements), legal decisions,actions by industry itself (perhaps in response to consumer pressure), orconsumer choice as a result of facilitiesbased competition, or whetherthey will become persistent features of the internet policy debate. another computer science and telecommunications board body, the committee on broadband lastmile technologies, is currently investigatingthese and other issues related to broadband services for homes and smalloffices, so they are not considered in detail here. however, a number ofpoints in this report are likely to help inform thinking about the issue,including the discussions of what constitutes transparent, open internetservice; related trends in the isp business; and the likely roles for qostechnologies on the internet.interconnectionthe second key openness issue is the nature of the interconnectionagreements whereby many independently operated networks are interlinked to create the internet. to become an isp, a new provider must haveone or more agreements with other isps to ensure that its customers cancommunicate with the customers of all the other isps. interconnectionhas three dimensionsñphysical (pointtopoint or connection at a publicexchange), logical (transit or peering routing), and financial (generallythe internet's coming of agecopyright national academy of sciences. all rights reserved.12the internetõs coming of ageeither a feefortransit or peertopeer barter arrangement). interconnection involves costs to the parties to the agreement and requires bilateralagreement on financial terms. in a transit agreement, one (typicallysmaller) isp pays another (typically larger) isp to accept and arrangedelivery for all data that leave the first ispõs network; in a peering agreement, two (typically similar size) isps agree that the value and costs ofinterconnecting to each other are roughly equal and they need not exchange payments. unlike transit agreements, peering agreements onlyprovide for the transport of data packets between one ispõs customersand the customers of another isp and do not provide for the transport ofcommunications across the ispõs network.clearly this market model has risks. it assumes a reasonably competitive environment, where competition among isps keeps transit agreement charges reasonable, where no one isp is so dominant that it canrefuse offers to peer with other networks and thus force all the other ispsto pay for access to its customers, and where there is not pervasive vertical integration of backbone isp and content and service businesses. whilesuch threats are possible, none have emerged. the number of socalledòtier 1ó internet service providers, distinguished by both size and theirpeer interconnections with one another, is small, but they have, at leastthus far, provided customers with choices. of the isps that provide service to consumers and small businesses, one ispñaolñis clearly farlarger than the others; however, contrary to analyst predictions, the number of customerfocused isps has not been shrinking precipitously, suggesting that the market is not closed.while concerns have long been expressed about interconnection inthe internet, interconnection arrangements are continuing to evolve inways that support the growing base of internet users and their changingneeds. there have been several areas of innovation in the nature ofinterconnection that provide alternatives to the conventional binary choicebetween attaining peer status or paying as a transit customer. tier 1providers have experimented with new forms of interconnection arrangements that might be thought of as somewhere between pure peering(where two isps agree to exchange traffic on a settlementfree basis) andtransit (where the customer isp pays fees to another isp to handle itstraffic). new entrants have also developed business models for interconnection. for example, the company internap has established connections to top isps, including a number of the tier 1 providers, using aconnection arrangement that lies halfway between peering and transit. itthen sells the resulting internet access to the isps and large businessesthat are its customers. other businesses have entered the market to provide more sophisticated revenue models than the traditional peeringmodel in which isps set revenueneutral boundaries at the internetõs centhe internet's coming of agecopyright national academy of sciences. all rights reserved.overview and recommendations13ter, where the major tier 1 isps connect, and in which transit service issold to downstream providers on the basis of rough measures. for example, akamaiõs content delivery model places servers within the networks of isps, which enables new financial arrangements such as payments by the content producer to the isps that serve enduser customers.another important observation with regard to interconnection is thatthe nature of isps is changing. the current interconnection model assumes that isps are all in the business of providing carriage of customertraffic. recently, however, the isp market has become far more complex,with the development of web and businesshosting isps and specializedisps providing only certain internet services (e.g., instant message or telephony) and not others. there are also demands for isps to provide multiple levels of service (e.g., better service for customers who pay more)that span the networks of multiple isps. it is not clear that the currenttransit agreements, which are designed to assess charges for the deliveryof data packets across a boundary, are consistent with the future businessneeds of these isps, suggesting that further evolution of interconnectionin the internet will be necessary.innovation and transparencythe internet is built on a set of open standards and on a process thatseeks to encourage the development of new open standards as needs fornew functionality arise. (the term òopen standardó has a variety of meaningsñas used here, an open standard is one that is made easily availableto all interested parties and owned and controlled by a noncommercialparty such that changes to the standard are not made capriciously.) companies can combine standard protocols to create new services and applications, and, freed from the need to develop basic technologies to get datafrom one end of the network to another, they can focus their energies ondeveloping new protocols that build on the existing system to offer newservices.the internet has been served well by an insistence that there is oftenmore than one right answer to a question. although there may be acommon standard, there are frequently several independent implementations of it, keeping any one company from cornering the market in goodtechnology. indeed, whenever one company has come close to a monopolistic position, the traditional internet community has been critical ofthe results, citing the potential for inhibiting innovation. the concern isnot the addition of features that can coexist with standard protocols butthe emergence of a monopolistic position that would eliminate the benefits that accrue from having independent implementations available frommultiple vendors. the internet has also been characterized by both stathe internet's coming of agecopyright national academy of sciences. all rights reserved.14the internetõs coming of agebility (standard protocols are modified only when there is consensus thatchanges are necessary) and adaptability (protocols and practices are updated to provide new capabilities and meet new challenges).a significant feature of the internet is that it becomes more valuableto each user as the size of the network grows, making it possible for asmall advantage in market share to snowball into a much larger marketshare and a very large economic benefit. this small advantage is oftenassociated with being first, but sometimes a sufficiently large investmentof resources (or some other circumstance) allows a newcomer to trumpthe original innovators. on the internet the ease and the negligible cost ofdistributing software through the network amplify these effects. theseproperties suggest a pattern of highly concentrated markets and marketleaders who greatly outdistance their competitors but remain, in theirturn, vulnerable to a motivated competitor with a better product.in this environment, companies will seek to differentiate their products on the basis of features other than what the standard protocols themselves provide or on the basis of content or services. but if products arebuilt on open standards, there is always a chance that a new company willdevelop a competing product based on the same open standards (or on anew standard that provides the same services) that cuts into a companyõsexisting market. the tippy quality noted above means that in an openstandards market, a company must always worry that the market willsuddenly tip away from it. thus it is not surprising that there are periodicattempts by major internetrelated companies to protect valuable products by declining to work on open standards or by making proprietaryextensions to open standards. nonetheless, as evidenced by the entire setof industries that have been built on top of open internet standards, thereis considerable value in the continued development and use of open standards for key internet functionality.another issue closely related to openness is preservation of theinternetõs endtoend transparency, whereby the networks that make upthe internet do not tamper with or restrict data in flight between computers attached to it. with suitable software running at each end and noknowledge other than each otherõs internet address, any two devices connected to the internet are, in principle, able to enter into any type ofcommunication, limited only by the existence of the requisite networkcapacity and sufficiently low or at least predictable latency (delay) tosupport the application. as a result, someone who develops a new application can place it on the internet without any changes needing to bemade to the internet to make the application work. this is highly advantageous; in a world with thousands of isps, coordinating changes to thenetwork for every new application would be a nightmare.in the realworld internet, a number of tradeoffs affecting transparthe internet's coming of agecopyright national academy of sciences. all rights reserved.overview and recommendations15ency are made. one is pragmatic actions taken in response to operationalconsiderations (e.g., deployment of nat to deal with address scarcity)that limit transparency. transparency is also traded off against otherattributes of internet service. for example, organizations and individualsmay install firewalls in order to better protect their network against unwelcome types of traffic (protection against attacks, for instance, or againstuse that is considered inappropriate) or they may take steps to enhancethe performance of a network by controlling the use of applications thatplace particular demands on network resources. however, the internetõsdesign places limits on efforts by governments, isps, organizations, andnetwork managers to filter content or applications. attempts to blockparticular types of content or applications can trigger an escalating battlein which the authors of internet applications make use of a variety oftechniques to allow application communications to masquerade as othertypes of traffic. also, widespread use of iplayer encryption (e.g., theipsec protocol) would preclude most blocking of applications or content.the likely consequence is that any user or isp that seeks to fully blockcertain types of traffic can rapidly find itself forced to block any trafficthat it does not know, a priori, to be acceptable (i.e., traffic would have tobe assumed to be undesirable unless proven otherwise). of course, sucha practice would be antithetical to openness. the internetõs openness isbest preserved when users are aware of the tradeoffs and able to striketheir own balance with respect to them.collisions between existing industries andemerging internet industries:telephony as a case studythe provision of voice services over the internet is an example of thekinds of industry and policy shocks that emanate from the collisions between the internet and traditional industries. telephony is a particularlysalient example because its practices and regulatory approaches aredeeply rooted at both the federal and state level, and as such it was selected as the industrypolicy nexus to be examined in this report.some attention to definitions is required. the terms òip telephonyóand òvoice over ipó are both used in this report to describe services thatemploy the internetõs underlying technology to provide voice telecommunications. ip telephony is a broad label covering a diverse set of architectures, service providers and transport media, enduser equipment, local access technology, and interfaces and gateways between ip and publicswitched network elements. not all forms of ipbased telephony makeuse of the public internet; the term òinternet telephonyó refers to theparticular case of using the public internet to carry telephone calls.the internet's coming of agecopyright national academy of sciences. all rights reserved.16the internetõs coming of agewhile ip telephony holds only a small share of the total telephonymarket today, it is growing rapidly. as ip telephony becomes more widespread and reaches more consumers, voice, data, and multimedia offerings will increasingly be linked. telephony is now provided primarily inone of two forms: (1) the conventional managed form, where a singleentity controls both the service and the communications infrastructure(including the public switched telephone network, pstn) or (2) anunmanaged form of ip telephony, such as pure internet telephony. in thefuture, however, ip telephony services will also include amalgams ofmanaged and unmanaged networks. as they mature, these new serviceswill provide a flexible and robust service creation platform for a range ofnew voice and multimedia services.in contrast to the pstn, ip telephony permits the data packets carrying voice to be transported by a different entity from the one providingapplication services (e.g., call agents and directory services). also, likeother packet data, traffic associated with a phone call over the internetwill transit one, two, or many providers depending on which networksthe calling parties are attached to and how the networks are interconnected. ipbased telephony also does not necessarily provide the samefunctionality as the pstn. for example, once a call has been set up, thedata stream associated with it may flow directly between the two endpoints rather than through a central facility. the generalpurpose natureof the internet also means that ip telephony can introduce novel servicesand features that do not parallel those offered by classic telephony.such differences signal impending conflicts between the new servicesthat ip telephony enables (and the companies that provide those services)and the practices and assumptions of the current regulatory system. asip telephony gains market share, its amenability to innovation and itsapparent cost advantage are provoking calls for voice over ip to be subjectto regulation like that to which existing pstn services are subject. at thesame time, the inconsistencies between the internet technology and theregulatory framework will frustrate this goal. for instance, in internettelephony there are no meaningful distinctions between local and interexchange carriers. furthermore, because the functions of moving voicepackets between parties and control functions such as call setup are completely separable, the regulatory assumptions that control and transportare necessarily performed by the same party are not valid.in contrast to the internet, the pstn evolved in a highly regulatedenvironment over much of the past century. design and regulation areclosely coupled; todayõs regulations reflect the technologies and designchoices underlying the pstn, but at the same time the regulatory environment has itself helped shape the pstnõs architecture. to the extentthat internet telephony becomes a significant force in voice services, dethe internet's coming of agecopyright national academy of sciences. all rights reserved.overview and recommendations17sign differences will pressure the existing regulatory regime and the existing players. there are difficult choices to be made on how and whetherto apply existing definitions and rules to new technologies and services.the inconsistencies between the architecture assumed in the currentregulatory regime governing the pstn and the architectures for ip telephony suggest that pstn regulation should not be transferred as is simply because the new services appear to constitute telecommunications.the committee recognizes that there are pressures for regulation preciselybecause of that appearance. acknowledging (1) that the processes fordeveloping the relevant laws and the regulations that implement thoselaws may be separate, (2) the uncertain prospects for major legal reconsideration (given the recentness of the 1996 telecommunications act), and(3) the awkward fit of regulations designed for the pstn to internet telephony, both the legal frameworkñthe rationale for regulationñand thedesign of specific regulations may need to be reconsidered. the analysisshould start by reconsidering the old rationales for regulation rather thanby looking for ways of accommodating existing regulation. as the sametechnologies and service offerings are introduced in the pstn, the challenge posed by mismatches between regulation and technology will nolonger be confined to new players.in addition to basic telephony service, the pstn provides some services, such as 911 emergency services, that are not necessarily provided(or readily realizable) in todayõs ip telephony offerings. the pstn also ismandated to provide certain facilities for interconnection among networksand to provide universal service. while legislators, regulators, and consumers are sure to expect certain functions now provided by the pstn tobe maintained, it is currently unclear how those functions will be implemented and maintained as pstn and ip telephony evolve. also, as theshape of telephony evolves in an internet environment, so, too, will expectations and requirements for these capabilities. a 911 emergency service in the future might not be limited to a simple voice channel; it could,for instance, be implemented with text messaging or a web page. norwill lifecritical services necessarily be confined to simply summoningassistance.the internet and broad social policyby providing a network with lower costs and increased functionality,the internet represents a disruptive force across domains that have separate bodies of wellestablished law. it will surely continue to raise manysocietal issues and questions about the need for additional governmentallaws and regulations. the committeeõs examination of ip telephony andthe pstn provides one important illustration of how an existing industrythe internet's coming of agecopyright national academy of sciences. all rights reserved.18the internetõs coming of ageand its associated policy framework come into collision with competingnew internetbased industries. the committee also explored several otherplaces where the internet is challenging social policies. many of the concerns existed in other contexts before the emergence of the internet, butthe internet, by virtue of its support for comparatively easy informationaccess and distribution and the relative speed with which new applications of it can be developed and deployed, amplifies these concerns. thesubset of issues explored by the committeeñprivacy, anonymity, andidentity; authentication; taxation of commerce transacted over theinternet; and universal serviceñis not comprehensive (nor could it be soin a study of this size). rather, these issues were chosen as significantpoints of interaction between the internet and the broader society.recommendationsthe internetõs coming of age has been marked by increased attentionacross the board. the businesses and organizations that design, build,and operate the internetõs constituent networks are working to shape thatnetwork to meet the demands of users. the work being done on theinside has been changing in character, reflecting the internetõs increasingimportance to customers, growth in the number and kinds of applications, and changes in the nature of the business itself (e.g., while they areinterdependent, isps are also competitors). accompanying these effortsare increased attention and heightened expectations from the outsideñindividuals, organizations, corporations, and government bodiesñthatreflect the importance of the internet as an infrastructure for society. theassimilation of the internet into society and the economy involves a growing role for a second business community in addition to the businessesthat design, build, and operate networks: these are the businesses thatprovide content, applications, and services that run over the internet. insome cases, of course, the same entity may be involved in both kinds ofbusiness, but overall, this second community tends to be distinct, larger,and more differentiated, and it raises a wider range of concerns. however, although much of the attention now being paid to the internet relates to the behavior (as regards, for example, online privacy and otherconsumer protection issues) of these businesses that leverage the internet,the committee concentrates its recommendations on the businesses thatdesign, build, and operate the internet.the committeeõs overview of social policy concerns completes thepicture by illuminating actions that leverage the internet and that, directly or through policy responses, may influence future decisions abouthow the internet develops. sound recommendations that respond to theparticulars of these social policy concernsñunlike the general principlesthe internet's coming of agecopyright national academy of sciences. all rights reserved.overview and recommendations19articulated belowñwould require further examination of the contexts andbehaviors associated with each concern.the principal conclusion of the committee, which underlies the discussionbelow, is that the internet is fundamentally healthy and that most of the problemsand issues discussed in this report can be addressed and solved by evolutionarychanges within the internetõs current architectural framework and associatedprocesses. multiple actorsñthe research community, industry, government, and the users themselvesñhave important roles to play in ensuringthe internetõs continued wellbeing and progress. the recommendationsprovided here cover both the general and the specific, reflecting overallprinciples as well as more targeted opportunities.the technology baseexhortations about the importance of research and development onscaling, reliability, and the like are not new, but the committee makesrecommendations in these areas to underscore their importance at thispoint in time. research and development have enabled the internet tobecome a mainstream infrastructure, but the job is far from done: use ofthe internet and dependence on it can be expected to grow. staying onthe internet growth curve, so frequently projected by pundits and analysts and expected by the internetõs users, will require continued, sustained effort in many places. some of the challenges are shorter term:these the research community and industry infrastructure seem wellplaced to solve, as they have in the past, through sustained effort andincremental enhancements. others are longerterm, enduring challengesand will need more fundamental breakthroughs. many research advanceswould provide benefits to all who operate and use the internet, not just asingle player. this outcome argues for using public funds to supportsuch work even in the face of considerable private investment in theinternet, particularly where selfinterest or nearterm gains are insufficient motivators for industry investment.research and development that address scaling challenges and enhance reliability and robustness should continue to receive supportfrom both industry and federal research funding agencies. priorityscaling issues include the continuing need to improve the scalability ofapplications deployed over the internet, scaling issues associated with thedns infrastructure, and longterm scaling issues related to addressingand routing in the internet. key research and development areas relatedto reliability and robustness include (1) the development of improvedtrust models that better describe the business relationships of organizations and what sessions and relationships they authorize; (2) research onthe internet's coming of agecopyright national academy of sciences. all rights reserved.20the internetõs coming of agetechnologies to cope with attacksñsuch as technologies for intrusion detection and isolation, including capabilities that would provide faster andmore focused isolation of attacks in a manner that scales to the internetõsincreasing speeds and complexity; (3) the design of mechanisms and protocols that better protect one part of the internet from attacks and operational errors in other parts and from damage to components withoutdisrupting the basic requirement for global connectivity; and (4) fast linkand node failure detection and healing mechanisms as well as interdomainrouting protocols that provide greater recovery speed.researchers, research funders, and network operators should worktogether to find opportunities that would allow more network researchto be done in realistic operational settings. a common theme across thetechnical challenges discussed in this report is that they have to do withproperties of the internet as a system, including how it scales or how ithandles failures or deliberate attack. these challenges are hard to studyin smallscale systems, which are what researchers generally have to workwith, and hard to study through simulation, because both theory andmodels pertaining to the operation of very large networks such as theinternet are weak. the need for researchers to have better access to realworld artifacts has been noted in earlier studies.1 the payoff from betteraccess to internet networks would be an improved understanding of network behavior, particularly behavior related to large scale and high congestion, that could lead to insights that would enable improvements inoperational networks. for example, research aimed at better understanding where and how qualityofservice mechanisms would best benefit aparticular class of applications needs to be done on a network with realistic congestion and cannot be done through simulation unless one hasgood models of how a congested network behaves. implementing thisrecommendation will require overcoming the reluctance of isps to maketheir networks available because they fear that researchers may inducemalfunctions or disclose proprietary information when they òplayaroundó in isp backbones. it will also require attention to the lag incapabilities between research instrumentation and the equipment foundin highcapacity isp networks. these inhibiting factors are not confinedto commercial networks operated by isps; they also arise in research net1computer science and telecommunications board (cstb), national research council(nrc). 2000. making it better. washington, d.c.: national academy press; computerscience and telecommunications board (cstb), national research council (nrc). 1994.academic careers for experimental computer scientists and engineers. washington, d.c.: national academy press.the internet's coming of agecopyright national academy of sciences. all rights reserved.overview and recommendations21works that are used in operational modes, such as for applicationsresearch.industry and researchers should continue to investigate the economics of interconnection and technologies to support interconnection.improved understanding of the economics that underlie interconnectionin the internet may be useful for better understanding how the internetõsinterconnection arrangements are evolving and may lead to new modelsthat improve the overall interconnection of the internet or that help address concerns such as barriers to entry. key topics include how to bestapproach the value relationships that exist across the internet; identifyingeconomic alternatives beyond simple peering and transit; and exploringthe organizational dimension of interconnection and openness issues, including the implications for industry structure and performance. at thesame time, industry should continue to explore new business models forinterconnection and for fostering a commercial environment that encourages competition and innovation. there are also challenges on the technical side: research on routing could provide better control and protectionof interconnecting providers, thus increasing the range of possible interconnection alternatives available to isps.government, industry, and other stakeholders should continue tofoster the development of open standards for the internet. each internetplayer will be tempted to diverge from the common standard if it lookslike it might be able to capture the entire market (or a large portion of it)for itself. however, a common, open standard maximizes overall socialwelfare as a result of the network externalities obtained from the largermarket. when competent open standards are made available, they can beattractive in the marketplace and may win out over proprietary ones. thegovernmentõs role in supporting open standards for the internet has notbeen, and should not be, to directly set or influence standards. rather, itsrole should be to provide funding for the networking research community, which has led to both innovative networking ideas as well as specifictechnologies that can be translated into new open standards.where there are societal expectations associated with particular existing industries, such as expectations for 911 emergency service as partof telephony, analogous capabilities for the internet should be developed and demonstrated through research and experimentation in themarketplace rather than by mandating particular technical solutions.whether, when, and how regulation is introduced can affect innovationin this area because the underlying telephony technologies and serviceofferings are themselves evolving rapidly and have yet to prove themthe internet's coming of agecopyright national academy of sciences. all rights reserved.22the internetõs coming of ageselves in the marketplace. for example, the interoperation of pstn andinternet telephony systems raises longerterm questions: how shouldnumber portability be implemented? how can customers be providedwith numbering and naming? both research and marketbased experimentation will be important in developing the best and most efficientmeans of implementing telephony services.designers and operatorsthe internetõs developers and operators have devised technologiesand processes that will do much to keep the internet healthy and growing. however, improvements in scalability, reliability, and robustnesswill involve more than technical advances per se; questions of implementation are also important and in many cases require collective action bymany thousands of entities. business imperatives generally motivate individual organizations and companies to act in ways that promote theirindividual business success, but such actions do not necessarily providebroadbased, global benefits for the internet as a whole. indeed, there aremany places where longterm, overall benefits for the internet as a wholeare traded off for shorterterm, local benefits to particular subsets ofinternet users and operators. for example, while the internet industryand its customers stand to gain in the long term from a shift to ipv6, thecosts for individual organizations will, at least in the short term, probablyoutweigh the benefits they themselves obtain. another example is thescalability of applications: applications whose deployment adversely affects the performance experienced by all internet users may, nonetheless,provide local benefits (because a short time to market can yield moreimmediate returns) and result in the capture of a greater market share(because the internet is what economists call a tippy market). one possible driver of collective action is the prospect of governmental regulatoryintervention. but the extent to which enlightened selfinterest can be amotivator will depend on the specific issues and circumstances.several private, nonprofit organizations play critical roles with respect to the internet, including the principal standards bodies (the internetengineering task force and the internet architecture board); organizations that deal with operational issues (e.g., the north american networkoperators group); and the internet corporation for assigned names andnumbers (icann), which has assumed overall responsibility for managing the internetõs addresses and names. the absence here of recommendations for these organizations should not be taken as an indication thatthe actions and evolution of these organizations are not important. thecommitteeõs lack of commentary on them should not be read as eithercritical of or supportive of either side in debates such as that surroundingthe internet's coming of agecopyright national academy of sciences. all rights reserved.overview and recommendations23the role of icann. while the committee has not examined these issues indepth, it believes that these institutions make important contributions tothe operation and development of the internet, notwithstanding theunstable circumstances.2 because the committeeõs membership includesseveral individuals who work closely with these organizations, the committee decided not to issue conclusions related to the specifics of theorganizationõs work but urges continued, close attention by internet operators, users, and policy makers alike.as a first step to improving robustness, the isp industry shoulddevelop an approach for reporting outages and make the informationavailable for studying the root cause of failures and identifying actionsand technologies that would improve the internetõs robustness. whileanecdotal reports of failures are available from both the popular pressand various internet community forums, these sources generally lack sufficient detail and are not systematically collected, making it hard to assessinternet reliability and robustness trends or conduct rootcause analysis.the availability of these data will make it possible to properly analyze therobustness of the internet, identify key related issues, and provide theinformation needed for research into how to make the internet more robust. the committee recognizes that there is currently no consensus onwhat data ought to be reported and that there would be strong resistanceto a mandated reporting of irrelevant information. it also anticipates thatsome form of reporting of outages is likely to become a requirement, atleast in the united states, which suggests that the industry should workto devise a program that represents a balance of interests as an alternativeto the imposition of governmentdeveloped reporting standards; the voluntary program initiated by the network reliability and interoperabilitycouncil is a first step. cooperative consideration of an approach forreporting outages and failures should determine what information oughtto be collected as well as to whom it should be reported. since the primary purpose of collecting this information is to inform industry activities as well as research aimed at improving reliability and robustness, itwill not be necessary that all of the information be reported publiclyñtheoperators themselves and the research community would be the mainbeneficiaries of some of the detailed information. a process for gatheringsystematic data on failures should be understood to be distinct from independent monitoring of isp performance, which is best performed by independent organizations that gather data on behalf of consumers.2another cstb committee is expected shortly to begin an examination of issues surrounding the assignment of domain names in the domain name system such as conflictsbetween dns names and trademarks.the internet's coming of agecopyright national academy of sciences. all rights reserved.24the internetõs coming of ageinternet service providers, content and service providers, and usersshould continue to adopt technologies and practices that improve thereliability and robustness of the internet as a whole. the internetõs trustmodel distributes responsibility for robustness across many actors, including isps, network operators, and end users, placing responsibility oneach to adopt the best practices and technologies. also, the internetõscomposite nature and international scope mean that no one can imposeoverall requirements for such things as reporting problems, minimumoperational standards, or controls on malicious actions. this limitationmakes it even more important to develop industry agreements addressing robustness that are international in scope, and it underscores the importance of developing technical mechanisms that permit one piece of theinternet to protect itself from another.nats (and the somewhat natlike ipv4toipv6 translators) are anecessary shortterm measure but should not substitute for a longtermtransition to ipv6. investment in the development and deployment ofipv6 technology, along with promotion of the longterm benefits ofipv6 for customers and isps alike, should be continued. in addition,there should be a concerted effort to address other pressing issues thatipv6 does not now completely address. ipv6 alone does not resolveother, related issues faced by the internet. for example, while it doesprovide some aids for automatic configuration, it does not adequatelysimplify the management of internal networks interfaced to the internet.nor does it solve the scaling problems mentioned above with respect tothe computational complexity of updating routing tables as the numberof addresses increases. also, while it includes stronger authenticationand confidentiality safeguards than ipv4, it does not respond to othersecurity considerations that may be critical to minimizing vulnerability toattack.decisions made by industry, government, and consumers shouldall take into account the significant longterm benefits of open, transparent ip service. the preservation of open ip service would have anumber of benefits for both isps and customers. because of its criticalrole in the continued dynamism and growth of the internet, governmentshould include considerations of openness in its inquires relating to theinternet and should favor policy decisions that are consistent with maintaining open ip service. government also has a role to play in conveningdialog and supporting research about openness issues. by the same token, concerns about the vertical integration of the data transport andcontent businesses and about content control, as seen in recent debatesabout access to cable broadband internet systems, could be eased if ispsthe internet's coming of agecopyright national academy of sciences. all rights reserved.overview and recommendations25committed to providing their customers with open ip service. from thisstandpoint, the continued delivery of open ip service would be an enlightened move in the longterm interest of the industry.isps should make public their policies for filtering or prioritizingcustomer ip traffic. many filtering and traffic prioritization policies workto the mutual benefit of both the provider and the customer. but giventheir subjectivity, all would benefit from an environment in which suchpolicies are publicly disclosed, allowing customers to understand the nature of service offerings and reducing the likelihood that isps will beperceived as manipulating the nature of their servicesñsuch as favoringtheir own contentñbehind the scenes, against the interests of consumers.further, such disclosure might foster a market in which isps compete onthe terms of their policies and in which a particular isp offers differentservice options so as to better meet the needs of its customers. also, thosewho monitor the industry or rate the quality of isps could use such information to inform consumers about the advantages and disadvantages ofthe various isp service offerings.government policy responsesby lowering the cost of communications and increasing the functionality and utility of the communications infrastructure, the internet hasenabled significant changes. experiencing a revolution on internet time isextraordinarily challenging. changes come quickly and unpredictably.fads appear suddenly and fade away just as rapidly. nor is the speed ofevents the only challenge. the distributed nature of the internet, with itsthousands of isps and software vendors and its millions of individualusers all contributing to the overall shape of the network, makes it verydifficult to understand what is happening. the technology is changingswiftly, and in many cases the perceived problem may fix itself or evolveinto an entirely different problem. in such a dynamic environment, flexibility is essential and regulatory caution is a virtue. this should be aperiod of watchful waiting.the present policy of nonregulation of the internet should be accompanied by close monitoring of the internetõs structures and operation by government, the internet industry, and internet users to ascertain enduring trends and identify what problems, if any, are due topersistentñas opposed to transientñphenomena. while this recommendation is intended to apply across the structure and operation of theinternet as a whole, the committee sees several important places where itshould be applied:the internet's coming of agecopyright national academy of sciences. all rights reserved.26the internetõs coming of age¥absent evidence of abusive control over internet interconnection,regulation here would be premature. however, as interconnection is soimportant to the health of the internet, all players, including government,should continue to monitor the evolution of interconnection carefully toensure that it remains competitive and innovative.¥in view of the importance of the various organizations that help tocoordinate various aspects of the internet, as well as the uncertain impacton them of the constellation of changing conditions relating to the internet,the activities and operations of these coordinating organizations meritcontinued close attention.¥regulation of ip telephony at this point in time would be premature because it is a newly emerging alternative to traditional telephonythat is evolving along multiple paths; further maturation and observationare needed to yield a realistic sense of the shape of the markets and theindustry. any regulation applied in the future to ip telephony should betechnologyneutral and minimally constrain innovation. in order to notinhibit growth of and innovation in ip telephony and internet services,regulatory intervention associated with ip telephony should take intoaccount the different architecture of the internet and the diverse set oftelephony technologies and designs being developed and deployed foruse on the internet and pstn. a technologyneutral approach wouldallow this diverse set of technologies and designs to be accommodated,particularly during this period of experimentation and rapid change, andwould permit emerging technologies to continue to evolve. a similarapproach should be applied to other internetbased alternatives to existing industries.monitoring should be supported by a broadbased research effort(including research in social science) to promote objective, methodologically sound measurements and analysis and should be complemented by efforts to understand what might one day be potential triggers for intervention. examples of technical information that wouldinform decision making include information on the growth of the internet(in terms of users, traffic, and range of uses), its reliability, and its socioeconomic impacts. federal efforts to collect technical and socioeconomicdata on the internet should be given adequate resources, and options forleveraging complementary private data collection should be explored.the following principles, derived from the committeeõs examination of the broad social policy issuesñprivacy, anonymity, and identity; authentication; taxation of commerce transacted over the internet;and universal serviceñshould be used to guide the development ofpolicy issues arising from the use of the internet.the internet's coming of agecopyright national academy of sciences. all rights reserved.overview and recommendations27principle 1. focus laws and regulations on the activities and behaviors of concern rather than on the network architecture or its constituentnetworks. use existing laws and regulations first, provided they areconsistent with the capabilities and design of the relevant technologies.in many cases, existing laws are adequate to address internetrelated issues, and they should be the default approach. one risk posed by internetspecific legislation or regulation is that of measures whose implementation would force modifications to the internetõs architecture. the adverseeffects of new laws and regulations on that architecture should be weighedagainst their usefulness for addressing a particular problem. indeed,requiring enforcement of a particular policy within the network couldentail breaking the hourglass transparency of the internet. existing lawsand regulations will not prove adequate in all circumstances, however;the salient instance at present is internet telephony.principle 2. where internetspecific government intervention isrequired, laws and regulations should establish the framework andoverall parameters, while industry and other nongovernment stakeholders should devise appropriate implementations. the rapid evolution of the internet and its interactions with societal interests argue forcaution in setting rules and crafting legislation. the extent to which specific actions are required today is unclear, in part because it is unclearwhich circumstances will endure or to what extent voluntary actions inresponse to public and government pressures are at least in part addressing some concerns. however, todayõs heated national and internationaldebate in areas such as privacy and anonymity illustrates that not allstakeholders believe status quo approaches will prove satisfactory, sogovernmental institutions will surely be monitoring progress and may, atsome stage, intervene through new regulation or laws. the committeedoes not recommend where government intervention should or shouldnot be undertaken. as noted above, it finds too many of the elements ofthe situation to be too dynamic, and it in any case did not conduct acomplete assessment of social policy issues. but if it is determined thatvoluntary action alone is not sufficient, a legislative or regulatory approach should be adopted that reflects the dynamic, evolving nature ofinternet applications and services and the internet marketplace. legislative and regulatory actions should establish a framework for desired outcomes and define the principles and parameters that bound online conduct. a flexible approach also helps create an environment that fostersalternative solutions, both in terms of new practices and new technologies, and that can both satisfy the established principles and provideadditional benefits such as easier implementation, decreased costs, andgreater investment in innovation.the internet's coming of agecopyright national academy of sciences. all rights reserved.28the internetõs coming of ageprinciple 3. keep a broad geographic perspective when thinkingabout internet issues. over the internet, it can be as easy to interact witha person, organization, or company thousands of miles away as withsomeone in the next town. issues surrounding sales tax collection haveshown how the internet weakens geographical boundaries and how localand national social and economic interests and concerns come into play aspolitical institutions attempt to address the geographical challenge. commerce is but one of many instances where the internetõs global natureraises issues and stresses existing regimes; another instance is cultural aswell as community identity. the global nature of the internet also meansthat many issues will have to be addressed in international forums, in theinterest of harmonizing approaches to transborder problems and establishing reciprocity and other arrangements in the event of transborderresponses to problems. in accordance with principle 1, internetrelatedissues are best resolved, wherever possible, by the established law of therelevant domain or established rules for handling crossborder activities.pursuant to principle 2, solutions that seek to establish performance objectives rather than specify implementation details are preferable. in someareas, existing national and multilateral frameworks (and adaptive processes) will be sufficient to address concerns. harmonization will, however, present an ongoing challenge, and resolution may necessitate countries making compromises on the specific approaches; global scopeimplies, among other things, a need to frame u.s. policy in the context ofpolicy in other parts of the world, which can affect the design and enforceability of measures taken in the united states.the internet's coming of agecopyright national academy of sciences. all rights reserved.29what is the internet?the internet is a diverse set of independent networks, interlinked toprovide its users with the appearance of a single, uniform network. twofactors shield the user from the complex realities that lie behind the illusion of seamlessness: the use of a standard set of protocols to communicate across networks and the efforts of the companies and organizationsthat operate the internetõs different networks to keep its elements interconnected.the networks that compose the internet share a common architecture(how the components of the networks interrelate) and software protocols(standards governing the interchange of data) that enable communicationwithin and among the constituent networks.1 the nature of these twoabstract elementsñarchitecture and protocolsñis driven by the set offundamental design principles adopted by the early builders of theinternet. because an appreciation of these principles is important to understanding what makes the internet what it is, several of them are discussed at length below. those who design and operate the internet generally characterize the internet in terms of these principles, which is notsurprising given that the internet derives from work done by researchers1some would argue that the term òinternetó embraces the entire interconnected dataworld rather than just the ipbased infrastructure. that broader definition includes networks using other protocols that interface with the ipbased internet.the internet's coming of agecopyright national academy of sciences. all rights reserved.30the internetõs coming of agein computer science and engineeringñfields that rely on abstraction as atechnique for managing the complexity of what computer scientists studyor build.the success of the abstracted interface through which users encounter the internet contributes to the internet illusion. software such as webbrowsers makes use of names for things attached to the internetñwww.example.com, for instanceñthat hide the nature of the networks towhich both they and the user are connected. this, of course, has enormous advantages for users, who need not worry about the complexities ofthe networks they are making use of. making things appear simple,however, can lead to unmet expectations. for example, a user who attempts but repeatedly fails to connect to a web siteñsay, www.example.comñwill look to the internet service provider (isp) to resolve the problem. however, the odds are good that the computer namedwww.example.com will turn out not to be attached to the network of theuserõs provider. the provider, in fact, may not have a direct connection tothe provider servicing www.example.com and may not be able to eventell the user what the problem is, where the problem is located, and thelikelihood of its happening again. if a user considers connecting to thatsite to be missioncritical, such a response is likely to be very frustrating.advances in technology and services can, however, improve the qualityof the illusion substantially, as can a better understanding by users ofhow the internet is constructed. we will return to the technological,economic, and policy issues surrounding interconnection in chapter 3.one consequence of the internet illusion is that the ordinary internetuser is likely to assume that a connection to the internet via a givenprovider of internet services amounts to a direct connection to the totalityof the internet. but in reality, the user has only contracted for internetservice with one of a number of ispsñenterprises that provide internetconnectivity to end users or other isps. each isp controls and operatesonly a fraction of the global network system. to reach all of the enddestinations, content, or services that a user wishes to reach, an internetservice provider may have to forward the userõs communication throughseveral other networks, none of which it controls. interconnections aremade largely though network links that are bilaterally coordinated between isps.it is important to distinguish between the public2 internet, which is2òpublicó is used here in the same sense it has in the context of the public telephonenetwork. it does not denote public ownership; it denotes instead a network to whichanyone can connect and in which any customer can exchange traffic with any other. theline between private and public is not always sharp; in particular, the physical networksthey use are not necessarily distinct.the internet's coming of agecopyright national academy of sciences. all rights reserved.introduction and context31normally what is meant when òinternetó is written with a capital i, andthe internetõs core technologies (standard protocols and routers), whichare frequently called òip technologyó in reference to the key protocolused on the internet. throughout this report, these termsñinternet andipñwill be used in this way. the public internet is distinguished byglobal addressability (any device connected to the internet can be givenan address and each address will be unique) and routing (any device cancommunicate with any other). in practice, however, as a consequence ofinterventions imposed by isps and local network managersñsuch as thedeployment of firewalls and other technologies for filtering communications trafficñnot all data are allowed to pass to all devices and not alldevices are assigned public addresses. ip technologies are also employedin private networks that have full, limited, or even no connectivity to thepublic internet; the distinction between public and private blurs becauseto the extent that private networks acquire connections to the internet,they by definition become part of it.if one looks at the elements that physically make up the internet, onesees two categories of objects. the networks that make up the internet arecomposed of communications links, which carry data from one point toanother, and routers, which direct the communications flow between linksand, ultimately, from senders to receivers. communications links mayuse different kinds of media, from telephone lines to cables originallydeployed for use in cable television systems to satellite and other wirelesscircuits. internal to networks, especially larger networks in more developed parts of the world, are links that can carry relatively large amountsof traffic, typically via optical fiber cables. the largest of these links arecommonly said to make up the internetõs òbackbone,ó though this definition is not precise and even the backbone is not monolithic.3 links closerto users, especially homes and small businesses, typically have connections with considerably less capacity. large organizations, on the otherhand, tend to have highcapacity links. over time the effective capacity oflinks within the network has been growing. links to homes and smallbusinessesñthe socalled òlast mileóñhave until recently, with the emergence of cable modem, digital subscriber line (dsl), and other technologies, been largely constrained to the relatively low speeds obtainable using analog modems running over conventional phone lines. analogmodems remain the dominant mode of home access.3there is no easy way to specify which networks comprise the internet backbone. forinstance, in some countries a rather modest link may serve as the local backbone. nor doall connections between providers take place through the backboneñthere is no assurancethat any particular data packet will flow through any part of the internetõs backbone.the internet's coming of agecopyright national academy of sciences. all rights reserved.32the internetõs coming of agerouters are computer devices located throughout the internet thattransfer information across the internet from a source to a destination.routing software performs several functions. it determines the best routing paths, based on some set of criteria for what is best, and directs theflow of groups of data (packets) through the network. path determination at each step along the way depends on information that each routerhas about the paths from its location to neighboring routers as well as tothe destination; routers communicate to one another some of this pathinformation. a number of routing algorithms that determine how routersforward packets through the network are in use; routing protocols mediate the interchange of path information needed to carry out these algorithms.the internet can be divided into a center, made up of the communications links and routers operated by internet service providers, and edges,made up of the networks and equipment operated by internet users. theline between center and edge is not a sharp one. users who connect viadialup modems attached to their computers clearly sit at the very edge.in most business settings as well as in an increasing number of homes,lans sit between the isp and the devices that the internet connects.these lans, and the routers, switches, and firewalls contained withinthem, sit near the edge, generally beyond the control of the isp,4 but not atthe very edge of the network.software applications running on these computing devicesñtoday,typically pcsñuse internet protocols to establish and manage information flows that support applications over the internet. much as a commonset of standard protocols lies at the core of the internet, common standards and a common body of software are features of many applications,the most common being those that make up the world wide web (theweb). the web adds its own protocols for information exchange thatbuild on top of the fundamental internet protocols, and it also provides astandard way of presenting information, be it text or graphics. morespecialized software, which also makes use of the internetõs basic protocols and frequently is closely linked to web software, supports such applications as realtime audio or video streaming, voice telephony, textmessaging, and a whole host of other applications. in light of the prominence of the web today, webbased applications and the content andservices provided by them are sometimes viewed as synonymous withthe internet; the internet, however, is a more generalpurpose networkover which the web is layered.4though isps do sometimes provide firewalls for their customers.the internet's coming of agecopyright national academy of sciences. all rights reserved.introduction and context33following usage from the telecommunications industry, the essentialphysical componentsñcommunications links and routersñof thenetwork (including links that are parts of other networks, such as thetelephone lines used by dialup modems or dsl or the highcapacityfiberoptic cables shared among internet, other data, and voice communications services) can be referred to as òfacilities.ó internet service providers use these facilities to provide connectivity using the internet protocols. what is done with the facilities and basic connectivity comes underthe heading òservices.ó these services, which include such things asaccess to content (e.g., viewing web sites, downloading documents, orlistening to audio), electronic commerce (e.g., shopping, banking, and billpaying), or telephony, are enabled by both devices and software in thehands of users and service providers. some services are enabled merelyby installing software on user computers, while others rely on functionality implemented in computers and software attached to the internet by athird party. in either case, the generalpurpose nature of the internet hasmeant that there does not have be any arrangement between the internetservice provider and the provider of a particular service. while this statement generally holds true today, we are seeing the emergence of exceptions to it in the form of applicationspecific delivery networks (e.g.,akamai) that employ devices located throughout the network, generallynear the edges. chapter 3 discusses these trends and their implicationsfor the future development of the internet.a multitude of businesses are based on selling various combinationsof these elements. for instance, many internet service providers (isps)integrate connectivity with content or services for their customers. someisps rely in part or in toto on access facilities (e.g., dialup modem pools)owned and operated by other providers, while others operate most or allof these facilities themselves. also, isps may opt to own and operate theirown communications links, such as fiberoptic cables, and networks orthey may run internet services over links and networks owned and operated by other communications companies (just as companies have resoldconventional voice telephony services for years).the tale is well told about how, over the past decade, the internetevolved from a novel, but still developing, technology into a central forcein society and commerce,5 and the committee will not belabor the pointhere. suffice it to say that the transformations resulting from the internetalong with expectations for continued growth in its size, scope, and influ5see, for example, computer science and telecommunications board (cstb), nationalresearch council. 1996. the unpredictable certainty: information infrastructure through2000. washington, d.c.: national academy press.the internet's coming of agecopyright national academy of sciences. all rights reserved.34the internetõs coming of ageence, have given rise to widespread interest and concern on the part ofgovernment and society. a more realistic and betterinformed appraisalof internet issues has become imperative now that governments at alllevels seek to control its evolution and use and dedicated issueadvocacygroups have begun to proliferate. this report, written by a group ofexperts in a number of areasñtechnologies, operation, and managementof the internet; associated communications infrastructures, such as thepublic switched telephone network; and related policy and social issuesñis intended to explain key trends in the internetõs evolution and theirimplications for policy. it focuses on trends that are often misunderstoodor incompletely treated by the mass media and it highlights specific areasof policy that warrant more or better consideration. the remainder of thischapter characterizes the internetõs special design attributes and outlinesseveral key trends in facilities and services.success by designñabstract features and principleswhy has the internet been so successful? much of the answer lies inthe combination of two factorsñfunctionality and lower costs. the newfunctionality stems from the internetõs unique design principles and features that make connection, interconnection, and innovation in both facilities and services relatively easy. the internetõs characteristics havealso made it possible to use the underlying communications infrastructure more efficiently, thereby setting a lower price point for the communications it enables. both factors have generated a pattern of innovationin internet technologies and uses.its relatively rapid responsiveness to users and other design attributesdistinguish the internet from other parts of the information infrastructure, such as the public switched telephone network (pstn) or the television networks (cable and broadcast). the design of those other networksis more focused on the center, and greater functionality is located withinthe networks. they have been more centrally developed and managedand historically have limited what users can do with them. in contrast, asdetailed below, the internetõs design is effectively neutral to what servicesoperate across the network. this enables a relatively unrestricted set ofapplications to run over it without the need for changes to be made withinthe network.much of the design of the internet can be traced to the principlesadopted by the research community that undertook its early development. these principles and the resulting architecture have been codifiedin research papers and in a special set of documents describing theinternetõs design known as requests for comments (rfcs), a name reflectthe internet's coming of agecopyright national academy of sciences. all rights reserved.introduction and context35ing the interactive and iterative nature of internet technology development. especially notable are the articulation of the endtoend argument6and rfc 1958.7these and other documents embody some value judgments and reflect the fundamental political and ethical beliefs of the scientists andengineers who designed the internet: the internet architecture reflectstheir desire for as much openness, sharing of computing and communications resources, and broad access and use as possible. for example, thevalue placed on connectivity as its own reward favors gateways and interconnections over restrictions on connectivityñbut the technology canbe used permissively or conservatively, and recent trends show both.another value underlying the design is a preference for simplicity overcomplexity.these values have been advanced through the architectural viewembodied in voluntary standards set by such bodies as the internet engineering task force (ietf),8 which has been the dominant standardssetting body. within this body, there has been open competition betweencompatible implementations. other standardssetting bodies have alsocontributed to the establishment of key standards. one such body is theworld wide web consortium, which has worked on standards related tothe web. another is the international telecommunication union (itu).to date, internet standards generally tend to be developed on a perceivedneed basis and respond to technological developments; they alsocontinue to be linked to the activities of the network research community.the design values of the internet have been reinforced by the environment in which the internet was developed. in its early years as acooperative research project, it was isolated from some of the stresses andstrains associated with commercial marketplace interactions. today theietf, like other organizations associated with the internet, must respondto the economic forces of a robust marketplace. whether and how thetraditional internet design values will be maintained is an important issuefor the future of the internet.6see j.h. saltzer, d.p. reed, and d.d. clark. 1984. òendtoend arguments in systemdesign,ó acm transactions on computer systems 2(4):277288, november.7internet architecture board. 1996. architectural principles of the internet, brian carpenter,ed., request for comments (rfc) 1958, june. available online at <http://www.ietf.org/rfc/rfc1958.txt >.8while the ietf does apply an architectural view to the development of internet standards, it does not have anything to do with controlling how the networks that make up theinternet are actually built and configured.the internet's coming of agecopyright national academy of sciences. all rights reserved.36the internetõs coming of agethe internetõs òhourglassó architectureas an open data network,9 the internet can operate over differentunderlying technologies, including those yet to be introduced, and it cansupport multiple and evolving applications and services. in this layeredarchitecture, bits are bits and the network does not favor by its design oreffectiveness any particular class of application.10 evidence of this openness lies in the fact that the internetõs essential design predated a numberof communications technologies (e.g., lans, atm, and frame relay) andapplications and services (e.g., email, the world wide web, and internetradio) in use todayñand that within the internet all of these technologiesand services, both new and old, can coexist and evolve. the shape of anhourglass inspired its selection as a metaphor for the architectureñtheminimal required elements appear at the narrowest point, and an everincreasing set of choices fills the wider top and bottom, underscoring howlittle the internet itself demands of its service providers and users.11as a consequence of this hourglassshaped architectural design, innovation takes place at the edge of the network, through software runningon devices connected to the network and using open interfaces. by contrast, the pstn was designed for very unintelligent edge devicesñtelephonesñand functions by means of a sophisticated core that provideswhat are termed òintelligent facilities.ó edgebased innovation derivesfrom a fundamental design decision made very early in the developmentof the internet and embodied in what is called the endtoend argument insystems design.12 aimed at simplicity and flexibility, this argument saysthat the network should provide a very basic level of serviceñdata transportñand that the intelligenceñthe information processing needed toprovide applicationsñshould be located in or close to the devices attached to the edge of the network.underlying the endtoend argument is the idea that it is the system9computer science and telecommunications board (cstb), national research council.1994. realizing the information future: the internet and beyond. washington, d.c.: nationalacademy press.10the layering principle is a powerful one and appears in other contexts. one sees operating system application interfaces such as those provided by windows or unix that allowa large number of application programs to run on diverse computing platforms as well asbus protocols (e.g., pci or usb) that allow a large number of peripheral devices to workwith a variety of different computing platforms.11some caution is needed in interpreting the hourglass metaphor. the narrow òwaistó atthe middle of the hourglass is a metaphor for the minimally specified choice of technologyat this point and is not intended to convey any sense of a choke point or bottleneck.12this was first expressed in j.h. saltzer, d.p. reed, and d.d. clark. 1984. òendtoendarguments in system design,ó acm transactions on computer systems 2(4):277288.the internet's coming of agecopyright national academy of sciences. all rights reserved.introduction and context37or application, not the network itself, that is in the best position to implement appropriate protection. if the network or network provider tries totake on this task it is likely to implement something that is too heavyhanded and performanceinhibiting for some applications and too lightfor others. both the sender and receiver are held ultimately responsiblefor assuring the reliability of communications services (e.g., making surethat what is received is complete and in order), so as to protect end usersagainst the vagaries of the networks that lie between them.13 end systemsare also responsible for protecting themselvesñan end system must beable, for example, to authenticate the sender of a message requesting, say,the deletion of a file located on the system.14the original architects of the internet made a key design decision touse the principle of layering to separate applications from the underlyingtransport infrastructure of the internet. by hiding the realities of how theinternet is constructedñfor instance, the topology of the network or thephysical configuration of its elements, how routing is performed withinthe network, or how particular data transport services are implementedñthe architecture enables people to write applications that run over it without having to possess any knowledge of these realities. in fact, withoutusing specialized diagnostic tools, there is very little way for applicationsoftware that makes use of the internet to discover the detailed characteristics of the underlying networks. in general, even a poorly designedapplication can be added in a few sites at the edge of the network withoutputting the network at risk; this is how new applications can be experimented with, tested, and improved.this manifestation of the internet illusion discussed above has beenkey to the explosion of new services and software applications of theinternet. the combination of a standardized interface to the network andthe location of intelligence at the edges means that developers can writeand field new devices or new software without any coordination withnetwork operators or users or any changes in the underlying transport13one counterexample is denialofservice attacks at the network level (i.e., òstormsó ofip packets sent to a network or router), which can be argued to deserve remedy within thenetwork itself.14one can think of the result of combining the endtoend argument and the hourglassarchitecture in another way. by providing an unreliable datagram delivery service in whichthe network attempts to deliver a given datagram (piece of information) but does not guarantee such delivery, the internet makes minimal assumptions about the characteristics ofthe underlying transmission networks and passes a minimal set of functions up to higherlevels of the protocol. this design allows complex networks of connectivity to be overlaidacross a highly diverse collection of communications elements.the internet's coming of agecopyright national academy of sciences. all rights reserved.38the internetõs coming of agenetwork. nor do new applications or changes need to be deployed all atonce. even though many developers of network applications do not understand or appreciate the technical and management challenges confronting those who build and operate internet networks, they are still ableto succeed in developing all kinds of popular new applications.thus, not only do we see pcs and larger computer systems attachedto the internet, we now see televisions (e.g., webtv), telephones, personal digital assistants (pdas), and other devices being attached as well;the future is likely to see many other devices emerge (e.g., music appliances directly connected to the internet). of course, not all such applications represent improvements (and many will fade away over time), butthe internet supports rapid feedback and the evolution of new and improved features and function, both of which are associated with theinternetõs culture of cumulative knowledge building.the corollary to ease of innovation at the internetõs edges is thatinnovation at the center of the network is difficult and can be very slowbecause building new features into the network requires the coordinatedactions of many providers and users. the problem is exemplified by thedifficulties of deploying new networklevel features such as enhancedquality of service or ip multicast (both discussed further in chapter 2).this is not to say that an increasing number of sophisticated thingsare not being implemented inside the internet to optimize the delivery ofvarious services. for example, algorithms for filtering and load balancingare found in some routers because they provide benefits in terms of service quality for certain traffic (perhaps at the cost of raw switching speed).web caching entails adding devices throughout the network to improvenetwork performance. such caching is achieved by moderating or redirecting specific types of network traffic in ways that can avoid congestionby making use of temporary local copies of frequently accessed information. also, businesses that are building applications that require a greatdeal of network capacity or lowlatency delivery of informationñrequirements not met very well on todayõs internetñare coping by building theirown applicationspecific delivery networks, which employ devices located throughout the edges of the network as a workaround. installationrequires cooperation (and may require colocation) with particular isps.these technologies are controversial from an architectural and robustnessstandpoint as they disturb the endtoend model. robustness implications are discussed in chapter 2 and architectural implications are discussed in chapter 3.the internet's coming of agecopyright national academy of sciences. all rights reserved.introduction and context39the robustness principlethe robustness15 principle is arguably the single most enabling characteristic of the internet.16 it was initially adopted for the arpanet inorder to accommodate the unpredictably changing topologies anticipatedfor defense applications (i.e., dynamic network reconfiguration) and thenfor the internet in order to accommodate interconnecting a diverse setnetworks built by multiple implementors out of components using multiple implementations (i.e., heterogeneity of devices and technologies).in accommodating both requirements, the internet accommodates decentralized management, growth, andñaccordinglyñevolution.in practice, this robustness principle has taken several forms. oneway of viewing robustness is that the rule for interpreting standards(and other specifications) that are not quite as precise as they might be ina perfect world should be for the sender to take the narrowest interpretation (i.e., the intersection of all possible interpretations) and for the receiver to be prepared for the broadest possible interpretation (i.e., theunion of all possible interpretations).17 robustness also entails conservative and careful design at the transport level that is able to deal with a15the robustness being discussed here should not be confused with the same term usedelsewhere in this report, especially chapter 2, where it denotes lack of vulnerability tofailures or attack.16this principle was written down by jon postel in the 1979 internet protocol specification: òin general, an implementation must be conservative in its sending behavior, andliberal in its receiving behavioró (jon postel. august 1979. internet experiment note (ien)111 (the ip specification), p. 22). the same text appears in september 1981, in rfc 791,p. 23, and a variant appears in the tcp specification, under the heading òrobustness principleó: òtcp implementations should follow a general principle of robustness: be conservative in what you do, be liberal in what you accept from othersó (information sciencesinstitute, university of southern california. 1980. dod standard transmission protocol, rfc761, january. available online at <http://www.ietf.org/rfc/rfc0761>). a conservativeapproach in internet protocol design appeared earlier in internet engineering note 12.however, that paper falls short of enunciating a òrobustness principle.ó see lawrence l.garlick, raphael rom, and jonathan b. postel. 1977. issues in reliable hosttohost protocols. internet engineering note (ien) 12. augmentation research center, stanford research institute, menlo park, calif., june 8. available online from <http://www.isi.edu/innotes/ien/ien12.txt.2>.17it should also be noted that the robustness principle can be (and has been) cited as ajustification for noninteroperability. for example, a vendor can release protocol elementsthat, by a narrow (or even reasonable) reading of the standards, are not compliant with thestandard, and then claim that other implementations that do not interoperate with them areinadequate because they are not robust enough. the most literal interpretation of therobustness principle could also be taken as a requirement that each application or systemmust protect itself against any behavior whatsoever by others, but the general assumptionis that others will behave in acceptable ways.the internet's coming of agecopyright national academy of sciences. all rights reserved.40the internetõs coming of agewide range of behavior such as packet loss, delay, and outoforder delivery.another facet of robustness is the use of soft state within the internet.here òstateó refers to the configuration of elements, such as switches androuters, within the network. soft state, in contrast to hard state, meansthat operation of the network depends as little as possible on persistentparameter settings within the network. basic transmission of data overthe internet takes this to the extremeñrouting of packets is done in acompletely stateless manner so that each packet transmission is a separateevent that takes place without reference to previous or following packets.there is no explicit establishment of a particular network path throughwhich information will flow. this principle underpins the robustness ofthe internet because it allows the network to dynamically reconfigure itsrouting state continually (including routing around links that have failed)yet still deliver packets.18 the manner in which routers receive theirrouting information is also based on soft state. a router is able to discovernecessary routing information from its neighbors, allowing a router thatfails and then restarts to resume operation without intervention.other network services that provide for explicit allocation of networkresources for a particular communications session (e.g., some forms ofquality of service) require state to be retained within the network. inthese cases, robustness dictates that this be done in accordance with theprinciple of soft state, meaning that the state must not be persistent. forexample, if an application requests network resources, that request willtime out if not rerequested periodically, ensuring that network resourcesare not tied up if the application fails.19scalable, distributed, and adaptive designthe internetõs design, in particular the design elements listed above,make it able to support a growing amount of communicationsñgrowthin the number of users and attached devices and growth in the volume ofcommunications per device and in total. the capacity of communicationslinks has been one element of internet scaling. roughly a decade ago, theinternet passed its first scaling hurdle with the replacement of 56kbpslines with higher capacity, 1.5mbps (t1) lines throughout much of the18stateless transmission also leads to the interesting property that a packet can be removed from an arbitrary location in the internet and reinjected at any other point in thenetwork and it will continue on the shortest path to its addressed destination.19this stands in contrast to the use of hard state for telephone call setup in the pstn, inwhich the network is requested to allocate the necessary circuits to complete a call and thecircuits are held until a request to òtear downó the call is issued.the internet's coming of agecopyright national academy of sciences. all rights reserved.introduction and context41nsfnet backbone. subsequently, it passed successive scaling hurdlesand a decade from now will have passed several more. growth in thenumber of devices attached to the internet gives rise to other scalingpressures such as the size of the routing table and the processing anddistribution of routing information (see chapter 2 for a further discussion). yet a risk remains that solutions will be deployed that work for themoment but fail at the next bandwidth or memory scale, which guarantees that the crisis that precipitated the 1988 t1 deployment will be repeated. other scaling issues arise outside the network itself and relate tothe scalability of the systems put in place to implement a particular service (e.g., a content server or an ecommerce site). if the service is designed in such a way that it can support only, say, a halfmillion internetusers, it will fail if the service becomes popular with users. the lesson tolearn from history is that scaling must be considered in every decision,and that lesson is increasingly important as there are more pressuresdriving innovations that may not scale well or at all (see chapter 2 forfurther discussion).reflecting its architecture and design and the associated reliance on amultitude of organizations to operate its constituent networks, the internetis more distributed and adaptive than other information networks. theinternet protocol (ip) enables distributed control of the network (e.g.,decisions made about routing) except for the allocation of address blocksand the management of toplevel domain names in the domain namesystem. this distributed control, enabled by the hourglass architecture,provides for the more rapid development and introduction of innovativeapplications and services.from internet technology to internet marketplacein large part as a reflection of these design principles and features, theinternet has come to possess a set of attributes different from those ofother networks. these unique attributes include the following:¥multiple and evolving pricing models.internet businesses offer manypricing models and do not necessarily stick with any one. the norm forconsumer internet access today in the united states is a flatrate monthlycharge,20 which encourages use of the internet by eliminating uncertainty20from the standpoint of users, internet access is not flat rate in many countries becausethe local telephone call required for dialup access is metered rather than flat rate, as iscommon in the united states. this results in significantly higher total costs for internetaccess and is a significant deterrent to increased internet use. in some countries the localcalling charges for internet access have been reduced; in other countries, the cost of internetaccess has been absorbed into the perminute charges for local calls.the internet's coming of agecopyright national academy of sciences. all rights reserved.42the internetõs coming of ageabout monthly charges. the level of charge may vary according to provider promises of service quality. other factors affecting price include theispõs dependence on advertising as a source of revenue, the bundling ofsales of equipment and internet service (e.g., òfree pcó deals), and thebundling of internet access with content and special services. businessclients of isps follow a variety of pricing models, frequently based onusage. pricing for interconnection within the internet itselfñthat is, thecharges that isps pay to other ispsñalso follows a variety of pricingmodels, ranging from flat (trafficinsensitive) rates for interconnection tobarter arrangements with peers; these models are in flux. notably, unlike the pstn, very few pricing models are based on either distance or theexact volume of traffic carried. interconnection prices are often privatelynegotiated rather than based on fixed rates. internet interconnection contrasts with the pstn, where terms for interconnection and financial settlement are well established and the subject of regulation.¥low barriers to entry for innovation.consistent with the òcriteria foran open data network,ó21 the internet is designed to be open from thestandpoint of users, service providers, and network providers, and as aresult it has been open to change in the associated industry base as well asin the technologies they supply and use. a wide range of applicationsand services, some leveraging the commonality of ip and others additionally leveraging standards layered on top of ip, most notably the webinterface, have flourished. as that industry base grows and matures,questions arise about whether innovation will face other kinds of entrybarriers.¥tippy markets.the internet is the epitome of a network market. bydefinition, participation in a large network market is more rewarding; thelarger the network, the larger the number of users. a small initial advantage in market share, often associated with being first, can snowball into alarge advantage. these snowball effects are amplified on the internet bythe ease and negligible cost of distributing software through the network,which can promote much faster change than is typical for a product withphysical distributionñon a scale of months or a couple of years ratherthan several years or a decade. the desire to tip the market, seen in manycompetitive markets, is epitomized by the struggle to build a leadershipposition in streaming audio and video. this tippiness of the internetmarketplace suggests a pattern of highly concentrated markets and market leaders who greatly outdistance their competitorsñan outcome that21computer science and telecommunications board (cstb), national research council.1994. realizing the information future: the internet and beyond. washington, d.c.: nationalacademy press.the internet's coming of agecopyright national academy of sciences. all rights reserved.introduction and context43would be at odds with the historic expectation of heterogeneity in technology implementation. indications are, however, that these positions ofleadership are unstable. at least sometimes, a sufficient investment ofresources or other circumstances can allow newcomers to trump the incumbents and tip the market in another direction.22 in recent years, forexample, microsoft acquired much of the market for web browsers, amarket that was once dominated by netscape communications. whetherthese patterns prove enduring and sustainable remains to be seen.the internet has been well served by an insistence that there is oftenmore than one òrightó answer to a question. its designers argue that nosingle technology solves all the problems well, or even well enough, sothat no single technology should be considered as the sole solution. thus,when there is a common standard, there are frequently multiple independent implementations of it. no single vendor has cornered the market ingood technology, and when one has gotten close to a monopolistic position, the traditional internet community has been critical of the situationbecause of its potential for inhibiting continued innovation. it is not thatdeveloping proprietary extensions or protocols to implement optionalfeatures (which can generally coexist with standard protocols) is a problem per se. rather, a monopolistic position would preclude the significant benefits of having the essential elements of the infrastructure (hardware or software) exist in multiple, independent implementations,available from multiple vendors. competition has served the internetwell.internet organizationsseveral private, nonprofit organizations play critical roles with respect to the internet. these include the internetõs principal standardssetting bodies: the internet engineering task force (ietf), the internetarchitecture board (iab), and the internet engineering steering group(iesg). along with the evergrowing number of other organizations involved in setting internet standards, they are grappling with a growingnumber and diversity of stakeholders and with the everlarger commercial stakes associated with the outcomes of their work. another class oforganizations deals with operational issues: for example, the northamerican network operators group (nanog) provides a forum fortroubleshooting and exchanging technical and operational information.22for an overview of network economics, see hal varian and carl shapiro. 1998. information rules: a strategic guide to the network economy. boston, mass.: harvard business schoolpress.the internet's coming of agecopyright national academy of sciences. all rights reserved.44the internetõs coming of agemost visible recently has been icann, a newly formed body that hasassumed overall responsibility for managing the internetõs addresses andnames. its work has received considerable attention and been the subjectof vigorous debate as address and name management become more contentious and controversial activities. and, while the internet corporationfor assigned names and numbers (icann) was not established to takeon the broader mission of internet governance, it has not been able toavoid some international governance questions in the course of its work,leading observers to see its potential to play a larger role in the ambiguous arena of internet governance. regional address registries have responsibility for managing the pool of addresses delegated to each regionof the world.key trends in internet developmentthe internet has already gone through several iterations. new routing protocols have been deployed in bounded administrative domains,for example, and replaced with other protocols as technology has matured. ip addresses at one time had to be given out in blocks of fixed size,whereas today they are assigned in blocks defined by demonstrated needs.what has worked over a period of some 25 years has been continual,generally gradual change, characterized in most cases by continued interoperation between newer and older hardware and software. suddenrevolutionary changesñfor instance, the sudden phasing out of one protocol in favor of anotherñhave not worked as well.23 for this reason, it isunrealistic to believe that major infrastructure components, whether hardware or software, can be changed without a significant period of coexistence and interoperation. the history of the internet argues for an expectation of change from time to time and for design choices that at each stepinclude the ability to transition to the next step.advancing the internet is about improvements in three areas: (1) thenature and business of supplying network facilities; (2) internet connectivity; and (3) applications, content, and services. one of the things that isspecial about the internet is that its architecture allows an internet business to separate these three areas or to combine them in different ways.23one notable òflag dayó transition occurred in the arpanet on january 1, 1983, whenall hosts had to simultaneously convert from ncp to tcp. the transition, which came at atime when the internet was far smaller, nonetheless required careful advance planning(barry m. leiner et al. 1998. a brief history of the internet. version 3.1, february 20.available online from <http://www.isoc.org/internet/history/brief.html>).the internet's coming of agecopyright national academy of sciences. all rights reserved.introduction and context45growth in backbone capacitythe heart of the internet grows through the interactions of isps andmajor equipment manufacturers (principally router vendors and communications circuit suppliers). increased capacityñspeed, performance, andthe accommodation of more users and more connectionsñis the watchword. in terms of fundamental communications, everincreasing exploitation of optical fiber facilities has been the trend. growth in internettraffic (by a factor of roughly 2 every year) has been outstripping growthof computing speed (by the mooreõs law factor of 2 every 18 months).24to maintain this trend, equipment manufacturers are constantly challenged to improve the performance of communications equipment nearlytwice as fast as the pc and pccomponent manufacturers improve pcs.for staying on this curve, the equipment industry is highly dependent onthe help of innovations from both industry and governmentfunded research (the latter comes chiefly from the defense advanced researchprojects agency (darpa) and the national science foundation (nsf)).it is generally believed that given current technology and some sustained research support, equipment manufacturers should be able to continue to improve performance in the time frames required to keep on thisperformance trajectory. in 2005, if current trends persist, the fastest linkwill be roughly 2 terabits per second (tbps), requiring routers that canmove data at 100 tbps rates internally, and 5 years later, links will beapproaching the 100 tbps level. at that point, routers that can handlepetabits (1000 tb) per second will be required, and the requirement forextremely fast routers becomes a major challenge. some predict that alloptical networkingñunlike the networks today, which combine opticalfiber with routers based on electronicsñwill provide a solution. however, the channel switching speeds of todayõs optical technologies are farslower than the speeds of todayõs routers, suggesting that opticalswitchingõs importance may come from automating and speeding up themanagement of aggregated traffic flows.2524see lawrence g. roberts. 2000. òbeyond mooreõs law: internet growth trends.ócomputer 33(1):117120.25one thing that appears crucial is further development of optical multiplexing. oneparticular technique, known as wavelength division multiplexing (wdm), allows one topack a great deal of data into a single fiber by using multiple lasers operating at differentcolors in parallel (it is much harder to use one laser to signal at very high bandwidths). oneapproach will be the use of link management techniques, whereby routers aggregate trafficto different destinations, so that traffic is placed onto a switched flow, bypassing intermediate routers. these large aggregates, not individual packets, would be switched optically.to switch a packet, the existing router technology works well. in this approach, differentcolors would be configured on a timescale of minutes to days to carry these aggregatesfrom source to destination routers, bypassing intermediate routers and switches.the internet's coming of agecopyright national academy of sciences. all rights reserved.46the internetõs coming of agegrowth and diversification of the isp marketthe several thousand internet service providers differ widely in size,type of service they provide, and type of interconnection they have withother service providers. as the market has grown in overall size, it hasevolved to comprise both very large playersñthe tier 1 providers thatconstitute the internetõs backbone and the large, consumeroriented ispsñand many smaller players that focus on particular segments of the market. some serve particular markets (e.g., consumers or businesses) whileothers provide such specialized services as hosting web servers for othercompanies. peering, transit, and other interconnection arrangementsñwhich have both technical and economic dimensionsñhave played a vital role in enabling the interlinking that defines the internet, and severalissues related to these arrangements and their evolution are covered inchapter 3.upgrading the local access infrastructuretoday most home users access the internet through narrowband connections made by modems using the public switched voice network. thisapproach has led to fairly ubiquitous access services from multiple providers that offer very similar pricing and features. such access achievesrelatively low data rates compared to what the telephone companyõs copper loops can provide and does not provide the continuous connectivitythat internet protocols were designed to take advantage of. the approachrequired little investment in new access infrastructure and could makeuse of the existing voice infrastructure fairly straightforwardly.now, however, dialup access, with its low bandwidth and need tocomplete a telephone connection each time access is desired, is increasingly seen as limited, though it remains the least common denominatorfor residential service today. at the same time, a range of new applications that require higher bandwidth and/or continuous connectivity arebeing developed.broadband access enables services that, because they require highcapacity and limited delay, cannot be provided via dialup access to theinternet; it also makes many existing services much faster and more responsive. broadband also enables multiple applications to be run, such assimultaneous telephony and web browsing. software or music downloads that require minutes or hours over a dialup connection take as littleas a few seconds via a broadband connection. another benefit of broadband internet connections is that, rather then requiring a phone call andconnection to be set up, they can be online all the time. this has twosignificant implications. routine monitoring tasks can easily occur conthe internet's coming of agecopyright national academy of sciences. all rights reserved.introduction and context47tinuously (e.g., notifying users that they have new mail), and networkinteractions can take place immediately (without waiting the minute ortwo required to establish a dialup connection), reducing the overheadrequired to retrieve information or conduct a transaction. looking up atelephone number online, for example, instead of in a phone book, is aviable option when one does not need to wait for an internet connectionto be established; similarly, other activities become possible with betterconnectivity. while these advantages are compelling, it remains easier atthis early stage of deployment to posit likely benefits than to quantifywith confidence actual consumer demand.while the wireline and wireless telephone companies still dominatethe provision of voice services and broadcast entertainment still uses radio signals delivered over the airwaves or via cable, delivery of voice,video, and other services over the internet is emerging. as an increasingnumber of users have broadband internet connections, it is reasonable toproject that the use of various ipbased telephony services is likely toincrease substantially and that video applications will probably grow aswell. a number of services that compete with existing broadcasting andentertainment businesses are emerging that are likely to increase in use,including internet delivery of music and internet òradioó broadcasting.another emerging trend is the use of distributed, peertopeer applications (napster and its offspring, particularly those that operate withoutany centralized server facility) to exchange content among internet users,capabilities that harken back to the early days of the internet, which wasdesigned to support peertopeer connectivity. these developments willhave several implications, including a change in the value consumersplace on internet access (especially broadband service) and potentialstresses on the internet itself.as an increasing number of people become familiar with broadbandand its implications, the rate and patterns of broadband deployment aswell as the types of services being offered have become the subject ofpublic debate today, with congressional and multilevel regulatory scrutiny in the united states and political activity by organizations representing consumer and industry perspectives.deployment is a nontrivial undertaking; it will require billions ofdollars in investment to deploy broadband pervasively. broadband technologies are being deployed at varying rates by a number of companies.cable companies are deploying twoway hybrid fiber/coax infrastructures capable of providing highspeed internet services. both incumbentand competitive local exchange carriers are also investing in broadbandaccess, primarily through a family of dsl technologies, which leverageexisting copper wiring to provide highspeed internet services. as withcable, the solution of leveraging copper plant is generally considered anthe internet's coming of agecopyright national academy of sciences. all rights reserved.48the internetõs coming of ageinterim step on the path to providing very high bandwidth connections tothe home using fiberoptic cable, although the time line for this is bothuncertain and likely to vary according to local circumstances.because two major facilities for broadbandñcable and the incumbentlocal exchange carrierõs copper loopsñare owned by incumbent playersin regulated industries and the third option todayñwirelessñdepends inpart on spectrum allocation, deployment issues are tightly coupled toboth the interests of incumbents and the evolution of the regulatory regimes that apply to these players. thus, for example, cableõs move intointernet access and telephony have led to increasing political activity andgovernment scrutiny of the terms and conditions of its internet serviceofferings and associated competitive conduct.the 1996 telecommunications act26 sought to promote competitionand consumer choice as key enablers of highquality, affordable broadband local access to the internet. efforts to enhance consumer choice fallinto two general classes: facilitiesbased competition and unbundling ofnetwork elements through regulation. facilitiesbased competition iscompetition among multiple access providers each of which operates itsown infrastructure. in such a regime, competition would exist, for example, between the copper pair infrastructure owned by the local exchange carriers, the hybrid fiber/coax infrastructure being deployed bycable operators, and wireless services. the premise of facilitiesbasedcompetition is that a multiplicity of facilitiesbased providers and theirheterogeneous business models will keep any one provider from dominating and creating a bottleneck for innovation and control of content onthe internet.another approach to ensuring consumer choice included in the 1996act is to use regulation to unbundle the elements of the incumbent carrierõsnetworks, thereby enabling the entry of competitors. for example, incumbent local exchange carriers are required to resell their copper lines tosubscribersñthe socalled òlocal loopsóñto other telecommunicationsproviders, allowing these entrants to offer competitive voice service orother services such as dsl over these lines. more recently, there havebeen calls for various forms of unbundling of the cable infrastructure, anidea generally referred to as òopen accessó (box 1.1). the architecture ofthe internet fundamentally supports unbundlingñthe issues that arisewith respect to unbundling include what particular approaches work technically in the context of particular access technologies, as well as a complex set of economic and policy issues.26telecommunications act of 1996, public law no. 104104, 110 stat. 56 (1996).the internet's coming of agecopyright national academy of sciences. all rights reserved.introduction and context49growing role for wireless servicesat the same time as cable and dsl technologies are starting to bedeployed, there have been considerable interest and investment in building competitive internet access via highspeed wireless networks. onecan also expect the development and deployment of wireless services toprovide mobile access. deployment has benefited from fcc efforts toopen up radiofrequency spectrum for such services, and it remains contingent on the ability to make spectrum available and to resolve issuesrelated to the siting of transmission towers in local communities. additionally, new satellite ventures are planning to deploy broadband combox 1.1 open access and cablecable operators have tended to select a single isp to provide internet serviceover their facilities, and the service has typically been bundled with at least somecontent or services aimed specifically at their own customers. several concernsappear to have prompted attention to cable open access. first, there are concernsstemming from cable operator assertions of control over what television content istransmitted over their systems. some fear that cable operators will seek analogous control over the internet content available to their subscribersñthat they willfavor particular content or create a òwalled gardenó service in which outside content is difficult or impossible to access. there are also concerns that those cableoperators who are in a monopoly position in their market, which is the case inmany but not all markets, will be able to charge other isps access fees that arehigher than the costs charged to the isp affiliated with the cable operator, givingthe latter an unfair competitive advantage. on the flip side, open access requirements could deter cable operators from investing in system upgrades to supportbroadband. the debate over open access has also been accompanied by debateover how to implement open access, the actual extent of proposed technical difficulties, and the additional costs of supporting competitive isp access over cablefacilities. several pilot efforts aim at demonstrating the feasibility of technical approaches that support access by multiple isps. it is unclear whether these concerns will be resolved as a result of moves by industry or regulatory action or legaldecisions, or whether they will persist.a detailed exploration of unbundling or open access policies and practicesand of other specific complex technical, social, and economic considerations thatunderpin the regulation of broadband local access is largely outside the scope ofthis work. the present report, therefore, does not recommend policy on regulatinglocal access, whether by dsl, cable, wireless, or other technologies. anothercstb body, the committee on broadband lastmile technologies, is currentlyinvestigating these and other issues related to broadband services for homes andsmall offices. however, several elements of the present report are likely to informthinking about the issue, including its discussions of what constitutes transparent,open internet service; trends in the isp business; and the likely roles for qualityofservice technologies on the internet.the internet's coming of agecopyright national academy of sciences. all rights reserved.50the internetõs coming of agemunications delivered from space; these will be a boon to sparsely populated areas where any sort of terrestrial infrastructure deployment is problematic.in addition to providing access in competition with wired technologies or access where terrestrial infrastructure is not costeffective, wirelessinternet can be expected to play a key role for a wide range of mobileapplications. there are many instances, such as when a user is in a car orin a public space, where being connected through a wire is simply not apractical option. ip connectivity in such situations could lead to all sortsof new applications (and businesses), some of which have not yet beenthought of and which might turn out to be as popular as normal email orweb surfing itself. the popularity of the imode phone service providedin japan by docomo shows the potential for rapid adoption of wirelessdata services elsewhere.voice and data servicesmany industry analysts predict that the rapid growth of data networks, particularly the internet, will result in voice traffic increasinglybeing carried using ip technology (voice over ip or ip telephony). whilethe time frame for completing such a transition is unclear today, it is clearthat many service providers, equipment manufacturers, and customersare moving in this direction. the public switched telephone network(pstn) is itself evolving to a more datacentric architecture, and the landscape of equipment suppliers is also rapidly changing. as use of theseservices grows, they will have significant impacts on the traditional, regulated voice service providers and may provoke calls for ip telephony to besubject to regulation akin to that in place for circuitswitched voice services. chapter 4 examines these issues, as well as the more general question of what happens when internetbased services compete with othercommunications industries.rise in the use of singlepurpose devicestoday the majority of devices connected to the internet are generalpurpose computers. most users access the internet through generalpurpose computers that are used for a multitude of tasks, and the serversused by providers of content and services over the internet are also generally based on generalpurpose computing systems. however, singlepurpose devices offer several advantages for both uses. first, a carefullydesigned, singlepurpose device can often be made much less inexpensively, with prices more in line with prices of other consumer electronicsdevices than those of generalpurpose computers. second, singlepurpose devices are likely to have fewer failure modes and harmful interacthe internet's coming of agecopyright national academy of sciences. all rights reserved.introduction and context51tions with other devices. third, singlepurpose devices lend themselvesto simpler interfaces and greater ease of use.27for those providing content and applications over the internet, possible singlepurpose devices include networkattached file servers andspecialized audio or video servers. for end users, a singlepurpose devicerunning a standard protocol can interact with any service that supportsthat protocol (e.g., consumers wishing to listen to music could use simpledevices that stream audio or simple standalone music players that download music).looking ahead, it is reasonable to project that networked systems willinclude a diverse set of embedded systems in homes and commercialsettings, and computers used in other infrastructures, such as electricpower, will also be networked. international data corporation, for example, has forecast that the number of devices connected to the internetwill more than double every year for the foreseeable future and that nonpc devices will account for nearly half of internet devices shipped by2002.28 acknowledging the limitations of market research, it is nonetheless reasonable to plan for their widespread use.such a future can have several significant implications for the internetinfrastructure. widespread use will, for example, increase the draw onthe ip address space. the existence of large numbers of more specializeddevices aimed at narrower applications also may put pressure on theinternet model of using a single, standard protocol (as illustrated by theintroduction of the wireless access protocol as a solution for the mobile,wireless space). finally, because they could be used for passive monitoring, deployment of a large number of smaller networked devices alsoraises privacy concerns, including the question of informed consent.future evolution and successreflecting its widespread deployment and adoption,29 substantialcommercial investment,30 and broad societal awareness, the internet hasbecome a mainline piece of the communications infrastructure. expan27conversely, a proliferation of singlepurpose devices, especially in the absence of standardized interfaces, could complicate the userõs experience.28international data corporation (idc). 1999. death of the pccentric era (idc executiveinsights). boston, mass.: idc. available online at <http://www.idc.com/f/ei/gens19.htm>.29nielsen media research and commercenet reported in june 1999 that the number ofu.s. and canadian internet users aged 16 and older was 92 million, an increase from79 million in the preceding yearõs study (associated press, june 18, 1999).30one measure comes from a ciscosponsored study conducted at the university of texasthe internet's coming of agecopyright national academy of sciences. all rights reserved.52the internetõs coming of agesion into the foreseeable future appears inevitable, and new technologiesand new applications that leverage these technologies and new opportunities will continue to emerge. the internetõs design principles and thevalues that underlie these principles have been critical to the spectacularsuccess of the internet. however, from todayõs vantage point of relativematurity, there are questions that need to be asked about the underlyingstructures that have brought us to this pointñthe looseness of theinternetõs internal coordination mechanisms, of the process by whichinternet standards are developed, and of the interconnection arrangements that tie it togetherñand questions about the internetõs scalabilityand reliability. because of the prominence of the internet, as well as itspotential for disruptive effects on both business and society, there arepressures for government to act in these and many other areas related tothe internet. thus far, telecommunications regulators have been reluctant to intervene. government involvement with the operation of theinternet has largely been limited to places where it was already involved,such as transitioning the administration of the domain name system, orplaces where internet and more traditional telecommunications issuesoverlap, such as within the pstn local exchanges. on the other hand,there have been both regulatory attention and legislative activity aimed atconsumer protection, such as protection of personal privacy and protection against junk email or spam, as well as measures aimed at contentand conduct over the internet (e.g., the communications decency actand gambling) and measures aimed at enhancing internetbased applications, such as legislation governing digital signatures.underlying this handsoff approach has been the belief that theinternet will continue to expand, mature, and evolve and that intervention could threaten that success. indeed, indications are that much of thisevolution can be expected to occur naturally, without recourse to remedial action by government or other players. however, the technical andpolicy challenges alluded to above raise questions about whether existingmechanisms are up to the task of supporting increasing demands andpressures and what the role of government should and should not be. inaddressing these questions, this report seeks to distinguish the issues thatwill probably be selfresolving from those whose resolution will requiregreater attention and/or new approaches.at austin (anitesh barua et al. 1999. measuring the internet economy: an exploratory study,technical report. austin, tex.: center for research in electronic commerce, graduate schoolof business, university of texas. available online at <http://cism.bus.utexas.edu/>), whichfound that the revenue of companies in the internet infrastructure business (isps, includingbackbone providers, network hardware and software companies, manufacturers of computers and servers, suppliers of security products, and manufacturers of optical fibers andassociated hardware) totaled nearly $115 billion annually in 1998.the internet's coming of agecopyright national academy of sciences. all rights reserved.53building a better internetthe internet has become a place where many live, work, and play. itis a critical resource for many businesses that depend on ecommerce.indeed, when attacks are made on internet infrastructure or commonlyused web sites like cnn, yahoo! and the like, they become frontpagenews.1 as a consequence, the internet must become and remain morerobust and reliable. reflecting demand for its capabilities, the internet isexpected to grow substantially worldwide in terms of users, devices, andapplications. a dramatic increase in the number of users and networkeddevices gives rise to questions of whether the internetõs present addressing scheme can accommodate the demand and whether the internetcommunityõs proposed solution, ipv6, could, in fact, be deployed to remedy the situation. the 1990s saw widespread deployment of telephonyand streaming audio and video. these new applications and protocolshave had significant impacts on the infrastructure, both quantitatively interms of a growing level of traffic and qualitatively in terms of new typesof traffic. the future is likely to see new applications that place newdemands on the internetõs robustness and scalability. in short, to meetthe potential demand for infrastructure, the internet will have to support1for example, matt richtel. 2000. òseveral web sites attacked following assault onyahoo.ó new york times, february 9, p. a1; and matt richtel. 2000. òspread of attacks onweb sites is slowing traffic on the internet,ó new york times, february 10, p. a1.the internet's coming of agecopyright national academy of sciences. all rights reserved.54the internetõs coming of agea dramatically increasing number of users and devices, meet a growingdemand for network capacity (scale), and provide greater robustness atany scale.scaling òscalingó refers to the process of adapting to various kinds of internetgrowth, including the following:¥the increasing number of users and devices connected to theinternet,¥the increasing volume of communications per device and totalvolume of communication across the internet, and¥the continual emergence of new applications and ways in whichusers employ the internet.while details of the growth in internet usage are subject to interpretation and change over time, reflecting the dynamic nature of internetadoption, it is only the overall trends that concern us here. in the unitedstates, a substantial fraction of homes have access to the internet, andthat number is likely to eventually approach the fraction of homes thathave a personal computer (a fraction that itself is still growing). over100 million people report that they are internet users in the united states.2overseas, while the current level of internet penetration differs widelyfrom country to country, many countries show rates of growth comparable to or exceeding the rapid growth seen in the united states,3 so it isreasonable to anticipate that similar growth curves will be seen in otherlesspenetrated countries, shifted in time, reflecting when the early adoption phase began.perhaps a more important future driver for overall growth is thetrend toward a growing number and variety of devices being attached tothe internet. some of those devices will be embedded in other kinds ofequipment or systems, and some will serve specific purposes for a givenuser. this trend could change the number of devices per user from thecurrent number, slightly less than 1 in developed countries, to much morethan thisñ10 or even 100.2data from computer industry almanac, available online at <http://www.cia.com>.3for an analysis based on oecd data, see gonzaolo diezpicazo figuera. 1999. ananalysis of international internet diffusion. masterõs thesis, mit, june, p. 83.the internet's coming of agecopyright national academy of sciences. all rights reserved.scaling up the internet and making it more reliable and robust55scaling of capacitythe basic design of the internet, characterized by the elements discussed in chapter 1, has proved remarkably scalable in the face of suchgrowth. perhaps the most obvious component of growth is the demandfor greater speed in the communications lines that make up the internet.as was noted in chapter 1, the first major scaling hurdle was seen abouta decade ago when, in response to growing demands, many of the56kbps lines in the nsfnet backbone were replaced with higher capacity 1.5mbps lines (also known as t1 lines).4 doing so required developing higher performance internet routers and some retuning of protocolsand software. since then, the internet has passed many scaling hurdlesand increased its capacity many times over. the fastest lines in theinternet were 2.5 gbps (oc48) in 1999, almost 50,000 times faster thanthe original lines, and the deployment of 10gbps lines (oc192) is underway.all expectations are that more such growth will be seen in the comingdecade. there is a persistent and reasonable fear that demand for capacity will outstrip the ability of the providers to expand owing to a lack oftechnology or capital. the 1990s were characterized by periodic scrambling by isps, equipment providers, and researchers to develop anddeploy new technologies that would provide the needed capacity in advance of demand. the success of those efforts does not, however, guarantee continued success into the future. furthermore, efforts to expandcapacity may not be uniformly successful. regional variations in theavailability of rights of way, industry strategies, and regulation couldslow deployments in particular areas.better use of existing bandwidth also plays a role in enhancingscalability. a recent trend has been to compensate for the lack of networkcapacity (or other functionality, such as mechanisms for assuring a particular quality of service) by deploying servers throughout the internet.cache servers keep local copies of frequently used content, and locallyplaced streaming servers compensate for the lack of guarantees againstdelay. in some cases, innovative routing is used to capture requests anddirect them to the closest servers. each of these approaches has sideeffects that can cause new problems, however. their implications forrobustness and transparency are discussed elsewhere in this report.4an abbreviation for bits per second is bps; kbps means thousands of bits per second,mbps means millions of bits per second, gbps means billions of bits per second, and tbpsmeans trillions of bits per second. the use of a capital b in place of lower case b means theunit of measurement is bytes (8 bits) rather than bits.the internet's coming of agecopyright national academy of sciences. all rights reserved.56the internetõs coming of agescaling of protocols and algorithmsa more difficult aspect of growth is in design of new or improvedprotocols and algorithms for the internet. the everpresent risk is thatsolutions will be deployed that work for the moment but fail as the number of users and applications continues to growñtodayõs significant improvement may be tomorrowõs impediment to progress. scaling mustthus be considered in every design. this lesson is increasingly importantas there are many pressures driving innovations that may not scale wellor at all.the ietf processes through which lowerlevel network protocols aredeveloped involve extensive community review. this means that theprotocols undergo considerable scrutiny with regard to scaling beforethey are widely deployed. however, particularly at the applicationslayer, protocol proposals are sometimes introduced that, while adequatein such settings as a local area network, have been designed withoutsufficient understanding of their implications for the wider internet.market pressures can then lead to their deployment before scaling hasbeen completely addressed. when a standard is developed through aforum such as the ietf, public discussion of it in working groups helps.however, a protocol can nonetheless reach the status of a òproposedstandard,ó and thus begin to be widely deployed, with obvious scalabilityproblems only partially fixed.the web itself is a good example of scaling challenges arising fromparticular application protocols. it is not widely appreciated that theòworld wide waitó phenomenon is due in part to suboptimal designchoices in the specialized protocol used by the web (http), not to thecore internet protocols. early versions of http relied on a large numberof short tcp sessions, adding considerable overhead to the retrieval of apage containing many elements and preventing tcpõs congestion controlmechanisms from working.5 an update to the protocol, http 1.1,adopted as an internet standard by the ietf in 1999,6 finally fixed enoughof the problem to reduce the pressure on the network infrastructure, butthe protocol still lacks many of the right properties for use at massive5though it took some time to launch an update, the shortcomings of http 1.0 wererecognized early on. see, for example, simon e. spero. 1994. analysis of http performanceproblems, technical report. cambridge, mass.: world wide web consortium, july. available online at <http://www.w3.org/protocols/http/1.0/httpperformance.html>.6r. fielding et al. 1999. hypertext transfer protocol ñ http/1.1.  rfc 2616. networkworking group, internet engineering task force, june. available online at <http://www.ietf.org/rfc/rfc2616.txt>.the internet's coming of agecopyright national academy of sciences. all rights reserved.scaling up the internet and making it more reliable and robust57scale. the challenge posed by this lack of scalability has been significantgiven httpõs large share of internet backbone traffic.7the case of ip multicast demonstrates the interplay between protocoldesign, the internetõs routing system, and scaling considerations. multicast is a significant example because it allows applications to simultaneously and inexpensively deliver a single data stream to multiple delivery points, which would alleviate internet scaling challenges. multicastcan be used in numerous applications where the same data are to be sentto multiple users, such as audio and audiovisual conferencing, entertainment broadcasting, and various other forms of broad information dissemination (the delivery of stock quotes to a set of brokers is one example). all of these applications are capable of running over todayõsinternet, either in the backbone or within corporate networks, but manyoperate via a set of individual, simultaneous (unicast) transmissions,which means that they use much more bandwidth than they might otherwise.despite its promise of reducing bandwidth requirements for onetomany communications, multicast itself presents scaling challenges. bydefinition, an internetwide multicast group needs to be visible throughout the internetñor at least everywhere where there is a groupõs member.the techniques available today require that routers track participation ineach active group, and in some case for each groupõs active senders. suchparticipation tracking requires complex databases and supporting protocol exchanges. one might reasonably assume that the number of groupsgrows with the size of the internet or with the growth of applications suchas internet radio broadcast, and that the footprint of each group (thefraction of the internet over which the group information must be transmitted) will grow as the size of the internet. however, the two factorsmultiply, meaning that under these assumptions, the challenges posed toproviders will grow as the square of the internetõs size. resolving thissituation requires not merely defining an appropriate protocol but alsoresearching a hard routing questionñhow to coalesce routing information of multiple groups into manageable aggregates without generatingtoo much inefficiency.7for example, internet traffic statistics for the vbns, a research backbone, show thatabout twothirds of tcp flows were http. see mci vbns engineering. 2000. nsf veryhigh speed backbone network service: management and operations monthly report, january.available online at <http://www.vbns.net:8080/nettraff/2000/jan.htm >.the internet's coming of agecopyright national academy of sciences. all rights reserved.58the internetõs coming of agescaling of the internetõs naming systemsgrowth in the number of names and an increasing volume of nameresolution requests, both of which reflect internet growth, are placingscaling pressures on the internetõs nametoaddress translation service,the domain name system (dns).8 there is broad consensus as well as astrong technical argument that a common naming service is needed onthe internet.9 people throughout the world need to be able to name objects (systems, files, and facilities) correctly in their own languages andhave them unambiguously accessible to authorized people under thosenames, which requires a common naming infrastructure. people alsoneed naming services to allow them to identify applications and servicesprovided by particular companies and organizations.the dns is instrumental in hiding the internetõs internal complexityfrom users and application developers. in the dns, network objects suchas the host computers that provide web pages or email boxes are designated by symbolic names that are independent of the location of the resource. the name provides an indirect reference to the network object,which allows the use of names instead of less mnemonic numbers andalso allows the actual address to which the name points to be changedwithout disrupting access via the name. because the computer associatedwith a particular named service can be changed without changing the ipaddresses of that machine (only the address associated with the name inthe dns needs changing), indirection provides users with portability ifthey wish to switch internet providers. while most users receive ip address allocations from their isp and thus have to change address if theychange isp, dns names are controlled by the userña change of providerrequires only that the address pointed to by the dns entry be changed.the significance of dns names was greatly increased as a result of thedecision by the original developers of the world wide web to use themdirectly to identify information locations. the importance attached todns names is reflected in the contention surrounding the systemõs management (box 2.1)the dns is organized as a hierarchy. at the very top of the hierarchy,the òroot serversó record the address of the toplevel domain servers,such as the .com or .uk servers (figure 2.1). the addresses of these root8the dns was first introduced in p. mockapetris. 1983. domain names ð concepts andfacilities, rfc 882. november. available online at <http://www.ietf.org/rfc/rfc0882.txt>.9internet architecture board. 2000. iab technical comment on the unique dns root, rfc2826. may. available online at <http://www.ietf.org/rfc/rfc2826.txt>.the internet's coming of agecopyright national academy of sciences. all rights reserved.scaling up the internet and making it more reliable and robust59servers are known locally to every name server of the internet, usinginformation provided by icann (in practice, coded into the dns software by the vendor). each toplevel domain server records the addressesof the domain name servers for the secondlevel domains, such asexample.com. these secondary servers are responsible for providing information on nametoaddress mappings for names in the example.comdomain. the hierarchical design permits the secondary servers to pointthemselves to thirdlevel servers, and so forth. to access named objects,internet sessions start with a transaction with a name server, known asname resolution, to find the ip address at which the resource is located, inwhich a domain name such as www.example.com is translated into anumerical address such as 128.9.176.32. assuming that the local nameserver has not previously stored the requisite information locally (see thediscussion of caching, below), three successive transactions are generallyrequired in order to find the address of a target server such as www.example.com: (1) to learn the address of the .com server from the rootserver, (2) to learn the address of the example.com server from the .comserver, and (3) to learn the address of the target web server, www.example.com, from the example.com name server.the situation in practice may, in fact, be more complicated. ifexample.com is a very popular service, it is useful to be able to distributethe load among multiple servers and/or to direct a user to the server thatis closest to him. to do either of these, the name servers run by example.com may make use of a clever trick: requests for the address corresponding to www.example.com, for example, may produce replies pointing tofigure 2.1 dns hierarchy.comneteduorg.........toplevellevel2level3gov...mil...us...uk......generic toplevel domainscountry toplevel domains...stanfordmitcs.........the internet's coming of agecopyright national academy of sciences. all rights reserved.60the internetõs coming of agebox 2.1 managing the domain name systemproviding a single, authoritative mapping between names and addresses requires internetwide coordination.1 the management of internet names was untilrecently provided by several organizations with little formal statusña group of regional registries, a u.s. government contract supporting the internet assignednumbers authority,2 and a u.s. government contract with network solutions.network solutions has had responsibility for assignments of names in the generic(i.e., noncountryspecific) .com, .net, and .org toplevel domains; regional internetregistries coordinated by iana have had responsibility for allocation of names incountryspecific domains (e.g., .uk for the united kingdom or .fi for finland).3 theregional registries have also been responsible for allocating blocks of numericaladdresses to internet service providers. as a result of the clinton administrationõsefforts to end the u.s. governmentõs direct involvement with internet address registration, responsibility for toplevel domain name administration was assigned in1999 to the internet corporation for assigned names and numbers (icann), anew nonprofit corporation established to assume responsibility for the management of the dns and the root server system (which forms the basis for all thedomain name servers distributed throughout the internet) as well as ip addressspace allocation and protocol parameter assignment. icann was charged withestablishing a system in which multiple competitors would be able to provide nameregistration services. network solutions remains the keeper of the root database.1names are not the only thing that require internetwide coordination. other, less visiblevalue assignments (technical parameters), such as tcp ports or application parameters, whichare also important to the internetõs operation, need coordination. such coordination has beenprovided by the iana for the ietf, a process that has worked well. the relationship betweeniana and the new icann organization remains to be resolved in detail. in addition, whereappropriate, other standards organizations have established their own registration processes,and again, these mechanisms seem to be working well.2iana, now incorporated into the new icann, was located at the university of southerncaliforniaõs information sciences institute, where it was led by the late isi computer scientistjon postel, supported by contracts from darpa and nsf.3while a majority of u.s. institutions make use of the generic toplevel domains, some alsouse a .us domain. the .us domain has been managed by iana.one of a number of different servers that, presumably, contain copies ofthe same information.10the rules governing dns names would seem to permit millions ofnaming domains each containing billions of names,11 which would seemadequate to support scaling demands. however, with the number of top10another nondns trick for load distribution makes use of socalled transparent proxiesor interception proxies. these intercept and divert data packets going to a particular address to one of a number of servers that contain the same content. because it interposesinformation processing outside the control of either the userõs computer or the server he isthe internet's coming of agecopyright national academy of sciences. all rights reserved.scaling up the internet and making it more reliable and robust61level domains currently limited to one national domain per country (e.g.,.fr for france), plus a limited number of global domains (e.g., .com and.org), many domains are organized with a very large number of namescontained at the next level rather than by distributing names further downmanaging the dns has also come to mean dealing with tensions resultingfrom the use of dns names in commerce and society at large. because theinternet never developed a true directory system, internet users frequently usedomain names as their key to navigate the web and connect to a specific organizationõs servers, usually of the form www.companyname.com. businesses mayalso register domain names for brand name products; there are entries for ivorysoaps (www.ivory.com) and crest toothpaste (www.crest.com) as well as for themanufacturer, procter & gamble, (www.pg.com). domain names are known explicitly by many consumers, are used in commercial advertising, and even used asthe name of some companies. limited lookup capabilities are provided by thecategorybased directories established by internet portals such as yahoo! or alternative naming systems provided by companies such as realnames or aol withits keywords, but none of these alternatives has, at least thus far, taken the placeof the dns.the allocation of internet names and addresses has been the subject of intense public debate during the last 3 years, especially the conflict of interest between domain name owners and trademark holders. two organizations/companies that wish to use the same name on the internet cannot both obtain a name ina single generic toplevel domainñthere can, for instance, be only oneexample.com. there can be a distinct example.org and example.net in the .organd .net generic toplevel domains, respectively, but companies that want to protect their trademarks may well register names in all the toplevel domains. also,with only one generic toplevel domain generally used for commercial entities, twocompanies with the same name that have established protection for their names(trademarks) in a particular geographical area or industry sector cannot both obtain this name in the .com domain. following a world intellectual property organization study, icann adopted a dispute resolution process to address conflictsrelated to the use of particular domain names, a move that has itself not beenwithout controversy. another issue is that the growing popularity of .com makes itincreasingly difficult to come up with short, readable names that are not alreadytaken. these considerations have given rise to proposals that would alleviate thesituation by expanding the number of generic toplevel domains. as of this writing,icann has proposed doing just that, but the details of such an expansion areunder discussion and the proposal remains to be implemented.connecting to, this technique runs counter to the endtoend principle and can sometimeshave the side effect of delivering inconsistent information to the user.11each dns name can be composed of up to 256 characters and up to 64 naming elements, each of which can be made of up to 64 characters (letters, digits, and hyphen).the internet's coming of agecopyright national academy of sciences. all rights reserved.62the internetõs coming of agein the hierarchy (e.g., using product.example.com instead of product.com). this can cause scaling problems, and there are concerns that theperformance of the dns will worsen over time.the multistage process required to find the address of a target, repeated for many web page accesses by millions of internet users, canresult in a heavy load on the servers one level down from the top of thetree. if the name servers were to be overwhelmed on a persistent basis, allinternet transactions that make use of domain names (i.e., virtually allinternet transactions) would be slowed down, and the whole internetwould suffer.todayõs dns design relies on two mechanisms to cope with thisloadñcaching and replication. these mechanisms have been effective inalleviating scaling pressures, but there are signs that they may not besufficient to cope with the continuing rapid growth of the network. dnscaching is a technique whereby the responses to common queries arestored on local dns servers. applications such as web browsers alsomay perform dns caching.12 using caching, a local dns server needonly request the addresses of the .com server from the root servers infrequently rather than repeatedly. similarly, once a request has been madefor the address of the example.com server, the local name server need notask for this information againñfor a period of time known as the òtime tolive.ó because of the dynamic nature of dns information, name serversreturn not only an address but a timetolive parameter selected by theadministrator of the name server for the relevant domain, usually on theorder of days or hours, which indicates how long the nametoaddressmapping can be considered valid, helping ensure that servers do notretain outdated information.caching works well when the same request is repeated many times.this is the case for highlevel queries, such as requesting the address ofthe .com name servers, and also for the most popular web servers, thesearch engines, and the very large sites. (it works even better for veryfrequently accessed services like a file server on a local area network.)however, the efficiency of caching decreases as the number of names that12why do applications also need to cache dns names? good dns performance dependson having local access to dns information. because the target platform was a tiny, disklessmachine, the earliest implementations of tcp/ip software for the ibm pc lacked dnscache functionality and depended on local lan access to a dns server for all name resolution requests. this resolveronly design has persisted in a number of machines today. notonly does this force the application designer to implement dns caching, but there areperformance costs as well. since an application cannot determine whether the host it isrunning on supports a caching server, applicationlayer caching makes it possible for caching to be carried out twice, potentially yielding inconsistent results.the internet's coming of agecopyright national academy of sciences. all rights reserved.scaling up the internet and making it more reliable and robust63are kept active by a given user or domain name server increases. whenmillions of names are registered and accessed in the dns, only a smallfraction can be present in any given cache. requests for the names of lessfrequently requested sites, which in total will represent a significant fraction of all requests, will have to be forwarded to the dns. even if userqueries are concentrated mostly on large sites or queries from the samelocal group of hosts are concentrated on the same group of sites, whichmay or may not be the case, the remaining fraction still constitutes animportant and growing burden for the dns. the effect of cache misses ismade even worse by the concentration of names in a small number ofpopular toplevel domains, such as .com, .net, and .org. consequently, aninordinate fraction of the load is sent to these domainsõ servers (a loadthat could be alleviated if the hierarchical design of the dns were used tolimit the number of highestlevel names). these servers need to scale intwo ways. they must support an evergrowing name population, whichmeans that the size of their database keeps increasing very quickly, andthey must serve ever more frequent queries. the growth of the databaseimplies increased memory requirements and an increased managementload.replication, whereby name databases are distributed to multiplename servers, is a way of sharing the load and increasing reliability. withreplication, the root server is able, for example, to provide the addressesof several .com servers instead of one. the volume of name resolutioninquiries could be met by splitting the load across a sufficiently largenumber of replicated servers. unfortunately, current dns technologylimits this approach because the list of the names and addresses of all theservers for a given domain must fit into a single 512byte packet. (evenafter efforts were made to shorten host names, the number of root serversremains limited to 13.) once the maximum number of servers that will fitwithin the singlepacket constraint has been deployed, increased load inthat domain can only be dealt with by increasing the capacity and processing power of each of the individual .com name servers. while theperformance of the most widely used dns software, bind, lags that ofmodern highperformance database systems and root serversõ softwarecan almost certainly be improved to handle much higher loads, internetgrowth rates suggest that the demand on the root servers is likely to begrowing faster than their processing speed is increasing and that in a fewyears the root servers could nonetheless be heavily overloaded.one proposal for addressing issues ranging from scaling to dnsnametrademark conflicts is to move toward a solution that makes use ofdirectories as an intermediate layer between applications and the dns. adirectory might help resolve conflicts between dns names and registeredtrademarks because a particular keyword could be associated with multhe internet's coming of agecopyright national academy of sciences. all rights reserved.64the internetõs coming of agetiple trademarks. it also would help alleviate pressures on the dns byfreeing companies from the necessity of registering separate domainnames for each of their brand names (as well as all possible variants). forexample, procter & gamble would not need to register domains forivory.com and crest.com and so forth to ensure that customers would beable to locate web pages describing these products. directories wouldalso support the association of a particular resource with multiple computers without resorting to clever tricks in the dns server. a directorycan be aware of the source making an inquiry and respond to a query byproviding the address of the nearest server that has the requested information.a number of directory proposals have been floated, many of whichmight prove adequate. the combination of rivalry among proponents ofvarious systems (many of them at least partially proprietary) and theinternet communityõs traditional resistance to changing something that isworkingñalthough only poorlyñis probably responsible for impedingdeployment of any one of these proposals. there is reason to hope thatrising pressure for new capabilities that the dns cannot easily accommodate, such the ability to support nonroman alphabet characters in domain names, could unlock the problem and speed deployment of a directorybased solution that would alleviate scaling pressures.scaling up the address space achieving global connectivity requires not only that every part beinterconnected but also that the constituent parts use common labelsñthenumerical addresses (of the form 144.171.1.26) that permit any connecteddevice to communicate with any other. provisioning these numbers tousers who need them to connect their computers to the internet raises anumber of technical, organizational, and management challenges. thissection briefly discusses organizations and management issues related toaddresses and then focuses on the scaling challenges associated with thecomplexity of internet routing and the threat that a likely explosion in thenumber of devices attached will exhaust the address space. both raisequestions about what technical measures will help alleviate the situationand how such measures can be implemented pervasively. in addition,the challenge of scaling up the address space exemplifies the technicalcomplexity, the interplay between problems and solutions, and the organizational deployment challenges that arise in scaling up the internetinfrastructure overall.the internet's coming of agecopyright national academy of sciences. all rights reserved.scaling up the internet and making it more reliable and robust65managing addressesthe overall concern with the allocation of internet addresses is thatthe address pool may be exhausted by the growing number of users andattached devices. the internet is not the only infrastructure facing scalingchallenges associated with its addresses.13 for traditional telephony, thesechallenges are being addressed in an established (if evolving) context ofglobal and regional numbering administrations and government scrutinyby federal and state regulators. by contrast, addressing functions in theinternet have been provided by a group of organizations with less formalstatus. these include a group of regional registries and the internet assigned numbers authority14 and network solutions (supported by a u.s.government contract). in 1997, the responsibility for network addressallocation in north america was shifted from network solutions to arin,a nonprofit organization funded by north american isps. in 1999, underarrangements coordinated by the u.s. department of commerce to replace and expand on iana, the overall responsibility for management ofaddress space was assumed by icann. under the current rules, regionalregistries such as arin receive large blocks of addresses from icann(formerly iana). they, in turn, distribute smaller blocks of addresses tointernet service providers. customers generally receive their addressesfrom these service providers, though in some instances large organizations are able obtain addresses directly from the registries.15the rules that determine how many addresses are allocated, and towhom they are allocated, are debated within the regional registries, withinicann, and within the ietf. these rules are often contentious, as there isan obvious tension between ensuring that there are enough addresses togo around and meeting the desire of users to be assigned the quantity andtype of address blocks that they feel best meet their needs. addresses areconsidered scarce today, and network managersõ initial requests for address space are often turned down and renegotiated. because the enforcement of address allocation rules has significant consequences, theregistries and icann must obtain and maintain the trust of all the orga13a growing tide of cell phones, fax machines, and second lines for dialup internetaccess have all put pressure on the pool of available telephone numbers. responses to thisdemand have included the allocation of phone numbers to local exchange carriers, implementation of local number portability, and establishment of new area codes.14iana, now incorporated into the new icann, was located at the university of southern californiaõs information sciences institute under the technical leadership of the late isicomputer scientist jon postel, supported by contracts from darpa and nsf.15the assignment of a number of very large address blocks to individual institutions(companies and universities) predates the practice under current rules.the internet's coming of agecopyright national academy of sciences. all rights reserved.66the internetõs coming of agenizations that make up the internet; icann will probably be under pressure to develop appropriate processes, including appeal procedures, tomake the allocation process as fair as possible.routing table scaling and address aggregationthe first part of an internet address, known as the òrouting prefix,ó isused to direct the routing of each packet from the source to the destination end point. just as a telephone number contains a country code, areacode, central office selector, and individual phone portion, ip addressescontain hierarchically organized topological identifiers that identify anaddress as belonging to a particular topological segment of the internet, asubset (e.g., a corporate network) within that region, a local area networkwithin that, and finally an individual interface connected to a particularcomputing device. in contrast to area codes and central office locators,which typically map to particular geographic regions, internet identifiersmap to logical regions which may or may not coincide with geographicalregions. to route packets through the internet successfully, each routermust store a table that provides a mapping between each known routingprefix and the correct routing decision for that prefix. at one extreme,within the network of a customer, this routing table can be very simple,and a òdefault routeó can be set to map all nonlocal prefixes to thecustomerõs isp. at the other extreme, in the backbones of tier 1 providers,the routing table must contain a complete list of all prefixes. today thisrequires tables that hold on the order of 75,000 entries.16as the internet grows, the routing tables are sure to grow, but thelimited capabilities of todayõs routers dictate that this growth must beconstrained. the first and most obvious consideration is that the size ofthe table cannot exceed the memory available in the routers. even thelatest switching equipment used in the internet can only accommodate acouple of hundred thousand routing entries, and older equipment as wellas new, lowend equipment can store many fewer. routers could beredesigned so that their memory could be increased to store more routes,but this is nontrivial. highperformance routing typically requires thatthe routing table be stored either in memory located directly on the chipor in memory connected at very high speed to the chip to avoid delays.while large amounts of memory are widely viewed as something cheap,very fast memory remains expensive and places severe constraints on the16as of may 2, 2000, 76,265 entries had been reported in tony bates. 2000. the cidrreport. available online at <http://www.employees.org/~tbates/cidrreport.html>. theprecise number of entries varies depending on where in the internet the measurement ismade, but this number gives the right sense of scale.the internet's coming of agecopyright national academy of sciences. all rights reserved.scaling up the internet and making it more reliable and robust67hardware design. furthermore, as the internet moves to supporting quality of service and various forms of filtering, router memory will be required for more than just holding routing tables. so despite continuingimprovements in the capabilities of microelectronic devices, the amountof highspeed memory available in routers remains a limiting factor.and router memory is only one of the factors that limit the size of therouting table. each of the routing entries in the table describes the pathfrom the router to a given network corresponding to a particular routingprefix. each entry must be updated in all routers across the networkwhenever that path changes. route update messages have to be carriedthroughout the internet; because each backbone router must maintain thefull table, each must receive and process each update. given that thenetwork is always changing (owing, for example, to installation, reconfiguration, or failure of network elements), the frequency of updates willincrease as the size of the table grows. the updates today are already sofrequent that many routers can hardly keep up, and network operatorshave to resort to òupdate dampingó techniques to limit the rate of updatethat they accept from their peer networks, slowing the rate at which routing information is distributed. recent data show that it often takes several minutes for route updates to propagate throughout the internet.17this results in long transition periods during which the routing tables areincorrect and information cannot be routed to portions of the internet. ifthe size of the table were to be further increased without increasing theprocessing power of the routers, the processing of updates would have tobe further slowed down, and the whole internet could be plagued byrouting failures.the explosive growth of the internet address structure from 1993 to1995 led to a routing table scaling crisis, when it was feared that thecapabilities of routers would be overwhelmed. this pressure resulted inremedial actions starting with the rapid deployment of classless interdomain routing (cidr). cidr controls the number of routes that a routermust remember by assigning addresses in a hierarchical manner thatforces addresses to be assigned in blocks. this involves, for example,internet service providers consolidating address space allocations intofewer, larger routing prefixes, which are then allocated to customers(these, in turn, may include smaller internet service providers) out of theservice providerõs block of addresses. in addition, to reduce pressures onthe size of the global routing table, the address registries were also forced17craig labovitz et al. 1999. analysis and experimental measurements of internet bgpconvergence latencies. presentation to the nanog conference, montreal, october. available online at <http://www.nanog.org/mtg9910/converge.html>.the internet's coming of agecopyright national academy of sciences. all rights reserved.68the internetõs coming of ageto adopt a very restrictive policy whereby small, independently routableaddress blocks would not be allocated to individual organizations. instead, the registries decided to require organizations to obtain addressallocations from their service providers. more recently, tensions surrounding address allocation were heightened when a few large backbone providers, in an effort to force reluctant network operators to aggregate theirrouting information into larger blocks, refused to pass routing information about prefixes that were smaller than a certain size. this causedsome address blocks that had been allocated independently to small providers and companies to lose global connectivity, since the global routingsystem would filter out their topology information and not allow manydestinations on the internet to be able to pass traffic to their networks.these networks generally were reconfigured into larger prefixes and thecrisis was resolved without widespread service outages. the overarchinginterest of the players in maintaining the internetõs interconnection led toselfcorrection of the problem.deployment of cidr, together with the adoption of a restrictive address allocation policy by the registries and the use of network addresstranslation, has contained the growth of the routing tables, and the growthin the global routing table has by and large been slow and linear (figure2.2). note, however, that the most recent data displayed in this figuresuggest that cidr and restrictive allocation policies have not entirelyalleviated pressures on the routing table size and that table size has recently grown faster than linear.whatever pain the new rules created, uncontrolled growth of therouting tables would have caused even greater pain by rendering thewhole internet unusable. without proactive aggregation, the table wouldhave grown exponentially, like the internet, and the routers would havebeen overwhelmed long ago. providerbased addressing also helps eachisp limit the growth in its routing table by eliminating the necessity ofknowing individual prefixes in other service provider domains. it shouldbe noted, however, that the switch to cidr has had a price. when anorganization is given aggregatable addresses by its internet service provider, it must relinquish these addresses and renumber its network if itchanges providers.18 note that this is more an issue of ease of networkmanagement than of portability of identity; the more familiar names bywhich the organization is known to the outside world are portable andcan remain the same when an organization switches isp. large users,18for organizations that outsource management of their dns service to their isp, thereare two barriers to switching: transferring dns functions and renumbering.the internet's coming of agecopyright national academy of sciences. all rights reserved.scaling up the internet and making it more reliable and robust69who had in the past been able to manage their own address space, havefelt constrained by the new rules. the limited availability of address spacewould make it harder to design managementfriendly addressing ruleswithin their network, while the providersupplied addressing rule meantthat they would have to renumber whenever they changed provider.despite the emergence of better tools for renumbering networks, thisremains an involved, expensive operation that may inhibit organizationsfrom seeking better deals from competing providers. as a result, therehave been calls for users to be provided again with portable addresses soas to minimize switching costs. however, because addresses would nolonger be aggregated within the blocks assigned to an isp network, allocating portable addresses in small blocks to small networks would triggera dramatic increase in the size of the routing tables. with the currentstate of routing technology, such a policy could destabilize the wholeinternet.the desire of users, cidr notwithstanding, to retain the ability tomanage their own address space led to the development of a technologyknown as network address translation (nat) (see box 2.2). nat permitsusers to use and manage a large amount of private address space independent of the allocation policies of the registrars, giving the networkfigure 2.2 border gateway patrol (bgp) route advertisements as reported bytelstra in april 2000, showing an overall linear increase in the routing table sizebut with a recent upward inflection in 19992000. source: geoff huston. 2000.bgp table size. technical report, telstra corporation, ltd., canberra, australia.data from april 12. available online at <http://www.telstra.net/ops/bgptable.html>.the internet's coming of agecopyright national academy of sciences. all rights reserved.70the internetõs coming of agebox 2.2 network address translationnetwork address translators (nats) are devices that sit between a privatenetwork and a network connected directly to the public internet. they may bestandalone devices (sometimes referred to as nat boxes) or included along withother capabilities in computers that serve as gateways between internal networksand the internet. in a small office/home office environment, they may run on ageneralpurpose computer that does double duty as the local area networkõs gateway to the internet. nats enable edge networks in homes, companies, and otherorganizations to use private address spaces and share a smaller set of globaladdresses among devices using those private addresses on an asneeded basis.nat uses a set of global addresses to give devices on a private network awindow on the global internet. in essence, nats work by detecting what appearsto be a data flow from a machine within the private network (e.g., a tcp/ip connection) and allocating one of the external ip addresses on demand to support theflow. of course, if many machines within the private network are active at anygiven time, that alone does not conserve many ip addresses. generally, in addition to the mapping between private and global addresses, nats perform anothermapping that makes use of port numbers, which are, in effect, another set of addresses that are local to each machine. many of these ports are associated withparticular services (e.g., web servers listen by default on port 80), while others areassigned on demand. because each data flow has not only an address but also aport associated with it, a single global address and group of ports can be mappedto a set of individual local addresses and single ports. when nats perform thisportnumber multiplexing, a smaller set of global addresses is able to serve alarger number of machines with local addresses.11for an overview of nats and a more indepth discussion of the technical nuances, see p.srisuresh and m. holdrege, eds. 1999. ip network translator (nat) terminology and considerations, rfc 2663. internet engineering task force. available online from <http://www.ietf.org/rfc/rfc2663.txt>.manager great latitude in assigning addresses because he or she need notworry about doing so in an efficient manner, unless the private network isso large as to push up against the limit set by the size of the privateaddress blocks. the expectation was that nat use would be a shorttermphenomenon that would be obviated by the deployment of a nextgeneration internet protocol, ipv6. but nat had an unintended side effectñtheexplosion of private addressing. this widespread use had the effect ofletting the wind out of ipv6õs sails, as the perception of crisis requiring awholesale replacement of the internet protocol faded. instead, it began tobe debated whether ipv4 needs to be replaced at all.efforts to improve the aggregation of addressing notwithstanding,the internet's coming of agecopyright national academy of sciences. all rights reserved.scaling up the internet and making it more reliable and robust71the number of address blocks that must be stored in the core routers of theinternet can be expected to continue to grow, suggesting that new approaches to routing may be important in the future. specific problemsthat deserve exploration include modifications to the current global routing scheme (border gateway protocol, or bgp) to better support controlled peering. further, while there is by no means consensus within theinternet community on this point, there are some who view routing concerns as stemming fundamentally from the approach taken in todayõsinterdomain routing protocol, bgp4. routing in bgp depends on eachbackbone or backbone node having a complete and fairly precise pictureof the internet. as the internet grows, this need to have complete knowledge of the system presents scaling problems. many other complex problems, such as ambulance routing, airplane routing, and chess games, havebeen solved using an approach where in place of complete knowledge,one starts things off in the right general direction, looks ahead a limiteddistance at each step, and keeps adjusting as the objective comes nearer.such algorithms have much better scaling properties in very large, complex systems than completeknowledge approaches.running out of addresses?in addition to scaling issues raised by constraints on the structure ofaddresses and their allocation, there exist concerns about running out ofnumbers altogether. the present internet protocol, ipv4, provides 32bitlong addresses, which translates into an address òspaceó of about 4.3billion unique addresses. for historical reasons, about seveneighths ofthis address space is available for use as addresses for connecting devicesto the internet; the remainder is reserved for multicast and experimentaladdresses. while more than 4 billion addresses was considered morethan sufficient in the internetõs early days, widespread internet adoptionmeans that this seemingly large number is becoming a constraint. thereis evidence today of pressures on the address space, but using this evidence to predict longterm trends is difficult. clearly, while there hasbeen no crisis thus far, there is still considerable risk associated withexhausting the ipv4 address space, particularly over the longer term. butjust how long is òlong termó? estimates vary wildly, from òneveró to 10to 20 years to as few as 2 or 3.estimating address use and demandhow much of the ipv4 address space is used? huitema reports thatabout half of the address space has now been allocated by the addressregistries, which along with the rate of address allocation, suggests thatthe internet's coming of agecopyright national academy of sciences. all rights reserved.72the internetõs coming of ageexhaustion is relatively near.19 however, not all the addresses that havebeen allocated are in active use. another measure is the sum of the size ofall the blocks of addresses (addresses with a common prefix) that aremade known (òadvertised,ó in internet lingo) through bgp to the routersthat lie within the core of the network. nlanr reports that 22 percent ofthe address space is advertised in bgp. this number is smaller than theallocation figure would indicate, reflecting the fact that not all allocatedaddresses are in use. the nlanr data also indicate relatively rapidconsumption: the advertised address space increased by roughly 6 percent from november 1997 to may 1999.20 interpretation of these data isfurther complicated because not all the addresses that are advertised areactually assigned to an active host. a provider using an address block fora set of devices will generally advertise the whole block (since there is nocost to it in doing so, and doing otherwise would result in many morerouting table entries) and then assign the unused addresses from theblock as necessary. in most cases, isps do not reveal whether they havemany or few unused addresses in their active blocks, an uncertainty thatconfounds determination of the immediacy of address space pressures.21the advertised address space reported by nlanr is sufficient toaddress over 960 million hosts. how does this figure compare to what wecan learn from other sources of information about the number of internetusers and computers attached to the internet? the computer industryalmanac estimates that in the year 2000 there are about 580 million computers in use worldwide. 22 if all of the 580 million deployed computerswere attached to the internet all the time, we would surely exhaust atleast the advertised addresses. this is because the addresses are allocatedin a hierarchical fashion, in fairly large blocks. each computer must therefore be assigned an address from a block associated with the part of theinternet it is attached to, so that not all 960 million addresses could, inpractice, be used.data on computers attached to the internet suggest a much smallernumber. estimates from telcordiaõs netsizer project, for example, are19christian huitema. 1999. evaluating the size of the internet. technical report, telcordia.available online at <ftp://ftp.telcordia.com/pub/huitema/stats/global.html>.20national laboratory for applied network research (nlanr). 1999. 18 months in therouting life of the internet. technical report, measurement and network analysis team,nlanr. university of california at san diego, may 27. available online at <http://moat.nlanr.net/ipaddrocc/18monthsequel/>.21for an examination of the inherent inefficiencies in addressing, see c. huitema. 1994.the h ratio for address assignment efficiency, rfc 1715. internet engineering task force.available online at <http://www.ietf.org/rfc/rfc1715.txt>.22data obtained from <http://www.cia.com/>.the internet's coming of agecopyright national academy of sciences. all rights reserved.scaling up the internet and making it more reliable and robust73that there were more than 86 million active internet hosts23 in july 2000.the internet software consortium estimates that there were over 72 million hosts in january 2000 (the telcordia data show a slightly smallernumber for that date).24 if these numbers are accurate, fewer than 10percent of the addresses within the advertised blocks are actually in usetoday, suggesting that pressures are, on average, not as great as one mightconclude based on other measures.another relevant piece of information is the number of internet users.according to nua internet surveys, there were an estimated 332 millionusers online as of june 2000.25 today this number exceeds the number ofactive internet hosts. if each of these users were to have an active connection to the internet, the resulting consumption of addresses would be asignificant issue. the ratio of users to hosts today is greater than 1,reflecting the fact that not all computers are attached to the internet at anygiven time (especially dialup connections) and that many computers areshared by more than one individual, particularly in the developing world.however, both of these factors are likely to lessen with time, and it isreasonable to project that in the long run the ratio of users to hosts willdrop below 1, meaning that there will be more hosts than users.latent demand further complicates the prediction of address consumption rates. available data reflect the outcome of the present addressallocation regime, in which addresses are tightly rationed for fear of depleting the address space. consequently, the address consumption statistics, even to the extent that they are accurate, do not reflect the actualdemand for addresses. nor do they reflect the degree of hardship experienced by customers as a result of this rationing. unique addresses wouldprobably be more widely used if they were not so tightly rationed, buthow much more widely is a matter of speculation.a further challenge is that todayõs rate of consumption may greatlyunderestimate demand in the future. one source of potentially largegrowth is the predicted emergence of many new ip devices.26 for example, it is reasonable to assume that within a few years individuals whocurrently have one or two ipaddressable devices will have five or ten23data obtained from <http://www.netsizer.com/daily.html>.24data obtained from <http://www.isc.org/ds/www9907/report.html>.25data from <http://www.nua.ie/surveys/howmanyonline/index.html>.26various forecasts project that the number of such networked devices will vastly exceedthe number of individual internet users within the next decade. see, for example, frankgens. 1998. death of the pccentric era, international data corporation (idc) executiveinsights, technical report. framingham, mass.: idc. abstract available online at <http://www.itresearch.com/alfatst4.nsf/unitabsx/w16276?opendocument>.the internet's coming of agecopyright national academy of sciences. all rights reserved.74the internetõs coming of age(considering pagers, phones, pdas, etc.). the need for ipaddressabilitywill also increasingly extend beyond humanheld and operated devices,to devices that are embedded in our physical infrastructures. while nationwide deployment of such systems would take many years, the next 3to 5 years could see explosive growth of this sort of instrumented environment in office buildings, factories, hospitals, etc.27 in the longer term(beyond 5 to 10 years), the size of the realizable devices will continue todecrease to the point where we might have hundreds or thousands ofdevices per person. some will be freestanding devices while others willbe embedded in the userõs environment.current research and development efforts on the part of governmentand industry are addressing a number of technical issues such as powerconsumption and wireless communication; progress on these issues willhelp propel deployment of new networked devices. the ieee 802.11standard for wireless local area networks and other efforts to standardizephysical layer communications are evidence of the readiness of the technology and market.not all devices need to be individually and globally addressable andnot all ip addresses in use are public (i.e., visible in the global addressspace). these private address spaces are used for a variety of reasons,including reducing the number of public addresses required and enhancing security.28 in many instances where the system is òclosedóñsuch aswhen a number of computers operate elements of a manufacturing facilityñit may be perfectly adequate to give individual devices local, networkspecific addresses. data transfer into and out of a system or facilitycan be mediated by a special computer that acts as a gateway or externalinterface between a group of computers and the outside network.the number of computers currently assigned private addresses couldbe a significant factor in estimating future demand for global addresses.many provide access to the internet in some way, but most often througha private address space indirectly attached to the internet. because anydevice with a private ip address may, with appropriate modifications to anetworkõs connectivity to the public internet, be connected to the internet,27more examples of possible applications and environments: medical (operating rooms,hospital beds, injected inbody monitoring), scientific (environmental monitoring, physiological data collection), office and home (tracking and coordinating people and objects,implementing sophisticated security systems), and industrial (factory floors, hazardous operations, distributed robotics).28the security afforded by nat is limited. in particular, it suffers from all of the limitations associated with perimeter defensive measures, as discussed later in the chapter.the internet's coming of agecopyright national academy of sciences. all rights reserved.scaling up the internet and making it more reliable and robust75its owner may at any time seek an address for it on the public internet.this latent demand is found in home networks as well as the networks oflarge organizations. in the home, some users today share a single providersupplied ip address among multiple home devices using nat running on a home gateway (which may be in the form of software runningon one of the computers in the home).29 however, many computers willbe used in applications that require devices to be capable of externalinterrogation in a flexible manner and thus not via explicitly configuredgatewaysñmeaning that they will need to be assigned unique global ipaddresses.international pressures for addressesaddress availability and consumption rates are not uniform fromcountry to country. demand from regions of the world where internetuse can be expected to continue to grow steeplyñnot surprisingly, theseare often places that historically have not had many computers connectedto the internetñcould rapidly exhaust their existing global address spaceallocations. forecasts for the next decade suggest, for example, that although there are relatively few computers and users in china today, bythe end of this decade that country may have more computers and userson the internet than the united states.30 the present disparity in addressallocation is illustrated by the observation that a number of organizationsand businesses, including several u.s. universities, were each allocated224, or 16,777,216, addresses many years ago under a different regime(about 40 class a networks were allocated that way), whereas the 1.3billion people in china have far fewer. because of the shortage of addresses, stanford university undertook a project, completed on may 1,2000, to consolidate its use of addressees for its roughly 56,000 computersand return the more than 15millionaddress block for reassignment by29ip address scarcity as seen by home users is not just a reflection of an overall shortageof addresses. many isps restrict residential customers to a single ip address on the assumption that a site that needs more than one address is really a commercial site, for which ahigher rate can be charged.30for example, a 1999 study from bda (china ltd.) and the strategis group projects thattotal internet users in china could exceed 33 million by the end of 2003. (see prc information technology review. 1999. òinternet users in china to reach 33 million by 2003.ó1(26), july 2.) more recently, the boston consulting group forecast that, based on projectedgrowth of 25 to 35 percent per year, the number of internet users in asia is likely to growmore than fivefold, to nearly 375 million, by 2005. (see agence france presse. 1999. òchinato propel asian internet users to 375 million in 2005.ó november 2.)the internet's coming of agecopyright national academy of sciences. all rights reserved.76the internetõs coming of agethe organization that allocates addresses in north america.31 the comparison between addresses assigned to these organizations and chinaprovides an extreme example of the dominant factor behind these allocation disparitiesñaddress assignments reflect needs that the requestingorganization has been able to substantiate on the basis of current use orthrough credible projections of future needs and reflect the overall availability of addresses at the time that the assignment is made.network address translationas described above, cidr and nat were adopted to alleviate address scalingrelated challenges. in addition to offering capabilities forlocal address management, nat enables reuse of global addresses. thetechnology is widely employed and is included in a number of currentand planned operating system releases.32 not just a technology used bylarger organizations, nat is being used in home and small office networks to allow a single ip address connection (e.g., through a dialup,dsl, or cable modem connection) to be shared by multiple computers onthe home network.however, the nat approach has significant shortcomings. to startwith, the growing support for nat and natlike facilities delivers thewrong message to anyone trying to resolve the address space shortage bydeploying ipv6 technology rather than nat. nat is viewed by many asa rather ugly, simplistic fix that is cheaper to deploy (something thatmight be referred to as a òkludgeó) than an architectural model backed bylongterm analysis. nat devices, which are computers where large numbers of data streams come through in one place, also are attractive targetsfor maninthemiddle attacks that can listen in on or redirect communications. nat is also not a satisfactory solution for some very large networks because the size of the address blocks designated for use in privatenetworks (i.e., blocks of ipv4 addresses that are not allocated for globaladdresses) is finite. these blocks would be, for instance, too small topermit an isp to provide unique addresses for each of potentially tens ofmillions of customers (e.g., settop boxes or wireless internet phones),although some isps, such as chinaõs uninet, have been forced to make31see networking systems staff. 2000. ip address changes at stanford. stanford university. available online at <http://www.stanford.edu/group/networking/ipchange/> andcarolyn duffy marsan. 2000. òstanford move rekindles ônet address debate.õó networkworld, january 24. available online at <http://www.nwfusion.com/news/2000/012ipv4.html>.32for example, both windows 98 se (as part of the internet connection software) andwindows 2000 include nat functionality.the internet's coming of agecopyright national academy of sciences. all rights reserved.scaling up the internet and making it more reliable and robust77use of nats to cope with address shortages. as an architectural approach, nats are seen by some as selfcontradictory. nats have theadvantage that they provide some degree of security by hiding privateaddresses behind the address translator, but the protection afforded islimited. another difficulty with the model is that it presumes that theinternet is limited in design to a finite set of edge domains surrounding asingle core network, each of which contains a number of machines sittingbehind a nat. this model is inadequate because there are, in fact, manyparts to the internet core.another difficulty is that because nat requires devices to in effecthave two addressesñthe external address (and port number) seen by theglobal internet and the internal address (and port number) used to connect to the device within the local networkñit breaks the transparency ofthe network. this difficulty is discussed in detail in chapter 3; the essential point is that globally unique addresses are an important design attribute of the internet and that applications are supposed to be able to relyon them without any knowledge of the details of the network betweensource and destination. if a nat is being employed, the address thedevice knows about (the local address) will differ from the address bywhich the device is known in the global internet (the external address).also, the mapping between local and external address is dynamic, depending on the actions of the nat.ipv6: a potential solution to addressing and configurationthe specter of an address shortage drove the development of ipv6, arecommended followon to todayõs ipv4 protocol. the internet engineering task force (ietf) produced specifications33 defining a nextgeneration ip protocol known variously as òipng,ó òip version 6,ó or òipv6.óipv6 was designed to improve on the existing ipv4 implementation, tackling the ip address depletion problem as well as a variety of other issues.its major goals were the following:¥expand addressing capabilities, eliminating the ip address depletion problem;¥incorporate qualityofservice capabilities, particularly for realtimedata;¥reduce the processing time required to handle the òtypicaló packetand limit the bandwidth cost of the ipv6 packet header;33s. deering and r. hinden. 1998. internet protocol, version 6 (ipv6) specification. rfc2460. december. available online from <http://www.ietf.org/rfc/rfc2460.txt>.the internet's coming of agecopyright national academy of sciences. all rights reserved.78the internetõs coming of age¥provide routing improvements through address autoconfiguration,reduced routing table sizes, and a simplified protocol to allow routers toprocess packets faster;¥provide privacy and security at the network layer supporting authentication, data integrity, and data confidentiality;¥improve network management and policy routing capabilities;¥allow a transition phase by permitting the old (ipv4) and newprotocol to coexist;¥allow the protocol to evolve further by imposing less stringentlimits on the length of options and providing greater flexibility for introducing new options in the future; and¥improve the ability of the network manager to autoconfigure andmanage the network.the request for proposals for a nextgeneration internet protocol wasreleased in july 1992, and seven responses, which ranged from makingminor patches to ip to replacing it completely with a different protocol,had been received by yearõs end. eventually, a combination of two of theproposals was selected and given the designation ipv6. the essentialimprovement offered by the new protocol is the expanded address. it is16 bytes long with 61 bits allocated to the network address, compared tothe 32 bits provided by ipv4. ipv6 addresses support billions of billions ofhosts, even with inefficient address space allocation.the new standard offered a number of additional features, includingthe following:¥increased efficiency and flexibility. some ipv4 header fields weredropped, so that the header now contains only 7 mandatory fields asopposed to the 13 required in ipv4. ipv6 also offers improved support forextensions and options.¥enhanced autoconfiguration. ipv6 offers improved autoconfigurationor negotiation of certain kinds of addresses. possible approaches include(1) using the ethernet or other media address as the òhost selectoró part ofan address otherwise learned from the local router, (2) procedures forcreating local addresses on pointtopoint links, and (3) improvements tothe dynamic host configuration protocol (dhcp) that may allow partialautoconfiguration of routers.3434recently, there have been concerns that using a unique identifier as part of the ipv6address would facilitate the tracking of users on the networks and could therefore pose aprivacy risk. this risk is in fact inherent in any system where the connection to the internetis always òon,ó a characteristic of most broadband services. this concern, while widelyexpressed, is not necessarily valid; there exist ways to limit the ability to isolate addressesthe internet's coming of agecopyright national academy of sciences. all rights reserved.scaling up the internet and making it more reliable and robust79¥enhanced support for quality of service (qos). qos functionality ispotentially enhanced through the addition of a new trafficflow identification field. qos products based on this flow identifier are still in theplanning stage.deploying an ipv6 solutionthe technical viability of ipv6 has been demonstrated by a number ofimplementations of ipv6 host and router software as well as the establishment of a worldwide ipv6 testing and preproduction deployment network called the 6bone, which reaches hundreds of academic and commercial sites worldwide. however, to date, ipv6 has not seen significantdeployment and use. this is not a surprise; ipv4 has been adopted by avery large number of users, and there are significant costs associated witha transition to ipv6. reflecting their perception that the gain for switchingto ipv6 is not sufficient to justify the pain of the switch, customers havenot expressed much willingness to pay for it, and equipment vendors andservice providers are for the most part not yet providing it. an importantexception is the planned use of ipv6 for the socalled thirdgenerationwireless devices now being developed as successors to present mobiletelephone systems.35for many, the devil they know is better than the one they donõt know.until an address shortage appears imminent, incremental stepsñrestrictions on address growth or use, or the use of nats or some other (painfulbut understood) kludgeñwill appear less painful than switching to ipv6.indeed, some believe that address exhaustion is not a problem meritingdeployment of a new solution, arguing that cidr makes the internetõscore a stable enough addressing domain that private address spacesaround the periphery of that domain can effectively serve demands forthe foreseeable future. on the other hand, as discussed in the previoussection, this approach could have serious drawbacks: an internet in whichnats become pervasive is an internet in which the assumption of transparency no longer holds (see chapter 3 for more on this issue). moreover,to machines, and since many machines are used by more than one person, the identifierdoesnõt track back to a person. full protection requires the use of explicit services, such asanonymizing relays. the specific ipv6 problem has been addressed by the ietf, which hasdevised a way to derive the address from the unique identifier without exposing the identifier itself.35see, e.g., nokia. 2000. nokia successfully drives forward ipv6 as the protocol for future3g networks. press release, may 26. available online at <http://press.nokia.com/pm/782371.html>.the internet's coming of agecopyright national academy of sciences. all rights reserved.80the internetõs coming of ageit ignores the needs of potentially large consumers for address space fordevices such as ipbased mobile telephones.transition to ipv6 requires the development and deployment of newnetworking software (internet protocol stacks) on all the devices that areconnected to networks running ipv6, updated software (and even hardware) in routers within these networks, and translation gateways betweennetworks running ipv4 and ipv6. also, many applications running oncomputers connected to ipv6 networks will probably also need to beadapted to use the longer address. desktop support alone is not enoughto trigger industry adoption, because ipv6 is not something that can takeplace as a result of actions taken at the edges of the network alone.36implementations of many of the requisite elements exist today; the hurdleis largely not one of developing the required technology but of makingthe investments needed to deploy it.there are various strategies by which ipv6 could be deployed. noone believes that a scheduled, òflagdayó transition would be effective,given the absence of incentives for players to comply and, worse still, theprospect that an abrupt shift to ipv6 could go awry catastrophically. (noone in the industry wants to wake up to news reports like òmajor ispbackbone melts after messing with ipv6.ó) the difficulty of making aswitch has long been recognized, along with the idea that deploymentwill need to be incremental. transition is complicated because an ipv4only system cannot communicate directly with an ipv6 system, meaningthat either the ipv6 device must also speak ipv4 or it must have a translator. a transition period is foreseen in which ipv4/ipv6 translators (somewhat like nats) sit between systems running different protocol versions,enabling a piecemeal deployment of ipv6 from the internetõs edge towardits core, and a model known as ò6 to 4ó is emerging in which every ipv4site automatically inherits an ipv6 prefix with the ipv6 traffic encapsulated in ipv4.finally, it should be noted that many of the addressing issues discussed here arise, in part, from routing scaling considerations. cidr wasa measure to control the number of routes that a router must remember.it assigns addresses in a hierarchical manner that forces them to be assigned in blocks (and also forces most internet subscribers to obtain theiraddresses from their isp). it is, however, viewed as a burden for thesubscriber, because it means that to switch from one isp to another, it isnecessary to renumber all the attached machines. with nat, this is notan issue, because subscriber machines are assigned private addresses,36another example of this is multicast, which, although it is supported in major operatingsystems such as windows, is not widely used.the internet's coming of agecopyright national academy of sciences. all rights reserved.scaling up the internet and making it more reliable and robust81and only the single addresses of the nat boxes need to be changed, amuch smaller number. however, if ipv6 were to be deployed and allmachines were to revert to the original architectural model of globallyunique, public addresses, this problem would resurface on a large scale.while it would not be fully automatic, ipv6 would provide for muchsimpler renumbering. still, while the current situation is generally adequate, the difficulties associated with assignment and renumbering suggest that continued research on new approaches to addressing and routing would be worthwhile.reliability and robustnessthe proliferation of devices and applications discussed above is symptomatic of growing use and an expectation that the internet can be depended on. this expectation translates into a requirement for greaterreliability, robustness, and predictability. as more and more critical functions are moved to the internet, there is increasing pressure for reliableand continuous service, even when things go wrong. as the uses of theinternet expand, the stakesñand thus the visibility of reliability and robustness concernsñwill rise.there are also public safety considerations, some of which will derivefrom expectations associated with conventional telephony; telephone users expect a high level of availability for normal calling and demandavailability for making emergency (911) calls. it is reasonable to assumethat such expectationsñincluding, for instance, the capability for automatically notifying authorities of the geographical location of a 911callerñwill transfer to internet telephony services. as internet use becomes more widespread, it is conceivable, or even likely, that other, newlifecritical applications will emerge. for example, internetconnectedappliances could be used to monitor medical conditions, e.g., remote ekgmonitoring.37concerns about the internetõs robustness and vulnerability to attackare reflected in the attention given to these matters by the national security/emergency preparedness (ns/ep) community. this community traditionally relied on the pstn for 911 emergency services and priorityrestoration and service delivery during designated crises. it promulgatesguidelines that are then codified in regulations and builds on years ofexperience and personnel training as well as the cost structures of regu37computer science and telecommunications board (cstb), national research council(nrc). 2000. networking health: prescriptions for the internet. washington, d.c.: nationalacademy press.the internet's coming of agecopyright national academy of sciences. all rights reserved.82the internetõs coming of agelated telephony. ns/ep organizations, with public and private sectorelements, have expanded their missions and compositions to embracenontelephony service providers and the internet, and they have begun tostudy and make recommendations regarding the internetõs robustness.38discussions between the ns/ep community and isps are in their earlystages, and what form any agreements between the two would take remains to be seen.designing for robustness and reliabilitywhile the terms are often used interchangeably, reliability, robustness, and predictability refer to different aspects of the communicationsservice. robustness refers to the ability of a system to continue to provideservice even under stress (e.g., if some of the systemõs components orcapabilities are lost to failure or it is subject to malicious attack.) a robustsystem is, essentially, one that is not subject to catastrophic collapse; instead, it degrades gracefully, providing the level of service that would beexpected from the available resources. reliability is a measure of whethera system provides the expected level of service for example, a systemdesigned for 99.999 percent reliability would fail not more than 5 minutesper year. reliability is typically achieved by the combination of component reliability, component redundancy, and a robust system design. animportant distinction between robustness and reliability is that while arobust system typically provides a reliable service, a reliable system neednot be robust. for example, many companies run certain vital functionson a single computer that is very well maintained. this computer and itsservices are reliable (because the system is carefully maintained, the service is almost always available) but they would not be considered robust(if the computer fails, the service is not available). predictability refers tothe userõs expectations about the availability and quality of service routinely being met. while a service can, in theory, be predictably poor (e.g.,telephone service in some developing countries), when most users speakof a predictable service they mean a reliable service whose occasionalperiods of degraded service can be anticipated (e.g., the difficulty of making longdistance phone calls on motherõs day due to the volume of calls).the internetõs design is based on a world view different from that ofthe pstn on how to provide the qualities of reliability, robustness, andpredictability. the choice to base the internet on richly interconnected38see, for example, national security telecommunications advisory committee(nstac). 1999. network group internet report: an examination of the ns/ep implications ofinternet technologies. washington, d.c.: nstac.the internet's coming of agecopyright national academy of sciences. all rights reserved.scaling up the internet and making it more reliable and robust83components stems from two observations that have their origins in research in the early 1960s.39 first, although the pstn depends on thereliability of its components, neither the overall system nor any of itscomponents are perfectly reliable. second, with the right design, thereliability of a network does not depend on all its components working; itonly requires that there are enough working components to connect anytwo end points. each component is itself reasonably reliable, but thedesign relies on the networkõs ability to adapt, i.e., to reconfigure itselfwhen a component fails so as to make efficient use of the remainingcomponents. consequently, it is able to recover from local outages ofcomponents by simply requiring the computers communicating across itto retransmit the messages that were in transit at the time of the failure.the result is a robust service built of components that are far less reliablethan those of the pstn.in contrast, the pstnõs design emphasizes building a network out ofhighly reliable components and using them reasonably well. the pstnachieves its reliability in part by working very hard to make sure thatevery important component of the pstn is, in and of itself, reliable. theresult is a system that may not be robust in all circumstances, because if acritical component of the pstn fails, the system fails. typically thesefailures are minor (e.g., a few phone calls get prematurely disconnected)but some can be spectacular (e.g., the multiday loss of phone service as aresult of a fire at a central office in hinsdale, illinois, in 1990). also, in thepstn the viability of a given call is viewed as secondary to the reliabilityof the total infrastructureñindividual calls need not be reliable in aninfrastructure failure. rather, user applications are required to be robustin the presence of failure, and the pstn concerns itself with restoringservice quickly when an outage occurs so that the application can restart.following service restoration, applications restartñthe caller places a newcall, and interrupted file transfers restart from the beginning or from asaved checkpoint. any resource optimization that might result from dynamic rerouting is viewed as secondary to the predictability of the callitself. the routing path for a connection through the pstn is establishedat call time; this routing does not change during the life of a call.while the internet is sometimes unreliable, it generally operates asthe underlying design presumed it would. there are a number of reasonswhy the internet is viewed as unreliable despite its robust design; thediscussion below outlines several of these as well as possible remedies.39see, for example, leonard kleinrock. 1964. communication nets: stochastic messageflow and delay. new york: mcgrawhill.the internet's coming of agecopyright national academy of sciences. all rights reserved.84the internetõs coming of agevulnerability of the internet to attackthere is increasing concern about deliberate attempts to sabotage theoperation of the internet. recent reports, including the 1999 cstb reporttrust in cyberspace40 and a national security telecommunications advisory committee report,41 identified a number of vulnerabilities of theinternet (as well as of the pstn). these vulnerabilities are associatedwith failures of hardware and software as well as the systemõs susceptibility to malicious attack. indeed, there are a number of points of potential massive vulnerability in todayõs internet, including the integrity ofthe addresses being routed, the integrity of the routing system, the integrity of the domain name system, and the integrity of the endtoend application communication (box 2.3).considerable attention has been devoted to the internet as part of thelate1990s examination of the nationõs critical infrastructure. in 1997, thepresidentõs commission for critical infrastructure protection issued a report42 highlighting concerns about the vulnerability of several infrastructure sectors, including information and communications; it found both agrowing dependence on and vulnerability associated with such infrastructures. the next year, 1998, saw the issuance of presidential decisiondirective 63, which focuses on protecting critical infrastructure againstboth physical and cyber attack, and the establishment of a national infrastructure protection center, located in the department of justice, which isindicative of law enforcementõs growing attention to the problem of malicious attacks on infrastructure, including the internet.the network also threatens the security of users because it can serveas the conduit for a variety of attacks against their systems. variousforms of attack exploit weaknesses in enduser systems, allowing an unauthorized attacker to read or manipulate information in the systems.some attacks involve breaking into a system for the purpose of obtainingor modifying its data. fraudulent entries might be made in accountingfiles, pictures on web pages might be replaced with others that the attacker finds humorous, or sensitive information might be accessed. or,an attack might simply overwrite (destroy) data. a maninthemiddle40computer science and telecommunications board (cstb), national research council.1999. trust in cyberspace. washington, d.c.: national academy press.41national security telecommunications advisory committee (nstac). 1999. networkgroup internet report: an examination of the ns/ep implications of internet technologies. washington, d.c.: nstac, june.42presidentõs commission for critical infrastructure protection (pccip). 1997. criticalfoundations: protecting americaõs infrastructures. washington, d.c.: pccip. available onlineat <http://www.pccip.ncr.gov/reportindex.html>.the internet's coming of agecopyright national academy of sciences. all rights reserved.scaling up the internet and making it more reliable and robust85attack does the same thing from the vantage point of some intermediatesystem that has access at some point between the sender and receiver(e.g., a router or gateway). other attacks do not target the data but attackthe resources for its transportña denialofservice attack might be implemented simply by filling a communications link with junk messages, consuming much or all of the available bandwidth, thereby slowing or denying its intended use. attacks can be launched against the systems ornetworks of particular isps or end users or more broadly against theinternetõs infrastructure.43 the internet also serves as a conduit for therapid transmission of virusesñcomputer code that can alter or destroydata in computer systemsñwhich are readily attached to files and sent byemail to users of a network.44 as new viruses continue to be inventedand distributed, techniques for detecting and mitigating their effects mustbe developed as well.the threat posed by internet vulnerabilities is magnified by severalcoincident developments. tools to exploit internet vulnerabilities arereadily disseminated. they can be distributed to a large number of technically unsophisticated users, some of whom may not even be aware of theimplications of using them. also, as an increasing number of users accessthe internet through broadband services, their machines become platforms for launching attacks, such as denial of service, that are more effective when the attacker has a highspeed connection to the internet. (thisis somewhat compensated for by the even greater speed used by isps andlarge servers, which renders saturation attacks more difficult simply because it is harder to completely congest the target computerõs communications link.) at the same time, the alwayson systems of broadbandusers become targets for attackers and may be captured and used, unbeknownst to their owners, as relays for further attacks on the network. insuch an environment, isps have to take steps to protect their own networks as well as the customers connected to it. the types of attacksdiscussed here are varied; while those who conduct the attacks may beunsophisticated, those who develop the capabilities are resourceful andcreative.while a number of measures can be implemented within the network43for example, the routing infrastructure itself might be attacked by hijacking a tcpconnection or inserting incorrect routing information into the system. or, the name serviceinfrastructure could be attacked on a global basis by launching a denial of service againstthe domain name systemõs root computers.44in 2000, several viruses disseminated as email attachments spread widely and receiveda great deal of media attention. most prominent was the òi love youó virus, which spreadby exploiting a vulnerability in the microsoft outlook email software.the internet's coming of agecopyright national academy of sciences. all rights reserved.86the internetõs coming of agebox 2.3 internet vulnerabilitiesa recent computer science and telecommunications board report, trust incyberspace,1 examined vulnerabilities associated with the internet. these fell intoseveral categories: link failures, congestion, operational errors, software and hardware failures, and malicious attacks. a 1999 report by the national security telecommunications advisory committee (nstac) also examined the vulnerabilitiesin the context of relying on the internet for national security/emergency preparedness activities.2 the following compilation is drawn primarily from the cstb report, supplemented by the nstac report:¥link failures. link failures, most commonly caused by damage to buriedcables by construction crews, have been cited as the biggest cause of public telephone network outages;3 they also have implications for internet reliability. theinternet was in fact specifically designed to tolerate this sort of component failure;routing by its very nature seeks out alternative paths. in practice, however, theability to respond is limited. there simply may not be enough capacity elsewherein the network to carry the traffic. also, routing may not adapt quickly enough toprevent packet loss or delays in packet delivery. to prevent instabilities and oscillations that might occur in the event of transient failures, routing algorithms aredesigned to not respond immediately to reports of communication link failures. inaddition, to provide increased stability, particularly in the face of possible attacks orconfiguration errors by other network operators, isps may rely on static configuration of major routes across network boundaries. these require explicit interventionto respond to some link failures.¥congestion. congestion may be just a result of increased demand or it maybe a result of failures elsewhere in a network or service denial attacks such astraffic flooding. the internet is not able to manage information associated with1computer science and telecommunications board (cstb), national research council(nrc). 1999. trust in cyberspace. washington, d.c.: national academy press.2national security telecommunications advisory committee (nstac). 1999. networkgroup internet report: an examination of the ns/ep implications of internet technologies.washington, d.c.: nstac, june.3network reliability and interoperability council (nric). 1997. final report of the networkreliability and interoperability council. washington, d.c.: federal communications commission, july 15.to enhance robustness, the internetõs architecture, in which service definition is pushed to the end systems connected to the network, requiresmany security issues to be addressed at the edges of the network. boththe performance of individual applications and services and the overallintegrity of the network are therefore placed in the hands of these endsystems. as more and more end systems with widely varying degrees oftrustworthiness are added to the network, the more tenuous the assumpthe internet's coming of agecopyright national academy of sciences. all rights reserved.scaling up the internet and making it more reliable and robust87tion of mutual trust becomes. thus, absent the adoption of new measures both within the network as well as at its edges, security violations ofvarying degrees of severity can be expected to continue. in particular, thedependence on end systems means that much of the security burden restson vendors of applications and operating system software as well as onthe users that configure and operate those systems.concerns about these issues have led to investment in research aimedspecific users, connections, and sources/destinations. routers, which are notdesigned to capture such information, can only resort to queuing, marking, or discarding packets when their buffers start to fill; the network relies on mechanismssuch as tcp by which the sender and receiver adapt on an endtoend basis tocongestion within the network. thus, applications that do not behave in thisfashion, whether for performance or malicious reasons, can create persistentcongestion.¥operational errors. like other complex systems, the internet is subject to avariety of operational errors. trust in cyberspace cites a number of examples ofoperational errors that have occurred in the internet, including errors in storinginformation in dns databases and routing tables.¥software and hardware failures. data about failures due to router outagesdo not seem to have been collected. what can be said, according to trust incyberspace, is that devices at the edges, which are generally ordinary computersrunning commercial operating systems, are the most vulnerable. on the hardwareside, one major issue is the availability of electrical power. many parts of theinternet have not been designed to the same standard for robust power supplies;the pstn, in contrast, has been engineered such that it has redundant powersupplies throughout.4 the pstn also supplies its own power to enduser devices;a simple telephone will operate even if the local power has failed. in recent years,however, this distinction has been fading as people increasingly make use of telephone devicesñcordless phones being the most common example todayñthatare dependent on local power, just as an internetconnected computer would be.¥malicious attacks. malicious attacks on the internet are facilitated by thegeneral accessibility of the internet, the widespread availability of both documentation and source code for internet protocols, and failures to fix known vulnerabilitiesin systems. trust in cyberspace pointed to several major avenues of attack onthe network infrastructure and measures to address them: name server attacks(development of new countermeasures that work in largescale, heterogeneousenvironments) and deployment of cryptography to secure the dns, attacks on therouting system (research on means to secure routing protocols), and denialofservice attacks (wider use of updated software, new product development, andbetter software engineering).4backup power is, of course, not a panacea. backup systems fail on occasion, and testingthem has itself resulted in power failures.the internet's coming of agecopyright national academy of sciences. all rights reserved.88the internetõs coming of ageat addressing some of these shortcomings. the internetõs vulnerabilitystems in part from the difficulty, today, of tracing the location from whichattacks such as denial of service have been launched, making it difficult toidentify an attacker after the fact. one approach under development aimsto allow an attack to be traced, without requiring access to isp logs, if theattack involves a lot of packets. other research is aimed at tracing attackswithout storing large amounts of information, thereby reducing the storage burden and alleviating the privacy concerns that would arise fromisps logging of detailed information on the traffic passing over their networks.in many cases, however, the internet is vulnerable to attack not because there are no technologies to protect it but because they are not used,whether for lack of perceived need or lack of knowledge about such measures. known vulnerabilities are frequently not addressed despite thepromulgation of patches to fix them. enhanced technologies such assingleuse passwords, cryptographic authentication, and message digestauthentication technology have been developed and are starting to seedeployment. nonetheless, many network operators do not necessarilyuse these capabilities to defend their systems; plain text passwords, whichare readily guessed or captured, remain the authentication technique inmost common use. likewise, while many data access or modificationattacks are made by insiders (people within the organization), the mostcommonly installed intrusion detection and prevention technology is afirewall, which by nature provides only limited defense and only protectsagainst attacks across the perimeter they defendñthe intruder who succeeds in getting inside the defense walls ends up with free run of theinterior unless a defense in depth has been put in place.responses to these vulnerabilities have been mixed, and continuedwork will be required. specifying and deploying the needed upgrades tothe protocols and software used in the internet is a complex problem.they are a result of both group development efforts and marketplacedynamics. these, in combination, determine what is developed andwhether and when it is adopted and deployed. an indicator of bothtodayõs problems and prospects for future improvements is the relativelyrecent move by the ietf to prevent those proposing internet standardsfrom sidestepping analyses of security implications.45 within the internet45the september 1998 guidelines and procedures for ietf working groups state as follows: òit should be noted that all ietf working groups are required to examine and understand the security implications of any technology they develop. this analysis must beincluded in any resulting rfcs in a security considerations section. note that merelynoting a significant security hole is no longer sufficient. ietf developed technologiesshould not add insecurity to the environment in which they are runó (internet engineeringtask force (ietf). 1998. ietf working group guidelines and procedures, rfc 2418. ietf, p.22. available online from <http://www.ietf.org/rfc/rfc2418.txt>).the internet's coming of agecopyright national academy of sciences. all rights reserved.scaling up the internet and making it more reliable and robust89marketplace, progress is evident because some isps see robustness andservice guarantees as an enticement to attract customers.more adaptive routingimprovements in robustness will become increasingly important asservicelevel agreements between providers and customers (includingother providers) begin to specify high reliability. there is considerabledebate among those who design and operate the internetõs networks aboutthe relative priority to be attached to improving the reliability of components and to increasing the adaptive performance of the network. oneapproach is to make components more reliable, the argument being thatthis improvement, taken in combination with the internetõs current levelof robustness, would yield a network potentially far more reliable thanthe pstn. another approach, which is based on the belief that engineering extremely reliable equipment is extremely expensive, is that theinternet should be able to achieve much better overall reliability with lessreliable but less costly components by improving its adaptive performance.robustness depends on software as well as hardware, and achievinga high level of robustness, particularly when a strategy of using lowercost, less reliable hardware is adopted, depends on software and deployments that enhance the internetõs adaptability in the face of individualcomponent failures. a principal means of adaptation is the modificationof routing paths to reflect information discovered about links and routersthat are not operational. todayõs routing protocols were developed at atime when major links were significantly slower than they are now. improved routing algorithms, suggested by djikstra in 1970 but widelyimplemented only in the past decade (in the open shortest path first routing protocol), have improved reliability and robustness. still, the rate atwhich the network adapts to failures remains fairly sluggish. routingcould, in principle, recover from failures (or incorporate newly availablecapacity) in tens of milliseconds, but the historically based timer settingsthat are found in deployed routers permit recovery only on much longertimescales of tens to hundreds of secondsña timescale long enough tohave a noticeable impact on users and to result in the failure of manyindividual transactions.why do the routing protocols have such long time constants? thebasic reason is that a local routing change can, in principle, have globalconsequences. for example, one might have a situation in which thefailure of just one link would cause significant traffic flows leaving theunited states to shift from an eastward (atlantic ocean link) to a westward (pacific ocean link) path. the circumference of the earth is apthe internet's coming of agecopyright national academy of sciences. all rights reserved.90the internetõs coming of ageproximately 40,000 kilometers, so for communication at the speed of lightit takes a data packet slightly more than 0.1 seconds to go around theearth. that represents a lower bound; in practice, the time it takes a datapacket to go around the earth will be substantially longer, owing to delays in the various network elements it must traverse in the internet. anyrouting protocol that takes into account the global consequences of arouting change has to adjust on a timescale longer than this roughly0.1second minimum; otherwise, the results of the routing computationwill not be globally consistentñfor example, there would be no use suddenly sending traffic via the pacific if routing information describing whatthe routers located in asia should do with the traffic has not had time tomake it there yet. however, it turns out that most local link failures do nothave global consequences. so if there were designs for propagating routing information that could limit the consequences of certain classes ofchanges, the network could respond much faster. the development ofthese sorts of bounded schemes would make a good class of research.faster adaptationñwhereby failure of an internet element triggers a quickrerouting of trafficñalso depends on supporting capabilities such as rapidfailure detection.to increase robustness, redundancy is also needed in the major communications links. optical fibers, for example, are carrying growingamounts of traffic as wave division multiplexing (wdm) increases thecapacity of each fiber and, accordingly, the impact if it fails. the sharing(multiplexing) of communications facilities means that it is increasinglydifficult to provision physically diverse communications links betweentwo given sites. for example, two nominally different communicationslinks, even when obtained from different providers, may in fact run overthe same fiber, in the same fiber bundle, or in the same conduit, meaningthat they are both vulnerable to a single physical disruption (e.g., a backhoe inadvertently cutting a buried fiber bundle). finally, as the nextsection discusses, the extent to which the internet is able to adapt tofailures depends on how the interconnections between providers are designed and implemented.putting it togetherthe heterogeneous, multiprovider nature of the public internet posesadditional robustness challenges. the internet is not a monolithic system,and the several thousand internet service providers in north americarange in size from large international providers to oneroom companiesserving a single town with dialup service. the business demand forthese operators to provide a high level of redundant facilities, constantlymanned operation centers, or highly trained staffñand their ability to dothe internet's coming of agecopyright national academy of sciences. all rights reserved.scaling up the internet and making it more reliable and robust91soñvaries considerably. nonetheless, all these different providers, withtheir different goals for availability, reliability, and so on, interoperate toprovide the global internet service.the section above described why it can be difficult to provide sufficiently diverse communications paths. doing so is a necessary but notsufficient condition for increasing the robustness of interprovider links.traffic cannot be routed to alternative paths if the interprovider connection agreements overly constrain the number of paths the data can follow.for example, interconnection agreements often contain provisions, aimedat preventing traffic dumping (one provider dumping traffic onto segments of another providerõs network), that have the effect of permittingtraffic between a particular point on one ispõs network and a point onanother ispõs network to follow only one specific path. if that specifiedlink fails, there are no alternative paths for the traffic to follow (until thefailure is detected and other traffic flow arrangements are put in place).isps devote considerable resources to detecting and responding tofailures and attacks. problems that cross isp boundaries represent a particular challenge as the reliability of any one part of the internet (and thusthe reliability of the whole internet) can depend on what happens withinother isp networks. not all isps have the same level of expertise orcapabilities, and one may trace a problem only to find that it originateswith a provider that lacks the facilities or expertise to address it. therehave been instances of failures in one isp that have caused dns failures,routing failures, or transmission failures in a neighboring isp. isps depend on expert network operators to maintain the health of their individual networks as well as the internet as a whole. they continue to trackproblems and troubleshoot on an ad hoc basis (e.g., through the informalrelationships that exist among the expert network operators) and throughloose coordination mechanisms (e.g., the north american network operators group) in order to minimize network service outages. and industry groups such as iops have been convened to tackle robustness issues.it is hoped, but is by no means certain, that better tools and methodswill result in operations that are more robust. isps must be able to resistattacks that originate within other isps (e.g., foreign isps with perhapsunfriendly intentions) but at the same time must interconnect with otherisps to preserve the basic transport function of the internet. however,more structured and effective methods and tools are required for robustoperation, especially to protect against intentional malicious attacks thatoriginate in a connected isp. the design of such protocols and methodsis, however, still largely a research issue.4646computer science and telecommunications board (cstb), national research council.1999. trust in cyberspace. washington, d.c.: national academy press.the internet's coming of agecopyright national academy of sciences. all rights reserved.92the internetõs coming of ageapplication reliability and robustnessgiven the internetõs layered architecture, responsibility for ensuringthat a given application works properly is diffuse. the internetõs designallows each user to decide what applications to run over it. because it isthe software running on computers attached to the network rather thansoftware in the routers that determines what applications can be run, theinternet service provider does not, in general, see what application theuser is running and may not take specific steps to support it. a consequence is that a user who obtains software for a new application such asinternet telephony may be surprised to discover that the part of theinternet to which he or she is attached is not provisioned to satisfactorilysupport the application.an application such as telephony thus raises not only the question ofhow to address reliability and quality across network providers but alsothe question of who in fact is providing the service. telephony may beoffered by an isp or by a third party. the third party may simply offerhardware or software, in which case users would be able to set themselves up for internet telephony on their own, without any internet service provider being involved (or even having knowledge of how its network is being used). or, a third party may offer services via the internetthat are related to placing calls, such as a directory service that allows oneto determine the internet device associated with a particular user andthus place a call. in this case, although a service provider can be identified, it need not have established any relationship with the internet service providers responsible for actually serving the customer and carryingthe data associated with a telephone call. in the absence of specific servicearrangements made by customers or an internet telephony provider, aninternet provider that happens to be carrying voice traffic will not ingeneral even know that its facilities are being used for telephony. a usermight then place a 911 call and discover that the isp serving that user hasnot provided a robust connection to any 911 facility. this separation ofinternet service from application means that, with todayõs internet, onecannot assume that the robustness of an infrastructure is appropriate forthe robustness of the application.one response to a view that òtelephony is telephonyó (that is, nomatter what technologies are used) would be to impose quality, robustness, and emergency service requirements only on the parties that specifically offer internet telephony services. these parties would be responsible for negotiating with the internet providers who carry traffic for themto provide an acceptable quality of service. however, this approach is noteasily implemented when an application comes in the form of shrinkwrap or downloadable software that is installed by the consumer andintended to be run over whatever network the consumer subscribes to.the internet's coming of agecopyright national academy of sciences. all rights reserved.scaling up the internet and making it more reliable and robust93another response would be to wait for market and technology developments. mobile phones were deployed despite obvious limitations inquality and availability, and the users simply adapted their expectations.internet applications may be introduced with similar expectations. theymay then be improved over time, either because the quality of the network increases or because the application designers incorporate featuresto query or test the networkõs quality and compensate for itña development that may or may not happen.robustness and auxiliary serverssometimes isps do make special provisions to support particulartypes of content or services. one popular, partial solution to scaling andquality of service issues is to distribute content servers at strategic pointsin the internet. their use raises an interesting reliability tradeoff. on theone hand, as evidenced by the substantial investment in deploying caching and proxy services, such capabilities are viewed by many as an essential tool for making usable services that would otherwise be compromisedby network congestion. on the other hand, the robustness concerns associated with some of these capabilities can be viewed as a good example ofthe principle that todayõs performance enhancements can becometomorrowõs problems.in some cases, the use of the servers is integrated into the applicationduring the publishing process. web pages are rewritten to point the usertoward secondary content servers. in other cases, the caches are introduced in the form of content routers that are deployed by the isp. thecontent routers, which operate outside the control of either the end useror the content provider, will typically filter packets that appear to requesta web service by looking at the tcp port numbers, and they will routethese packets to the local cache server or content server regardless of theip destination address.because both practices insert a set of auxiliary servers in the infrastructure, they have implications for robustness, as any failure of theservers would affect the endtoend availability of the service. but theirimplications for robustness are different. when the usage of auxiliaryservers is decided by the content provider at the application level, theresponsibility for providing the right content and providing robust servers remains with that content provider. wrong decisions, poor reliabilityof auxiliary servers, or implementation mistakes only affect the productsof a specific publisher, without compromising the service experienced byother users. in contrast, a network provider that decides to intercept webrequests and to route them to its own content servers may run a greaterrisk, as there is no way for the content provider or end user to correct thethe internet's coming of agecopyright national academy of sciences. all rights reserved.94the internetõs coming of agefailure of a cache server by requesting the same information from anotherserver; by the same token, there is no way to stop the content server frommodifying the information.toward greater reliability and robustness:reporting outages and failuresoverloads, failures, operator errors, and other events on occasiondisrupt user activities on the internet. industry organizations such asnanog47 and iops48 provide forums for the exchange of informationamong isps about both problems and solutions. anecdotal reports (e.g.,press reports and email exchanges among network operators and otherindustry experts) provide additional information. thus we have someindication of the sources of failure, which include the following:¥communications links that are severed by construction workersdigging near fiberoptic cables;¥network operators that issue incorrect configuration commands;¥inadequately tested software upgrades that cause systems to fail;and¥deliberate attacks that are carried out on internet systems and services.but information on internet failures has not been reported or analyzed on a systematic basis. from the standpoint of both customers andpolicy makers, more systematic internet statistics49 would be helpful inboth appraising and improving the internetõs robustness. also, a requirement to report problems would create pressure to increase the overallreliability of the internet. it could also help consumers distinguish amongproviders with different service records and it might help providers detect recurring patterns of failures, so that the root cause could be eliminated.5047see <www.nanog.org>.48see <www.iops.org>.49such information was available on the nsfnet, for which merit, inc., collected andpublished statistics.50for example, in the case of pstn reliability reporting, the network reliability steeringcommittee used the fccmandated data to identify construction activity resulting in damage to fiberoptic cables as the factor responsible for more than 50 percent of facility outagesand, further, found that more than onehalf of these occurrences were due to the failure ofexcavators to notify infrastructure owners or provide adequate notice before digging.the internet's coming of agecopyright national academy of sciences. all rights reserved.scaling up the internet and making it more reliable and robust95there is an important distinction between the needs of customers andthe needs of those seeking to understand and eliminate problems. theperformance of isps can be monitored by independent organizations thatpublish reports for subscribers or the public. a buying guide of sorts, thistype of information helps a customer assess the service of the variousisps. independent monitoring may discover and report outages, but itdoes not necessarily provide access to the causal and diagnostic information that is needed to try to eliminate the root cause of problems. thelatter goal requires that detailed information on failures be made available to industry groups and/or researchers.the government has mandated problem reporting in other circumstances. where public safety is involved, the government has imposedvery strict requirements for reporting and analysis (e.g., crashes of commercial aircraft). the government has also required reporting in caseswhere the issue is not public safety but where a perceived need exists tohelp the consumer understand the service being provided. airlines are,for example, required to track and report how late their flights are, lostbaggage statistics, and the like.the federal communications commission requires outage reportingby operators of some elements of the pstn.51 it was motivated by concerns that it did not have the means to systematically monitor significanttelephone outages on a timely basis, as well as a more general interest inseeing that information on vulnerabilities is shared. a series of majorservice outages in interexchange and local exchange carrier networks inthe early 1990s underscored the need to obtain information that wouldimprove reliability. under these rules, outages of more than a specifiedlevel of severity (duration, number of subscribers affected, etc.) must bepublicly documented.52 for example, service outages that significantlydegrade the ability of more than 30,000 customers to place a call for morethan 30 minutes must be reported.5351see fcc common carrier docket no. 91273, paragraphs 4 and 32 (february 27, 1992),as cited in network reliability and interoperability council (nric). 1997. networkinteroperability: the key to competition. washington, d.c.: nric, federal communicationscommission federal advisory committee, p. 12.5247 c.f.r. section 63.100 sets mandatory reporting thresholds for wireline telephonecompanies. the requirements are based on the type of outage, the number of affectedsubscribers, and the duration of the failure.53see network reliability and interoperability council (nric). 1997. networkinteroperability: the key to competition. washington, d.c.: nric, federal communicationscommission federal advisory committee, p. 11. available online at <http://www.nric.org/pubs/nric3/reportj9.pdf>.the internet's coming of agecopyright national academy of sciences. all rights reserved.96the internetõs coming of agechanges in the telecommunications industry led the fcc, in 1998, toask the network reliability and interoperability council (nric) iv toexplore reliability concerns in the wider set of networks (e.g., telephone,cable, satellite, and data, including the internet) that the pstn is part of.the report of the nric iv subcommittee looking at needs for data onservice outages54 called for a trial period of outage reporting. nric v,chartered in 2000, has initiated a 1year voluntary trial starting in september 2000 and will monitor the process, analyze the data obtained from thetrial, and report on how well the process works.55isps are not, at present, mandated to release such information. indeed, the release of this type of information is frequently subject to theterms of private agreements between providers. this situation is notsurprising, given the absence of regulation of the internet and the highdegree of regulation of the telephone industry. as the internet becomesan increasingly important component of our society, there will probablybe calls to require reporting on overall reliability and specific disruptions.it is not now clear what metrics should be used and what events shouldbe reported, what the balance between costs and benefits would be fordifferent types of reporting, or what the least burdensome approach tothis matter would be. one response to rising expectations would be forinternet providers to work among themselves to define an industry approach to reporting. doing so could have two benefitsñit might provideinformation useful to the industry and it might avoid government imposition of an evenlesswelcome plan.as noted above, one important reason for gathering information ondisruptions is to provide researchers with the means to discover the rootcauses of such problems. for this to be effective, outage data must beavailable to researchers outside the isps; isps do not generally have research laboratories and are not necessarily well placed to carry out muchof the needed analysis of the data much less design new protocols orbuild new technologies to improve robustness. also, data should not beanonymized before they are provided to researchers; the anonymity hidesinformation (e.g., on the particular network topology or equipment used)54see network reliability and interoperability council (nric). 2000. network reliabilityinteroperability council iv, focus group 3, subcommittee 2, data analysis and future considerations team. washington, d.c.: nric, federal communications commission federal advisory committee, p. 4. available online at <ftp://ftp.atis.org/pub/nrsc/fg3sc2final.pdf>.55see network reliability and interoperability council (nric). 2000. revised networkreliability and interoperability council  v charter. washington, dc: nric, office of engineering and technology, federal communications commission. available online at <http://www.nric.org/charterv/>.the internet's coming of agecopyright national academy of sciences. all rights reserved.scaling up the internet and making it more reliable and robust97from the researcher. however, in light of proprietary concerns attachedto the release of detailed information, researchers must agree not to disclose proprietary information (and must live up to those agreements).disclosure control in published reports is not simply a matter ofanonymizing the results; particular details may be sufficient to permit thereader of a research report, including an ispõs competitors, to identify theisp in question. attention must, therefore, also be paid to protectingagainst inadvertent disclosure of proprietary information.looking to the future, the committee can see other reasons why ispswould benefit from sorting out what types of reliability metrics shouldbe reported. for example, it is not hard to imagine that at some pointthere would be calls from highend users for a more reliable service thatspans the networks of multiple isps and that some of the isps woulddecide to work together to define an òindustrialstrengthó internet service to meet this customer demand. when they interconnect their networks, how would they define the service that they offer? since theperformance experienced by an ispõs customer depends on the performance of all the networks between the customer and the application orservice the customer is using, each isp would have an interest in ensuring that the other isps live up to reliability standards. absent a goodsource of data on failures (and a standardized framework for collectingand reporting on failures), how would the isps keep tabs on each other?in the process of defining a highergrade service, isps will want to understand what sort of failure would degrade the service, and it is this sortof failure that they ought to be reporting on. from this perspective,outage reporting shifts from being a mandated burden to an enabler ofnew business opportunities.it is unlikely that simple, unidimensional measures that summarizeisp performance would prove adequate. creating standard reporting orrating models for the robustness and quality of isps would tend to limitthe range of services offered in the marketplace. what form might suchuser choices take? consider, as an example, that an isp that experiencesthe failure of a piece of equipment might face a tough tradeoff. it couldcontinue to operate its network at reduced performance in this conditionor undergo a short outage to fix the problemña choice between anextended period of uptime at much reduced performance and a shortoutage that restores performance to normal. some of the ispõs customersñe.g., those who depend on having a connection rather than on theparticular quality of that connectionñwill prefer the first option, whileothers will prefer the second. indeed, some may be willing to pay extra toget a service that aims to provide a particular style of degraded service.(such a òguaranteed style of degradationó is an interesting variation onqos and does not impose much overhead.) these considerations suggestthe internet's coming of agecopyright national academy of sciences. all rights reserved.98the internetõs coming of agethat, more generally, there is a need for many different rating scales or,put another way, a need for measuring several different things that mightbe perceived as òqualityó or reliability. combining them into a singlemetric does not serve the interests of different groups (user or vendor orboth) that are likely to prefer different weighting factors or functions forcombining the various measures.quality of servicethe internetõs besteffort quality of service (qos) makes no guarantees about when, or whether, data will be delivered by the network. together with the use of endtoend mechanisms such as the transmissioncontrol protocol (tcp), which provides capabilities for reassembling information in proper order, retransmitting lost packets, and ensuring complete delivery, best effort been successful in supporting a wide range ofapplications running over the internet. however, unlike web browsing,email transmission, and the like, some applications such as voice andvideo are very timesensitive and degrade when the network is congestedor when transmission delays (latency) or variations in those delays (jitter)are excessive. some performance issues, of course, are due to overloadedservers and the like, but others are due to congestion within the internet.interest in adding new qos mechanisms to the internet that would tailornetwork performance for different classes of application as well as interest in deploying mechanisms that would allow isps to serve differentgroups of customers in different ways for different prices have led to thecontinued development of a range of qualityofservice technologies.while qos is seeing limited use in particular circumstances, it is notwidely employed.the technical community has been grappling with the merits andparticulars of qos for some time; qos deployment has also been thesubject of interest and speculation by outside observers. for example,some ask whether failure to deploy qos mechanisms represents a missedopportunity to establish network capabilities that would foster new applications and business models. others ask whether introducing qoscapabilities into the internet would threaten to undermine the egalitarianquality of the internetñwhereby all content and communications acrossthe network receive the same treatment, regardless of source or destinationñthat has been the consequence of besteffort service.beyond the baseline delay due to the speed of light and other irreducible factors, delays in the internet are caused by queues, which are anintrinsic part of congestion control and sharing of capacity. congestionoccurs in the internet whenever the combined traffic that needs to beforwarded onto a particular outgoing link exceeds the capacity of thatthe internet's coming of agecopyright national academy of sciences. all rights reserved.scaling up the internet and making it more reliable and robust99link, a condition that may be either transient or sustained. when congestion occurs in the internet, a packet may be delayed, sitting in a routerõsqueue while waiting its turn to be sent on, and will arrive later than apacket not subjected to queuing, resulting in latency. jitter results fromvariations in the queue length. if the queue fills up, packets will bedropped.in todayõs internet, which uses tcp for much of its data transport,systems sending data are supposed to slow down when congestion occurs (e.g., the transfer of a web page will take longer under congestedconditions). when the internet appears to be less congested, transfersspeed up and applications complete their transaction more quickly. because the adaptation mechanisms are based on reactions to packet loss,the congestion level of a given link translates into a sufficiently largepacket loss rate to signal the presence of congestion to the applicationsthat share the link. congestion in many cases only lasts for the transientperiod during which applications adapt to the available capacity, and itreaches drastic levels only when the capacity available to each applicationis less than the minimum provided by the adaptation mechanism.congestion is generally understood to be rare within the backbonenetworks of major north american providers, although it was fearedotherwise in the mid1990s, when the internet was commercialized. instead, it is more likely to occur at particular network bottlenecks. forexample, links between providers are generally more congested than thosewithin a providerõs network, some very much so. persistent congestionis also observed on several international links, where long and variablequeuing delays, as well as very high packet loss rates, have been measured.56 congestion is also frequent on the links between customersõ localarea networks (or residences) and their isps; sometimes it is feasible toincrease the capacity of this connection, while in other cases a highercapacity link may be hard to obtain or too costly. where wireless links areused, the services available today are limited in capacity, and wirelessbandwidths are fundamentally limited by the scarcity of radio spectrumassigned to these services as well as vulnerable to a number of impairments inherent in overtheair communication.at least some congestion problems can be eliminated by increasingthe capacity of the network by adding bandwidth, especially at known56see v. paxson. 1999. òendtoend internet packet dynamics,ó ieee/acm transactionson networking 7(3):277292, june. logs of transatlantic traffic available online at <http://bill.ja.net/> show traffic levels that are flat for most of the day at around 300 mbps on a 310mbps (twin oc3) terminating in new york.the internet's coming of agecopyright national academy of sciences. all rights reserved.100the internetõs coming of agebottlenecks. adding bandwidth does not, however, guarantee that congestion will be eliminated. first, the tcp rateadaptation mechanismsdescribed above may mask pentup demand for transmission, which willmanifest itself as soon as new capacity is added. second, on a slightlylonger timescale, both content providers and users will adjust their usagehabits if things go faster, adding more images to web pages or being morecasual about following links to see what is there and so on. third, on alonger timescale (on the order of months but not years), new applicationscan emerge when there is enough bandwidth to enough of the users tomake them popular. this has occurred with streaming audio and is likelyto occur with streaming video in the near future.also, certain applicationsñnotably, realtime voice and videoñrequire controlled delays and predictable transfer rates to operate acceptably. (streaming audio and video are much less sensitive to brief periodsof congestion because they make use of local buffers.) broadly speaking,applications may be restricted in their usefulness unless bandwidth isavailable in sufficient quantity that congestion is experienced very rarelyor new mechanisms are added to ensure acceptable performance levels.a straightforward way to reduce jitter is to have short queue lengths, butthis comes at the risk of high loss rates when buffers overflow. qosmechanisms can counteract this by managing the load placed on the queueso that buffers do not overflow however, the situation in the internet,with many types of traffic competing in multiple queues, is complex.better characterization of network behavior under load may provide insights into how networks might be engineered to improve performance.concerns in the past about being able to support multimedia applications over the internet led to the development of a variety of explicitmechanisms for providing different qualities of service to different applicationsñe.g., best effort for web access and specified realtime servicequality for audio and video.57 today, two major classes of qos supportdifferent kinds of delay and delivery guarantees (see box 2.4). they arebased on the assumption that applications do not all have the same requirements for network performance (e.g., latency, jitter, or priority) and57in essence, these proposed qos technologies resemble those that have proven effectivein atm and frame relay networks, with the exception that they are applied to individualapplication sessions or to aggregates of traffic connecting sets of systems running sets ofapplications rather than to individual circuits connecting pairs of systems. the mathematical difference between ip qos and atm qos is that atm sends variablelength bursts ofcells, while ip sends variablelength messages. the biggest operational difference is thatatm qos is generally used in atm networks carrying realtime traffic, while qos isgenerally not configured in ip networks today.the internet's coming of agecopyright national academy of sciences. all rights reserved.scaling up the internet and making it more reliable and robust101that the network should provide classes of service that reflect these differences.58there is significant disagreement among experts (including the experts on this committee) as to how effective qualityofservice mechanisms would be and which would be more efficient, investing in additional bandwidth or deploying qos mechanisms. one school of thought,which sees a rising tide of quality, argues that increasing bandwidth inthe internet will provide adequate performance in many if not most circumstances. as higher capacity links are deployed, the argument goes,internet delays will tend to approach the theoretical limit imposed by thepropagation of light in optical fibers, and the average bandwidth available on any given connection will increase. as the overall quality increases, it will enable more and more applications to run safely over theinternet, without requiring specific treatment, in the same way that arising tide as it fills a harbor can lift everlarger boats. voice transmission,for example, is enabled if the average bandwidth available over a givenconnection exceeds a few tens of kilobits per second and if the delays areless than onetenth of a second, conditions that are in fact already true forlarge business users; interactive video is enabled if the average bandwidth exceeds a few hundred kilobits per second, a performance levelthat is already obtained on the networks dedicated to connecting universities and research facilities. if these conditions were obtainable on thepublic internet (e.g., if the packet loss rate or jitter requirements for telephony were met 99 percent of the time), business incentives to deployqos for multimedia applications would disappear and qos mechanismsmight never be deployed.proponents of the rising tide view further observe that the causes ofjitter within todayõs internet are poorly understood, and that investmentin better understanding the reasons for this behavior might lead to anunderstanding of what improvements might be made in the network aswell as what qos mechanisms would best cope with network congestionand jitter if tweaking the network is not a sufficient response.there are, however, at least some places within the network wherethere is no tide of rising bandwidth, and capacity is intrinsically scarce.one example is the more expensive and limited links between local areanetworks (or residences) and the public network. even here, however,58this presumes, of course, that one should meet the full range of requirements in asingle infrastructure with a single switching environment. this is not necessarily an optimal outcome; while the internet has been able to support a growing set of service classeswithin a single network architecture, it is an open question what network models wouldbest support the broad range of communications service profiles.the internet's coming of agecopyright national academy of sciences. all rights reserved.102the internetõs coming of agebox 2.4 qualityofservice mechanismsdifferentiated services (diffserv)diffserv,1 a set of proposed ietf standards, would allow isps to providecustomers with a quality of service that is better than the default besteffort service.the idea is to use part of the packet header as a service class indication. (it doesso an ongoing basis; unlike intserv, described below, it does not provide for ondemand requests for a quality of service for a particular communications session.)such an enhanced service might give the customer better service up to a particularbandwidth ceiling, perhaps with a cap on the total amount of data transmitted in agiven time interval. a premium service might also provide guarantees such asspecified maximum packet loss rate or an upper bound on latency. the classdefinition can be one of strict separation (such as òclass x gets at least 10 percentof the total available resourceó), one of priority (òclass y gets what class x doesnot useó), or one of service (òpackets in class z are never queued more than zmillisecondsó). there is quite a lot of debate on the actual definition of classes.with diffserv, customers can select one or more service classes and are able tospecify different qos classes for different applications (e.g., using a highend service for video conferencing while using besteffort service for file transfer and email exchanges). access to these service classes would be enforced at the edgeof the network through the use of filters by ip addresses, and routers in the network would use the serviceclass label to determine how packets are queued. diffserv depends in large part on isps using the additional revenue derived from premium services to adequately provision their networks to support the offeredservices. while simple diffserv mechanisms have been demonstrated to providea high probability of meeting usersõ qos expectations for pointtopoint communications,2 the guarantees are probabilistic, meaning that the isp cannot make absolute service guarantees.integrated services (intserv)intserv3 provides quantifiable, endtoend qos guarantees for particulardata flows. it makes use of a signaling mechanism, the resource reservation pro1s. blake et al. 1998. an architecture for differentiated services. internet engineeringtask force, network working group, rfc 2475. december. available online at <http://www.ietf.org/rfc/rfc2475.txt>.2d. clark and j. wroclawski. 1997. an approach to service allocation in the internet. ietfdraft report, july. cambridge, mass.: massachusetts institute of technology. available online at <http://diffserv.lcs.mit.edu/drafts/draftclarkdiffsvcalloc00.txt>.3r. braden, s. shenker, and d. clark. 1994. integrated services in the internet architecture:an overview. network working group, internet engineering task force. rfc 1633, june.available online at <http://www.ietf.org/rfc/rfc1633.txt>.the internet's coming of agecopyright national academy of sciences. all rights reserved.scaling up the internet and making it more reliable and robust103tocol (rsvp)4 that allows applications to specify requirements for bandwidth andendtoend latency. the intserv model is somewhat analogous to that of the telephone network, where services are requested on an asneeded basis and where ifresources are not available to provide the requested service then the networkreturns a busy signal rather than providing a degraded service. as currently defined, every application flow (e.g., a single video call) needs its own reservation,and each reservation requires that a moderate amount of information be stored atevery router along the path that will carry the application data, which can lead toscaling challenges if intserv is widely used. another set of challenges ariseswhen multiple isps are needed to connect the end points of a connection; intservreservations that cross network boundaries are difficult to administer, and methodsare needed to allocate and bill for the costs of reserving capacity in multiple isps.other approachesshortcomings of diffserv and intserv have prompted researchers to explorealternative qos mechanisms that lie somewhere between the twoñproviding finer granularity and stronger guarantees than are provided by diffserv while avoiding the scaling and administrative problems associated with intserv. one suchapproach, integrated services over specific link layers, would combine the endtoend service definitions and signaling of intserv with the scalable queuing andclassification techniques of diffserv.5 another approach, referred to as virtual overlay networks (vons), would add capabilities to routers within the internet to permitthe creation of virtual networks in which traffic within an individual flow would compete with other packets on the same von but not with traffic from other flows.6multiple vons could be created to serve different applications; they would be connected to different end points and offer different levels of service. among theopen questions associated with this approach are how to specify properties of anoverlay network, how to dynamically administer resources on routers associatedwith an overlay network, how to avoid scaling issues as the number of overlaysbecomes large, and how to rapidly classify large numbers of flows.4r. braden, l. zhang, s. berson, s. herzog, and s. jamin. 1997. resource reservationprotocol (rsvp): version 1 functional specification, rfc 2205. network working group,internet engineering task force, september. available online at <http://www.ietf.org/rfc/rfc2205>.5this work is being carried out by the ietfõs integrated services over specific link layersworking group. the groupõs charter is available online at <http://www.ietf.org/html.charters/issllcharter.htm>.6for an overview, see kenneth p. birman. 2000. òthe next generation internet: unsafe atany speed?ó computer 33(8).source: adapted in part from computer science and telecommunications board, nationalresearch council. 2000. networking health: prescriptions for the internet. washington,d.c.: national academy press. available online at <http://www.nap.edu/catalog/9750.html>.the internet's coming of agecopyright national academy of sciences. all rights reserved.104the internetõs coming of agesome will argue that it is better to invest in increased capacity of thegateway link than in mechanisms to allocate scarce bandwidth. as notedabove, wireless links are inherently limited in capacity and are thereforecandidates for qos. prospects for the use of internet qos technologies inthis context depend in part on whether qos services are provided at theinternet protocol layer or through specialized mechanisms incorporatedinto the lowerlevel wireless link technology. current plans for thirdgeneration wireless services favor the latter approach, suggesting thatthis may not be a driver of internet qos.service quality, like security, is a weaklink phenomenon. becausethe quality experienced over a path through the internet will be at least asbad as the quality of the worst link in that path, quality of service may bemost effective when deployed end to end, on all of the links betweensource and destination, including across the networks of multiple isps. itmay be the case that localized deployment of qos, such as on the linksbetween a customerõs local area network and its isp, would be a usefulalternative to endtoend qos, but the effectiveness of this approach andthe circumstances under which it would prove useful are open questions.the reality of todayõs internet is that endtoend enhancement of qosis a dim prospect. qos has not been placed into production for endtoend service across commercial isp networks. providing endtoend qosrequires isps to agree as a group on multiple technical and economicparameters, including on technical standards for signaling, on the semantics of how to classify traffic and what priorities they should be assigned,and on the addition of complex qos considerations to their interconnection business contracts. perhaps more significantly, the absence of common definitions complicates the process of negotiating qos across all ofthe providers involved end to end. isp interest in differentiating theirservice quality from that of their competitors is another potential disincentive to interprovider qos deployment.there are also several technical obstacles to deployment of endtoend qos across the internet. one challenge is associated with the routingprotocols used between network providers (e.g., border gateway protocol, or bgp). while people have negotiated the use of particular methodsfor particular interconnects, there are no standardized ways of passingqos information, which is needed for reliable voice (or other latencysensitive traffic) transport between provider domains. also, todayõs routing technology provides limited control over which peering points interprovider traffic passes through, owing to a lack of symmetric routing andthe complexities involved in managing the global routing space.exchanging latencysensitive traffic (such as voice) will, at a minimum,require careful attention to interconnect traffic growth and routingconfigurations.the internet's coming of agecopyright national academy of sciences. all rights reserved.scaling up the internet and making it more reliable and robust105while the original motivation for developing qualityofservicemechanisms was support of multimedia, another factor has been responsible for a sizable portion of recent interest in quality of service: isps thatwish to valuestratify their users, that is, to offer those customers whoplace a higher value on better service a premiumpriced service, needmechanisms to allow them to do so. in practice, this may be achieved bymechanisms to allocate relative customer dissatisfaction, degrading theservice of some to increase that of others. (anyone who has flown on acommercial airliner understands the basic principle: lowerfarepayingcustomers in coach have fewer physical comforts than their fellow travelers in first class, but they all make the same trip.) value stratification maybe of particular interest in situations where there is a scarcity of bandwidth and thus an interest in being able to charge customers more forincreased use, but value stratification may also find use under circumstances where isps are able to provision sufficient capacity to meet thedemands of their customers and customers perceive enough value in apremium service to pay more for it.there is a central tension in the debate over qos. if the providers, inorder to make their customers happy, add enough capacity to carry theimposed load, why would one need more complex allocation schemes?put another way, if there is no overall shortage of capacity, all that can beachieved by establishing allocation mechanisms is to allocate relative dissatisfaction. would providers intentionally underprovision certain classesof users? as indicated above, the answer may be yes under certain marketing and business plans. such differentiation of service packages andpricing are sustainable inasmuch as customers perceive differences andare willing to pay the prices charged.one consequence of the development of mechanisms that enable disparate treatment of customer internet traffic has been concern that theycould be used to provide preferential support for both particular customers and certain content providers (e.g., those with business relationshipswith the isp).59 what, for instance, would better service in delivery ofcontent from preferred providers imply for access to content from providers without such status? what people actually experience will dependnot only on capabilities possible from the technology and the design ofmarketing plans but also on what customers want from their access to theinternet and what capabilities isps opt to implement in their networks.59see, for example, center for media education. 2000. what the market will bear: ciscoõsvision for broadband internet. washington, d.c.: center for media education. availableonline at <http://www.cme.org/access/broadband/marketwillbear.html>.the internet's coming of agecopyright national academy of sciences. all rights reserved.106the internetõs coming of agethe debate over quality of service has been a longstanding one withinthe internet community. over time, it has shifted from its original focuson mechanisms that would support multimedia applications over theinternet to mechanisms that would support a broader spectrum of potential uses. these uses range from efficiently enhancing the performance ofparticular classes of applications over constrained links to providing ispswith mechanisms for valuestratifying their customers. the committeeõspresent understanding of the technology and economics of the internetdoes not support its reaching a consensus on whether qos is, in fact, animportant enabling technology. nor can it be concluded at this timewhether qos will see significant deployment in the internet, either overlocal links, within the networks of individual isps, or more widely, including across isps.research aimed at better understanding network performance, thelimits to the performance that can be obtained using besteffort service,and the potential benefits that different qos approaches could provide inparticular circumstances is one avenue for obtaining a better indication ofthe prospects for qos in the internet. another avenue is to accumulatemore experience with the effectiveness of qos in operational settings;here the challenge is that deployment may not occur without demonstrable benefits, while demonstrating those benefits would depend at leastin part on testing the effectiveness of qos under realistic conditions.the internet's coming of agecopyright national academy of sciences. all rights reserved.107what is referred to as òthe internetó is actually a set of independentnetworks interlinked to provide the appearance of a single, uniform network. interlinking these independent networks requires interconnectionrules, open interfaces, and mechanisms for common naming and addressing. (the issues associated with interlinking the internet with the publicswitched telephone network are considered separately in chapter 4.)the architecture of the internet is also designed to be neutral with respectto applications and context, a property referred to here as transparency.this chapter examines the current and expected future state of these interconnections and interfaces.interconnection: maintaining endtoend servicethrough multiple providersthe internet is designed to permit any end user ready access to anyand all other connected devices and users. in the internet, this designtranslates into a minimum requirement that there be a public addressspace to label all of the devices attached to all of the constituent networksand that data packets originating at devices located at each point throughout the networks can be transmitted to a device located at any other point.indeed, as viewed by the internetõs technical community in a documentthat articulates the basic architectural principles of the internet, the basicthe internet's coming of agecopyright national academy of sciences. all rights reserved.108the internetõs coming of agegoal of the internet is connectivity.1 internet users expect that theirinternet service provider will make the arrangements necessary for themto access any desired user or service. and those providing services orcontent over the internet expect that their internet service provider(s) willsimilarly allow any customer to reach them and allow them reach anypotential customer. (subject, of course, to whatever controls are imposedat the behest of the subscriber for security purposes.)to support these customer expectations, an internet service providermust have access to the rest of the internet. because these independentnetworks are organized and administered separately, they have to enterinto interconnection agreements with one or more other internet serviceproviders. the number and type of arrangements are determined bymany factors, including the scope and scale of the provider and the valueit places on access for its customers. without suitable interconnection, aninternet service provider cannot claim to be such a providerñbeing partof the internet is understood to mean having access to the full globalinternet.in 1995, interconnection relied on public network access points wheremultiple providers could exchange traffic.2 today, there is a much largerset of players and a much greater reliance on private interconnectsñthatis, direct pointtopoint linksñbetween major network providers. indeed, there are multiple arrangements for interconnecting internet service providers, encompassing both public and private (bilateral) mechanisms, connections between commercial networks and public networkfacilities, and even arrangements for connecting networks defined byownership or policy as ònationaló to the international internet complex.some of these international connections are constrained by concerns raisedby national governments about specific kinds of content being carriedover the internet.connections among internet service providers are driven primarilyby economicsñin essence who may have access to whom with what quality of access and at what priceñbut all kinds of considerations are translated into policies, frequently privately negotiated, that are implementedin the approaches to interconnection and routing. a significant feature oftodayõs competitive internet service marketplace is that direct competitors must reach interconnection agreements with each other in order toprovide the overall internet service that their customers desire. these1b. carpenter, ed. 1997. architectural principles of the internet, rfc 1958. network working group, internet engineering task force, june.2private interconnections existed then as well, but since everyone was also connected viathe governmentfunded nsfnet backbone, they were viewed as backdoor connections tohandle instances of high traffic volume.the internet's coming of agecopyright national academy of sciences. all rights reserved.keeping the internet the internet109business agreements cover the technical form of interconnection, themeans and methods for compensation for interconnection based on theservices provided, the grades and levels of service to be provided, and theprocessing and support of higher level protocols. interconnection alsorequires that parties to an agreement establish safeguardsñchiefly in theform of rules and proceduresñto ensure that one providerõs network isnot adversely affected by hostile behavior of customers of the other provider.while, as evidenced by the internetõs continued growth as an interconnected network of networks, the existing interconnection mechanismshave proven adequate thus far, concerns have been expressed about interconnection. interprovider, publicprivate, and international connectionsall raise questions of public policy, or internet governance. this sectionfocuses on interprovider connections because it is these connections thatdrive the shape and structure of the internet.structure of the internet service provider industrythere are several thousand internet service providers in the unitedstates.3 these providers cover a range of sizes, types of services theyprovide, and types of interconnections they have with other service providers. the internet service provider business has grown substantially,with entry by many new players, following the phasing out in the mid1990s of the governmentsupported nsfnet backbone. changes in thenature of these players are as significant as changes in the number. as themix has evolved, so have business strategies. one sees isps chasing particular segments of the market (e.g., they specialize in consumers or businesses or they run web server farms), trends toward consolidation thoughmergers and acquisitions, and moves to vertically integrate a full range ofservices, from internet access to entertainment, news, and ecommerce.the interlinked networks that are the internet form a complex web withmany layers and levels; the discussion that follows should not be taken tosuggest simplicity.43one source of information on internet service providers is boardwatch magazineõs directory of internet service providers. golden, colo.: penton media, june 1999. available onlinefrom <http://boardwatch.internet.com/isp/summer99/introduction.html>), it lists 5078isps in north america, a figure that covers a wide range of sizes and business models.4see, for example, the results of bell labsõ internet mapping project, which provides avisualization of data gathered in mid1999 indicating the complexity of the internet. anumber of maps are available online at <http://www.cs.belllabs.com/who/ches/map/gallery/index.html>.the internet's coming of agecopyright national academy of sciences. all rights reserved.110the internetõs coming of agea straightforward and useful way to categorize isps is in terms of theinterconnection arrangements they have in place with other providers.the backbone service providers, which include commercial companies aswell as several governmentsponsored networks like doeõs esnet, usetrunk capacities that are measured in gigabits, or billions of bits, per second. roughly a dozen of the isp companies provide the backbone services that carry a majority of internet traffic. these providers, termedòtier 1,ó are (recursively) defined as those providers that have full peeringwith at least the other tier 1 backbone providers. tier 1 backbones bydefinition must keep track of global routing information that allows themto route data to all possible destinations on the internetñwhich packetsgo to which peers. they also must ensure that their own routing information is distributed such that data from anywhere else in the internet willproperly be routed back to its network. tier 1 status is a coveted positionfor any isp, primarily because there are so few of them and because theyenjoy lowcost interconnection agreements with other networks. they donot pay for exchanging traffic with other tier 1 providers; the peeringrelationship is accompanied by an expectation that traffic flowsñand anycosts associated with accepting the other providerõs traffic between tier 1networksñare symmetrical. tier 1 status also means, by definition, thatan isp does not have to pay for transit service.much of the internetõs backbone capacity is concentrated in the handsof a small number of tier 1 providers, and there is some question as towhether it is likely to become even more concentrated, in part throughmergers and acquisitions. concerns about market share in this segmenthave already emerged in the context of the 1998 merger between mci andworldcom, at that time the largest and second largest internet backboneproviders. in that instance, european union regulators expressed concerns about the dominant market share that would have resulted fromsuch a combination. in the end, to get approval for the merger, some ofmciõs internet infrastructure as well as mciõs residential and businesscustomer base was sold off to cable & wireless and the merger wentforward.5some of the advantage held by the very large players lies in theirability, owing to their large, global networks, to provide customers willing to pay for it an assured level and quality of service. these very largecompanies provide customers with solutions intended to allow those customers, in turn, to connect with higher levels of performance to other5see, for example, mike mills. 1998. òcable & wireless, mci reach deal; british firm tobuy entire internet assets.ó washington post. july 14, p. c1.the internet's coming of agecopyright national academy of sciences. all rights reserved.keeping the internet the internet111users in the same network, using such technologies as virtual privatenetworks, and they also offer widely dispersed customers the convenienceof onestop shopping. such large players also allow customers to interconnect to the public internet but generally without making the serviceguarantees. part of their dominant position also stems from their tier 1status, which assures their customers (including tier 2 and tier 3 isps) oftheir ability to provide a high quality of access to the public internet. inaddition, tier 1 providers, by determining how and with whom they interconnect, affect the position of wouldbe competitors.below tier 1 sit a number of socalled tier 2 and tier 3 service providers, which connect corporate and individual clients (which, in turn, connect users) to the internet backbone and offer them varying types of service according to the needs of the target marketplaces. this group spans awide range of sizes and types of providers, including both a small set ofvery large providers aimed at individual/household customers (e.g.,america online) and a large number of smaller providers. these includeproviders of national or regional scale as well as many small providersoffering dialup service in only a limited set of area codes.6 a recent trendhas been the emergence of socalled free isps, which provide residentialinternet service at no charge, typically in exchange for a demographicprofile of the customer and an agreement by the customer to view advertising material delivered along with the internet service. this class alsoincludes the networks operated by large organizations, including those oflarge corporations, educational institutions, and some parts of government. these isps cannot generally rely on peering alone and must enterinto transit agreements and pay for delivery of at least some of theirtraffic. some of these providers have not invested significantly in building their own facilities; instead they act as resellers of both access facilities(e.g., dialup modem banks) and connectivity to the internet backbone.while industry analysts have long predicted increased consolidationand the demise of the smaller providers, recent trends indicate that thebusiness remains open to a large number of players.7 however, optimismhere is tempered by two considerations. first, many of the very smallplayers are only active in small markets or geographical regions. second,6matt richtel. 1999. òsmall internet providers survive among the giants.ó new yorktimes. august 16, p. d1.7boardwatch magazineõs directory of internet service providers in north america showedcontinual growth in the number of isps from february 1996 to july 1999. see boardwatchmagazineõs directory of internet service providers. golden, colo.: penton media, june 1999.available online from <http://boardwatch.internet.com/isp/summer99/introduction.html>.the internet's coming of agecopyright national academy of sciences. all rights reserved.112the internetõs coming of agesubscriber data show that a single player, america online, with morethan 20 million subscribers, has a significant share of the consumer market.8 another area of interest is the emerging broadband market. therecent flap over open access illustrates the concerns that some have aboutthe market share and the behavior of the providers of the communications links themselves (i.e., the facilitiesõ owners), the internet serviceproviders, and the content providers, with which both facilities and service providers may have business arrangements.another recent trend has been the establishment of a new form of isp,the hosting provider. this type of isp operates both singlecustomer(dedicated) and sharedapplication servers, typically providing web services on behalf of companies who would rather outsource the management of machine rooms and internet connectivity. they offer customers acertain level of service (as seen by those throughout the internet thatmake use of the customerõs service) by arranging for (purchasing) transitservices with a sufficient set of backbone connections.interconnection mechanisms and agreementsinternet interconnection arrangements in some ways echo those oftelephony, since the public telephone network is also a collection of distinct networks linked together to provide a uniform service. however,telephony, unlike the internet, leverages and reflects decades of state,federal, and international regulation and standardssetting that haveshaped the terms and conditions of interconnection, including financialsettlements. internet interconnection, by comparison, is relatively new,and the technology, market structure, and arrangements are evolving.providing internetwide interconnectivity requires that the partieswho own and operate the constituent networks reach agreement on howthey will interconnect their networks. the discussion in this sectionlooks at interconnection at three levels: the physical means of interconnection, the different patterns of traffic exchanged by providers (transitand peer), and the financial arrangements that underlie and support thephysical means and different traffic patterns. the focus here is on teasing out the essential elements of interconnection, but this should not betaken to mean that interconnection is a simple matter. there are manyplayers at many levels, and in each case there is more than one choice ofphysical interconnection, logical interconnection, and financial arrange8data from telecommunications reportõs online census, january 2000, reported in davidlake. 2000. òno deposit, no return: hard numbers on free isps.ó the industry standard,march 27.the internet's coming of agecopyright national academy of sciences. all rights reserved.keeping the internet the internet113ment, and implementation of each choice depends on a complex set ofnegotiated agreements.physical interconnectionpublic exchanges are a way of making the interconnections between anumber of providers more costeffective. if n providers were individually to establish pairwise interconnections, they would require n(nð1)/2direct circuits. a public exchange, where all n providers can connect at acommon location, permits this to be done much more inexpensively, withn circuits and a single exchange point. a provider interconnects to anexchange point, either physicallyñby installing his own equipment andcircuit into a specific location (e.g., the maewest facility at nasa amesresearch center or the sprint nap in pensauken, new jersey)ñor logicallyñby using a leased network connection to an interconnect providerthrough an atm or ethernet network (e.g., the maeeast atm nap innorthern virginia or the ameritech atm nap in chicago). these interconnect networks are usually operated by large access providers, whohope to derive considerable revenue by selling access lines to isps wishing to attach to each other through the access providerõs facilities.9in recent years, the public interconnects have acquired a relativelypoor reputation for quality, in part owing to congested access lines fromthe exchanges to tier 1 providers, which results in packet loss, and in partowing to exchange point technology that cannot operate at speeds comparable to major backbone trunks. this trend is likely to accelerate aslarge backbones move to extremely highspeed wavelength division multiplexing (wdm)based trunking, which exceeds the data rates that canbe handled by todayõs exchange point technology.another option is to use a direct, pointtopoint connection. onemotivation for pointtopoint connections is to bypass the bottleneckposed by a public exchange point when traffic volumes are large. between large providers, connections are usually based on highperformance private interconnects, for example pointtopoint links at highspeeds (ds3 or higher). direct connection can also provide for bettermanagement of traffic flows. the very large volume of traffic that wouldbe associated with a major public access point can be disaggregated intosmaller, more easily implemented connections (e.g., a provider manages9if they provide direct connections to multiple provider networks, public exchanges canalso turn out to be very efficient places to locate other services such as caches, dns servers,and web hosting services. and because public exchanges bring together connections tovarious providers, they are also useful places to conduct private bilateral connectionthrough separate facilities.the internet's coming of agecopyright national academy of sciences. all rights reserved.114the internetõs coming of age10 oc3 connections to 10 different peers in different locations rather thana single oc48 connection to a single exchange point that then connects tomultiple providers). another reason for entering into private connectionsis the desire to provide support for the particular service level agreementsand qualityofservice provisions that two networks agree to in their peering or transit agreement.logical (routing) interconnectionwhen two or more isps establish an interconnection, they exchangeroute advertisements to specify which data packets are to be exchangedbetween them. route advertisements describe the destination internetaddresses for which each provider chooses to accept packets from theother. these advertised routes are loaded, generally through automatedmechanisms, into each otherõs routing tables and are used to determinewhere (including to which providers) packets should be routed based ontheir destination address.there are two common options for how providers accept each otherõstraffic: transit and peer. in the transit model, the transit provider agreesto accept and deliver all traffic destined for any part of the internet fromanother provider that is the transit customer. it is possible that two providers in a transit arrangement will exchange explicit routing information, but more typically the transit provider provides the transit customerwith a default route to the transit network while the transit customerprovides the transit provider with an explicit set of routes to thecustomerõs network. the transit customer then simply delivers to thetransit provider all packets destined for ip addresses outside its ownnetwork. each transit provider establishes rules as to how another network will be served and at what cost. the transit provider will thendistribute routing information from the transit customer to other backbones and network providers and will guarantee that full connectivity isprovided. address space for the customer provider may come from itstransit provider or from its own independent address space should thatprovider have qualified for such allocation. (the issues surrounding address allocation and assignment are discussed in chapter 2.) 10the preferred way for large providers today to interconnect is throughpeer arrangements. in contrast to transit arrangements, where one provider agrees to accept from the other traffic destined for any part of the10some providers or customers engage in the practice of multihoming, whereby theyestablish transit connections with multiple isps, generally to provide redundancy. this canintroduce both technical and management issues, including how to allocate traffic amongthe multiple paths, that will not be discussed in detail here.the internet's coming of agecopyright national academy of sciences. all rights reserved.keeping the internet the internet115internet, in a peering relationship, each provider only accepts traffic destined for the part of the internet it provides. peers exchange explicitrouting information about all of their own addresses along with all of theaddresses of their transit customers. based on that routing information,each peer only receives traffic destined for itself and its transit clients.this exchange of routing information takes the form of automated exchanges among routers. because the propagation of incorrect routinginformation can adversely affect network operations, each provider needsto validate the routing information that is exchanged.for smaller providers the only option (if any) for physical interconnection is typically at a public exchange point. location at a peering pointimplies that the peering relationship may still suffer from poor (or at leastuncontrolled) service quality, since the exchange point or the connectionsto it may be congested; they may, however, be very costeffective, especially for smaller providers. once interconnectivity is established througha public exchange, providers may attempt to enter into a bilateral peeringagreement with other providers located at the same interconnect. thiscan be a costeffective means of bilateral peering, because connectivity tomany other providers can be aggregated onto a single connection to theexchange.financial arrangements for interconnectionthe issue of compensation for interconnection is a complex one. theessence of interconnection is the handing over of packets, according to therouting information that has been exchanged, to be routed onward toward their destination. compensation reflects the costs associated withprovisioning and operating sufficient network capacity between andwithin isp networks. as a basic unit of interconnection, packets are somewhat akin to call minutes in voice telecommunications. however, architectural differences between the internet and pstn make accounting interms of packets much more complicated than callminutebased accounting. even if an infrastructure were to be put in place to count and chargeon a packetbypacket basis, the characteristics of packet routing wouldmake it difficult to know what the cost associated with transmitting agiven packet would be.11 as a result, interconnection schemes that are11several of these characteristics are noted in a paper by geoff huston. 1999. interconnection, peering, and settlements, technical report. canberra, australia: telstra corporation,ltd., january. they include the following: packets may be dropped in the course of theirtransmission across the internet; the paths that packets follow are not predetermined andcan be manipulated by the end user; and complete routing information is not available at allpoints, so that the undeliverability of a packet may not be known until it approaches itsdestination.the internet's coming of agecopyright national academy of sciences. all rights reserved.116the internetõs coming of ageused in other contexts, such as the bilateral settlements employed in international telephony, are not used in the internet, and interconnection hasgenerally been established on the basis of more aggregated informationabout the traffic exchanged between providers. some of these issues haveto do with the cost of the interconnection, traffic imbalances (e.g., oneprovider originates more traffic than it terminates), and relative size (oneprovider offers greater access to users, services, and locations than theother). two financial models predominate; one is linked to the transitmodel and the other to the peer provider model discussed above.in the transit model, a transit customer buys transit service from atransit provider and pays for an access line to that larger providerõs network. these arrangements take the form of bilateral agreements thatspecify compensation (if any) and the terms of interconnection, includingservice guarantees (level and quality of service) that each party makes. inthe early days of the commercial internet, providers did not pay for transit services. before isps insisted on payment for transit, nonbackboneisps could become free riders in the socalled hot potato scenario, wherebya network would dump traffic for destinations beyond those advertisedby a particular provider, thereby forcing the backbone isp to carry trafficit had not agreed to carry. private interconnects help prevent free riding,because it is more straightforward to identify this condition given a directmapping between the link and a single provider.in the peer model, two isps agree to a peer relationship based on aperception of comparable value. these agreements are generally barteragreements between peers that assume an exchange of a roughly comparable level of traffic or, on some other basis, that the costs and benefits ofa peer relationship will be mutually beneficial. peer barter arrangementsecho what is called in telephony òsender keeps alló or òbill and keepóñthe network to which a customer connects keeps the fees paid by thatcustomer for traffic carried on both its and another providerõs network.peering among the tier 1 providers is perhaps the most visible, but peering is also conducted among smaller players and at the regional or locallevel. logical peering and financial peer relationships generally coincide,but there are exceptions. in some instances a customer will pay for anontransit service that, logically though not financially, looks like peering. for example, isp a may pay isp b for access to bõs customers but notbõs peers.the value attached to either transit or peer relationships is not basedonly on the number of bits exchanged nor is it based solely on the origin,destination, or distanceñit also reflects the value attached to particularcontent. consider, for example, a large, consumerfocused isp (òisp aó)and a major, popular content provider that is connected to the internetthrough another provider (òisp bó). isp a will be judged by its customthe internet's coming of agecopyright national academy of sciences. all rights reserved.keeping the internet the internet117ers based on the quality of service that it provides. to the extent that aõscustomers value content directly available from isp b, customer judgment of isp a will depend on the quality of the interconnect establishedbetween a and b. thus isp a may be willing to pay extra for highercapacity links to isp b in order to ensure better performance for customers accessing the content provider. the complementary argument mayalso hold true: the content provider may well derive revenue from advertising that in turn depends on the return rate of viewers, so it (and, consequently, its isp) will be willing to pay extra for interconnection relationships that ensure that customers of isp a receive a good quality of service.(this is a major business consideration for the internet hosting providersdescribed below.) accordingly, the performance that a consumer experiences with a particular piece of content depends in part on the capacity ofthe interconnects between the consumerõs and content providerõs computers, which in turn depends in part on the willingness and ability of theconsumer and content provider (and their isps) to pay for those interconnections.chapter 2 discusses a number of issues surrounding quality of service(qos) mechanisms, including the dim prospects for deployment of interprovider qos; here we discuss some issues related to interconnection. ifthe stresses associated with the development and evolution of todayõspeering and transit agreements, which have generally only addressedmuch broader service level agreements, are any guide, establishing agreements that enable interprovider quality of service would prove difficult.providing guarantees of better service to a subset of users means thatresources are set aside that become unavailable for other users. this canonly develop if higher grades of quality of service are sold at a premiumprice and if there are mechanisms to adequately compensate isps. if thenecessary business agreements would take years to develop, then interprovider qos would take years to deploy. also, congested interconnections exacerbate qualityofservice differences between connections acrossa given providerõs network, as compared with connections across multiple provider networks. they often result in companies connecting alltheir sites through a single providerõs network rather than through avariety of providers and depending on this interprovider connectivity.they also result in large contenthosting providers almost always attaching to each of the major backbone networks (usually as a transit customerrather than a peer) to bypass interprovider interconnects and improveoverall robustness of access for their customers.specific mechanisms for quality of service are starting to show up inparts of the internet, but not as generally deployed, endtoend servicesthat any application can take advantage of to reach users internetwide.they are being offered only inside specific isps as product differentiatorsthe internet's coming of agecopyright national academy of sciences. all rights reserved.118the internetõs coming of ageor being bundled with specific applications (such as internet telephony).thus there is pressure for alternatives to the baseline internet. whatcontent or service providers do today is enter into an agreement with acompany that delivers specialized content services located throughoutthe internet so as to improve the quality of the connection seen by endusers. for example, realaudio or akamai will load streaming mediacontent onto their servers. the internet is being overlaid by these applicationspecific delivery networks. these overlay networks do not provideendtoend connectivity between the original content or service providerand the end user and are open only to those providers who are willingand able to pay for specialized services.considerations affecting decisions to enter into peering agreementsas noted above, peer status is advantageous to isps because it meansthat they will not have to pay other providers for transit and because forits customers it is taken as evidence of a high service quality. in makingfinancial arrangements to support its interconnection with the rest of theinternet, each provider is strongly motivated to maximize its revenuestreams from its customers and minimize its expenses, including chargespaid to other providers. there are, therefore, natural pressures for eachprovider to want to become a peer and for a peer to resist one of itscustomers asserting a peer relationship and for it to resist one of its peersasserting that it should, in fact, be a customer rather than a peer.the issue of who could attain peer status first received considerablepublicity in 1997 as a result of announcements by uunet and sprint thatthey would no longer peer freely with any and all networks (along withan announcement by psinet that it explicitly would agree to peer withsmaller players), raising concerns in some circles about the implicationsfor smaller networks. these concerns have lingered; reflecting the significant barrier to entry for an internet provider that peering represents,smaller isps and new entrants have resorted to litigation to attempt toattain peering.part of the difficulty of assessing peering issues is the fact that theterms of peering agreements are private. there are some generally understood criteria used by backbone providers to determine whether ornot another network qualifies for peering or is viewed as a potentialcustomer for that ispõs transit services. these criteria generally include(1) having a national network with a dedicated transcontinental backboneof at least a certain speed, (2) exchanging a minimum amount of trafficwith that isp (usually with comparable amounts of traffic travelling inboth directions), (3) providing aroundtheclock operational support, and(4) agreeing to abide by certain rules and policies in how routing traffic isthe internet's coming of agecopyright national academy of sciences. all rights reserved.keeping the internet the internet119processed and/or filtered. however, these are only general considerations, and the terms of peering agreements are private. as the standardsand requirements are usually covered by nondisclosure agreements, theircontents and components are not widely known. this private approachreflects in part the multidimensional, subjective determination of who isand who is not a peer.isps are motivated to be conservative in this process. if an isp publishes explicit rules, these are an invitation to lawsuits in the event that iteither declines to peer with an entity that arguably meets the publishedrules or agrees to peer with an entity that arguably does not. by contrast,companies that really are peers, and thus clearly stand to benefit frompeering, will usually realize this and conclude appropriate agreements ifdiscussions can be carried out in a relatively private context. if an objective framework for deciding who are peers were to be developed, it wouldentail either the industry itself agreeing on one or a governmental (or,given the internetõs global reach, an intergovernmental) entity developing one, both of which are problematical and may lead to fewer peeringarrangements, not more. absent such a framework, peering will be basedon the premise that two parties try to prove to each other that they arepeers.the economics of a proposed peering relationship is the dominant,but not the only, consideration that goes into a decision to peer or not.fundamentally, agreements between tier 1 providers and smaller providers pose additional challenges because the asymmetrical traffic carried bythe two classes raises questions about compensation for the costs associated with the connection and the termination of the smaller networkõstraffic. the absolute volume of traffic also matters. because the expenseassociated with setting up dedicated links makes sense only when lots oftraffic is being exchanged, it is not costeffective to establish a privateinterconnect unless a significant amount of traffic is being exchanged.few small providers are in a position to use the private peering approach,and large providers are unlikely to view private interconnects with smallproviders as attractive. the costs of establishing these links are also usually much less for a facilitiesbased provider; nonfacilitiesbased providers are at a cost disadvantage in implementing such interconnects. thereare also concerns on the part of tier 1 providers about the potential forfreeriding in peering. for this reason, many large tier 1 backbone providers are reluctant to peer with smaller networks because doing so wouldopen them up to this vulnerability.1212interconnect technologies that provide more pointtopoint control over traffic flows,such as atm, offer some advantages in dealing with this problem but do not completelyeliminate it.the internet's coming of agecopyright national academy of sciences. all rights reserved.120the internetõs coming of agepeers need not be the same size, and there are cases where the majorbackbones will peer with smaller providers despite the asymmetry intraffic capacity. for instance, even if swapping traffic on a barter basiswould not be supported based solely on the amount and balance of trafficexchange, it may prove attractive to the backbone provider if the smallerprovider has a network, albeit modest in size, that is national or worldwide in scope. it is possible in such circumstances to fashion a peeringagreement in which the smaller provider interconnects with the backboneat enough places such that all the smaller providerõs traffic stays withinthe smaller providerõs own network most of the time, thus minimizingthe cost to the larger peer.competitive positioning also enters into the equation. in general,there is an interest in retaining a competitive advantage over new entrants. the type of interconnections that a provider has in place is animportant business consideration because it establishes the service quality that customers experience when transferring data across the providerõsboundary. indeed, many wouldbe isp customers rely on the type ofpeering being used as an indicator of quality. because private interconnects can provide a better service quality owing to their greater capacity,dedicated nature, and ability to more carefully manage the traffic acrossthem, the existence of such interconnects is often seen by customers as asign that a provider offers generally higher quality internet service.13 peerstatus is used at least in part because there are no agreedon quantitativemetrics and processes for evaluating the quality of internet interconnections, particularly public metrics that detail the status of connectivity.the question of measuring quality is exacerbated by the dynamic growthin the volume of traffic throughout the internet as well as by changes inthe types of traffic being carried.in addition, to alleviate concerns that a customer of today may become a competitor tomorrow, most tier 1 providers have rules in placedisqualifying existing customers from becoming peers. thus, a smallerprovider just entering the market may find that it must by definitionpurchase transit in order to provide internet access service, but that itsstatus as a customer will prohibit it from attaining peer status in thefuture. these trends of course reinforce the position of the establishedlarger players. indeed, it has been asserted that in the past several yearsno isp has been able to attain tier 1 status without doing so by purchasing13large customers often specify tier 1 status as an element in requests for proposals forinternet service (at least in part because there are no wellagreedto quantitative metricsand processes for evaluating the quality of internet interconnection; this problem is exacerbated by the dynamic growth of traffic in backbones, such that connectivity that might havebeen considered good at one point in time may be wholly inadequate 6 months later).the internet's coming of agecopyright national academy of sciences. all rights reserved.keeping the internet the internet121an isp that already had full peering status.14 as a result, in order to growits traffic with a tier 1 provider so that it can attain peer status, a newentrant may have to discount its product substantially to entice new customers who will put up with a lower quality of service in the meantime.alternatively, it could pay a substantial amount to a backbone provider inorder to achieve high quality interconnection or purchase a provider thatalready has peering agreements.robustness considerations also come into play when providers consider entering into a peering relationship. limits in the protocols used toexchange critical routing information as well as in the hardware and software used in the core of the internet mean that full peering can lead to abreakdown of the routing system. provider a may peer with provider bin such a way that provider b is only supposed to provide routing information for provider bõs directly attached customers and those providersit is providing transit for. but a very simple error in a routing configuration may flood provider aõs router with bad routing data such that a largeamount of provider aõs traffic would be inadvertently sent through provider b, which may not have the capacity to properly deliver that data,resulting in a service outage for provider aõs users.15 also, while routinginformation is exchanged and processed automatically, configuring therouting requires judgment, as does troubleshooting when problems arise.problem resolution depends in part on informal interactions among network operators. isps must acquire the necessary skills, some of which arebest obtained from prior employment at a provider that already has anetwork of the size, scope, and complexity of a tier 1 provider. as aconsequence, an isp may avoid entering into a peer relationship withanother provider if it feels the other provider does not have the properpersonnel or processes in place to prevent routing disturbances.evolution of interconnection modelsthe discussion above reflects the interconnection arrangements thathave historically prevailed in the internet industry. recently there have14the cook report on internet, november 1999, p. 10. available online at <http://www.cookreport.com/08.08.shtml>.15one example of this was reported in òrisks list.ó risks digest 19(12), may 2, 1997. òon23 apr[il] 1997 at 11:14 am edt, internet service providers lost contact with nearly all of theu.s. internet backbone providers. as a result, much of the internet was disconnected, someparts for 20 minutes, some for up to 3 hours. the problem was attributed to mai networkservices . . . which provided sprint and other backbone providers with incorrect routingtables, the result of which was that mai was flooded with traffic.ó available online at<http://www.infowar.com/iwftp/risks/risks19/1912.txt>.the internet's coming of agecopyright national academy of sciences. all rights reserved.122the internetõs coming of agebeen innovations in interconnection that provide alternatives to the conventional binary choice between attaining peer status or becoming a transit customer. similar innovations have been tried before on a notforprofit basis, such as in some of the public exchanges. what is new todayare moves by both existing players and new entrants to use new businessmodels for interconnection. the committee is aware of a number of instances where tier 1 providers have entered into arrangements that aresomewhere between the pure peer interconnection and the payfortransit interconnection. as with conventional peering agreements, the termsare subject to nondisclosure, so it is difficult to characterize or examinethem in detail. but their existence is one indication that the internetindustry is responding to market forces by providing such alternatives.one prominent example of an entrant following a new business modelis internap, which has built a business around providing an alternativeto conventional peering or transit. internap establishes highperformance interconnection points in key locations and then connects thesepoints to top isps, including a number of the tier 1 providers. the connection arrangement lies halfway between peering and transit. the businessrelationship resembles a transit model inasmuch as internap pays for theconnection/service and thus has a predictableservicelevel agreement.but the routing relationship is peeringñit only forwards traffic into anisp if it will be terminated there. it does have to pay for transit services insome cases but claims that a majority of the routes in the default freezones are available to it through peer routing. internap is able to pay areasonable price for its connections because it is mostly delivering trafficinto each isp that would eventually get there anyway by some otherroute. with these interconnection relationships established, it then sellsinternet access to smaller isps, whoñit claimsñreceive a better qualityservice than if they had purchased transit service from just one isp ormade use of a public exchange and experience less hassle than if they hadtried to negotiate a number of peering relationships on their own.internap also provides service to major web hosts, who then can connectto most large isps without having to manage individual relationshipswith them.because it was historically seen as too cumbersome to charge on anindividual basis for the transfer of data packets, the internet has traditionally been based on the establishment of revenueneutral boundaries at itscenter, where the major tier 1 isps connect, and on the selling of transitservice to downstream providers on the basis of rough measures such aslink capacity or average data rates. this simple model has worked reasonably well but does not give content providers or isps any way to makeadditional payments to support a desired service level. nor are thereprocesses in place that allow a content producer to transfer moneythe internet's coming of agecopyright national academy of sciences. all rights reserved.keeping the internet the internet123through its isp to the isps of its target consumers to reduce the costs thateither isp incurs in carrying its content to the target consumers.in response to these perceived shortcomings in the internetõs interconnection arrangements, content cache providers are introducing newfinancial models. akamai provides a good example. it places serverswithin the networks of isps, as near the consumer as possible. contentproducers who want to ensure access by their customers pay akamai tohost their content, and akamai (in some cases) pays the isp to host theakamai server. this new financial mechanism allows the content producer to pay the isps that serve end users. the arrangement has bothadvantages and disadvantages. one advantage is that it provides anonconsumer source of revenue for isps. on the other hand, as discussedin more detail below, this sort of infrastructure is an applicationspecificdelivery overlay network that a producer can only use by paying for it, soit is somewhat of a departure from the internetõs traditional architecture.monitoring internet interconnectionin contrast to telephony, which has been the subject for many years ofeconomic oversight and regulation, the internet is by and large unregulated, with the federal communications commission having, thus far,demonstrated no interest in intervening in it. another avenue for intervention is the application of antitrust law in particular instances; in theinternet context, this has taken the form of reviews of proposed mergers.it is the view of this committee that current policy should continue, asshould monitoring.the internet interconnection market model has risks. it assumes areasonably competitive environment, where competition among ispskeeps transit agreement charges reasonable, where there is no one isp sodominant that it can refuse to peer with any other and thus force all theother isps to pay for access to the dominant ispõs customers, and wherethere is not pervasive vertical integration of backbone isp and contentand service businesses. the small number of tier 1 providers and thedifficulties of attaining this status have, however, raised concern aboutthe competitiveness of the isp marketplace, in particular about the barriers to market entry. as discussed above, for a provider to attain tier 1status, it must by definition reach peering agreements with all (or at leastmost) other tier 1 providers; a providerõs inability to reach agreementswith all or most of them is sufficient to prevent the provider from becoming a tier 1 provider. additionally, providers offering transit service frequently incorporate into their interconnection agreements restrictions ontransit customers becoming peers. thus, where a provider starts withsome or all of its relationships being of the transit sort, it may be unable tothe internet's coming of agecopyright national academy of sciences. all rights reserved.124the internetõs coming of ageattain tier 1 status. the emergence of alternatives to the pure peer ortransit interconnection models suggests, however, that the marketplacemay be finding ways to reduce these pressures by introducing interconnection models that better suit the business needs of parties that seek toestablish an interconnection agreement.the existence of public exchanges means that some form of connection to the internet is generally available to all providers; this, in turn,means that concerns can focus on the nature, terms, and quality of interconnection rather than on participation. while these considerations arealso key to interconnection in the pstn, they are significantly more complex and dynamic in the internet. at any given point in time there is awide range of applications and services in use across the internet, eachwith different implications for interconnection. and all indications arethat new types of applications and services will continue to emerge on aregular basis. related to this is the variability in the value attached tointernet data packetsñin other words, the price that a party would, inprinciple, be willing to pay for transmission of an individual packet depends on a number of factors, including the application the packet isassociated with, its content, and its points of origination and termination.openness and innovationthe internet was developed first as a joint research effort and then asa joint engineering effort by the research community. the standardsdevelopment process, which came to be formalized through the internetengineering task force (made up of technical experts from academia andindustry), emphasized standardization because it grew out of a highlydiffuse but collaborative development environment. in that environment,new concepts would be experimented with and then a project would belaunched that included parallel development of implementations andstandards. eventually, several companies would offer compatible products. these attributes were responsible for a healthy competition amongdesigns for applications and the protocols they use and frequently for thedevelopment and availability of multiple implementations of productsthat would be available from multiple vendors. crucially, these multipleimplementations have been consistent with the development and adoption of a single standard for key functions. it should be noted that the term òstandardó refers to several differenttypes of specifications, including the following:1.an application programming interface (api) published by a software provider. a developer may need to enter into a contract to make usethe internet's coming of agecopyright national academy of sciences. all rights reserved.keeping the internet the internet125of the api, and the specification is subject to change at any time at thediscretion of the software provider;2.a complete specification published by a corporation (e.g., sunõsjava language, the microsoft windows apis, and the microsoft/inteldriven pc architecture). in such cases, the companies or organizationsthat develop the specification have at least some degree of control overwhat changes are made to the specification.3.an open specification published by a neutral institution, such asthe world wide web consortium (w3c) or the ietf. developed underappropriate procedures, this approach permits multiple actors to developand control a standard without running afoul of antitrust laws.4.a standard that is enforced by some regulatory authority, such asthe national television systems committee (ntsc). these are standardsthat have, until the development of the hdtv standard, been mandatedfor all u.s. television broadcasters. many of them are developed in industry (by individual companies or industry consortiums) and are thenadopted as official standards.the core standards employed in the internet tend to fall into the thirdcategory, although some fall into the second.16the terms òstandardó and òopen standardó are not synonymous. theinternet model for development is characterized by openness, which refers to the ability of multiple vendors to independently construct products that work with one another. openness means that customers canmix products from one vendor with products from another (e.g., use onevendorõs client software with another vendorõs server) and that applications from one vendor operate over infrastructure provided by another.openness relies crucially on the development and adoption of standards.an interface designed with only one vendorõs products in mind can bereadily implemented only in that single vendorõs environment. from thisperspective, privately designed interfaces that are publicly published arenot open interfaces. standardization processes enable multiple vendorsto cooperate on the development of new elements that will allow them to16closely allied with open standards is the practice of open source, best known as themechanism through which the linux operating system is distributed and developed. opensource practices resemble the third type (open specification published by a neutral institution) but in a somewhat different fashion. all parties are free to implement their ownmodifications to the open source (with the use of the resulting code subject to whatever useagreement was attached to the original code base), but an individual or organization generally decides which modifications are incorporated into what is considered the standardcode base.the internet's coming of agecopyright national academy of sciences. all rights reserved.126the internetõs coming of agedevelop new marketsñmarkets that are much larger than would be thecase if each developed its own competing technology. then the vendorscompete by providing competitive products that build on the standardized elements. when this process works well, it results in greater benefitsfor both vendors and customers.critical open standards in the internetñthe hourglass architecturethe existence of an abstract bitlevel network service as a separatelayer in a multilayer suite of protocols provides a critical separation between the actual network technology and the higherlevel servicesthrough which users actually interact with the internet. realizing theinformation future17 depicted this layered modularity as an hourglass,with an òopen bearer serviceó at the narrow waist (figure 3.1). in theinternet, this abstract, bitlevel transport service is provided by theinternet protocol (ip). at this level, bits are bits and nothing more. abovethe waist, the glass broadens out to include a range of options for datatransport and applications. right above the ip layer is the transport layer,which is made up of the enhancements that transform the basic ip bittransport service into the range of endtoend delivery services needed bythe applicationsñreliable, sequenced delivery; flow control; and endpoint connection establishment. the transport control protocol (tcp),the most commonly used transport mechanism, is often lumped togetherwith ip as tcp/ip. there is, however, an important distinction: ipdefines those features that must be implemented inside the network, inthe switches and routers, while the transport layer defines services thatare the responsibility of the end node. the upper layers, above ip andtransport, are where the applications recognized by typical users reside,such as email, streaming audio and video, and the web. the technologies below the waist make up the bitcarrying infrastructure and includeboth the communication links (copper wire, optical fiber, wireless links,and so on) and the communication switches (packet routers, circuitswitches, and the like). figure 3.2 shows where some familiar internettechnologies, protocols, and applications fit within the hourglassconstruct.imposing a narrow point in the protocol stack removes from the application builder the need to worry about details and evolution of theunderlying network facilities and removes from the network provider the17computer science and telecommunications board (cstb), national research council.1994. realizing the information future: the internet and beyond. washington, d.c.: nationalacademy press.the internet's coming of agecopyright national academy of sciences. all rights reserved.keeping the internet the internet127open bearerservice interfaceelectronic mailvideo serveraudio serverteleconferencingremotelogininformationbrowsingfinancialservicesinteractiveeducationfaximageserverapplicationsmiddleware servicestransport services and representation standards(fax, video, audio, text, and so on)filesystemssecurityprivacyname serversstoragerepositorieselectronicmoneyservicedirectoriesmultisitecoordinationodn bearer servicenetwork technology substratelayer 4layer 3layer 2layer 1lansatmwirelessdirectbroadcastsatellitedialupmodemspointtopointcircuitsframe relaysmdsfigure 3.1 the hourglass model of internet architecture. source: computerscience and telecommunications board (cstb), national research council. 1994.realizing the information future: the internet and beyond. washington, d.c.: national academy press.the internet's coming of agecopyright national academy of sciences. all rights reserved.128the internetõs coming of ageemailfigure 3.2 how some internetrelated technologies, protocols, and applicationsfit into the hourglass model. source: adapted from a figure by steve deering,cisco systems.need to make changes in response to whatever standards are in use at thehigher levels. this separation of ip from the higherlevel conventions isone of the tools that ensure an open network; it hinders, for example, anetwork provider from insisting that only a controlled set of higherlevelstandards should be used on the network, a requirement that would inthe internet's coming of agecopyright national academy of sciences. all rights reserved.keeping the internet the internet129hibit the development and use of new services and might be used to limitcompetition. the core functionsñthose that lie at or near the waist of thehourglassñare the most critical functions where openness must be guaranteed to enable innovation. when these core interfaces to the networkare not open, multivendor application innovation is more difficult. possible consequences include constrained user choice and deterioration inthe quality of products that vendors offer.just which functions should be considered to lie in the waist of thehourglassñthat is, implemented according to a single, internetwide standard and available throughout the internetñis open to interpretation anddebate, and there is no consensus among those who design, operate, oruse the internet. the core standards are understood by many to includemore than just ip, but opinions differ as to what else should be included.indeed, in the hourglass metaphor, the curved side walls of the glass donot draw a sharp distinction between what is in the waist and what liesabove it. what other than ip is needed in practice?¥domain name system. the dns, which provides a common set ofnames for hosts connected to the internet, is generally viewed as an essential core function of the internet. some would also include additionalnetwork directory services in the category of core functionality. (while asingle directory service has not been universally adopted, this is one ofthe solutions offered to deal with conflicts between the dns name spaceand trademarked names; see box 2.1 in chapter 2.)¥routing protocols. providers must typically exchange routing information at interconnect points. have the routing structure and routingprotocols become critical enough for interoperability that they should beconsidered to lie at the core? this issue is especially important in the lightof increasing doubts that todayõs routing architectures will continue to beadequate as the network continues to expand.¥dynamic host configuration protocol (dhcp). this network protocol enables a dhcp server to automatically assign an ip address to anindividual device attached to a network. it might be the case that manyapplications that do not even require tcp functionality will still dependon dhcp to obtain an internet address when they are started up.some also believe that significant benefits would result if standardmechanisms for authentication were widely available. this and othermiddleware functions are ones where application builders and users canboth realize substantial benefits when standard solutions are deployed.in recent years, the same processes that enabled growth and innovation in the network layers have started to have an even more dramaticeffect on higherlevel protocols and, accordingly, on uservisible applicathe internet's coming of agecopyright national academy of sciences. all rights reserved.130the internetõs coming of agetions. for instance, a protocol like http also provides a type of corefunctionality, albeit in the narrower space of the world wide web ratherthan the internet as a whole. the flap over instant messaging openness(e.g., aolõs instant messenger and microsoftõs msn messenger) illustrates the tensions that arise between those who argue for openness(through standardized interfaces that are open to all application developers) and those who seek to retain or increase their market share (throughclosed, proprietary protocols). in each of these instances, there are tensions between creativity and openness, typical of any standardizationeffort. in each, the affected partiesñapplication developers, service providers, and consumersñmust decide when and where one or the othershould be emphasized.while most of this discussion has examined the upper half of thehourglass, the innovation that the internetõs architecture enables at theòtransmissionó level is another crucial element of the internetõs success.keeping ip service independent of the technology below it has severalbenefits. first, competition at the technology levelñwhich can be expected to reduce cost and increase functionñwill be greater the less theservice definition constrains innovation in communications technologies.the abstract interface means that users are free to select among competitive service providers. underlying hardware (and the software requiredto enable it) can be changed without changing the application software.the consumer who uses a particular web browser with his dialupinternet service can use the same browser if he switches to a dsl or cablemodem service and is able to shop around for better performance or pricewithout incurring a switching cost for the applications he runs (althoughinvestments may have to be made in new hardware or software associated with the internet service itself). second, the technology independence also provides significant stability over time. ip can outlive anyparticular technology over which it is implemented and ip can be implemented on top of new communications technologies as they emergeñashas happened already with ethernet, atm, frame relay, and cellular digital packet data (cdpd), to name a few.1818however, the emergence of new communications technologies has led to efforts tomodify tcp in order to improve performance. for example, the throughput with standardtcp is reduced below the apparent capacity of the communications link when traffic flowsover a satellite link because the standard tcp algorithm uses a probing algorithm thatrequires the sender to wait for old data to be acknowledged before increasing the data rate.because the distance over which signals must travel is considerably greater for geostationary orbit satellites than for terrestrial links, one must wait correspondingly longer for theradio signals carrying the acknowledgment to be transmitted from receiver to sender. thethe internet's coming of agecopyright national academy of sciences. all rights reserved.keeping the internet the internet131the internet as a platform for application innovationthe internet is widely acknowledged to be a key platform for creativeand innovative applications across the communications, information,commerce, and entertainment businesses. much of this innovation restson the hourglass architecture, discussed above, which encourages independent evolution of the network, services, and applications, enablingincremental support for new media (e.g., sound, animation, video) without changing the infrastructure visible to the application. this architecture allows applications to take advantage of network bandwidth innovations and permits users to run applications regardless of who theirnetwork provider is.while some of these applications that run on top of ip are vendorspecific and proprietary, many others are themselves based on open standards. the processes established by the internet engineering task forcefor creating new protocols that rely on the core protocols are open toanybody and designed to be vendorneutral. the open process by whichprotocols are developed also means that the protocols are very well documented, facilitating the development of applications and creating a largebase of expertise.these simple, standard interfaces also allow applications to aggregateother applications very simply. for example, an email application (e.g.,hotmail) can be combined with an advertising application (e.g.,doubleclick) and a news service (e.g., reuters) with relatively little work.this ease of aggregation also permits secondary opportunities to buildservices that internet applications can reuse, such as news feeds; advertising; middleware services such as authentication and name registration;and infrastructure services such as online data storage and applicationhosting.the rosy expectations for electronic commerce rest on the standardized, open internet protocols and the ease with which applications can bedeveloped and aggregated. ecommerce, particularly where the sale ofsame problem occurs, to a lesser extent, with any longlatency internet link, and the satelliteissue is recognized as one instance of a broader class of longlatency link performanceproblems. another communications technology development driving efforts to revise tcpis the use of wireless data links for internet traffic. in this case, the higher errors rate andconsequent packet loss associated with wireless transmission reduce throughput becausethe tcp algorithm interprets packet loss as an indication of network congestion and attempts to adapt to this apparent congestion by reducing the transmission rate. efforts areunder way in the ietf and other venues to develop modifications to tcp that accommodate these new technologies while remaining backwardcompatible with existing tcpimplementations.the internet's coming of agecopyright national academy of sciences. all rights reserved.132the internetõs coming of agephysical goods is involved, also depends on successful implementation ofthe backoffice functions of inventory management, order fulfillment, andshipping. it has also leveraged other key business innovations such asjustintime inventory and rapid package delivery services.19these characteristics of the internet have been instrumental in attracting the thousands of companies developing applications and services thatrely on the internet, leading to billions of investment dollars. the netresult is a competitive industry that rapidly channels new ideas into products of value to end users and that has been rapidly creating a fountain oftechnology and customer assets. also, by providing what appears to theuser to be a single network, the internet allows an application to reachnearly every customer and business, creating an enormous market opportunity through the network effect, which says that the value in connectingpeople and services is proportional to the square of the number of connected people and services.in a reflection of its successes, the term òinternetó has attained astatus akin to a valued brand name for both businesses and end users.indeed, no other platform for computing and communications applications today shares all of these attributes. investors, developers, and usersalike have viewed the internet as a place of enormous opportunity and acommunity rich in information and applications. the amount of privateand corporate investment dollars poured into developing new internetapplications has been stunning. this climate set the stage for tensionbetween, on the one hand, the potential for seemingly unbounded innovation in applications and services and, on the other, the potential forinternetbased businesses to foster market consolidation, to raise barriersto open access, and to drive other outcomes in their effort to make andmaximize profits.evolution of internet standards settingseveral trends have emerged that run counter to the openness paradigm that has characterized the internetõs development. companies develop products and technologies in the hope of capturing a market. onetrend is that technical issues are becoming complicated by the desire toachieve or exploit a competitive or proprietary advantage as well as quality. this may well be an inevitable consequence of the market forces in19because it depends on shipment of goods to individuals, which tends to be more expensive than bulk shipments to retail outlets, businesstoconsumer commerce may also havebenefited from the ability to offset the perceived costs by not collecting sales tax for outofstate shipments. (see the section on taxation of internetbased commerce in chapter 5 for adiscussion of these tax issues.)the internet's coming of agecopyright national academy of sciences. all rights reserved.keeping the internet the internet133volved, but given the benefits afforded by standards, maintaining a balance between standards and proprietary trends is important. companieswill push for standards when they need them for business but will keepmany key aspects of the technology, such as specific data structures oralgorithms, proprietary, often protecting them as patents or trade secrets.another important factor is marketplace demands for speed of innovation; it costs time as well as other resources to develop a product thatinvolves proposing a new standard, which means that a standards process can work against innovation and responsiveness to customer demand.the growing stakes in the standards process itself threaten to overwhelm the traditional open standards mechanisms along the lines of thoseprovided by the ietf (box 3.1). they mean that the interests reflected byparticipation in the ietf are increasingly not only technical but also commercial, and participants are more political in what they do and do notsay to influence standards setting. companies also may seek to protecttheir ideas through patent protection. these factors make it more difficultfor standards bodies to address and fill gaps in what the market hasprovided. the larger market and more widespread interest also meanthat the number of participants has grown; it is very difficult for a working group of 100 or 200 people to do design work. as was the case in thepast, much of the standards development work is done in smaller designteams within the working group and then vetted by the larger group.nonetheless, the participation of many more individuals increases thelikelihood that compromises will be made that degrade the quality andcrispness of a standard.institutions have reacted to these challenges in many different ways.the ietf standard process underwent several revisions, all of whichtended toward more formality in order to cope with the increased attendance. the international telecommunication union (itu) has tried tostreamline its already formal processes in order to shorten the standardsetting cycles. and various new forums have arisen that focus on specificsubjects; they have adopted policies that expedite the development ofstandards. in fact, the ietf does not hold the monopoly on internetstandards development. when developing internet standards, companies and industry groups are likely to select whichever standards bodythey believe will be the most effective avenue for their business plan, andthey may pursue simultaneous standardization efforts in multiple forums.in addition to the ietf, several more traditional standards bodies, including the itu, international organization for standardization (iso), theeuropean telecommunications standards institute (etsi), the americannational standards institute (ansi), and the institute of electrical andelectronics engineers (ieee), are developing and adopting standards rethe internet's coming of agecopyright national academy of sciences. all rights reserved.134the internetõs coming of agebox 3.1 ietf standards processthe internet engineering task force (ietf) is generally acknowledged asthe body with primary responsibility for reviewing and establishing key standardsthat allow computers connected to the internet to interact and communicate witheach other, including the prevailing standards for routing, network management, email delivery, and so forth. the ietfõs standards process is designed to be a òfair,open, and objective basis for developing, evaluating, and adopting internet standards.ó the stated goals for this process are (1) technical excellence, (2) priorimplementation and testing, (3) clear, concise, and easily understood documentation, (4) openness and fairness, and (5) timeliness. the ietf differs from moretraditional international standardsmaking organizations such as the iso or itu inthat it charges no dues, has no formal membership, and uses a less formal andmore open standardsmaking process.the ietf holds three meetings each year, which anyone may attend, but thebulk of its work is accomplished through public electronic mailing lists. the ietf isdivided into eight functional areas: applications, internet, nextgeneration internetprotocol, network management, operational requirements, routing, security, andtransport and user services. these areas are divided further, as the need arises,into working groups chartered to achieve particular goals. working groups generally disband after they complete their assigned tasks. involvement in ietf activities is open to all individuals who are interested in its work, and membership isdetermined largely by which electronic mailing lists one subscribes to or what working group within the ietf one participates in. community involvement is considered an important element of the internet standards process. the standards process is designed to provide all interested parties with an opportunity forparticipation and comment. at each stage of standards development, specifications are repeatedly discussed and their merits are debated in open meetings orthrough public electronic mailing lists. before a particular standards action is takenup by the ietfõs management body, for example, a last call for comments is issued using the ietfõs public announcement email list.internet standards are developed, disseminated, and published formally bythe ietf in the request for comments (rfc) series, which started in 1969 asdescriptions of design work for the arpanet. each internet standard has a corresponding rfc label (for example, the file transfer protocol, ftp, is described inrfc 959). specifications generally arise from the efforts of a working group, although some are developed by individuals and some are specifications developedby other standards groups (for example, http, developed by the world wide webconsortium (w3c), was adopted as an ietf standard). some rfcs are standards, with varying levels of maturity (òproposedó, òdraftó, or òfulló protocol standards) and a separate class for policies (òbest current practiceó). however, unliketraditional standards bodies, which publish standards, the concept behind the rfcseries is òcommunity memory.ó as a result, the vast majority of rfcs are eitherhistoricalñthat is, they describe experiments in progress or carried out at onetimeñor simply informational. the informational category includes proprietaryproduct specifications, white papers on various subjects, poetry, and annual aprilfoolsõ day jokes.the internet's coming of agecopyright national academy of sciences. all rights reserved.keeping the internet the internet135the ietf works in close cooperation with three other bodies. the internetsociety (isoc) is a professional society concerned with the growth and evolutionof the internet; the isoc board of trustees is responsible for approving appointments to the internet architecture board (iab) from among the nominees submitted by the ietf nominating committee. the internet architecture board (iab) is atechnical advisory group of the isoc that is chartered by the isoc trustees toprovide oversight of the architecture of the internet and its protocols and to servein the context of the internet standards process as a body to which the decisions ofthe internet engineering steering group (iesg) may be appealed. the iab isresponsible for approving appointments to the iesg from among the nomineessubmitted by the ietf nominating committee. the iesg, composed of the ietfarea directors and the chairperson of the ietf, is responsible for the technicalmanagement of ietf activities; it administers the internet standards process according to rules and procedures that have been accepted and ratified by the isoctrustees and is directly responsible for the actions associated with the development and approval of internet standards. together, these bodies provide a structure in which the ietf operates, with the isoc and iab bodies providing ultimateoversight of ietf standards making.an rfc that is intended by its authors to develop into an internet standardmust work its way through the three ietf maturity levels mentioned above: proposed standard, draft standard, and internet (full) standard. standards actionsñentering a specification into, advancing it within, or removing it from the standardstrackñmust be submitted for approval by the iesg. the first level, proposedstandard, is accorded to specifications that have undergone extensive communityreview and are generally stable, well understood, and considered useful.the next level in the standards track is that of the draft standard, which refersto specifications that are sufficiently stable and unambiguous to provide the basisfor developing the software that implements them. draft standards are consideredto be nearfinal specifications, and any changes are likely to be changes to solvespecific problems that arise when the standards are placed in largescale use inproduction environments.the final step in the standards track is the internet (full) standard, reachedwhen technologies are mature and generally believed to be of significant benefit tothe internet community. while they are by and large stable, specifications adoptedas internet standards may continue to be refined based on experience with theiruse or the emergence of new requirements.sources: s. bradner. 1996. the internet standards process ð revision 3, rfc 2026.network working group, internet engineering task force. available online at <http://www.ietf.org/rfc/rfc2026.txt>; internet architecture board (iab), internet engineering steeringgroup (iesg). 1994. the internet standards process ð revision 2, rfc 1602. availableonline at <http://www.ietf.org/rfc/rfc1602.txt>; g. malkin and the ietf secretariat. 1994. thetao of the ietf ð a guide for new attendees of the internet engineering task force, rfc1718. available online at <http://www.ietf.org/rfc/rfc1718.txt>; and d. crocker. 1993. òmakingstandards the ietf way.ó standardview 1(1). available online at <http://www.isoc.org/internet/standards/papers/crockeronstandards.shtml>.the internet's coming of agecopyright national academy of sciences. all rights reserved.136the internetõs coming of agelated to the internet. also, there are a number of instances where morenarrowly focused interests and a desire for faster standards developmenthave led to the use of consortiumbased alternatives to either the ietf ormore traditional standards bodies. these groups, such as the world wideweb consortium or the wireless access protocol forum, tend to be narrower in scope, less open, and more industrycentered. internet standards are being developed in an active, diverse, and dynamic marketspaceña model that parallels the freewheeling creativity of the internet.there are two basic, conflicting views on internet standards. one isthat there should be exactly one standard for any function, and that thisstandard should be debated in an environment that guarantees fair representation of all parties and fair processing of all contributions. the second view is that there may well be many competing standards for thesame function, and that market competition will select which standardsbest serve a given function. the telecommunications world embodied bythe itu traditionally adopted the first view. the reality of the internetmarket, on the other hand, fosters the second view. today it can beargued that the market impact of standards from treaty bodies such asitu is essentially indistinguishable from the impact of those from otherbodies. the acceptance and use of a standard has more to do with itsapplicability to marketplace demand or the ability of a dominant vendorto deploy code that becomes a de facto standard than with what standards body approved it. examples such as java, developed by sun, or theinitial web protocols, which were developed by an informal group ofresearch institutions, show that the market can also widely adopt moreopen solutions before they are blessed by any standards group.given incompatible options for protocols, the internet market, as atippy market, will pick one when the need arises. this does not mean thatonly one protocol will necessarily be adopted for a particular purpose:pop and imap are a compatible set of protocols for internet mail both ofwhich can be employed locally without affecting the standard interfaceused by senders and recipients of email. however, between incompatible suites, the market picks a solution: the very strong force of whateconomists call network externality means that the benefits of being ableto communicate widely are so strong that they drive the widespread adoption of one of the alternatives. the force here is much stronger than it isin, say, operating systems, where consumers continue to sustain bothmacintosh and unix platforms despite the dominant share held by windows. in networking, the need to communicate is the dominant factor,and losers are prone to fall by the wayside. for example, despite the factthat some believed that it was less capable, the simple network management protocol (snmp), which was more widely used, won out over anthe internet's coming of agecopyright national academy of sciences. all rights reserved.keeping the internet the internet137arguably better, competing protocol for network management, the common management information protocol (cmip).the choices made by the market vary. sometimes it chooses avendorõs proprietary solution and in other cases it chooses an open standard over the vendorcontrolled solution. a key example of the openstandard winning was the choice of basic network transport protocol, inwhich the internetõs open standard, tcp/ip, beat out the proprietaryxerox network systems (xns) standard despite the fact that xerox andother vendors supported the latter. in the absence of an open standard,the market will also generally pick a winner. for example, when sundeveloped the network file system (nfs) and remote procedure call(rpc) protocols (used to access files across networked computers andcontrol the execution of programs on other computers), there was noopen standard (e.g., ietf) alternative in development. after the fact,there were some weak calls for the development of an open alternative,but these never resulted in the development of an alternative standard.20if a competent open standard is made available, it would be attractivein the market and could win out over proprietary standards. but if thereis no competent standard, the market still will pick an alternative (thetipping phenomenon). why are open standards less frequently developed than proprietary ones? several factors contribute. today, industrialdevelopment is so rapid that pressures to focus on products limit theamount of time technical staff in industry can spend on efforts aimed atthe broader internet community. moreover, there is a fundamental tension between, on the one hand, having a freedom of choice that enablesindividual players to reap the benefits of innovation and, on the other,picking standards that benefit all. in essence, this is a prisonersõ dilemmagame. a common standard maximizes social welfare because the largermarket engenders network externalities. but each player is tempted todiverge from the common standard if it believes it might be able to capture the entire market (or a large portion of it) for itself. at the same timeas industry is less likely to support the development of open standards,government is investing less to support the work of an academic, noncommercial core of people who care about developing open standards.and, finally, incentives are drawing people from the research community into industry.a situation where standards are more likely to be proprietary (or atleast vendorcontrolled) is not an obviously bad thing. vendorcontrolledstandards can, like patents, be proinnovation. if vendors are unable to20a working group in the ietf is developing an improved version of nfs in cooperationwith sun.the internet's coming of agecopyright national academy of sciences. all rights reserved.138the internetõs coming of agereap the benefit of investment, investment would be stifled. however,vendor interests in proprietary standards sometimes reflect less an interest in turning a vendor standard into a revenue stream through licensingthan a desire to use vendor control of a standard to hold onto marketshare. however, this is not a case where one situation is clearly bad andthe other clearly good (e.g., open versus proprietary or licensing versuscontrol); rather, an appropriate balance must be struck.one contributor to continued vitality in the development of openstandards is support for the networking research community. government has supported open standards for the internet not by directly setting or influencing standards but by providing funding for the networking research community. such research leads both to innovativenetworking ideas and to specific technologies that can be translated intonew open standards, which in turn can offer a richer set of alternatives inthe marketplace.endtoend transparencyclosely associated with the concept of openness, which speaks to theuse of common standards for communications across the internet, is thenotion of endtoend transparency. a product of two fundamental properties of the internetñthe hourglass, endtoend architecture and theunique addressability of devices attached to the internetñtransparency isa defining characteristic of the internet. the hourglasslike architecture,in which the internet protocol provides the fundamental means of sending data across the internet, allows any type of communication, application, or service to ride on top of the internet. with suitable softwarerunning at each end and no knowledge other than each otherõs internetaddress, any two devices connected to the internet are able, in principle,to enter into any desired type of communication, provided there is enoughnetwork capacity and sufficiently low or predictable latency (delay) tosupport the application.crucially, this communication takes place as a result of actions byusers at the edges of the network; new applications can be brought to theinternet without the need for any changes to the underlying network orany action whatsoever by internet service providers. indeed, over the lifeof this report, many new applications and associated communication protocols have emerged. a noteworthy example is the rapid emergence andensuing widespread use of a group of new protocols (e.g., napster andgnutella) that are designed to allow distributed sharing of files amonginternet users, frequently for the purpose of exchanging music encoded inthe mp3 format. (the challenges to intellectual property protection prethe internet's coming of agecopyright national academy of sciences. all rights reserved.keeping the internet the internet139sented by these protocols have, of course, given rise to controversy aboutthe implications of their use and led some to attempt to block their use.)as has been noted by a number of observers of the internet, transparency often falls short of the ideal described above.21 pragmatic measurestaken in response to operational considerations (e.g., making addressmanagement more tractable or coping with a shortage of available addresses) are one factor that clouds transparency. another factor is technical measures taken by both users and isps aimed at protecting networkedcomputers from attack or enhancing the performance of a network bycontrolling the use of applications that place particular demands on network resources. and the business and marketing strategies of someinternet players involve offering services that are not fully transparent. inexamining transparency issues, it is important to distinguish betweentransparency violations that users choose to adopt and those violationsthat are imposed on them.addressing issuesone transparency challenge concerns the means by which computersare assigned internet addresses. it is common practice today to assigninternet addresses in a dynamic rather than static fashion. dynamic assignment provides an address on request from a networked computer,generally via the dynamic host configuration protocol (dhcp), from apool of globally unique internet addresses. this makes configuration andmanagement easier and also reduces the number of ip addresses requiredto support a group of computers. when a device is turned on or reset (inthe case of a permanently connected computer) or makes a connection toa network (in the case of a dialup connection), it uses the dhcp protocolto send a message to a dhcp server to have an address assigned to it.the server responds with a message containing an ip address, and thesoftware running on the device configures the device to adopt that address. when addresses are assigned in this fashion, the relationship between device and address is not constant over time; the address is fixed21for example, transparency has been a topic of interest to the internet architecture board(iab). a recent draft report issued through the ietf echoes a number of these issues (briancarpenter. 1999. internet transparency. internet engineering task force internet draft (workin progress), december. available online from <http://www.ietf.org/internetdrafts/draftcarpentertransparency05.txt>). the iab also held a workshop on the subject (m. kaat.1999. overview of 1999 iab network layer workshop. iab internet draft (work in progress),october. available online from <http://search.ietf.org/internetdrafts/draftiabntwlyrwsover02.txt >).the internet's coming of agecopyright national academy of sciences. all rights reserved.140the internetõs coming of ageonly until the device is disconnected from the network, reset, or powereddown.as a result, an application cannot rely on the ip address to reach adevice directly to complete a callña dynamically assigned ip addressdoes not uniquely identify a particular device over time. this situation isquite unlike that of other sorts of addresses such as phone numbers, wherea personõs phone number is statically mapped to a telephone or a location(though there are calling features, such as call forwarding, that allow alimited form of dynamic rerouting to occur by making use of databaseswithin the telephone network). thus if one were to implement an ipbased telephony service, one could not use a dynamically assigned address directly. dynamic assignment is not an insurmountable problem,however. solutions must make use of indirection, in which a directoryservice is established to provide a mapping between some sort of identifying name and the current ip address that should be associated with thatname. keeping the directory up to date requires that each device send amessage to the server on startup notifying it of the current ip addressthat should be associated with its name. maintaining an uptodate directory with accurate data and operating the directory with sufficient integrity that its information can be trusted is a difficult technical and socialproblem. work on a protocol that provides such a capability is now aproposed standard from the ietf. provided that a suitably robust servicecan be implemented, dynamic addresses are as suitable as static addressesfor any sort of application, and dynamic address assignment can bethought as a situation that requires additional technology developmentand deployment rather than a fundamental obstacle to transparency.another addressingrelated challenge to transparency is posed bynetwork address translation (nat), a technology introduced in chapter 2in connection with addressing and routing issues. nat provides a workaround that permits multiple computers attached to a network to share asmaller number of globally assigned internet addresses. nats and firewalls including nat functions are employed by users and isps for avariety of reasons. these include providing a larger number of computers with internet access using a limited pool of internet addresses, providing local control over the addresses assigned to individual computers,and providing the limited degree of security that is obtained by hidinginternal addresses from the internet.network address translation involves the mapping of a set of localaddresses, which are not visible to the outside world (i.e., not visible onthe internet), to a global address (i.e., visible on the internet). a crucialdistinction between nat and dynamic addressing is that the mappingtakes place without any explicit communication between the device andthe nat about the address assignment that has been made. the devicethe internet's coming of agecopyright national academy of sciences. all rights reserved.keeping the internet the internet141continues to use its local address without regard to the action of the nat;the nat takes care of translating the addresses on packets flowing in andout of the network between the two sets of addresses.a transparency problem arises because this translation is performedonly on the portion of the packet that labels the destination addresses(analogous to the address on an envelope) not on any addresses that arecontained within the packet (analogous to the addresses contained in thetext of a letter inside the envelope). the reason that translation cannot ingeneral be done on the addresses within the packet lies at the heart of thetransparency question: because the internet architecture permits any application to run over the internet, the nat cannot in general know whereand in what form the addresses are placed within the packets.to make such an application work, one of two things must happen.one option is for the nat to include an application layer gateway thathas knowledge of the applicationõs protocol, thereby allowing it to identify and translate the address as it is transmitted. many nats providethis gateway function for commonly used applications such as file transfer protocol (ftp). this need for nats to be applicationaware violates abasic attribute provided by the hourglass architectureñthat one is free toemploy new applications running over the network without having tomake any changes whatsoever within the network. there are also costsassociated with deploying computers with sufficient computing power tocarry out the applicationlevel translations. the other option would befor the application to discover that the network is making use of nat andthen make the necessary translations itself; requiring an application tolearn about the details of the network is an undesirable violation of thebasic internet architecture.22significant problems arise if one wishes to initiate communicationsbetween two computers, each of which is sitting behind a nat, sinceneither has a way of knowing the internal address of the other. consideran application like ip telephony. with nat, one must resort to using athird computer outside either network to act as a telephony server thatbridges between the other two. a particular problem is that the only wayfor a computer behind the nat to discover that it is receiving an incoming call is for it to repeatedly ask, or poll, the telephony server if there is a22one other option is to avoid passing addresses. this solution works in some caseswhere a protocol does not inherently require the exchange of global identifiers but wasimplemented that way prior to the advent of nat. however, the applicability of thissolution is limited because some types of applications require that globally unique identifiers be transmitted from one computer to another.the internet's coming of agecopyright national academy of sciences. all rights reserved.142the internetõs coming of agecall. such a workaround places increased demands on both networkcapacity and the telephony server.another set of situations where nat raises difficulties are ones wheresimultaneous communications among devices that sit behind a nat (i.e.,local) and devices that sit outside a nat (i.e., remote) are desired. examples of such situations include multiparty conferencing (telephony orvideo) and games; both are situations where there can be a mix of localand remote participants. signaling becomes more complicated becausean application cannot provide the same address information to applications running on local and remote machines. it is not impossible to handlethese situations, but they make the software more complicated to implement correctly and more difficult for users to configure properly. similarproblems arise if people start installing appliances, such as security devices, that need to be accessed from both the inside and the outside of thehouse (i.e., behind the home gateway or outside of it).nat also interferes with security protocols such as ipsec,23 thoughnot with higherlayer security protocols such as ssl or s/mime. thebasic problem is that if the packet payload is encrypted, addresses withinit cannot be translated by a nat. because ipsec is a more broadly applicable protocol, used notably for standard internetlayer virtual privatenetworks, the incompatibility is a significant concern for some users.nonuniform treatment of bitsinternet transparency also implies the uniform treatment of all trafficñin terms of the application, protocol, and format and in terms of thecontent of the communications being carried across the internet. in itsidealized form, the hourglass architecture treats all bits uniformly, withtheir transmission through the network a function of one thing onlyñavailable capacity (and whatever controls the end points place on thecommunications, such as the tcp pacing algorithms). the situation isslightly different when qualityofservice technologies are built into thenetwork (discussed in detail in the section on quality of service in chapter2) in order to provide for special treatment of particular classes of traffic,in accordance with a customerõs contract with an isp; in this context,òuniformó means uniform within a particular class.transparency is limited by the blocking of particular types of internetcommunications, pursuant to choices reflecting isp policy, the preferences of individual customers, or, in the case of larger organizations that23s. kent and r. atkinson. 1998. security architecture for the internet protocol, rfc 2401.available online at <http://www.ietf.org/rfc/rfc2401.txt>.the internet's coming of agecopyright national academy of sciences. all rights reserved.keeping the internet the internet143operate their own network infrastructure, organizational policy. theserestrictions fall into two broad categories: restrictions placed at the edgesin order to meet the objectives of end users and restrictions placed withinthe network by internet service providers.the classic example of a restriction placed on transparency at theedge of the network is the firewall, which is a blocking device placed atthe entry point to a subnetwork and operated by either the customer orthe isp on behalf of the customer. it can be configured to exclude thosetypes of communications that are not desired or, more stringently, toblock all content not explicitly designated as acceptable. typically, theserestrictions are used to block traffic that could be used to exploit vulnerabilities of the computers within the network. communications may alsobe blocked on the basis of the application being run (e.g., when a businessseeks to enforce a prohibition on the use of streaming media applicationsby its employees to reduce bandwidth use or increase worker productivity) or content (e.g., filters that block objectionable content).how is undesired traffic filtered? internet applications are generallyassociated with particular òports,ó which are a set of numerical identifierseach of which is associated with a particular type of service or application. these are somewhat standardized; for instance, an http server isfrequently associated with port 80. to protect computers against certaintypes of attack, a firewall can block packets associated with particularports (and thus applications) that are known to pose a risk. firewalls willfrequently block packets associated with unknown ports as well, in orderto keep rogue applications from carrying on unauthorized communications. an application not identified to the firewall as permissible canattempt to circumvent the firewall by making use of another port, perhaps one that is dynamically adjusted (socalled portagile applications).for example, real networks software is both port and protocolagile,able to switch from the default udp protocol to tcp or even http running over a standard port for http traffic when firewalls block the preferred protocol. from the perspective of the application developer, this isdone for legitimate (i.e., nonmalicious) reasons, to increase access to endusers. from the perspective of the operator of a particular network, however, it may be viewed as subverting a policy decision that may have alsobeen made for legitimate reasons (e.g., to reduce the traffic on a networkor prevent those connected to that network from running applicationsthat an organization has decided to prohibit). port numbers are perhapsthe easiest method of filtering, but filtering can also be performed usingother information contained in packet headers or the contents of the datapackets themselves.in a response to the difficulties of providing large quantities of data ora high quality of service to end users, the internet is being overlaid bythe internet's coming of agecopyright national academy of sciences. all rights reserved.144the internetõs coming of ageapplicationspecific delivery networks and caching services. content orservice providers may, for example, enter into an agreement with a company that delivers specialized content services located throughout theinternet so as to improve the quality of the connection seen by their endusers. local caching or overlay distribution networks do not provideendtoend connectivity between the original content or service providerand the end user. also, depending on the particular technical and business model, such networks may only be available to those providers whoare willing and able to pay for specialized services.such service elements in the internet provide optimizations that makethe network more usable for particular applications. if they work properly, they maintain the illusion of endtoend behavior. but if they fail towork properly, the illusion of transparency can be broken (see the sectionòrobustness and auxiliary serversó in chapter 2). importantly (from atransparency perspective), these are not inherent services that end systems can depend upon. this has several implications. first, where theseservice elements are not implemented in the network, the end user canstill employ the full range of services and applications, though the performance may be degraded relative to what would be possible if the enhancements offered by network service elements were available. second,applications cannot depend on these enhancements being present in allnetworks. third, a new application can be deployed without necessitating changes within the network, although its performance may not beoptimal in the absence of supporting elements within the network. inshort, the introduction of supporting elements does not necessarily violate the endtoend architecture but at some point makes it effectivelyimpossible to use a nonsupported service.a related issue has to do with isp interpositioning, in which an ispadds facilities to the network to intercept particular requests for webpages or elements of web pages, such as graphics, and replace them withispselected content. for example, an isp might select information oradvertisements that are locally relevant, in much the same way as localadvertisements are inserted into network programming by local broadcast stations or cable system operators, to rewrite web pages or someportions of web pages. such a practice may, of course, be seen as a valueadded service, but it diverges from the endtoend content delivery modelthat has characterized the internet thus far. it has the potential to depriveboth end users and publishers of full control over how content is delivered, particularly where it occurs without control by the end user.the portagile tactic described above illustrates the broader point thatthere are limits to the extent to which the content or applications can beblocked. since the internetõs architecture allows application writers tolayer traffic of their choosing over the basic internet protocols, it is, inthe internet's coming of agecopyright national academy of sciences. all rights reserved.keeping the internet the internet145general, difficult to recognize all instances of an application. applicationwriters can also modify their application protocols to stay one step aheadof attempts to block them. a likely result of persistent attempts at blocking would be an escalating battle in which firewall software authors andisps attempt to identify and block applications while application developers work to find ways to slip past these filters. the longterm result ofsuch a struggle might well be a situation where much of the traffic is hardto identify, making it difficult to implement blocking policies. anothertechnical development could also fundamentally limit the ability of ispsto filter traffic: widespread adoption of encryption at the ip layer (e.g.,deployment of ipsec) would preclude isps from examining the information being transmitted or deducing the application being run using information contained in packet headers above the ip layer. if it wished tocontinue to impose controls under such conditions, an isp might be forcedto adopt a policy that blocks everything that is not identifiable and expressly permitted.market and business influences on opennesseconomic pressures as well as technical developments are having animpact on transparency and the endtoend principle. in the consumerisp market, there are many consumers who are more than willing tosubscribe to networks that do not follow the classic internet providermodel in all respects, selecting from among a small number of isps thatprovide a somewhat sheltered environment or at least preferential offering of selected services and content. for example, if one looks at massmarket consumer behavior today, with thousands of isps to pick from,most consumers select aol, an offering that emphasizes access to itscustom content over access to the full internet. (of course, aol ended upresponding to consumer pressure by adding access to the full range ofinternet content.) the 2000 aoltime warner merger is another signthat internet players believe there is a business case for combining accessand content offerings. such vertical integration, where a network provider attempts to provide a partial or complete solution, from the transmission cable to the applications and contents, could, if successful, causea change in the internet market, with innovation and creativity becomingmore the province of vertically integrated corporations. microsoftõs òeveryday internetó msn offering further supports the notion that businessessee a market for controlled, preferred content offerings as a complementto the freeforall of the public internet.vertical integration has several obvious economic motivations. openinterfaces can make it harder either to coordinate changes at more thanone level, which might be needed for some forms of innovation, or tothe internet's coming of agecopyright national academy of sciences. all rights reserved.146the internetõs coming of agecapture as much of the benefits of innovation as integration might allow.on the other hand, the telecommunications industry, once highly integrated vertically, provides evidence that there are limits to vertical integration. today, pressures are confronting the makers of telecommunications equipment as multiple vendors whose products are organizedhorizontally collectively challenge the vertically integrated circuit switch.an important enabler of this trend has been the transparency and openness of ip technologies. ipcapable hardware and software can be purchased from many vendors. and it is no longer necessary to own thefacilities of a network to offer voice services over it.many businesses are customers of a fairly small number of very largenetworks, which have substantial incentives to hold on to their customers. to do so, they can use such means as leveraging the billing relationship with the customer or their ability to deliver service to the customerpremises. also, providers that have a large market share have incentivesto try to own or coown the most popular services for their customers soas to keep them inside their network.these economic and business forces can act as disincentives to thecontinued free sharing of the best infrastructure ideas. large scale creates additional incentives to providers to build their network with internal proprietary protocols that optimize the performance of both applications and the network in such areas as reliability, security, or controlover bandwidth and latency. a leading example of where such optimizations might be deployed is telephony, but other possibilities includeemail and chat, caching, video, and routing. hotmailõs valuation as anemail service or icqõs as a service for instant messaging reflects thenumber of customers they are able to serve. for example, hotmail doesnot use the internetstandard mail protocols internally nor does it usestandard pop or imap for external access by users.24 such internaldeployments start to have implications for applications running at theedges of the internet. today hotmail is at sufficient scale that specialcode to support its proprietary protocol is written into email clients; theresult is that a frequently used internet service is no longer running thestandard internet protocols. equipment suppliers are similarly willingto accommodate such customer demands; their routers are more programmable than ever to support custom protocols. there is a tensionhere between immediate improvements and longterm benefits: todayõsoptimization may be tomorrowõs roadblock, and design choices made tooptimize a particular application may or may not turn out to be benefi24the proprietary protocols are intended to allow it to scale better. hotmail does, however, allow users to read standard pop email accounts through hotmail.the internet's coming of agecopyright national academy of sciences. all rights reserved.keeping the internet the internet147cial when a new application emerges. also, the extent to which optimization will occur in a decentralized network such as the internet is limited by difficulties in reaching agreement to deploy optimizationsnetworkwide. another pressure for nonstandard protocols, illustratedby the recent flap over instant messaging protocols, is the desire to differentiate oneself from competitors and capture value above the basic ipbittransport service.thus, market pressures, combined with technical pressures relatingto optimization, raise the prospect that we might end up with severalòseparateó internets differentiated by the use of proprietary protocols orcustomized content. one scenario would be that some dozen or halfdozen tier 1 service providers would operate somewhat separately, although still using ip and other standard internet protocols to enable somedegree of interoperability among them. if a situation develops whereseveral large providers start using proprietary protocols inside their networks, the incentives for new content and application development couldshift. content and application developers will target the networks ofthese large companies rather than an abstract internet, and at the sametime the large providers will have a huge incentive to make it difficult forcustomers to switch to another provider. as a result, tying applications totheir proprietary protocols becomes good businessñearly on they mighteven pay application developers to do this. a base of, say, many millionsof customers might justify the cost of the extra coding and maintenancethat supporting multiple protocols would require. some of this can beseen today, for example, in aolõs system, where providers of contentand services, most of whom also do business on the internet, registeraol keywords and develop aolspecific content to allow aol users toaccess their content and services. the potential viability of applicationsbeing developed for environments that are less inclusive than the internetis also illustrated by the content being developed for the wireless accessprotocol (wap, a standard aimed at mobile phones and similar wirelessplatforms) and palmõs web clippings.however, there are forces arrayed against the possibility of this moreclosed model supplanting the more open internet model. one is thatanonymous rendezvous and the ability to support transitory relationships appear to be important capabilities. ecommerce, which is an important internet application, depends on the ability to establish connections between two previously noncorresponding companies; multivendorvalue chains have become critically important in todayõs networkedeconomy. in fact, many customers explicitly need to work across multiple organizational overlays without having to agree to use a particularnetwork. this point was demonstrated by past attempts to develop standards for electronic data interchange (edi): interoperable protocols arethe internet's coming of agecopyright national academy of sciences. all rights reserved.148the internetõs coming of agemandatory and balkanization appears to be useful only in the short term.providers offering noninteroperable edi solutions were profitable for awhile, but the lack of interoperability among systems ultimately stifledthe growth of edi. on the internet today, there is a good deal of investment in a new data description standard from the world wide web consortium, xml, to be used for businesstobusiness ecommerce, and manyindustry groups are working to define standards for describing data inspecific domains so as to enable interoperability on a large scale.suppose three isps develop different protocols to deliver a particularapplication over the internet. to reach customers within closed networks,they would need to make their protocol work over each of the closednetworkõs proprietary protocols and might also need the closed networksto configure their networks to enable the applications to work. from theperspective of the wouldbe application provider, the isps become a roadblock to innovation. if, on the other hand, we assume that there areproprietary isp protocols, but isps also support ip endtoend in somefashion, application providers can choose to make their protocol run overip and bypass the constraints of the closed network provider (at leastuntil the provider notices a large fraction of its ip traffic in this newprotocol). it is this sort of marketplace dynamic that is valuable.another drawback of the closed solution is that it may end up imposing undesirable costs on all parties. for example, for both consumers andapplication developers, closed solutions represent a lockin to a singlesolution (where the lockin reflects the cost of switching). for the customer, it may mean investing in new hardware and software; for thedeveloper, it may mean retooling a product. from the perspective of aprovider, there is the risk that deviation from standards means that theywill miss out on some new òkiller appó developed elsewhere that offersthem dramatic new business opportunities (e.g., an increased customerbase or demand for enhanced services).there have been examples of proprietary solutions that found it difficult to gain widespread acceptance on the internet. for instance, the pastdecade saw a debate on whether to adopt the iso x.400 standard, theinternet standard smtp, or one of a number of proprietary systems for email. the market settled on smtp, and the other proposals have becomelargely moot. more generally, there are obstacles to proprietary approaches being adopted internetwide. all of the users on the internetwill not sign up with a single provider all at once, and few users onlyneed to be able to interact with other users connected to the same singleprovider. for example, in an email exchange, neither the sender nor thereceiver is likely to know (or care) which isp the other is using or which email standard their respective isps are using. they simply want to exchange an email message. the success of a proprietary solution dependsthe internet's coming of agecopyright national academy of sciences. all rights reserved.keeping the internet the internet149on the internet provider developing and offering working gateways to allthe other services, which will entail additional cost to the provider. fromthe perspective of customers, open standards help maximize the benefitsthey realize from the sheer quantity of people and services they can interact with. if all belong to a single internet, the benefits of adding a newuser to it accrue to everyone located anywhere on the internet, whereas ina partitioned internet, the benefits of an additional user would be limitedto the customers of that userõs isp. the recent past has also seen pressuresplaced on online providers that relied on proprietary technologies. noninternetbased online providers have had to respond to the internet phenomenon by supplementing their more closed content and services withaccess to the full internet or by reinventing themselves as internetbasedservices.today, both fully open and sheltered models are being pursued withvigor in the internet marketplace, reflecting different business modelsand different assumptions about the desires of consumers. consumerisps cover a broad spectrum, with more closed services at one end thatemphasize their custom content and services and more open services atthe other end that emphasize internet connectivity but may also providesome preferred content or services. to date, the more closed providershave also continued to offer some degree of access to the wider internetthrough the connectivity afforded by the basic internet protocol (with afew having adding this support in response to market demands). whichpath the internet market takes from here will affect the shape of futureinnovation.keeping the internet openprovision of open ip service ensures that whichever service providera consumer or business picks, the consumer or business can reach all theparties it wishes to communicate with. much as the pstn dial tone offerscustomers the ability to connect to everyone else connected to the globalpstn network, open ip service offers access to all the services and contentavailable on the public internet. in the absence of an open ip service, whoyou can communicate with is a function of the service provider you pick.(note that the quality of service is still a function of the service provideryou pick.) open ip service is also an enabler of continued innovation andcompetition. anyone who creates a better service that runs over ip candistribute software supporting it to thousands or millions of users, whocan then use it regardless of who their service provider is.open ip service requires support of the internet protocol (ip), globally unique internet addresses, and full interconnection with the rest ofthe networks that make up the internet. (some additional capabilitiesthe internet's coming of agecopyright national academy of sciences. all rights reserved.150the internetõs coming of agemay be required; some perspectives on what else should be included as acore service were presented above.) open ip service is content independentñthat is, the service provider does not filter the customerõs trafficbased on content, except with the consent and knowledge of the customer. however, because the internetõs default service is best effort, thisdefinition can make no promises about the quality of the access. thequality of connectivity will depend on the agreement a customer has withits service provider and the agreements that this provider has with otherinternet service providers. indeed, in a free market, it is reasonable tohave differentiation of services to satisfy customers who want to paymore for a service they deem better. it is important to point out that onepossible outcome of the tension between open service and isp servicedifferentiation is that the current besteffort service will continue to beprovided as a transparent, endtoend service but that endtoend transparency across multiple providers will not be provided for the more demanding (and potentially more interesting from a commercial standpoint)applications such as telephony and audio and video streaming that maydepend on qos mechanisms.because ip connectivity affords users the potential to misbehave orpose unacceptable demands on the network, this definition of open ipservice is not intended to preclude service providers that want to ensuresafe, effective operation or meet the desire of customers to block certaintypes of ip traffic from restricting how their network is used. an isp may,for example, block particular traffic to prevent its customers from launching attacks on other customers of the isp (or users elsewhere on theinternet). it may also filter particular types of traffic to protect its customer computers from attacks. and an isp may restrict traffic volumeswhere bandwidth resources are limited to ensure that all users have fairaccess. of course, isps and their customers may differ over whether aparticular filter enhances the operation of the ispõs network or unnecessarily restricts the behavior of a customer; full disclosure of filtering practices provides consumers with the means to make informed choices whenselecting an isp.the internet's coming of agecopyright national academy of sciences. all rights reserved.151˙˝ˆintroduction the generalpurpose nature of the internet and its basic technologymakes it possible to provide services at least comparable to those provided by other communication systems. perhaps the clearest such trendtoday is the growing use of internet technology by providers of telephoneservice and the emergence of a variety of voice communications servicesover the internet. for reasons that range from reducing the costs of conventional telephony by using less expensive technology, to bypassing thetariff structure of the existing public switched telephone network (pstn),to innovating new forms of telephony services and applications, manyefforts are under way to use the internet and its technology componentsto provide voice services. these services do not merely duplicate thoseprovided by the pstn; the ease with which new applications can beintroduced over the internet opens up the possibility of a wide range ofapplications involving voice and the introduction of many different enhancements to what we think of as telephony today.telephony over the public internet is in its infancy, but telephonyover ip networks that provide appropriate provisioning or qualityofservice technology is already viable today. where internet voice is deployed on existing data backbones lacking supporting quality of serviceand/or provisioning, such service today is generally a secondbest alternative. however, in other cases, where ip telephony is used either withseparately provisioned bandwidth or with supporting qualityofservicethe internet's coming of agecopyright national academy of sciences. all rights reserved.152the internetõs coming of agetechnologies, it has proven to be competitive with circuitswitched technologies. there are numerous smallscale examples of such deploymentsfor longhaul telephony as well as several instances of largescale systems, including two chinese telephone networks (china telecom anduninet).1 telephony over ip networks is also being deployed withinenterprises (generally replacing pbx functionality), with the goal ofachieving lower costs by operating a single network carrying both dataand voice.2these developments are emblematic of a broader trend towardinternet services that overlap and potentially exceed or even supersedecommunications services that have long existed as distinct industries withdistinct policies and regulations governing them. another emerging service is internet distribution models for audio and video, which overlapthe capabilities of traditional radio and television broadcasting and musicand video publishing and distribution. in both cases there are conflictsbetween the new technologies, systems, and players made possible by theinternet and the policies, regulations, and practices that have shaped theextant industry. this chapter focuses on telephony, where the conflictshave now become quite visible, but many of the issues explored here arealso relevant to these other sectors.3what is ip telephony?the terms òip telephonyó and òvoice over ipó are used synonymouslyin this report to describe the broad range of options for using internetprotocolbased, packetswitched networks in support of telecommunications services of various sorts, typically for voice communications. (thecommittee is careful in its use of the term òinternet telephony,ó becausethe term implies that the public internet is used to carry telephone calls,which not all ipbased telephony does.) ip may be used for some or all ofthe transmission and interconnectivity path as well as to provide switching, control, and services facilitation (e.g., call setup). ip telephonyencompasses services that provide connectivity among normal telephone1see leslie chang. 1999. òinternet phone service catches on with millions in chinañrivals challenge china telecom with cheap rates.ó wall street journal. december 21,p. a14.2the committee does not provide data on the extent of ip telephony deployment becauseassembling such information is complicated by the diverse offerings, definitional questions,and the highly dynamic nature of the business.3this chapter is not intended as a thorough examination of technical issues underlyinginternet telephony. discussion of some key underlying technical dimensions, such as network robustness, scaling, and quality of service, is located elsewhere in this report.the internet's coming of agecopyright national academy of sciences. all rights reserved.telephony as a case study153sets, computers, or other computer appliances, and combinations of thesetypes of devices. ip telephony may be used over a privately owned network, a network owned by the telephony service provider or some otherthird party, or over the public internet.to carry voice over ip, the analog signals are digitized, (usually) compressed to reduce bandwidth requirements, and then broken up into fragments or packets with accompanying routing information for transportover a packet network such as the internet. they are transported acrossone or more ip networks, making use of algorithms running on the devices at each end of the call to deal with late or lost packets and possiblyalso making use of qualityofservice mechanisms in the networks the calltraverses. call quality is determined by the algorithm used to digitizeand compress the signals, the capacity of the network, and the effectiveness of any explicit qualityofservice mechanisms that are employed.ip telephony comes in many flavors; these may or may not involvethe public internet and may or may not involve the pstn. some iptelephony applications make use of only an ip network, whereas othersmake use of both ip networks and segments of the pstn. if the pstn isinvolved in any segment of the call, a gateway is required to translate thedata and signaling (e.g., call setup or termination instructions) associatedwith the call. another function that is generally provided in any telephony application is a directory lookup service that associates a phonenumber or other identifier with a particular telephone line or ip device.4some designs amount to little more than a substitution of ip technology for transport within the network. for instance, ip may be substitutedfor other means of transporting communications within the networks oftelephony providers; this internal substitution of ip technology for transport within the network raises few policy issues that are distinguishablefrom those raised by the introduction of other new technologies or theentry of new players into the òtraditionaló pstn business. another flavor is the use of ip within the private network of an enterprise, where itreplaces the elements of a traditional pbx system or other corporate voicenetwork. other designs make sole use of ip for transport and internetapplication servers for control and directory services, while still othersemploy hybrids that combine elements of both the internet and the pstn.the set of possible sorts of services that might be labeled òip telephonyó ishighly diverse and encompasses the following:¥network operators and transport media. for example, backbone trans4the pstn provides several such mappings, including between subscriber name andphone number (what the telephone directory contains) and between phone number andtelephone line.the internet's coming of agecopyright national academy of sciences. all rights reserved.154the internetõs coming of ageport within a carrier network, telephony over the public internet, enterprise networks, virtual private networks, pstn, and combinations ofthese;¥enduser equipment. for example, conventional telephones, ipbased specialpurpose devices (appliances), and generalpurpose computers;¥local access technology. for example, pstn analog lines, ip datanetworks, and ip over dsl;¥interfaces and gateways between ip and pstn elements. for example,analog phone lines, isdn primary rate interfaces, switching system 7(ss7)ñor none in the case of pure ip telephony; and¥architectures. for example, telephony services may be provided bya single vendor in a centrally managed way (perhaps over a dedicatednetwork) or provided as a distributed service with individual users placing calls endtoend over the internet, perhaps making use of servicessuch as directory lookup provided by a third party.new and evolving architectures for telephonyto understand the technical, operational, economic, and policy issuesrelated to ip telephony, it is useful to understand the role of architecturesand examine some specific examples. the concept of an architecture hasbeen a cornerstone in the development of telecommunications systems.an architecture first requires that the underlying system be treated interms of a set of commonly understood elements and that these elementshave a clearly demarcated set of functions and interfaces that allow forcombining the elements.5 an architecture is driven by two factors inaddition to the identification and selection of elements and interfaces:technology and world view. the state of the technology places bounds onwhat is achievable. the world view is the way an individual, entity, ororganization views the world, its relationship to that world, and how thatworld and that relationship will evolve over time. the pstn, for example, with its more centralized architecture, reflects a world view different from that of the internet, with its more distributed design. the limitsimposed by technology are typically less confining than the limits that areselfimposed by the designers or architects; in practice, therefore, worldview is often the more powerful driver of architecture.5terrence mcgarty. 1990. òalternative networking architectures: pricing, policy, andcompetition, information infrastructures for the 1990s.ó john f. kennedy school of government, harvard university, november.the internet's coming of agecopyright national academy of sciences. all rights reserved.telephony as a case study155ip telephony architecturesa multitude of network architectures and implementations are encompassed by the label òip telephony.ó one way to portray this diversityis to portray it in terms of architectural classes. a fourclass taxonomy,with illustrations of each class, is presented in figure 4.1.even a single class of architecture has many possible configurations.as shown in figure 4.2, the telcototelco (class 1) architecture encompasses two quite different ways of employing voice over ip, even thougheach way makes use of a basic pstnip gateway to convert telephonysignals into ip packets. the first (figure 4.2a) consists of a dedicatednetwork or channel configuration that uses a private network. the operation of this design can be explained as follows. a telephone user in, forexample, new york desires to call a telephone user in moscow. the localtelephone user in both cases will access the system by means of a standardtelephone. the user places the call to a local exchange carrier, which thensends the call to another switch. this switch places the call through an ipgateway node (ign) that connects locally to a router and the call proceedsover a private network. the process is reversed on the terminating side.the gateway provides translation from the telephony world to the ipdomain by performing three conversionsñbetween telephone signalingand ip signaling, between the conventional voice signal and data packets,and between telepone numbers and ip addresses (if required). the second class 1 approach (figure 4.2b) is the public internet approach adoptedby a number of ip telephony startup companies. it uses the same genericform of entryña gateway between the pstn and an ip data networkñbut replaces packet transport over dedicated ip links with transport overthe public internet. it reflects both a different architecture and a differentworld view of ip telephony. for example, in contrast to the architectureusing a private ip network, there is no pstn switch or function (such as abilling system) provided by a switch.increasingly, one sees new applications based on a comingling of theinternet and pstn telephony architectures. these emerging offeringsinclude òclicktospeakó voice conversations launched from web pagesand unified messaging services (which combine voice mail, email, andfax). another application, internet call waiting, is aimed at customerswho use a single phone line for both normal phone service and their dialup connection to the internet. while the service, which uses the internetto inform a customer when a call is being made to the customerõs phoneline, appears on the surface to be quite similar to conventional call waiting, it has very different requirements behind the scenes. implementation of conventional call waiting is internal to the telephone switches inthe internet's coming of agecopyright national academy of sciences. all rights reserved.156the internetõs coming of agetelcodevicetelco edgedevicetelcotransporttelco edgedeviceiptelcogatewayip edgedevicetelcodevicetelco edgedevicetelcotransporttelco edgedeviceiptelcogatewayip edgedeviceiptransportclass 1: telco to telco. the end users employ existing telephony equipmentthrough a conventional pstn connection while ip (either private network orinternet) is used for a portion of the connection through interconnects with thepstn (e.g., ip used over longdistance segments).telcodevicetelco edgedevicetelcotransporttelco edgedeviceiptelcogatewayip edgedeviceip deviceip edgedeviceiptransportfigure 4.1 four classes of ip telephony architectures. source: taxonomyadapted (with extensions) from david d. clark. 1997. a taxonomy of internettelephony applications. proceedings of the telecommunications policy researchconference (tprc).class 2: telco to ip. conventional telephone devices are interconnected with iptelephony devices (a computer or other ipenabled device).the internet's coming of agecopyright national academy of sciences. all rights reserved.telephony as a case study157ip edgedeviceip deviceip edgedeviceiptransportip devicetelcotransportip deviceip edgedeviceiptransportiptelcogatewaytelco edgedeviceip edgedeviceip deviceip edgedeviceiptransportiptelcogatewaytelco edgedeviceip edgedeviceclass 3: ip to ip.  computer telephony devices are used at both ends and theconnection is made solely through ip networks or the internet. while gatewaysto the pstn would be added to provide interconnection with pstn customers,this architecture does not make use of any pstn elements and thus depends onthe development of a full range of signaling services using ip only.class 4: ip to telco to ip.  other combinations of pstn and public and private ipnetwork links. the figure illustrates the use of the pstn to provide a connectionbetween two internet òsubclouds.ó this architecture might be used to enable iptelephony across internet providers in the absence of adequate quality of serviceor other capabilities over an ip connection.the internet's coming of agecopyright national academy of sciences. all rights reserved.158the internetõs coming of agenew yorkmoscowlecpttswitchswitchignignignignrouterrouterbilling and customer caresystemnoccompanyprivateip networknew yorkmoscowlecpttswitchswitchignignignignrouterrouterinternetrouterignignpttseoulfigure 4.2 two examples of class 1 ip telephony configurations. lec, localexchange carrier; ign, ip gateway node; and ptt, post, telegraphs, and telephones.(a) using a dedicated private network(b) using the public internetthe internet's coming of agecopyright national academy of sciences. all rights reserved.telephony as a case study159the pstn; in a hybrid pstninternet application, switches have to communicate with ip routers.although the commercial viability of various forms of ip telephonyremains untested, the prospect of future developments makes it possibleto think in terms of a revolutionaryñas opposed to an evolutionaryñuseof ip technology, in which phone calls become just one application on thepublic internet. not everyone agrees on how compelling the technicaland business cases are for offering voice services over the public internetor what the time frame for such a transition might be, but there is littledoubt that both new and existing players will enter this market. thetiming and viability of such developments depend in part on being able toprovide the transmission resources needed to obtain sufficient voice quality (either by deploying qos mechanisms across isp boundaries or bytaking advantage of a rising tide of capacity that makes the average quality sufficient to meet voice requirements; see chapter 2). the motivationfor such a shift would not be just the cost reductions associated with usingipbased components and systems but would also be the ability to offernew features, particularly at the human interface, made possible by a shiftto more intelligence in enduser devices.the evolving architecture of the pstnover the past several decades, the pstn has also been changing, froma network with only basic circuitswitching functionality to a circuitswitched network architecture known as the advanced intelligent network (ain). this architecture makes use of centralized, closedapplication databases and a pstnspecific software development environment(known in pstn lingo as the service creation environment) to enable arange of capabilities such as tollfree number dialing (including routing acall to the appropriate point), credit card calling, and flexible call forwarding.as the architecture evolves, the intelligence of the pstn is becomingincreasingly distributed and open. the pstnõs architecture is evolvingfrom the ain model, which uses centralized switching system databasesand interoffice signaling, into a model that uses direct signaling to moredistributed databases. there have also been efforts to further separatethe applications from the operating system (i.e., the basic switching software) and the underlying infrastructure so as to make the pstn a moreopen service creation environment.the opening up of the pstn service creation architecture is aimed atincreasing the flexibility of the infrastructure and enabling a greater number of parties to create useful services. in the ain, the service creationenvironment and the applications are provided by the service providerthe internet's coming of agecopyright national academy of sciences. all rights reserved.160the internetõs coming of ageand generally closed to third parties. open ain was proposed by pstnoperators in the mid1990s as a way of enabling a set of entities beyondthe service provider to create services. today, by virtue of the pstnõsconnectivity to the internet, service creation is being enabled at the edgesof the network. these moves toward a more open environment increaseflexibility but pose challenges related to maintaining the stable operationof the network; some of these are discussed below.while the intelligent network is becoming more distributed and open,there is another fundamental change under way in the pstn: it is becoming more datacentric, with highspeed data access, transport, and switchingñan architecture that many believe will ultimately replace the presentintelligent, circuitswitched architecture. at the same time, the transportbackbone is evolving, with greater deployment of optical transport, suchas wave division multiplexing (wdm), to accommodate the voraciousdemand for bandwidth. ip over atm over wdm or ip directly overwdm are emerging to replace other optical communications technologies, such as sonet or sdh, as capabilities are introduced into the optical layer. also, local access is evolving to support higher data speeds asdatacentric communications are introduced in the local loop in the formof dsl and digital optical fiber deployments closer to end users in telephone networks and in the form of hybrid fiber coax deployments incable networks.pstnõs evolution from a predominantly circuitswitched to a packetswitched architecture is producing a significant change in architectureconstruct and concomitant operations. new packetbased data (atmand ip) capabilities are being introduced in combination with the existingss7.6 this transformation has included the adoption of new, packet network telecommunications protocols. along with new protocols to establish connections between ip and pstn systems, these packetbased capabilities allow voice telephony to continue to be provided even as thenetwork architecture moves away from the circuitswitched model. thenew architecture, protocols, and intelligence capabilities together enablea richer set of prospective applications and services, such as internet callwaiting. while there is surely room for the market to experiment withvarious price/quality tradeoffs, customers will expect the emerging datacentric architecture and technologies to provide the quality they associatewith todayõs pstnña challenge being addressed at this time.6in the long term, a possible direction is that the introduction of mpls with core atmswitching could provide the necessary infrastructure and control capabilities.the internet's coming of agecopyright national academy of sciences. all rights reserved.telephony as a case study161architectural contrasts between ip telephony and todayõs pstnalthough the pstn architecture is changing in significant ways, thereremain considerable differences of approach between it and the variousarchitectural alternatives emerging for ip and internet telephony. thesedifferences, which stem in large part from the fundamental contrast between the pstnõs centralized design and the internetõs open, distributedarchitecture, include the following:¥unlike in the pstn, in ip telephony the function of transporting thepackets carrying voice can be carried out by an entity different from the oneproviding application services (e.g., call agents and directory services). thismeans that voice telephony service can exist as an overlay to ip networksñindeed, voice traffic can be carried over networks unbeknownstto the ip network operators. application support servers are not evenconstrained to be located within the same network as the customerñoreven the same country. it also means that internet telephony qualitydepends not only on the quality of the functions (e.g., a call agent) offeredby the telephony provider itself but also on the quality (including reliability and freedom from congestion) of all of the underlying networks overwhich communications pass.¥like other packet data, traffic associated with a phone call over the internetwill transit one, two, or many providers depending on which networks the callingparties are attached to and how the networks are interconnected.  the routetaken by a packet is a function of numerous routing decisions made withinthe network; it may change dynamically, and it is outside the control ofthe end user.¥ip telephony application servers do not necessarily provide the same functionality as the pstn. for instance, an application server established by avoice telephony overlay service might be concerned only with providingdirectory and call setup services. once a call is established, data packetswill only flow between the callers, and the voice telephony providerõsserver will have no access to the content of voice calls. it would, forexample, be impossible in this architecture for the telephony provideritself to carry out a wiretap order to provide the content of a call to lawenforcement. nor would the provider itself be able to offer priority routing for emergency calls, as might be desired to provide functionalityanalogous to 911 telephone calls.¥the generalpurpose nature of the internet means that it can offer voicecommunication in novel ways that do not parallel classic telephony. telephonyis understood to refer to communication between devices attached to thepublic switched telephone network (e.g., telephony regulations do notapply to twoway radios or inbuilding intercom systems). the line bethe internet's coming of agecopyright national academy of sciences. all rights reserved.162the internetõs coming of agetween telephony and other applications could start to blur. for example,todayõs popular textbased chat applications may well turn into voicechat. are the latter to be considered telephony? there are, in fact, avariety of voicecommunicationsbased applications running over theinternet today. internetbased video games that allow players to exchangevoice comments along with the game play do not, owing to their specialized purpose, look like telephony, but the line is not sharp. edgedriveninnovation of voice applications means that such questions are likely toemerge frequently and rapidly.scenarios for future evolutionthe extent, pace, and nature of a transition to ip or internetbasedtelephony services is unclear. the views of those in the two industriesñthe internet and the pstnñas well as outside observers are based ontheir perceptions of how fast the internet and its underlying technologiesare evolving as well as on their world views. many industry analystspredict that the rapid growth of data networks, particularly the internet,will lead to the melding of voice and data networks, and that voice trafficwill increasingly be carried using ip technology.there are a number of practical reasons why ip telephony could overtime supplant traditional pstn voice services. carrying data and voiceservices over a common enterprise network promises reduced infrastructure cost and ease of management. this would be especially true whenthe volume of data traffic is steadily growing, making the case for investment in a datacentric architecture more compelling. bypassing the pstnby using a private data network for voice would also allow an enterpriseto avoid longdistance charges.others argue that for technical and economic reasons ip may notalways be the best or most efficient choice for telephony, particularly inthe short run. for instance, voice over ip is sometimes less efficient in itsuse of communications link capacity than existing pstn technologiessuch as time division multiplexing (tdm) or atm. another challenge isthat putting timesensitive voice traffic on a corporate data network placessubstantial new demands (e.g., properly engineering the ip fabric and theunderlying network) on network managers unaccustomed to managing anetwork for voice traffic.several technical trends are promoting the integration of voice andip service and the shift to ip. efforts are under way to move the fullrange of voice services to ip technology, which would one day allow ipbased service to supplant circuitswitched network service. as a consequence of this shift, there is likely to be a far richer set of voice servicesavailable, raising the prospect of ipbased telephony offering a richerthe internet's coming of agecopyright national academy of sciences. all rights reserved.telephony as a case study163and more powerful suite of services while the traditional pstn remainsin place. at the same time, considerable attention is being paid to thedevelopment of ippstn interoperability capabilities in the form of gateways between ip and circuitswitching technologies, enabling the creation of hybrid services.the course of ip and internet telephony will be determined in part bythe organizational and cultural factors at work in the two historicallyseparate technical communities: those responsible for the pstn and theinternet. from the pstn community perspective, given the extensiveinvestment made in deploying ss7 and the capabilities and features required for telephony services, it would be easier to develop new servicesby leveraging the capabilities of ss7 than by building voice services fromscratch.efforts to bridge the gap between the communities are evident in theefforts of several ietf working groups, including their collaboration withmore traditional telephony groups such as the international telecommunication union. constructing robust gateways between the two infrastructures and building de novo ipbased implementations of all of therequired functionality of the pstn are both formidable challenges. thenature of the challenge as well as the inevitability of a long period ofcoexistence and interoperation between the technologies argues that success in building a seamless network for the future will benefit from greatercooperation between the two communities.from the internet community perspective, the more centralized pstnarchitecture runs counter to the distributed, edgecontrolled internetmodel. motivated by a belief that open standards will permit more rapidinnovation and development of a whole range of new services and applications related to telephony, some advocate ietflike processes to develop standards that can stand apart from the pstn. their advocacystems in part from an expectation that the pstn world will not open uppstn network elements enough to permit the kind and pace of innovation characteristic of the internet.telephony is very much in flux, with many decisions being made inthe marketplace today by customers, equipment vendors, and serviceproviders whose landscape is changing rapidly. the pstn is not monolithic; it has always been composed of multiple networks, and those networks are managed according to diverse philosophies about deployment,upgrading technology, and so forth. the latest trend is the significantincrease in the number of players as new competitors emerge in the localtelephone market (the competitive local exchange carriers, or clecs),with entrants such as qwest and level 3 challenging the incumbent longdistance telecommunications carriers. cable operators are providing telephony and internet access as well as content, and traditional entertainthe internet's coming of agecopyright national academy of sciences. all rights reserved.164the internetõs coming of agement and media firms are moving to distribute voice, data and videoservicesñthe aoltime warner merger accentuates that trend. at thesame time, the traditional pstn service providers are offering cable,internet, and content services.the landscape of equipment suppliers is also changing rapidly. telephone switch suppliers realize that they need to stay competitive and areresponding accordingly, although it will be a challenge for them to evolvetheir business models fast enough to respond to the competition. traditional suppliers to telecommunications companies for the pstn in theunited states, for example, consisted of a few major switching supplierssuch as lucent (formerly at&t), nortel, and siemens andñsince divestiture of at&tña plethora of transport and access product suppliers.their competition now includes manufacturers of other kinds of products, and mergers and acquisitions further blur the lines between previously distinct market segments. in the past few years, traditional routersuppliers acquired atm equipment manufacturers (e.g., cisco acquiredstratacom), while traditional atm/frame relay equipment manufacturers (one such was ascendñpreviously cascade) were acquired by lucent, and the router/hub manufacturer bay networks was acquired bynortel. now, the traditional telecommunications circuitswitching equipment suppliers, and others as well, are paying greater attention and devoting more resources to packet switching and data networking products, whereas companies such as cisco are working or combining withother suppliers to meet pstn and other service provider needs. at thesame time, qualityofservice capabilities are being added to cable modems, improvements reflected, for example, in the new docsis 1.1 standard, which supports ip telephony.will ip telephony be largely confined to those places where it hasproved viable on a large scale today, such as backbone transport withincarrier networks or within private ip networks? or will it ultimately bedeployed much more widely on the public internet? one possibility isthat ip telephony networks will be melded together using ip, so that callsare carried endtoend over ip. it is also possible that there will be manyip telephony networks built by a variety of players (both traditional localand interexchange carriers) that form growing clouds of end users employing only ip telephony inside but relying on the pstn to carry trafficbetween them. a factor that argues for this scenario is the difficulty ofproviding adequate quality of service for calls that flow across multiple ipproviders. because of the larger number of players involved, this difficulty is greatest in the case of telephony over the public internet. disincentives to using the public internet appear likely to decline to the extentthat the qualityofservice and reliability issues are resolved, for withthe internet's coming of agecopyright national academy of sciences. all rights reserved.telephony as a case study165these barriers removed, the public internet allows leveraging of both common technology and common infrastructure.how will the internet and the pstn relate to each other in the future?one possible outcome is that ip telephony and pstn telephony servicescontinue to coevolve, with considerable effort going into dealing withaddressing and signaling issues between the two networks. another isthat the two will develop in parallel but that ultimately the pstn as it isknown today will disappear. a third outcome is that the two will increasingly interoperate and, over time, will converge in terms of the networkarchitecture and the technology employed. in this case, the pstn willmove to a more datacentric architecture, with the internet and pstnunderlying architectures becoming increasingly similar. circuitswitching technology would not disappear immediately but would phase outfor voice services over the next decade. underlying this range of possibleoutcomes is a basic issue: will ip telephony be driven primarily by theneeds of interoperating with the pstn or by a desire to have new, iponlyfeatures?interoperation between ip telephony and the pstnwhatever the course of change for the internet and the pstn, it isclear that for the foreseeable future, dedicated ip networks, the publicinternet, the pstn, and hybrids of these will all play a role in deliveringtelephony services. from a customer perspective, a smooth transitionwill depend on the extent to which telephony networks remain interconnected (anyone can continue to place a call to anyone else) and the extentto which it appears seamless to the end user, who will not have to act invery different ways when calling different parties. achieving this goalwill require the development of standards for telephony functions withinip networks and at gateways between ip and pstn networks. theseinclude the algorithms used to digitize and compress the voice signal(codecs), gateway functions, addressing, directory services, call control,and the like. as with other standardization efforts, this effort will have tobalance standardization with flexibility that permits ongoing innovationin telephony services.a number of groups, with origins in both the telecommunicationsand internet sectors, are working to address these issues. among themare the following:¥ietf. a number of ietf groups are working on telephony issues,including the ip telephony working group, which is developing protocols for call processing and distributing information about gateway capathe internet's coming of agecopyright national academy of sciences. all rights reserved.166the internetõs coming of agebilities;7 the ietf media gateway control working group, which is developing an architecture for controlling gateways between the ip networks;8and the telephone number mapping group, which is working on protocols for mapping telephone numbers to other attributes (e.g., urls) thatcan be used to contact a resource associated with the numbers.9¥international telecommunication union standardization sector (itut). itut activities include itut study group 13,10 which is studyingipbased network issues such as interoperability with other networks,signaling requirements, numbering, and security, and study group 16,which is working on protocols and standards for multimedia services.¥european telecommunications standards institute. etsi, the developer of the gsm standard for cellular telephones, has a project, telecommunications and internet protocol harmonization over networks(tiphon), that is developing an architecture and requirements for interoperability and exploring technical aspects of billing; call control; naming, numbering, and addressing; and quality of service.11¥softswitch consortium. this industry group provides interoperability testing facilities, testing events, specifications, reference implementations, and development resources for a number of voice and multimediacommunications standards.12as this list suggests, there are areas of overlap and conflict among theactivities of these groups. these have been diminishing, however, andthere are instances of collaboration between groups. for example, theietf media gateway control working group is collaborating with itutstudy group 13.central to a successful coevolution is the resolution of interoperabilityissues in numbering and addressing; signaling and control and servicecreation capabilities; and robustness concerns. each area is treated in turnbelow.7see ietf ip telephony working group charter, available online at <http://www.ietf.org/html.charters/iptelcharter.html>.8see ietf media gateway control working group charter, available online at <http://www.ietf.org/html.charters/megacocharter.html>.9see telephone number mapping group working charter, available online at <http://www.ietf.org/html.charters/enumcharter.html>.10see itut study group 13 home page, available online at <http://www.itu.int/itut/com13/index.html>.11see the tiphon home page, available online at <http://webapp.etsi.org/tbhomepage/tbdetails.asp?tbid=291>.12see softswitch consortium frequently asked questions, available online at <http://www.softswitch.org/faqs/index.html>.the internet's coming of agecopyright national academy of sciences. all rights reserved.telephony as a case study167addressing and number portabilityno matter what the ultimate end point, telephone number assignment will be more complicated as ip telephony services emerge. numberportability across the public switched telephone network13 and ip telephony services is an important consideration, at least over the short term.today, local phone number portability in the pstn is provided by thelocal exchange carriers, permitting customers to switch local carriers without changing their standard pstn phone number.14 in the future, theremay be no conventional telephone number associated with customersusing ip telephony; in fact, depending on how their ip address is assigned, they may or may not have a fixed ip address. how will numberportability be handled? several requirements for a smooth evolutionstand out. first, customers should be able to in some fashion transfer(òportó) their existing telephone number to the internetbased service,just as customers can today retain, at the same location, the same telephone number when they switch from one local exchange carrier to another, allowing them to continue to receive calls directed to the phonenumber provided by their original local exchange carrier. second, thereis the question of how someone on the pstn calls an ip telephony subscriber. this suggests that internet service providers and other internetbased telephony providers should be able to issue new telephone numbers to their customers, even to those who do not have conventionaltelephone service, so as to provide compatibility across calls originated orterminated on conventional and ipbased telephones.additional issues arise when a given household uses both pstn andinternet services. for example, what address or number should be usedwhen a standard pstn (e.164) number is assigned to a household thatuses an ip address (perhaps from an ip telephony directory server thatmaps a name or other identifier to an address) for an ip telephony appliance? portability needs to address both aspects for subscribers who wishto port their service as well as for subscribers using ip who wish to changetheir internet service providers.13e.164 is the itu standard (òrecommendationó) for the international public telecommunications numbering plan, which specifies a geographically hierarchical numbering plan inwhich numbers are assigned to customers by carriers.14local number portability was mandated by the telecommunications act of 1996. in itsrules implementing the act, the fcc allowed local telephone companies to assess charges torecover the costs of implementing and providing portabilityñboth a charge to be paid byother carriers in exchange for the use of number portability facilities and a monthly chargefor all telephone customers.the internet's coming of agecopyright national academy of sciences. all rights reserved.168the internetõs coming of agebeyond these more immediate interoperability considerations liebroader questions of addressing. for example, telephone services todayalready allow using abstractions of the phone numbers. voiceactivateddialing allows a caller to substitute a phrase (e.g., òcall momó) for a phonenumber. other services offer a single number that then follows the customer to whatever phone is in use at the time a call is put through. suchabstractionsñsomewhat like the level of indirection provided by todayõsdomain name system, which allows access to internet devices via a namerather than a numerical addressñcould also be applied to the ip telephony domain.15 in the long term, phone numbers could be replaced byother identifiers. how to provide suitable directory services is currentlybeing explored in the forums listed at the start of this section; agreementon a standard will be crucial to building hybrid internetpstn or internetonly telephony networks that appear seamless to the end user.while this report does not explore these issues in depth, the issuesthat arise when one starts to explore solutions for all of these name/number portability requirements closely resemble those that arise for thedomain name system (see òscaling of the internetõs naming systemsó inchapter 2). for example, there are similar issues of ensuring scalability,supporting dynamic updating, ensuring that the name/number infrastructure is robust and secure, and authenticating updates to directoryentries.signaling and control and service creationas architectures for ip telephony emerge, there are unresolved questions of how they will interface with the existing ss7based global pstn,including how signaling and control, the functions that allow calls to beset up and the network to be managed, will be provided for a hybridpstn/internet infrastructure. solutions must be sufficiently standardized to allow interoperation and yet flexible enough to encourage innovation in new applications. open questions include the following: whatsignaling capabilities need to be established to provide messaging acrossall media (wireless and wireline) and different types of devices? what isrequired to meet the needs of telephony that runs over the public internetin contrast with telephony that runs over private networks using ip technology, and what are the implications for the signaling and control infra15unlike the old naming schemes for telephone exchanges (e.g., òjacksonville 6500ó), inwhich there was a fixed mapping between name and exchange, a naming scheme thatprovides indirection allows the number with which a name is associated to be easilychanged, perhaps at the direction of the telephone user.the internet's coming of agecopyright national academy of sciences. all rights reserved.telephony as a case study169structure to support these changes? how is quality of service assuredacross the multiple providers for data services? these are some of theimportant questions that the working groups described above are addressing.the control capabilities must be able to handle both the pure ip telephony and the interdomain calls between internet telephony and pstnelements. the committee foresees some interesting complications. forinstance, a household of the future might well employ both conventionalpstn telephone and ip telephony equipment, with the latter making useof one or more local access technologies (e.g., ip over dsl or ip over acable modem). telephones of the future might use a variety of localaccess technologies, such as ip over atm or over dsl. it will need to bepossible to manage each of these capabilities with the appropriate protocols. another example of these complications is the clicktospeak servicementioned earlier, which may involve a voice connection between thecustomer and the service representative or the downloading of streamingvideo from a server to the prospective customer. a suite of open standardprotocols will be needed to enable these interoperable sets of pstn/internet services.robustnessthe robustness of the pstn benefited in the past from agreementamong a relatively small number of players to follow a tightly definedarchitecture and set of operating practices. as the pstn market hasgrown, the number of players has grown, too, potentially affecting robustness. however, the opening up of the pstn architecture and interoperation with other ip networks give rise to robustness concerns that gobeyond those that would be posed by conventional new entrants to thepstn. these concerns fall into two categories. first, there are concernsabout how to ensure the reliability of a more open architecture in whichthere are fewer controls over the inputs, e.g., signaling and control messages, received by pstn networks and in which these inputs may interact. second, there are concerns about how to ensure overload controlsand network availability in this more open environment.as a network is opened up, additional attention must be paid toauthentication (is the originator of the message authorized to perform thefunction?) and validation (is a request a reasonable one that can be executed without harm to the network of signaling and control messages?).another issue is how to localize the impact of problems. how can a moreopen service creation environment be designed to create new applicationswithout adversely affecting the applications of others? what mediationcapabilities and other new servicecreation or application programmingthe internet's coming of agecopyright national academy of sciences. all rights reserved.170the internetõs coming of ageinterfaces should be developed? can there be service creation at theedges without having to worry about multiplecreator application interactions and adverse consequences? if so, how does this manifest itself inan internet/pstn environmentña hybrid of centralized or partially distributed service creation in the pstn plus the fully distributed servicecreation of the internet? how can the combination of sces flourish in thisevolution? how do the providerproprietary applications for specificcustomers affect the service creation environments?if networks carrying voice networks are to continue to meet stringentperformance requirements in the face of opening them to a diverse set ofproviders, the ability to control against overload and provide enhancedrobustness will need to be included. these mechanisms will need to bedesigned not only into the systems comprising the pstn/internet combination but also into the basic signaling and control architecture, to prevent serious congestion, service degradation, and severe outages.implications of ip telephonyfor telephony regulationthe emergence of ip telephony heralds conflicts between, on the onehand, ip telephonyõs practices and assumptions and, on the other hand,the practices and assumptions of the existing regulatory regime. theseconflicts stem in large part from the contrast between the dynamic, rapidchange that internetbased innovation enables and the historically relatively stable nature of pstn technologies and businesses. as ip telephony gains market share, it is likely to have a dramatic impact on thetraditional, regulated voice service providers. such developments haveprovoked and will continue to provoke calls for voice over ip to be subjectto regulation akin to that in place for circuitswitched voice services or forthe regulatory regime to be modified in other ways to cover this new formof telephony. as new ip and internetbased services emerge that in someway resemble pstn services that are currently regulated, there will be anumber of questions about whether these new services should be treatedin the same fashion. at the same time, in the face of competition, pstnoperators may develop and offer new services in order to attract customers, raising questions about how those services will be accommodatedwithin the existing regulatory framework.the position of the federal communications commission has been tokeep the internet free from unnecessary application of the existing regulations on telecommunications, and many have argued that this handsoffapproach has been a significant factor in the internetõs explosive growth.1616see, for example, jason oxman. 1999. the fcc and the unregulation of the internet.office of plans and policy (opp) working paper no. 31. washington d.c.: opp, federalthe internet's coming of agecopyright national academy of sciences. all rights reserved.telephony as a case study171nonetheless, to the extent that ip telephony applications increase theirmarket share, it is reasonable to anticipate that pressure will come fromthe other players (e.g., other telephony carriers and consumers) to increase regulatory attention to internet telephony.the potential inconsistency between the assumptions underlying existing regulations and those that would be applicable to new forms oftelephony is well illustrated by one concern raised by local exchangecarriers that has been the subject of fcc regulatory attention: wouldlocal termination tariffs be applied to phone calls carried on the internet?internet telephony services (or the isps that carry the data associated withthe phone call) do not, for example, have to pay local exchange carriersfor terminating calls when one of the callers is connecting to the internetvia a modem running over a pstn telephone line (as a conventional longdistance provider is required to do when it hands off a call to a localexchange carrier). some of the advantages of using the internet for voiceare thus amplified or in some instances even driven by tariff and regulatory artifacts that treat ip and public switched telephone networks ratherdifferently. such advantages can be seen either as unfair to incumbents oras appropriately reflecting the emerging, evolving nature of telephony.a key question is whether ip telephony should be subject to commoncarrier provisions. the implication of such status could be positive (e.g.,no liability for the content it carries) or negative (e.g., the need to meetcertain standards of operational integrity that go beyond what is requiredof an internet, cable, or other noncommon carrier provider) for the iptelephony provider. the telecommunications act of 1996 defines telecommunications as the òtransmission, between or among points specifiedby the user, of information of the userõs choosing, without change in theform or content of the information as sent and received.ó a telecommunications carrier is defined as òany provider of telecommunications services, except that such term does not include aggregators of telecommunications services (as defined in section 226).ó the act goes on to say thatòa telecommunications carrier shall be treated as a common carrier underthis act only to the extent that it is engaged in providing telecommunications services, except that the commission shall determine whether theprovision of fixed and mobile satellite service shall be treated as commoncarriage.ó17 finally, telecommunications is defined as òthe offering oftelecommunications for a fee directly to the public, or to such classes ofusers as to be effectively available directly to the public, regardless of thefacilities used.ócommunications commission, july. available online at <http://www.fcc.gov/opp/workingp.html>.17a subsequent fcc ruling in fact specifically exempted mobile satellite carriers.the internet's coming of agecopyright national academy of sciences. all rights reserved.172the internetõs coming of agethese definitions immediately give rise to the question of whether iptelephony providers or isps could fall under common carrier provisions.resolution of this sort of question will depend on how existing definitionsare applied to new technologies and services and, more fundamentally,on whether it is deemed appropriate to apply existing definitions andrules to these newly emerging services. for example, the actõs definitionof telecommunications could be read to say that if the input is voice andthe output is intended to be the same voice, then regardless of the detailsof how the voice signal is processed internally, voice communicationscarried over ip would also be considered telecommunications.one answer to the common carrier question was provided by the fccin its august 31, 1999, ruling18 on the communications assistance forlaw enforcement act (calea) requirements (see box 4.1).19 calearequired telecommunications carriers to provide assistance for law enforcement in carrying out wiretaps. in its ruling, the fcc mandated thatcable television and ip servicesñinasmuch as they provide telecommunications services and are a telecommunications carrier and thus a commoncarrier, at least for the purpose of caleañare subject to calea. thefcc has given ip carriers until september 30, 2001, to comply.20 thisruling opens the door for a broader interpretation and acceptance of iptelephony as common carriage.in addition to suggesting future directions with respect to the common carriage status of ip telephony, the effort to apply calea in thisarea also suggests the sorts of societal expectations that surround telephony. the advent of ip telephony raises the questions of whether andhow these expectations will be extended to the new technologies andnetworks. one such expectation is the use of telephony for public safety.conventional wireline telephones provide enhanced 911 service, whichautomatically provides public safety organizations with the address of a911 caller. cellular telephone service providers are being required toprovide analogous information for 911 callsñthat is, precise informationon the location of the caller.18federal communications commission (fcc). 1999. second report and order in the matter of communications assistance for law enforcement act. cc docket no. 97213, fcc 99229.washington, d.c.: fcc, august 31. available online at <http://www.fcc.gov/bureaus/commoncarrier/orders/1999/fcc99229.txt>.19calea, 47 usc 1001, pl 103414.20the question of whether the ietf should participate in the design or modification ofinternet protocols to accommodate requests for law enforcement access to internet trafficwas discussed with much fanfare in the november 1999 plenary meeting of the ietf. predictably, there were public statements critical of the notion. there were also statementssupporting the proposition, citing, for example, vendor anticipation of demand for suchcapabilities in at least some segments of their worldwide markets.the internet's coming of agecopyright national academy of sciences. all rights reserved.telephony as a case study173box 4.1 fcc ruling on calea and common carrier statusthe view that ip telephony is a common carrier for the purpose of calea issupported in paragraph 17 of the fccõs august 31, 1999, order:17. common carriers and utilities. we adopt our tentative conclusion,with which most commenters agree, that all entities previously classified asòcommon carriersó should be considered telecommunications carriers forthe purposes of calea, as should cable operators and electric and otherutilities to the extent they offer telecommunications services for hire to thepublic. such entities offer services (some subject to calea, some not) thatuse copperwire, cable, fiberoptic, and wireless facilities to provide traditional telephone service, data service, internet access, cable television, andother services. the actõs legislative history identifies such entities as subject to calea to the extent that their service offerings satisfy caleaõs description of covered services. entities are not subject to calea, however,with respect to services and facilities leased for private networks, pursuantto the statute. in addition, cable television is an example of a service notcovered by calea because it is not a òtelecommunicationsó service, evenif delivered via the same transmission facility as other, covered services.paragraph 27 of the order has the following to say about information servicessuch as the internet or similar ipbased services:where facilities are used solely to provide an information service,whether offered by an exclusivelyis provider or by a common carrier thathas established a dedicated is system apart from its telecommunicationssystem, we find that such facilities are not subject to calea. where facilities are used to provide both telecommunications and information services,however, such jointuse facilities are subject to calea in order to ensurethe ability to surveil the telecommunications services. for example, digitalsubscriber line (dsl) services are generally offered as tariffed telecommunications services, and therefore subject to calea, even though the dsloffering often would be used in the provision of information services. on theother hand, where an entity used its own wireless or satellite facilities todistribute an information service only, the mere use of transmission facilitieswould not make the offering subject to calea as a telecommunicationsservice.robustness and quality are also expected of telephony services, manifested in state and federal regulatory attention to carrier performance.the relative ease with which the internet can offer voice services meansthat many businesses will be in a position to offer voice services, probablyover a broad spectrum of service quality and reliability, from robust andhigh quality to fragile, unreliable, and low quality. in addition, the abilityto overlay telephony on top of existing ip networks and the emergence ofthe internet's coming of agecopyright national academy of sciences. all rights reserved.174the internetõs coming of agemixed pstn and ip network environments can be expected to raise newquestions about which parties are responsible for service quality. business imperatives and customer demands can be expected to address manyof these concerns, but consumers will still need to be clear about thedifferences between service offerings (e.g., from a consumer protectionstandpoint) and their acceptability from a performance standpoint (e.g.,to meet public safety requirements).there are also particular economic arrangements in the form of universal service charges and fees attached to telephony aimed at increasingaccess to telephone service. today, these are imposed only on conventional telephony service. there are two views on this matter: somewould like to see a level playing field across technologies and providersand others are concerned about the potential for declining revenue fromthese fees. (a related question is what form universal service takes withrespect to internet service; see chapter 5 for a discussion of the options.)another set of issues arises from inconsistencies between the newtechnologies and architectures and the assumptions about architecturethat are embodied in the existing regulatory regime. three such issuesare as follows:¥in internet telephony, there are no meaningful distinctions between localand interexchange carriers. the existing regulatory regime, in which localand longdistance services are separated and in which customers select aparticular longdistance carrier to carry calls outside their local area, doesnot map onto internet telephony because the architectures of the twonetworks are so different. in the pstn, it is possibleñand indeed necessaryñto specify the longdistance carrier over which a call is to beswitched. in contrast, the internet architecture and protocols were notdesigned to provide carrier selection capabilities like those in the pstn.21today, the only way in which the longdistance segment of a call could beseparated from the local segment would be a handoff to voice servicesover a psn interexchange carrier. an insistence that local and longdistance carriers be distinct entities could only be complied with by establishing requirements that run counter to the internetõs basic architecture.¥owing to the separability of data transport and application service functions, meeting some regulatory requirements that have been established for telephony will be difficult or require different implementations. for example, it ispossible to construct a telephony service where the application serversconcern themselves only with providing directory services and call setup21however, under current regulations incumbent local exchanges that provide internetservice must offer their subscribers a choice of longdistance ip network providers.the internet's coming of agecopyright national academy of sciences. all rights reserved.telephony as a case study175while the communications associated with the call depend only on thepublic internet. if a wiretap is ordered for a customer of an isp, theapplication service provider will have no means of allowing law enforcement to access the content of the calls (although it could provide information on the identity of the calling parties). the wiretap might require adifferent technical approach, such as one that relies on accessing datapackets at their entry point to the internet at the customerõs isp.¥attempts to force the internet to fit the existing regulatory model couldinhibit innovation by forcing modifications to the internetõs architecture. thecurrent regulatory apparatus is, for example, not set up to respond to aworld in which new telephony applications can be deployed simply byhaving a third party distribute some new software and set up a few servers. already, a number of telephony providers exist as overlays to thepublic internet, and it is reasonable to suppose that these will grow innumber and market share. one question these providers raise is whom tohold responsible for meeting such mandates as provision of 911 service orcompliance with calea. there are two general ways in which this couldbe done. the parties offering themselves as telephony providers could bedesignated as the ones responsible for meeting the requirements. or, therequirements could be imposed on the internet service providers. thesecond alternative, while it might prove attractive because it makes theisps responsible for determining who is and who is not providing voicetelephony, could have profound implications for the internet. if ispswere required to ensure that voice traffic carried over their networks fallsunder a particular set of rules, it would become necessary for them toexamine all the traffic over their network to screen out òunacceptableóvoice communications. because telephony providers are free, in keepingwith the internetõs edgebased innovation model, to design their ownprotocols for telephony, it can be impossible to reliably identify whichtraffic is associated with telephony applications. compliance with screening requirements might, in the end, make it necessary for isps to onlyallow traffic of known, acceptable types to be transmitted, an outcomethat runs counter to the hourglass transparency of the internet. this lineof reasoning is discussed in greater detail in chapter 3.looking forward:the internet and other industry sectorsthe preceding sections illuminate, in the context of telephony, issuesof regulatory inconsistency, protection of incumbent companies, and restructuring of the service as perceived by the consumer. technologicaldevelopments and deployment of the first generations of new servicesthe internet's coming of agecopyright national academy of sciences. all rights reserved.176the internetõs coming of agemake it apparent that the next decade or so will see similar collisionsbetween internetbased businesses and other large industry sectors.looking beyond telephony, some of the easiest collisions to foreseetoday are internetbased distribution of music, which is exemplified bythe collision between the mp3 encoding protocol and traditional channelsof music distribution; the more general transmission of radiolike audiocontent over the internet; and, eventually, internetbased televisionlikeservices, which would collide with broadcasting. these applications arenot visions of the distant future. internetbased music distribution is arapidly growing service, and while little entertainment video is transmitted over the internet, specialized applications such as continuing education and training video are run over the internet today. many radiostations are sending their content over the internet simultaneously withtheir overtheairwaves broadcasts. as industry groups such as traditional network broadcasters, retail cd distribution chains, advertisingmarketers, and large contentcreation organizations find their marketsbeing nibbled at by internet alternatives, they can be expected to react.the result may be large transformations and dislocations in existing markets; the result may also be stressful for the internet, its design principles,and its service providers.one predictable trend is the use of existing regulation by incumbentsto protect a legacy industry position. for example, in radio and television,there are rules (about, for example, public access or political access) thatpresume that space on the broadcast spectrum is scarce. since capacity onthe internet is not scarce and transmission does not require a federallicense, anyone can, in principle, generate and distribute content, and it isnot clear that there is a rationale for applying these rules to the internet.however, much as one comes across similar arguments with respect totelephony, one can easily imagine calls for these rules to be imposed onsome forms of internet content providers. (the argument would be that itis unfair to impose different burdens on producers of similar sorts ofcontent that happen only to use different forms of distribution.) anotherarea where existing practices and the capabilities afforded by the internetcollide is copyright, where the assumptions underlying the current copyright regime are being stressed by the ability to make perfect copies ofdigital works as well as by the ease with which they can be distributedover the internet. the technological, legal, economic, and social factorssurrounding copyright are too complex to analyze here; for more discussion, see a recent cstb study of these issues.2222computer science and telecommunications board, national research council. 2000.the digital dilemma. washington, d.c.: national academy press.the internet's coming of agecopyright national academy of sciences. all rights reserved.177introductionso far, the bulk of this report has focused on the internet from theinside outñhow its essential technologies are evolving and how the parties that build and operate it are evolving. in this chapter, the committeelooks at the internet from the outside in, examining some of the broaderinfluences on the internet that stem from the interests of the individualsand organizations that use it and the special concerns of governments,which have their own objectives and which can help balance and protectthe interests of individuals and other parties that use the internet.the internet has become the basis for a widening set of social, political, and economic functions and is becoming ever more pervasivethroughout society and its institutions. the benefits resulting from theinternetõs intrinsic qualities have an accompanying cost: disruption of thesocial, political, legal, and economic conventions on which a wide varietyof useful understandings have been based. while these consequenceshave been recognized for a number of years, they have grown in importance as the internet has become a key societal infrastructure. reflectingthe internetõs increased prominence, a diverse set of stakeholdersñboththe existing players and new, internetfocused onesñare paying attention to its impacts.the scope of the discussion here is limited, consistent with the scopeand resources of the project: a small but important and interconnected setof policy issues is outlined. the discussion is intended to illuminate thethe internet's coming of agecopyright national academy of sciences. all rights reserved.178the internetõs coming of ageinterplay of technical, economic/business, and public policy factors,drawing on the committeeõs experience in all three areas. because theinterplay is dynamicñtodayõs observations differ from yesterdayõs andwill be overtaken by events tomorrowñit can be hard to devise practicalresponses to perceived problems. nevertheless, responses are being devised, and a variety of technical, business, and public policy actions arealready being proposed or attempted. understanding and monitoringthe kinds of issues discussed here is important for making judgmentsabout how individuals, organizations, and governments could or shouldact in using or shaping the internet. the question is not just what theinternet does to policy but also what policy can do to the internet. thesecond part of the questionñhow policy affects the internetñasks howpolicy decisions that seek to impose particular technological solutionscould adversely affect the internetõs architecture and growth as well ashow policy decisions in areas such as privacy could affect user acceptanceof the internet and the services that run over it. how these issues areresolved is important to realizing (or limiting) the potential of the internet.while they are not solely technological issues, their emergence as policyquestions and the capacity to address them are shaped by technologicaldevelopments. this chapter addresses how the architecture of the internetcreates new issues and challenges and requires new approaches frompolicy makers if policy goals are to be met consistent with the strengths ofthe design and architecture that underlie the internet.the first set of issuesñprivacy, autonomy, and identity, along withauthenticationñarises from the sheer size of the internet and from growthin the number of people and organizations that it interconnectsñincreasingly, people communicate over the internet with strangers and others ofwhom they have limited knowledge or control. this set of issues centerson how the internetõs design, which provides limited information aboutthe identity or location of users, affects how we control our identity orevaluate the identities others present to us1 and what that means for ourunderstanding of privacy and the uses of anonymity. the internet provides weak clues about location or identity. its essential indifference togeography is, of course, valuable when it allows us to check our emailfrom new york one day and from los angeles the next, readily retrievematerials stored on a distant computer, or engage in a commercial transaction with someone a continent away. yet it also raises challenges tolaws and practices that are premised on knowing the location of parties toa transaction. an ip address is only loosely related to the geographicallocation or identity of a user or networked computing resourceñthis in1see, for example, sherry turkle. 1995. life on the screen: identity in the age of the internet.new york: simon & schuster.the internet's coming of agecopyright national academy of sciences. all rights reserved.implications for broad public policy179formation can sometimes be inferred, perhaps after an exhaustive investigation, but it is not readily available.2 that some isps believe their network topology and the location of their facilities is sensitive proprietaryinformation contributes to the lack of information about location. nor domany of the applications that run over ip provide authoritative information on either identity or location. the absence of identifying informationprovides benefits in terms of free expression but raises serious issuesabout how we manage, recognize, or negotiate identifying informationabout people and things in the electronic world. the discussion in thischapter examines these competing directions and explores whether relevant design enhancements/changes should be left to competitive forcesin the marketplace or require some focused attention by industry and/orgovernment.the second set of issuesñtaxation and universal serviceñis relatedto government missions. government is empowered to collect taxes tofund its operations, and it has an interest in both preserving its revenueswhile also fairly allocating the associated burdens, issues captured indebates about taxation of transactions conducted over the internet. asthe principal actor when it comes to social policy, governments havemoved to promote equitable access to the internet because of its growingvalue as a medium for economic, educational, civic, and other kinds ofopportunities, much as they have done for other infrastructure, such as2interestingly, the internet did not always have such a loose coupling between ip addressand location. this quality stems less from the basic internet design than from subsequentdecisions related to address space management and security. in the early days of thearpanet, interface message processors (imps) required a direct correlation of ip addressto port number on the imp, so one was more likely to be able to tell where a computer waslocated. there were, however, various ways in which hosts could be connected that wouldhave made it harder to tell where they were located. moreover, users generally interactedwith the network via terminal devices attached to host computers, and these could belocated far from the host computers. also, before cidr and address aggregation (describedin chapter 2), users did not receive their addresses from providers. precidr, addresseswere more likely to be globally routed, down to a much finer level of aggregation, whichagain made it easier to know where devices associated with particular ip addresses werelocated. address allocation policies coupled with service issues, such as maintaining security and increasing the ease of getting an internet connection, did induce large organizations to route all traffic in and out of their entire enterprise, which could span many different locations, through one connection. but with the advent of cidr and a crackdown oninefficient address space utilization, providers and users were forced into denser and moreobscure addressing relationships. motivated by address shortages and security issues,enterprises are using nat and firewall technology, in which globally unique addresses arenot used within corporate networks, further obscuring location information.the internet's coming of agecopyright national academy of sciences. all rights reserved.180the internetõs coming of agetelecommunications. to the degree that the internet can credibly claim tobe an essential infrastructure for transactions in commerce, political participation, basic education, and many other areas, it will become ripe forconsideration for universal service arrangements, which are interventionspremised on arguments that market mechanisms will not support widespread access affordable by all. the tradition of universal service waseventually attached to all important information infrastructures in thepastñpost, telephone, broadcasting, and, less obviously, basic education.privacy, anonymity, and identitydriven in part by the ease with which information about individualscan be gathered, the internet has amplified concerns about an interdependent set of issuesñprivacy, anonymity, and identity (box 5.1). theclosely associated subject of authentication is discussed in the followingsection. also discussed here are the tradeoffs that all of these mightimpose on privacy and individual rights. the section outlines importantinteractions among the internetõs technology, the internet service providers and related industry actors, internet users, and policy development and identifies some avenues where progress might be expected.these issues are not new, but internet growth and penetration haveheightened attention to them and are influencing the context withinwhich the internet is evolving.privacyconcerns about privacy have accompanied and been shaped by thedevelopment of technology for over a century.3 they have grown inbox 5.1 privacy, anonymity, and identity¥privacy is the right of individuals to control how information about them isshared, distributed, and used by other parties.¥anonymity is the ability of individuals to interact with others without lettingthose other parties have any knowledge of their identity. as such, anonymity is asufficient but not a necessary condition for achieving privacy.¥identity is any distinguishing attribute that is uniquely linked to an individualor object.3see, for example, samuel warren and louis brandeis. 1890. òthe right to privacy.óharvard law review 4(193), which laid out some of the fundamental arguments in favor ofthe internet's coming of agecopyright national academy of sciences. all rights reserved.implications for broad public policy181recent years with the introduction and widespread practice of such innovations as sophisticated customer profiling and telephone soliciting. theinternet has aggravated the situation. in surveys, people express concernabout the amount of personal information available on the internet, whocontrols that information, and how it may be used. for example, a 1998business week survey4 found privacy to be the number one consumerissue facing the internet, surpassing cost, ease of use, security, or spam.this survey found that 78 percent of online users would increase their useof the internet if privacy practices were disclosed and that 61 percent ofnonusers would be more likely to begin using the internet if privacypractices were disclosed. (survey answers notwithstanding, many peopledo provide personal information; most notably, some have chosen to provide a good deal of personal information in exchange for free internetaccess.) the absence of generally accepted, workable solutions is likely tocontinue to lead to calls for regulation, at least in the most troubling areas.for instance, concern about the online privacy of childrenñin particular,information they might be induced to reveal about themselves or theirfamiliesñresulted in the passage of special legislation, the childrenõsonline privacy protection act of 1998. although there are pressures forbroader change today,5 the outcome is uncertain. historically, privacyadvocates have been in the minority, peopleõs actions belie the results ofopinion surveys, and political pressure in the united states has been insufficient to invoke significant action. mid2000 debates over federaltrade commission interest in legislation related to online privacy areemblematic. in the united states, people continue to argue about privacyas a legal, protected right, while governmentbased inquiries into privacypolicy have articulated principles for public policy and private action,notably socalled fair information practices. the essential elements of fairinformation practices are generally described as awareness, choice, dataprivacy in modern society. the article was occasioned in part by privacy issues created bydevelopments in photography and photojournalism. see also alan westin. 1967. privacyand freedom. new york: atheneum; spiros simitis. 1987; òreviewing privacy in an information society.ó university of pennsylvania law review 135:707746; and james katz andannette tessone. 1990. òpublic opinion trends: privacy and information technology.ópublic opinion quarterly 54:125143.4as reported by truste at <http://www.truste.org/webpublishers/pubbottom.html>.5a survey in 2000 by odyssey, a market research firm, found that 82 percent of onlinehouseholds in the united states agreed that the government needed to play a role in howcompanies use personal information and 92 percent expressed some distrust of companieswhen it comes to protecting the confidentiality of personal information. see steve lohr.2000. òsurvey finds few trust promises on online privacy.ó new york times, may 17, p.c4.the internet's coming of agecopyright national academy of sciences. all rights reserved.182the internetõs coming of agesecurity, and customer access.6 first advanced in the 1970s by a congressionally chartered commission on information privacy,7 these practiceswere also discussed in the context of 1990s policy making on the national/global information infrastructure and electronic commerce.8privacy relates to the use, release, and availability of personal information,9 that is, any information that is linked to an individualõs identityor to attributes closely associated with that individualõs identity. personal information is collected or revealed both directly, as happens whena user enters information into a web form and submits it, and indirectly,as happens when information is gathered from publicly available information such as an email directory. whether in physical space or cyberspace, individuals are motivated to provide information about themselvesfor a variety of reasons, including the following: to obtain a desiredproduct, service, or end result (e.g., a loan commitment or a health claimbenefit); to obtain better, more customized/personalized products andservices (e.g., personalized news); to obtain specific information of value(e.g., stock quotes) or in anticipation of gaining some unspecified benefit(e.g., current bargains or offers); or to be rewarded with rebates and discounts, loyalty points, or frequent flyer miles. at the same time, peopleworry, sometimes with justification, that their personal information mayfall into the wrong hands or be misused. these concerns include suchundesirable results as receipt of annoying and unwanted information orsales pitches; denial of a desired product, service, or end result (e.g.,health coverage, an auto loan, or a job); personal embarrassment; damageto oneõs reputation; loss of trade secrets; or becoming a victim of somecriminal activity such as stalking, theft, fraud, or identity takeover.6see department of commerce. 1998. òelements of effective selfregulation for protection of privacy.ó available online from <http://www.ntia.doc.gov/reports/privacydraft/198dftprin.htm>. while all seem to agree on basic principles, there are variations in theprivacy frameworks that are used. for example, the federal trade commission addedanother element, enforcement/redress, in a recent report to congress. this is not surprising, given the ftcõs nature as an agency that makes and enforces rules. see federal tradecommission. 1999. òselfregulation and privacy online: a report to congress.ó washington, d.c.: federal trade commission, july.7privacy protection study commission. 1977.  personal privacy in an information society.8similar principles are contained in the òelectronic bill of rightsó presented in the firstannual report of the u.s. government working group on electronic commerce . see u.s.government working group on electronic commerce. 1998. first annual report, november, p. 17. available online at <http://www.doc.gov/ecommerce/ecomm.pdf>.9òprivacyó is distinct from confidentiality, which refers to the protection of all types ofsensitive information and is not necessarily approached from the perspective of protectionof personal information per se.the internet's coming of agecopyright national academy of sciences. all rights reserved.implications for broad public policy183one area of particular concern is that information provided with theuserõs knowledge can also be used for purposes other than that for whichit was originally provided. web servers can store personal informationgiven by the user while visiting the site; web site operators can subsequently use that information for other purposes, such as marketing, orprovide or sell it to third parties. users do not necessarily understand orappreciate the value of the information they provide, especially whendifferent bits of information can be combined and used for new purposes.an individual data item by itself might not appear to pose a privacyconcern. but when information is combined or associated with other,seemingly harmless bits of information often collected under differentcircumstances to build a dossier on a user, it may provide insights theuser would consider detrimental. for instance, behindthescenes tracking of users by means of cookies10 could permit address information entered at one web site to be linked with tracking data indicating that a userhad browsed several adult content web sites. the result might be theunwanted, unexpected arrival in the mail of advertisements for adultfilms. in other cases, combining personal data from different sources canadd value for customers. for example, an online bookseller that knows acustomer enjoys danielle steel novels may send the customer a review ofa new book by a different author that has been purchased by other customers with similar tastes.11 the customer never requested this information but may be glad to receive it. as another illustration, an airline website may refer a registered user who wants information about inexpensivevacations to a travel packager, who then emails the airline customerabout bargain travel opportunities. in both examples a firm uses personalinformation to offer services and products that some customers perceiveas valuable and others as a waste of time, offensive spam, or gratuitousinvasions of privacy.a second area of concern with respect to the internet is that somepersonal information can be collected online without the userõs direct10a cookie is a small piece of information that a web site stores on your web browser onyour pc and can later retrieve. the cookie cannot be read by a web site other than the onethat set the cookie. cookies can be used for a number of administrative purposesñforexample, to store your preferences for certain kinds of information or to store a passwordso that you do not have to input it every time you visit a web site. most cookies last onlythrough a single visit to a web site. users can set up their web browser to inform themwhen cookies are set or to prevent cookies from being set.11this type of service relies on collaborative filtering technology, which guides peopleõschoices of what to read, view, purchase, etc. based on information gathered from otherpeople, such as other customers with similar preferences or purchasing patterns.the internet's coming of agecopyright national academy of sciences. all rights reserved.184the internetõs coming of ageknowledge or consent (either implied or by optout or optin decisions12).for example, a web site can store personal information on the userõscomputer as cookies or as hidden fields in urls and forms that are accessible by that or other web sites.not only can personal information be collected by all of the organizations and businesses that people interact with on the internet, but it canalso be collected by the internet service providers themselves. one suchexample is that isps can and do record information on user actions (forinternal purposes or to comply with a court order). such informationcould include which dns names are looked up by a particular customer,which web sites are visited by a customer at what times, and how muchinformation is transferred. though the technical capability exists, ispsmay or may not regularly gather such information. considerations include the potential for alienating their customers and the performancedegradation that could result from extensive monitoring. although it isbecoming more widely known that service providers can collect personalinformation, many users are still unaware of this possibility and its implications.another privacy concern relates to internet infrastructure databases.information can be captured from user email addresses or directoriesmade public by internet service providers. for example, records of domain name registrations and address allocations have traditionally beenavailable to the public to permit users in other domains to track downproblems and get assistance in resolving them. now, however, thesedatabases are being captured and used for targeted marketing purposes,which has led to calls for not making the data public. this echoes recentlitigation over whether the customer proprietary network information(cpni) collected by telephone companies, which includes the duration,frequency, and location of calls, may be given or sold to telemarketerswithout the explicit permission of customers.13 similarly, when government information that is in principle public but in practice hard to ac12in recognition of both the positive and negative aspects of collecting personal information, privacy experts distinguish between optin and optout approaches to managing theuse of personal information. the first requires that the individual specifically authorize theuse to which the information is put, while the second requires only that individuals havethe option to state that they do not wish the information to be used in a particular manner.13in august 1999, the u.s. tenth circuit court of appeals issued a ruling vacating thefccõs cpni rules. (u s west, inc. v. fcc, 10th circuit no. 989518, filed august 18, 1999),holding that the fccõs cpni rules òmust fall under the first amendment.ó the tenthcircuit courtõs mandate has not yet been issued. according to the fcc, further litigation ispossible. (see fcc. 1999. common carrier bureauõs homepage for the cnpi proceeding.common carrier bureau, fcc, september 9. available online at <http://www.fcc.gov/ccb/ppp/cpni/welcome.html>).the internet's coming of agecopyright national academy of sciences. all rights reserved.implications for broad public policy185cessñsuch as property tax or motor vehicle recordsñis made readilyavailable over the internet, this may be viewed as a violation of privacy.14òonline privacyó is generally understood to refer to information collected via email, chat applications, user interactions with web sites, andthe like. the likely proliferation of networked appliances, sensors, andother embedded systems, which was discussed in chapter 2, introducesnew modes of information collection and new issues with respect to individual privacy on the internet. networked embedded devices are expected to become a pervasive technology because of the powerful instrumentation that can be achieved by placing sophisticated but lowcostsensor/actuators within physical environments.15 much as other networked resources have been used in novel and unexpected ways, it is alsoreasonable to foresee that networked devices will be used in ways thatsurpass the original intended uses of the collected data. some of thesewill raise new privacy concerns and trigger debates similar to those surrounding online privacy. the same technologies that allow trackingpeople for legitimate purposes can also be used to monitor their activitiesinvasively. this sort of debate has already arisen in the context of therecent federal communications commission mandate that cellular telephone operators provide the means to determine much more preciselythe position of callers when they place 911 (emergency) calls.these concerns are sure to grow in importance and attention as thesedevices are more and more widely deployed, as they almost surely willbe. as a starting point, it appears reasonable to apply the same basicprinciples that have been applied to personal information to informationthat is passively collected by networked devices. that is, individualsshould be informed that information about them is being collected andfor what purpose, and they should be given the opportunity to view thatinformation and make corrections. there are also issues of whether explicit consent must be obtained (both for initial use and any subsequentuses). for consent to be meaningful and informed, it must be solicited ina carefully stipulated manner, and the individual must be given recourse.it is unclear, thus far, to what extent voluntary actions are addressingthese privacy concerns. the most visible indicator may be statements ofprivacy policiesñverbal disclosures about what information is collectedand how it is used. on the positive side, a study by mary culnan of themcdonough school of business at georgetown university found that14a complementary situation where government seeks to capitalize on the broad reach ofthe internet is the posting of information about individuals who have violated certain laws.15a separate report from cstb on these technologies is anticipated in 2001.the internet's coming of agecopyright national academy of sciences. all rights reserved.186the internetõs coming of agenearly twothirds (65.9 percent) of commercial web sites that collect personal information post some sort of privacy disclosure.16 however, thesame survey also indicates that only about 14 percent (and fewer than 10percent of sites that collected personal information) provided privacydisclosure statements that addressed all four basic privacy elements (i.e.,awareness, choice, data security, and customer access; for definitions seelater in this chapter) and offered contact information to consumers withquestions about the firmõs privacy policies. a contemporaneous forresterresearch brief17 echoes this point, stating that ò90 percent of sites fail tocomply with the basic four privacy principles,ó and regular assessmentsby the federal trade commission and privacy advocates raise questionsabout the willingness and ability of organizations to undertake this comparatively simple measure.technical approaches to protecting privacyjust as internet technology can accelerate and complicate the loss ofprivacy on the internet, it can also protect that same privacy. for instance, as a countermeasure to the hidden gathering of information viacookies, web browsers now can be set to deny loading of this personalinformation or to let users approve them on a casebycase basis, andother software tools are available to help users manage these cookies.web users are likely to find this a complex and tedious process, however,and some sites may not function with cookies disabled. as a consequence, new technologies are being developed to automate negotiationsover privacy between users and the web sites they wish to reach and tocontrol the gathering of information based on these negotiations. all ofthe privacyenhancement mechanisms are controversial; each embodies aparticular set of features and tradeoffs. some have attracted many supporters, but there is thus far no consensus on the best mechanism. this isnot surprising, because the technology is still evolving (through experimentation), as are privacy policies and procedures and attitudes aboutthe mix of technical and nontechnical approaches.one new technological approach is the platform for privacy preferences (p3p), which is being developed by the world wide web consor16mary j. culnan. 1999. georgetown internet privacy policy survey: report to the federaltrade commission. washington, d.c.: mcdonough school of business, georgetown university, june. available online at <http://www.msb.edu/faculty/culnanm/gipps/mmrpt.pdf>.17paul r. hagen. 1999. privacy wakeup call. cambridge, mass.: forrester research inc.,september 1. available online at <http://www.forrester.com/er/research/brief/excerpt/0,1317,7803,ff.html>.the internet's coming of agecopyright national academy of sciences. all rights reserved.implications for broad public policy187tium.18 p3p helps the user screen information requests and gives the usercontrol over the delivery of requested information, including negotiationof privacy terms between the user and the service provider. it operates asa kind of digital analog to caller id and caller id blocking, whereby ananswering party wishes to know who is phoning but may be denied thisinformation if the calling party blocks the request. p3p increases theexplicitness with which privacy policies are expressed, allowing the userand the service provider to specify the terms of use for each data itemñi.e., how and for what purpose the information will be used and withwhom it will be shared. p3p has the appeal of automationñit can diminish the need for ongoing monitoring and intervention by internet usersñbut it requires significant setup effort. to specify privacy preferencesdown to each data element, a user may have to set 100 or more parameters; alternatively, he or she can rely on a program that maps/infersthese parameters from a smaller set of higher level preferences (or throughlearning user preferences by observing behavior). the system can alsosimply work with default settings that can be overridden by the user.further complicating use of this technology is that user preferences maychange frequently, making it hard to track what was agreed to for eachdata exchange. it is also an approach that requires multilateral actionsñthe installation and use of appropriate softwareñby both providers anduser. p3p has already been valuable for its contribution to the debateabout online policy. as a sophisticated technical mechanism, it showshow the technology needs to be meshed with practice and procedure byindividuals and organizations and illuminates some of the tradeoffs involved in protecting online privacy.policy and regulatory approaches to privacy protectionthe legal and regulatory environment surrounding privacy protection on the internet remains quite mixed and uncertain. the united stateshas generally dealt with privacy sector by sector, and policy has generallyfavored industry selfregulation or other nongovernmental solutions suchas the technical approaches described above, although the governmenthas articulated the fair information practices described above. many firmsparticipate in industry selfregulatory efforts, and many provide custom18world wide web consortium (w3c). 2000. the platform for privacy preferences 1.0(p3p1.0) specification, w3c working draft 10 may 2000. cambridge, mass.: w3c. available online at <http://www.w3.org/tr/p3p/>. see also joseph reagle and lorrie faithcranor. 1999. òthe platform for privacy preferences.ó communications of the acm 42(2):4855.the internet's coming of agecopyright national academy of sciences. all rights reserved.188the internetõs coming of ageers with privacy guarantees that go beyond legislative or regulatory requirements, such as optin policies, which require explicit customer consent before sharing information. under the optin approach, it is left toindividuals to make their own judgments about which commercial firms(and other individuals) can be trusted to use their personal informationwisely. in the case of commercial service providers, trust can be engendered and cultivated through a number of factors, including the firmõsbrand and reputation; the customerõs experience and relationship withthe service provider; referrals and testimony by third parties; the serviceproviderõs stated policies and guarantees, backed by appropriate recourse;thirdparty seals of approval that the service providerõs policies are acceptable; industry selfregulation; and enforceable contracts, laws, andregulations. there have been a number of exceptions to the prevailingselfregulation approach. federal and state legislation has been passed toincrease the privacy of individual records in certain sectors such as healthcare, credit reporting,19 cable television, and video rentals, and legislationwas passed aimed at protecting the privacy of children online (thechildrenõs online privacy protection act). ongoing debate over sectorspecific protections attests to the importance attached to the protection ofcertain kinds of personal information; the prospects for generalizing thatimportance remain uncertain, however, although a number of crosssectoral issues are getting more attention (e.g., the privacy of employeesvis‹vis their employers). the appointment in the late 1990s of a chiefcounselor for privacy at the office of management and budget has atleast symbolic value and may be the beginning of a more comprehensiveconsideration of protecting information about individuals collected bythe government as well as the private sector.some other countries have acted more broadly or directly to protectprivacy. in particular, the european union (eu) has adopted a more19a set of principles resembling those adopted for privacy protection is contained in thefair credit reporting act (fair credit reporting act (fcra), 15 u.s.c. ¤ 1681 et seq.) asamended by the consumer credit reporting reform act of 1996 (public law 104208, theomnibus consolidated appropriations act for fiscal year 1997, title ii, subtitle d, chapter1), section 311 of the intelligence authorization for fiscal year 1998 (public law 105107),and the consumer reporting employment clarification act of 1998 (public law 105347);see the discussion and full legislative text, available online at <http://www.ftc.gov/os/statutes/fcra.htm>). the fcra requires consumer credit reporting agencies to informindividuals when information is being collected about them and for what purpose, andindividuals have the right to see this information and to correct it if it is inaccurate. moreover, credit issuers must let customers know they may opt out of information sharing,which includes both using the data internally for crossmarketing and selling the data tothird parties.the internet's coming of agecopyright national academy of sciences. all rights reserved.implications for broad public policy189inclusive, governmentled regulatory framework for privacy protection.the eu directive on data protection, which took effect in october 1998,permits eu members to block the transfer of personal information abouteu citizens to other countries that do not offer adequate protection ofprivacy. to pass the threshold, organizations must fulfill a number ofrequirements: among other things, they must tell individuals when theycollect information about them, disclose how it will be used, and provideindividual access to the information. individuals must provide informedconsent before a company or other organization can legally use the data.thus, the eu directive mandates optin usage policies rather than the optout policies common in the united states, japan, and many other countries. each eu member nation must enact its own laws to implement thedirective. the united states is one of the nations whose privacy protections have not met the eu threshold.20 as a consequence, the unitedstates and the european commission (ec) of the eu have negotiatedguidelines that would serve as a òsafe harboró for u.s. companies wanting to receive information from the eu.21 the pressures to harmonizeprivacy policies on an international basis are a good example of the effectthe internetõs global reach is having on national policies.2220for example, the optout policies that are used by many u.s. firms, whereby customersmust request that their personal information not be used or disclosed to others, do not meetthe eu threshold of adequate privacy protection. u.s. firms also cite cases in which theyuse personal information obtained from others (e.g., mailing lists) and do not retain it, sothat it would be impractical for them to give customers unconditional access to the information.21the safe harbor guidelines state that u.s. organizations must inform individuals whythey are collecting personal information, with whom they will share that information, andhow individuals can limit its use and disclosure. organizations must also offer individualsaccess to the information, as well as the opportunity to choose whether and how the personal information they provide is used or disclosed to third parties. in addition, organizations must take òreasonable measures to assure its [data] reliability for its intended use andreasonable precautions to protect it from loss, misuse, and unauthorized access, disclosure,alteration and destruction.ó the agreement also states that eu member states will be boundby the agreement; that it will be presumed that companies within the safe harbor providedata protection; that data flows to those companies will continue; that, generally, only theec will be able to interrupt personal data flows; and that u.s. companies will have a graceperiod in which to implement the policies. see electronic commerce task force. 2000.òsafe harbor privacy principles.ó washington, d.c.: electronic commerce task force, international trade administration, u.s. department of commerce, draft of june 9. available online at <http://www.ita.doc.gov/td/ecom/menu.html>.22the recent activity associated with the eu privacy directive builds on a history of concern for transborder data flow that preceded the commercialization of the internet. it buildson different national traditions relating to social policy and rights and responsibilities inthe internet's coming of agecopyright national academy of sciences. all rights reserved.190the internetõs coming of agea very different approach to privacy protection, advocated by somescholars more than 30 years ago, would have governments enact legislation giving individuals explicit property rights to their personal information.23 individuals would then be in a position to bargain with organizations over the price and other terms for using their personal information;and they could legally enforce such agreements if violated. some firms(e.g., microsoft and privaseek) are investigating how they might profitfrom commercially implementing such a system. proponents of this approach argue that, at least on the internet, standardized agreements forthe use of personal information, software technologies such as p3p, andagents acting for individuals could reduce transaction costs to a workablelevelñassuming sufficient ease of implementation. privacy could thenbecome a matter of consumer choice, backed by normal commercial andconsumer protection laws, rather than a difficult and often intractablepolitical issue. opponents contend, among other arguments, that implementing individual property rights to personal information would beunworkable; that it would unnecessarily impede the development of electronic commerce; that those people most in need of privacy protectionwould be the least able to negotiate with large organizations; and that inany case, society should not let individuals bargain away their fundamental rights to privacy.24anonymityidentification for the purpose of granting access to systems and information is a basic function of computer systems, and it has been an objective of mechanisms and procedures put in place by managers of largecomputer systems for decades. at the same time, technologies are beingdeveloped that make it difficult or impossible to identify the origin ofcomputermediated communications.general, and it also builds on the competitive posturing of nations. the euõs actions, forexample, reflect concerns about privacy per se as well as european recognition of the competitive impact of u.s.owned businesses that interact with europeans.23alan westin. 1967. privacy and freedom. new york: atheneum, pp. 324325; arthur r.miller. 1969. òpersonal privacy in the computer age: the challenge of new technologyin an informationoriented society,ó michigan law review 67(april):12241225.24some argue that the òfundamental asymmetry between individuals and bureaucraticorganizations all but guarantees the failure of the market for personal information.ó oscarh. gandy, jr. 1996. òlegitimate business interests: no end in sight? an inquiry into thestatus of privacy in cyberspace.ó university of chicago legal forum 1996:77137.the internet's coming of agecopyright national academy of sciences. all rights reserved.implications for broad public policy191individuals have a variety of reasons for wishing to be anonymous. 25anonymity is of particular importance for some types of political speech26and in other instances, such as when reporting certain kinds of incidentsto the police or regulatory authorities. more generally, some people prefer to protect their privacy when they communicate or engage in commercial or other transactions in general or under certain circumstances.27 acounterweight to individual interests in anonymity is the demand byindividuals, organizations, and society in general that other individualsand organizations be accountable for their actions, often to protect againstfraud and illicit or improper actions. identity can be important if a commercial dispute arises, if a crime or tort is suspected, or for taxation andother legitimate government purposes. anonymity is perceived as undesirable when it becomes an enabler of such activities as libel, distributingpornography to minors, or engaging in money laundering. the internetamplifies these conflicting needs by simultaneously making it easier totrack and monitor individuals (sometimes invisibly) and making it easierfor people to act anonymously. for example, web sites often allow orencourage users to adopt anonymous or pseudonymous identities whenparticipating in chat rooms and other public meeting places. email usersare free to take on pseudonymous identities (e.g., johndoe@example.com,guesswho@example.com) when using many email services. some believe that the ease of assuming an anonymous identity on the internetencourages unethical or illegal activities (such as spamming, harassment,25for a recent examination of the role of anonymity online, see al teich, mark s. frankel,rob kling, and yaching lee. 1999. òanonymous communication policies for the internet:results and recommendations of the aaas conference.ó the information society 15(2).26see, for example, mcintyre v. ohio elections commission, 514 u.s. 334 (1995):. . . the interest in having anonymous works enter the marketplace of ideasunquestionably outweighs any public interest in requiring disclosure as a condition of entry. accordingly, an authorõs decision to remain anonymous, like otherdecisions concerning omissions or additions to the content of a publication, is anaspect of the freedom of speech protected by the first amendment.under our constitution, anonymous pamphleteering is not a pernicious,fraudulent practice, but an honorable tradition of advocacy and of dissent. . . .the state may, and does, punish fraud directly. but it cannot seek to punishfraud indirectly by indiscriminately outlawing a category of speech, based on itscontent, with no necessary relationship to the danger sought to be prevented.27english and american law also recognize the legitimacy of cash transactions whereeither the buyer or seller, or both, prefer not to be identified. however, the right to anonymity in commercial transactions is not absoluteñlimits are imposed in the united states ininstances such as hand gun purchases or, as a countermeasure to money laundering, cashdeposits exceeding $10,000.the internet's coming of agecopyright national academy of sciences. all rights reserved.192the internetõs coming of agedefamation, pirating of music or software, stalking, or exchanging childpornography) and thus poses additional risks to society that requiregreater government surveillance and action.28 one area of particular concern because of its potential to damage an innocent third party withouthis knowledge or consent is masquerading, in which an anonymous entity transmits a libelous message or conducts a transaction that appears asif it came from another individual.29the debate over anonymity on the internet epitomizes the challengeposed by the sophisticated, complex, and poorly understood technologythat underpins the internet. people tend to act based on what they seeand understand, and it appears that unless they have specialized technical knowledge, people may have false expectations of anonymity. serviceproviders cooperate with government authorities that are investigatingsuspected criminal activity and may choose to cooperate with efforts tocontrol other forms of undesirable behavior online. employers may monitor online conduct, a practice that is reportedly growing, with unevennotification of employees. individuals seeking anonymity may use services that provide less of that quality than they think, just as they mayadopt information security measures such as encryption or firewalls that,because of their design or implementation details, are less effective thanthey think. given that concerted efforts to identify an individual willonly be made in particular circumstances, imperfect protections of anonymity may well have a silver lining. with experience, users may becomemore aware of what does and does not provide them the anonymity theydesire, but this awareness may only come through bad experiences.technology can significantly increase anonymity. email senders cantake measures to further hide their identities by using anonymousremailers, which strip off header information about the sender beforeforwarding the message to its destination.30 more sophisticated services28this point was echoed in the report of a government task force looking at unlawfulonline conduct. (see presidentõs working group on lawful conduct on the internet. 2000.òthe electronic frontier: the challenge of unlawful conduct involving the use of theinternet: a report of the presidentõs working group on unlawful conduct on the internet.ómarch. available online from <http://www.usdoj.gov/criminal/cybercrime/unlawful.htm>.)29this concern should be distinguished from issues related to anonymity in general. thenames used for anonymous communications could, for example, be restricted to a specialset reserved for just that purpose and thus not useful for masquerading; this is the approach taken in rfc 1422, which defines an approach for privacyenhanced internet email.(s. kent. 1993. privacy enhancement for internet electronic mail: part ii: certificatebased keymanagement, rfc 1422. networking working group, internet engineering task force.available online at <http://www.ietf.org/rfc/rfc1422.txt>.)30a seminal paper on which practical designs have built is david l. chaum. 1981. òuntraceable electronic mail, return addresses, and digital pseudonyms.ó communications ofthe acm 24(2).the internet's coming of agecopyright national academy of sciences. all rights reserved.implications for broad public policy193make use of encryption and transmission through a series of anonymousremailers (perhaps located in several different countries), which maketracing the message very difficult even if some of the remailers are compromised.31 similar approaches can be used to anonymize web transactions and the transmission of ip traffic across the internet. these haveprogressed in status from being the subject of academic research to commercial deployment; zero knowledge systems, for example, offers a commercial service called freedomnet, which is designed to block the tracingof ip traffic back to its source and to provide tools that control otheridentifiers such as cookies.32meanwhile, email software and webbased email services could allow users to filter out anonymous messages if they do not want to receivethemñcapabilities analogous to not accepting phone calls from callerswho have disabled caller identificationñif an infrastructure is in place toascertain the identity of senders. while they are imperfect solutions andaddress only one class of problem associated with anonymity, such capabilities show that technology can respond to some of the concerns raisedby online anonymity.the legal status of anonymity on the internet remains contentiousand unresolved. anonymity on the internet has positive as well as negative social value, suggesting that a blanket prohibition is unlikely. it is notan absolute right in all circumstances, however, and society generallyexpects that individuals be held accountable for harmful or illegal actions,whether or not under the cloak of anonymity. thus, abuses of anonymityundoubtedly will bring political pressures to shut down anonymous services or impose close monitoring or registration requirements on them.existing laws and regulations that govern the behaviors and actions ofconcern may or may not be deemed adequate to cope with online anonymous behaviors.some recent legislative proposals would make certain anonymousactions illegal, holding providers of anonymous email services, as well as31for a good discussion of current battles to preserve or defeat anonymity, see davidmazieres and m. frans kaashoek. 1998. òthe design, implementation and operation of anemail pseudonym server.ó proceedings of the 5th acm conference on computer and communications security. available online at <http://www.lcs.mit.edu/impact/perspect/9901.pdf>.32much of the design is traceable to research on onion routing. see p. syverson, m. reed,and d. goldschlag. 1997. òprivate web browsing.ó journal of computer security 5(3):237248; michael g. reed, paul f. syverson, and david m. goldschlag. 1998. òanonymousconnections and onion routing,ó ieee journal on selected areas in communication specialissue on copyright and privacy protection; and david m. goldschlag, michael g. reed, andpaul f. syverson, òonion routing for anonymous and private internet connections,ó communications of the acm 42(2).the internet's coming of agecopyright national academy of sciences. all rights reserved.194the internetõs coming of agemessage originators, responsible for message contents or even prohibiting anonymous messages altogether. anonymous online conduct hasalso been the subject of international discussions on how to address aperceived rise in òcyber crime.ó law enforcement agencies, in particular,seek ways to pierce the veil of anonymous remailers when investigatingsuspected criminal activities. such efforts are constrained by the globalnature of the internet; anonymizers can be located in other countries andmessages can be passed through servers in multiple countries. somepeople, in the interest of combating child pornography, drug trafficking,and other crimes, would prohibit anonymous email altogether. otherspoint to the value attached to anonymity in u.s. legal tradition, such as a1995 u.s. supreme court decision that upheld an individualõs right tosend anonymous political leaflets,33 and argue that the same principlesshould apply to internet communications as well.an interesting, unresolved question is the extent to which specificsolutions might alleviate the need for wholesale use of the more generalanonymity services discussed here. for example, some of the need foranonymity would go away if satisfactory solutions were found for preserving individual privacy. and some other needs for anonymity, suchas those occasioned by political expression or whistleblowing withoutfear of reprisal, might be accommodated by special anonymous forums(e.g., an anonymous posting site) established for those particular purposes.importantly, there are steps that the various interested parties cantake to identify ways of resolving tensions associated with anonymousonline communication. for instance, email service providers, remailers,web sites, and online communities can develop, publicize, and implement specific policies about appropriate and inappropriate use of anonymous or pseudonymous communications. industry and user groups canwork together to develop standard guidelines for such policies. anotheruseful step, in line with the practices of some services today, would be forsuch policies to clearly state the situations in which user identities may bedisclosed to others.identityissues of privacy and anonymity ride on top of how we establish,manage, and even understand identity against a background of globalnetworks and powerful databases. controlling information about oneõs33mcintyre v. ohio elections commission, 514 u.s. 334 (1995).the internet's coming of agecopyright national academy of sciences. all rights reserved.implications for broad public policy195identity and the links to data about that identity is one way that we canachieve privacy. anonymity can be thought of as one extreme of identification, namely, the absence of any personal identification. in this section,the committee presents some basic principles of òdigital identityó thatmay help reconcile the conflicts surrounding information disclosure, privacy, and accountability on the internet. it hopes these principles willserve as a useful framework for future policy in the privacy area as well asfor finding and implementing technical solutions in support of the various privacy requirements resulting from industry selfregulation and government legal and regulatory actions.individual identity is a complex concept. each of us has differentidentities, with different roles and attributes, in different situations and atdifferent times. a person can simultaneously be a parent, spouse, employee, consumer, auction buyer or seller, patient, and member of variousorganizations. people often try to keep these different identities separate.they may do this by providing more, less, or simply different informationto different persons, government organizations, or commercial serviceproviders. to the extent that people are able to do so, they make itdifficult for third parties to link the actions and activities of their differentidentities and conclude they belong to a single individual.34 the nature ofinternet interactions and the explosive growth of electronic communities,sliced across a number of different geographic and demographic factors,amplify this need greatly and mean that more information is availablefrom more sources and to more parties in a context that allows collectingpieces of information from disparate sources and matching them by technical means.while the internet indeed poses serious challenges related to identity,it also offers elegant and powerful tools for balancing individual andsocietal needs for privacy, anonymity, and accountability. examples ofthe tools emerging include tools that allow the individual to manage and34this is significantly easier to do in the united states, where the absence of a mandatorynational identifier makes it easier for an individual to maintain separate identities thatcannot be easily associated with one another. a number of other countries have measuresin place that embrace a single, unique identifier for each person (e.g., a national id card),and some are moving to implement this approach electronically (e.g., the òqualified certificateó concept expressed in a recent european union directive on public key certificates andwork in the ietfõs publickey infrastructure (x.509) working group). not only does thisrun counter to u.s. practice and philosophy, it represents a more simplistic view of identitythan the discussion here would argue for and enables the easy linking of oneõs variousactivities with a single individual profile. the widespread use of such common identifyinginformation as the social security number does, however, limit the extent to which one canseparate identities.the internet's coming of agecopyright national academy of sciences. all rights reserved.196the internetõs coming of agecontrol òmultiple identities,ó using one or more certifying authorities tovalidate various attributes where required. these tools also help theindividual track and monitor what information is being collected andhow this information is actually used. a caution, however, is that suchtools depend in many cases on a foundation of effective security mechanisms and practices, including authentication capabilities that are notwidely available (the exceptions being such instances as a merchant authenticating a customerõs credit card account or two parties with priorrelationship authenticating each other).it is possible for individuals to have multiple digital identities on theinternet for use in different contexts and situations. personal informationñsuch as name, school, and job affiliations, home and business addresses, telephone and fax numbers, email addresses, driversõ licenses,passports, credit cards, and other identifying recordsñcan be stored andmade selectively available on the internet by the user, much as a personnow physically takes out various cards from his or her wallet for differentpurposes. today, users who make use of multiple services are likely tohave established what effectively amounts to a portfolio of identities inwhich varying amounts of personal information have been provided, depending on the nature of the transaction, the information requested bythe other party, and the type of information the user has chosen to provide. in many cases these different identities can be linked using eitherinformation provided by the user (e.g., name and address matching) orinformation gained from cookies that different web content and serviceproviders have placed on user machines. but in many other cases, correctidentity information is needed for accountability. when a firm gives anemployee an email address (johndoe@example.com) it in effect certifiesthat the person is affiliated with that company. cookies on usersõ computers often contain a mixture of selfreported information and information certified by a provider. certification authorities exist to authenticatecharacteristics of identity (such as age, location, or ability to pay) that arenecessary to engage in certain activities or complete certain transactions.35it is also possible to provide mechanisms that allow users to createdigital identities that provide explicit control over what personal information is disclosed under what circumstances. discussion groups andchat rooms, like email, often tolerate if not encourage pseudonyms, andsome online interactions, such as multiuser domains, encourage creationof artificial, even fantastic, identities in the spirit of play. even when atransaction needs to be authenticated, a userõs name does not need to be35for a discussion of this approach to public key infrastructure, see s. kent. 1997. òhowmany certification authorities are enough.ó proceedings of milcom 97 (unclassified papers) 1 (november): 6168.the internet's coming of agecopyright national academy of sciences. all rights reserved.implications for broad public policy197disclosed; in a purchase transaction, it may be sufficient for a vendor to beable to associate an identity with a certification that the user has a validcredit card (or some other indication of ability to pay, such as an accountat one of the new electronic money services).36 for example, the user canchoose to send only a reference to the needed information and can useencryption or other authentication tools to make sure that only the intended party receives the information and that the receiving party is whoor what it represents it is. the user is then able to give general instructions to his or her digital identity agent about what information to releasefor which activities under what safeguards, or he or she can personallyapprove each use of data or each transaction. such tools can providedistinct levels of digital identity on the internet; these, in turn, can varywith the kind or degree of interaction and according to the stringency orauthority with which the information about identity is established or attested to, ranging from the selfidentified to private certification to government certification of identity.a logical complement to relying on institutions with which one hasestablished some relationship, such as a creditcard issuer, for identifyingand certifying information is thirdparty repositories and/or certifiers.one approach may be usercontrolled: the information making up oneõsdigital identity can be stored in wellknown places or managed by agentsaccessible to authorized parties on the internet. users of such a toolwould be able to manage all components of their digital identity exceptfor those that require certification, which will be controlled by the certifying entities. when organizations doing business on the internet set outtheir privacy policies on their web sites, users can instruct their digitalidentity agent to negotiate with the web sites about release of personaldata. this sort of automated process is, of course, only as good as thesystem and the userõs choices. as described above, the p3p technologyunder development by the world wide web consortium is one approachto facilitating the flow of necessary identifying information while stillprotecting individual privacy as defined by the user. an alternative isthat, seeking greater convenience, people will choose not to maintain ahigh level of control over such information, electing instead to let thirdparties manage it for them. for example, users might use services that areless complex and less robust (e.g., there is no use of certifiers), relinquishing direct control over the use of their information. examples of thirdparty systems now in the marketplace include microsoftõs passport andprivaseek, online services that provide such features as password man36this is the approach used in the set (secure electronic transactions) specification forpayment card transactions over the internet. see <http://www.setco.org/setspecifications. html>.the internet's coming of agecopyright national academy of sciences. all rights reserved.198the internetõs coming of ageagement and automatic completion of online forms based on saved information. other services, such as yodlee.com and verticalone work onbehalf of the user by gathering the private information that users provideto diverse web sites and aggregating them to present the user with asingle, integrated view. these services are new, and the merits and consumer acceptance of approaches ranging from tight user control to delegation of decisions to third parties remain to be proven. the prospect ofthirdparty involvement in identity management begs the question ofòidentity portabilityóñthe ability to easily switch between identity agentservices in an analogous fashion to number portability among telephonecarriersñmeaning in this context that peopleõs selected identities are nottied to a particular site or service.the emergence of technical approaches that allow individuals to manage their online identities is a positive development. such approachesdepend on government and business cooperation. for example, responsible service providers will clearly state their privacy policies in digitalform so that an individualõs identity agent can readily determine whetheror not to use the site. privacy advocates argue that, in general, the amountof information required and revealed should be minimal for the purpose.it is also likely that we will see the emergence of tools and services thatallow users to identify and document when privacy promises are violated; such tools and services may be another alternative to regulation asa remedy. if digital identity is to become widely accepted, there will alsoneed to be a legal framework for behaviors with respect to identity technologies: who, for instance, would be expected to decide who controlswhat about identity, and under what circumstances would they make thisdecision? how would conflicts be resolved?some basic principles for such a framework include the following:¥the information associated with each identity should be under theuserõs control.¥the holder of a digital identity should not be compelled to revealelements of that identity against his or her will except to the extent andunder the circumstances dictated by law for a similar transaction offline.¥an identity holder should be allowed to modify its elements asneeded.¥the theft or unauthorized modification of the elements of someoneelseõs digital identity should be subject to appropriate penalties undereither civil or criminal law.¥governmentcreated or certified identities need not be the default,although it may be necessary for governments to recognize certain certifications as authoritative for official purposes. such official certificationsexist today, such as passports and driversõ licenses; these are issued forthe internet's coming of agecopyright national academy of sciences. all rights reserved.implications for broad public policy199particular purposes and have specific regulatory criteria associated withthem.authentication on the internetauthenticationñthe process of establishing that a particular claimabout an entityõs (e.g., a personõs or organizationõs) identity is, in fact,validñis needed for many transactions, including electronic buying andselling, voting, access to health records, and so forth, and it is an essentialelement of the processes that bear on privacy and anonymity. but identity authentication is a complex issue, not least owing to the multifacetednature of identity. for example, ecommerce may not necessarily requireverifying the identity of an individual but may instead require verifyingwhether an individual possesses certain properties relevant to a purchase(e.g., whether the individual is above a certain age, is authorized to makepurchases using a particular payment mechanism, or is trusted to maintain privacy/confidentiality). a related additional service, nonrepudiation, allows the receiver of a message to not only authenticate the senderbut also be able to prove that the sender in fact originated the message.authentication technology and practice build on a history of work incomputer and communications security, including experience with peoplewho have sought unauthorized access to information and systems, sometimes by hiding or misrepresenting who they are. because perfect security is a chimera, ecommerce security, like security in other domains, isgenerally thought of as a risk management process, whereby stronger andmore costly technologies are introduced in response to the dynamics andmagnitude of the risk. issues such as buyer authentication and recoursefor nonpayment arise in other commercial transactions, just as inecommerce, and are dealt with in those contexts through a combinationof technology, business practices, and sound risk management principles.therefore the processes that are required to implement buyer protection,transaction enforceability, and dispute resolution for ecommerce can relyprimarily on existing business risk management and legal frameworks.however, we do not yet have a large enough base of experience in typesof risk and the legal and regulatory challenges a business faces whenproviding these authentication services over the internet to fully understand what changes in business risk management and legal frameworkswill be necessary, including clearly establishing the liabilities of authentication providers.support for authentication over the internet is fragmented today.there is no standardized, widely accessible, cheap, and easy approachñin short, there is no single besttechnology solution. multiple technologies for authenticating the identity or related attributes of individuals andthe internet's coming of agecopyright national academy of sciences. all rights reserved.200the internetõs coming of ageorganizations are in use and/or development. these range from simplesolutions that make use of exchanging shared secrets (e.g., passwords)over secured communications links to more sophisticated systems thatrely on the exchange of credentials or certificates that attest to the authenticity of an individualõs identity or other attributes. user authenticationmechanisms can be characterized as relying on something an individualknows (such as a password), something an individual has, and/or something an individual is (an innate biological property). none of these iswithout shortcomings.to illustrate, a simple authentication mechanism is to challenge theother party with questions about a shared secret, such as a password, thatonly the intended recipient should know the correct answer to. this iswhat is generally used today in transactions with consumers over theinternet, typically in conjunction with secure sockets layerbased (ssl)encryption that makes it difficult for eavesdroppers to steal the consumerõs password by monitoring network communications. shared secret mechanisms such as passwords have a number of weaknesses; forinstance, passwords chosen by users are often relatively easy to guess.the mechanisms can be strengthened by making the secrets dynamic,such as by using information contained in recent transactions or communications (e.g., asking the customer what the amount of the first purchaselisted on his most recent statement is). because users are likely to makeuse of multiple services, there are easeofuse issues when they have tomanage multiple passwordsñpotentially a separate password for eachinternet account or service they use. such considerations have led tocommercial interest in services that offer to manage these passwords forconsumers; several of these were discussed above in the section on identity. and, more fundamentally, mechanisms that rely on shared secretsrequire that a relationship has already been established between the parties. the exception would be when the two parties do not know eachother but do know and trust a third, neutral party who can use sharedsecrets with each to authenticate and vouch for each party on behalf of theother. note, however, that thirdparty verification of individual identityis only as good as the verification conducted by the certifying authorityñfor example, how did it verify the identity of the individual? how muchinformation has it certified?37 what guarantees and recourse does it provide if the certified information proves false?3837for example, if the certifying authority is not the same entity as the individualõs agentin that arena (e.g., the credit card issuer) can it responsibly perform that authorization (e.g.,authenticate that the individual is the rightful and authorized user of the card)?38there is an important distinction between authoritative certification authorities andthe internet's coming of agecopyright national academy of sciences. all rights reserved.implications for broad public policy201public key cryptography39 is commonly used to provide authentication and nonrepudiation services. a major challenge in public key cryptography work is distributing the public keys in a secure manner so that,for example, an individual cannot be misled into using the public key ofsomeone who wishes to impersonate that individual by substituting theimpersonatorõs public key for the correct public key. transacting partiesthat have a prior relationship can make use of already exchanged andvalidated public keys to validate the received digital signature. otherwise, each could ask the other party to provide some sort of credential,such as one provided by a trusted third party that attests to the authenticity of their identity or some set of attributes including their public keys.40the term public key infrastructure (pki) is frequently used as a labelfor the technology, processes, and policies that underlie public key management. pki makes used of certificates that allow the identity of users tobe authoritatively associated with their public key. one pki model makesuse of trusted third parties, known as certification authorities, who themselves use public key cryptography to digitally sign certificates that bindthe identity of subscribers to a public key.41 pki is a complex undertakingthat requires the implementation of technical mechanisms, the establishment of procedures for the issuance and revocation of certificates, possibly updating directories, certificate revocation lists, and so forth. thecomplicated nature of certificate management (e.g., registering, revoking,and updating certificates) is one hindrance to pkiõs widespread use.42trusted certification authorities. no one questions, for example, whether a company istrusted to identify its employees, because the company is understood to be the source ofauthoritative information on this subject. in contrast, an arbitrary third party does not havethe same authoritative status.39public key cryptography also provides the basis for more robust encryption and authentication. the technique relies on a public, freely published key that is used to encryptmessages together with another, private key that is used for decryption. importantly, thedecryption key cannot be derived from the encryption key. this allows secret communications where the only one who can decrypt a message is the holder of the private key thatmatches the public key that the message was encrypted in.40much of this discussion is based on the discussion of public key cryptography in computer science and telecommunications board (cstb), national research council. 1998.trust in cyberspace. washington, d.c.: national academy press, pp. 121132.41there are other pki models; for example, pretty good privacy (or pgp) makes use of aòweb of trustó in which any user with a public key can issue a certificate for any other user.42specific difficulties include root and cross certifications, certificate revocation, and certificate management. see computer science and telecommunications board (cstb), national research council. 1998. trust in cyberspace. washington, d.c.: national academypress, pp. 130132. there are also questions as to when certificates need to be exchangedthe internet's coming of agecopyright national academy of sciences. all rights reserved.202the internetõs coming of agelack of compatibility among different implementations is another obstacle. while there are base standards for pki, they are not always followed, in part because some of todayõs systems were designed to be standalone systems. the technology is also still relatively costly, and its useincurs significant performance penalties; thus its use today requires sufficient transaction risk to justify the investment and some inconvenience inuse.biometrics, which is based on measurement of such physical characteristics as fingerprints, hand geometry, or iris shape, is also being usedfor identity authentication. biometrics at first appears ideal because of itsdependence on a physical feature that is unique to a person. however, itsuse of pattern matching carries the risk that an unauthorized user will beaccepted or that an authorized user will be rejected and/or that the identifying information will be misappropriated.43the selection of an authentication technology entails tradeoffs. forexample, exchanging shared secrets has the disadvantage that these secrets must be remembered and can be guessed or stolen, whereas moresophisticated authentication approaches, such as those that rely on pki,often require certificate management, rely on sophisticated software and/or hardware components, and cost more to deploy, along with other drawbacks. biometrics technologies have issues with performance, such asuser acceptance, the impact of compromise, and the risks of false rejectionor acceptance. the use of two or more of these technologies (often calledtwofactor authentication) makes for stronger (harder to spoof) authentication than the use of a single one but generally increases the cost andinconvenience. the ideal combination of authentication technologies fora given class of applications has yet to be agreed upon.another set of tradeoffs must be made between the strength of thecryptographic algorithms required and the risk exposure associated withselecting lower strengths to reduce cost and system performance penalties. in addition, all systems, even those assumed to be strong, are susceptible to vulnerabilities associated with flaws in their implementation.44and whether they can be stored and accessed via directories. for example, directorybasedsolutions have been proposed for simplifying some of the certificate management and public key distribution issues, but this is still relatively new and just emerging and not completely compatible with current implementations. it is also important to distinguish between when one needs certificates and when authentication can take place withoutcertificates (where public keys are previously known or exchanged by other means).43see computer science and telecommunications board (cstb), national research council. 1998. trust in cyberspace. washington, d.c.: national academy press, pp. 123124.44history has also shown that the adequacy of cryptographic algorithms is continuallybeing challenged by inevitable advances in computer processor performance that makescodebreaking easier as well as by advances in codebreaking techniques and growth in thenumber of people who attempt to break cryptosystems.the internet's coming of agecopyright national academy of sciences. all rights reserved.implications for broad public policy203deployed systems may not be as strong as they might at first glanceappear. for example, in many implementations of digital signatures andcertificates for authentication, the digitalsigning software can itself beaccessed through a simple password. also, the strength of softwareonlysystems is tied to the strength of the security mechanisms in the computers on which they run; systems that run on personal computers are subject to all of the wellknown security limitations of these platforms. additionally, the vulnerability of a cryptographic algorithm will change overtime with advances in computing and power cryptoanalysis techniques(and key lengths will be increased in response). specialpurpose hardware and tokenbased mechanisms, which rely on closed hardware devices such as smart cards or pc cards in place of software running on auserõs pc, can offer greater security. the greater costs associated withhardwarebased solutions may, however, make such solutions impractical for many applications. these systems have their vulnerabilities aswell, especially if they lack builtin displays and input devices, and onemust rely on the pc for interaction with the user.45 the relation betweenthese tradeoffs and the various security technology options is poorlyunderstood.it appears likely that, despite their many limitations, shared secrets(e.g., passwords) will continue to be used for consumer authentication forthe foreseeable future and that stronger authentication technologies willfirst begin to be deployed in corporate and commercial applications wherethe larger risk exposure warrants the introduction of stronger, but morecostly and intrusive, authentication approaches and where these approaches can be implemented over a more limited and manageable number of players and covered by contract law. unfortunately, these solutions are likely to be deployed initially as proprietary islands, optimizedto particular applications by competing service providers. this couldforestall the emergence of a widespread standard approach sufficientlyopen and minimalist that interoperability can be enabled without innovation being stifled.two challengesñthe chickenandegg challenge associated with deployment of authentication infrastructures, whereby authentication providers and potential users wait for each other to make the first investments in authentication, and the challenge of increasing the potential45another open question is what the right sort of dedicated authentication hardwarewould be: should one use a smart card, which offers a limited amount of data storage andprocessing power, or should one use a much simpler device such as a radio frequencyidentification (rfid) tag or, alternatively, a more powerful one such as a personal digitalassistant?the internet's coming of agecopyright national academy of sciences. all rights reserved.204the internetõs coming of agedemand from a range of different users and usesñsuggest that widespread deployment awaits one or more first movers making significantinvestments. both private and government actors can play a role in propelling online authentication. authentication is an area where governments can and do intervene: at state, national, and international levelsthere has been support for a legal framework, for technology development, and for the implementation of specific authentication technologiessuch as biometrics and pki.in their efforts to establish an enabling framework for authenticationand nonrepudiation, congress and state legislatures have moved forwardwith attempts to legitimize electronic signatures. most notably, the electronic signatures in global and national commerce act was enacted in2000. and, there are encouraging signs of moves to harmonize state laws,to eliminate the current inconsistencies among them. other laws, such asthe government paperwork elimination act and government programsthat promote electronic transactions, especially over the internet, within agovernment and between a government and its citizens, promote greateruse of authentication. (they also raise new questions about the interaction of authentication, privacy, and anonymity considerations.) effortsare also under way in such international forums as the itu and the european union.various online federal and state government initiatives have beenlaunched that involve the widespread government deployment of authentication and nonrepudiation services. they include initiatives foronline government purchase (by, for example, the general services administration), electronic voting (by, for example, the department of defense), and online electronic tax filing. these initiatives are expected notjust to satisfy government needs but also, it is hoped, to demonstratetechnologies and practices in ways that make discussion of pki less abstract. they could also lead to the transfer of knowledge to private industry and could serve as a critical mass for an authentication infrastructure.but, important as they are, legislative action and government investment alone cannot address the many technical and business issues discussed in this section that are the root causes for the lack of widespreadacceptance of authentication technologies. the complexity of these issuesand the range of relevant stakeholders across all levels of government,industry, and the population at large suggest that an indepth inquiry bya joint industrygovernment advisory panel would be appropriate. thecongress, the department of commerce, and other federal agencies, aswell as state and local agencies, would all benefit from an examination ofthe factors that are impeding the early introduction of authentication services and from an identification of the factors that might accelerate investment in these services andñultimatelyñtheir deployment.the internet's coming of agecopyright national academy of sciences. all rights reserved.implications for broad public policy205taxation of internetbased commercetaxation of commerce conducted over the internet has emerged as acontroversial, highprofile policy issue. unlike other policy issues relatedto ecommerce, taxation is a nonmarket objective for which it may beharder to assume that voluntary action and selfinterest by individualsand organizations can offer adequate solutions. it involves informationand mechanisms that may bear on privacy and anonymity and it is likelyto involve authentication at some point. the taxation issue is not onlyimportant in its own right, it also illustrates a large class of issues that arelocation and identitydependent. ecommerce benefits from its independence of geography, which allows retailers to offer both specialty andmassmarket products to a wide variety of customers without regard towhere they live. in contrast, commerce in the physical world has beenmostly governed by relatively independent, discrete geographic domains,and tax obligations have been enforced based on physical presence. problems arise, however, for governments that seek to impose laws determined by geography, such as state or city sales tax, on ecommerce transactions.46 any resolution of how to tax ecommerce transactions willhave to take into account the inherent difficulties of verifying the location,identity, and residence of a seller or purchaser over the internet. in theunited states alone, considering the intersection of the various levels oflocal, state, and national government, there are a very large number ofseparate geopolitical regions with differing tax structures. presently, local governments in 34 states are authorized to impose local sales taxes;approximately 7,600 jurisdictions have chosen to do so, and this numbercould grow significantly if other local governments choose to exercise thisoption.47 international ecommerce, of course, raises an additional set of46existing u.s. sales tax law treats goods sold over the internet the same way it treatsgoods sold from catalogs using mail or phone orders. a company without a physicalpresence in a state (known as nexus) cannot be required to collect that stateõs sales tax evenif the customer lives in the state. the purchaser is nonetheless responsible for remitting aòuse taxó to his state of residence. for example, if a buyer in boston orders a book from anonline retailer located in washington state that has no physical presence in massachusetts,then massachusetts cannot require the retailer to collect the use tax, even though the purchaser owes this use tax to massachusetts. instead, states must rely on selfreporting andpayment by the customers. compliance is, of course, harder to ensure with a use tax paidby the individual purchaser than it is with a sales tax collected by the merchant. see, forexample, austan goolsbee and jonathan zittrain. 1999. òevaluating the costs and benefitsof taxing internet commerce.ó national tax journal 52(3):413428.47national tax association (nta). 1999. communications and electronic commerce taxproject: final report. washington, d.c.: nta, september 7, p. 12. available online at <http://www.ntanet.org/ecommerce/finalreportcover.htm>.the internet's coming of agecopyright national academy of sciences. all rights reserved.206the internetõs coming of agecomplications associated with the different tax rates and rules. there aredifferent tax structures (e.g., national or state/regional) and different approaches to taxation (e.g., a sales tax versus a valueadded tax48). potentially taxable transactions can cross even national borders without therebeing any traceability other than that contained in transaction audit trailson vendorsõ systems. additionally, the internet environment has beenconducive to creating new forms of value, both nonmonetary (e.g., onlinebarter exchanges) and monetary (new forms of currency such as flooz,beenz, and rocketcash), that portend further strains on tax structures atleast until they become widespread and commonplace enough to be monetized.on the issue of taxing online transactions, governments have competing interests. many state and local government leaders do not want tolose sales tax revenue when purchases are made from companies withoutnexus in that state, while other policy makers are interested in not retarding the growth of commerce over the internet. another consideration isthat changing the current tax policy could affect the volume and distribution of commerce that is conducted over the internet, which could in turnaffect the way in which the internet is used.there is a lot of ambiguity and uncertainty as to how a change in taxpolicy would affect tax revenues or, directly, the growth of the internet.work by goolsbee49 gives empirical support to the idea that taxes (andother price differences) will have significant effects on the purchasingbehavior of individuals living in a òworld without borders.ó he projectsthat the price impact of applying existing sales taxes to internet commercemight reduce the number of online buyers by up to 24 percent. to theextent that ecommerce is an important driver of investment in internetinfrastructure (and supports other internet services via advertising), theoutcome would affect the internetõs future development and growth.forecasts of the volume of tax revenue at stake vary. the nationalgovernors association has quoted forecasts that by 2002 there may bemore than $300 billion in commerce over the web or through mail orderand concluded that this would result in up to $20 billion in lost tax revenue,50 and similar numbers are often cited by advocates of internet taxation. goolsbee and zittrain51 offer a different perspective. they observe48in june 2000, for example, the eu considered imposing a broad vat obligation.49austan goolsbee. 2000. òin a world without borders: the impact of taxes on internetcommerce.ó the quarterly journal of economics 115(2):561576.50juliana gruenwald. 1998. òvote bodes ill for internet tax agreement.ó congressionalquarterly this week, august 3.51austan goolsbee and jonathan zittrain. 1999. òevaluating the costs and benefits oftaxing internet commerce.ó national tax journal 52 (3):413428. available online at <http://papers.ssrn.com/paper.taf?abstractid=175666>.the internet's coming of agecopyright national academy of sciences. all rights reserved.implications for broad public policy207that the previously predicted amounts seem to include businesstobusiness as well as businesstocommerce sales; that they ignore the possibility of trade creation; and that the calculations fail to account for the typesof products being sold. they find that for the next several years, there islittle tax revenue to be gained from enforcing taxes on internet sales.when confronted with the issue of how to collect sales taxes for transactions conducted over the internet, given that states and local municipalities are the ones that levy sales tax, congress passed the internet taxfreedom act (itfa), which put in place a 3year moratorium on imposing new taxes associated with internet transactions and established a congressional advisory commission on electronic commerce to study thequestion of sales tax revenue.52 this action was motivated by not wantingto do anything that could adversely impact the growth of commerce overthe internet and by the ambiguity resulting from the inherently borderlessnature of the internet. the itfa, however, does not restrict the right ofstates to apply sales and use taxes to online commerce (these are not, afterall, new taxes). instead it primarily prevents states from applying newtaxes to internet access. the commission completed its work in april 2000without the mandated supermajority of the committee reaching consensus. resolution of the tensions between retaining a tax revenue base andfostering ecommerce are likely to remain a contentious political issue forsome time.53the difficulty of knowing the identity and location of parties to an ecommerce transaction is exacerbated by several factors. as discussed inan earlier section, the internet enables a range of anonymous transactions,making it difficult to ascertain the identity of a purchaser let alone his orher location. the location capabilities that are offered by emerging wireless data services may prove an exception, although they would be enabled by mechanisms provided as part of the wireless service rather thanconventional internet connectivity provided through the wireless link.also, digital systems acting on behalf of people or organizations, ratherthan the people or organizations themselves, can be the actors responsiblefor buying and distributing a product or service over the internet. ofcourse in some instances location could still be established based on thedelivery address. however, many goods are electronic (e.g., downloadedsoftware or music) and many services can be delivered via communications over the internet, so these goods and services can be delivered to a52title xi of public law 105277.53intel leader andrew groveõs june 2000 statement supporting taxation of internetbasedcommerce indicates that the hightech industry does not have a uniform position on theseissues.the internet's coming of agecopyright national academy of sciences. all rights reserved.208the internetõs coming of agecomputer attached to the internet without any physical goods being delivered to a verifiable physical address. the recipient computer sits in aphysical location, but associating a physical location with that computerõsnetwork address cannot be done with certainty in todayõs internet. andinternationally, even if there were agreements to collect taxes, enforcement of tax collection for physical goods shipped across national boundaries would depend on customs agencies to block shipments or collectduty.one suggested remedy is to build into the internetõs infrastructureitself the ability to provide the location and/or identity of the parties to atransaction. however, efforts to embed solutions to the taxation problemat the network level would have farreaching consequences for theinternet, which currently has no concept of locality in a geographical orgeopolitical sense. as discussed in chapter 1, a central design tenet of theinternet is the placing of applications and intelligence in the end systemsrather than within the network. a solution to the taxation issue thatrelied on building into the network mechanisms that provide knowledgeof the geographical location of a network element would violate this principle, as it would require a new, intelligent capability within that networkthat determines, and on request provides, the physical location of a device or some surrogate, which is not easy to do in any case. such asolution could also have adverse privacy implications. it would, incontrast, be possible to build such knowledge into end systems or higherlayer authentication infrastructures. it might, for example, be more productive to try to derive location information from authenticated information about a party to a transaction. because internet technology andecommerce technology are evolving so rapidly, it is clearly preferable forsolutions to avoid placing requirements on the internet infrastructurethat are dependent on a specific ecommerce technology.given that the internet generally ignores geography and makes itdifficult for vendors or third parties to assure identification of a purchaser, there are significant difficulties associated with solutions that depend on ascertaining the location of the purchaser. this suggests, first,that if taxation policy relating to ecommerce must be changed, thechanges should be as unspecific to geopolitical region as possible and,second, that todayõs sales tax system, which involves many thousands ofdistinct jurisdictions, would need to be simplified. and, in order to reflect the internetõs architecture, including the dynamic nature of its routing, taxation schemes should be based on the end points only and not onmechanisms embedded in intermediate points within the network.there are a number of solutions that avoid these problems. some taxstructures would not require a complex knowledge of geography to bebuilt into the technology. a flat ecommerce sales tax collected by thethe internet's coming of agecopyright national academy of sciences. all rights reserved.implications for broad public policy209seller at the time of payment would not be localitysensitive, would require no technology changes at the buyerõs end, and would be easier forthe vendor to implement. the collection of taxes would be simpler if theywere added to each transaction at the point of sale, much as is done witha cash transaction at the physical point of sale today. (such a schemewould also enable the appropriate tax to be collected even if the transaction were conducted anonymously.) another simplification would be toseparate the means of tax collection from the distribution and allocationof tax revenues. the issue of which governmental bodies get some of thetax collected and what portion of it they get could then be addressedseparately by the respective governmental bodies. it would be easier toallocate revenue if such allocation did not depend on knowing the location of the purchaser. of course other simplifying schemes could bedevised that would not be such a radical departure from todayõs sales taxsystem. these approaches would help reconcile the tensions betweenthose who want to preserve the tax revenue base for state and local governments and those who want to foster the growth of ecommerce, butstriking a balance between these interests is likely to remain a contentiouspolitical issue for some time.universal serviceequity in access to and use of the internet is a matter of values andsocial policy. such policy has been reflected in universal service for telephony, with access provided to people across all income classes.54 universal service programs fall into two general classesñsetting rates thatbenefit particular classes of customers (e.g., residential or rural users)who might otherwise face considerably higher rates and offering subsidized lifeline rates to lowincome subscribers to expand the number ofhouseholds with access to basic telephone service, thereby expandingopportunities for economic, community, and political participation andemergency (911) service.universal service has long been an element of u.s. telecommunications policy.55 indeed, whether or not one agrees that universal service54universal service policies go beyond establishing uniform rates for service: they createsubsidized òlifelineó rates for basic service at prices low enough to permit the poorestfamilies to have access to the telephone network.55basic telephone service has long been regarded as a social good, universal access towhich required a deliberate policy effort to achieve. however, the history of universalservice policies can support a different interpretation. milton mueller argues that, giventodayõs rates, universal access would easily have been achieved even without subsidization. see milton mueller. 1997. òuniversal service and the telecommunications act: mythmade law,ó communications of the acm 40(3):3948.the internet's coming of agecopyright national academy of sciences. all rights reserved.210the internetõs coming of agefor networks is an appropriate objective of public policy, it is worth pointing out that extension of universal service policies to new communications networks has always enjoyed popular support. historically, thegovernment intervened to establish universal service at uniform rates forpostal services as well as telephone service and to extend to remote areasand impoverished areas the benefits of such infrastructure as electrification and highway construction. given the rapid pace at which internetbased applications and services are being deployed in both the privateand public sectors, social and political demands for expanded access tointernetbased communications services, which are increasingly seen asessential for commerce, education, employment, or political participation, can be expected to increase.56there are several geographyrelated factors associated with internetaccess. these geographical considerations are, of course, closely linked toeconomic factors. places with less access will generally be those wherethe remoteness, the lower density of potential customers, or the lowerability (or willingness) of the population to pay make the provision ofservice a highercost undertaking or a less attractive investment. thereare differences in the availability and price of dialup access that dependon (1) the number of carriers that have established local access points(whereby dialup access is via a local, typically flatrate billed call) in agiven location and (2) the availability of dialup internet access serviceproviders. a recent study by downes and greenstein57 found that, as ofthe spring of 1998, most of the u.s. population had access to competitivedialup internet service. according to this research, more than 90 percentof the population lived in areas served by more than 10 isps, while fewerthan 1 percent lived in areas without any dialup service.5856concerns about universal access to the internet are not new. for example, in the fall of1993, they were featured in an administration policy statement (william jefferson clintonand albert gore. 1993. the national information infrastructure: agenda for action. washington, d.c., september 15. available online at <http://metalab.unc.edu/nii/toc.html>). thestate of access to internet and related services has also been the subject of a series of national telecommunications and information administration (ntia) reports from 1995 tothe present. (ntia, department of commerce. 1995. falling through the net: a survey ofthe òhave notsó in rural and urban america. washington, d.c.: ntia, july. available onlineat <http://www.ntia.doc.gov/ntiahome/fttn99/contents.html>.)57thomas a. downes and shane m. greenstein. 1998. òdo commercial isps provideuniversal access?ó competition, regulation, and convergence: current trends in telecommunications policy research. sharon gillet and ingo vogelsang, eds. mahwah, n.j.: lawrenceerlbaum. available online at <http://skew2.kellogg.nwu.edu/~greenste/research/papers/tprcbook.pdf>.58even in unserved areas, the upper bound on the cost of dialup internet is set by theroughly $4 to $5 hourly rate offered by a number of isps for access via tollfree numbers.the internet's coming of agecopyright national academy of sciences. all rights reserved.implications for broad public policy211highspeed (broadband) access is less widespread than lowerspeeddialup service.59 constraints include technical factors (e.g., dsl servicesare limited to locations within a given distance from a local exchange,with the exact distance depending on the variant of dsl technology employed, the bandwidth, and the condition of the phone lines); the extentto which the necessary telecommunications infrastructure is present (e.g.,cable modem service is limited to locations passed by cable service, whichis less prevalent in rural areas), and the pace at which investment is madein service deployment, including the associated required infrastructureimprovements. with deployment of such services in its early stages, afew communities have both dsl and cable service and many have nohighspeed services at all. higher bandwidth services require investmentin upgraded facilities (e.g., deployment of dsl facilities in the telephonelocal exchange or upgraded cable plants) and so are more likely to occur,at least in the earlier phases of deployment, in wealthier communities,where more customers are likely to purchase service as a result of anupgrade. whether physical access to highspeed services will begin toapproach the nearuniversal level seen for dialup service as deploymentcontinues remains to be seen. it is the subject of political and regulatorydebate.access to a service does not mean that it will be used; this can be seenin the history of telephone and television use. there have been a varietyof studies conducted by both government and market research firms tomonitor and describe patterns of internet access and use. one such effortculminated in a series of reports from the national telecommunicationsand information administration (ntia). for example, a july 1999 ntiareport based on u.s. census bureau data from december 199860 indicatesthat, of the households able to access the internet, the fraction that actually subscribe to an isp is far from 100 percentñ41.1 percent of u.s.households owned computers and roughly 25 percent had internet service. such studies point not only to the persistence of disparities but alsoto their instability: whole groups can increase their use of the internetbetween studies, and the implications of the findings are hard to pindown. a number of efforts have been made to understand the extent of59broadband lastmile technologies and local access are the concerns of a separate cstbstudy, to be completed in 2001.60national telecommunications and information administration (ntia), u.s. department of commerce. july 1999. falling through the net: defining the digital divide: a reporton the telecommunications and information technology gap in america. washington, d.c.:ntia. available online at <http://www.ntia.doc.gov/ntiahome/digitaldivide/>.the internet's coming of agecopyright national academy of sciences. all rights reserved.212the internetõs coming of agedisparities in service, including those between high and lowincomehouseholds and between urban and rural status and those based on education or race. the situation is volatile, with a panoply of data showingboth persistent disparities as well as instances where disparities havedecreased over time.to what extent is the fraction of the population that has internetaccess likely to broaden? recent years have seen a drop in the cost ofcomputer equipment required to access the internet; relatively inexpensive pcs and a range of internet access appliances have entered the market. also, various new businesses are offering free internet access or evenfree pcs along with internet service in exchange for viewing advertising.complementing homebased access, kiosks operated by public institutions and commercial enterprises make internet access available in a number of public places. one motivation for the federal erate program thatprovides subsidies to schools, libraries, and hospitals is to increase thenumber of public access points. free email services allow people unableto afford internet service to maintain private email accounts that areaccessible from public locations. however, the longterm viability of newschemes and business models for providing internet service remains to beproven. pricing schemes and bundling are in flux as new business models emerge. declining costs and increasing utility may result in universalor nearly universal access to the internet without any government action,but this outcome is neither certain nor guaranteed.universal service programs applied to internetbased services raise anumber of social and political questions, including who should receiveuniversal service benefits, what value judgments underlie these choices,whether funds should be obtained from servicespecific sources or fromgeneral funds, what constitutes universal service (e.g., which collection ofservices, from personal/household access to access through public facilities), and even whether universal service programs should be implemented at all. in this section, the committee briefly reviews what is knownabout internet penetration in the united states and then addresses someissues that would arise if one were to try to implement universal servicepolicies for the internet. it does not take a position on whether universalservice programs should be extended to the internet or on how they shouldbe funded and managed, because to do so adequately would require fullconsideration of the costs and benefits over time, a complex matter wellbeyond the scope of this report.universal service policies have, at their core, sought to ensure thatprice or geographical location is not a barrier to use. these policies combine two distinct elementsñuniversality of physical access and universality of financial access. in the case of the internet, the cost is the sum ofseveral elements. first, there is the price of the internet service itself.the internet's coming of agecopyright national academy of sciences. all rights reserved.implications for broad public policy213second, there is the cost of access to the internet (e.g., via cable modem,dsl, or dialup services), which is frequently bundled together with thecost of internet service or, in the case of dialup access, already paid for aspart of local telephone service. third, there is the cost of the associatedhardware (computer and modem or other connection device) and thecommunications software (generally available at little or no cost orbundled with a pcõs operating system or with the internet service). finally, there are additional costs associated with the use of particular onlineservices, such as subscription fees for accessing particular content.the traditional universal service obligation applied to telecommunications carriers was a bundle of obligations that users, service providers,and regulators experienced as a single package. the characteristics ofuniversal telephone serviceñsuch as expectations for quality and accessto callers within the local area, to interexchange carriers, and to operatorand emergency servicesñwere well defined and had been developedover the course of many years. internet services stand in marked contrast.for the internet user, service translates into a set of internet applications(e.g., web browsing, listening to audio programming, video conferencing,or banking online). what the user experiences as òserviceó depends onthe software running on both his and an application providerõs computeras well as the characteristics of the network links over which the communication passes. the notion of guaranteeing a particular service to a broadclass of potential subscribers needs to be revised, since service in the endtoend model is in the control of applications running on machines at theedges of the network as well as the capacity and explicit quality of servicemechanisms offered by a network operator.61 in some cases these applications and services are offered as part of a bundle from the internetservice provider, but in general they are provided separately. the desiredapplications will vary from user to user, and the available mix will changeover time as new internet applications emerge and old ones fall out offavor (few, for example, make use of ògopheró today). which servicesand applications (e.g., email, web, chat, telephony, or streaming video)61moreover, even in the telephony model, no single service was adequate to provideequivalent network access for all users. users with disabilities, for example, were not wellserved under the traditional universal service schemes. interface and access issues wereexplored by the cstb (computer science and telecommunications board (cstb), nationalresearch council. 1997. more than screen deep: toward everycitizen interfaces to thenationõs information infrastructure. washington, d.c.: national academy press). implementing provisions of section 255 of the 1996 telecommunications act and section 251(a)(2) ofthe communications act of 1934, the fcc issued rules on july 15, 1999, aimed increasingaccess to such telecommunications hardware as telephones and to services such as callwaiting and operator service.the internet's coming of agecopyright national academy of sciences. all rights reserved.214the internetõs coming of ageshould be included in a universal service package, and at what performance level (e.g., speed of download) and quality level (e.g., reliability)?the emergence of alternatives to dialup access for residential internetaccess means there are more choices with respect to bandwidth and service quality as well a range of service and price options. also, theinternetõs default besteffort service quality allows no guarantees thatadequate capacity and quality will be provided to support particular applications. approaches for providing explicit quality of service are emerging, but there is neither broad agreement on which approaches to use norwidespread deployment, particularly for the home users who are thetarget of universal service policies. also, while some applications wouldbe provided by a customerõs isp directly, most require communicationsacross internet provider boundaries. in the case of the internet, termination would, at a minimum, mean that any internet user could access anyother user. by virtue of the arrangements that interlink the internetõsconstituent networks, basic besteffort connectivity is provided to all network users. however, especially in the case of residential customers,there is generally no service level agreement nor are explicit qualityofservice mechanisms supported. one concern expressed by consumer activists is that people with subsidized or lowestcost access might not receive service that supports explicit quality of service and might thereforeexperience a significantly degraded internet service or be unable to usecertain demanding applications.efforts have been made to define classes of internet services. forinstance, several years ago the crossindustry working team project ledto the concept of ònii class profilesó for characterizing endtoend performance.62 such classification schemes are intended, for example, tosimplify specification of the requirements of an application and the abilityof equipment and services to meet those requirements. while such aclassification might be useful to consumers for evaluating the technologyoptions available to them, its use to define fixed bundles of service in auniversal service policy would be too limiting. because internet technologies are immature and still in the early stages of deployment, it would bepremature to embody them in regulation.even the application that was the basic element of traditional universal service, voice telephony, presents difficulties from a universal servicestandpoint when it is offered as an internet service. as the discussion inchapter 4 indicates, there are many approaches to providing ip or internettelephony, complicating efforts to define a standard service. there are62crossindustry working team (xiwt). 1997. class profiles for the current and emergingnii. white paper. reston, va.: xiwt, corporation for national research initiatives. available online from <http://www.xiwt.org/documents/classprofiles.html>.the internet's coming of agecopyright national academy of sciences. all rights reserved.implications for broad public policy215multiple tiers of service quality, some of which offer consumers a lowerprice in exchange for reduced guarantees of service quality. moreover,the issue of guaranteed access to critical services such as 911 service,which is an element of telephony universal service obligations, is an especially difficult one in an internet context. while it can be anticipated thatfuture obligations will build on those associated with todayõs 911 service,it is difficult to predict how expectations for critical services will evolve,how they will be shaped by new internet capabilities, or how they will beimplemented.in telephony, universal service programs, whether aimed at addressing financial or geographical disadvantages, generally involve settingrates and fees so as to achieve a particular objective, whether it is tobenefit particular subscribers, extend the telephone network, or increasethe total number of subscribers. a number of mechanisms can be employed, including crosssubsidies among service offerings (e.g., subsidizing local calling at the expense of longdistance calling), different rates fordifferent classes of users (e.g., lower rates for residentialñas opposed tobusinessñusers), the establishment of particular service obligations inexchange for other regulatory relief, or, more recently, explicit fees.63the internet, by contrast, has a much richer assortment of serviceproviders, ranging from singleniche service providers to full, verticallyintegrated service providers. therefore, if it is decided that a universalservice program aimed at households is warranted, a range of options forachieving this aim should be considered. the rapid change in internetservices and service offerings would argue for a technologyneutral approachñone that does not rely on mandates to service providers thatspecific types of service be made available at regulated, possibly subsidized rates. a recent example of this approach is the erate program,which has used a fee levied on telephone service to establish a fund tohelp schools, libraries, and hospitals to pay for internet access. it is notnecessary to transfer resources through crosssubsidies among classes ofcustomer. other options include subsidies funded by general tax revenueand used by needy citizens to purchase services of their choosing (withinestablished guidelines)ñsomething more akin to food stamp programs.63the committee has been careful to avoid labeling these mechanisms as crosssubsidiesas there are arguments refuting the idea. for example, there are data that suggestñand anumber of state public utility commissions have accepted this viewpointñthat residentialrates fully cover the costs of providing that service, which suggests that there is not atransfer between business and residential customers. also, since universal service programs have network extension as one goal, it can be argued that what might appear to be atransfer of money is, in fact, the regulatory apparatus positioning subscribers to capture thenetwork externality. in other words, if new subscribers come on the network, then theremay or may not be a subsidy of the new user.the internet's coming of agecopyright national academy of sciences. all rights reserved.the internet's coming of agecopyright national academy of sciences. all rights reserved.217eric schmidt, chair, joined novell in april 1997 as chairman of theboard of directors and chief executive officer. at novell, schmidt is involved in all significant operating and strategic decisions for the companyand plays a central role in the technical development and management ofthe company. dr. schmidt came to novell from sun microsystems, inc.,where he was chief technology officer and corporate executive officer. inhis 14 years at sun, dr. schmidt held a range of progressively more responsible executive positions, earning international recognition as aninternet pioneer. he was also instrumental in the widespread acceptanceof java, sunõs platformindependent programming language. prior to joining sun, dr. schmidt was a member of the research staff at the computerscience lab at xerox palo alto research center (parc). he also heldpositions at bell laboratories and zilog. dr. schmidt has a b.s. degree inelectrical engineering from princeton university and an m.s. in electricalengineering and a ph.d. in computer science from the university of california at berkeley.terrence mcgarty, vice chair, is the chairman of the telmarc group,llc, a company he founded in 1984 that invests in and manages severalhightech startup ventures. he is currently chairman and ceo of zephyrtelecommunications, an international record carrier, and managing director of crossconnect, a venture capital company.dr. mcgarty has been active in the telecommunications industry forover 30 years. he was until 1992 a senior vice president at nynex andthe internet's coming of agecopyright national academy of sciences. all rights reserved.218appendixthe chief operating officer of nynex mobile, a cellular carrier. prior tothat he was the first head of r&d for nynex, during which time hecreated the organization and conceived and developed one of the firstmultimedia communications systems and the first network managementsystem using the manager of managers concept. all developments weresuccessfully commercialized. dr. mcgarty also spent 5 years in the catvbusiness, as group president at warner communications, and 6 years inthe satellite communications business as a division director and generalmanager of comsatõs first nonregulated businesses. while at warnercommunications, dr. mcgarty developed and implemented the firstvideoondemand cabletelco videotex system in the united states, whichis the predecessor of all interactive multimedia cable/telco systems. hisearly career was as a faculty member and research staff member at themassachusetts institute of technology, where he was involved in researchin communications and imaging systems as well as microeconomic policydevelopment.dr. mcgarty is also very active in the ongoing development of telecommunications policy and is internationally known for his policy development work in this area. he has served in many government advisoryroles, specifically as senior advisor to the u.s. negotiating team on thecomprehensive test ban treaty during the carter administration. hehas also advised the defense department, the energy department, thestate department, and the transportation department.dr. mcgarty holds a ph.d. from mit in electrical engineering as wellas his two other degrees and also studied medicine in the joint harvard/mit program. he is the author of four books on random process theory,business planning, and telecommunications policy and over 75 professional papers in areas from telecommunications to law to radiology andmedical imaging. he sits on the boards of several companies, includingmdsi, a publicly traded company.anthony s. acampora is a professor of electrical and computer engineering at the university of california at san diego, which he joined in1995. there he is involved in numerous research projects addressingvarious issues at the cutting edge of telecommunication networks, including the internet, atm, broadband wireless access, network management,and dense wavelength division multiplexing. from 1995 through 1999,he was director of ucsdõs center for wireless communications, responsible for an industrially funded research effort that included circuits, signal processing, smart antennas, basic communication theory, wireless telecommunications networks, infrastructure for wireless communications,and software for mobility.before that, prof. acampora taught electrical engineering at columthe internet's coming of agecopyright national academy of sciences. all rights reserved.appendix219bia university, which he joined in 1988, and was director of its center fortelecommunications research, a national engineering research facility.he was involved in research and education programs on broadband networks, wireless access networks, network management, optical networks,and multimedia applications.for 20 years before that he was at at&t bell laboratories, most of thetime in basic research, where his interests included radio and satellitecommunications, local and metropolitan area networks, packet switching, wireless access systems, and lightwave networks. his last positionat bell labs was as director of the transmission technology laboratory,where he was responsible for a wide range of projects, including broadband networks, image communications, and digital signal processing.he received his ph.d. in electrical engineering from the polytechnicinstitute of brooklyn and is a fellow of the institute of electrical andelectronics engineers (ieee) and a former member of the ieee communication society board of governors. prof. acampora has published over160 papers, holds 30 patents, and has authored a textbook entitled òanintroduction to broadband networks: mans, atm, bisdn, self routing switches, optical networks, and network control for voice, data,image and hdtv telecommunications.ó he sits on numerous telecommunications advisory committees and frequently serves as a consultant togovernment and industry.walter s. baer is a senior policy analyst at the rand corporation,santa monica, california. he directs research on interactive media, telecommunications, and information infrastructure developments, as wellas on the public policy and business implications of new communications, information, and educational technologies. dr. baer was appointedin 1994 to the governorõs council on information technology for thestate of california. he currently chairs the telecommunications policyresearch conference and serves on the ieee committee on communications and information policy and the editorial board of telecommunications policy, as well as on the advisory boards of the u.s. committee forthe international institute of applied systems analysis, the columbiauniversity institute for teleinformation, the los angeles learning center network project, and the childrenõs partnership. he holds a b.s. fromthe california institute of technology and a ph.d. in physics from theuniversity of wisconsin.fred baker has worked in the telecommunications industry since1978, building statistical multiplexors, terminal servers, bridges, and routers. at cisco systems, his primary interest is the management of congestion for besteffort and realtime traffic. in addition to product development, as a cisco fellow he advises senior management of industrythe internet's coming of agecopyright national academy of sciences. all rights reserved.220appendixdirections and appropriate corporate strategies. his principal standardscontributions have been to the internet engineering task force (ietf),but he has contributed to the international telecommunication unionõsh.323 and to such industry consortia as winsock ii and the atm forum. in the ietf, he has contributed to network management, routing,ppp, and frame relay, the intserv and diffserv architectures, and thersvp signaling protocol. in addition to being a technical contributor, hecurrently serves as the ietf chair.andrew blau is a program designer and strategist working with foundations and other organizations developing programs at the intersectionof technology and society. building on 15 years as a policy analyst focused on the social and policy impacts of the internet, telecommunications networks, and digital media, he launched flanerie works in 2000 tohelp foundations better incorporate these technologies and their effectsinto their work. current or recent clients include the ford foundation, therockefeller foundation, the paul g. allen foundation, the surdna foundation, and the carnegie corporation.previously, mr. blau was program director at the markle foundation,and prior to that he directed the benton foundationõs program in communications policy and practice. he also analyzed federal and state telecommunications and internet policy for leading public interest groups including the electronic frontier foundation and the united church of christõscommunications policy program. in 1991, he spent a year as a seniormember of the research staff at the columbia institute for teleinformation (citi).at the request of the clinton administration, mr. blau was the principal organizer of the first national meeting to bring together leaders fromnonprofits, foundations, and the white house to discuss public interestpolicies in communications. he has testified before congress about therole of nonprofits in the information age, participated in scores of regulatory proceedings before federal and state regulatory agencies, and published and lectured internationally on developments in u.s. telecommunications policy. he is a member of the program committee for the 2001telecommunications policy research conference and has been an advisor on technology projects to many organizations, including the u.s. department of health and human services, the national endowment forthe humanities, the city of seattle, and the microsoft corporation.deborah estrin is a professor of computer science at the university ofcalifornia at los angeles. from 1986 to 2000, she was on the faculty ofthe university of southern california in los angeles. in 1987, dr. estrinreceived the national science foundationõs (nsfõs) presidential younginvestigator award. she is a codesigner of the pim and rsvp protocols,the internet's coming of agecopyright national academy of sciences. all rights reserved.appendix221and her current research interests include multicast, selfconfiguring systems, and scaling issues in general. dr. estrin received her ph.d. (1985)and m.s. (1982) from the massachusetts institute of technology and herb.s. (1980) from the university of california at berkeley. she is a memberof the association for computing machinery, ieee, and the americanassociation for the advancement of science. she has served on severalpanels for the national science foundation, on the national researchcouncilõs computer science and telecommunications board (cstb), andas a member of darpaõs information science and technology studygroup (isat). she currently chairs a cstb study on networked embedded computing.christian huitema is the architect in the windows networking &communications group at microsoft, a group responsible for networkingsupport for windows. from 1996 to 2000, he was the chief scientist intelcordiaõs internet architecture research laboratory, where he workedon internet telephony and internet quality of service. before joiningtelcordia, he was a senior scientist at inria in france, leading a networking research project that investigated innovative applications, such asvideo on the internet, and innovative technologies, from directories toprotocol compilers. he is the author of several books (among them, routing on the internet and ipv6: the new internet protocol), a former memberof the internet architecture board, which he chaired from april 1993 tojuly 1995, and a trustee of the internet society.edward jung is copresident of intellectual ventures. prior to that hewas general manager of the web platforms group in the interactive media group and a microsoft chief architect, serving as a technical strategyadvisor to the executive staff on advanced technology. he also led objectoriented and enduser interface technologies in the advanced systemsdivision for microsoftõs futuregeneration operating systems products,now part of windows nt. he also served as software architect in applications architecture, where he codeveloped com and ole, and as information at your fingertips (iayf) coordinator, where he developed anddeployed the iayf technical strategy.before joining microsoft in 1990, dr. jung ran several startup companies and was a biophysicist investigating protein structurefunctionrelationships. his research has been published in proceedings of the national academy of sciences, biophysics journal, and the journal of biologicalchemistry.david a. kettler is vice president for bellsouth and is in charge of thescience & technology organization and chief architect for the bellsouthnetwork. his responsibilities include applied research, systems engithe internet's coming of agecopyright national academy of sciences. all rights reserved.222appendixneering, software application development, network architecture, technical analysis and support, technical standards, network fundamental planning, technology deployment directives, and infrastructure planning.dr. kettler joined bellsouth in 1987 to form the new science & technology organization. prior to that he was employed by at&t bell laboratories for over 15 years and managed departments in network architecture, signaling, and network management. in addition, he led systemarchitecture activities in strategic planning at at&t corporate headquarters. dr. kettler has led and continues to lead major technology thrusts,including data networking services, residential broadband, fiberintheloop, advanced intelligent networks (ains), and emerging wireless andvideo technologies. dr. kettler has been a world leader in the introduction of ain, highspeed packet switching (atm) for information highways, adsl for highspeed data access, and opticalfiber distributionsystems such as fibertothehome. presently, dr. kettler is leading activities in the transformation of bellsouthõs network to a datacentric architecture.dr. kettler received his b.e.e., m.s.e.e.., and ph.d.e.e. from the university of virginia. dr. kettler is an ieee fellow. he has served onnumerous technical and scientific committees, has organized and chairedconferences and technical sessions, and has presented talks at conferencesaround the world.john c. klensin is vice president for internet architecture at at&t.prior to joining at&t in 2000, he was distinguished engineering fellowat mci and then mci worldcom. outside his corporate commitments, hehas had significant responsibility for the present generation of internetapplications standards. his involvement with what is now the internetbegan in 1969 and 1970, when he participated in the working group thatcreated the file transfer protocol and that made the decision to includeelectronic mail capability in the networkõs design. dr. klensin was on thepermanent research staff at mit for about 25 years, participating in ordirecting a wide variety of projects, many of them involving the application or development of computer networking or related technologies. dr.klensin has also been involved with international development work witha united nations university project on food composition data, archives ofimages in islamic architecture, and the network startup resource center.milo medin is the chief technology officer of excite@home, where heoversees the development of the companyõs highspeed backbone.@homeõs performanceengineered scalable network removes internetòtraffic jamsó and enables true endtoend management. in addition, thenetwork employs replication and caching technologies that dramaticallyimprove network efficiency.the internet's coming of agecopyright national academy of sciences. all rights reserved.appendix223prior to joining excite@home, mr. medin served as project managerat nasa ames research center. during his tenure, he directed the nasanational research and education network project that, in combinationwith partners at the lawrence livermore national laboratory, deployeda highspeed national atm infrastructure connecting major supercomputing and data archiving centers. he also supervised the primary westcoast internet interconnect network. in addition, he pioneered the globalnasa science internet project, providing network infrastructure for science at more than 200 sites in 16 countries and 5 continents, includingantarctica, and helped establish the tcp/ip protocol as an industry standard.before nasa, mr. medin held various positions at science applications inc., programming supercomputers for defense program activitiesat the lawrence livermore national laboratory and the los alamos national laboratory, under contract to the defense nuclear agency. he hasa b.s. in computer science from the university of california at berkeley.craig partridge is a chief scientist at bbn technologies (a part ofverizon communications), where he leads a variety of internetrelatedresearch projects. his most recent major projects involved building anexperimental multigigabit router and developing a nextgeneration routing protocol. dr. partridge is the chairman of the association for computing machineryõs special interest group in data communication (oneof the two major professional societies in data communications) and aparttime professor at stanford university. he is the former editor inchief of both acmôs computer communication review and ieee networkmagazine and a consulting editor for addisonwesleyõs professional computing series. he is a fellow of the ieee and holds his a.b., m.sc., andph.d. degrees from harvard university.daniel schutzer is vice president and director of external standardsand advanced technology in the advanced development group atcitibank. he is responsible for interfacing with external organizationsand standards bodies and for representing citibank. this includes coordinating technology with business goals and priorities and keeping citibankuptodate with the latest technology and standards advances. projectsinclude electronic banking and electronic commerce, bill presentment andpayment, risk management, customer behavioral modeling and mathematical marketing, and new product design. advanced technologiesunder investigation include agent technology, machine learning, multimedia, biometrics, image and voice processing, and highperformancecomputing.his previous positions included technical director for naval intelligence; technical director for navy command, control and communicathe internet's coming of agecopyright national academy of sciences. all rights reserved.224appendixtions; and program manager at sperry rand. he also worked at bell labs,syracuse university, and ibm. he currently teaches part time at ionacollege in new rochelle, new york, and george washington universityin washington, d.c. he is the author of over 65 publications and 7 books:parallel and distributed processing, application of emerging technologies inbusiness, applied artificial intelligence, military communications, commandand control, a chapter on financial risk management in a financial management handbook, and a chapter in a book on electronic commerce.forthcoming books are on electronic payment and electronic commerce.he is a board member of the financial services technology consortiumboard and chairman of iso subcommittee 2. he received his b.s.e.e.from the college of the city of new york and m.s.e.e. and ph.d. fromsyracuse university.the internet's coming of agecopyright national academy of sciences. all rights reserved.225aaccessbroadband, 46costs of, 4142equal, 213nto the local loop, 11open, and cable, 49access infrastructure, trends in upgradingthe local, 4950adaptive design, scalable, 4041adaptive routing, to increase reliabilityand robustness, 8990address aggregation, routing table scalingand, 6671address space, scaling up, 6481address translation, network, 15, 6970, 7677, 140142address translators, network, 70, 141addressesassigning, 78blocks of held by organizations, 7managing, 6566numerical, 64portability, 167168scarcity of, 7176tensions surrounding allocation of, 68administration, of the domain namesystem, 52advanced intelligent network (ain), 159160advertisements, route, 69, 114aggregation, address, 6671agreements, interconnection, 112118ain. see advanced intelligent networkalgorithms, scaling of, 5657allocation of addresses, 68, 105america online (aol), 111112, 147merger with time warner, 145american national standards institute(ansi), 133anonymity, 190194defined, 180in public policy, 190194ansi. see american national standardsinstituteaol. see america onlineapplication programming interface (api),specifications for, 124125applications, 4, 36innovative, 131132multimedia, 100reengineering, 6reliability and robustness of, 9293requirements of new, 100specifications for, 125architectures, 34, 154endtoend, 4the internet's coming of agecopyright national academy of sciences. all rights reserved.226indexhourglass model of, 4, 3638, 127128ip telephony, 155159pstn, 1617telephony, new and evolving, 154162arin, 65arpanet, 39, 44n, 134assigning addresses, 78atm networks, 100nattacksdenialofservice, 8vulnerability of the internet to, 8489audio, streaming, 100authentication issuesin public policy on the internet, 199204safeguards for, 24bbackbone capacity, 3, 20, 31ngrowth in, 45backdoor connections, 108nbalkanization, of the internet, 10bandwidthadding, 99100utilizing, 55barriers to entry, lowering, for innovation,42bell labs, 109nbenefits, longterm, of open ip service, 2425besteffort networks, 910bgp. see border gateway protocolbilateral peering, 115bind software, 63biometrics, 202, 204bits, 55nnonuniform treatment of, 142145border gateway protocol (bgp), 71, 104route advertisements, 69boundaries, revenueneutral, for theinternet, 1213broadbandaccess, 46deployment of, 4748broadcasting business, 47buffer overflow, 100business influences on openness, endtoend transparency in, 145149ccable & wireless, 110cable modems, 31cable operators, 49cachingdns, 62web, 38calea. see communications assistancefor law enforcement actcapacity. see also bandwidthgrowth in backbone, 45scaling of, 55, 116117certification authorities, 196challengesof distributing information, 6of harmonization, 28scaling, 46childrenõs online privacy protection act of1998, 181, 188china, internet use in, 7576cidr. see classless interdomain routingclassless interdomain routing (cidr), 6769, 7980cmip. see common managementinformation protocolcollaboration, among researchers, fundersand network operators, 2021collisions between existing industries andemerging internet industries, 1517,151176future of the internet and otherindustry sectors, 175176implications of ip telephony fortelephony regulation, 170175interoperation between ip telephonyand the pstn, 165170ip telephony defined, 152154new and evolving architectures fortelephony, 154162scenarios for future evolution, 162165committee on broadband lastmiletechnologies, 11, 49common management informationprotocol (cmip), 137communications assistance for lawenforcement act (calea), federalcommunications commissionruling on, 172173, 175communications media, 34communications technologies, 4, 31, 36computer industry almanac, 72the internet's coming of agecopyright national academy of sciences. all rights reserved.index227confidentiality safeguards, 24congestion, 8687, 99100, 117connectivity, value placed on, 35, 108controldecentralized, 4for interoperation between ip telephonyand the pstn, 168169cookies, 183n, 196cpni. see customer proprietary networkinformationcredit card use, 197200critical open standards in the internet, 126130dynamic host configuration protocol,129routing protocols, 129crossindustry working team project, 214cryptography, public key, 201customer proprietary network information(cpni), 184customersrequirements of large, 120nvaluestratification of, 9, 105, 118121òcybercrime,ó 194ddarpa. see defense advanced researchprojects agencydata collection, 26data packets, routing of, 4, 12data services, trends in, 50data transfer, mediating, 74decentralized control, 4dedicated private networks, ip telephonyconfigurations using, 158defense advanced research projectsagency (darpa), 45degraded service, 9798demand for addresses, estimating, 7175denialofservice attacks, 8deployment of ipv6 technology, 7981recommendations for investing in, 24designdistributed, 4for robustness and reliability, 15, 8283scalable distributed and adaptive, 4041success by, 35devices. see singlepurpose devicesdhcp. see dynamic host configurationprotocoldialup modems, 32differentiated services (diffserv), 102òdigital identity,ó 195digital subscriber line (dsl), 31directory system proposals, 64discounts, 121distributed design, 4scalable, 4041distribution of informationchallenges of, 6peertopeer, 2diversification, trends in the isp market,4648dns. see domain name systemdomain name system (dns), 4, 6, 5864,129, 168administration of, 52caching, 62conflicts in, 23nhierarchy, 59organization of, 58scaling, 5864dsl. see digital subscriber linedynamic host configuration protocol(dhcp), 129, 139improvements to, 78eecommerce, 131132, 147taxation issues in, 132n, 205, 208209email, 5, 131, 146, 192erate program, 215ec. see european commissioneconomics of interconnection. see alsofinancial arrangements forinterconnection; supporting r&drecommendations for investigating, 21edgebased innovation, 4, 36edi. see electronic data interchangeelectronic data interchange (edi), 147148electronic signatures in global andnational commerce act, 204emergency (911) calls, 81, 172, 175emerging internet industries, collisionswith existing industries, 1517, 151176employers, monitoring online activity, 192encryption, 193iplayer, 15end systems, 37the internet's coming of agecopyright national academy of sciences. all rights reserved.228indexendtoend architecture, 4endtoend latency, 103endtoend performance, 214endtoend service, 104endtoend transparency, 138150addressing issues in, 139142keeping the internet open, 149150market and business influences onopenness, 145149nonuniform treatment of bits, 142145enduser equipment, 154entertainment business, 47entry, barriers for innovation, lowering, 42esnet, 110estimating address use and demand, 7175european commission (ec), 189european telecommunications standardsinstitute (etsi), 133addressing interoperation between iptelephony and the pstn, 166european union (eu) regulators, 110, 188189, 204evolutionof internet standards allowinginnovation, 132of models for interconnection throughmultiple providers, 121123of pricing models, 4142scenarios for future, 5152, 162165existing industries, collisions withemerging internet industries, 1517,151176ffair credit reporting act (fcra), 188nfcc. see federal communicationscommissionfcra. see fair credit reporting actfederal communications commission(fcc), 123, 170171opening radiofrequency spectrum, 49ruling on calea and common carrierstatus, 173federal trade commission, 181file transfer protocol (ftp), 134, 141filtering ip traffic, 143publicizing ispsõ policies for, 25financial arrangements for interconnection,115118discounts, 121peer model, 116, 118121transit model, 116firewalls, 143frame relay networks, 100nfree internet service providers, 111, 212ftp. see file transfer protocolfunders, recommendations forcollaboration with, 2021future evolutionscenarios for, 162165and success, 5152ggames, 1gateways, 154155general services administration, 204geographic perspective on internet issues,recommendations for maintainingbroad, 28government paperwork elimination act, 204government policy responses, 2528creating laws and regulations thatestablish overall parameters only,2728focusing laws and regulations onconcerns identified, 27maintaining broad geographicperspective on internet issues, 28monitoring internet phenomena, 2526researching triggers for internetintervention, 26growthin backbone capacity, 45of the isp market, 4648growth of the internet, 510quality of service, 910robustness and reliability, 89scaling challenges, 56scaling up the address space, 68hhardware failures, 87harmonization, challenge of, 28hierarchy, dns, 59hosting providers, 112hourglass model of internet architecture,4, 3638, 127128http, 6, 5657, 130, 134, 143the internet's coming of agecopyright national academy of sciences. all rights reserved.index229iimode phone service, 50iab. see internet architecture boardicann. see internet corporation forassigned names and numbersidentitydefined, 180in public policy, 194199òidentity portability,ó 198ieee. see institute of electrical andelectronics engineersiesg. see internet engineering steeringgroupietf. see internet engineering task forceindustry sectors. see also existing industriesfuture of, 175176infrastructurepublic key, 201202for routing, 85ntrends in upgrading local access, 6, 4950innovation, 1, 124138edgebased, 4, 36evolution of internet standards setting,132the internet as a platform for, 131132to keep the internet interconnected andopen, 1315lowering barriers to entry for, 42institute of electrical and electronicsengineers (ieee), standards from,74, 133integrated services (intserv), qualityofservice mechanisms for, 102103integrated services over specific linklayers, 103interactive chat, 1interconnectionof isps, 108to keep the internet open, 1113new models for, 1to preserve the internet, 107124interconnection mechanisms andagreements, 112118considerations affecting decisions toenter into peering agreements, 118121financial arrangements for, 1112, 115118logical routing, 11, 114115physical, 11, 113114interconnection openness andtransparency, 107150endtoend transparency, 138150maintaining endtoend service throughmultiple providers, 107124openness and innovation, 124138interconnection through multipleproviders, 107124evolution of interconnection models,121123monitoring internet interconnections,123124structure of the internet serviceprovider industry, 109112interfaces, 154internap, 122international data corporation, 51international organization forstandardization (iso), 133international pressures for addresses, 7576international telecommunication union(itu), 35, 133, 136, 204standardization sector (itut),addressing interoperation betweenip telephony and the pstn, 166internet architecture board (iab), 22, 43,135, 139ninternetbased commerce, taxation issuesin public policy, 205209internetbased telephony, 1internet corporation for assigned namesand numbers (icann), 2223, 4344, 5960, 6566internet engineering steering group(iesg), 43, 135internet engineering task force (ietf)groups, 22, 35, 43, 77, 124125addressing interoperation between iptelephony and the pstn, 165166guidelines for, 88standards process, 134135internet industries, emerging, 1517, 151176internet mapping project, 109ninternet marketplace, 4143lowering barriers to entry forinnovation, 42multiple and evolving pricing models,4142tippy markets, 14, 4243, 136internet portals, 61the internet's coming of agecopyright national academy of sciences. all rights reserved.230indexinternet protocol. see ipinternet revolution, 12internet service providers (isps)free, 111growth and diversification of marketsfor, 4648hosting providers, 112interconnecting, 108interpositioning, 144mergers among, 109policies for filtering or prioritizing iptraffic, 25role of, 3, 67structure of the industry forinterconnection through multipleproviders, 109112tier 1, 1213, 110111, 119, 122124valuestratification of customers by, 9,105internet society (isoc), 135internet software consortium, 73internet tax freedom act (itfa), 207internet telephony, 1516, 27, 118interoperation between ip telephony andthe pstn, 165170addressing and number portability,167168groups addressing, 165166robustness considerations, 169170signaling and control and servicecreation, 168169interpositioning, isp, 144intervention, triggers for internet, 26investment, in deployment of ipv6technology, 24iops, 91, 94ipaddressability, 74iplayer encryption, 15ip technology, 31ip telephonyarchitectural contrasts with todayõspstn, 161162architectures for, 155159defined, 1516, 152154groups addressing interoperationinvolving, 165166implications for telephony regulation,170175interoperation with the pstn, 165170ipng. see ipv6 technologyipsec protocol, 15, 142ipv6 technologyfor addressing and configuration, 7779deploying, 78, 53, 70, 7981recommendations for investing in, 24iso. see international organization forstandardizationisoc. see internet societyisps. see internet service providersitfa. see internet tax freedom actitu. see international telecommunicationunionjjitter, 99reducing, 100101kkeeping the internet interconnected andopen, 1015access to the local loop, 11innovation and transparency, 1315interconnection, 1113llarge customers, requirements of, 120nlatency, 99endtoend, 103latencysensitive traffic, 104laws and regulationsrecommendations for creating, 2728recommendations for focusing onconcerns identified, 27layering principle, 36nlevel 3, 163linksfailures of, 86pointtopoint, 108wireless, 104local access infrastructure, 154trends in upgrading, 4950local area networks, wireless, 74local loop, access to, 11logical (routing) interconnection, 114115longterm benefits, of open ip service, 2425lowering barriers, to entry, for innovation,42the internet's coming of agecopyright national academy of sciences. all rights reserved.index231mmalicious attacks, 87managementof addresses, 6566of the domain name system, 6061marketplacebased research, developingand demonstrating internetcapabilities through, 2122marketplace issues, 4143growth and diversification of ispmarkets, 4648influence of endtoend transparency onopenness, 145149low barriers to entry for innovation, 42market pressures, 56multiple and evolving pricing models,4142tippy markets, 14, 4243, 136mci, 110media gateway control, ietf workinggroup, 166mergers, among internet service providers,109, 145, 164microsoft, 43, 145modemscable, 31dialup, 32monitoringinternet interconnections, 123124persistent internet phenomena, 2526monopolistic positions, 1314mooreõs law, 45multihoming, 114nmultimedia applications, 100multiple pricing models, 4142multiple providers, interconnectionthrough, 107124nnaming systems, scaling, 5864nanog. see north american networkoperators groupnational identifier, 195nnational infrastructure protection center,84national laboratory for applied networkresearch (nlanr), 72national science foundation (nsf), 45national security telecommunicationsadvisory committee (nstac), 84,86national telecommunications andinformation administration (ntia),211netscape communications, 43netsizer project, 7273network address translation (nat), 7, 15,6970, 7677, 140142network address translators (nats), 24,70, 141network operators, 88recommendations for collaborationwith, 2021network reliability and interoperabilitycouncil (nric), 23, 96network reliability steering committee,94nnetwork solutions, 65networks. see also internet; local areanetworks; private networksatm and frame relay, 100nbesteffort, 910role of, 34television, 34virtual overlay, 103ònii class profiles,ó 214nlanr. see national laboratory forapplied network researchnorth american network operatorsgroup (nanog), 43, 91, 94nric. see network reliability andinteroperability councilnsf. see national science foundationnsfnet, 41, 55, 108nnstac. see national securitytelecommunications advisorycommitteentia. see national telecommunicationsand information administrationnumber portability, 167168numerical addresses, 64ooffice of management and budget, 188online privacy, 185open access, and cable, 49open ip service, recommendations forlongterm benefits of, 2425the internet's coming of agecopyright national academy of sciences. all rights reserved.232indexopen specifications, 125open standards in the internet, 124138business influences on, 145149critical open standards in the internet,126130domain name system, 129dynamic host configuration protocol,129evolution of internet standards setting,132recommendations for fostering thedevelopment of, 21routing protocols, 129operational errors, 87operators, 2225cable, 49considering longterm benefits of openip service, 2425improving reliability and robustness, 24improving reporting of outages, 9, 23investing in deployment of ipv6technology, 24network, 2021, 88publicizing ispsõ policies for filtering orprioritizing ip traffic, 25optical fiber cables, 3organizationsblocks of addresses held by, 7internet, 4344ns/ep, 82outages, improving reporting of, 9, 23overall parameters, recommendations forcreating laws and regulations whichonly establish, 2728overlay networks, virtual, 103ppacket switching, 45n, 115nparameters, creating laws and regulationsthat only establish overall, 2728peer model financial arrangements forinterconnection, 116, 118121alternatives to, 124peertopeer applications, 47distribution of information, 2peering agreements, 12, 115, 118121performance objectivesendtoend, 214seeking to establish, 28, 106physical interconnection, 113114pki. see public key infrastructureplanning, uncertainty in, 2platform for privacy preferences (p3p),186187, 197pointtopoint links, 108policy approaches. see public policy; socialpolicy concernsportability, addressing, 167168prefixes, routing, 66preserving the internet, 107150endtoend transparency, 138150maintaining endtoend service throughmultiple providers, 107124openness and innovation, 124138presidential decision directive 63, 84presidentõs commission for criticalinfrastructure protection, 84pressures for addresses, international, 7576pricing models, multiple and evolving, 4142prioritizing ip traffic, publicizing ispsõpolicies for, 25privacydefined, 180online, 185policy and regulatory approaches toprotection of, 187190in public policy, 180190setting preferences, 187technical approaches to protection of,186187private networks, 70ip telephony configurations using adedicated, 158problem resolution, 121protocols, 34, 39n. see also individualprotocolsrouting, 129scaling of, 5657security, 142psinet, 118pstn. see public switched telephonenetworkp3p. see platform for privacy preferencespublic debate, about the internet, 2, 2728public exchanges, 113public internet. see also private networksdefined, 30nip telephony configurations using, 158public key cryptography, 201the internet's coming of agecopyright national academy of sciences. all rights reserved.index233public key infrastructure (pki), 201202, 204public policy issues, 177215anonymity, 190194authentication on the internet, 199204identity, 194199privacy, 180190taxation of internetbased commerce,205209universal service, 209215public switched telephone network(pstn), 34, 83, 89architectural contrasts with iptelephony, 161162evolving architecture of, 159160interoperation with ip telephony, 1617,151, 153154, 165170publicizing ispsõ policies, for filtering orprioritizing ip traffic, 25qqualityofservice (qos) mechanisms, 910,102103, 117debate over, 105106differentiated services (diffserv), 102enhanced support for, 79integrated services (intserv), 102103measuring, 120other approaches, 103relative efficiency of, 101technologies for, 100nqueuing, 99qwest, 163rr&d, supporting on scaling challenges andreliability and robustness issues, 1920rateadaptation mechanisms, 100realizing the information future, 126recommendations, 1828collaboration among researchers,funders, and network operators, 2021considering longterm benefits of openip service, 2425creating laws and regulations thatestablish overall parameters only,2728for designers and operators, 2225developing and demonstrating internetcapabilities through marketplacebased research, 2122focusing laws and regulations onconcerns identified, 27fostering the development of openstandards for the internet, 21for government policy responses, 2528improving reliability and robustness, 24improving reporting of outages, 9, 23investigating the economics ofinterconnection, 21investing in deployment of ipv6technology, 24maintaining broad geographicperspective on internet issues, 28monitoring persistent internetphenomena, 2526publicizing ispsõ policies for filtering orprioritizing ip traffic, 25researching triggers for internetintervention, 26supporting r&d on scaling challengesand reliability and robustness issues,1920for the technology base, 1922redundancy, 114nreengineering applications, 6regulatory approaches, 26. see also lawsand regulations; telephony;individual regulatory agenciesto privacy protection, 187190standards enforced under, 125reliability and robustness, 89, 8198in applications, 9293and auxiliary servers, 9394designing for, 8283improving, 24integrating, 9091and more adaptive routing, 8990recommendations for supporting r&don, 1920and reporting outages and failures, 9,23, 9498and vulnerability of the internet toattack, 8489remailers, 192193reporting of outages, recommendations forimproving, 9, 23requests for comments (rfcs), 3435, 134the internet's coming of agecopyright national academy of sciences. all rights reserved.234indexresearchers, recommendations forcollaboration with, 2021resource reservation protocol (rsvp), 102103revenueneutral boundaries, for theinternet, 1213rfcs. see requests for commentsrobustness. see also reliability androbustnessand auxiliary servers, 9394considerations for interoperationbetween ip telephony and the pstn,121, 169170principle of, 3940technologies to improve, 9route advertisements, 69, 114route update messages, 67routingadaptive, 8990of data packets, 4, 12infrastructure for, 85nprotocols for, 32, 89, 129suppliers, 164routing interconnections, 104, 114115routing prefix, 66routing tablesproblems with, 121nscaling and address aggregation for, 6671rsvp. see resource reservation protocolrunning out of addresses. see scarcity ofaddressesssafe harbor guidelines, 189sales tax law, 205nscalability, 4scalable distributed and adaptive design,4041scaling, 5464of capacity, 55of the internetõs naming systems, 5864of protocols and algorithms, 5657recommendations for supporting r&don challenges over, 1920scaling challenges, 46scaling up the address space, 68, 6481deploying an ipv6 solution, 7981ipv6 a potential solution to addressingand configuration, 7779managing addresses, 6566network address translation, 7677routing table scaling and addressaggregation, 6671scarcity of addresses, 7176scaling up the internet, 53106building a better internet, 5354improving quality of service, 98106improving reliability and robustness,8198scarcity of addresses, 8, 7176estimating address use and demand,7175international pressures for addresses,7576security protocols, 142service creationat the edges, 170for interoperation between ip telephonyand the pstn, 168169service delivery, 105services, 36signalingfor interoperation between ip telephonyand the pstn, 168169mechanisms for, 102103simple network management protocol(snmp), 136singlepurpose devices, rise in the use of,5051smart card, 203nsnmp. see simple network managementprotocolsocial policy concerns, 1819, 177215softswitch consortium, addressinginteroperation between ip telephonyand the pstn, 166software failures, 87specific link layers, 103specificationsfor an application programminginterface, 124125for complete applications, 125open, 125speed of light considerations, 98standards. see also open standards in theinternetenforced under regulatory authority,125stanford university, 75stateless transmission, 40the internet's coming of agecopyright national academy of sciences. all rights reserved.index235streaming audio and video, 100, 118subsidy issue, 215nsuccess by design, 35abstract features and principles, 3444distributed design and decentralizedcontrol, 4endtoend architecture, 4òhourglassó architecture, 4, 3638, 127128internet organizations, role of, 4344the internetõs òhourglassó architecture,3638, 127128the robustness principle, 3940scalability, 4scalable distributed and adaptivedesign, 4041supporting r&d, on scaling challenges andreliability and robustness issues, 1920sustaining the growth of the internet, 510quality of service, 910robustness and reliability, 89scaling challenges, 56scaling up the address space, 68ttables. see routing tablestaxation issuesin ecommerce, 132nin public policy on internetbasedcommerce, 205209tcp. see transmission control protocoltdm. see time division multiplexingtechnical approaches, to protectingprivacy, 186187technologiescommunications, 4to improve robustness, 9ip, 31qualityofservice, 100nworkaround, 7technology base, 1922collaboration among researchers,funders and network operators, 2021developing and demonstrating internetcapabilities through marketplacebased research, 2122fostering the development of openstandards for the internet, 21investigating the economics ofinterconnection, 21supporting r&d on scaling challengesand reliability and robustness issues,1920telco to ip architecture, 156telco to telco architecture, 156telcordia, 7273telecommunications act of 1996, 167ntelecommunications and internet protocolharmonization over networks(tiphon), 166telephony, 1517, 151176future of the internet and otherindustry sectors, 175176implications of ip telephony fortelephony regulation, 170175internetbased, 1, 151176interoperation between ip telephonyand the pstn, 165170ip telephony defined, 152154new and evolving architectures for, 154162scenarios for future evolution, 162165television networks, 34thirdgeneration (3g) wireless services, 78, 104tier 1 internet service providers, 1213,110111, 119, 122124time division multiplexing (tdm), 162time warner, america online mergerwith, 145tiphon. see telecommunications andinternet protocol harmonizationover networkstoplevel domains, 6263traffic, latencysensitive, 104transit agreements, 12, 114transit model financial arrangements forinterconnection, 116translation, network address, 15, 6970, 7677, 140142translators, network address, 70, 141transmission control protocol (tcp), 98100, 126, 130131ntransparency. see also endtoendtransparencyto keep the internet interconnected andopen, 1315trends in internet development, 4451growing role for wireless services, 50the internet's coming of agecopyright national academy of sciences. all rights reserved.236indexgrowth and diversification of the ispmarket, 4648growth in backbone capacity, 45rise in the use of singlepurposedevices, 5051upgrading the local accessinfrastructure, 4950voice and data services, 50triggers for internet intervention,recommendations for researching,26troubleshooting, 121trunking, wdmbased, 113trust in cyberspace, 84, 8687uuninet, 76universal service issuesconcerns about, 210nin public policy, 209215upgrading the local access infrastructure,trends in, 4950usage of addresses, estimating, 7175òuse tax,ó 205nusers. see customersvvaluestratification of customers, by isps,9, 105vertical integration, 145146video, streaming, 100virtual overlay networks (vons), 103voice servicesqualityofservice requirements for, 101trends in, 15, 47, 50vons. see virtual overlay networksvulnerability, of the internet to attack, 8489wwap. see wireless access protocolwavelength division multiplexing (wdm),90, 113, 160wdmbased trunking, 113web caching, 38web interface, 5wireless access protocol (wap), 51, 147wireless access protocol forum, 136wireless links, 104wireless servicesgrowing role for, 50local area networks, 74thirdgeneration, 78, 104workaround technologies, 7world wide web, 2, 56world wide web consortium (w3c), 125,134, 136, 148, 186187, 197worldcom, 110w3c. see world wide web consortiumxxml, 148
detailsdistribution, posting, or copying of this pdf is strictly prohibited without written permission of the national academies press. (request permission) unless otherwise indicated, all materials in this pdf are copyrighted by the national academy of sciences.copyright © national academy of sciences. all rights reserved.the national academies pressvisit the national academies press at nap.edu and login or register to get:œ œ 10% off the price of print titlesœ special offers and discountsget this bookfind related titlesthis pdf is available at sharecontributorshttp://nap.edu/4871preserving scientific data on our physical universe: a newstrategy for archiving the nation's scientific informationresources80 pages | 8.5 x 11 | paperbackisbn 9780309051866 | doi 10.17226/4871steering committee for the study on the longterm retention of selectedscientific and technical records of the federal government, national researchcouncilpreserving scientific data on our physical universe: a new strategy for archiving the nation's scientific information resourcescopyright national academy of sciences. all rights reserved.preserving scientific dataon our physical universea new strategy for archiving the nationõsscientific information resourcessteering committee for the study on the longterm retentionof selected scientific and technical records of the federal governmentcommission on physical sciences, mathematics, and applicationsnational research councilnational academy presswashington, d.c.1995preserving scientific data on our physical universe: a new strategy for archiving the nation's scientific information resourcescopyright national academy of sciences. all rights reserved.notice: the project that is the subject of this report was approved by the governing board of the national researchcouncil, whose members are drawn from the councils of the national academy of sciences, the national academy ofengineering, and the institute of medicine. the members of the committee responsible for the report were chosen for theirspecial competences and with regard for appropriate balance.this report has been reviewed by a group other than the authors according to procedures approved by a report reviewcommittee consisting of members of the national academy of sciences, the national academy of engineering, and the instituteof medicine.the national academy of sciences is a private, nonprofit, selfperpetuating society of distinguished scholars engaged inscientific and engineering research, dedicated to the furtherance of science and technology and to their use for the generalwelfare. upon the authority of the charter granted to it by the congress in 1863, the academy has a mandate that requires it toadvise the federal government on scientific and technical matters. dr. bruce alberts is president of the national academy ofsciences.the national academy of engineering was established in 1964, under the charter of the national academy of sciences, asa parallel organization of outstanding engineers. it is autonomous in its administration and in the selection of its members,sharing with the national academy of sciences the responsibility for advising the federal government. the national academyof engineering also sponsors engineering programs aimed at meeting national needs, encourages education and research, andrecognizes the superior achievements of engineers. dr. robert m. white is president of the national academy of engineering.the institute of medicine was established in 1970 by the national academy of sciences to secure the services of eminentmembers of appropriate professions in the examination of policy matters pertaining to the health of the public. the institute actsunder the responsibility given to the national academy of sciences by its congressional charter to be an adviser to the federalgovernment and, upon its own initiative, to identify issues of medical care, research, and education. dr. kenneth i. shine ispresident of the institute of medicine.the national research council was established by the national academy of sciences in 1916 to associate the broadcommunity of science and technology with the academyõs purposes of furthering knowledge and advising the federalgovernment. functioning in accordance with general policies determined by the academy, the council has become the principaloperating agency of both the national academy of sciences and the national academy of engineering in providing services tothe government, the public, and the scientific and engineering communities. the council is administered jointly by bothacademies and the institute of medicine. dr. bruce alberts and dr. robert m. white are chairman and vice chairman,respectively, of the national research council.support for this project was provided by the national archives and records administration (under contract no.namas920019), the national oceanic and atmospheric administration (under contract no. 50dgne300105), and thenational aeronautics and space administration (under contract no. s54040z). the views expressed in this report are those ofthe authors and do not necessarily reflect the views of the sponsoring agencies or subagencies.library of congress catalog card number 9468991international standard book number 030905186xadditional copies of this report are available from:national academy press2101 constitution ave., nwbox 285washington, dc 2005580062462422023343313 (in the washington metropolitan area)b499copyright 1995 by the national academy of sciences. all rights reserved.printed in the united states of americapreserving scientific data on our physical universe: a new strategy for archiving the nation's scientific information resourcescopyright national academy of sciences. all rights reserved.steering committee for the study on thelongterm retention of selected scientific and technical recordsof the federal governmentjeff dozier, university of california, santa barbara, chairshelton alexander, pennsylvania state universitymarjorie courain, consultant (deceased, january 14, 1994)john a. dutton, pennsylvania state universitywilliam emery, university of coloradobruce gritton, monterey bay aquarium research instituteroy jenne, national center for atmospheric researchwilliam kurth, university of iowadavid lide, consultant, gaithersburg, marylandb.k. richard, trwjoan warnowblewett, american institute of physicsnational research council staffpaul f. uhlir, associate executive director, commission on physical sciences, mathematics, andapplicationsmark david handel, program officer, board on atmospheric sciences and climatealice killian, research associate, commission on geosciences, environment, and resourcesjames e. mallory, staff officer, computer science and telecommunications boardscott t. weidman, senior program officer, board on chemical sciences and technologyjulie m. esanu, research assistant, commission on physical sciences, mathematics, and applicationsdavid j. baskin, project assistant, commission on physical sciences, mathematics, and applicationsiiipreserving scientific data on our physical universe: a new strategy for archiving the nation's scientific information resourcescopyright national academy of sciences. all rights reserved.commission on physical sciences, mathematics, and applicationsrichard n. zare, stanford university, chairrichard s. nicholson, american association for the advancement of science, vice chairstephen l. adler, institute for advanced studysylvia t. ceyer, massachusetts institute of technologysusan l. graham, university of california at berkeleyrobert j. hermann, united technologies corporationrhonda j. hughes, bryn mawr collegeshirley a. jackson, department of physicskenneth i. kellermann, national radio astronomy observatoryhans mark, university of texas at austinthomas a. prince, california institute of technologyjerome sacks, national institute of statistical sciencesl.e. scriven, university of minnesotaa. richard seebass iii, university of coloradoleon t. silver, california institute of technologycharles p. slichter, university of illinois at urbanachampaignalvin w. trivelpiece, oak ridge national laboratoryshmuel winograd, ibm t.j. watson research centercharles a. zraket, mitre corporation (retired)norman metzger, executive directorpaul f. uhlir, associate executive directorivpreserving scientific data on our physical universe: a new strategy for archiving the nation's scientific information resourcescopyright national academy of sciences. all rights reserved.prefacein january 1992 the national archives and records administration (nara) sponsored a threedayplanning meeting at the national research council (nrc) to review the issues related to the longtermretention of the federal governmentõs scientific and technical data in the physical sciences. the planningmeeting was organized by the nrcõs commission on physical sciences, mathematics, and applicationsand provided the basis for this study, which was initiated in the fall of 1992 at the request of nara. thenational oceanic and atmospheric administration (noaa) and the national aeronautics and spaceadministration (nasa) subsequently provided additional support.the studyõs steering committee, in consultation with the sponsors, developed the following charge toguide the writing of this report:¥describe the status and plans for the governmentõs archiving of observational and experimentaldata in the physical sciences. identify the principal scientific, technical, information management, andinstitutional issues regarding the permanent archiving of such data.¥assess the commonalities and differences among the case studies provided by the panelsorganized under this study (see below) in order to determine the extent to which common longtermretention policies and appraisal guidelines can be applied to disciplines that collect observational andexperimental data in the physical sciences.¥establish a set of goals, principles, and priorities, as well as generic retention criteria andappraisal guidelines that nara can incorporate into its mission, program, and budget planning.¥suggest mechanisms and processes for nara and noaa to use in implementing a program ofdata appraisal, retention, and preservation, and later in evaluating the effectiveness of the program.¥provide a summary of findings, conclusions, and recommendations.the steering committee formed five panelsñin space sciences, atmospheric sciences, ocean sciences, geosciences, and physics, chemistry, and materials sciencesñto provide their views on the key dataretention issues from different disciplinary perspectives in the physical sciences. these panels each mettwice and produced a set of working papers, which are published separately in study on the longtermretention of selected scientific and technical records of the federal government: working papers(national academy press, washington, d.c., 1995). the work of the panels was invaluable to thevpreserving scientific data on our physical universe: a new strategy for archiving the nation's scientific information resourcescopyright national academy of sciences. all rights reserved.viprefacesteering committee in framing the issues, in forming its conclusions and recommendations, and inproducing its final report.there are several aspects regarding the scope and focus of this report that should be mentioned. thecommittee devoted most of its attention to data stored on electronic media, rather than on paper or onother media. almost all data are now acquired, stored, and distributed electronically. thus, thepreponderance of data archiving problems and their solutions must be considered in this context.nevertheless, much of the advice offered here is equally relevant to data in other formats.the principal focus of this report is on the longterm retention of data in the physical sciences. muchof the discussion, however, includes nearterm data management issues, because effective archivingbegins when the plans for acquiring a data set are made and extends throughout the life cycle of the data.although the focus is exclusively on data in the physical sciences, the committee believes that thedistinctions it has drawn between the experimental and the observational data, as well as the datamanagement principles it has provided, are broadly applicable to most data in the other natural sciences.in addition, the strategic approach adopted by the committee necessarily involves all federal agencies thatacquire and manage physical science data, and not simply the three agencies that sponsored this study.finally, it is necessary to point out that the committee was unable to achieve consensus on one majorrecommendation of the study, namely, the proposal to establish the national scientific informationresource (nsir) federation. appendix b contains the minority opinion of the dissenting committeemember, roy jenne. the rest of the committee members, who strongly support the nsir federationrecommendation, are disappointed by this lack of unanimity and consider many of the assertions in theminority opinion to be based on an erroneous interpretation of what the report actually states orrecommends. we leave that to the reader to judge. nevertheless, we believe that the minority opinioncan perhaps serve a useful purpose by drawing greater attention to these issues and by broadening thediscussion of them among the sponsors of the study, the other science agencies, and the researchcommunity.in conclusion, the committee hopes that its advice will help bring about the changes necessary toeffectively preserve the valuable scientific data on our physical universe.jeff dozierpaul f. uhlirsteering committee chairstudy directorpreserving scientific data on our physical universe: a new strategy for archiving the nation's scientific information resourcescopyright national academy of sciences. all rights reserved.summaryviiacknowledgmentsthe steering committee is very grateful to the many individuals who played a significant role in thecompletion of this study, including the members of the five ad hoc panels that provided conclusions andrecommendations on data archiving from the different physical science disciplines; the individuals whobriefed the steering committee and panels; and members of the national research council (nrc) staffwho worked on various aspects of this study. the steering committee also extends its thanks to trudypeterson and kenneth thibodeau of the national archives and records administration (nara),william turnbull and helen wood of the national oceanic and atmospheric administration (noaa),and joseph king of the national aeronautics and space administration (nasa), from the studyõssponsoring agencies.gerd rosenblatt, of lawrence berkeley laboratory, chaired the physics, chemistry, and materialssciences data panel. the members were r. stephen berry, university of chicago; edward galvin, theaerospace corporation; j.g. kaufman, the aluminum association; kirby kemper, florida stateuniversity; david r. lide, jr., consultant; and edgar westrum, jr., university of michigan. the steeringcommittee gratefully acknowledges the detailed briefings and information provided to this panel bydonald alderson, department of defense nuclear information analysis center; frank biggs, sandianational laboratories; robert billingsley, defense technical information center; mark conrad, nara;suzanne leech, bionetics, inc.; victoria mclane, brookhaven national laboratory; and patriciaschuette, battelle pacific northwest laboratory.the space sciences data panel was chaired by christopher russell of the university of california atlos angeles. the panel members were guiseppina fabbiano, harvardsmithsonian center for astrophysics; sarah kadec, consultant; william kurth, university of iowa; steven lee, university ofcolorado; and r. stephen saunders, jet propulsion laboratory. the steering committee extends itsthanks for the assistance of the following individuals, who provided briefings and other information tothe space sciences data panel: joe allen, national geophysical data center; steven blair, los alamosnational laboratory; joseph bredekamp, nasa; dean bundy, naval research laboratory; daviddeyoung, national optical astronomy observatories; robert frederick, air force space forecastcenter; joseph king, national space science data center; knox long, space science telescopeinstitute; guenther riegler, nasa astrophysics division; thomas smith and jud stailey, air forceenvironmental technical applications center; earl tech, los alamos national laboratory; raymondwalker, university of california at los angeles; and james willet, nasa space physics division.viipreserving scientific data on our physical universe: a new strategy for archiving the nation's scientific information resourcescopyright national academy of sciences. all rights reserved.viiiprefacewerner baum, of florida state university, was the chair of the atmospheric sciences data panel.the members were marjorie courain, consultant (deceased, january 14, 1994); william haggard,climatological consulting corporation; roy jenne, national center for atmospheric research; kellyredmond, desert research institute; and thomas vonder haar, colorado state university. the steeringcommittee gratefully acknowledges the diverse and substantial inputs provided by the following individuals to the atmospheric sciences data panel: larry baume, nara; thomas boden, carbon dioxideinformation and analysis center; dean bundy, naval research laboratory; donald collins, nasa;richard davis, national climatic data center, p.c. hariharan, johns hopkins university; and geraldstokes, pacific northwest laboratories.the ocean sciences data panel was chaired by bruce gritton, monterey bay aquarium researchinstitute. the members were richard dugdale, university of southern california; thomas duncan,university of california at berkeley; robert evans, rosenstiel school of marine and atmosphericscience; terrence joyce, woods hole oceanographic institution; and victor zlotnicki, jet propulsionlaboratory. the steering committee extends its thanks for the briefings and other information providedto the ocean sciences data panel by larry baume, nara; donald collins and susan digby, jetpropulsion laboratory; ronald fauquet, noaa; ted tsui, naval research laboratory; and r.s.winokur, office of naval research.the geoscience data panel was chaired by theodore albert, a private consultant. the memberswere shelton alexander, pennsylvania state university; sara graves, university of alabama in huntsville; david landgrebe, purdue university; and soroosh sorooshian, university of arizona. thesteering committee gratefully acknowledges the information provided at the meetings of the geosciencesdata panel by the following individuals: roger barry, national snow and ice data center; danielcavanaugh, u.s. geological survey; donald collins, jet propulsion laboratory; katrin douglass,southern california earthquake center data center; william draegar, u.s. geological survey; johndwyer, nara; claire henson, national snow and ice data center; herb meyers, national geophysicaldata center; ron weaver, national snow and ice data center; and thomas yorke, u.s. geologicalsurvey.finally, the steering committee is grateful to the staff of the national research council: paul f.uhlir, associate executive director of the commission on physical sciences, mathematics, and applications, who served as study director; mark david handel and theresa fisher (board on atmosphericsciences and climate), alice killian (commission on geosciences, environment, and resources), jamese. mallory (computer science and telecommunications board), and scott t. weidman and taœaspencer (board on chemical sciences and technology), who provided staff support for the five panels;julie m. esanu, for the program assistance provided to the steering committee and panels and for thepreparation of the final manuscript; david baskin, for his work on preparing the final manuscript; lizpanos, for coordinating the report review; and roseanne price, who edited the final manuscript.viiiacknowledgmentspreserving scientific data on our physical universe: a new strategy for archiving the nation's scientific information resourcescopyright national academy of sciences. all rights reserved.summaryixcontentsixsummary 11introduction 10imperatives for preserving data on our physical universe, 11a new future for scientific data, 122the challenge: preservation and use of scientific data 13experimental laboratory data, 13observational data in the physical sciences, 15summary of major issues, 293retention criteria and the appraisal process 33retention criteria, 33other elements of the appraisal process, 39recommendations, 404the opportunities: the relationship of technologicaladvances to new data use and retention strategies 42enabling technologies and related developments, 43opportunities for new organizational structures, 475a new strategy for archiving the nationõs scientificand technical data49fundamental principles for longterm data retention, 50the proposed national scientific information resource federation, 51recommendations for the creation of the nsir federation, 55recommendations specifically for nara, 57recommendations specifically for noaa, 59references62appendix alist of acronyms64appendix bminority opinion66preserving scientific data on our physical universe: a new strategy for archiving the nation's scientific information resourcescopyright national academy of sciences. all rights reserved.preserving scientific data on our physical universe: a new strategy for archiving the nation's scientific information resourcescopyright national academy of sciences. all rights reserved.this study is dedicatedin fond memory ofmarjorie courain.preserving scientific data on our physical universe: a new strategy for archiving the nation's scientific information resourcescopyright national academy of sciences. all rights reserved.preserving scientific data on our physical universe: a new strategy for archiving the nation's scientific information resourcescopyright national academy of sciences. all rights reserved.1summaryscientific data reflect both the organization and the chaos of the natural world. they stimulate us todevelop concepts, theories, and models to make sense of the patterns they represent. the resultingabstractions are the formal and systematic ideas that constitute the understanding of relationshipsbetween causes and consequences, and perhaps may enable prediction of future sequences of events.because scientists transform data from the material world into ideas, the observations of objects andprocesses in the physical world are the stimuli of scientific thought. data are thus the seeds of scientificideas.there are strong motivations for preserving scientific observations:¥many observations about the natural world are a record of events that will never be repeatedexactly. examples include observations of an atmospheric storm, a deep ocean current, a volcaniceruption, and the energy emitted by a supernova. once lost, such records can never be replaced.¥observed data provide a baseline for determining rates of change and for computing the frequency of occurrence of unusual events. they specify the observed envelope of variability. the longer therecord, the greater our confidence in the conclusions we draw from it.¥a data record may have more than one life. as scientific ideas advance, new concepts mayemergeñin the same or entirely different disciplinesñfrom study of observations that led earlier todifferent kinds of insights. new computing technologies for storing and analyzing data enhance thepossibilities for finding or verifying new perspectives through reanalysis of existing data records. thus,the relative importance of data, both current and historical, can change dramatically, often in entirelyunanticipated directions.¥the substantial investments made to acquire data records justify their preservation. the cost ofpreservation will almost always be small in comparison with the cost of observation. because we cannotpredict which data will yield the most scientific benefit in years ahead, the data we discard today may bethe data that would have been invaluable tomorrow.the assembled record of observational data thus has dual value: it is simultaneously a history ofevents in the natural world and a record of human accomplishment. the history of the physical world isan essential part of our accumulating knowledge, and the underlying data form a significant part of thatheritage. they also portray a history of our scientific and technological development.preserving scientific data on our physical universe: a new strategy for archiving the nation's scientific information resourcescopyright national academy of sciences. all rights reserved.2preserving scientific data on our physical universethere are numerous socioeconomic reasons, in addition to the compelling scientific and historicalmotivations, for the longterm retention of observational, as well as certain types of experimental, data.for example, historical climate data have had welldocumented uses in a broad range of applications inthe manufacturing, energy, agriculture, transportation, communications, engineering, construction, insurance, and entertainment sectors. such applications are common as well for other types of observational data on the earthõs environment. experimental data in the physical sciences also have many industrialand other practical uses.today we can foresee the possibility of using the national resource of scientific data more advantageously than ever before as technological advances open new vistas for managing scientific information.advances in data storage technologies make the longterm retention of virtually all data both feasible andaffordable. the existence of the internet and of the emerging national information infrastructure (nii)enables nationwide sharing and application of data that reside in appropriately configured databases.our new power to store, distribute, and access data and information is changing the way we work andthink. however, the communities involved in the creation, retention, and use of scientific data about thephysical world are not optimally organized. they commonly work toward disparate goals, are not wellconnected, and do not take full advantage of technological and conceptual advances in data managementand communication. an entirely new approach to the longterm preservation of scientific data is nowboth feasible and essential. it must take advantage of advancing technology and of distributed communications and management structures to empower both the creators and the users of such data.this study, performed at the request of the national archives and records administration (nara),and partially supported by the national oceanic and atmospheric administration (noaa) and thenational aeronautics and space administration (nasa), identifies the major issues regarding efforts toarchive and use data in the physical sciences, establishes retention criteria and appraisal guidelines forthose data, reviews important technological advances and related opportunities, and proposes a newstrategy to help ensure access to the data by future generations.the challenge of effective preservationand use of scientific datathe results of scientific research are disseminated in this country through a hybrid system thatincludes professional society and other notforprofit publishers, the commercial sector, and the government. the formal journals are published largely by the professional society and commercial sectors,while government agencies manage less formal reports (gray literature). secondary abstracting andindexing services provide access to this literature, increasingly by electronic means. while there arestrains in this system because of rising costs, increasing workload, and issues related to the protection ofintellectual property, it has served u.s. science well and has been an invaluable link in the process oftranslating scientific advances into further advances, useful technology, and economic benefits.the current system, however, is not well suited to handle the scientific and technical electronicdatabases that are the focus of this study. the cost of maintaining these databases is typically too great tobe covered by user fees; instead these databases must be considered part of the national scientificheritage. some government agencies have accepted responsibility for maintaining and disseminating thedata resulting from their research and development. in some cases, this system is working reasonablywell, but in others there are problems even with providing current access. archiving for the long termraises questions in all cases, however.a general problem prevalent among all scientific disciplines is the low priority attached to datamanagement and preservation by most agencies. experience indicates that new research projects tend toget much more attention than the handling of data from old ones, even though the payoff from optimalutilization of existing data may be greater.preserving scientific data on our physical universe: a new strategy for archiving the nation's scientific information resourcescopyright national academy of sciences. all rights reserved.summary3with regard to laboratory data, government programs have existed since the 1960s to compile resultsfrom the world scientific literature, to check the data carefully, and to prepare databases of criticallyevaluated data. despite chronic underfunding, these programs have produced databases of lasting valueto the nation, and the government investment in creating and maintaining these databases has been repaidmany times over.in the area of observational databases, the situation is mixed. federal agencies collect large amountsof observational data, which in many cases are continuously added to the available record of earth andspace processes. the data sets resulting from these activities are sometimes welldocumented andmaintained in readily accessible form; in many other cases, however, while the data are saved, they areexceedingly difficult or impossible to access or use, and thus are effectively unavailable.the most important deficiencies are in the documentation, access, and longterm preservation of datain usable form. insufficient documentation is a generic problem that affects, in varying degrees, all theclasses of data addressed in this study. furthermore, few of the federal data centers can give adequateattention to longterm archiving because they are stretched thin by current demands and inadequateresources. even the data that are archived may become inaccessible because they are not regularlymigrated to new storage media as the hardware and software used to access the data become obsolete orinoperable.another major problem inhibiting access to data is the lack of directories that describe what data setsexist, where they are located, and how users can access them. in many cases the existence of the data isunknown outside the original scientific groups, and even if known, there frequently is not enoughinformation for a potential user to assess their relevance and usefulness. the lack of adequate directoriesadversely affects the exploitation of our national data resources and leads to unnecessary duplication ofeffort.a significant fraction of the archived scientific data is held by the federal agencies that collected thedata as part of their mission. however, a large amount of valuable scientific data gathered with federalfunds is never archived or made accessible to anyone other than the original investigators, many of whomare not government employees. in many instances, the organizations and individuals that receivegovernment contracts or grants for scientific investigations are under no obligation to retain the datacollected, or to place them in an accessible archive at the conclusion of the project. thus, data sets thatcommonly are gathered at great expense and effort are not broadly available and ultimately may be lost,squandering valuable scientific resources and much of the public investment spent in acquiring them.clearly, there is a great need for the agencies to get more return on their investment in science by thesimple expedient of making the data collected under their auspices accessible to others.finally, the holdings of scientific and technical data by nara in electronic or any other form arevery small in comparison with the data holdings of the federal agencies and the organizations supportedby them. moreover, naraõs budget for its center for electronic records, which has the formalresponsibility for archiving all types of federal electronic records, was only $2.5 million in fy 1994, abudget lower than that of many of the individual agency data centers reviewed by the committee in thisstudy. given naraõs current and projected level of effort for archiving electronic scientific data, it isobvious that nara will be unable to take custody of the vast majority of these scientific data sets.therefore, a coordinated effort involving nara, other federal agencies, certain nonfederal entities, andthe scientific community is needed to preserve the most valuable data and ensure that they will remainavailable in usable form indefinitely. the challenge is to develop data management and archivingprocedures that can handle the rapid increases in the volumes of scientific data, and at the same timemaintain older archived data in an easily accessible, usable form. an important part of this challenge is topersuade policymakers that scientific data and information are indeed a precious national resource thatshould be preserved and used broadly to advance science and to benefit society.preserving scientific data on our physical universe: a new strategy for archiving the nation's scientific information resourcescopyright national academy of sciences. all rights reserved.4preserving scientific data on our physical universeretention criteria and the appraisal processthe national archives and records administration appraises records on the basis of their informational and evidential value. it is concerned with records of longterm value, those records that willprobably have value long after they cease to have immediate, or primary, uses. the value of scientificand technical data is primarily informational and is based on the scientific content of the records, ratherthan on the evidence they provide concerning the activities of the agency that collected or created them.recommendationsthe recommendations below regarding the retention criteria and appraisal process should be appliedñby those responsible for stewardshipñto all physical science data. similar criteria and appraisalguidelines must be developed for data in other disciplines. this is a topic of primary concern not only tonara, noaa, and nasa, but to all scientists, data managers, and archivists who work with suchrecords.as a general rule, all observational data that are nonredundant, useful, and documented wellenough for most primary uses should be permanently maintained. laboratory data sets arecandidates for longterm preservation if there is no realistic chance of repeating the experiment, orif the cost and intellectual effort required to collect and validate the data were so great that longterm retention is clearly justified. for both observational and experimental data, the followingretention criteria should be used to determine whether a data set should be saved: uniqueness,adequacy of documentation (metadata), availability of hardware to read the data records, cost ofreplacement, and evaluation by peer review. complete metadata should define the content, formator representation, structure, and context of a data set.the appraisal process must apply the established criteria while allowing for the evolution ofcriteria and priorities and must be able to respond to special events, such as when the survival ofdata sets is threatened. all stakeholdersñscientists, research managers, information managementprofessionals, archivists, and major user groupsñshould be represented in the broad overarchingdecisions regarding each class of data. the appraisal of individual data sets, however, should beperformed by those most knowledgeable about the particular datañprimarily the principalinvestigators and project managers. in some cases, they may need to involve an archivist orinformation resources professional to assist with issues of longterm retention.classified data must be evaluated according to the same retention criteria as unclassified datain anticipation of their longterm value when eventually declassified. evaluation of the utility ofclassified data for unclassified uses needs to be done by stakeholders with the requisite clearancesto access such data.opportunities created by technological advances fornew data use and retention strategiesrapid progress in information technology continually alters both the quantity and the quality ofscientific information and periodically stimulates fundamental modification of data management andarchiving strategies. recent technological advances have enabled new methods and strategies for datastorage and retrieval and have created better ways of connecting users to data resources and to each other.moreover, the evolving technologies are catalysts for revising organizational structures to managedistributed scientific data archives much more effectively.table s.1 provides a summary of new technologies and related developments that enable a newstrategy for the management of scientific and technical data. these advances in information technologiespreserving scientific data on our physical universe: a new strategy for archiving the nation's scientific information resourcescopyright national academy of sciences. all rights reserved.summary5table s.1 new technologies and related developments that enable a new strategy for the management ofscientific and technical datanew technology trendsand related developmentskey featureswhat is enabled?highperformance computer networksdistributed functions; rapidlocation of databases and archivesdelivery of large data volumeswhere best managed; collaborativework; distributed organizations;distributed responsibilitylow and declining cost of storageinexpensive backup; continuallydeferral of archiving decisions; trust indeclining cost; ease of migration distributed management due to safe storage backupadvanced data managementability to rigorously and formallymore complex data structures (othermanage diverse data typesthan òflat filesó) handled in archives,with great potential advantageschanging requirements forability of personnel with lowerability to entrust scientific datainformation technology professionals technical skills to succeed inmanagement in a distributed data management rolesenvironmenthigh reliability of technology componentsavailability of better componentsreduced cost and effort in dataand connections; reducedmigration; trusted connections forprocurement and operations costscommunication and collaborationdevelopment and acceptance of standardsagreement on terms, interfaces,reduced effort to communicate andmedia, proceduresapply results of others; ability toconcentrate on mission issues andnot on technology supportand data management support the creation of a highly distributed, federated management structure forour nationõs scientific information resources.a new strategy for archivingthe nationõs scientific and technical datain order to respond adequately to the imperatives for preserving data about the physical universe andto take advantage of the technological advances described above, the federal government should createan integrated and adaptive infrastructure and related processes for providing ready access to the nationalresource of scientific and technical data and related information. such an effort must support the needs ofdata originators, users, and custodians across all phases of the data life cycle, from origin to use by futuregenerations. the committee believes that the following principles should guide the effort of thegovernment agencies in the longterm retention of scientific and technical data:¥data are the lifeblood of science and the key to understanding this and other worlds. as such,data acquired in federal or federally funded endeavors, which meet established retention criteria, are acritical national resource and must be protected, preserved, and made accessible to all people for alltime.¥the value of scientific data lies in their use. meaningful access to data, therefore, merits as muchattention as acquisition and preservation.preserving scientific data on our physical universe: a new strategy for archiving the nation's scientific information resourcescopyright national academy of sciences. all rights reserved.6preserving scientific data on our physical universe¥adequate explanatory documentation, or metadata, can eliminate one of todayõs greatest barriers to use of scientific data.¥a successful archive is affordable, durable, extensible, evolvable, and readily accessible.¥the only effective and affordable archiving strategy is based on distributed archives managed bythose most knowledgeable about the data.¥planning activities at the point of data origin must include longterm data management andarchiving.the proposed national scientific information resource federationthe committee believes that the federal government should create a national scientific informationresource federationñan evolutionary and collaborative network of scientific and technical data centersand archivesñto take on the challenge of providing effective access to and preservation of important dataand related information. such an initiative would begin to exploit fully our nationõs significantinvestment in the physical (and other) sciences and the data acquired with that investment. severalcritical concepts must govern any federated management structure for it to function properly (handy,1992):¥subsidiarityñthe power is assumed to lie with the subordinate units of an organization. powercan be relinquished, but not taken away. the subordinate units typically are best qualified to makeoperational decisions that directly affect them and that they will be implementing. the central management is allowed only those powers needed to ensure that the subordinates do not damage the organization. it is clear that the strengths of the current system for managing scientific and technical data andinformation in the united states are distributed among a number of diverse data centers and archives,both within and outside the government. a successful federation of these existing institutions wouldrecognize that they are the locations of expertise on their respective data holdings. thus the centralorganization should be small and should not micromanage the daytoday operations of the subsidiaryorganizations.¥pluralismñthe members are interdependent. in a federation, the individual subsidiary organizations recognize the advantages of belonging to the federation, because of products or services that can beobtained from other elements in the federation. the existence of many specialized data centers andarchives, as well as the possibility of creating new ones in a networked environment, can offer significanteconomies of scale and improved sharing of ideas and expertise. what is good for the subsidiary elementalso should be good for the whole. pluralism, coupled with subsidiarity, guarantees a measure ofdemocracy in the federation.¥standardizationñinterdependence requires compatible languages, communications, basic rulesof conduct, and units of measurement. these elements may be summarized as technical and proceduralstandardization. standards that are developed by consensus of the subsidiary elements (e.g., theparticipating data centers, archives, and researchers) are widely recognized as essential to the successfulmanagement of data.¥separation of powers (responsibilities)ña system of checks and balances is necessary to ensurethat the central authority does not take on unnecessary power. this principle must be incorporated intothe federationõs organizational structure.¥strong leadershipñthe central coordinating element or executive office must act as the standardbearer, promoting the federationõs established goals and objectives while reminding the subsidiaryorganizations of the importance of carrying out their responsibilities.a federated data management system would be consistent with the goal of the national informationinfrastructure to distribute information resources broadly throughout our society. the technology ispreserving scientific data on our physical universe: a new strategy for archiving the nation's scientific information resourcescopyright national academy of sciences. all rights reserved.summary7available to make a fully networked, but highly distributed system of data centers and archives bothfeasible and desirable. such a system would be efficient in providing access to scientific data andinformation to a large number of potential users and would maximize the governmentõs return on the verylarge investment that initially went into acquiring those data. from an organizational standpoint, afederated management structure would allow the disparate elements to continue to specialize in what theyeach do best and to fulfill their individual organizational mandates, while providing some efficiencies ofscale and political leverage in addressing the most pressing issues. the committee believes this approachis especially timely and important in an era of federal government budget reductions.recommendationsthe committee thus recommends that the federal government take the following steps for adequatelypreserving and providing access to data about our physical universe:adopt the national scientific information resource (nsir) federation concept as an integralpart of the national information infrastructure (nii). this concept must encompass not only anelectronic network, but also individuals, organizations, communities, data resources, procedures,guidelines, and associated activities of data generation, management, custodianship, and use. thensir federation thus should provide the means for defining a coherent approach to managing the lifecycle of scientific data. this approach should be developed and implemented through consensus ofcollaborating organizations with diverse and autonomous missions. the interagency global changedata and information system is an example of a prototype nsir federation, focused on data for aspecific set of interdisciplinary science problems. the nsir federation would build on such efforts,providing for better coordination and interaction among them, and would help organize fledgling effortsto preserve and provide broad access to data in other disciplines.the administration should take the steps necessary to fully define and create the nsirfederation. there are at least two potential focal points within the administration for planning such anactivity. these are the interagency information infrastructure task force for the nii and the nationalscience and technology council. a convocation of representatives from the scientific, data andinformation management, and archiving communities would be a good way to help define and inauguratethis initiative.following the formal authorization by the federal government for creating the nsir federation, the principal parties, including nara and noaa, should conclude agreements for theimplementation of a distributed archive system. the system should involve all relevant institutions, including nongovernmental entities that are funded by the federal government or thatmaintain data that were acquired with federal funds. as a general principle, data collected by anagency should remain with that agency indefinitely. the committee recognizes that this recommendation may require significant operational changes for agencies other than noaa, and even some changeswith respect to noaaõs data activities. furthermore, the associated agencies in the nsir federationmust work together, under the lead of a small executive office with the expertise to establish datamanagement guidelines and minimum criteria for adequate metadata that could be applied across theentire federation. the executive office could be either a highlevel interagency coordinating committeeor a new office at an appropriate federal agency, such as the national science foundation, which has abroad scientific and technical as well as communication mandate. in any case, the executive officeshould resist the typical tendency toward bureaucratic accretion of power, personnel, and resources, aswell as the tendency to consolidate and centralize data holdings. a management council consisting ofrepresentatives of the member organizations should be created to help ensure that the executive officefunction remains fully responsive to all members of the federation.preserving scientific data on our physical universe: a new strategy for archiving the nation's scientific information resourcescopyright national academy of sciences. all rights reserved.8preserving scientific data on our physical universedata access and preservation services should be implemented on the most costeffective basispossible for the federation. for example, one institution should provide a service to one or more otherinstitutions in order to exploit potential economies of scale and focal points of expertise. this measuremight increase the cost to the providing institution, but would decrease the overall cost to the federation,the government, and the taxpayer.the institutions belonging to the nsir federation should develop a process for collaboratingeffectively on specific initiatives. this process should provide a mechanism to define and prioritize datamanagement and preservation initiatives, to establish the required agreements between collaboratingorganizations, and to secure funding for each initiative. each participating organization would contributeto the federation according to its particular strengths and in a manner consistent with the founding charter.in addition, an independent advisory board consisting of experts from user groups should be formed insupport of each initiative.the nsir federation should develop a national resource of information technology that isconsistent with its chartered objectives and that can be effectively distributed to institutions thatmust manage data. these technologies would include complete products, designs, guidelines, standards, and methodologies. a related longterm technology strategy, or òtechnology navigationó function,should be developed to help guide these efforts.the nsir federation should institute an independently managed process for awarding nsircertification to member scientific institutions and their data and information systems on the basisof welldefined criteria and standards. the certification process should be managed by a nongovernmental, notforprofit organization, which would receive technical guidance from the participatingfederal agencies. the certification needs to have credibility in the community, so that nonmemberinstitutions will aspire to attain certification and have it tagged to their products. the certification alsoshould be something that commercial valueadded providers seek to increase the credibility of theirproducts.it also is important for the committee to state what the nsir federation should not be. it should notbecome an expensive bureaucratic entity. the executive office must not impose any standards orinformation technologies from above that have not been validated through a consensus process of themember organizations. finally, the executive office must not attempt to micromanage the operations ofthe participants, nor should it have any direct control over their budgets and funding allocations.recommendations specifically for naraalthough nara has a legislative mandate to preserve federal records, it cannot today, nor will itlikely ever be able to, act as the custodian of most physical science data. the data volume is too great inrelation to the very low funding appropriated to nara, the nara staff do not have the specializedscientific knowledge, the interagency linkages are not in place, and a huge infrastructure similar to thatwhich already exists at other agencies would need to be duplicated by nara. in addition, thedesignation of a federal record is sometimes irrelevant to the archival process for scientific and technicaldata, and many data of longterm interest do not meet the existing definition of a federal record.* hence,*òô[federal] recordsõ includes all books, papers, maps, photographs, machine readable materials, or other documentarymaterials, regardless of physical form or characteristics, made or received by an agency of the united states government underfederal law or in connection with the transaction of public business and preserved or appropriate for preservation by that agencyor its legitimate successor as evidence of the organization, function, policies, decisions, procedures, operations, or other activitiesof the government or because of the informational value of the data in themó (44 u.s.c. 3301).preserving scientific data on our physical universe: a new strategy for archiving the nation's scientific information resourcescopyright national academy of sciences. all rights reserved.summary9nara has a special role as a partner in the archiving process for scientific and technical data sets that isdifferent from its traditional role as the nationõs archives.the committee makes the following specific recommendations to nara in addition to those madeelsewhere in this report:nara should strengthen its liaison with each federal agency that produces scientific andtechnical data to ensure that appropriate attention is devoted to their longterm retention in adistributed storage environment.nara should form standing advisory committees with managers of scientific data, historians,and scientific researchers to address the retention and appraisal of scientific and technical datacollections and related issues.nara should collaborate with other agencies that maintain longterm custody of data todevelop an effective access mechanism to these distributed archives. the initial step should focuson locator systems and evolve toward a transparent access system.finally, nara should work with the scientific community and potential sources of scientificdata to develop adaptable performance criteria for data formats and media, rather than mandatingnarrow and inflexible product standards.recommendations specifically for noaaas the largest holder of earth sciences data in the united states, noaa has a vast amount ofscientific data stored at a number of facilities across the country. noaa thus has an especially importantrole in the preservation of our nationõs observational data on the physical environment. the committeemakes the following specific recommendations to noaa:noaa should place a higher priority on documenting and establishing directories of its dataholdings.noaa, with the active cooperation of nara, should lead efforts to better define technologyindependent standards for archiving, storing, and transmitting the data within its purview.finally, noaa, as well as every other federal science agency, should ensure that:¥all its data are shared and readily available;¥it fulfills its responsibility for quality control, metadata structures, documentation, andcreation of data products;¥it participates in electronic networks that enable access, sharing, and transfer of data; and¥it expressly incorporates the longterm view in planning and carrying out its data management responsibilities.the creation of the committeeõs proposed nsir federation would help provide a collaborativemechanism and more sustained peer pressure to meet these objectives, and thus enhance the value ofscientific and technical data and information resources to the nation.preserving scientific data on our physical universe: a new strategy for archiving the nation's scientific information resourcescopyright national academy of sciences. all rights reserved.10preserving scientific data on our physical universe101introductionstanding at the intersection of past and future, we humans are fascinated with the events of yesteryearand intrigued with what tomorrow will bring. our prehistoric ancestors began the process of recordingaspects of the environment that were important to them (marshack, 1985; boorstin, 1992). today we arecurious about many more worlds, ranging from those of atomic size to those of cosmic scale. withinstruments on earth and in space, we seek to capture views of reality that will help us understand natureand our relationship to it.scientific data reflect both the organization and the chaos of the natural world. they stimulate us todevelop concepts, theories, and models to make sense of the patterns they represent. the resultingabstractions are the product of scientific endeavor, the goal being to develop the formal and systematicideas that constitute the understanding of relationships between causes and consequences and perhapsmay enable prediction of future sequences of events. because scientists transform data from the materialworld into ideas, the observations of objects and processes in the physical world are the stimuli ofscientific thought. data are thus the seeds of scientific ideas.science generally works by proceeding from data to understanding through a process of organizingthe data and analyzing their implications. the following definitions, adapted from setting priorities forspace research: opportunities and imperatives (nrc, 1992a), indicate how the process works:¥data are numerical quantities or other factual attributes derived from observation, experiment, orcalculation.¥information is a collection of data and associated explanations, interpretations, or other textualmaterial concerning a particular object, event, or process.¥knowledge is information organized, synthesized, or summarized to enhance comprehension,awareness, or understanding.¥understanding is the possession of a clear and complete idea of the nature, significance, orexplanation of something; it is the power to render experience intelligible by ordering particulars underbroad concepts.this process is cyclical. new data confirm or refute existing theories and stimulate new understanding, which generates new and deeper questions that often need entirely new sets of observations to beginthe process of answering them. new understanding also leads to increased technological capability, andpreserving scientific data on our physical universe: a new strategy for archiving the nation's scientific information resourcescopyright national academy of sciences. all rights reserved.introduction11that in turn makes new observations possible and again allows us to contemplate more sophisticatedquestions.thus observations and scientific progress are intertwined; data from the physical world ensure thatscience is founded on reality as we try to answer the unending òhowó and òwhyó questions that are part ofbeing human. the answers become understanding that enables us to develop schemes for predicting ornot being surprised by future events. and understanding, we hope, ultimately leads to wisdom about ourinteractions with the world around us.imperatives for preserving data on our physical universethe scientific reasons for preserving data derive from the fact that observations, knowledge, andunderstanding are cumulative. thus we believe that the more complete the record, the more we canextract from it.many observations about the natural world are a record of events that will never be repeated exactly.examples include observations of an atmospheric storm, a deep ocean current, a volcanic eruption, andthe energy emitted by a supernova. once lost, such records can never be replaced.observed data provide a baseline for determining rates of change and for computing the frequency ofoccurrence of unusual events. the longer the record, the greater our confidence in the conclusions wedraw from it. our traditional observational records have portrayed frozen instants of reality. ifpreserved, they will continue to provide insights, but if neglected, they will melt away.a data record is also worth preserving because it may have more than one life. as scientific ideasadvance, new concepts emergeñin the same or entirely different disciplinesñfrom study of observations that led earlier to different kinds of insights. new computing technologies for storing and analyzingdata enhance the possibilities for finding or verifying new perspectives through reanalysis of existingdata records. thus, the relative importance of data, both current and historical, can change dramatically,often in entirely unanticipated directions. this means that the reanalysis of data, even in the distantfuture, may bring new understanding, which will again increase the value of those data over that whichwe might have assigned to them at the time of their archiving. finally, the substantial investments madeto acquire data records usually justify their preservation. the cost of preservation will almost always besmall in comparison with the cost of observation. because we cannot predict which data will yield themost scientific benefit in years ahead, the data we discard today may be the data that would have beeninvaluable tomorrow.the assembled record of observational data thus has dual value: it is simultaneously a history ofevents in the natural world and a record of human accomplishment. the history of the physical world isan essential part of our accumulating knowledge, and the underlying data form a significant part of thatheritage. they also portray a history of our scientific and technological development.with appropriate explanatory documentation, often referred to as metadata, the data demonstrate theincreasing sophistication of our attempts to understand our natural surroundings and the technologicalcapabilities we apply to the task. preserved for study by future generations, the data will speak across theyears about what we tried to do, where we succeeded, and where we failed. with increasing capabilitiesfor analyzing and conceptualizing patterns in data, those who follow may find in our archived dataimportant clues that we could not or did not see. at the same time, our descendants will be grateful thatwe preserved a sufficiently long history of their world that they can make important decisions about theirown future.there are numerous socioeconomic reasons, in addition to the compelling scientific and historicalmotivations, for the longterm retention of observational, as well as certain types of experimental, data.for example, historical climate data have had welldocumented uses in a broad range of applications inmanufacturing, energy, agriculture, transportation, communications, engineering, construction, insurance, and entertainment (ota, 1994). such applications are common for other types of observationalpreserving scientific data on our physical universe: a new strategy for archiving the nation's scientific information resourcescopyright national academy of sciences. all rights reserved.12preserving scientific data on our physical universedata on the earthõs environment. experimental data in the physical sciences also have many industrialand other practical uses. additional examples of the longterm uses of the various physical science dataare provided in the next chapter. a new future for scientific datathe collections of scientific data acquired with government and private support are the foundationfor our understanding of the physical world and for our capabilities to predict changes in that world. inthe years ahead, the volumes of those collections of data will increase dramatically. they will stimulateadvances in our scientific understanding and in our applications of that understanding to pursue importantnational goals. the scientific data in federal, state, and private databases thus constitute a critical nationalresource, one whose value increases as the data become more readily and broadly available.today, we can foresee the possibility of using the national resource of scientific data more advantageously than ever before, as technological advances open new vistas for managing and accessingscientific information. growing computational power enables new approaches to the analysis, management, and application of data. advances in data storage technologies make the longterm retention ofvirtually all data both feasible and affordable. the existence of the internet and of the emerging nationalinformation infrastructure (nii) enable unprecedented nationwide sharing and application of data thatreside in appropriately configured databases. automatic search procedures, file transfer capabilities, andthe accelerating use of the world wide web functions on the internet illustrate the power of thecontemporary technology. it is important to note that these enabling technologies have emerged in ashort time span; equally rapid advances can be anticipated in the years ahead, which will further facilitatethe search for and access to the nationõs data resources.our new power to store and distribute data and information is changing the way we work and think.however, the communities involved in the creation, retention, and use of scientific data about thephysical world are not optimally organized. they commonly work toward disparate goals, are not wellconnected, and do not take full advantage of technological and conceptual advances in data managementand communication. an entirely new approach to the longterm preservation of scientific data is nowboth feasible and essential. it must take advantage of advancing technology and of distributed communications and management structures to empower both the creators and the users of such data.this study identifies the major issues regarding existing efforts to archive and use data in the physicalsciences, establishes retention criteria and appraisal guidelines for those data, reviews important technological advances and related opportunities, and proposes a new strategy to ensure access to the data byfuture generations.preserving scientific data on our physical universe: a new strategy for archiving the nation's scientific information resourcescopyright national academy of sciences. all rights reserved.132the challenge:preservation and use of scientific datawe advance our understanding of the physical universe by building on current and past studies inindividual disciplines, by collecting and analyzing new types of data, and by using past observations inentirely new ways not envisioned when the data were initially collected. the more complete the record ofscientific data and information, the more new understanding and knowledge we can extract from it.observations of natural phenomena typically represent a record of events that will never be repeated in adynamic universe that continually changes in time and varies in space. new scientific advances have hadsignificant, sometimes profound, societal and economic impacts and may be expected to be equallyimportant in the future. scientific data and information are at the heart of these advances and are essentialfor new discoveries. therefore, they constitute a precious national resource.the sections that follow describe briefly the two major types of data that are of critical importance inthe physical sciencesñexperimental laboratory data in physics, chemistry, and materials sciences, andobservational data in the earth and space sciences. in each of these broad areas the progress that has beenmade to date in terms of longterm preservation and accessibility is characterized, and the key issuesidentified. more comprehensive descriptions of the status of longterm data retention in the variousphysical science discipline areas are in the volume of working papers prepared as background for thisreport (nrc, 1995).experimental laboratory datathe experimental sciences have progressed over the centuries by building on the concepts, theories,and factual information resulting from each generation of scientific inquiry. the observations of tychobrahe were used by kepler to develop his laws of planetary orbits, and newtonõs formulation ofmechanics drew upon the previous work of galileo, kepler, and others. a century of measurements onproperties of the chemical elements provided the raw material needed for mendeleev to construct hisperiodic table. the history of science is rich in examples where the introduction of new, oftenrevolutionary, concepts rested on data that had been preserved from previous scientific investigations.furthermore, the technology of tomorrow is often based on the laboratory data of today or yesterday.the explosive growth of science in this century provides many other examples of the key role of datafrom previous experiments. when townes and schawlow published their landmark 1958 paper thatdemonstrated the theoretical possibility of building a laser, intensive efforts were started to find a realpreserving scientific data on our physical universe: a new strategy for archiving the nation's scientific information resourcescopyright national academy of sciences. all rights reserved.14preserving scientific data on our physical universephysical system that would meet the necessary requirements. data on atomic spectra, some of them 60 to70 years old, provided the key to creation of the first working gas laser. if it had been necessary to makenew measurements on every conceivable system in order to select the most promising for trial, theinvention of the laserñand all the new technology and economic benefits that it has broughtñwouldhave been delayed for many years.the crash program to improve rocket propulsion systems following the launch of the first sovietsputnik provides another example. data on the thermodynamic properties of a wide range of substanceswere essential to the efforts to optimize rocket engine performance. a concerted government programwas started to build a database of thermodynamic properties for rocket engine design. although somenew laboratory measurements were required, many of the needed data were in the scientific literature,some published as early as 1880. the availability of these older data significantly aided the rocket engineprogram.data generated by scientists and engineers in the fields of physics, chemistry, and materials sciencehave traditionally been published in research journals, which serve both a current dissemination and anarchival function. this journal system has served science well for 300 years. many scientific librariesthroughout the country provide access to these journals. because back volumes are kept in libraries inmany different places, there is little danger of irreparable loss from a natural catastrophe. many scientificsocieties also have depository systems that allow authors to submit voluminous data sets that cannot bepublished in the journals because of lack of space. the societies maintain these archives, generally onmicrofilm, and supply copies on request.while the growing use of electronic recording and storage techniques is already affecting thetraditional journal system, we can expect publishers to take advantage of the new technology to meet newneeds. scientific societies are beginning to implement electronic archives for preserving data that are toovoluminous to publish in paper formats. for example, the american chemical society recently began tomake data from papers in its leading journal (journal of the american chemical society) available on theinternet. it is a natural step from the paper and microfilm archives that such societies now maintain to theelectronic archives of the future. clearly, these private sector archives must be an integral part of theoverall concept of a ònational scientific information resource.óelectronically recorded data in the laboratory physical sciences are of two forms, original experimental measurements and evaluated compilations of published data. these are examined here in turn.original experimental measurementsrecent decades have seen significant changes in the form of òoriginal data.ó a raw experimentalresult was, in the past, typically a measured value such as a voltage or distance. the investigator readthese measurements from instruments, wrote them in a notebook, treated them arithmetically to obtainthe desired scientific variable from the raw measurement, and interpreted them. the original measurements were eventually discarded in most cases. today, many raw data are acquired and processedelectronically as soon as they are entered into the computer, so that only the processed data exist longenough for anyone to look at. with rapid, automated data acquisition and manipulation, the option existsto keep electronic data and reanalyze them as required. however, automated data collection often resultsin large volumes of insignificant data, so that in many experiments the data stream is screened and mostof the data are discarded in real time by a computer program or by the experimenter. for example,spectroscopists used to keep, at least temporarily, the photographic plates or recorder charts from whichthey had taken measurements. now the spectral features may be analyzed electronically immediatelyupon measurement, and only the attributes of relevant features are recorded. the fraction of the raw datathat is saved after initial processing may be small, sometimes less than one part in 10,000. in virtually allcases, there is no justification for preserving the raw data, because the experiment can be repeated inthose rare instances in which an unanticipated future interest appears.preserving scientific data on our physical universe: a new strategy for archiving the nation's scientific information resourcescopyright national academy of sciences. all rights reserved.the challenge: preservation and use of scientific data15when considering laboratory data of this kind, it is usually best to recognize that no one knows asmuch about the original data as the original experimenter. if the experimenter does not find the raw dataworth preserving (and worth documenting), then the data are probably not going to be of use to anyoneelse. because the number of stages of processing (e.g., replication, averaging, coordinate transformations, applying corrections, and so on) differ for every type of measurement and undergo continualevolution as new techniques are introduced, it would be fruitless to try to formulate generic retentioncriteria for all types of laboratory data.however, there are certain classes of laboratory data (where òlaboratoryó is used in a broad sense)that should be candidates for preservation if properly documented, because it would be impossible orimpractical to reproduce the measurements. some of the data taken in large plasma physics facilities fallin this category, because reproduction of the facilities would be extremely costly. a more strikingexample is the spectroscopic and other measurements from nuclear tests in the atmosphere, which it ishoped will never be reproduced. on a more mundane level, properties of engineering materials,measured as a part of large government research and development programs, provide many data ofpossible interest in the future. such data are acquired as a small step in a larger program and usually arenot published in the scientific literature or disseminated by the usual channels. they would be costly toreproduce because many of the materials were specially prepared with unique fabrication technology.examples include polymer and sensor data from the strategic defense initiative, engineering data fromthe national aeronautics and space administration (nasa), and the superconducting materials measurements carried out to develop magnet fabrication techniques for the canceled superconducting supercollider. even though this project will not be completed, the materials measurements should be saved,because they may well be applicable to future engineering projects.evaluated compilationscompilations resulting from the critical analysis of a large body of data from the scientific literatureare a separate area for consideration. wellknown examples include thermodynamic property compilations such as the national institute of standards and technologyõs joint armynavyair force (janaf)tables and the thermophysical properties disseminated by the department of defenseõs center forinformation and data analysis and synthesis at purdue university (see the physics, chemistry, andmaterials sciences data panel report in the nrc (1995) report for a detailed discussion of theseexamples). the department of energy operates several data evaluation centers in nuclear physics andchemistry. in such centers, the data and backup documentation are not impossible to replace; they simplyrepresent so much effort and exercise of specialized scientific judgment that it would be extremely costlyto redo the work. the cost of not having the data available, although usually difficult to measure otherthan anecdotally, can be much higher than the cost of preserving them. in particular, if it becomesnecessary in the future to expand or extend the compilation, the full documentation (e.g., data extractedfrom references, fitting programs, notes on the analysis techniques, and the like) will provide a valuablebase for the new work. a major concern in considering these data collections is how the data and theunderlying documentation can be preserved and made accessible if the centers producing them lose theirfunding or expert personnel. this concern increases as government agencies downsize their activities.observational data in the physical sciencesover the past two decades, the national research council and other groups have issuednumerous reports that have addressed data management issues, including longterm retention requirements, for digital observational data in the earth and space sciences (nrc, 1982, 1984, 1986a,b, 1988a,b,1990, 1992b, 1993; gao, 1990a,b; haas et al., 1985; napa, 1991). most of these reports have focusedquite narrowly on the data management or archiving problems of specific disciplines or agencies, andpreserving scientific data on our physical universe: a new strategy for archiving the nation's scientific information resourcescopyright national academy of sciences. all rights reserved.16preserving scientific data on our physical universenone has addressed comprehensively the issues associated with the longterm retention of observationaland experimental data in the physical sciences.major characteristics of observational dataobservational data sets, like laboratory data, include digital information (in both written andelectronic form), graphical records, and verbal descriptions. the records exist as ink on paper, punchedpaper, film (including microforms), magnetic tape of many types (including videotape), magnetic disk,and digital optical media (including cdrom). over the past three decades, however, the dominantform of data collection and storage has been electronic.observational data can be characterized by the collection and management practices applied throughout the life cycle of their existence. one might characterize two major practices driven by the fundingmodels for conducting the underlying science. the òbig scienceó funding model creates a fundingumbrella for multiple individuals and institutions to conduct coordinated data acquisition, investigation,and publication. often, these large programs adopt a standard approach for lifecycle data management.however, there is usually little standardization among the big science programs. examples of suchprograms include the world ocean circulation experiment, the world climate research program, andnasaõs mission to planet earth (cenr, 1994). the other funding model, òsmall science,ó fundsindividuals or small groups of individuals to conduct independent data acquisition, analysis, andpublication. typically, these investigators plan, design, and implement their own data managementstrategy with little interaction with the rest of the scientific community. the data generated under bothmodels have longterm value, both for science and for the broader interests of the nation.specific subdisciplines also impose different requirements on longterm data management. forinstance, while there is general agreement within the physical oceanography community on the definitionof standard observation variables and the processes of measuring those variables, the same cannot be saidfor biological oceanography. because of differences in measuring techniques, lack of communityagreement on naming standards, and the scientific process by which biology progresses, data management for biological data sets is inherently more complex than in physical oceanography. the data fromthese two subdisciplines will have to accommodate multiple naming schemes and alternate taxonomies.therefore, data managers and archivists have to deal with differing approaches and vocabularies amongdisciplines, evolution of discipline research paradigms over time, and diverging concepts and methodswithin a discipline.scientific research leads to the creation of data that can be processed and interpreted at differentlevels of complexity. typically, each level of processing adds value to the original (level0) data bysummarizing the original product, synthesizing a new product, or providing an interpretation of theoriginal data. the processing of data leads to an inherent paradox that may not be readily apparent. theoriginal unprocessed, or minimally processed, data are usually the most difficult to understand or use byanyone other than the expert primary user. with every successive level of processing, the data tend tobecome more understandable and often better documented for the nonexpert user. one might thereforeassume that it is the most highly processed data products that have the greatest value for longtermpreservation, because they are more easily understood by a broader spectrum of potential users. in fact,just the opposite is usually the case for observational data, for it is only with the original unprocessed datathat it will be possible to recreate all other levels of processed data and data products. to do so, however,requires preservation of the necessary information about processing steps and ancillary data.another important characteristic of observational data is their volume. in this respect, observationaldata can be divided into two different classes: smallvolume and largevolume data sets. the majority oftraditional groundbased, in situ observations form smallvolume data sets because they are based onindividually conducted measurements or sample collections. satellite and other remotely sensed observations generally form largevolume data sets.preserving scientific data on our physical universe: a new strategy for archiving the nation's scientific information resourcescopyright national academy of sciences. all rights reserved.the challenge: preservation and use of scientific data17the committee defines smallvolume data sets as those with volumes that are small in relation to thecapacity of lowcost, widely available storage media and related hardware. the hardware and software towrite and produce cdroms are now generally available for less than $10,000, and personal computerscapable of reading cdroms are being marketed as homeuse, consumer items. for example, the totalvolume of the smallvolume oceanographic data is projected to be less than 50 gigabytes by 1995, andthus the entire historical data set for all observations could be stored on fewer than 100 cdroms. thisis fewer diskettes than many people have in their compact disk music collections.issues such as archiving cost, longevity of media, and maintenance of the data holdings are not thedominant considerations with regard to retaining smallvolume data sets. rather, the major issue withrespect to this class of data is the completeness of the descriptive information, or metadata. if a data sethas been properly prepared and documented, the operations required to migrate the data should beamenable to significant automation and therefore pose only a minor challenge to the longterm maintenance of the archive. further, these data may be widely distributed with simple replication of the media.for example, the various noaa and nasa data centers have provided copies of their data sets to manyusers for a number of years.a different problem is posed by largevolume data sets. the biggest data sets typically come fromearth observation satellite sensors and space science missions, and are challenging to some contemporarystorage devices. however, it is clear that for the data set to exist at all, an adequate storage mediumcapable of capturing and maintaining the data for some time period must exist when the data aregenerated. further, the time period for reliable, initial storage should at least cover the lifetime of the dataset at the organization acquiring and using the data before the records need to be migrated to new mediaor transferred to another organization, such as noaa or nara. in addition, during the initial storageperiod, there are likely to be major increases in the density of mass storage accompanied by significantdecreases in the cost of storage of the data. thus, data sets that are challenging today will gradually betransformed to òsmallvolumeó status in the future, as advancing technology increases the capacity andlowers the cost of storage devices. nevertheless, it is important to note that the largest data sets (e.g.,larger that one terabyte) can present significant organizational and management problems that requirespecial analysis of the data flow, volume, access, and timing characteristics.observational data in the space and earth sciences astronomy and astrophysics dataastronomy and astrophysics are observational sciences; that is, they are based on what the skyprovides and we collect. therefore, in many astronomical investigations there is no such thing asòrepeating an experimentó with the expectation of getting the same results. many objects have propertiesthat change with time either because of their intrinsic nature (e.g., variable stars), evolution (e.g., starsgoing supernova), or reasons yet unknown. it happens quite frequently that a highly variable object isfound in satellite data and subsequent archival research in optical plates allows its identification as agiven type of star.astronomy and astrophysics data are acquired by both groundbased and spacebased observatories.groundbased observatories, which are operated by universities or other nonprofit organizations (e.g.,association of universities for research in astronomy, the smithsonian institution) and funded by theseorganizations or by the national science foundation (nsf), have traditionally been used to study the skyat visible wavelengths. since the second world war, astronomers have used improving technologies toobserve at radio and infrared wavelengths. consortia of universities, including both u.s. and foreigninstitutions, are constructing new telescopes, which use advanced technology to build larger mirrors thatwill allow us to look deeper into the universe. radio observatories range from smaller ones operated byuniversities to larger national facilities, such as the national radio astronomy observatory, funded bypreserving scientific data on our physical universe: a new strategy for archiving the nation's scientific information resourcescopyright national academy of sciences. all rights reserved.18preserving scientific data on our physical universensf. most telescopes are for individual observing programs, but some are dedicated to systematic skysurveys.data from ground observations have traditionally been the property of the observer; therefore,observatories have no standard policies for data archiving. the exceptions are some big projects, such asthe palomar sky survey, where data either are made public and sold or are archived within the universityor observatory. some centers, such as the national radio astronomy observatory, the national opticalastronomy observatories, and the harvardsmithsonian center for astrophysics, have begun to archivemost data obtained from major telescopes. these data are valued and used broadly by astronomers.nevertheless, archival activities remain of generally low priority.although the older astronomical data consist of photographic plates and other analog data, virtuallyall data today are collected digitally. there also have been major efforts to digitize old photographic datato allow their analysis by computer. an example of this is the digitization of a wholesky survey by thespace telescope science institute, and this survey is now available for sale on cdrom from theastronomical society of the pacific. recently, the astronomical community adopted a standard formatfor transfers of digital files (fits). with the advent of digital data, there also has been an evolution fromindividual data analysis packages to a few widely distributed packages (e.g., iraf, aips, vista,xanadu), which provide standard tools for baseline analysis.because of the filtering and distortion produced by the earthõs atmosphere, the amount of energyemitted by celestial bodies that can be detected on the ground is limited significantly. observations fromspace above the atmosphere remove such limitations. from its inception, space astronomy and astrophysics have been mostly under nasaõs purview, although some important experiments have beenfinanced by the department of defense. the data are collected through telescopes and detectors placedon airborne devices (balloons or planes), rockets, nasaõs space shuttle, and orbiting satellites. thelargest volume of data is collected by satellites, and most of these missions are international collaborations. the u.s. portion has always been handled by nasa.within nasa, space astronomy and astrophysics are organized in different wavelengthbaseddisciplines, reflecting the organization in the scientific community. these disciplines include theinfrared, whose main data center is the infrared processing and analysis center in pasadena, california,where the data from the infrared astronomy satellite mission are archived; the optical and ultraviolet,with data centers at the space telescope science institute in baltimore, maryland, where the hubblespace telescope data are archived, and at the nasa goddard space flight center in greenbelt,maryland, where the international ultraviolet explorer archive resides; and highenergy astrophysics,which maintains xray data at the einstein observatory data center in cambridge, massachusetts.table 2.1 provides a representative sample of nasa astrophysics archives. the earlier nasaastrophysics projects were socalled òprincipal investigatoró missions, where a contract was awarded to agroup of principal investigators, who built the hardware, received the data from the experiments, andanalyzed and interpreted them. these principal investigators had no clearly stated guidelines to preparedata for archiving, other than to deliver the reduced data to the nasa data depository at the nationalspace science data center (nssdc) at the nasa goddard space flight center. documentationgenerally was minimal, and the data, which often were not welldocumented or wellorganized, weredifficult to retrieve for scientific use, even if they were adequately physically preserved.it has become fully apparent, however, that the uniqueness and high acquisition cost of these spacedata make their effective preservation and archiving a high priority. even after the active operation of aspace observatory has ended, the data typically are retrieved and used by scientists for many more years.as a result, the situation has improved considerably at the nssdc in recent years. moreover, nasanow funds wavelengthspecific scientific data centers to process the data, eliminate anomalies in the data,and provide software for scientific analysis.preserving scientific data on our physical universe: a new strategy for archiving the nation's scientific information resourcescopyright national academy of sciences. all rights reserved.the challenge: preservation and use of scientific data19table 2.1 a representative sample of nasa astrophysics archives, by satellite missionhigh energyinternationalinfraredhubblecomptonastrophysicalultravioletastronomicalspacegamma rayobservatory 2explorersatellitetelescopeobservatorydata typexray dataultraviolet datainfrared dataoptical/ultraviolet data gammaray datayear of launch19781978198319901990duration2.5 yearsongoing300 daysongoingongoingtotal data volume~100~100~150~5500 by year 2005~1000 by year 2000(gigabytes)data centereinstein observatorynational space scienceinfrared processing andspace telescopenational space sciencedata center, cambridge,data center,analysis center,science institute,data center,massachusettsgreenbelt, marylandpasadena, californiabaltimore, marylandgreenbelt, marylandpreserving scientific data on our physical universe: a new strategy for archiving the nation's scientific information resourcescopyright national academy of sciences. all rights reserved.20preserving scientific data on our physical universeplanetary science dataplanetary data also are acquired by both groundbased and spacebased observations. planetary datainclude observations of the entire physical system and forces affecting a planet or other body, includingthe geology and geophysics, atmosphere, rings, and fields. the sensors used collect data across much ofthe electromagnetic spectrum. currently, most planetary observations are supported by nasa, either asthe direct result of planetary missions or as groundbased observations that support a mission. over thepast three decades, nasa has sent robotic spacecraft to every planet in the solar system except pluto, totwo asteroids, and to a comet. men have walked on the moon, performed experiments there, and returnedsamples. the knowledge we have about the bodies in the solar system, with the exception of our ownplanet, comes mostly from space missions. in some cases, such as the gas giants jupiter, saturn, uranus,and neptune, robotic space probes have provided most of our current knowledge. many of the satellitesof the other planets were no more than points of light with minimal spectral and lightcurve measurements before the voyager mission. now each is recognized as a separate world with highly individualcharacteristics.the scientific and historical importance of spacebased planetary observations, the realization thatadditional missions cannot replicate the original observations, and the expense of planetary missions allprompted nasa to create the planetary data system (pds) to improve the acquisition, archiving, anddistribution of planetary data. the developers and current staff of the pds recognize that the data fromplanetary missions make up the scientific capital of the agencyõs planetary exploration program and thatthese data are a national resource. the pds tries to acquire all existing planetary data from nasaõsmissions and even from international ventures, in order to have a complete archive of our exploration ofthe solar system. in addition to the spacebased measurements, the pds accepts relevant groundbasedobservations and laboratory measurements that support planetary missions by providing baseline orcalibration data. a basic condition for acceptance is that the data set must be properly documented andinclude all relevant ancillary data, including planet and spacecraft ephemerides, calibration tables, andexperimenter notes about the shortcomings of the data. members of the pds scientific staff and scientistsin the community who have expertise within the relevant disciplines peerreview each data set.one of the more important contributions of the pds, especially with regard to the ongoing preservation of data in a useful form, is the electronic òpublicationó of the majority of the data from manyplanetary missions in the form of cdroms. these include not only the data, but also documentation,format specifications, ancillary data, and even, in some cases, display and analysis tools.space physics dataspace physics involves the study of the largest structures in the solar systemñthe plasma environments of the planets and other bodies and the solar wind. those environments consist of plasmas rangingfrom low energies (the thermal component) to charged particles of high energies, including cosmic raysaccelerated by galactic processes. they also consist of the magnetic fields (if they exist) of planets or thesun, as well as electrostatic and electromagnetic fields generated from natural instabilities in plasmas andchargedparticle populations. furthermore, in many locales, such as comets and the earthõs ionosphere,dust and neutral gases play an important role in mediating the behavior of plasmas and electromagneticfields. as a consequence, the field of space physics requires a broad array of sensors and instruments atall levels of complexity.many instruments make in situ observations, but novel techniques enable remote sensing of variousplasma regimes. because some of the most apparent manifestations of space physics processes result inthe northern lights and in planetaryscale modifications of the terrestrial magnetic field (and subsequentcatastrophic effects on power grids and communications), space physics relies heavily on a wide array ofgroundbased observations, including magnetometers, ionospheric sounders, incoherent radar facilities,preserving scientific data on our physical universe: a new strategy for archiving the nation's scientific information resourcescopyright national academy of sciences. all rights reserved.the challenge: preservation and use of scientific data21allsky cameras, and photometers. in addition, a broad range of groundbased and spacebased solarmonitors has become crucial to study the correlations between various disruptions in the terrestrialplasma environment and solar activity, including sunspots, flares, and prominences.for many reasons, it is essential to preserve space physics data for long periods of time. the sundrives solarterrestrial relationships, and many studies require observations over 22year solar cycles.during this cycle the sun reverses its magnetic polarity twice and goes through periods of increasedactivity with sunspots and associated flares. at solar activity minimum, flare and sunspot activitydecreases, but expanded coronal holes appear. long intervals of records are required because each solarcycle is different from previous ones and because there are longterm deviations, such as the maunderminimum, from ònormaló patterns. from the terrestrial point of view, there are motions of the magneticdipole and even magnetic field reversals on time scales of thousands of years.because many space physics observations are taken in situ, models of the magnetosphere need datacollected by many spacecraft, having different kinds of orbits and trajectories. to make sense out of datafrom one of these missions, it is important to be able to examine what another spacecraft in a differentorbit found. only by preserving the data from numerous missions do we acquire a sufficient archive.space physics has generated about 50 gigabytes of data per year over the last 30 years. the field hasenjoyed this extraordinary productivity primarily because most missions were in earth orbit and weretracked continuously for years. many of these data sets were òarchivedó by sending the tapesñandsometimes the relevant documentationñto the nssdc. copies of the data on microfilm or on othermedia were sent there as well. unfortunately, for every wellprepared, thoroughly documented spacephysics data set at the nssdc, there are several poorly prepared and improperly documented data sets.for the earliest space missions, the archiving techniques were undeveloped, and archiving was notdeemed a high priority. thus, there are many data at the nssdc that most scientists would find difficultto use with only the information originally supplied. given the recent emphasis on the proper preservation of data and the importance of archivingñprompted in part by two general accounting officereports (1990a,b) and also by a heightened awareness and desire for highquality archives by thecommunityñmany recently archived data sets are in better condition than their predecessors. eventhough the space physics data system has been in existence only since 1993, the more advanced dataactivities in other disciplines have influenced the space physics community favorably. hence, it isbecoming more likely that the data now being submitted are of a higher quality, have more adequatedocumentation, and are more complete than earlier data sets.noaa, nsf, the department of defense, private and educational institutions, and foreign organizations typically support the groundbased observations. most of these data, not managed by nasa,eventually come under the purview of the national geophysical data center, operated by noaa atboulder, colorado. the centerõs holdings consist of over 300 digital and analog databases, some ofwhich are very large. however, many important data sets still reside solely in the hands of the originalinvestigators, the military, or foreign sources.atmospheric science dataatmospheric science data sets are diverse and present a variety of problems for distribution,archiving, and later interpretation. some data sets on the atmosphere stand out as the largest in anyscientific discipline, particularly those from remote sensing by satellite or radar; others consist ofcontributions from thousands of individuals all over the world, and the provenance of those data issometimes uncertain. many data sets span decades, and a few span more than a century, withaccompanying problems due to lack of homogeneity in measurement techniques and sampling strategies.the largest atmospheric science data holdings in the united states are those of the federal government.however, significant amounts of material are available only from state or private sources.preserving scientific data on our physical universe: a new strategy for archiving the nation's scientific information resourcescopyright national academy of sciences. all rights reserved.22preserving scientific data on our physical universenot all atmospheric data sets are large and conspicuous; many are small. there are hundreds of datasets of only a few megabytes or less. there are also many mediumsized data sets that range fromperhaps 100 megabytes to tens of gigabytes, as well as very large data sets, many terabytes in volume.table 2.2 provides a sampling of some of the larger data sets. data volume does not drive the cost ofarchiving smallsized and mediumsized data sets if proper technical choices are made. rather, it is thelaborintensive process of readying a data set for indefinite preservation that can be costly.many atmospheric data sets are dynamic, continually growing or being otherwise modified. becauseweather keeps occurring, observational time series from operational meteorological activities are neveròcomplete.ó in contrast, field programs usually have finite extent, and the resulting data sets have adefinite end. however, many recent large, complex field programs have spawned associated monitoringactivities that have continued after the initial phases of the project. despite the frequent usage of the termòexperimentó to denote field programs, these intensive efforts are observational, rather than experimental, exercises. some truly experimental data exist, including a few data sets that include the results fromsuch work as sensor development and tests, fluid dynamics experiments, thermodynamic measurements,and laboratory chemical studies. nevertheless, the vast majority of atmospheric science data describeobservations of everchanging phenomena, and thus they are unique, valuable, and irreplaceable.for much meteorological and climate research, as well as for many applications, it is essential to havearchives of global data. this goal has been largely achieved in the united states, although older data setsstill need to be digitized. collectively, u.s. archives have the best sets of global data of any nation,particularly for data since the early 1950s. however, many valuable data stored in other nations areinaccessible to u.s. scientists (and in some cases are inaccessible to those nationsõ scientists as well).meteorological and other atmospheric data are used for varying purposes on different time scales. itis convenient to delineate three: (1) realtime or current, (2) recent past or shortterm retrospective, and(3) distant past or retrospective. compared with other disciplines, meteorological data are probably usedby a wider segment of the u.s. population than other scientific data, because they relate directly topractical, daily concerns. there is a large lay audience for weather and climate information.the realtime or current use of most data sets usually motivates decisions on collection strategies andtherefore quality. for example, the primary reason for collecting most meteorological data is foroperational weather forecasting and warning, including forecasting for aviation operations. these dataare perishable, and timeliness and spatial resolution are more important than absolute accuracy andcontinuity.there are many recent past or shortterm retrospective uses of meteorological data that can be ofgreat significance. in this context, short term typically means from yesterday to a few weeks, oroccasionally a few months, ago. a good example of such usage of data is in monitoring the developmentof a drought, a significant function for predicting crop yields. the transportation industry uses past datafor verification of weather conditions for delay claims.most retrospective uses require data from several months old through the traditional (though nowsuspect) 30year averaging periods used for climate normals. the national climatic data center handlesover 100,000 data requests per year. the state climatologists and regional climate centers also processabout this many. legal proceedings and insurance claims often require accurate meteorological recordsfor corroboration of witness testimony, criminal investigations, and validations of weather claims relatedto accidents and property damage. farmers and agronomists need data covering months to years forstudies of pesticide residue and toxicology, decisions about pesticide spraying, planning of fertilizerusage, and crop selection. architects and building engineers require sitespecific data on heating andcooling needs, wind stresses, snow loads, and solar availability. airport designers need prevailing windpatterns. utility planners need aggregate heating and cooling loads for their areas.longterm retrospective uses of atmospheric data are the primary concern in this study. these usesare highly diverse, difficult to predict, and make great demands on the data and their associated metadata.preserving scientific data on our physical universe: a new strategy for archiving the nation's scientific information resourcescopyright national academy of sciences. all rights reserved.the challenge: preservation and use of scientific data23most of the uses discussed above do not need data covering more than a few decades. several of theseapplications, however, require the longest time series we can provide.when technology advances and alters the method of data collection, there is a strong impetus to scrapthe data collected by òobsoleteó technology. however, these old data may become critical in the future.a notable example involves upper air wind profiles. these were originally collected by kites and later byradiosondes carried on balloons. with the onset of the space program, there was an urgent need fordetailed lowaltitude wind data for analysis of stresses on rockets at launch. appropriate data could nottable 2.2volume of selected data sets in atmospheric sciencestype of data setcommentsdatesyearsvolumeatmospheric in situ observationsworld upper airtwo times per day, 1,000 stations1962199332 25 gbworld land surfaceevery 3 hours, 7,500 stations196719932760 gbworld ocean surfaceevery 3 hours (~40,000 observations1854199313915 gb per day)world observations duringsurface and aloft, but not satellite19781979110 gbfirst garp global experimentu.s. surfacedaily, now 9,000 stations190019939415 gbselected analyses(mostly global)main national meteorologicaltwo times per day,194519934850 gbcenter analyses increasing at 4 gb/yearnational meteorologicalfour times per day,19901993458 gb center advanced analyses increasing at 19 gb/yearnational center for atmosphericthirtyeight data sets8 gbresearchõs ocean observations andanalyseseuropean center for medium rangefour times per day,19851993976 gbweather forecasting advancedincreasing at 8 gb/yearanalysesselected satellitesnoaa geostationary satelliteshalfhour, visible and infrared1978199316130 tbnoaa polar orbiting satellites1978199315sounders (tiros operational15720 gbvertical sounder)advanced very high resolution155 tbradiometer (4km coverage,5 channel)nasa earth observingin development, 88 tb/year,1998 satelliteam level1 datau.s. radar datadomains of 30 to 60 km19731991191 gbnext generation radar650 gb per radar each year,system (nexrad)a104 tb/year for 160site system1997100s tbnotes: many other atmospheric data sets have volumes of only 1 to 500 mb.1 mb (megabyte) = 106 bytes; 1 gb (gigabyte) = 109 bytes; 1 tb (terabyte) = 1012 bytes.afirst radars were deployed in 1993.preserving scientific data on our physical universe: a new strategy for archiving the nation's scientific information resourcescopyright national academy of sciences. all rights reserved.24preserving scientific data on our physical universebe obtained from radiosondes, because of their high ascent rate, but older kitebased data, which had beenscheduled for disposal, were available. fortunately, they had not yet been destroyed when they wereagain needed.there have been dramatic retrospective uses for military purposes (e.g., jacobs, 1947). planning forthe dday invasion of france, bombing runs over japan, and the recent desert war in iraq all requireddetailed climatic information, some long thought useless but not yet discarded. such unexpected usesrequire the retention of many types of data from many places for a long time. since the first flights ofmeteorological satellites in 1959, we already have had several examples of important retrospective usesof satellite data sets. for instance, a combination of reprocessed nimbus7 satellite data and old datafrom the dobson network helped to confirm the recurring seasonal loss of stratospheric ozone over theantarctic in the early 1980s.if meteorologists are to study past weather events, such as severe hurricanes, damaging winterstorms, or outbreaks of tornadoes, they must have at their disposal all data for the periods of time andgeographical areas involved. hurricane track records spanning more than a century are still regularlyused for both research and operational purposes.an increasingly significant use of meteorological data is the monitoring of the climate of the planet.although barely two decades ago the study of climate was not a very high priority, today climate researchissues are prominent; some of the nationõs leading scientists specialize in climate studies, and policymakers seek information on likely climatic conditions of the future. the importance of old atmospheric datahas become clear, but the reanalysis of these old data in the search for trends has often found theminadequate and poorly documented. the growing interest in global climate change and the difficultieswith historical data that it helped uncover have strongly motivated earth scientists to take a seriousinterest in the longterm preservation of atmospheric data. similarly, studies of longterm water and landusage require time series of many decades, or more. such data needs also apply to planning aquifer usageand studies on deforestation and desertification.some historians examine connections between environmental conditions and human events. thetime scales studied can range from the immediate, such as the influence of weather on battles, to the verylong term, such as the rise or decline of a civilization affected by water availability. workers in this fieldoften search through the oldest existing data and have even provided meteorological information toatmospheric scientists from unconventional sources such as diaries and agricultural records.contemporary arrangements for the storage and archiving of atmospheric data are diverse, complex,and present many problems. some of these arrangements could be improved. atmospheric data are inmany locations, and they have a broad range of life cycles. difficult problems arise in preparingmetadata, packaging data for extended archiving, motivating researchers to prepare their data for use byothers, and simply dealing with the large size of some of the atmospheric data sets. criteria foridentifying data sets to save indefinitely are not necessarily obvious. finally, any proposed solutionsmust be made in full recognition of their impact on budgets and other resources.geoscience dataspatially, the domain covered by the geosciences extends from the earthõs core to the surface andinto space. temporally, it covers broad trends from the remote origins of the earth to possible futurescenarios, but it also is concerned with rapidly varying, often shortlived phenomena. data in thegeosciences fall into two broad categories. one is the observation and description of unique events, suchas earthquakes, volcanic eruptions, and floods. in most cases, such data need to be archived for a longtime period, regardless of their quality. the other category consists of observations of quantitiescontinuous in space and time, such as gravity and the earthõs magnetism and structure, seismic sampling,and groundwater distribution.preserving scientific data on our physical universe: a new strategy for archiving the nation's scientific information resourcescopyright national academy of sciences. all rights reserved.the challenge: preservation and use of scientific data25the volume of geoscience data obtained with public funding has increased dramatically over the pastfew decades. this increase is the result of several converging factors, including the extremely variedtypes of observational data collected by the scientific community; the large volumes available throughbetter measurement techniques, more sophisticated instrumentation, and advancing computer technology; and increasing demand from not only the scientific community but also the general public, includingengineers, lawyers, and statisticians. nongovernmental and commercial institutions also are majorcollectors and sources of pertinent data.two examplesñthe landsat database and the nationõs holdings of seismic datañillustrate many ofthe characteristics and issues inherent in the longterm archiving of geoscience data. other examples areprovided in the working paper of the geoscience data panel (nrc, 1995).the landsat database consists of multispectral images of the earthõs surface, which have beenaccumulating since the launch of landsat 1 in july 1972. the archive includes digital tapes ofmultispectral image data in several formats, blackandwhite film, and falsecolor composites of synopticviews of the earthõs surface, all from 700 km in space. this database thus constitutes an important recordof the evolving characteristics of the earthõs land surface, including that of the united states, itsterritories, and possessions. the record documents not only the results of various federal governmentpolicies and programs, but also those of many state and local governments and private programs andactivities. it further provides documentation of the impact of various largescale episodic events, such asfloods, storms, and volcanic eruptions, and is of great value to both current and future public and privateactivities.landsat data are currently available in either image or digital form from the earth resourcesobserving system (eros) data center in sioux falls, south dakota. the landsat satellites wereoriginally under the control of nasa. however, in 1980 they became the responsibility of noaa. thecurrently operational landsat 4 and 5 spacecraft were placed under control of the eosat company in1985. under eosatõs control, the data are not in the public domain, are significantly more expensive,and carry proprietary restrictions on their use. beginning with the launch of landsat 7, responsibility forthe landsat system will pass back to nasa, which will build and launch the satellite the late 1990s.nasa will operate the systems and deliver the data to the eros data center for distribution. the datawill once again be in the public domain, although the eros data center still plans to charge more thanthe marginal cost of reproduction in fulfilling user requests. it is now widely recognized that the shift toprivate control of the landsat system significantly reduced the access to and use of the data.as of january 1993 the landsat database contained more than 100,000 tapes of varying density andformats, and over 2,850,000 frames of hard copy imagery. digital landsat data are usually delivered tousers as magnetic tapes. other media, such as cdroms and streaming tapes, also may soon be used.data requests occur most frequently in reference to a particular geographic location, commonly expressed as latitude and longitude, for a particular time of the year, and meeting certain cloud coverlimitations.landsat data are used widely across the spectrum of geoscience applications in both civilian andmilitary operations and research. these include such applications as the impact of human activities onthe environment, landuse planning and resourceallocation decisions, disaster assessment, measurementand assessment of renewable and nonrenewable resources, and many others. they are used also by thegeneral public in any context where views of the earthõs surface are needed. examples include suchdiverse applications as visual aids in elementary and secondary education, background for highwaymaps, and illustrations for magazine articles about various regions of the world.the landsat database is unique because data from any given area may be available at sampledinstants over a period of more than 20 years, thus making possible for the first time the study of slowlyvarying phenomena on earth. even though data from the early 1970s may now have a low frequency ofuse, their potential value remains high and they represent a significant archival record.preserving scientific data on our physical universe: a new strategy for archiving the nation's scientific information resourcescopyright national academy of sciences. all rights reserved.26preserving scientific data on our physical universein contrast to the landsat database, seismic data are broadly distributed rather than concentrated inone data center or system. this example focuses primarily on seismic data from earthquakes andexplosions, both nuclear and chemical. some federal agencies, notably the u.s. geological survey(usgs) and noaaõs national geophysical data center, collect and archive important seismic exploration data. in addition, the department of defense (dod), department of energy (doe), u.s. nuclearregulatory commission (usnrc), usgs, and noaa have been and continue to be engaged in thecollection and archiving of earthquake and explosion data. these agency programs are carried outindependently of one another with the result that each agency has its own data management and archivingpolicies and practices. consequently, these data holdings are greatly distributed among the agencies infundamentally different forms and formats.global earthquake data have been acquired systematically since the early 1960s, when the u.s. coastand geodetic survey of the department of commerce deployed a global seismic network of about 130stations called the worldwide standardized seismographic network (wwssn) and produced anarchive of photographic film òchipsó of the 24hour/day recordings at all stations. researchers and otherapplications could obtain copies of these analog data at modest cost. the success of this precursor totodayõs global digital network cannot be overestimated, because the availability of a global data set instandard format from wellcalibrated instruments permitted previously impossible studies of globalseismicity patterns, earthquake source mechanisms, and the earthõs structure. these studies have led toa vastly improved understanding of the dynamics of the earth as a whole, including tectonic platemovements, generation of new ocean floor, evolution of the earthõs crust, and occurrences of destructiveearthquakes and volcanic eruptions.the usnrc has funded the operation of regional seismic networks over much of the united states,some since the early 1970s, in support of programs for the siting and safety of nuclear power plants.usgs also has cofunded or separately funded regional networks for earthquake hazard assessments inseismogenic areas of the united states. however, changes in the funding priorities of usgs and usnrcin recent years have resulted in the interruption or discontinuation of some of these networks, particularlyin the eastern united states. this has adversely affected data flow and seismic research. seismic datahave been archived in a broadly distributed, nonuniform mode by the organizationsñmostly universitiesñthat collected the data from the various networks. many of these data have longterm value forcharacterizing in detail the tectonic activity of seismogenic areas in the united states.in addition to the federal agencies, several private sector organizations now collect, distribute, andarchive seismic data sets of longterm significance. the incorporated research institutions for seismology (iris), a notforprofit consortium of universities and private research organizations, is engaged in amajor development of a global digital seismic network of about 100 continuously recording stations (theglobal seismic network) in cooperation with usgs. the project also includes a versatile, portabledigital seismic array of up to 1,000 stations that can be deployed for various time intervals for specialseismological studies. data sets from the global and portable array are being permanently archived at theiris data management center (dmc) in seattle, washington. the dmc also serves as the internationalfederation of digital networksõ center for continuous digital data, which adds observations from manyadditional stations to the archive. iris funding for this activity comes primarily from nsf and dod.finally, individual universities, such as the california institute of technology, the university of california at berkeley, the university of alaska, the university of washington, columbia university, memphisstate university, and st. louis university, also maintain archives of the seismic data that they collect.the volume of digital data currently held and anticipated to be acquired by the iris dmc issummarized in table 2.3. although some data sets have been completed because they are project orprogramspecific, most of the current operations continue to add large amounts of new data andimplement new technology for recording, storage, retrieval, and distribution, thereby creating a dynamic,highly distributed archive whose holdings and access protocols change with time. for example, the irispreserving scientific data on our physical universe: a new strategy for archiving the nation's scientific information resourcescopyright national academy of sciences. all rights reserved.the challenge: preservation and use of scientific data27dmc recently began providing both archived and nearrealtime data on the internet, thereby greatlyfacilitating rapid access.significant volumes of exploratory seismic data obtained by geophysical contractors are held by thedepartment of interior. these data are used by the federal government and by petroleum companies inpreparing for oil and gas exploration activities. there are, however, various proprietary restrictions onaccess to these data by other users.in summary, the sources of seismic data are diverse, the archiving is highly distributed, and the dataare in many different formats with different metadata structures. moreover, data sets with longtermscientific and historical value reside in both federal and nongovernmental organizations, although in mostof the latter cases federal funds have paid at least in part for their acquisition, archiving, and distribution.the users of seismic data are many and diverse as well. they include federal and state governmentagencies, universities, and private industry, particularly the petroleum industry. thousands of individualsare direct or indirect users of seismic data. certainly, the public as a whole is an end user of historicalseismic data and information, including the location, magnitude, and damage associated with earthquakes around the world.most seismic data sets have been or are now used both for operational purposes and for research,although for operational activities the data are used primarily immediately following their collection.examples of their use for operational activities include tsunami warning and the rapid determination ofthe magnitude, location, and fault mechanism of destructive earthquakes and their aftershocks, both toinform the public and to assist in emergency response and special monitoring. on a longer time scale thedata are used for hazard reduction and seismic safety in seismogenic regions, including local zoningdecisions for future development, and siting and safety of critical facilities such as nuclear power plants.data are obtained and used for continuous global monitoring of earthquake activity and of threshold orcomprehensive test bans on underground nuclear explosions. of course, there also is a broad spectrum oftable 2.3 summary of actual and projected data volumes archived in the iris data management centerprojected data volumes (gigabytes/year)number ofinstrumentsa1994199519961997199819992000gsn1001,1592,3593,9596,0038,04710,09112,281fdsn1463706701,0701,5302,0502,6703,416jsp arrays51,0952,1903,6505,4757,3009,12510,950osn30001558218498936passcalbb5001,3182,2773,5565,1547,0739,31211,867passcal rr5005428851,3411,9122,5973,3974,310regionaltrig5001502904907301,0301,3901,755total1,7814,6348,67114,08120,86228,31536,48345,515 note: abbreviations are as follows:gsnglobal seismic network (iris)fdsnfederation of digital seismic networksjspjoint seismic program (with the former soviet union) (iris)osnocean seismic networkpasscalbbprogram for array studies of the continental lithosphereñbroadband (iris)passcalrrprogram for array studies of the continental lithosphereñregional recordings (iris)regionaltrigregional triggered recordingsaprojected numbers by year 2000.source: iris data management center, private communication, 1994.preserving scientific data on our physical universe: a new strategy for archiving the nation's scientific information resourcescopyright national academy of sciences. all rights reserved.28preserving scientific data on our physical universeresearch that uses historical seismic data, including studies of the physics of earthquake and explosivesources, propagation effects on seismic signals, imaging of the earthõs structures at all scales, seismicitypatterns, and earthquake prediction or hazard estimation. older data are important and are commonlyused for most of these types of research. for example, establishing the recurrence rate for largermagnitude earthquakes requires decades to centuries of observations, even in the most seismically activeareas.in conclusion, most of the seismic data have longterm value for scientific research, disastermitigation, and various socioeconomic uses. the data are archived in a broadly distributed manner.however, only a fraction of the archived data are under the direct control of federal government agencies,and it appears that many of these data sets are not considered official federal records. except for mostcommercial exploratory seismic data, federal funds have paid for much of the instrumentation, stationoperation and maintenance, collection, storage, and distribution of seismic data. these important seismicdata sets should be kept indefinitely in a form accessible to both the scientific community and other users.ocean science datathe oceans and atmosphere are turbulent fluids, constantly changing over many spatial and temporalscales. the numerous types of data that describe the oceans are often unrelated to one another, and eventhose that are related frequently have nonlinear and poorly understood interactions. for example,temperature data from a specific point and time in the north atlantic cannot be accurately predicted fromdata collected in the same place the year before, or even the week before, or from data collected at thesame time 1,000 kilometers or even 100 kilometers away, or from salinity data collected at the same placeand time. each datum contributes unique information as long as it is accurate, corresponds to a differentphysical quantity, is obtained from a different time and place, and cannot be accurately computed fromother existing data.one source of oceanographic data is the field program. large and small field programs conducted insupport of specific research projects are the prime contributors of in situ and in vitro observational datasets for all the ocean disciplines. in situ data sets are those that are derived by processing themeasurements from sensors immersed directly into the ocean environment. processing of in situ data islargely automated, and so the data sets are relatively dense. in vitro data sets are produced by laboratoryanalyses of samples collected from the ocean environment. these laboratory analyses combine sophisticated measurement equipment with labor and time intensive procedures. therefore, in vitro data aretypically sparse. remotely sensed observations also may be associated with field program data bysynchronizing in situ sampling with the use of remote sensing platforms.the harsh and remote nature of the world ocean environment has inhibited the establishment of aroutine data collection system. although several remote sensing platforms do provide daily monitoringof ocean surface conditions on a global basis, continuous measurement of subsurface conditions withadequate time and space resolution for effective monitoring is not a reality. the lack of continuous andcomprehensive oceanographic data may contribute most to the inconsistent data management practicesand lack of communitywide standards for data reporting and exchange in the ocean disciplines. becauseof the need for daily global prediction, such standards and practices are much more highly developed inthe atmospheric community. the establishment of the global ocean observation system presents anopportunity to engage the ocean community in the identification and implementation of appropriatestandards.like other observational data, oceanographic data extend beyond directly or remotely measuredobservations of the environment. the data products based on the analyses, interpretations, and presentations of aggregates of observations also must be considered in the design, implementation, and maintenance of any data management and archiving mechanism. the more traditional products, such asparameter grids and output from ocean models, will surely be supplemented from innovative sourcespreserving scientific data on our physical universe: a new strategy for archiving the nation's scientific information resourcescopyright national academy of sciences. all rights reserved.the challenge: preservation and use of scientific data29likely to emerge from the interactive scientific collaboration and valueadded services that are becomingincreasingly available through electronic networks.the principal federal agency ocean data holdings are at the noaa national oceanographic datacenter (nodc), the nasa physical oceanography distributed active archive center (po.daac) atthe jet propulsion laboratory, and at several navy centers, which hold mostly classified data sets. inaddition, significant amounts of data are held by the universities.located in washington, d.c., the nodc archives physical, chemical, and biological oceanographicdata collected by other federal agencies, including data collected by principal investigators under grantsfrom the national science foundation; state and local government agencies; universities and researchinstitutions; and private industry. the center also obtains foreign data through bilateral exchanges withother nations and through the facilities of world data center a for oceanography, which is operated bythe nodc under the auspices of the national academy of sciences. the nodc provides a broad rangeof oceanographic data and information products and services to thousands of users worldwide, andincreasingly, these data are being distributed on cdroms and on the internet. table 2.4 presents asummary of the nodcõs data holdings.the po.daac is a major federally sponsored oceanographic data center, which is operated by thecalifornia institute of technologyõs jet propulsion laboratory in pasadena, california. as one elementof the nasa earth observing system data and information system, the mission of the po.daac is toarchive and distribute data on the physical state of the oceans. unlike the data at the nodc, most of thedata sets at the po.daac are derived from satellite observations. data products include seasurfaceheight, surfacewind vector, surfacewind stress vector, surfacewind speed, integrated water vapor,atmospheric liquid water, seasurface temperature, seaice extent and concentration, heat flux, and in situdata that are related to the satellite data. the satellite missions that have produced these data include thenasa ocean topography experiment (topex/poseidon, done in cooperation with france), geos3,nimbus7, and seasat; the noaa polarorbiting operational environmental satellite series; and thedodõs geosat and defense meteorological satellite program.summary of major issuesthe results of scientific research are disseminated in this country through a hybrid system thatincludes professional society and other notforprofit publishers, the commercial sector, and the government. the formal journals are published largely by the professional society and commercial sectors,while government agencies manage less formal reports (gray literature). secondary services, such asabstracting and indexing, provide access to this literature, increasingly by electronic means. while thereare strains in this system because of rising costs, increasing workload, and issues related to the protectionof intellectual property, it has served u.s. science well and has been an invaluable link in the process oftranslating scientific advances into further advances, useful technology, and economic benefits.the current system, however, is not well suited to handle the scientific electronic databases that arethe focus of this study. the costs of maintaining these databases are typically too great to be covered byuser fees; instead, these databases must be considered part of the national scientific heritage. somegovernment agencies have accepted responsibility for maintaining and disseminating data resulting fromtheir own research and development. in some cases, this system is working reasonably well, but in othersthere are problems even with providing current access. archiving for the long term raises questions in allcases, however.a general problem common to all scientific disciplines is the low priority attached to data management and preservation. experience indicates that new experiments tend to get much more attention thanthe handling of data from old ones, even though the payoff from optimal utilization of existing data maybe greater. for instance, according to figures supplied by noaa, noaaõs budget for its national datacenters in fy 1980 was $24.6 million, and their total data volume was approximately one terabyte. inpreserving scientific data on our physical universe: a new strategy for archiving the nation's scientific information resourcescopyright national academy of sciences. all rights reserved.30preserving scientific data on our physical universetable 2.4 national oceanographic data center dataholdings (as of october 1994)disciplinevolume(megabytes)physical/chemical datamaster data filesbuoy data (wind/waves)9,679currents4,290ocean stations1,645salinity/temperature/depth1,557bt temperature profiles872sea level125marine chemistry/marine pollutants89other68subtotal18,325individual data sets, for examplegeosat data sets12,841coastwatch data60,000levitus ocean atlas 1994 data sets4,743other (estimated)11,000subtotal88,584total physical/chemical106,909marine biological datamaster data filesfish/shellfish115benthic organisms69intertidal/subtidal organisms30plankton32marine mammal sighting/census21primary productivity7subtotal274individual data sets, for examplemarine bird data sets52marine mammal data sets4marine pathology data sets4other (estimated)200subtotal260total biological534total data holdings107,443source: noaa, private communication, 1994.preserving scientific data on our physical universe: a new strategy for archiving the nation's scientific information resourcescopyright national academy of sciences. all rights reserved.the challenge: preservation and use of scientific data31fy 1994, the budget was only $22.0 million (not adjusted for inflation), while the volume of theircombined data holdings was about 220 terabytes! during this same period, the overall noaa budgetincreased from $827.5 million to $1.86 billion.with regard to laboratory data, government programs have existed since the 1960s to compile resultsfrom the world scientific literature, to check the data carefully, and to prepare databases of criticallyevaluated data. for instance, the national institute of standards and technology operates its standardreference data program, which covers a broad range of data in physics, chemistry, and materialsscience. the department of energy also supports a number of data centers of this type. despite chronicunderfunding, these programs have produced databases of lasting value to the nation. to cite oneexample, the mass spectral database managed by the national institute of standards and technology,the national institutes of health, and the environmental protection agency contains spectra of over60,000 compounds. it has been installed in many thousands of mass spectrometers that are being used formonitoring environmental pollution, designing drugs, characterizing new materials, and many otherapplications. the government investment in creating and maintaining this database has been repaidmany times over.in the area of observational databases, the situation is mixed. federal agencies collect large amountsof observational data, which in many cases are continuously added to the available record of earth andspace processes. the data sets resulting from these activities sometimes are welldocumented andmaintained in readily accessible form; but in many other cases, they are exceedingly difficult orimpossible to access or use, and thus are effectively unavailable. in general, the agencies and otherorganizations do a good job of making data and information available to the scientists (primary users)during the active stages of projects and for some time afterward. examples of notable successes includethe nasa planetary data system, where the premise has been that the data have longterm value andmust be accessible indefinitely into the future, and the noaa national data centers, where the policy isto migrate archived data to new media every 10 years.technological advances have kept pace with the large growth in data volumes in scientific disciplines such that the longterm retention of all or nearly all of the data collected is feasible. indeed, in mostfields the entire collection of data from the past is not large in comparison with the current and anticipateddata volumes that will be collected during only a year or two. however, significant fractions of the olderdata are difficult or in some cases impossible to access, because they have not been transferred to newstorage media. this transfer often has received low priority because many data management and dataretention activities are chronically underfunded and just handling the current data flow uses nearly all ofthe available resources. thus, many valuable data sets are stored on lowdensity round tapes or onspecialized magnetic tape media requiring hardware that is now obsolete or inoperable. for example, alarge volume of the early landsat coverage of the earth resides on tapes that cannot be read by anyexisting hardware. recent datarescue efforts have been successful in getting older data into accessibleform, but these efforts are timeconsuming and costly. the reason these efforts have been undertaken,particularly in the observational sciences, is the recognition that retrospective data are vital to understanding longterm changes in natural phenomena. given the extraordinarily rapid advances in computing and storage technology in recent years, planned periodic migration of data to new media will beincreasingly important in all scientific disciplines to ensure longterm access to our scientific dataresources.it is axiomatic that a database has limited utility unless the auxiliary information required tounderstand and use it correctlyñthe metadatañis included in the record. an unambiguous descriptionof the storage format is obviously essential for interpretation of an electronic database. the requirementis even more stringent to support meaningful access to data over the long term, because the hardware,software, and even the language by which formats are described will likely be different decades andcenturies from now. the same is true regarding the scientific details of the content of the data. auxiliaryinformation such as environmental conditions (e.g., temperature and pressure), method of calibrating thepreserving scientific data on our physical universe: a new strategy for archiving the nation's scientific information resourcescopyright national academy of sciences. all rights reserved.32preserving scientific data on our physical universeinstruments, and data analysis techniques must be given to be able to fully and correctly use the data.providing this information is time consuming and costly if done retrospectively, but much less so if it isprepared at the time the data are collected. documentation that is inadequate for understanding and usingthe data greatly diminishes the value of the data, particularly for secondary and tertiary users.another major problem inhibiting access to data is the lack of directories that describe what data setsexist, where they are located, and how users can access them. this, too, is especially a problem forpotential secondary and tertiary users. in many cases the existence of the data is unknown outside theprimary user groups, and even if known, there frequently is not enough information for a potential user toassess their relevance and usefulness. this realization has resulted in an interagency effort, led bynasa, to build a master directory of global change data and information. this master directory isintended to inform users of where data sets of potential interest reside and how to access them. similardirectories are needed in other scientific disciplines, as well as across all disciplines. the lack ofadequate directories adversely affects the exploitation of our national data resources and commonly leadsto unnecessary duplication of effort.a significant fraction of the archived scientific data is held by the federal agencies that collected thedata as part of their mission. however, a large amount of valuable scientific data gathered with federalfunds is never archived or made accessible to anyone other than the original investigators, many of whomare not government employees. in many instances, the organizations and individuals that receivegovernment contracts or grants for scientific investigations are under no obligation to retain the datacollected, or to place them in a publicly accessible archive at the conclusion of the project. at best,scientists in the same field may be able to obtain desired data sets on an ad hoc basis by contacting theoriginal investigators directly; secondary and tertiary users typically are unaware of the existence of thedata and have no mechanism (other than personal contact) to access the data. thus, data sets thatcommonly are gathered at great expense and effort are not broadly available and ultimately may be lost,squandering valuable scientific resources and much of the public investment spent in acquiring them.clearly, there is a great need for the agencies to get more return on their investment in science by thesimple expedient of making the data collected under their auspices accessible to others.as seen from the discussion in earlier sections and addressed in detail in the individual disciplinepanel reports (nrc, 1995), there is a large and diverse collection of scientific data and information extantin federal agencies and nonfederal organizations, including state and local agencies, universities, notforprofit institutions, and the private sector. at a minimum, those data that are acquired with the support offederal funding should be regarded as part of the national scientific information resource.finally, naraõs holdings of scientific and technical data in electronic or any other form are verysmall in comparison to the data holdings of these other organizations. moreover, naraõs budget for itscenter for electronic records, which has formal responsibility for archiving all types of federalelectronic records, was only $2.5 million in fy 1994, a budget lower than that of many of the individualagency data centers reviewed by the committee in this study. given naraõs current and projected levelof effort for archiving electronic scientific data, it is obvious that nara will be unable to take custody ofthe vast majority of the scientific data sets that require archiving. therefore, a coordinated effortinvolving nara, other federal agencies, certain nonfederal entities, and the scientific community isneeded to preserve the most valuable data and ensure that they will remain available in usable formindefinitely. the challenge is to develop data management and archiving infrastructure and proceduresthat can handle the rapid increases in the volumes of scientific data, and at the same time maintain olderarchived data in an easily accessible, usable form. an important part of this challenge is to persuadepolicymakers that scientific data and information are indeed a precious national resource that should bepreserved and used broadly to advance science and to benefit society.preserving scientific data on our physical universe: a new strategy for archiving the nation's scientific information resourcescopyright national academy of sciences. all rights reserved.333retention criteria and the appraisal processthe national archives and records administration appraises and retains records on the basis of theirinformational and evidential value. it is concerned with records of longterm valueñthose records thatwill probably have value long after they cease to have immediate, or primary, uses. although scientificdatabases can provide evidence of the research conducted by an agency, their value is primarilyinformational; it is based on the content of the records rather than on their description of activities by theagency that collected or created them.special problems arise in appraising scientific data for their longterm value, particularly beyond thecommunity of research scientists working in the specific field to which the measurements refer.scientific data are voluminous, constantly increasing, and often difficult for those in other fields to use intheir original formats. the data typically are expensive to collect, provide baselines for future observations, enhance understanding of other data, and are of immense importance for advancing scientificknowledge and for educating new scientists. the data also are important to an understanding of the worldin which we live; the data (or the conclusions drawn from them) may be important to economists,historians, statisticians, politicians, and the general public. at the same time, it is difficult to predict thefull value of the data to researchers and other users decades or centuries from now, although pastexperience has shown that scientific data collected many years ago provide unique contributions to newunderstanding of our physical universe.retention criteriathe criteria that follow are to be used during the appraisal process to determine retention of physicalscience data. they should be appliedñby those responsible for stewardshipñto all physical sciencedata, whether created by small individual projects or in the course of largescale research programs.similar criteria and guidelines must be developed for data in other disciplines. this is a topic of primaryconcern not only to nara, noaa, and nasa, but to all scientists, data managers, and archivists whowork with such records, and was provided in the charge to the committee as a central issue. although thecommittee found that many retention criteria apply to both the observational and the laboratory sciences,significant differences are noted below. the metadata requirements, which tend to be either poorlyunderstood or ignored, are given particular emphasis. additional details and distinctions are discussed inthe working papers of the discipline panels (nrc, 1995).preserving scientific data on our physical universe: a new strategy for archiving the nation's scientific information resourcescopyright national academy of sciences. all rights reserved.34preserving scientific data on our physical universe criteria common to both observational and laboratory sciences¥uniqueness of data. do other authenticated copies of the data under consideration already exist inan accessible repository that meets nara standards of permanence and security? if so, are theyadequately backed up? if the answers are yes, the data set need not necessarily be retained.¥accessibilityñadequacy of documentation. though we might wish that all data sets were of highquality and accompanied by detailed metadata, that is not always the case. at a minimum, the metadatashould be sufficient for a scientist working in the discipline to make use of the data set. if documentationis lacking or is so poor that a data set is not likely to be of value to someone interested in data of that type,or the data are more likely to mislead than to inform, that data set should have a low priority for archiving,or perhaps should not be archived even if resources are available. nevertheless, the committee does notbelieve that many data sets should be purged because they lack sufficient documentation. the vastmajority of data sets now meet minimum standards of documentation, which means that a skilled usereither is given sufficient information or can figure it out. adequacy of documentation is thus but onecriterion to consider in the appraisal of data for longterm retention. metadata requirements are discussedin greater detail below.¥accessibilityñavailability of hardware. is the hardware needed to access the data obsolescent,inoperable, or otherwise unavailable? if so, the data are not usable. decisions on whether to keep suchdata should be based on the feasibility of building or acquiring the necessary hardware, the usability ofthe data if they were accessible, and the nature of the data set, if known. to avoid this situation, migrationof data to current storage media should be part of the normal routine to maintain the archive.¥cost of replacement. could the data be reacquired if a future national need for the data were toarise? if so, would reacquisition of the data be more costly than their preservation? for the observationalsciences, the answer is almost always that the data cannot be reacquired. the exception is with a data setin a discipline in which the changes of nature are so slow that the data could be recaptured at another time.for example, data on the fossil record of evolution contained in stratigraphic rock units could bereacquired.the laboratory sciences generate data that can, in principle, be reacquired. the question is whetherthe data can be reproduced at an acceptable cost. data sets in the laboratory sciences that are candidatesfor longterm preservation can be classified into three generic types: (1) massive records and data froman original experiment, particularly a costly òmegaexperiment,ó that there is no realistic chance ofreplicating (e.g., data obtained from expensive facilities such as plasma fusion devices, or data of interestin physics and chemistry derived from special events such as nuclear tests); (2) unique, perhaps sampledependent or environmentdependent, engineering data, many of which never reach the publishedliterature; and (3) critically evaluated compilations of data from a large number of original sources,together with the backup data and documentation on selection of recommended values, that representtremendous accumulated effort.¥peer review. has the data set undergone a formal peer review to certify its integrity andcompleteness, or is there documented evidence of use of the data set in publications in peerreviewedjournals? have expert users provided evidence that this data set is as described in the documentation?formal review of data sets is not now common. it should be encouraged, however, especially in theobservational sciences. a good model is the peer review system for nasaõs planetary data system. inthe laboratory sciences, the critically evaluated compilations of data referred to in chapter 2 haveundergone extensive peer review.differences between the observational and the laboratory sciencesdata derived from laboratory experiments, such as the hardness of steel produced in a particular melt,differ from data based on observations of transient natural phenomena, such as the records of the 1993preserving scientific data on our physical universe: a new strategy for archiving the nation's scientific information resourcescopyright national academy of sciences. all rights reserved.retention criteria and the appraisal process35midwestern floods. thus, they stimulate different questions related to data preservation issues. as hasalready been noted, one difference arises from the fact that transient natural phenomena are notreproducible; the fact that the resulting observational data are òsnapshots in timeó sometimes means thatthe data have historical or evidential value in addition to their informational value. observational datasets that provide a continuous timeseries record of the physical universe, or of human impact upon it, areimportant to future generations for comparison and the identification of trends. in addition, manyobservational data sets represent major engineering or workerintensive collection activities that warrantdocumentation and could not feasibly be carried out again.experimenters have good reason to believe that if and when their data are recreated in the future,instruments will be better. in many experiments, raw data (e.g., the initial sensor readings before anytransformations, conversions, averaging, or corrections are made) may exist only for a fleeting instantbefore they are discarded or further processed. even when raw (level0) data are acquired and saved,principal investigators frequently fail to provide appropriate documentation because they do not expectanyone else to use these data. instead, the processed data sets are more likely to have adequate metadataand meet the committeeõs other criteria for retention.quite the opposite situation seems to prevail for the observational sciences, where many secondaryscientific users feel they need to be able to get back to the level0 data and are becoming more active indemanding that the collectors of the data provide adequate metadata.special issues in the retention of observational dataall observational data that are nonredundant, reliable, and usable by most primary users should bepermanently maintained. this judgment is based on the committeeõs belief that advancing technologiesand better data management practices make it possible to stay ahead of the growing data volumes, asdiscussed in chapter 4. it also is likely that it will be more expensive to reappraise data sets than simplyto keep them. if the committee is wrong on these two counts, it may be possible that the volume of thedata can be reduced through sampling techniques and through intelligent selection of the data sets ofhighest priority, as explained below.data sampling issues arise in measurement systems and in considering archival strategies to provideready user access. even before a data manager faces archiving decisions, many sampling rate decisionsalready have been made. for example, in the atmospheric sciences, we could easily sample temperaturesensors and wind gauges 100 times per minute, but that frequency is unnecessary for nearly all uses. ingeneral, it is necessary to keep only data properly sampled in time and space; that is, the sampling intervalmust be such that the mostrapidlyvarying component is not aliased. at least two samples per cycle arerequired according to the sampling theorem. thus reduction of oversampled data to the minimumsampling rate needed, coupled with lossless data compression, can significantly reduce data volumeswith no loss of scientific content. however, if the phenomena of interest are slowly varying, then morerapid fluctuations, which might have value for other purposes, can be filtered out and the data reduced toretain the desired data unaliased; this technique can further reduce the data volume at the expense oflosing higherfrequency data. the archiving of only òrepresentativeó subsets of our largest data sets isoften suggested, but the notion raises difficult issues in statistics, data management philosophy, andbudgeting. in concept, there may be acceptable procedures for the longterm archiving of representativesubsets of large data sets, but no effective methodology exists today to choose those that would satisfy theneeds of future users.an example of the approach to deciding which observational data sets to retain comes from theatmospheric sciences. in this field the value of a data set as part of a long time series is an importantcriterion for archiving decisions. the temperature record for a given year from a station operating over acentury is much more valuable than a similar record from a nearby station with a shorter lifetime. studiesof climate change and other types of environmental change find long time series to be essential. forpreserving scientific data on our physical universe: a new strategy for archiving the nation's scientific information resourcescopyright national academy of sciences. all rights reserved.36preserving scientific data on our physical universeexample, confirmation of the seasonal stratospheric ozone depletion over the antarctic in the 1980srequired reference back to the dobson column ozone data from the first half of this century forcomparative purposes. the u.s. historical climate network data are a high priority for archivingbecause they represent a long time series of highquality data, with excellent metadata; this combinationof attributes of data of a common type makes the overall data set exceptionally valuable.metadata issuesthe committee has arrived at several related conclusions concerning the importance of documentation, or metadata, to the effective archiving of scientific data. these include the following:¥effective archiving needs to begin whenever a decision to collect data is made.¥originators of data should prepare them initially so they can be archived or passed on withoutsignificant additional processing.¥the greatest barrier to contemporary and future use of scientific data by other researchers,policymakers, educators, and the general public is lack of adequate documentation.¥a data set without metadata, or with metadata that do not support effective access and assessmentof data lineage and quality, has little longterm use.¥for data sets of modest volume, the major problem is completeness of the metadata, rather thanarchiving cost, longevity of media, or maintenance of data holdings.¥lack of effective policies, procedures, and technical infrastructureñrather than technologyñisthe primary constraint in establishing an effective metadata mechanism.this suite of conclusions led the committee to recommend that òadequacy of documentationó be acritical evaluation criterion for data set retention. the following discussion illuminates the multipleperspectives of metadata, the essence of the problem, and important elements of any metadata solution.perspectives on metadatathe term metadata often is used to denote òdata about data,ó that is, the auxiliary information neededto use the actual data in a database properly and to avoid possible misinterpretation of those data. theterm is used in many scientific disciplines, but not always with precisely the same meaning. somecomments on different types of metadata may be helpful.the most basic class of metadata comprises the information that is essential to any use of the data.an obvious example is the units in which physical quantities are expressed. if units are not specified, thenumbers are ambiguous; at best, the user must attempt to deduce the units by comparison with other datasources. in dealing with observational data, the coordinates and the coordinate system (spatial andtemporal) obviously must be specified. laboratory data are often sensitive functions of some environmental condition such as temperature or pressure. for example, the boiling point of a liquid varies withpressure, so that a boiling point value has no meaning unless the pressure is specified. although this iswell known, many mistakes occur when a user assumes a value taken from a compilation to be a boilingpoint at normal atmospheric pressure, while it actually refers to a reduced pressure.a significant problem in planning a longterm data archive is simple carelessness on the part of thecreators and custodians of the data. current practitioners in a scientific field may implicitly understandwhat the units or environmental conditions are. shortcuts are taken by the authors that cause no problemin communicating with their contemporary colleagues (although they may be confusing to those in adifferent discipline), but practices and language can change over a generation or two. for a longtermarchive, even the most obvious metadata should be specified in detail.preserving scientific data on our physical universe: a new strategy for archiving the nation's scientific information resourcescopyright national academy of sciences. all rights reserved.retention criteria and the appraisal process37beyond this basic type of metadata, there is auxiliary information that is not needed by the majorityof users (present or future), but is of interest to a few specialists. included here are the parameters thathave only a slight influence on the data in question, so that most users do not need to know about them.for example, the typical user of a database of atomic spectra is concerned only with the wavelength anda rough value of the intensity of each spectral line. however, a few users who are trying to extract furtherinformation from the data may want to know the conditions under which the spectrum was recorded, suchas the current density, type of electrode, and gas pressure. referring to the janaf thermochemicaltables, which are discussed in the physics, chemistry, and materials sciences data panel report (nrc,1995), most users are perfectly content with the values given (along with the confidence that thecompilers did a good job of selecting the most reliable values). a minority of users, however, will wantmore details on how the data were analyzed, such as whether the heat capacity values were fitted to afifthdegree polynomial or a cubic spline, and so forth.perhaps the most pervasive form of metadata is the accuracy of the values. to a purist, no numberhas meaning unless it is accompanied by an estimate of uncertainty. specifying the uncertainty of eachdata point increases the size and complexity of the database, but sometimes may be necessary. at aminimum, the metadata should include general comments on the maximum expected errors, even if aquantitative measure such as standard deviation cannot be given. finally, the term metadata is sometimesunderstood to encompass the full documentation necessary to trace the pedigree on the database. forlaboratory data, this includes citations to all the primary research papers relevant to the database. acritical evaluation of especially important quantities (such as the fundamental physical constants or keythermodynamic values) may end up with only a few hundred data points, but include massive documentation and citations to a hundred years of literature. in such cases the metadata occupy far more spacethan the data themselves.from this discussion, it is evident that metadata can span the range from a few simple statementsabout the data to very extensive (and expensive) documentation. it is difficult to give general guidelineson the amount of metadata needed; each case must be considered in the context of how future users mayuse the data and what auxiliary information they will need. some guidance may be obtained from formalefforts to set metadata standards for experimenters to follow in preserving their data. in chemistry, forexample, many organizations have developed detailed recommendations on reporting data from specificsubfields. these have been collected in a recent book, reporting experimental data (acs, 1993). theamerican society for testing and materials committee e49 on computerization of material propertydata has an ambitious program to develop consensus standards for metadata requirements for databasesof properties of engineering materials. these documents emphasize that metadata requirements must beapproached on a casebycase basis and must involve experts in each field.the conclusion is that metadata, whatever the particular form, are crucial to the use of almost everydata set and must be included in any archiving plan. the necessary metadata usually add very little to thestorage requirements, but may require considerable intellectual effort to prepare, especially if they areassembled retrospectively rather than when the data are first collected.the preceding discussion defines metadata from the perspective of the research scientist. anadditional, and somewhat overlapping, perspective is provided by the computer science community. inthis community, the term metadata refers to the specification of electronic representation of individualdata items, the logical structure of groups of data items, and the physical access and storage media andformats that hold the data. to the computer scientist or database administrator, the contextual data thatthe research scientist refers to as metadata encompass other data entities. in fact, divergence can existeven among research scientists as to the differences between data and metadata. what is metadata for onemay be data for the other.in view of this confusion, the committee has chosen to keep the term metadata and to explicitly defineits fundamental components. as such, the committee views metadata as representing information thatpreserving scientific data on our physical universe: a new strategy for archiving the nation's scientific information resourcescopyright national academy of sciences. all rights reserved.38preserving scientific data on our physical universesupports the effective use of data from creation through longterm use. it spans four ancillary realms:content, format or representation, structure, and context. the content realm identifies, defines, anddescribes primary data items including units, acceptable values, and so forth. the representation realmspecifies the physical representation of each value domain, often technology dependent, and the physicalstorage structure of aggregated data items, often arbitrary. the structure realm defines the logicalaggregation of items into a meaningful concept. the context realm typically supplies the lineage andquality assessment of the primary data. it includes all ancillary information associated with thecollection, processing, and use of the primary data. on the basis of this explicit definition, the followingsection describes metadata objectives, implementation issues, and potential for defining a standardizedframework.analysis of metadata: from challenge to solutionthe problem of data set documentation is receiving increased attention in the context of scientificdata management. in the earth sciences, global climate change research and general environmentalconcerns have ignited interest in a more interdisciplinary and longterm approach to conducting science.interdisciplinary collaboration requires more effective sharing of data and information among individualresearchers, disciplines, programs, and institutions, all of which may operate under different paradigmsor have different terminology for similar concepts (nrc, in press). further, longterm research requiresthat researchers be able to access and compare data sets that were created by past researchers andcollected in different contexts by different technologies. therefore, to support the interdisciplinarysharing and longterm usefulness of data, adequate metadata must be included within a framework thataccomplishes the following objectives: ¥provides meaningful selection criteria for accessing pertinent data; ¥supports the translation of logical concepts and terminology among communities; ¥supports the exchange of data stored in differing physical formats; and ¥enhances the assessment of data sets by consumers.a critical question is how to motivate the user community to participate in the process of metadatapreparation and standardization. the issue of motivation is best addressed by the value system of thecommunity itself. it may be argued that the problem will not be solved until the production of verifieddata sets and their provision to scientific colleagues become more highly valued activities. developments such as the peerreviewed publication of data sets should contribute to this shift in values.however, until these activities are assimilated into the fabric of career advancement, such as beingincorporated into criteria for tenure in academic institutions, progress will continue to be slow anduneven.nevertheless, there are a number of specific actions that can be taken to promote the preparation andstandardization of metadata. funding agencies could help facilitate change by requiring and enforcingminimal documentation of data sets created under their grants (as well as other desirable data management and archiving practices discussed elsewhere in this report). this will not be an effective mechanism, however, unless the minimal standards for consistency and completeness are provided as a targetfor grantees and as a measuring stick for the funding agent. to be effective, these standards must becreated through the collaboration of researchers, data managers, librarians, archivists, and policymakers.individuals and institutions in the scientific community could contribute by recognizing that datamanagement and the provision of appropriate documentation of data are an essential science infrastructure function spanning all disciplines. greater costeffectiveness, consistency, and quality can beachieved if the many diverse data management activities are better coordinated. the essential requirement for making these value system changes and developing effective solutions is the recognition that allpreserving scientific data on our physical universe: a new strategy for archiving the nation's scientific information resourcescopyright national academy of sciences. all rights reserved.retention criteria and the appraisal process39segments of the scientific community need to be educated on this issue. funding agencies and thescientific community thus must move forward together in the development of a coherent strategy for endtoend management that focuses on metadata requirements as a major element.the ultimate solution for metadata handling will include an approach that not only supports thedocumentation of a data set throughout its life cycle, but also supports evolutionary documentationrequirements. for example, early in the development and use of an instrument system, the scientificcommunity may not be able to specify completely what metadata will be important for the effective use ofthe observations produced by this system. in this case, some of the documentation may include freeformnarratives without the benefit of controlled vocabularies. documentation of this nature is useful only toa limited audience that understands the specialized vocabulary of the source instrument, project, discipline, or institution. in addition, it is still difficult to make these descriptions useful to an automatedagent performing a search on behalf of a user. as instrument use becomes more routine, this documentation could evolve to a more structured, but not cumbersome, form. one potentially useful approachconstrains the textual descriptions to a welldefined, controlled vocabulary. if the vocabulary is clearlyspecified and made easily available with the data and associated documentation, users beyond thoseclosely associated with the creation of the data set may be able to use this information to assess itsrelevance, significance, and reliability. eventually, this more structured alternative will evolve into thespecification of structured records with appropriately defined fields, standard value domains, andrelationships with data set records. the committee also expects that improvements in software for naturallanguage understanding will enable the automatic translation of freeform narratives into easily searchedmetadata fields.an equally important component of the metadata solution is the identification and detailed definitionof classes of information that are critical to the complete and consistent documentation of data sets.information modeling techniques can be used to develop these classes of information, some of which willhave clear, concise definitions and a set of defined attributes, while others will be identified but will nothave clearly defined attributes or boundaries with other classes. the resulting information model shouldpresent a technologyindependent description of metadata entities and their relationships with theprimary data. the model should identify metadata that may be generalized across all classifications ofdata sets and usage patterns, as well as accommodate specialized needs. such a model should provide thebasis for intelligent information policies, data management practices, and metadata standards. theinformation policies, however, must not saddle data providers with long, cumbersome òformsó to fill out.that would discourage the contribution of the data themselves, and the committee recognizes that datawith incomplete documentation are better than no data at all. nevertheless, appropriately establishedmetadata standards do not necessarily need to be difficult or costly to apply, and therefore need not beonerous to the data provider. an example of a generalized metadata framework in the observationalsciences is presented in the working paper of the ocean sciences data panel (nrc, 1995).other elements of the appraisal processa data management plan should be created for any new research project or mission plan, consistentwith the requirements of omb (1994) circular a130. a good example of this is the project datamanagement plan of the nasa national space science data center (nasa, 1992). at a minimum,those individuals who have responsibility for implementing the data management plan and ensuringaccessibility and maintenance of the data should play a key role in the subsequent appraisal process.most individual investigators and peer reviewers do not recognize their roles as appraisers forarchival purposes, but the views of these experts should weigh heavily in the decisions relating to longterm value or permanency of the data obtained. the principal investigators and project managers whocollect and analyze the data clearly have the best sense of how long the data will be valuable for their ownscientific purposes. primary users also can provide a detailed understanding regarding the uses of thepreserving scientific data on our physical universe: a new strategy for archiving the nation's scientific information resourcescopyright national academy of sciences. all rights reserved.40preserving scientific data on our physical universedata for their own discipline, but they may not comprehend the longterm value of the data for applicationto other research or national problems. because such primary users and other data collectors sometimesdo not think beyond their own needs, the agencies should work with nara to provide good documentation at the inception of scientific projects, especially documentation that would be useful to secondaryand tertiary users. although providing more extensive documentation often may be viewed as an extraburden by the principal investigators and data managers, the labor and expense can be minimized if it isplanned at the inception of a project, whereas it is extremely difficult after the project is completed.proper data management practices can be promoted by considering data management in the evaluation ofan investigatorõs past performance.because many scientific endeavors require participation by a number of agencies and organizations,it is important to coordinate data management activities and assign responsibilities for the maintenance ofthe data during periods of primary use. nara is currently responsible for the final appraisal of federalrecords and the determination of their value as accessions to the permanent national collection under itsstatutory mandate. however, nara should take advantage of the expertise of the other participantsinvolved throughout the life cycle of the data.the committee believes that all stakeholdersñscientists, research managers, information management professionals, archivists, and major user groupsñshould be represented in the broad, overarchingdecisions regarding each class of data. the appraisal of individual data sets, however, should be seen asan ongoing, informal process associated with the active research use of the data, and therefore should beperformed by those most knowledgeable about the particular datañprimarily the principal investigatorsand project managers. in some cases, they may need to involve an archivist or information resourcesmanager to help with issues of longterm retention. although the committee believes that formalappraisals should be kept to a minimum, appraisals should be performed according to the data management plan established for each project.although the committee was not expressly charged with advising on classified data, there is anobvious need to save classified scientific data as well. the complete records of the atmospheric atomicbomb tests are a clear example. it is more difficult to provide and assess metadata for a classified data set,and it costs more to maintain classified data. also, there is a tradeoff between the value of the data fornational security, the risk to national security if the data are declassified, and the potential value to societyof having the data declassified. thus, it is highly beneficial and costeffective to have mechanisms inplace that consider these issues periodically for any given classified data set and that promote declassification when appropriate.recommendationsthe committee makes the following recommendations regarding the retention criteria and appraisalprocess for physical science data:as a general rule, all observational data that are nonredundant, useful, and documented wellenough for most primary uses should be permanently maintained. laboratory data sets arecandidates for longterm preservation if there is no realistic chance of repeating the experiment, orif the cost and intellectual effort required to collect and validate the data were so great that thelongterm retention is clearly justified. for both observational and experimental data, the following retention criteria should be used to determine whether a data set should be saved: uniqueness,adequacy of documentation (metadata), availability of hardware to read the data records, cost ofreplacement, and evaluation by peer review. complete metadata should define the content, formator representation, structure, and context of a data set.the appraisal process must apply the established criteria while allowing for the evolution ofcriteria and priorities, and be able to respond to special events, such as when the survival of datapreserving scientific data on our physical universe: a new strategy for archiving the nation's scientific information resourcescopyright national academy of sciences. all rights reserved.retention criteria and the appraisal process41sets is threatened. all stakeholdersñscientists, research managers, information managementprofessionals, archivists, and major user groupsñshould be represented in the broad, overarchingdecisions regarding each class of data. the appraisal of individual data sets, however, should beperformed by those most knowledgeable about the particular datañprimarily the principalinvestigators and project managers. in some cases, they may need to involve an archivist orinformation resources professional to assist with issues of longterm retention.classified data must be evaluated according to the same retention criteria as unclassified datain anticipation of their longterm value when eventually declassified. evaluation of the utility ofclassified data for unclassified uses needs to be done by stakeholders with the requisite clearancesto access such data.preserving scientific data on our physical universe: a new strategy for archiving the nation's scientific information resourcescopyright national academy of sciences. all rights reserved.42preserving scientific data on our physical universe424the opportunities: the relationship of technologicaladvances to new data use and retention strategiesrapid progress in information technology continually alters both the quantity and the quality ofscientific information and periodically stimulates fundamental modification of data management andarchiving strategies. recent technological advances have enabled new methods and strategies for datastorage and retrieval and have created better ways of connecting users to data resources and to each other.moreover, the evolving technologies are catalysts for revising organizational structures to managescientific data archives much more effectively in a distributed manner. assumptions about effectivemanagement of scientific data that have been long and firmly held are being directly challenged by newinformation technology. these assumptions have been based on experience with management of paperrecords, generally in domains outside of science. some of the outdated assumptions that are rapidlylosing their relevance include the following:¥physical possession of the data is essential to their management and archiving. this principlehas outlived its usefulness in the context of electronic physical science data and has made access difficultfor legitimate users. electronic information is easily copied and disseminated. this feature removesconstraints imposed by the limited physical access. because most government physical science data areconsidered to be in the public domain, the constraints of copyright and fee collection to the freemovement of data are removed as well.¥cost of an archive increases in proportion to collection size and use. physical archive cost is afunction of space, as well as cataloging, repair, and access efforts. improved inventory technology haseased some of the cost burden over the last several years, but, fundamentally, archives with large physicalholdings operate in traditional ways with linearly scaling costs. such costs actually discourage use, sincephysical handling of items scales with use, whereas budgets reflect usage indirectly. in contrast,electronic information storage and management costs have declined as rapidly as the costs of computertechnology and processing over the last 30 years. there is no foreseeable end to this process. storing andusing the next byte will be cheaper than storing and using the most recent byte for a long time to come.¥only archivists and librarians have the capabilities to manage archived data. while librariansand archivists are important advisors and participants in scientific data management, the dominantmanagement responsibility falls to the scientific community and its designated scientific data managers(who are a blend of scientist, computer scientist, and librarian/archivist). if practicing scientists do notparticipate in the management of scientific information, such data will fall into obscurity or obsolescence.preserving scientific data on our physical universe: a new strategy for archiving the nation's scientific information resourcescopyright national academy of sciences. all rights reserved.the opportunities: the relationship of technological advances to new data use and retention strategies43¥the locator information (catalog) about the managed objects is simple and compact. findingrelevant scientific information often requires searching the full contentñand this content generally is notin the conveniently compressed form of text. for example, to search for all data sets where thestratospheric ozone concentration is less than some ad hoc threshold in some region, one would need toexecute a complex algorithm on every data sample covering the region in question. queries such as thisbecome even more complex if the region of interest is determined after retrieval (e.g., how many days ina row was the areal extent of the ozone hole over open ocean greater than 5,000 square kilometers?). theselection and use of scientific data to solve complex problems can be simplified through the use of theconcept of browsing information based on content. browsing often involves examination of largenumbers of samples and data volumes. specialized òbrowsing productsó can be defined to locate recordsof interest. for the query examples above, lowresolution ozone maps could be used to find candidatedata sets with high probability of relevance. information about the processes (including sensor characteristics, computer program capabilities, and calibration points) used to develop the data set is needed for itsproper use. such information increases the size and complexity of the locator service.the remainder of this chapter describes how advancing information technologies enable the datamanager, librarian, and archivist to deal with the challenges of scientific data managementñin acollaborative fashion with the scientific user community.enabling technologies and related developmentstable 4.1 provides a summary of aspects of scientific data management changed by new technologiesand related developments. these six areas are discussed in more detail below.highperformance computer networksthe rapid expansion of computer networks and their use for electronic mail and database access haveobviated the need for researchers and other users of scientific and technical data to be in physicalproximity to colleagues, information resources, and even advanced technical facilities. this haspresented a menu of choices about the best means to distribute data and the responsibility of managingthem.a worldwide, òvirtualó library is being created on the internet. application programs such as mosaicare demonstrating the power of free and simple navigation across an ocean of available resources.improving network capacity, reliability, performance, and security measures are helping to make theseresources more widely accessible and useful.highperformance networks also support movement of information for new applications (e.g., forproducing safely managed backup copies, òprofilingó information for individual userõs needs, or stagingdata through a number of refinement steps in different locations for focused research). networks supportcollaborative work and research projects that span traditional research boundaries. such work requireseasy access to a variety of data sources at once.highperformance networks enable scientific data resources to be widely distributed and managed bygroups of scientists. users thus are freed to concentrate on the most effective use of the data, rather thanon their own data management issues. networks can provide a vehicle for regularly distributing backupcopies of data and metadata to ensure safe storage. distribution of data to users can be done via thenetwork in addition to, or instead of, via physical media such as tapes and cdroms. data can be linkedtogether to help users navigate among related items. this kind of linking is at the heart of the world wideweb concept and brought to users by mosaic. the population of information providers (e.g., people whocan contribute to the knowledge base) has now grown to include all networked members of a userpreserving scientific data on our physical universe: a new strategy for archiving the nation's scientific information resourcescopyright national academy of sciences. all rights reserved.44preserving scientific data on our physical universetable 4.1 new technologies and related developments that enable a new strategy for the management ofscientific and technical datanew technology trendsand related developmentskey featureswhat is enabled?highperformance computer networksdistributed functions; rapidlocation of databases and archivesdelivery of large data volumeswhere best managed; collaborativework; distributed organizations;distributed responsibilitylow and declining cost of storageinexpensive backup; continuallydeferral of archiving decisions; trust indeclining cost; ease of migration distributed management due to safe storage backupadvanced data managementability to rigorously and formallymore complex data structures (othermanage diverse data typesthan òflat filesó) handled in archives,with great potential advantageschanging requirements forability of personnel with lowerability to entrust scientific datainformation technology professionals technical skills to succeed inmanagement in a distributed data management rolesenvironmenthigh reliability of technology componentsavailability of better componentsreduced cost and effort in dataand connections; reducedmigration; trusted connections forprocurement and operations costscommunication and collaborationdevelopment and acceptance of standardsagreement on terms, interfaces,reduced effort to communicate andmedia, proceduresapply results of others; ability toconcentrate on mission issues andnot on technology supportpopulation. such contributions can be as simple as an annotation on an existing item, or as complex as afully processed and peerreviewed new item. most profoundly, the evolving network infrastructureenables new concepts for distribution of functions and responsibility in organizations (nrc, 1994).although networks can provide a quick and easy means to distribute data, it must be noted that cdroms have been used to distribute data for several years and have been very successful. cdroms notonly permit users to have a huge local library of data, but they often come with a better set of data accesstools than are normally available. some data sets are large enough that the most costeffective method todeliver them is on media such as exabyte tapes (8 mm).low and declining cost of storageas for most aspects of computer hardware, the cost of storage has declined continuously and rapidlyfor the 30 years of the modern computer age. new storage technology is also increasingly compact andsupports ever greater access speeds (gelsinger et al., 1989). the historical trends are expected tocontinue for up to 20 years. already, laboratory engineering results confirm this projection for at least thenext decade. the most significant implication is that the decisions about sampling or discardingscientific data can generally be deferred, particularly for data sets for which the necessary metadata existand whose quality has been certified. for relatively smaller data sets, the deliberation regarding longterm retention may well cost more than the recurring acts of migration. the cost of storage is small inrelation to overall mission or investigation costs and therefore should not be a decision driver. experipreserving scientific data on our physical universe: a new strategy for archiving the nation's scientific information resourcescopyright national academy of sciences. all rights reserved.the opportunities: the relationship of technological advances to new data use and retention strategies45ence suggests, however, that the funds to meet these costs need to receive special protection in the annualagency budget cycles. the support for the data management aspects of scientific missions has typicallyhad a lower priority than the data collection aspects. the low cost of storage also implies that theincremental cost of supporting a remote safe copy of data and metadata also will be small, except for thevery large data sets. therefore, over the next few decades, data received and stored may be expected to becheaply and quickly migrated to new technologies when storage media reach their nominal limits ofreliability or for convenience of improved access.it is important not to expect a perpetual advantage from this technological discontinuity. the factthat data require significant time periods for their migration must be considered. the cost decay trendwill slow down at some point in the future, causing the overall cost of storage to return to somethingcloser to the linear relationship to volume. we also must be realistic and expect that funds will not alwaysbe available to save and back up every data set. decisions on retention or sampling will have to be made.nevertheless, the already low and continually declining cost of storage allows a priori decisions to bemade in certain circumstances to keep scientific data sets indefinitely. backup or safe storage copies ofdata are becoming more affordable as data migration becomes less expensive with smaller, faster, andcheaper storage devices. reliability also is improving with new softwarebased archive systems(including migration and backup features). however, there is an enhanced need for ongoing technologymonitoring by an appropriate body for media, standards, and migration automation. such monitoringshould be incorporated in any scientific data management and archiving strategy.the rapid change of storage technologies suggests that efforts to protect todayõs scientific data legacymust be accelerated. the obsolescence of media types and recorders/players is occurring within shorterand shorter time periods. this implies that òsalvageó activities will be increasingly difficult for data leftout of migrations to new media. this òjoin or be left behindó byproduct of rapid technological changeintensifies shortterm budget pressures on archives. it demands in response a strong managementcommitment to provide resources and save important data sets.if digital data are to survive, it is of fundamental importance to manage and constrain the costs ofarchive maintenance. the problem is that new data will be coming in, old data will need to be migratedto new media, the building will need to be repaired, and there usually will not be a lot of extra money fornew equipment or added staff. to avoid problems, the data migration process in the system design mustbe almost totally automated. this refinement often has not been achieved, and it can cause unnecessarybudget difficulties. finally, it is essential for agencies to preserve all the hardware and softwarenecessary to access all their data until the data have been successfully migrated or otherwise disposed of.advanced data managementthere are signs that data management technology is beginning to address and, perhaps, to catch upwith the complexities of the very large volumes of scientific data. improvements have occurred indatabase management systems, hierarchical file systems, data representation standards, query optimizers,data distribution techniques, specialized access methods, and data security tools (silberschatz et al.,1991). further, investment in standards and cooperative approaches is accelerating, fueled in part by thedemands of medicine, education, entertainment, journalism, financial services, and other commercialapplications. while competing approaches and inconsistent vocabulary create nearterm confusion, theattention and investment levels bode well for the longerterm capability to go beyond òflat fileórepresentations of data that need to be archived. the new tools and techniques are more descriptive of thedata, their heritage, the processes that have worked upon the data, and the relationships of data to eachother.new data management technology will enable easier representation of more diverse types ofscientific data. because of the rigor that new techniques require (e.g., for selfdocumentation or forprecise definition of access methods), longterm archives will benefit from data structures other than flatpreserving scientific data on our physical universe: a new strategy for archiving the nation's scientific information resourcescopyright national academy of sciences. all rights reserved.46preserving scientific data on our physical universefiles. the new technology also implies that the creation of a richer set of metadata will be easier toimplement and that these data will be of high scientific value for contentbased retrievals. to realize thepotential of this enabled facility with metadata, the scientific community will have to accept and supportefforts to develop and apply new metadata requirements.the changing requirements for information technology professionalsinformation technology professionals with high skill levels can now be found in all parts of theunited states and around the world. but as they bring the information technology industry to higherlevels of maturity, the effect is to reduce the complexity of major tasks in managing information. suchtasks previously required their skilled use of sophisticated assembly language or job control language(jcl) programming. jcl programming refers to the steps in the old days that one used at the systemconsole to get programs to run, attach the right files, print to the right printer, and similar functions.today, much of this work is masked, made automatic, and controlled through icons and other means.these tasks can now be performed by competent scientists or professionals with lower technical skills,rather than by highly trained specialists. because more functions can be completely handled bymachines, management of the data can be greatly automated and operated by less skilled individuals. thedata themselves can be widely distributed without fear of loss, particularly with a backup copy in safestorage.over the next 5 to 10 years, the costs for information technology professionals at individual scientificdata centers and archives can be dramatically reduced. the reasons for the reduction in costs includemore automatic processes for storage management, rudimentary learning capability in systems, servicesperformed by end users based on their preferences, improved systems management, higher componentreliability, improved application of standards, and vendor consistency with standards.although the dominant trend will be for a smaller, less technically skilled staff to manage thephysical aspects of the archive, there will be a pressing demand for fewer, highly skilled people whoblend the skills of physical scientist, computer scientist, and archivist. these people must be able tohandle the intellectual challenges of bridging these disciplines while providing the coaching anddirection to help develop data and operations standards for scientific communities.high reliability of technology componentsmicroprocessors, new storage media technologies, mature software, error correction capabilities,improved packaging, and reduced power consumption have all made significant contributions to thereliability of computer systems and networks. what was recently considered unreliable, requiringconstant attention and expensive repair, is now regarded as reliable and not worthy of effort to repair.although precautions have always been taken to protect against loss of valuable data, many of theseprecautions are now built into the base of mature software or are increasingly familiar parts of facilitiesõoperating procedures.high reliability of technology supports a capacity for high levels of trust and the ability to widelydistribute functions and databases. these distributed systems can achieve the same levels of quality andtrust as centralized archives through the use of the same underlying hardware and software technology,operating procedures, safe storage of copies, and highquality (errorcorrected) telecommunicationconnections. high reliability has enabled new applications such as the world wide web, in whichcontext switching from one machine to the nextñon a worldwide basisñis readily accomplished.increased reliability also has allowed computing technology to be put into the hands of businessmanagers, consumers, and shop clerks. without such reliability, maintenance effort would outweighproductivity benefit. as a result, powerful organizational or operational frameworks can be built, muchas new materials enable new architecture or new machines.preserving scientific data on our physical universe: a new strategy for archiving the nation's scientific information resourcescopyright national academy of sciences. all rights reserved.the opportunities: the relationship of technological advances to new data use and retention strategies47development and acceptance of standardsthe development of effective standards has been pivotal to promoting the widespread use ofelectronic information. communication protocols such as tcp/ip have fueled the growth of the internet.other format standards for documents support their interchange. for example, the standard generalizedmarkup language (sgml) provides a uniform way of formatting textual documents so that they can beread by different document processing tools. the hypertext markup language (html) is a standardused to represent and link documents; it is used to describe pages viewed with internet viewers such asmosaic. hardware and software standards such as the instruction set architectures for microprocessorbased computers, modem protocols, media formats, and query languages also have played critical roles.standards can simplify many of the traditional data management jobs. for example, the time thatwould be used to decipher a tape format is saved and the job of installing a new application is facilitated.having effective standards in place reduces the level of tedious, nonproductive effort and frees up timefor new tasks for the archivist. standards determined now will typically be in effect for long periods oftime, perhaps a decade or more, with some small evolutionary augmentations. this means that a baselineof appropriate standards can be selected for a body of information with some reasonable expectation thatthey will not be quickly replaced. when it appears that the existing standards baseline needs to beupdated, the information can then be migrated to a new one. a deliberate data migration strategy basedon standards tracking is possible.the role of standards certainly is not limited to the general computing community. scientific teamsand discipline groups continuously work to codify best practices, definitions, and algorithms. these arepropagated as community standards. standards developed by the scientific community are often the mostimportant to promote and apply. if properly promulgated, they can enable improved understanding,broader collaboration, and facilitation of the data management and related research.finally, it should be emphasized that standards and guidelines to support longterm archiving mustnot inhibit innovation, or the evolution of information systems and technology. often the best standardsand guidelines are those that are independent of technology.opportunities for new organizational structureswith rapid technological improvements and newly enabled capabilities, it is sometimes easy toforget the importance of longterm commitment by managers to policy and resource requirements. notechnological changes will by themselves replace the basic, unsung efforts of highquality scientific datamanagement. in fact, although technology itself can improve the availability of data, truly accessible anduseful scientific information will be achieved only through such management commitment. thiscommitment must be based on a coherent strategy for lifecycle management of data, including technology acquisition, data and information management practices, and technologyindependent standards toensure that the minimum levels of data content and consistency for research uses are met. further, sucha comprehensive strategy will be successful only with the active and committed involvement of thescientific community itself. the level of effort and change that may be required to achieve thiscommunity involvement cannot be underestimated, and fundamental change to the value system of thecommunity may be required.nevertheless, as discussed above, technological advances allow the creation of new infrastructure,challenging existing organizational assumptions. effective organizational designs based on new allocations of responsibility are enabled. for scientific data management, the technological changes supportorganizations with the following attributes:¥widely distributed responsibility. new telecommunications, data management, and standardstechnology allows for high levels of trust in distributed data management. physical possession of data bypreserving scientific data on our physical universe: a new strategy for archiving the nation's scientific information resourcescopyright national academy of sciences. all rights reserved.48preserving scientific data on our physical universearchivists is no longer essential. the wide availability of information technology professionals and otherskilled data managers (along with the lower technical skill levels actually needed) enhances the ability todistribute the data more broadly and increase user participation. such distribution of data and theirownership (whether actual or implied) by user groups improves the utility of the data and helps createimportant support for longterm retention.¥highvalue peertopeer communication. with access to data and to people on line, a variety ofnew collaborative relationships can develop. information can be broadcast to interested individuals in atimely fashion. data can be provided directly to field researchers to focus new data collection. physicalproximity and formal lines of communication are no longer vital to effective organizational operation.indeed, closed, highly structured organizations often will be uncompetitive or fail to take full advantageof innovation.¥specialized data centers. distribution of resources implies that some specific locations canspecialize and yet still contribute effectively to all. specialized groups or institutions could be created ina scientific discipline or in some aspect of data management, archives, or standards. designation of suchspecialized centers, in addition to those already in existence, is a significant mechanism for achievingeconomies of scale, reducing overall costs while enhancing the effectiveness of certain functions for thebenefit of all.¥explicit longterm (technology) strategies. a longterm technology strategy needs to be developed. the rapidly changing base of technology requires that a deliberate sequence of phases be selected,through which data and data management will migrate. the constant evolution of information technologies demands that an organizational element take on this òtechnology navigationó function.¥measurement as a vital tool. in a fastpaced, and, perhaps, widely distributed effort, metrics areimportant to clearly communicate expectations of performance, register results, and help in detectingweak spots for corrective action. in particular, metrics could be established to determine data set use andto support archiving strategy decisions. metrics also could be developed to help ensure highqualityservice and proper data protection.preserving scientific data on our physical universe: a new strategy for archiving the nation's scientific information resourcescopyright national academy of sciences. all rights reserved.495a new strategy for archivingthe nationõs scientific and technical datathe scientific and technical data held by federal government agencies and by other institutionssupported by federal funds constitute an extremely valuable national resource. unfortunately, in manycases this resource can be exploited only with great difficulty because key elements of the infrastructurefor broad and easy access to it are incomplete or missing.currently, the most important development within the federal government for improving the management and longterm retention of scientific and technical data is the national information infrastructure (nii) initiative. the nii focuses on the application of public, private, and academic resources todefine, implement, and maintain an evolving network of knowledge resources (iitf, 1993). thisinfrastructure will be the foundation for informationcentered enterprises of the next century (nrc,1994). the scientific community, whose lifeblood is widely available data and information, mustbecome fully engaged in this national effort. a coherent strategy needs to be defined and implemented,to combine new technological capability with a new way of doing business throughout all phases of thescientific information life cycle (observation, measurement, analysis, interpretation, application, dissemination, and education).an effective information infrastructure must build on enabling technologies to create an integratedand adaptive system that is easily accessible to all potential users. each user community will have its ownview of what the nii means to its enterprise and how the nii can best serve its users because the nii willbe made up of many separate òenterprise information infrastructures.ó the existing scientific andtechnical data centers and archives already constitute a separate enterprise information infrastructure,which must become fully integrated into the nii.in the discussion that follows, the committee lays out a threepart strategy for the longterm retentionof scientific and technical data. the elements of this strategy are based on the technological advancesoutlined in chapter 4 and on the issues raised in chapter 2, which provide the context and the need foraction.the strategy begins with a set of fundamental principles for the longterm retention of scientific andtechnical data. the second major element outlines the committeeõs proposal to form a national scientificinformation resource federation, which would provide a coordination mechanism for endtoendmanagement of networked scientific and technical data facilities. the final sections highlight somespecific recommendations for nara and noaa in their longterm retention of scientific and technicaldata.preserving scientific data on our physical universe: a new strategy for archiving the nation's scientific information resourcescopyright national academy of sciences. all rights reserved.50preserving scientific data on our physical universe fundamental principles for longterm data retentionin order to respond adequately to the imperatives for preserving data about the physical universe andeventually to create an integrated, adaptive, and accessible infrastructure, the federal government shouldhelp establish effective and affordable processes for providing ready access to the vast national resourceof scientific and technical data and related information. the process must support the needs of dataoriginators, users, and custodians across all phases of the data life cycle, from origin to use by futuregenerations. the committee believes that the following principles should guide the effort of the government agencies in the longterm retention of scientific and technical data:¥data are the lifeblood of science and the key to understanding this and other worlds. as such,data acquired in federal or federally funded endeavors, which meet established retention criteria, are acritical national resource and must be protected, preserved, and made accessible to all people for alltime. the original collection and analysis of scientific and technical data traditionally have been usedprimarily to support the scholarly publication of scientific interpretation by individual investigators. theavailability of complete and consistent data sets for broader uses, both within and outside the scientificcommunity, would significantly increase the return on the investment made in obtaining those data andprovide insights not attainable if the original data were lost or unusable.¥the value of scientific data lies in their use. meaningful access to data, therefore, merits as muchattention as acquisition and preservation. technology can make data available through fast computers,largebandwidth networks, massive storage capabilities, and portable media. however, if the paths todata are obscure, or there is no way for a user to determine what is significant and relevant, then the databecome inaccessible and are effectively lost.¥adequate explanatory documentation, or metadata, can eliminate one of todayõs greatest barriers to use of scientific data. the problem of inadequate metadata is amplified when users are removedfrom the point of origin by being in a different discipline, by having a different level of expertise, or bytime. addressing this problem comprehensively will make data useful in the broadest possible context.¥a successful archive is affordable, durable, extensible, evolvable, and readily accessible. theseterms may appear to be vague targets, but they imply basic goals. the costs of developing, operating, andusing an archive must not be excessive. the archive must endure the ravages of longterm use, and itmust be able to extend broadly the services it offers and the records it manages. it must evolve to supportthe assimilation of new technology, policies, procedures, and uses. finally, an archive is not effective ifa broad population of users cannot use it. the archiving system thus should provide multiple levels ofaccess to any subset of its holdings, although holdings not accessed often may not require a sophisticatedaccess mechanism.¥the only effective and affordable archiving strategy is based on distributed archives managed bythose most knowledgeable about the data. archive centers generally should be at the agencies orinstitutions that collect the data, and they should be responsible for archiving and providing access to thedata as long as the agencyõs or institutionõs mission and scientific competence continue to encompass thesubject field. physical transfers of the data should be avoided if possible, so agencies and institutions willneed to allocate adequate resources to the entire life cycle of their data holdings.¥planning activities at the point of data origin must include longterm data management andarchiving. this principle is recognized in the office of management and budget circular a130 on theòmanagement of federal information resourcesó (omb, 1994). the scientific information managementspectrum spans data collected from a sensor to the scholarly publications that report scientistsõ interpretations of the data. scientists, information technology professionals, data managers, librarians, andarchivists must unify their expertise in the establishment of a coherent strategy for endtoend data andinformation management. although these communities traditionally have not worked closely together,preserving scientific data on our physical universe: a new strategy for archiving the nation's scientific information resourcescopyright national academy of sciences. all rights reserved.a new strategy for archiving the nationõs scientific and technical data51their combined knowledge and effort are now required. the benefit of incorporating planning at the pointof origin is that it is cheaper and more effective to plan for retention than to reconstruct data sets later. the proposed national scientific information resource federationthe committee believes that the federal government should create a national scientific informationresource federationñan evolutionary and collaborative network of scientific and technical data centersand archivesñto take on the challenge of providing effective access to and preservation of importantscientific and technical data and related information. such an initiative would begin to exploit more fullyour nationõs significant investment in the physical (and other) sciences and the data acquired with thatinvestment. in the discussion that follows, the committee reviews the basic elements of a federatedmanagement structure, describes some notable examples of existing federal government organizationsfor largescale distributed data management, and outlines the most important aspects of the proposednational scientific information resource federation.elements of a federated management structureseveral critical concepts must govern any federated management structure for it to function properly.these include the notions of subsidiarity, pluralism, standardization, the separation of powers, and strongleadership at all levels (handy, 1992).subsidiarity means that power is assumed to lie with the subordinate units of an organization andcan be relinquished, but not taken away. the subordinate units typically are best qualified to makeoperational decisions that directly affect them and that they will be implementing. the central management is allowed only those powers needed to ensure that the subordinates do not damage the organization. for example, the constitution of the united states reserves only specified powers for the federalgovernment, with any unstated powers belonging to the states. applied to the situation at hand, it is clearthat the strengths of the current system for managing scientific and technical data and information in theunited states are distributed among a number of diverse data centers and archives, both within andoutside the government. a successful federation of these existing institutions would recognize that theyare the locations of expertise on their respective data holdings. thus the central organization should besmall and should not micromanage the daytoday operations of the subsidiary organizations.pluralism may be defined as interdependence of the members. in a federation, the individualsubsidiary organizations recognize the advantages of belonging to the federation, because of products orservices that can be obtained from other elements in the federation. as noted in the previous chapter, theexistence of many specialized data centers and archives, as well as the possibility of creating new ones ina networked environment, can offer significant economies of scale and improved sharing of ideas andexpertise. what is good for the subsidiary element also should be good for the whole. pluralism, coupledwith subsidiarity, guarantees a measure of democracy in the federation.interdependence, in turn, requires standardization of languages, communications, basic rules ofconduct, and units of measurement. these elements may be summarized as technical and proceduralstandardization. this too was discussed in chapter 4, regarding the development of standards insoftware, hardware, and data management. standards that are developed by consensus of the subsidiaryelements (e.g., the participating data centers, archives, and researchers) are widely recognized asessential to the successful management of data.a separation of powers (responsibilities), with a system of checks and balances, is necessary toensure that the central authority does not take on unnecessary power. this principle must be incorporatedinto the federationõs organizational structure.finally, a federation requires strong leadership that is effective, yet not overbearing. the centralcoordinating element or executive office must act as the standard bearer, promoting the federationõspreserving scientific data on our physical universe: a new strategy for archiving the nation's scientific information resourcescopyright national academy of sciences. all rights reserved.52preserving scientific data on our physical universeestablished goals and objectives while reminding the subsidiary organizations of the importance ofcarrying out their responsibilities.examples of distributed data management organizationssuccessful examples of a federated management structure are numerous in the private sector (handy,1992). more specifically, however, there already are two largescale, federal government, distributeddata management groups that embody many, though not all, of the federated management attributesoutlined above. these are the interagency working group on data management for global change andthe federal geographic data committee.interagency working group on data management for global changein 1990, congress formally established the u.s. global change research program (gcrp), òaimedat understanding and responding to global change, including the cumulative effects of human activitiesand natural processes on the environment, [and] to promote discussions toward international protocols inglobal change research . . .ó (cenr, 1994). the activities of the gcrp are coordinated by thecommittee on environment and natural resources (cenr), under the presidentõs national science andtechnology council.the timely availability of a broad spectrum of scientific data and information, from both governmental and nongovernmental sources, is fundamental to meeting the goals of this program. a global changedata and information system (gcdis) is being created to facilitate access to and use of the data andinformation necessary to support global change research. the federal organizations involved in thegcdis planning include the departments of agriculture, commerce, defense, energy, interior, andstate, as well as the environmental protection agency, the national aeronautics and space administration, and the national science foundation.according to the u.s. global change data and information system draft implementation plan(cenr, in press), the gcdis is building on the resources and responsibilities of each participatingagency, linking the data and information services of the agencies to each other and to the users. thesystem thus is composed largely of the separately funded components contributed by the participatingagencies. it is supplemented by a minimal amount of crosscutting new infrastructure through the use ofstandards, common management approaches, technology sharing, and data policy coordination. neithera lead agency nor a separately funded budget for the gcdis is planned; rather, implementation of thesystem is being coordinated through the interagency working group on data management for globalchange (iwgdmgc). decision making, therefore, is done through a consensus process based on thecommon interests of all participants.plans for the gcdis recognize that the global change data must be available for a very long time,regardless of the changing interests of the researcher, group, or agency that originally collected andanalyzed the observations. although each agency participating in the gcdis is expected to manage,store, and maintain the data sets under its purview, the plan does allow an agency to designate anothergcdis agency to archive some of its data. the participating agencies are expected to adhere togovernment standards for media, storage, and handling as prescribed by nara and the national instituteof standards and technology. the agency archives associated with the gcdis access system will bestaffed by professionals who understand the data and their sources. the iwgdmgc expects to developguidelines for preparing data sets and associated documentation for longterm retention at the participating agencies. ideally, the gcdis archives also will be associated with research groups, both within andoutside government, who, as principal users of those data, will verify quality and documentation of thedata.preserving scientific data on our physical universe: a new strategy for archiving the nation's scientific information resourcescopyright national academy of sciences. all rights reserved.a new strategy for archiving the nationõs scientific and technical data53the gcdis plan gives each agency responsibility for its own datapurging policies, althoughinteragency coordination procedures will be developed to prevent the loss of important data sets. beforeany data sets are purged, however, an agency will be required to notify the iwgdmgc of its plans atleast one year in advance, and to allow other gcdis agencies to indicate their requirements for thosedata, or to agree to assume responsibility for the archiving of those data. in the event that no agreementcan be reached on the disposition of a data set identified for purging, existing nara procedures willapply (cenr, in press).federal geographic data committeethe other major federal data coordination entity important to the longterm management of observational data (including some data from the biological and social sciences) is the federal geographic datacommittee (fgdc). the office of management and budget (omb) established the fgdc in 1990 todevelop a national spatial data infrastructure (nsdi) to work toward the coordinated development, use,sharing, and dissemination of geographic data (omb, 1990). participating government organizationsinclude the departments of agriculture, commerce, defense, energy, housing and urban development,interior, state, and transportation, as well as the environmental protection agency, federal emergencymanagement agency, library of congress, national aeronautics and space administration, nationalarchives and records administration, and tennessee valley authority. in fulfilling its mandate, thefgdc carries out the following activities, among others:¥promotes the development, maintenance, and management of distributed database systems thatare national in scope for geographic data;¥encourages the development and implementation of standards, exchange formats, specifications,procedures, and guidelines;¥promotes technology development, transfer, and exchange; and¥promotes interaction with other existing federal coordinating mechanisms that have interest in thegeneration, collection, use, and transfer of spatial data (fgdc, 1994).the fgdc has received authority and some limited funding to pursue these objectives. specifically,executive order 12906 on òcoordinating geographic data acquisition and access: the national spatialdata infrastructure,ó assigns to the fgdc the responsibility to coordinate the federal governmentõsdevelopment of the nsdi. that executive order also instructs the fgdc to involve state and localgovernments in its nsdi activities, and to use the expertise of academia, professional societies, theprivate sector, and others as necessary to assist the fgdc.the fgdc has established a matrix of subcommittees and working groups according to disciplinerelated data categories and interests. the working group issues include a framework for data, aclearinghouse for data, standards, technology, and data archiving. the fgdc plans for data archivingare still being developed, however.creation of the national scientific information resource federationthe two examples cited above indicate that a federated management structure for highly distributedscientific data can be created. in fact, between these two groups, the lifecycle management of many ofthe data that are the topic of this report is beginning to be systematically approached. nevertheless, asdiscussed in this report and in the volume of working papers (nrc, 1995), many important gaps andinadequacies remain in the management and retention of our nationõs scientific data and related information. the committee believes that these deficiencies can best be addressed by a comprehensive federatedsystemña national scientific information resource (nsir) federationñthat builds on the successes ofpreserving scientific data on our physical universe: a new strategy for archiving the nation's scientific information resourcescopyright national academy of sciences. all rights reserved.54preserving scientific data on our physical universethe existing groups and helps coordinate them with other data management entities that still needimprovement.there are many reasons why it is now propitious to establish a system of federated data management,with an emphasis on longterm retention. from a policy perspective, it would be consistent with the goalof the national information infrastructure to distribute information resources broadly throughout oursociety, with the federal government acting as facilitator for such activities. the technology is availableto make a fully networked, but highly distributed, system of data centers and archives both feasible anddesirable. such a system would be efficient in providing access to scientific data and information to alarge number of potential users and would maximize the governmentõs return on the significant investment that initially went into acquiring those data. from an organizational standpoint, a federatedmanagement structure would allow the disparate elements to continue to specialize in what they each dobest and to fulfill their individual organizational mandates, while providing some efficiencies of scaleand political leverage in addressing the most pressing issues. moreover, this type of approach isespecially timely and important in an era of federal government budget reductions. the committeetherefore envisions a broadly networked organization, which would be implemented through the collaboration of the federal governmentõs scientific and technical agencies as well as commercial and noncommercial organizations outside the government, and integrated into the emerging national informationinfrastructure.most of the elements of the nsir federation are already in place. these include the data centers andfield archives run by several of the federal agencies that are among the primary generators and collectorsof the nationõs scientific data and information. in addition to holding data, these centers and archiveshave highly skilled staff with the requisite expertise. the organizations are widely distributed, bothgeographically and by discipline.the existing data centers and field archives, however, do not approach the federated organizationalmodel for several reasons. there is no unifying organization among the various elements, there is widedisparity in the quality and depth of service provided, and few of them have a charter to preserve dataòpermanently.ó although nara has the statutory charter to preserve federal records in perpetuity, itscurrent and projected holdings of electronic scientific records are very small. while the committee doesnot believe that naraõs archives of scientific data should increase substantially, it found little evidenceof activity within the scientific and technical agencies that would indicate that their ability to provide forlongterm retention and access to their data would improve without some restructuring.a fundamental precept is that those most familiar with scientific datañthe scientists themselvesñare in the best position to oversee the management of those data (nrc, 1982). in light of the volume anddiversity of scientific data, a distributed approach that maintains the data closest to the primary usercommunity is the most effective method for managing them. as mentioned above, several agencies haveadopted an approach of caring for their data in systems of field archives or discipline data centers.although these agencies have devoted significant attention to the preservation of data, their concern islimited to providing immediate service to primary users of the data for their originally intended purpose.little thought has been given to the perpetual archiving of the data within most agencies, with the notableexception of nara and noaa, which already have a statutory mandate that allows them to preservedata collected by the federal government. because it is not possible to be sure that any data center willexist in perpetuity, some mechanism must be in place to ensure that the data will be retained by anappropriate organizationñwithin or outside the governmentñin the event that the continued existenceof a data center is jeopardized.if a lead agency can be determined for a subject matter, then it should take responsibility forcoordination of scientific data on that subject, no matter which agency has physical ownership or custodyof those data. the committee recognizes, however, that some data sets are largely of interest at theboundaries of disciplines or agency charters and that consequently these may be more difficult to manageor document properly. large data sets that are of an interdisciplinary nature cause special problems inpreserving scientific data on our physical universe: a new strategy for archiving the nation's scientific information resourcescopyright national academy of sciences. all rights reserved.a new strategy for archiving the nationõs scientific and technical data55this regard. for these complex situations, no simple rule will take the place of negotiations among theinvolved agencies to make the necessary arrangements for longterm archiving. indeed, every agencyshould assume the obligation to keep its holdings of scientific data in usable form, even if the data are notin active use, until agreeing on disposition of those data with nara or another agency.in addition to the agencyadministered data centers, there are educational or private concerns thathold and administer data important to one or more agencies, such as the archived data from the noaageostationary operational environmental satellites at the university of wisconsin or the seismic dataheld by the incorporated research institutions for seismology. while some of these nonfederal archivesare firmly associated with one or more federal agencies through contractual and funding relationships, inother cases a onetoone association is less clear. it follows that a welldefined chain of responsibilitymust be established for all data that are to be preserved. this decision should be made by the individualsand institutions most closely associated with and interested in those data, and it should be made with dueconsideration for cost efficiency, appropriate expertise, scientific interest, and convenience, among otherfactors. establishing a clear connection between a field archive and an agency should in no way limit thecommunity of users served by the archive, but should ensure an orderly and secure path of responsibilityfor the data.the structure of the nationõs scientific and technical organizations continues to change. in someinstances, institutions or even agencies will merge, while in other cases, organizations may disappear.when such changes occur, it is likely that the scientific interests formerly represented by those organizations will be subsumed by existing or new agencies or organizations. the general topology of the nsirfederation, however, would not change.the committee does not anticipate that the creation and implementation of the federation will requiremuch additional funding, if any, because it will consist primarily of improving linkages and coordinationamong existing data centers, archives, and related organizations within a highly decentralized management structure. moreover, any costs incurred in this process should be more than offset by theimprovements in efficiency and access to the data and related information resources.recommendations for the creation of the nsir federationthe committee thus recommends that the federal government take the following steps for adequatelypreserving and providing access to data about our physical universe:adopt the national scientific information resource (nsir) federation concept as an integralpart of the national information infrastructure (nii). this concept must encompass not only anelectronic network, but also individuals, organizations, communities, data resources, procedures,guidelines, and associated activities of data generation, management, custodianship, and use. thensir federation should provide the foundation for defining a coherent approach to management of thelife cycle of scientific data, with the goal of providing broad and effective access to all potential users ascost effectively as possible. the federation should be developed and implemented through consensus ofcollaborating organizations with diverse and autonomous missions. the gcdis, in particular, is anexample of a prototype nsir, focused on data for a specific set of interdisciplinary science problems.the nsir federation would build on such efforts, providing for better coordination and interactionamong them, and would help organize fledgling efforts to preserve and provide access to data in otherdisciplines.the administration should take the steps necessary to fully define and create the nsirfederation. there are at least two potential focal points within the administration for planning such anactivity. these are the interagency information infrastructure task force for the nii and the nationalscience and technology council. the nsir federation could be created in a manner similar to thecreation of the federal geographic data committee and its national spatial data infrastructure (e.g.,preserving scientific data on our physical universe: a new strategy for archiving the nation's scientific information resourcescopyright national academy of sciences. all rights reserved.56preserving scientific data on our physical universethrough an office of management and budget circular and executive order), or of the interagencyworking group on data management for global change and its global change data and informationsystem (e.g., through legislation in cooperation with the administration). a convocation of representatives from the scientific, data and information management, and archiving communities would be a goodway to define and inaugurate this initiative, focusing on the most significant issues and problemsidentified at the end of chapter 2.following the formal authorization by the federal government for creating the nsir federation, the principal parties, including nara and noaa, should conclude agreements for theimplementation of a distributed archive system. the system should involve all relevant institutions, including nongovernmental entities that are funded by the federal government or thatmaintain data that were acquired with federal funds. as a general principle, data collected by anagency should remain with that agency indefinitely. the committee recognizes that this recommendation may require significant operational changes for agencies other than noaa, and even some changeswith respect to noaaõs data activities. in addition, nara should consider concluding interagencyagreements to give formal recognition of this process as appropriate. furthermore, the associatedagencies in the nsir federation must work together, under the lead of a small, coordinating executiveoffice with the expertise to establish data management guidelines and minimum criteria for adequatemetadata that could be applied across the entire federation. the executive office could be either a highlevel interagency coordinating committee, similar to the fgdc, or a new office at an appropriate federalagency, such as the national science foundation, which has a broad scientific and technical as well ascommunication mandate. in any case, the executive office should resist the typical tendency towardbureaucratic accretion of power, personnel, and resources, and the tendency to consolidate and centralizedata holdings. a management council consisting of representatives of the member organizations shouldbe created to help ensure that the central executive function remains fully responsive to all members ofthe federation.data access and preservation services should be implemented on the most costeffective basispossible for the federation. for example, one institution may provide a service to one or more otherinstitutions in order to exploit potential economies of scale and focal points of expertise (e.g., thespecialized data centers suggested in chapter 4). this measure might increase the cost to the providinginstitution, but would decrease the overall cost to the federation, the government, and the taxpayer. anexample of this is the method by which backup copies of data might be kept. nara may have at anygiven time the most costeffective òvaultó in which to keep physically separate backup copies of data forall agencies, and, hence, the federal government would save money by increasing naraõs budget toprovide this service for the other agencies. on the other hand, if cost tradeoff studies were to find that asingle large òvaultó is not as costeffective as distributed facilities, then each agency would be responsible for its own backup. in all nsir federation activities, emphasis should be placed on control of costs,with the most successful methods used by individual members identified and shared with all othermembers.the institutions belonging to the nsir federation should develop a process for collaboratingeffectively on specific initiatives. this process should provide a mechanism to define and prioritize datamanagement and preservation initiatives, to establish the required agreements between collaboratingorganizations, and to secure funding for each initiative. each participating organization would contributeto the federation according to its particular strengths and in a manner consistent with the foundingcharter. in addition, an independent advisory body consisting of experts from user groups should beformed in support of each initiative.the nsir federation should develop a national resource of information technology that isconsistent with its chartered objectives and that can be effectively distributed to institutions thatmust manage data. these technologies would include complete products, designs, guidelines, stanpreserving scientific data on our physical universe: a new strategy for archiving the nation's scientific information resourcescopyright national academy of sciences. all rights reserved.a new strategy for archiving the nationõs scientific and technical data57dards, and methodologies. a related longterm technology strategy, or òtechnology navigationó function, should be developed, as suggested in chapter 4.the nsir federation should institute an independently managed process for awarding nsircertification to member scientific institutions and their data and information systems on the basisof welldefined criteria and standards. the certification process should be managed by a nongovernmental, notforprofit organization, which would receive technical guidance from the participatingfederal agencies. the certification needs to have credibility in the community so that nonmemberinstitutions will aspire to attain certification and have it tagged to their products. the certification alsoshould be something that commercial valueadded providers will seek to increase the credibility of theirproducts.it also is important for the committee to state what the nsir federation should not be. it should notbecome an expensive bureaucratic entity. the executive office must not impose any standards orinformation technologies from above that have not been validated through a consensus process of themember organizations. finally, the executive office must not attempt to micromanage the operations ofthe participants, nor should it have any direct control over their budgets and funding allocations.recommendations specifically for narain order to improve its responsibilities in the longterm retention of scientific and technicaldata, the committee recommends that nara strengthen its liaison with each federal agency thatproduces such data to ensure that appropriate attention is devoted to longterm data retention in adistributed storage environment.as shown earlier in this report, nara cannot today, nor will it likely ever be able to, act as thecustodian of most physical science data. the data volume is too great in relation to the fundingappropriated to nara, the nara staff do not have the necessary specialized scientific knowledge, theinteragency linkages are not in place, and a huge infrastructure similar to that which already exists atother agencies would need to be duplicated at nara. the agencies closest to the data sets and bestequipped to deal with them are themselves already struggling with these issues. however, nara doeshave great expertise in issues involving the longterm storage of data and the packaging requirements fordata to be of value to future users.the committee therefore believes that naraõs role should be primarily advisory or consultative, tohelp ensure that the agencies that are the actual custodians of data at the working level follow all therelevant federal laws and guidelines in taking care of the data. the committee suggests that scientificdata and related information should go to naraõs physical possession only as a last resort, when theagency that collected the data can no longer provide access for the user community. as has already beennoted, scientific data are best maintained by the agency that originally acquired those data as long as thereis any regular active use. the holding agencies should collect, analyze, store, and make available themaximum feasible amount of relevant physical science data, consistent with the principles and goals setforth for the nsir federation and with the retention criteria and appraisal guidelines discussed above.currently, agencies inform nara of their intentions for their federal records, including scientificdata, through various schedules. all agencies are required to schedule records when they reach 30 yearsof age, although they are encouraged to do so earlier. the national climatic data center even providesschedules for data that it plans to hold indefinitely, noting that intention. for most types of records, thepressure to schedule provides the useful function of preventing an agency from simply warehousingcontinually increasing volumes of unused records without examination. for data that an agency does notwish to destroy, but that are not frequently accessed, nara makes available storage space withouttaking ownership. if nara did not provide some worthiness test for records before agreeing to providepreserving scientific data on our physical universe: a new strategy for archiving the nation's scientific information resourcescopyright national academy of sciences. all rights reserved.58preserving scientific data on our physical universestorage for another agency, the federal records centers could become inundated with records of littlevalue or potential for future use.as discussed in this report, we are heading increasingly toward a system of distributed archives forelectronic records. data sets are distributed among various physical locations, and the expertise tointerpret these data sets is likewise already distributed and becoming more so. the rapid increase incomputer networks within the united states and in the rest of the world is beginning to significantlyaffect the way people access information. there is a lessening need for data users and providers tophysically possess the data they need or distribute, and users are increasingly unaware of the sourcelocation(s) of the data they are accessing. nara therefore should continue to study arrangementsregarding the physical custody of electronic records, the relationship between nara and other agencies,and how these will and should be affected by the expansion of electronic networks.during the course of this study, the committee found that with the exception of some staff membersat government data centers, many government scientists and most nongovernment scientists are notaware of the requirements of the records disposal act (44 u.s.c. 3301 et seq.). even some of thoseentrusted with large quantities of valuable data were largely unaware of nara and its related responsibilities until contacted by the committee, or by its panels. this may be partially because scientists, eventhose within the federal government, sometimes do not respond to the bureaucratic requirements of theirown institutions. the committee is encouraged that nara is working to address this problem.nevertheless, many panel visitors and members observed that the nara brochures have an authoritarianand legalistic tone and are not conducive to establishing productive partnerships with nara. naraõsfuture effectiveness in overseeing and advising on the archiving of scientific and technical data requiresthat it improve its relations with other agencies and institutions.as a corollary, none of the committeeõs suggestions should be construed to imply that nara shouldissue additional proclamations or regulations. the goal should be to present more carrots than sticks. forexample, nara should consider providing rewards and recognition to researchers, managers, andfunders for developing and implementing successful data retention plans, with appropriate metadata.with better communications and greater sensitivity to the needs of the scientific community, nara canplay the role of a òservice provideró and òappraisal consultant.ó for instance, nara is already workingwith the dod legacy resource management program to identify and preserve cultural resources underdod jurisdiction. nara and this dod program together have sponsored a conference to assist militarycontractors in preserving their documentary heritage. the committee suggests that nara pursue othersuch collaborations in the same spirit of partnership.as a matter of formal responsibility and training, nara staff are more concerned with longtermarchiving issues than most staff at other agencies. nara therefore can serve an essential role inreminding agencies of the longterm value of data and should regularly provide advice to agencies thatkeep scientific data on hand for extended periods of time. nara also should conduct continuousresearch on retention and appraisal issues to remain wellinformed. the committee recommends thatnara form standing advisory committees with managers of scientific data, historians, andscientific researchers to address the retention and appraisal of scientific and technical datacollections, and related issues.unfortunately, nara has almost no scientific expertise within its ranks (except related to physicalrecords preservation). despite the large amounts of scientific information within some federal records,nara officials have indicated that they do not believe that they could keep a scientist on the staffinterested in the work and do not plan to hire any permanent scientific personnel. nevertheless, narawill continue to be faced with difficult issues involving the archiving of scientific data. in the interim, thecommittee suggests that nara should arrange for temporary staff assignments from the active scientificranks of the federal government on a frequent asneeded basis. given the great challenges that narawill face from scientific data and the proven ability of other agencies to hold scientifically trainedpreserving scientific data on our physical universe: a new strategy for archiving the nation's scientific information resourcescopyright national academy of sciences. all rights reserved.a new strategy for archiving the nationõs scientific and technical data59personnel in data management positions, nara should rethink its position and consider creating a cadreof permanent staff with scientific expertise.nara also might consider setting up an inhouse database to track federal holdings, especially toanticipate problems with data sets housed in other agencies that may eventually need nara protectionor other help from nara. to do this effectively would require establishing a set of contacts in otheragencies with people who understand the databases in the agency collections.this brings us to the need for a more general locator function, or òdirectory of directories,ó for thensir federationõs network of networks. archives must not be viewed or managed as data cemeteries,with only rare and dwindling visits after the deposition of data. the provision of broad access to datamust be part of archive design and construction, and thus some sort of broad locator is much needed. thecommittee is encouraged by the recent interagency efforts, organized by the office of management andbudget, to develop a government information locator service. nevertheless, there is a need for anaramaintained directory of archived data within its own system. this should include archivedrecords maintained by other government agencies and federally funded institutions that are recognized aspart of a distributed archive system overseen broadly by nara. the committee recommends thatnara collaborate with other agencies that maintain longterm custody of data to develop aneffective access mechanism to these distributed archives. the initial step should focus on locatorsystems and evolve toward a transparent access system.finally, with regard to its requirements for accession of data, nara should work with thescientific community and potential sources of scientific data to develop adaptable performancecriteria for data formats and media, rather than mandating narrow and inflexible productstandards. the goal would be to meet naraõs basic need to ensure longterm usability while alsoenabling accession of data, such as images and structures, that cannot be accommodated by naraõscurrent restrictive fileformat and media standards.recommendations specifically for noaaas the largest holder of earth sciences data in the united states, noaa has a vast amount ofscientific data stored at many facilities across the country. the primary storage sites are the nationaldata centers, which include the national climatic data center (ncdc), the national oceanographicdata center (nodc), and the national geophysical data center (ngdc). each of these data centersnow has its own online information service. the data centers are accessible through common nodes, forexample through noaaõs web server or nasaõs master directory server. thus a user who understandsthe structure of noaaõs data holdings can navigate through the different data centers, look for data ofinterest in each centerõs holdings, and retrieve the data over the internet. however, it is not possible tosearch noaaõs data holdings with the same precision and accuracy with which one can search forbibliographic data, through, for example, the current contents or inspec databases. the diversity andvolume of data that the national data centers hold and regularly receive make it difficult to produce anoverall directory for all of noaaõs data holdings. in particular, ncdc receives daily all of the weatherinformation for the united states. without such a general directory it is difficult for users to query acrossnoaa archives to locate and integrate diverse data. moreover, once the user finds data, the variety ofstorage formats and data types makes access cumbersome. thus, the committee encourages noaa to beambitious. development of a new comprehensive directory covering all noaaõs holdings of geosciencedata would set the standard for other agencies and would make the data much more accessible to thepublic.this directory may incorporate capabilities of the many different online directory services currentlyin use at the national data centers, but the emphasis should be on connectivity, data access, andinformation. for this reason, noaa should concentrate first on the more recent digital data that can mosteasily be incorporated into such a directory system. efforts to get older analog data digitized shouldpreserving scientific data on our physical universe: a new strategy for archiving the nation's scientific information resourcescopyright national academy of sciences. all rights reserved.60preserving scientific data on our physical universecontinue, although some data may have to remain in their original format. an important facet of thisdirectory is to list, along with the directory entry, how to locate and access the data. once they havelocated the data of interest, most users want mainly to retrieve the data in a form that they can use forfurther analysis.thus, the directory should specify the actual location of the data, as well as the methods by which thedata can be acquired. under the present noaa system, acquisition involves a formal ordering procedureand the transfer of funds, at least for any data that must be transferred via tape or hard copy. experimentalnoaa systems (noaaõs satellite active archive) make it possible to order limited satellite imageryover the network at no cost. for those orders requiring the transfer of funds, the directory service shouldbe able to estimate the cost of the data order so that the user can factor cost into the decision to order.this interconnected noaa directory service also would assist the noaa data centers in theirmanagement of data. by having access to tools and techniques developed at other noaa data centersand elsewhere in the data storage community, the noaa data centers would be better able to stay abreastof new developments and to incorporate them into their data access systems. similarities among variousearth science data and the emerging need for interdisciplinary research make it necessary to implementsuch an overall directory for managing noaa data, for both data location and access. as noted earlier,noaa already has started to develop data directories, online data systems, and data access.noaa and nasa have made progress in data rescue and in deriving better products from old data.since 1990, ncdc has copied thousands of tapes of satellite data that were at the end of their useful shelflife. the noaa/nasa pathfinder program was established to make the satellite data more generallyavailable to researchers and to calculate new products; it has been an effective program. although thecommittee supports activities to preserve old data, rescued data (including data moved to better mediaand analog data that have been digitized) are of little value if they cannot be accessed or retrieved. thecommittee advocates more emphasis on improving access to data for interested users.most federal agencies are now aware that storage and retrieval of data are important. problems arisebecause each agency, and sometimes even different parts of the same agency, sets up data centers andfacilities, and each of these establishes its own type of system. in addition, because the technology forstoring data changes frequently, it is difficult if not impossible to decide just what hardware and softwaresystem should be used. this uniqueness of systems often hinders system portability and the exchange ofdata among systems.there are some approaches and procedures that are designed to be technologyindependent andtherefore can be used to avoid some of these problems. moreover, the technological and portabilityrequirements for archiving, storage, and transmission are different, so a òuniversaló format will not work.an archival format must be utterly portable and selfdescribing, on the assumption that, apart from thetranscription device, neither the software nor the hardware that wrote the data will be available when thedata are read. a storage format should be optimized for retrieving any addressable subset of a dataset.a secondary, but important, consideration is the ease with which the storage format may be cast into atransmission format. a transmission format should be optimized for ease of conversion to otherformats, accommodation of both data and metadata in a single data stream, portability, and extensibility(i.e., accommodating data and metadata types and structures not yet invented). because both noaa andnara have a longterm archival problem, the committee suggests that they work together to locate andtest hardware and software units that can be used for this technologyindependent approach. by locatingthe most simple common technologies, it should be possible to set up systems that are sufficientlycapable, but yet are able to interact with each other. once a few of these òstandardsó are set up andoperating, it is likely that other users will want to run this suite of software. ideally, this type of projectwould be best carried out under the auspices of the nsir federation.preserving scientific data on our physical universe: a new strategy for archiving the nation's scientific information resourcescopyright national academy of sciences. all rights reserved.a new strategy for archiving the nationõs scientific and technical data61considering the foregoing discussion, the committee makes the following recommendations:noaa should place a higher priority on documenting and establishing directories of its dataholdings.furthermore, noaa, with the active cooperation of nara, should lead efforts to better definetechnologyindependent standards for archiving, storing, and transmitting the data within itspurview.finally, noaa, as well as every other federal science agency, should ensure that all its data areshared and readily available; it fulfills its responsibility for quality control, metadata structures,documentation, and creation of data products; it participates in electronic networks that enableaccess, sharing, and transfer of data; and it expressly incorporates the longterm view in planningand carrying out its data management responsibilities.the creation of the committeeõs proposed nsir federation would help provide a collaborativemechanism and more sustained peer pressure to meet these objectives, and thus enhance the value ofscientific and technical data and information resources to the nation.preserving scientific data on our physical universe: a new strategy for archiving the nation's scientific information resourcescopyright national academy of sciences. all rights reserved.62preserving scientific data on our physical universe62referencesamerican chemical society (acs). 1993. reporting experimental data, h.j. white (ed.), washington, d.c.boorstin, d.j. 1992. the creators, random house, new york.committee on environment and natural resources (cenr). 1994.  our changing planet: the fy 1995 u.s.global change research program, national science and technology council, washington, d.c.committee on environment and natural resources (cenr). in press. the u.s. global change data andinformation system draft implementation plan, national science and technology council, washington, d.c.federal geographic data committee (fgdc). 1994. october 1994 fact sheet, federal geographic datacommittee, washington, d.c.gelsinger, p.p., p.a. gargini, g.h. parker, and a.y.c. yu. 1989. microprocessors circa 2000, ieee spectrum,october: 4347.general accounting office (gao). 1990a.  environmental datañmajor effort is needed to improve noaaõsdata management and archiving, washington, d.c.general accounting office (gao). 1990b. space operationsñnasa is not archiving all potentially valuabledata, washington, d.c.haas, j.k., h.w. samuels, and b.t. simmons. 1985. appraising the records of modern science and technology:a guide, massachusetts institute of technology, cambridge, mass.handy, c. 1992. balancing corporate power: a new federalist paper, harvard business review 70(6): 5972.information infrastructure task force (iitf). 1993.  the national information infrastructure: agenda for action,washington, d.c.jacobs, w. 1947. wartime developments in applied climatology, meteorological monographs 1(1), 52 pp.marshack, a. 1985. hierarchical evolution of the human capacity: the paleolithic evidence, american museumof natural history, new york.national academy of public administration (napa). 1991. the archives of the future: archival strategies forthe treatment of electronic databases, a report for the national archives and records administration,washington, d.c.national aeronautics and space administration. 1992. draft guidelines for development of a project datamanagement plan (pdmp), nasa office of space science and applications, washington, d.c.national research council (nrc). 1982. data management and computationñvolume i: issues andrecommendations, space science board, national academy press, washington, d.c.national research council (nrc). 1984.  solarterrestrial data access, distribution, and archiving, spacescience board and board on atmospheric sciences and climate, national academy press, washington, d.c.national research council (nrc). 1986a. atmospheric climate data: problems and promises, board onatmospheric sciences and climate, national academy press, washington, d.c.preserving scientific data on our physical universe: a new strategy for archiving the nation's scientific information resourcescopyright national academy of sciences. all rights reserved.references63national research council (nrc). 1986b. issues and recommendations associated with distributed computationand data management systems for the space sciences, space science board, national academy press,washington, d.c.national research council (nrc). 1988a.  geophysical data: policy issues, committee on geophysical data,national academy press, washington, d.c.national research council (nrc). 1988b. selected issues in space science data management and computation,space science board, national academy press, washington, d.c.national research council (nrc). 1990.  spatial data needs: the future of the national mapping program,board on earth sciences and resources, national academy press, washington, d.c.national research council (nrc). 1992a. setting priorities for space research: opportunities and imperatives,space studies board, national academy press, washington, d.c.national research council (nrc). 1992b.  toward a coordinated spatial data infrastructure for the nation,board on earth sciences and resources, national academy press, washington, d.c.national research council (nrc). 1993. 1992 review of the world data centera for rockets and satellites,national space science data center, board on earth sciences and resources, national academy press,washington, d.c.national research council (nrc). 1994. realizing the information futureñthe internet and beyond,nrenaissance committee, computer science and telecommunications board, national academy press,washington, d.c.national research council (nrc). 1995. study on the longterm retention of selected scientific and technicalrecords of the federal government: working papers, commission on physical sciences, mathematics, andapplications, national academy press, washington, d.c.national research council (nrc). in press.  finding the forest in the trees: the challenge of combining diverseenvironmental data, u.s. national committee for codata, national academy press, washington, d.c.office of management and budget (omb). 1990. coordination of surveying, mapping, and related dataactivities, circular no. a16, washington, d.c.office of management and budget (omb). 1994. management of federal information resources, circular no. a130 (59 f.r. 37906, july 25, 1994), washington, d.c.office of technology assessment (ota). 1994. remotely sensed data: technology, management, and markets,otaiss604, government printing office, washington, d.c.silberschatz, a., m. stonebreaker, and j. ullman. 1991. database systems: achievements and opportunities,communications of the acm 34(10): 110120.preserving scientific data on our physical universe: a new strategy for archiving the nation's scientific information resourcescopyright national academy of sciences. all rights reserved.64preserving scientific data on our physical universe64appendix alist of acronymscdromcompact diskread only memorycenrcommittee on environment and natural resourcesdmcdata management centerdoddepartment of defensedoedepartment of energyerosearth resources observing systemesdmearth science data managementfgdcfederal geographic data committeefitsflexible image transport systemgarpglobal atmospheric research programgcdisglobal change data and information systemgcrpglobal change research programgilsgovernment information locator servicehtmlhypertext markup languageirisincorporated research institutions for seismologyiwgdmgcinteragency working group on data management for global changejanafjoint armynavyair forcejcljoint control languagenaranational archives and records administrationncdcnational climatic data centerngdcnational geophysical data centerniinational information infrastructurenoaanational oceanic and atmospheric administrationnodcnational oceanographic data centernrcnational research councilnsdinational spatial data infrastructurensfnational science foundationnsirnational scientific information resourcenssdcnational space science data centeromboffice of management and budgetpreserving scientific data on our physical universe: a new strategy for archiving the nation's scientific information resourcescopyright national academy of sciences. all rights reserved.appendix a65pdsplanetary data systempo.daacphysical oceanography distributed active archive centersgmlstandard generalized markup languagetcpiptransmission control protocolinternet protocolusgsunited states geological surveyusnrcunited states nuclear regulatory commissionwwssnworldwide standardized seismographic networkpreserving scientific data on our physical universe: a new strategy for archiving the nation's scientific information resourcescopyright national academy of sciences. all rights reserved.66preserving scientific data on our physical universe66appendix bminority opinionthis report has a wealth of good material in it, but i feel that i must write a minority opinion on onemain issue, the committeeõs recommendation to create the nsir federation. i think that the exactfunctions of the nsir federation are still not clear enough to immediately form it, especially sincemechanisms to coordinate data activities already exist.a group such as the nsir federation would not be a good method to set the hardware standards thatare used in data systems (networks, tapes, etc.). the coordinated part of data directory efforts can be builtaround present interagency work. it is reasonable that nara should request lists of datasets intended forlongterm archival, but most of the process of evaluating datasets needs to be kept close to the workinglevel. the discussion of standardization in the report should not be interpreted to mean that all agenciesand archives should be forced to adopt certain standards and rework their data holdings into a commonform and format. there are other concerns for which an analysis of the issues could be useful, but ibelieve that the nsir federation requires a better description of tasks and more debate before such a newbody is established. otherwise we may have more coordination, more systems, more cost, and less data.consider the important task of developing information about data. information about datasets isneeded in at least two or three levels of detail. at the highest level of information, the master directorymethods that are in place for the gcdis can be adopted (or even simplified more) to describe thedatasets. this interagency directory interchange format (dif) is used nationally and internationally.we need to keep it simple enough so that people will submit the information. some agencylevel catalogefforts for datasets have existed since about 1968, and became more serious in the late 1970s. we shouldbuild on the gcdis catalog efforts, and certainly not invent more complicated systems. other datainformation efforts are needed, but they will be based on a bottomup flow of ideas, on workshops, andthe like. each data system does not have to do exactly the same thing, but they must be easy to use. it isnot clear that a formal nsir federation is needed to coordinate this.how does the nsir federation relate to other data coordinating mechanisms? the interagencyworking group on data management for global change (iwgdmgc) meets regularly to help coordinate data issues across many òglobal changeó disciplines, which include air, water, ice, rocks, soils, andsome biology. it seems to me that the iwgdmgc and the proposed nsir federation are mainly tryingto do the same thing. they cover much of the same turf in terms of disciplines. they both wantinformation about data, access to data, and data that will exist for more than 20 years. if we createseparate organizations doing roughly the same thing, then it becomes even less likely that key agencypreserving scientific data on our physical universe: a new strategy for archiving the nation's scientific information resourcescopyright national academy of sciences. all rights reserved.appendix b67people will attend the meetings. nara asked the committee to consider how to deal with all theobservational and laboratory physical sciences. the argument is made that we need an nsir federationbecause the iwgdmgc does not include some disciplines that were included in this study. however,nasa has control of most of the data for planetary sciences and astronomy so that this area may not bevery hard to coordinate, except that data from groundbased telescopes should be included. this leavesthe laboratory sciences, which can be handled as a special case.can the iwgdmgc be characterized as only agencies talking to agencies? no, there is a longstanding nrc panel, the committee on geophysical and environmental data, that has been asked tooversee its work, and that group has sponsored periodic national data forums. does this mean that it isperfect? no, but it is not convincing to me that a roughly parallel coordination effort by an nsirfederation would be necessary.some coordinating mechanisms besides the iwgdmgc will be needed to achieve the goal ofmaking sure that longterm digital archives do exist, are adequately described, and can be used. naracould hold periodic advisory panel meetings or workshops to talk about concerns and possible solutions.similar issueoriented meetings have been sponsored by other agencies and should continue. ininteragency planning, the agencies should remember that some good data activities outside of agenciesare funded by the agencies, but are probably not adequately represented by typical agency planning. thiscould be an argument for an nsir federation, but the problem could be handled in other ways. the ideaof an nsir federation that is nimble, nonbureaucratic, and small is attractive, and it could even be acounterweight to the agencies when that is needed. but we still have to ask: what would the nsirfederation really do? why would not it be just another coordinating office? why would the agencieswant to support it?i believe that the nrc staff for this study has been very able and conscientious in helping to pulltogether this report. the report underwent significant change, but i was unable to fully support thecommitteeõs majority position regarding the proposed nsir federation. nevertheless, i think that somedivergence in viewpoints can help the sponsors and other readers to evaluate the best course of action.roy jennepreserving scientific data on our physical universe: a new strategy for archiving the nation's scientific information resourcescopyright national academy of sciences. all rights reserved.
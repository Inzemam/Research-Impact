detailsdistribution, posting, or copying of this pdf is strictly prohibited without written permission of the national academies press. (request permission) unless otherwise indicated, all materials in this pdf are copyrighted by the national academy of sciences.copyright © national academy of sciences. all rights reserved.the national academies pressvisit the national academies press at nap.edu and login or register to get:œ œ 10% off the price of print titlesœ special offers and discountsget this bookfind related titlesthis pdf is available at sharecontributorshttp://nap.edu/21886future directions for nsf advanced computing infrastructureto support u.s. science and engineering in 20172020156 pages | 6 x 9 | paperbackisbn 9780309389617 | doi 10.17226/21886committee on future directions for nsf advanced computing infrastructure tosupport u.s. science in 20172020; computer science and telecommunicationsboard; division on engineering and physical sciences; national academies ofsciences, engineering, and medicinefuture directions for nsf advanced computing infrastructure to support u.s. science and engineering in...copyright national academy of sciences. all rights reserved.committee on future directions for nsf advanced computing infrastructure to support u.s. science in 20172020computer science and telecommunications boarddivision on engineering and physical sciencesfuture directions for nsf advanced computing infrastructure to support u.s. science and engineering in...copyright national academy of sciences. all rights reserved.the national academies press 500 fifth street, nw washington, dc 20001this activity was supported by award no. oci1344417 from the national science foundation. any opinions, ndings, conclusions, or recommendations expressed in this publication do not necessarily re˚ect the views of any organization or agency that provided support for the project.international standard book number13: 9780309389617international standard book number10: 0309389615digital object identier: 10.17226/21886additional copies of this report are available for sale from the national academies press, 500 fifth street, nw, keck 360, washington, dc 20001; (800) 6246242 or (202) 3343313; http://www.nap.edu.copyright 2016 by the national academy of sciences. all rights reserved.printed in the united states of americasuggested citation: national academies of sciences, engineering, and medicine. 2016. future directions for nsf advanced computing infrastructure to support u.s. science and engineering in 20172020. washington, dc: the national academies press. doi:10.17226/21886.future directions for nsf advanced computing infrastructure to support u.s. science and engineering in...copyright national academy of sciences. all rights reserved.the national academy of sciences was established in 1863 by an act of congress, signed by president lincoln, as a private, nongovernmental institution to advise the nation on issues related to science and technology. members are elected by their peers for outstanding contributions to research. dr. ralph j. cicerone is president.the national academy of engineering was established in 1964 under the charter of the national academy of sciences to bring the practices of engineering to advising the nation. members are elected by their peers for extraordinary contributions to engineering. dr. c. d. mote, jr., is president.the national academy of medicine (formerly the institute of medicine) was estab lished in 1970 under the charter of the national academy of sciences to advise the nation on medical and health issues. members are elected by their peers for distinguished contributions to medicine and health. dr. victor j. dzau is president.the three academies work together as the national academies of sciences,  engineering, and medicine to provide independent, objective analysis and advice to the nation and conduct other activities to solve complex problems and inform public policy decisions. the academies also encourage education and research, recognize outstanding contributions to knowledge, and increase public understanding in matters of science, engineering, and medicine. learn more about the national academies of sciences, engineering, and medicine at www.nationalacademies.org. future directions for nsf advanced computing infrastructure to support u.s. science and engineering in...copyright national academy of sciences. all rights reserved.other recent reports of the computer science and telecommunications boardprivacy research and best practices: summary of a workshop for the intelligence community (2016) bulk collection of signals intelligence: technical options (2015)cybersecurity dilemmas: technology, policy, and incentives: summary of discussions at the 2014 raymond and beverly sackler u.s.u.k. scientic forum (2015)interim report on 21st century cyberphysical systems education (2015)a review of the next generation air transportation system: implications and importance of system architecture (2015)telecommunications research and engineering at the communications technology laboratory of the department of commerce: meeting the nation™s telecommunications needs (2015) telecommunications research and engineering at the institute for telecommunication sciences of the department of commerce: meeting the nation™s telecommunications needs (2015)at the nexus of cybersecurity and public policy: some basic concepts and issues (2014)emerging and readily available technologies and national security: a framework for addressing ethical, legal, and societal issues (2014) future directions for nsf advanced computing infrastructure to support u.s. science and engineering in 20172020: an interim report (2014)interim report of a review of the next generation air transportation system enterprise architecture, software, safety, and human factors (2014)geotargeted alerts and warnings: report of a workshop on current knowledge and research gaps (2013)professionalizing the nation™s cybersecurity workforce? criteria for future decisionmaking (2013)public response to alerts and warnings using social media: summary of a workshop on current knowledge and research gaps (2013)computing research for sustainability (2012)continuing innovation in information technology (2012)the safety challenge and promise of automotive electronics: insights from unintended acceleration (2012, with the board on energy and environmental systems and the transportation research board)the future of computing performance: game over or next level? (2011)public response to alerts and warnings on mobile devices: summary of a workshop on current knowledge and research gaps (2011)strategies and priorities for information technology at the centers for medicare and medicaid services (2011)wireless technology prospects and policy options (2011)limited copies of cstb reports are available free of charge fromcomputer science and telecommunications boardnational academies of sciences, engineering, and medicinekeck center of the national academies500 fifth street, nw, washington, dc 20001(202) 3342605/cstb@nas.eduwww.cstb.orgfuture directions for nsf advanced computing infrastructure to support u.s. science and engineering in...copyright national academy of sciences. all rights reserved.vcommittee on future directions for nsf advanced computing infrastructure to support u.s. science in 20172020william d. gropp, university of illinois, urbanachampaign, cochairrobert j. harrison, stony brook university, cochairmark r. abbott, woods hole oceanographic institutionrobert l. grossman, university of chicagopeter m. kogge, university of notre damepadma raghavan, pennsylvania state universitydaniel a. reed, university of iowavalerie taylor, texas a&m university katherine a. yelick, university of california, berkeleystaffjon eisenberg, director, computer science and telecommunications board, and study directorshenae bradley, administrative assistantfuture directions for nsf advanced computing infrastructure to support u.s. science and engineering in...copyright national academy of sciences. all rights reserved.vicomputer science and telecommunications boardfarnam jahanian, carnegie mellon university, chairluiz andré barroso, google, inc.steven m. bellovin, columbia universityrobert f. brammer, brammer technology, llcedward frank, cloud parity inc. and brilliant lime inc.seymour e. goodman, georgia institute of technology laura haas, ibm corporationmark horowitz, stanford universitymichael kearns, university of pennsylvaniarobert kraut, carnegie mellon university susan landau, worcester polytechnic institutepeter lee, microsoft corporationdavid e. liddle, us venture partners (retired)barbara liskov, massachusetts institute of technologyfred b. schneider, cornell universityrobert f. sproull, university of massachusetts, amherstjohn stankovic, university of virginiajohn a. swainson, dell, inc.ernest j. wilson, university of southern californiakatherine a. yelick, university of california, berkeleystaffjon eisenberg, director lynette i. millett, associate director virginia bacon talati, program ofcershenae bradley, administrative assistantjanel dear, senior program assistantemily grumbling, program ofcerrenee hawkins, financial and administrative manager herbert s. lin, chief scientist (emeritus)for more information on cstb, see its website at http://www.cstb.org, write to cstb at national academies of sciences, engineering, and medicine, 500 fifth street, nw, washington, dc 20001,  call (202) 3342605, or email cstb at cstb@nas.edu.future directions for nsf advanced computing infrastructure to support u.s. science and engineering in...copyright national academy of sciences. all rights reserved.viiprefaceadvanced computing, a term used in this report to include both compute and dataintensive capabilities, is used to tackle a rapidly growing range of challenging science and engineering problems. the national science foundation (nsf) requested that the national academies of sciences, engineering, and medicine carry out a study examining anticipated priorities and associated tradeoffs for advanced computing in support of nsfsponsored science and engineering research. the study encompasses advanced computing activities and programs throughout nsf, including, but not limited to, those of its division of advanced cyberinfrastructure. the statement of task for the full study is given in box p.1. in response to this request, the academies established the committee on future directions for nsf advanced computing infrastructure to support u.s. science in 20172020 (see appendix c). the rst phase of the study culminated in an interim report issued in 2014, future directions for nsf advanced computing infrastructure to support u.s. science and engineering in 20172020: an interim report, that identied key issues and discussed potential options. the interim report set forth nine major areas where the committee sought input from the scientic computing community (box p.2). the committee received over 60 comments from individuals, research groups, and organizations (listed in appendix a) in response to its call for comments. it gathered further input through additional datagathering sessions convened by the committee and listed in appendix b. this is the committee™s nal report. as this study was being completed, an executive order was issued estabfuture directions for nsf advanced computing infrastructure to support u.s. science and engineering in...copyright national academy of sciences. all rights reserved.viii prefacelishing a national strategic computer initiative (nsci), a measure that underscores the importance of advanced computing for the nation in generalšand for science in particular. this report brie˚y discusses nsf™s role in the nsci; see section 2.7 and box 2.5.william d. gropp and robert j. harrison, cochairscommittee on future directions for nsf advanced computing infrastructure to support u.s. science in 20172020box p.1 statement of taska study committee will examine anticipated priorities and associated tradeoffs for advanced computing in support of national science foundation (nsf)sponsored science and engineering research. advanced computing capabilities are used to tackle a rapidly growing range of challenging science and engineering problems, many of which are compute, communications, and dataintensive as well. the committee will consider:1. the contribution of highend computing to u.s. leadership and competiveness in basic science and engineering and the role that nsf should play in sustaining this leadership; 2. expected future nationalscale computing needs: highend requirements, those arising from the full range of basic science and engineering research supported by nsf, as well as the computing infrastructure needed to support advances in modeling and simulation as well as data analysis;3. complementarities and tradeoffs that arise among investments in supporting advanced computing ecosystems; software, data, communications;4. the range of operational models for delivering computational infrastructure, for basic science and engineering research, and the role of nsf support in these various models; and 5. expected technical challenges to affordably delivering the capabilities an interim report will identify key issues and discuss potential options. it might and programs. the framework will address such issues as how to prioritize needs and investments and how to balance competing demands for cyberinfrastructure investments. the report will emphasize identifying issues, explicating options, and articulating tradeoffs and general recommendations.the study will not make recommendations concerning the level of federal funding for computing infrastructure.future directions for nsf advanced computing infrastructure to support u.s. science and engineering in...copyright national academy of sciences. all rights reserved.preface ixbox p.2 questions posed to the scientific community the committee explored and sought comments on the following:how to create advanced computing infrastructure that enables integrated discovery involving experiments, observations, analysis, theory, and simulation; technical challenges to building future, more capable advanced computing systems and how nsf might best respond to them; the computing needs of individual research areas; how to balance resources and demand for the full spectrum of systems, for both compute and dataintensive applications, and the impacts on the research community if nsf can no longer provide stateoftheart computing for its research community; the role of private industry and other federal agencies in providing advanced computing infrastructure; the challenges facing researchers in obtaining allocations of advanced computing resources and suggestions for improving the allocation and review processes; whether wider and more frequent collection of requirements for advanced computing could be used to inform strategic planning and resource allocation, how these requirements might be used, and how they might best be collected and analyzed; ity as well as alternative models that might more clearly delineate the distinction between performance review and accountability and organizational continuity and service capabilities; and how nsf might best set overall strategy for advanced computingrelated activities and investments as well as the relative merits of both formal, topdown coordination and enhanced, bottomup process.future directions for nsf advanced computing infrastructure to support u.s. science and engineering in...copyright national academy of sciences. all rights reserved.future directions for nsf advanced computing infrastructure to support u.s. science and engineering in...copyright national academy of sciences. all rights reserved.acknowledgment of reviewersthis report has been reviewed in draft form by individuals chosen for their diverse perspectives and technical expertise, in accordance with procedures approved by the report review committee. the purpose of this independent review is to provide candid and critical comments that will assist the institution in making its published report as sound as possible and to ensure that the report meets institutional standards for objectivity, evidence, and responsiveness to the study charge. the review comments and draft manuscript remain condential to protect the integrity of the deliberative process. we wish to thank the following individuals for their review of this report:daniel e. atkins iii, university of michigan,david a. bader, georgia institute of technology,robert brammer, brammer technology, llc,andrew a. chien, university of chicago,jeff dozier, university of california, santa barbara,dennis gannon, microsoft research (retired),gary s. grest, sandia national laboratories,laura m. haas, ibm,anthony (ﬁtonyﬂ) john grenville hey, university of washington escience institute,david keyes, king abdullah university of science and technology,michael l. klein, temple university,david a. lifka, cornell university,xifuture directions for nsf advanced computing infrastructure to support u.s. science and engineering in...copyright national academy of sciences. all rights reserved.xii acknowledgment of reviewersjeremiah p. ostriker, columbia university,terrence j. sejnowski, salk institute for biological studies,marc snir, argonne national laboratory,warren m. washington, national center for atmospheric research, andjohn west, texas advanced computing center.although the reviewers listed above have provided many constructive comments and suggestions, they were not asked to endorse the conclusions or recommendations, nor did they see the nal draft of the report before its release. the review of this report was overseen by marcia j. rieke, university of arizona, and butler w. lampson, microsoft research, who were responsible for making certain that an independent examination of this report was carried out in accordance with institutional procedures and that all review comments were carefully considered. responsibility for the nal content of this report rests entirely with the authoring committee and the institution.future directions for nsf advanced computing infrastructure to support u.s. science and engineering in...copyright national academy of sciences. all rights reserved.contentssummary 11 overview and recommendations 9 1.1 position the united states for continued leadership in  science and engineering, 10 1.2 ensure resources meet community needs, 16 1.3 aid the scientic community in keeping up with the revolution in computing, 19 1.4 sustain the infrastructure for advanced computing, 222 background 25 2.1 study task and scope, 25 2.2 past studies of advanced computing for science, 27 2.3 highperformance computing terminology, 29 2.4 state of the art, 30 2.5 nsf investments in advanced computing, 45 2.6 demand for and use of nsf advanced computing  resources, 46 2.7 national strategic computing initiative, 503 maintaining science leadership 52 3.1 critical role of nsf, 53 3.2 global issues, 58xiiifuture directions for nsf advanced computing infrastructure to support u.s. science and engineering in...copyright national academy of sciences. all rights reserved.xiv contents4 future nationalscale needs 64 4.1 the structure of nsf investments and the branscomb pyramid, 64 4.2 dataintensive science and the needs for advanced computing, 69 4.3 forecasting future requirements, 73 4.4 thinking about a new approach to develop  requirements for advanced computing, 75 4.5 roadmapping, 775 investment tradeoffs in advanced computing 83 5.1 tradeoffs among compute, data, and communications, 84 5.2 tradeoffs for dataintensive science, 85 5.3 tradeoffs for simulation science, 88 5.4 datafocused, simulationfocused, and converged architectures, 90 5.5 tradeoffs between support for production advanced computing and preparing for future needs, 91 5.6 conguration choices and tradeoffs, 94 5.7 example portfolio, 1006 range of operational models 102 6.1 goals and opportunities, 103 6.2 organizational challenges and community needs, 107 6.3 potential sustainability approaches, 109appendixesa list of individuals, research groups, and organizations  127 that submitted comments b informationgathering meetings 129c  biosketches of committee members 131d acronyms and abbreviations 138future directions for nsf advanced computing infrastructure to support u.s. science and engineering in...copyright national academy of sciences. all rights reserved.summarythe national science foundation (nsf) asked the national academies of sciences, engineering, and medicine to provide a framework for future decision making about nsf™s advanced computing strategy and programs. advanced computing refers here to the advanced technical capabilities, including computer systems, software, and expert staff, that support a wide range of science and engineering research and that are of a large enough scale and cost that they are typically shared among multiple researchers, institutions, and applications. advanced computing encompasses support for datadriven research as well as modeling and simulation.the recommendations of the committee on future directions for nsf advanced computing infrastructure to support u.s. science in 20172020 are aimed at achieving four broad goals: (1) positioning the united states for continued leadership in science and engineering, (2) ensuring that resources meet community needs, (3) aiding the scientic community in keeping up with the revolution in computing, and (4) sustaining the infrastructure for advanced computing.position the united states for continued leadership in science and engineeringlargescale simulation and the accumulation and analysis of massive amounts of data are revolutionizing many areas of science and engineering research. increased advanced computing capability has historically 1future directions for nsf advanced computing infrastructure to support u.s. science and engineering in...copyright national academy of sciences. all rights reserved.2 future directions for nsf advanced computing infrastructureenabled new science, and many elds today rely on highthroughput computing for discovery. modeling and simulation, the historical focus of highperformance computing, is a wellestablished peer of theory and experiment. datadriven research, a complementary ﬁfourth paradigmﬂ for scientic discovery, needs dataintensive computing capabilities and resources. to support this research, nsf is a major provider of the advanced computing used for u.s. basic science, not only for its own grantees but also in support of research sponsored by other agencies, such as the national institutes of health and the department of energy. meeting future needs will require systems that support a wide range of advanced computing capabilities, including largescale parallel systems and dataintensive systems. approaches that combine largescale computing and data resources in ﬁconvergedﬂ systems can play a role; more specialized systems may also be needed to meet some requirements. commercial cloud computing offers certain advantages and can play a role in nsf™s advanced computing strategy. however, nsf computing centers already exploit economies of scale and load sharing, and commercial cloud providers do not currently support very large, tightly coupled parallel applications, especially for highend simulation workloads. for other applications, especially datacentric workloads and communities that share data sets, cloud computing is positioned today to play a growing role. recommendation 1. the national science foundation (nsf) should sustain and seek to grow its investments in advanced computingšto include hardware and services, software and algorithms, and expertisešto ensure that the nation™s researchers can continue to work at frontiers of science and engineering.recommendation 1.1. nsf should ensure that adequate advanced computing resources are focused on systems and services that support scientic research. in the future, these requirements will be captured in its roadmaps. recommendation 1.2. within today™s limited budget envelope, this will mean, rst and foremost, ensuring that a predominant share of advanced computing investments be focused on production capabilities and that this focus not be diluted by undertaking too many experimental or research activities as part of nsf™s advanced computing program. recommendation 1.3. nsf should explore partnerships, both strategic and nancial, with federal agencies that also provide advanced future directions for nsf advanced computing infrastructure to support u.s. science and engineering in...copyright national academy of sciences. all rights reserved.summary 3computing capabilities, as well as federal agencies that rely on nsf facilities to provide computing support for their grantees.recommendation 2. as it supports the full range of science requirements for advanced computing in the 20172020 time frame, the national science foundation (nsf) should pay particular attention to providing support for the revolution in datadriven science along with simulation. it should ensure that it can provide unique capabilities to support largescale simulations and/or data analytics that would otherwise be unavailable to researchers and continue to monitor the costeffectiveness of commercial cloud services.recommendation 2.1. nsf should integrate support for the revolution in datadriven science into nsf™s strategy for advanced computing by (a) requiring most future systems and services and all those that are intended to be general purpose to be more datacapable in both hardware and software, (b) expanding the portfolio of facilities and services optimized for dataintensive as well as numerically intensive computing, and (c) carefully evaluating inclusion of facilities and services optimized for dataintensive computing in its portfolio of advanced computing services.recommendation 2.2. nsf should (a) provide one or more systems for applications that require a single, large, tightly coupled parallel computer and (b) broaden the accessibility and utility of these largescale platforms by allocating highthroughput as well as highperformance work˚ows to them.recommendation 2.3. nsf should (a) eliminate barriers to costeffective academic use of the commercial cloud and (b) carefully evaluate the full cost and other attributes (e.g., productivity and match to science work˚ows) of all services and infrastructure models to determine whether such services can supply resources that meet the science needs of segments of the community in the most effective ways.maintaining leadership in advanced computing will be challenging. the resources available for advanced computing are inherently limited by research budgets, even as the demand for computing is growing and changing rapidly across the scientic enterprise and as the gap between supply and demand grows. if nsf is unable to increase or better leverage its resources for advanced computing, it seems inevitable that it will be unable to meet future demand for computational resources and will have future directions for nsf advanced computing infrastructure to support u.s. science and engineering in...copyright national academy of sciences. all rights reserved.4 future directions for nsf advanced computing infrastructureto reduce the size of the very largest research projects that are supported by its advanced computing facilities.ensure that resources meet community needsdespite various ongoing efforts to collect and understand requirements from some science communities and occasional efforts to chart strategic directions, the overall planning process for advanced computing resources and programs is not systematic or uniform and is not visibly re˚ected in nsf™s strategic planning, despite its foundationwide importance. the creation of an ongoing and more regular and structured process would make it possible to collect requirements, roll them up, and prioritize advanced computing investments based on science and engineering priorities. recommendation 3. to inform decisions about capabilities planned for 2020 and beyond, the national science foundation (nsf) should collect community requirements and construct and publish roadmaps to allow it to better set priorities and make more strategic decisions about advanced computing.recommendation 3.1. nsf should inform its strategy and decisions about investment tradeoffs using a requirements analysis that draws on community input, information on requirements contained in research proposals, allocation requests, and foundationwide information gathering. recommendation 3.2. nsf should construct and periodically update roadmaps for advanced computing that re˚ect these requirements and anticipated technology trends to help it set priorities and make more strategic decisions about science and engineering and to enable the researchers that use advanced computing to make plans and set priorities.recommendation 3.3. nsf should document and publish on a regular basis the amount and types of advanced computing capabilities that are needed to respond to science and engineering research opportunities.recommendation 3.4. nsf should employ this requirements analysis and resulting roadmaps to explore whether there are more opportunities to use shared advanced computing facilities to supfuture directions for nsf advanced computing infrastructure to support u.s. science and engineering in...copyright national academy of sciences. all rights reserved.summary 5port individual science programs such as major research equipment and facilities construction projects.the roadmaps would re˚ect the visions of the science communities supported by nsf, including both large users and those (in the ﬁlongtailﬂ) with more modest needs. the goal is to develop brief documents that set forth the overall strategy and approach rather than highresolution details. they would look roughly 5 years ahead and provide a vision that extends about 10 years ahead. the roadmaps would help inform users about future facilities, guide investment, align future procurements and services with requirements, and enable more effective partnerships within nsf and with other federal agencies.the roadmapping and requirements process could be strengthened by developing a better understanding of the relationships among requirements, the costs of different approaches (roadmap choices), and science benets. such information would inform program managers about the total cost of proposed research, help focus researcher attention on effective use of these valuable shared resources, and encourage more efcient software and research techniques.recommendation 4. the national science foundation (nsf) should adopt approaches that allow investments in advanced computing hardware acquisition, computing services, data services, expertise, algorithms, and software to be considered in an integrated manner.recommendation 4.1. nsf should consider requiring that all proposals contain an estimate of the advanced computing resources required to carry out the proposed work and creating a standardized template for collection of the information as one step of potentially many toward more efcient individual and collective use of these nite, expensive, shared resources. (this information would also inform the requirements process.)recommendation 4.2. nsf should inform users and program managers of the cost of advanced computing allocation requests in dollars to illuminate the total cost and value of proposed research activities.aid the scientific community in keeping up with the revolution in computingcomputer architectures are changing rapidly along with programming models to use the hardware, creating challenges for the science future directions for nsf advanced computing infrastructure to support u.s. science and engineering in...copyright national academy of sciences. all rights reserved.6 future directions for nsf advanced computing infrastructurecommunity, which depends on and has invested signicantly in science codes written for yesterday™s systems. the rise of dataintensive science brings with it new software and systems. better software tools, technical expertise, and more ˚exible service models (ways of delivering software and computing resources) can improve the productivity of researchers both today and in the future. recommendation 5. the national science foundation (nsf) should support the development and maintenance of expertise, scientic software, and software tools that are needed to make efcient use of its advanced computing resources.recommendation 5.1. nsf should continue to develop, sustain, and leverage expertise in all programs that supply or use advanced computing to help researchers use today™s advanced computing more effectively and prepare for future machine architectures.recommendation 5.2. nsf should explore ways to provision expertise in more effective and scalable ways to enable researchers to make their software more efcient; for instance, by making more pervasive the xsede (extreme science and engineering discovery environment) practice that permits researchers to request an allocation of staff time along with computer time.recommendation 5.3. nsf should continue to invest in and support scientic software and update the software to support new systems and incorporate new algorithms, recognizing that this work is not primarily a research activity but rather is support of software infrastructure.if nsf was to invest solely in production, it would miss some key technology shifts and its facilities would quickly become obsolete. by taking a leadership role in dening future advanced computing capabilities and helping researchers use them more effectively, nsf can help ensure that its software and systems remain relevant to its science portfolio, that researchers are prepared to use the systems, and that investments across the foundation are aligned with this future.recommendation 6. the national science foundation (nsf) should also invest modestly to explore nextgeneration hardware and software technologies to explore new ideas for delivering capabilities that can be used effectively for scientic research, tested, and transitioned future directions for nsf advanced computing infrastructure to support u.s. science and engineering in...copyright national academy of sciences. all rights reserved.summary 7into production where successful. not all communities will be ready to adopt radically new technologies quickly, and nsf should provision advanced computing resources accordingly. sustain the infrastructure for advanced computingexpertise and other longlived assets, such as the physical infrastructure for computing centers, are an essential part of a robust and sustainable advanced cyberinfrastructure. in recent years, nsf has adopted a strategy for acquiring computing facilities and creating centers and programs to operate and support them that relies on irregularly scheduled competition among host institutions roughly every 2 to 5 years and on equipment, facility, and operating cost sharing with those institutions. mounting costs and budget pressures suggest that a strategy that relies on state, institutional, or vendor cost sharing may no longer be viable. repeated competition can lead to proposals designed to win a competition rather than maximize scientic returns. moreover, it is important to ensure the development and retention of the talent that is needed to effectively manage systems, support users, and evolve software to make effective use of today™s and tomorrow™s architectures. recommendation 7. the national science foundation (nsf) should manage advanced computing investments in a more predictable and sustainable way. recommendation 7.1. nsf should consider funding models for advanced computing facilities that emphasize continuity of support.recommendation 7.2. nsf should explore and possibly pilot the use of a special account (such as that used for major research equipment and facilities construction) to support largescale advanced computing facilities.recommendation 7.3. nsf should consider longerterm commitments to centerlike entities that can provide advanced computing resources and the expertise to use them effectively in the scientic community. recommendation 7.4. nsf should establish regular processes for rigorous review of these centerlike entities and not just their individual procurements.future directions for nsf advanced computing infrastructure to support u.s. science and engineering in...copyright national academy of sciences. all rights reserved.8 future directions for nsf advanced computing infrastructuremanaging its advanced computing investments in a more predictable and sustainable way, as it does for other longterm (10 years or more) infrastructure, not only would benet the researchers currently supported by nsf™s advanced computing programs, but also would provide opportunities to apply the same expertise more broadly within nsf, such as the largescale science projects that have longterm needs for advanced computing. it would also create new opportunities for nsf™s advanced computing programs to address longterm storage, preservation, and curation challenges for data.future directions for nsf advanced computing infrastructure to support u.s. science and engineering in...copyright national academy of sciences. all rights reserved.1overview and recommendationsthe national science foundation (nsf) requested that the national academies of sciences, engineering, and medicine carry out a study examining anticipated priorities and associated tradeoffs for advanced computing in support of nsfsponsored science and engineering research. in this study, advanced computing is dened as the advanced technical capabilities, including both computer systems and expert staff, that support research across the entire science and engineering spectrum and that are of a scale and cost so great that they are typically shared among multiple researchers, institutions, and applications.1 as used here, the term encompasses support for datadriven research as well as modeling and simulation.2 data have always been an important element of advanced computing, but the emergence of ﬁbig dataﬂ has created new opportunities for research and stimulated new demand for dataintensive capabilities. the scope of the study encompasses advanced computing activities and programs throughout nsf, including, but not limited to, 1 also critical to nsfsupported advanced computing activities are widearea and campus networks, which provide access and the infrastructure necessary to bring together data sources and computing resources where they cannot practically be colocated. both types of networks have been supported by nsf programs. understanding future networking needs would involve examination of a much wider range of activities across nsfšnot just advanced computing, including many aspects of cyberinfrastructure, but also planned major experimental facilitiesšand is therefore not addressed in this report.2 throughout this report, ﬁcomputingﬂ should be read broadly as encompassing data analytics and other dataintensive applications as well as modeling and simulation and other numerically intensive or symbolic computing applications.9future directions for nsf advanced computing infrastructure to support u.s. science and engineering in...copyright national academy of sciences. all rights reserved.10 future directions for nsf advanced computing infrastructurethose of its division of advanced cyberinfrastructure. the statement of task for the committee on future directions for nsf advanced computing infrastructure to support u.s. science in 20172020 is given in box p.1. this nal report from the study follows the committee™s interim report issued in 2014.3the committee™s recommendations are aimed at achieving four broad goals: (1) position the united states for continued leadership in science and engineering, (2) ensure that resources meet community needs, (3) aid the scientic community in keeping up with the revolution in computing, and (4) sustain the infrastructure for advanced computing.1.1 position the united states for continued leadership in science and engineeringnsf™s investments in advanced computing are critical enablers of the nation™s science leadership. advanced computing at nsf has been used to understand the formation of the rst galaxies in the early universe and to analyze the impacts of cloudaerosolradiation on regional climate change. advanced computing has been a key to awardwinning science, including the 2011 nobel prize in physics and the 1998 and 2013 nobel prizes in chemistry (see box 3.2). its use has moved outside of traditional areas of science to understanding social phenomenon captured in realtime video streams and the connection properties of social networks.largescale simulation, the accumulation and analysis of massive amounts of data, and other forms of advanced computing are all revolutionizing many areas of science and engineering research. modeling and simulation, the historical focus of highperformance computing systems and programs, is a wellestablished peer of theory and experimentation. increased capability has historically enabled new science, and many elds increasingly rely on highthroughput computing. datadriven research has emerged as a complementary ﬁfourth paradigmﬂ for scientic discovery4 that needs dataintensive computing capabilities and resources congured for the transfer, search, analysis, and management of scientic data, often under realtime constraints. even in modeling and simulation applications, dataintensive aspects are increasingly important as large data sets are produced by or incorporated into the simulations. both datadriven and computationally driven sci3 national research council, future directions for nsf advanced computing infrastructure to support u.s. science and engineering in 20172020: an interim report, the national academies press, washington, d.c., 2014.4 j. gray, t. hey, s. tansley, and k. tolle, ﬁjim gray on escience: a transformed scientic method,ﬂ in the fourth paradigm: dataintensive scientic discovery, microsoft research, redmond, wash., 2009.future directions for nsf advanced computing infrastructure to support u.s. science and engineering in...copyright national academy of sciences. all rights reserved.overview and recommendations 11entic processes involve a range of algorithms and work˚ows that may be computeintensive or bandwidthintensive, making simple machine characterizations difcult, especially given that science and engineering discovery frequently integrates all of these. as a result, leadership in frontier science also requires that the united states maintain leadership in both simulation science and datadriven science.nsf has been very successful in making advanced computing resources, especially in support of modeling and simulation, available to an expanding set of disciplines supported by nsf, and has an opportunity to assert similar leadership in datadriven science. nsf is a major provider of computing support for the nation™s science enterprise, not just for the research programs it directly supports. for example, about half of the computer resources allocated under the extreme science and engineering discovery environment (xsede) program are to nonnsfsupported researchers, including 14 percent for work supported by the national institutes of health. moreover, the science and engineering community and other federal agencies that support scientic research look to nsf to provide leadership and to play crucial roles in developing and applying advanced computing, including advancing the intellectual foundations of computation, creating practical tools, and developing the workforce.an exponential rate of growth in demand is now observed that is outpacing the rate of growth in advanced computing resources. at the same time, the cost of provisioning facilities has risen because demand is rising faster than technology improvements are now able to deliver at xed price. the rise in datadriven science and increasing need for both numerically intensive and dataintensive capabilities (recommendation 2) create further demand for resources. production support is needed for software (including preinstalled popular applications and libraries) as well as hardware, to include community software as well as frameworks, shared elements, and other supporting infrastructure. nsf™s software infrastructure for sustained innovation (sisi) program is a good foundation for such investments. however, sisi needs to be grown in partnership with nsf™s science directorates to a scale that matches need, and then be sustained essentially indenitely; the united kingdom™s collaborative computational projects (ccps) provide examples of the impact and successful operation of communityled activities that now span nearly four decades. production support is further needed for data management. curation, preservation, archiving, and support for sharing all need ongoing investment.recommendation 1. the national science foundation (nsf) should sustain and seek to grow its investments in advanced computingšto include hardware and services, software and algorithms, and experfuture directions for nsf advanced computing infrastructure to support u.s. science and engineering in...copyright national academy of sciences. all rights reserved.12 future directions for nsf advanced computing infrastructuretisešto ensure that the nation™s researchers can continue to work at frontiers of science and engineering. an important element of fullling its role of maintaining the nation™s science leadership and achieving the vision in nsf™s cyberinfrastructure framework for 21st century science is providing the research community with access to the needed advanced computing capabilities. this will includeproviding access to sufcient computing facilities and services to support nsf™s portfolio of science and engineering research, including both aggregate capacity and largescale parallel computers and software systems;assuming leadership in providing access to generaluse hardware and software that integrate support for datadriven science as well as large hardware and software systems focused on datadriven science; andassuming leadership for datadriven science, rst by integrating support for datadriven science into most or all of the systems it provides support for on behalf of the research community and next by deploying advanced computing systems focused on datadriven science. recommendation 1.1. nsf should ensure that adequate advanced computing resources are focused on systems and services that support scientic research. in the future, these requirements will be captured in its roadmaps. recommendation 1.2. within today™s limited budget envelope, this will mean, rst and foremost, ensuring that a predominant share of advanced computing investments be focused on production capabilities and that this focus not be diluted by undertaking too many experimental or research activities as part of nsf™s advanced computing program. recommendation 1.3. nsf should explore partnerships, both strategic and nancial, with federal agencies that also provide advanced computing capabilities, as well as federal agencies that rely on nsf facilities to provide computing support for their grantees.today™s landscape for advanced computing is far richer in terms of an expanding range of needs and in terms of technical opportunities for meeting those needs. key elements of this landscape include the following:future directions for nsf advanced computing infrastructure to support u.s. science and engineering in...copyright national academy of sciences. all rights reserved.overview and recommendations 13scientists supported by nsf advanced computing increasingly include a ﬁlong tailﬂ of users with more modest requirements for advanced computing than those with research applications that require parallel computers with a large number of tightly coupled processors. the latter applications cannot be run (or run with acceptable efciency) on smaller systems or on current commercial cloud systems. increased capability has historically enabled new science (see examples in box 3.1). without at least some growth in capability, researchers pursuing science that requires capability computing will have difculty making advances.many elds increasingly rely on highthroughput computing that requires a greater aggregate amount of computing than a typical university can be expected to provide. such applications can be run efciently on both large and mediumsize machines. although a largescale system can run many smaller jobs with good efciency, systems capable of running only smaller jobs cannot run largescale jobs with acceptable efciency. it is not necessary or more efcient to restrict large, tightly coupled systems to run only large, highly scalable applications. modestly sized jobs may still require tight connections, even though at smaller scale, and the utilization of large systems is improved with a mixture of job sizes.the rise in the volume and diversity of scientic data represents a signicant disruption and opportunity for science and engineering and for advanced computing. dataintensive advanced computing represents a signicant opportunity for u.s. science and engineering leadership. some dataintensive applications can be accommodated in more datacapable generalpurpose platforms; other applications will require specically congured systems. supporting datadriven science also places additional demands on widearea networking to share scientic data and raises challenges around longterm storage, preservation, and curation. it also requires diverse and hardtond expertise.large systems are more accessible to a larger group of users; both cloud technologies and science gateways lower the barriers to access applications at scale.cloud computing has shown that access can be ﬁdemocratizedﬂ: many users can access a large system for small amounts of total time in a fashion not supported by current approaches to allocating supercomputer time. moreover, cloud computing users can leverage extensive libraries of software tools developed by both commercial providers and individual scientists. in many ways, this ability for a far larger community to access the power of largescale systems, whether it is a conventional supercomputer or a commercial cloud congured to support some aspects of scientic discovery, represents a qualitative change in the computing landscape. however, nsf computing centers already exploit economies of future directions for nsf advanced computing infrastructure to support u.s. science and engineering in...copyright national academy of sciences. all rights reserved.14 future directions for nsf advanced computing infrastructurescale and load sharing, and commercial cloud providers do not currently support very large, tightly coupled parallel applications, especially for highend simulation workloads. however, this area is under rapid development, and the price (i.e., cost to nsf) and types of services are likely to change. the cost of commercial cloud services could be greatly reduced by reducing or eliminating the overhead charged on these services, bulk purchase by nsf of cloud resources, and/or partnering with commercial cloud providers.the greater complexity of the landscape means that it will be especially important, as recommended in section 1.2, to derive future requirements for advanced computing platforms from an analysis of science needs, workload characteristics, and priorities. to maximize performance, nsf could deploy systems that were optimal for each class of problem. but as a practical matter and for costeffectiveness, nsf must secure access to capabilities that will represent compromises with respect to individual applications but reasonably support the overall research portfolio. put another way, it will require careful resource management driven by an understanding of the science and engineering returns on investments in advanced computing. understanding which compromises to make requires a comprehensive understanding of science requirements and priorities; see the discussion of requirements and roadmapping below.recommendation 2. as it supports the full range of science requirements for advanced computing in the 20172020 time frame, the national science foundation (nsf) should pay particular attention to providing support for the revolution in datadriven science along with simulation. it should ensure that it can provide unique capabilities to support largescale simulations and/or data analytics that would otherwise be unavailable to researchers and continue to monitor the costeffectiveness of commercial cloud services.recommendation 2.1. nsf should integrate support for the revolution in datadriven science into nsf™s strategy for advanced computing by (a) requiring most future systems and services and all those that are intended to be general purpose to be more datacapable in both hardware and software, (b) expanding the portfolio of facilities and services optimized for dataintensive as well as numerically intensive computing, and (c) carefully evaluating inclusion of facilities and services optimized for dataintensive computing in its portfolio of advanced computing services.future directions for nsf advanced computing infrastructure to support u.s. science and engineering in...copyright national academy of sciences. all rights reserved.overview and recommendations 15to support datadriven science, advanced computing hardware and software systems will need adequate data capabilities, in most cases more than is currently provided. some research will need largescale datacentric systems with datahandling capabilities that are quite different from traditional highperformance computing systems. for example, data analytics often requires that data reside on disk for extended periods. several factors suggest that meeting these needs will require one or more large investments, rather than just multiple small projects, including the following: (1) the scale of the largest problems, (2) the opportunities for new science when disparate data sets are colocated, and (3) the cost efciencies that come from consolidating facilities. indeed, the growth in datadriven science suggests that investments will ultimately be needed on a scale comparable to those that support modeling and simulation at the very least, the systems should be better balanced for data (input/output and perhaps memory size), thereby allowing the same systems to be used for different problems without needing to double the size of the resources. as data play a growing role in scientic discovery, longterm data management will become an important aspect of all planning for advanced computing. a partnership with a commercial cloud provider could provide access to larger systems than nsf could afford to deploy on its own. of course, even as it moves to provide better support for datadriven research, nsf cannot neglect simulation and modeling research.recommendation 2.2. nsf should (a) provide one or more systems for applications that require a single, large, tightly coupled parallel computer and (b) broaden the accessibility and utility of these largescale platforms by allocating highthroughput as well as highperformance work˚ows to them. simply meeting current levels of demand will require continuing to provide at least the capacity currently provided by the xsede program and the capability currently provided by blue waters. even as nsf develops its future requirements (recommendation 3) that can be used to develop longterm plans, the observed growth in demand suggests that some growth be included in nsf™s shortterm plans.recommendation 2.3. nsf should (a) eliminate barriers to costeffective academic use of the commercial cloud and (b) carefully evaluate the full cost and other attributes (e.g., productivity and match to science work˚ows) of all services and infrastructure models to determine whether such services can supply resources that meet the science needs of segments of the community in the most effective ways.future directions for nsf advanced computing infrastructure to support u.s. science and engineering in...copyright national academy of sciences. all rights reserved.16 future directions for nsf advanced computing infrastructurefor 2020 and beyond, many of these recommendations may well still hold true, but nsf should rely on the requirements process outlined in the next section. 1.2 ensure resources meet community needsat a time when resources are tight and demand for advanced computing resources continues to grow, it is especially important for nsf to maximize the return on investment in terms of science and engineering outcomes by improving the efciency of advanced computing facility use. one part of this is ensuring that the resources provided match the requirements of the science applications, and this aspect is discussed separately below; another is to ensure that the resources are effectively used. how nsf can help the community use the computing infrastructure effectively is discussed in sections 1.3 and 1.4. the resources available for advanced computing are inherently limited by research budgets as compared to the potentially everexpanding demand for advanced computing. despite various ongoing efforts to collect and understand requirements from some science communities and occasional efforts to chart strategic directions, the overall planning process for advanced computing resources and programs is not systematic or uniform and is not visibly re˚ected in nsf™s strategic planning, despite its foundationwide importance. further, much of what quantication there is makes use of measurements related to ˚oatingpoint performance; this is misleading both because the performance of many applications is not well modeled using just ˚oatingpoint performance and because the sustained as opposed to peak performance of some processors (especially most highly parallel processors) is low on many of those applications.the creation of an ongoing and more regular and structured process would make it possible to collect requirements, roll them up, and prioritize advanced computing investments based on science and engineering priorities. it would re˚ect the visions of science communities and support evaluation of potential scientic advances, probability of success, advanced computing requirements and their costs, and their affordability. such a process needs to be nimble enough to respond to new science opportunities and computing technologies but have a longenough time horizon to provide continuity and predictability to both users and resource providers. the process also needs to involve the growing body of researchers from a growing number of disciplines who use nsf infrastructure. requirements established for future systems and services must also address tradeoffsšfor example, within a given budget envelope for hardware, more memory implies less compute or input/output capacity. future directions for nsf advanced computing infrastructure to support u.s. science and engineering in...copyright national academy of sciences. all rights reserved.overview and recommendations 17the criteria established for future procurements should re˚ect scientic requirements rather than simplistic or unrepresentative benchmarks. one way to capture requirements and enable the science community to participate in the process is to establish roadmaps. roadmaps do not suggest a single path to a destination but rather multiple routes to a variety of goals. such roadmaps would help make science requirements concrete and relate them to future computing capabilities and facilitate planning by researchers, program directors, and facility and service operators at centers and on campuses over a longer time horizon. by capturing anticipated technology trends, the roadmaps can also provide guidance to those responsible for scientic software projects. the roadmaps can also address dependencies between investments by federal agencies through consultation with agencies that use nsf advanced computing facilities or provide computing to the nsfsupported research community. the goal is to develop fairly brief documents that set forth the overall strategy and approach rather than highresolution details, looking roughly 5 years ahead with a vision that extends perhaps for 10 years ahead. roadmaps would help inform users about future facilities, guide investment, align future procurements with requirements and services, and enable more effective partnerships within nsf and with other federal agencies. if researchers are given information about the capabilities they can expect, they can make better plans for their future research and the software to support it. by describing what types of resources nsf will and will not provide, roadmaps would permit other agencies, research institutions, and individual principal investigators to make complementary plans for investments. they would also encourage re˚ection within individual science communities about their future needs and the challenges and opportunities that arise from future computing technologies. by establishing predictability over longer timescales, roadmaps would help those proposing or managing major facilities to rely on shared advanced computing resources, helping reduce the overall costs of advanced computing. the provision in 2015 of such a roadmap for the department of energy (doe) by its ofce of advanced scientic computing research has already enabled the community and science programs to direct their investments and software development efforts toward systems that, in some detail, they know will appear in 20182019 and, in less detail, toward a path that extends into the exascale era of around 2023 and beyond. the nsf academic community presently lacks this ability to plan.the roadmapping process would also be an opportunity to address data curation and storage requirements and link them to individual programs developing data capabilities such as the big data regional innovation hubs. in essence, it could provide ingredients of an nsfwide data future directions for nsf advanced computing infrastructure to support u.s. science and engineering in...copyright national academy of sciences. all rights reserved.18 future directions for nsf advanced computing infrastructureplan that supports the needs of nsf™s grantees and the science communities nsf supports.requirementssetting and roadmapping efforts could be built or modeled on activities undertaken to dene requirements for large scientic facilities such as the academies™ astronomy and astrophysics decadal surveys or doe™s particle physics project prioritization panel. however, the requirements will need to be aggregated at a higher level given that advanced computing facilities generally serve many scientic disciplines. in addition, because of the wide use of computing and data at all scales of resources, it is critical that any such requirements gathering include input from the whole community, including those with more modest (midrange) computing and data needs. sometimes called ﬁthe long tail of science,ﬂ these users have more modest requirements (but still beyond that available in a group, departmental, or campus system) and make up the majority of researchers.recommendation 3. to inform decisions about capabilities planned for 2020 and beyond, the national science foundation (nsf) should collect community requirements and construct and publish roadmaps to allow it to better set priorities and make more strategic decisions about advanced computing.recommendation 3.1. nsf should inform its strategy and decisions about investment tradeoffs using a requirements analysis that draws on community input, information on requirements contained in research proposals, allocation requests, and foundationwide information gathering. recommendation 3.2. nsf should construct and periodically update roadmaps for advanced computing that re˚ect these requirements and anticipated technology trends to help it set priorities and make more strategic decisions about science and engineering and to enable the researchers that use advanced computing to make plans and set priorities.recommendation 3.3. nsf should document and publish on a regular basis the amount and types of advanced computing capabilities that are needed to respond to science and engineering research opportunities.recommendation 3.4. nsf should employ this requirements analysis and resulting roadmaps to explore whether there are more opportunities to use shared advanced computing facilities to supfuture directions for nsf advanced computing infrastructure to support u.s. science and engineering in...copyright national academy of sciences. all rights reserved.overview and recommendations 19port individual science programs such as major research equipment and facilities construction projects.the roadmapping and requirements process could be strengthened by developing a better understanding of the relationship among the cost of different approaches (roadmap choices), requirements, and science benets. for example, the information would inform program managers about the total cost of proposed research and help focus researchers™ attention on effective use of these valuable shared resources, encouraging more efcient software and research techniques. nsf™s xsede program has adopted this practice, which could be expanded to cover all aspects of nsfsupported advanced computing including campuslevel resources. recommendation 4. the national science foundation (nsf) should adopt approaches that allow investments in advanced computing hardware acquisition, computing services, data services, expertise, algorithms, and software to be considered in an integrated manner. recommendation 4.1. nsf should consider requiring that all proposals contain an estimate of the advanced computing resources required to carry out the proposed work and creating a standardized template for collection of the information as one step of potentially many toward more efcient individual and collective use of these nite, expensive, shared resources. (this information would also inform the requirements process.)recommendation 4.2. nsf should inform users and program managers of the cost of advanced computing allocation requests in dollars to illuminate the total cost and value of proposed research activities.1.3 aid the scientific community in keeping up with the revolution in computinghowever, even with a good match to the science requirements, getting the most out of modern computing systems is difcult. better software tools and more ˚exible service models (ways of delivering software and computing resources) can improve the productivity of researchers. improvements to software and new algorithms can often signicantly reduce computational and dataprocessing demands. one class of improvements increases performance on current computer architectures; another takes better advantage of new architectures. there is considerable future directions for nsf advanced computing infrastructure to support u.s. science and engineering in...copyright national academy of sciences. all rights reserved.20 future directions for nsf advanced computing infrastructureuncertainty about future architectural directions for computing in general and for advanced computing for science and engineering specically. architectures are already changing in response to power density issues, which have had limited clock speed growth since 2004, even as transistor density continued to grow. as a result, the creation and evolution of software for scientic applications have become more difcult, especially for those problems that do not readily lend themselves to massive parallelism. the service model, application programming interfaces, and software stacks offered by cloud computing complement the existing supercomputing batch models and software stacks. both the economics and applicability across the full range of science applications will need careful examination. production support is needed for software as well as hardware, to include community software as well as frameworks, shared elements, and other supporting infrastructure. nsf™s sisi program is a good foundation for such investments. production support is further needed for data management. curation, preservation, archiving, and support for sharing all need ongoing investment. recommendation 5. the national science foundation (nsf) should support the development and maintenance of expertise, scientic software, and software tools that are needed to make efcient use of its advanced computing resources.recommendation 5.1. nsf should continue to develop, sustain, and leverage expertise in all programs that supply or use advanced computing to help researchers use today™s advanced computing more effectively and prepare for future machine architectures.recommendation 5.2. nsf should explore ways to provision expertise in more effective and scalable ways to enable researchers to make their software more efcient; for instance, by making more pervasive the xsede (extreme science and engineering discovery environment) practice that permits researchers to request an allocation of staff time along with computer time.recommendation 5.3. nsf should continue to invest in supporting science codes and in continuing to update them to support new systems and incorporate new algorithms, recognizing that this work is not primarily a research activity but rather is support of software infrastructure.future directions for nsf advanced computing infrastructure to support u.s. science and engineering in...copyright national academy of sciences. all rights reserved.overview and recommendations 21if nsf was to invest solely in production, it would miss some key technology shifts and its facilities would quickly become obsolete. some innovation takes the form of netuning of production systems, yet nontrivial but small investments in exploratory or experimental facilities and services are also needed to create, anticipate, and prepare for technology disruptions. nsf needs to play a leadership role in both dening future advanced computing capabilities and enabling researchers to effectively use those systems. this is especially true in the current hardware environment, where architectures are diverging in order to continue growing computing performance. leadership by nsf will help ensure that its software and systems remain relevant to its science portfolio, that researchers are prepared to use the systems, and that investments across the foundation are aligned with this future. it will be especially important for nsf to be not only engaged in but helping to lead the national and international activities that dene and advance future software ecosystems that support simulation and datadriven science, including converging the presently distinct tools and programming paradigms, and the software required for exascale hardware technologies. nsf may be especially well positioned to collaborate internationally compared to the mission science agencies given its long track record of open science collaboration. doe is currently investing heavily in new exascale programming tools that, through the scale of investment and buyin from system manufacturers, could plausibly dene the future of advanced programming even though the design may not re˚ect the needs of all nsf science because the centers and researcher communities it supports are not formally engaged in the specication process. it is also important for nsf to be engaged with the private sector and academia for insights into data analytics.recommendation 6. the national science foundation (nsf) should also invest modestly to explore nextgeneration hardware and software technologies to explore new ideas for delivering capabilities that can be used effectively for scientic research, tested, and transitioned into production where successful. not all communities will be ready to adopt radically new technologies quickly, and nsf should provision advanced computing resources accordingly. investments by other federal agencies in new computing technologies and nsf™s own computing research programs will both be sources of advanced hardware and software architectures to consider adopting in nsf™s advanced computing programs. achieving continued growth in nsf™s aggregate computing performance on a xed budget will likely require new architectural models that are more energy efcient. the future directions for nsf advanced computing infrastructure to support u.s. science and engineering in...copyright national academy of sciences. all rights reserved.22 future directions for nsf advanced computing infrastructurerequirementsgathering and roadmapping process can be used to obtain longterm predictions of available capabilities and their energy requirements. that process will also provide insights into which technology advances are suitable for future production services.1.4 sustain the infrastructure for advanced computinga hard but essential part of managing advanced computing in a xed budget envelope will be discontinuing activities in order to start or grow other activities. the requirements analysis will provide a rational and open basis for these decisions, and the roadmaps will enable communities to plan and adapt in advance of future investments. even in a favorable budget environment for science and engineering generally and for advanced computing specically, nsf will need to manage exponentially growing demand and rising costs (see section 1.1). one response to these challenges is to take advantage of the opportunities described in sections 1.2 and 1.3 to increase efciency and productivity in the use of advanced computing facilities, to use the requirements process to inform tradeoffs, and to exploit new technologies. in addition, there are several possibilities for nding more or better leveraging resources. these include the following:making a case for additional resources based on the requirements analysis. for example, the 2003 report a sciencebased case for largescale simulation5 is widely credited with developing the rationale and science case for a major expansion of doe™s advanced scientic computing research program. it may also be useful to look retrospectively at what computing capabilities were needed to achieve past science breakthroughs. seeking funding mechanisms that ensure consistent and stable investments in advanced computing. adopting approaches that make it easier to accommodate the costs of large facilities within annual budgets, such as leasing to smooth costs across budget years.exploring partnerships, both strategic and nancial, with federal agencies that also provide advanced computing capabilities as well as federal agencies that rely on nsf facilities to provide computing support for their grantees. for example, nsf might enter into a nancial agreement with other federal (or possibly private) providers of advanced computing 5 ofce of science, u.s. department of energy, a sciencebased case for largescale simulation, washington, d.c., 2003. future directions for nsf advanced computing infrastructure to support u.s. science and engineering in...copyright national academy of sciences. all rights reserved.overview and recommendations 23services for access to a fraction of a large system, thus maintaining some ability to support research that involves a large system without incurring its full acquisition cost.chapter 7 of this report provides more details on these options and discusses several others for nsf to consider.in recent years, nsf has adopted a strategy for acquiring computing facilities and creating centers and programs to operate and support them that relies on irregularly scheduled competition among host institutions roughly every 2 to 5 years and on equipment, facility, and operating cost sharing with those institutions. mounting costs and budget pressures suggest that a strategy that relies on state, institutional, or vendor cost sharing may no longer be viable. moreover, there are reasons to consider models that provide a longer funding horizon for service providers that operate facilities and the expertise needed for their effective utilization.in particular, one key reason is to ensure the development and retention of the advanced computing expertise that is needed to effectively manage systems, support their users, address the increasing complexity of hardware and software, and manage the needed transition of software to make effective use of today™s and tomorrow™s architectures. doing so requires sustained attention to the workforce and more viable career pathways for its members. a longer funding horizon would also better match the depreciation period for buildings, power, and cooling infrastructure. another reason to consider other models is that repeated competition can lead to proposals designed to win a competition rather than maximize scientic returns. for example, it can unduly favor unproven technology over more proven, productionquality technology. by contrast, a model with longer time horizons may be better positioned to deliver systems that meet the scientic requirements established by the requirements denition and roadmapping activities. supporting at least two entities will provide healthy competition as well as stability. the acquisition of individual systems from commercial vendors would remain competitive. such longerterm entities can take the form of distributed organizations; xsede, for example, has evolved in this direction in providing the scientic research community with expertise and services.a longer funding horizon would also better match the duration of major scientic facilities and the useful lifetime of scientic data, creating new opportunities to address longterm challenges of storage, preservation, and curation. greater continuity would also foster greater leveraging of advanced computing expertise and facilities across nsf. for instance, longlived experimental or observational facilities could better manage the risk of standing up their own cyberinfrastructure by partnering with centers. future directions for nsf advanced computing infrastructure to support u.s. science and engineering in...copyright national academy of sciences. all rights reserved.24 future directions for nsf advanced computing infrastructurerecommendation 7. the national science foundation (nsf) should manage advanced computing investments in a more predictable and sustainable way. recommendation 7.1. nsf should consider funding models for advanced computing facilities that emphasize continuity of support.recommendation 7.2. nsf should explore and possibly pilot the use of a special account (such as that used for major research equipment and facilities construction) to support largescale advanced computing facilities.recommendation 7.3. nsf should consider longerterm commitments to centerlike entities that can provide advanced computing resources and the expertise to use them effectively in the scientic community. recommendation 7.4. nsf should establish regular processes for rigorous review of these centerlike entities and not just their individual procurements.future directions for nsf advanced computing infrastructure to support u.s. science and engineering in...copyright national academy of sciences. all rights reserved.2background2.1 study task and scopethe national science foundation (nsf) requested that the national academies of sciences, engineering, and medicine carry out a study examining anticipated priorities and associated tradeoffs for advanced computing in support of nsfsponsored science and engineering research. the scope of the study encompasses advanced computing activities and programs throughout nsf, including, but not limited to, those of its division of advanced cyberinfrastructure. the statement of task for the study is given in box p.1. this nal report from the study follows the committee™s interim report issued in 2014.1in this study, advanced computing is dened as the advanced technical capabilities, including both computer systems and expert staff, that support research across the entire science and engineering spectrum and that are so large in scale and so expensive that they are typically shared among multiple researchers, institutions, and applications. the term also encompasses higherend computing for which there are economies of scale in establishing shared facilities rather than having each institution acquire, maintain, and support its own systems. at the midscale, the demarcation between institutional and nsf responsibility is not well established (box 2.1). for computeintensive research, it includes not 1 national research council, future directions for nsf advanced computing infrastructure to support u.s. science and engineering in 20172020: an interim report, the national academies press, washington, d.c., 2014.25future directions for nsf advanced computing infrastructure to support u.s. science and engineering in...copyright national academy of sciences. all rights reserved.26 future directions for nsf advanced computing infrastructurebox 2.1  one of the consequences of the exponential growth in computing power is ago. for many researchers, a laptop or desktop system provides all of the computing power that they might need. other researchers may need slightly more, while others depend on the capabilities only available in current supercomputer systems. who should be responsible for providing this computing infrastructure?at the very high end (national scale in terms of cost, operation, and use), the computing infrastructure is like other nationalscale research facilities and supports research that is not possible without it. at the very low end (individual desktops or laptops), it can be argued that this should now be the responsibility of individual institutions, just like the other basic research support that they provide. what about the midrange? how capable of a system should individual institutions or regional consortia be expected to provide for their researchers? what about researchers who need large amounts of computing in the aggregate, but where each individual run could be done on a small machine?their researchers; this is often viewed as a competitive advantage both in attracting and retaining faculty and staff and in winning grants. but many institutions, notably for their researchers to pool funds into a shared computing infrastructure (creating what in many ways is a private cloud), which may also be partly supported by institutional funds. as the national science foundation (nsf) considers how it supports advanced computing, it will need to consider how much computing is the responsibility of the institution, how much may be supported at individual institutions and regional consortia (in part through grants from nsf or other agencies), and how much is provided as a national resource. this is a complex issue, and one that will require more study and engagement with stakeholders. among the issues to consider are the following:how best to take advantage of economies of scale;how to ensure that all researchers, not just those at the bestfunded research institutions, have access to the computing resources needed for their research;how to avoid wasted or unused cycles and ensure systems are wellmanaged and secure;how to ensure that the systems match the needs of researcheršthat is, to the system for all collaborators; andhow to encourage and help institutions to provide a basic level of computing support, taking advantage of ways to share infrastructure and expertise.the requirements analysis recommended by the committee (see chapter 4) will provide valuable data in addressing these issues.future directions for nsf advanced computing infrastructure to support u.s. science and engineering in...copyright national academy of sciences. all rights reserved.background 27only today™s supercomputers, which are able to perform more than 1015 ˚oatingpoint operations per second (known as ﬁpetascaleﬂ), but also highperformance computing (hpc) platforms that share the same components as supercomputers but may have lower levels of performance. as used here, the term encompasses support for dataintensive research that involves analyzing terabytes (and increasingly petabytes) of data as well as modeling and simulation. historically, and even now, nsf advanced computing centers have focused on highperformance computing primarily for simulation. although these applications are essential and growing, the new and very rapidly growing demand for more datacapable services still needs to be addressed. this chapter looks chie˚y at traditional hpc, while the new opportunities and challenges of the ﬁdata revolutionﬂ are emphasized in chapters 4 and 5.2.2 past studies of advanced computing for sciencein the early 1980s, the science community developed several reports regarding the lack of access to advanced computing resources. the 1982 report largescale computing in science and engineering, known as the ﬁlax report,ﬂ2 was jointly sponsored by the department of defense (dod) and nsf, with cooperation from the department of energy (doe) and the national aeronautics and space administration. it focused on the growing importance of supercomputing in the advancement of science and the looming gap in access to and capability of these resources. the lax report noted that the united states was at risk of losing its lead in supercomputing and that the development of new systems (especially those relying on new architectures such as massively parallel machines) would require continued investment by the federal government and that the commercial sector could not be expected to provide the necessary research and development (r&d) support. the report proposed four thrusts for a national program:1. increased access to supercomputer resources through a nationwide network,2. research in software and algorithms for the expected changes in hardware architectures,3. training of staff and graduate students, and4. r&d for future generations of supercomputers.2 panel on large scale computing in science and engineering, report of the panel on largescale computing in science and engineering, national science foundation, washington, d.c., 1982, http://www.pnl.gov/scales/docs/ laxreport1982.pdf.future directions for nsf advanced computing infrastructure to support u.s. science and engineering in...copyright national academy of sciences. all rights reserved.28 future directions for nsf advanced computing infrastructurethe lax report led to the rst round of nsf supercomputer centers established in 19851986. while a subset of these centers continued through 1997, the director of nsf commissioned the task force on the future of the nsf supercomputer centers program in 1994, chaired by edward hayes.3 the report of the task force, issued in 1995, put forth many of the points from the lax report, noting that supercomputing would enable progress across many areas of science and this progress would depend on continuing development of highly trained personnel as well new algorithms and software. the hayes report made many recommendations that focused on both ﬁleadingedge sitesﬂ and broader partnerships that would include experimental and regional facilities. the net result was that the report recognized that there would be fewer leadingedge sites to accommodate more systems below the apex of the computational pyramid. this was manifested as the partnership for advanced computational infrastructure (paci) from 1997 to 2004. paci was supplemented by the terascale initiatives in 2000, which led to the creation of the teragrid in 2004, which transitioned to the presentday extreme science and engineering discovery environment program. the 2003 atkins report4 articulated a more ecological, holistic view of cyberinfrastructureenabled research, including computing, data stewardship, sensing, activation, and collaboration, to create a comprehensive platform for discovery. it was followed by a series of workshops and reports exploring the role of cyberinfrastructure to particular research communities.5 in 2005, nsf™s ofce of cyberinfrastructure released the solicitation ﬁhigh performance computing system acquisition: towards a petascale computing environment for science and engineeringﬂ (nsf 05625). this was the rst in a series of solicitations along different tracks, culminating in the blue waters petascale facility at the national center for supercomputing applications (ncsa) that began operating in 2013.the past reports present common themes, many of which persist today, as this report will show. today, advanced computing capabilities are involved in an even wider range of scientic elds and challenges, and the rise of datadriven science requires new approaches. the gap between 3 task force on the future of the nsf supercomputer centers program, report of the task force on the future of the nsf supercomputer centers program, national science foundation, washington, d.c., september 15, 1995, http://www.nsf.gov/pubs/1996/nsf9646/nsf9646.pdf.4 national science foundation, revolutionizing science and engineering through cyberinfrastructure: report of the national science foundation blueribbon advisory panel on cyberinfrastructure, 2003, http://www.nsf.gov/cise/sci/reports/atkins.pdf. 5 national science foundation, ﬁreports and workshops relating to cyberinfrastructure and its impacts,ﬂ http://www.nsf.gov/cise/aci/reports.jsp, accessed january 27, 2016.future directions for nsf advanced computing infrastructure to support u.s. science and engineering in...copyright national academy of sciences. all rights reserved.background 29supply and demand, noted in the lax report, remains an important issue. the need to maintain and grow the workforce, especially in regard to the needed skills, remains a persistent issue. the evolution in hardware and the subsequent impacts on algorithms and software has been a recurring concern. however, changes in architectures have been far more disruptive over the past decade, and broad commercial trends in˚uence the hpc market more than ever. finally, increasing use of largescale computing by the commercial sector offers some new opportunities and challenges.2.3 highperformance computing terminologythis report refers to a number of concepts from hpc. these terms do not have precise denitions but are valuable in referring to qualitative properties of different kinds of computing and computing systems.capability computing refers to computing that requires the most capable systems, typically the most powerful supercomputers.capacity computing refers to computing with large numbers of applications, none of which require a ﬁcapabilityﬂ platform but in their aggregate require large amounts of computing power.highthroughput computing refers to the use of many computing resources over a period of time to attack a particular set of computational tasks.leadership class is a term for the most powerful computing systems. this has typically been based on the ˚oatingpoint performance of the computing system, though a more comprehensive metric can be used. see figure 4.1 (branscomb pyramid) for one (though dated) ranking of computer systems from desktop through leadership class.highend computing covers computing from systems larger than a system that a single research group might operate through leadership class systems. there is no accepted denition for how powerful a system must be to be considered a highend computing system. the terms ﬁsupercomputerﬂ and ﬁhighperformance computerﬂ have similar, imprecise meanings. ensemble computing often refers to the use of many runs with different input data or parameters to explore the sensitivity of the problem to small changes. tightly coupled computing refers to computations where each computing element must exchange data with some other computing elements very frequently, such as once per simulation time step. such computations require a highperformance internode interconnect.memory capacity limited refers to applications that have more demanding requirements than others. for example, simulations in three future directions for nsf advanced computing infrastructure to support u.s. science and engineering in...copyright national academy of sciences. all rights reserved.30 future directions for nsf advanced computing infrastructuredimensions of large domains can require very large amounts of memory; a 10,000 × 10,000 × 10,000 cube requires 1012 points or roughly 1 tb of storage per variable stored.peak and sustained performance. peak performance refers to the performance of a computing system that is theoretically possible. it usually refers to ˚oatingpoint performance and assumes that the maximum number of ˚oatingpoint operations is performed in every clock cycle. no applications run at the peak rate. sustained performance is the performance that an application or a collection of applications can sustain over the course of the entire application.this report avoids the terms ﬁcapability computingﬂ and ﬁcapacity computingﬂ because they are too imprecise and have also historically been too focused on ˚oatingpoint performance.2.4 state of the artthe past several decades have seen remarkable progress in computer hardware, algorithms, and software. this section reviews the state of the art in hardware, software, and algorithms, with a particular emphasis on the challenges created by the disruptive changes in computer architecture driven by the need to increase computing power.2.4.1 hardwarethe past decade has seen an enormous disruption in computer hardware throughout the computing industry, as processor clock speed increases have stalled and parallel processing has moved onchip with multicore processors.6 the primary drivers have been power density and total energy consumptionšconcerns that are important in portable devices and increasingly in large data and compute centers due to fundamental cooling limits of packaging and overall facility infrastructure and operations costs. the continued growth in transistor density had been used primarily to add more processor cores, starting with dualcore chips in the mid2000s to 20core chips a decade later. but these processors were historically designed to maximize performance without a strong constraint on energy use; a second trend has been the growth of manycore architectures that involve a larger number of smaller and simpler cores, each more energy efcient than a traditional processor. in aggregate, a 6 for more on this challenge and its implications, see national research council, the future of computing performance: game over or next level? the national academies press, washington, d.c., 2011. future directions for nsf advanced computing infrastructure to support u.s. science and engineering in...copyright national academy of sciences. all rights reserved.background 31processing chip with hundreds of simpler cores can often provide much higher computational performance than a smaller number of more powerful cores. the manycore designs include graphical processing units (gpus) and were initially designed as accelerators to a traditional cpu, whereby software primarily ran on the cpu but could of˚oad computingintensive kernels to the accelerator. more recent manycore designs provide for stronger integration between the accelerator and cpu, allowing for shared memory between the two or standalone processors made entirely of manycore chips. box 2.2 contains further discussion of these architectural challenges. one consequence of the growth in the use of computing by all aspects of society and not just for science research is that much of the investment by both computer hardware and software vendors is directed at the larger commercial market for computing. an example of this is the use of gpus in computational science. these processors have been adapted to support computational science, but the initial innovations were made to serve the gaming market. as the commercial markets continue to grow and new applications are developed, advanced cyberinfrastructure will need to continue to gure out how best to exploit innovations and advancements in the greater commercial market.looming ahead is the end of transistor scaling, which will mean an end to the current strategy of improving computing performance by adding more cores per chip. the result is unlikely to be a discrete stopping box 2.2 ing (hpc) systems for decades, by doubling the number of transistors on a die at regular intervals, with the speed of these smaller transistors getting faster at essentially an equal rate. although transistor density will continue to increase for some time to come, the year 2004 represented a watershed where hpc architectures were forced to change direction dramatically. getting heat out of chips hit a limit, so that increasing the inherent transistor speed no longer translates into faster core clocks. the only alternative was to use extra transistors in more, but slower, cores and require applications to use that resulting parallelism explicitly. this development, combined with a rapid growth in the number of racks for a system, permitted benchmark performance for the linpack kernel (solution of a dense system of linear equations by gaussian elimination) to continue on its near doubling of growth per year. the emergence of ﬁlightweightﬂ processors that were even slower than the powerlimited, highend servers paradoxically continuedfuture directions for nsf advanced computing infrastructure to support u.s. science and engineering in...copyright national academy of sciences. all rights reserved.32 future directions for nsf advanced computing infrastructureadded to this increase by allowing many more nodes to be physically packaged in the same volume. such architectures took over the bulk of the top 10 (of the top500) ranking1 until 2008, when a second architectural transition occurred with the introduction of numericintensive chips with very large numbers of even simpler cores derived from highend graphics processors. ﬁhybrid systemsﬂ that join such chips with conventional cores have yet again changed the complexion of the top 10 systems (figure 2.2.1). it appears, however, that even these changes have hit at least a temporary roadblock, with no growth in the top system for dense linear algebra since 2013. the same phenomena can be seen in benchmarking of hpc systems for applications that are decidedly nonnumeric and have many of the properties one might expect for big data. figure 2.2.2 is similar to figure 2.2.1, except that the rapid rise in yearoveryear performance hit a wall in 2013, with very little growth computing it represents) and the nonnumeric graph500 is a third benchmark where reported data are becoming available and which represents problems that lie between these two. the highperformance conjugate gradients (hpcgs), figure 2.2.1 speed of top 10 systems from top500 ranking. source: updated from high performance computing: 30th international conference, isc high performance 2015, frankfurt, germany, july 1216, 2015, proceedings, using data from http://top500.org.box 2.2 continuedfuture directions for nsf advanced computing infrastructure to support u.s. science and engineering in...copyright national academy of sciences. all rights reserved.background 33but one that is extremely sparse and is solved using a different and iterative algorithm. the data to be involved in the computations are now embedded in a sparse computation rates to peak computational rates over a variety of systems, with a this has been well known in the hpc community, where memory performance is performance.(flop) were common but went into a precipitous decline after that, especially for hybrid systems. the average supercomputer today has between 1/100th and 1/10th the memory per flop of a decade ago.figure 2.2.2traversed edges per second. note: gteps, gigatraversed edges per second. source: in high performance computing: 30th international conference, isc high performance 2015, frankfurt, germany, july 1216, 2015, proceedings, using data from http://graph500.org.continuedfuture directions for nsf advanced computing infrastructure to support u.s. science and engineering in...copyright national academy of sciences. all rights reserved.34 future directions for nsf advanced computing infrastructurethe reason for this constraint goes back to architecture and the way commercial memory chips are attached to modern processors. the basic memory cell suffered the power issue that changed processor chip architectures. instead, the need to keep such chips cheap has meant that vendors have downsized the size of memory chips to provide better yield, giving up memory size per chip as a result. also, the way memory is connected to modern processors has hit a wall of its own. there are only so many pins available on modern processor chips to connect to memory, regardless of the number or speed of cores on the processor. this means that the maximum number of memory chips that may be attached to a processor chip is relatively limited, and with the slower growth rate of memory chip capacity relative to processor performance, the result is exactly what has been observed.this issue of the path between processor and memory is also most probably at the root of the poor performance observed for both graph500 and hpcg, as the rate at which commands can be sent from the processor chip to the memory into modern processors becomes largely wasted.these observations do not doom our capability to advance toward exascale; instead they warn us that a major upheaval in architecture is likely, one that will end up having as much effect on programming and algorithms as the advent of  ratio of high performance conjugate gradients (hpcg) computation rates to peak computational rates over a variety of systems. source: updated from peter kogge, ﬁupdating the energy model for future exascale systems,ﬂ in high performance computing: 30th international conference, isc high performance 2015, frankfurt, germany, july 1216, 2015, proceedings.box 2.2 continuedfuture directions for nsf advanced computing infrastructure to support u.s. science and engineering in...copyright national academy of sciences. all rights reserved.background 35the mid2000s. this change is visible today with the introduction of ﬁ3d stacked memoryﬂ components, where multiple memory die are placed on top of a logic die. and decreases in the energy of such accesses. today such ﬁstacksﬂ are still tied to conventional processor chips, enabling just a ﬁfasterﬂ memory path. in the near term, however, combinations of lightweight and hybrid architectures will move cores onto the logic die along with the network interface controller, resulting in a standalone compute node. hundreds of these may be placed in the space of a modern compute node, breaking the barriers presented today. the upshot of this is that the advanced computing facilities of the near future ensuring that the programs and algorithms being written today that need to scale into these new regimes are designed with these differences in mind and that early facilities should be available as such machines come online to allow validation of the portability of such codes.1 see the top500 website at http://top500.org, accessed january 27, 2016.the past 20 years. source: updated from peter kogge, ﬁupdating the energy model for future exascale systemsﬂ in high performance computing: 30th international conference, isc high performance 2015, frankfurt, germany, july 1216, 2015, proceedings.future directions for nsf advanced computing infrastructure to support u.s. science and engineering in...copyright national academy of sciences. all rights reserved.36 future directions for nsf advanced computing infrastructurepoint in chip density, but rather a continued slowing of improvements based on technical and cost challenges, as well as diminishing returns on investments if the density improvements do not immediately equate to improvements in cost performance of computing devices. the problem of declining performance improvements is not limited to science and engineering applications, but highend computing with its emphasis on benchmarks and scaling may be the place where slowing the rate of performance improvements will be most obvious. one place where this slowing of performance improvement can be seen is in the bottom of the top500 list, which is based on the performance of a simple dense linear algebra algorithm. since the late 2000s, the rate of performance improvement for the systems at the bottom of the list (still very fast) has fallen considerably.memory system design is also undergoing rapid changes, as new forms of onpackage dynamic randomaccess memory (dram) memory provide enormous bandwidth improvements but currently less capacity than offchip dram. at the same time, new forms of nonvolatile memory have been developed with much higher bandwidths than disks but somewhat different performance characteristics than dram. these features may be of particular interest to data analysis applications, although many simulations are also limited by data sizes and could benet. these new types of memory may be added to the hierarchy in a current system design, but they may be under software rather than hardware or operating system control. in general, data movement between processors or to memory is expensive in both time and energy, so hardware mechanisms that automatically schedule and move data may be replaced by simpler mechanisms that leave data movement under software control. although each of these innovations is designed to increase performance while minimizing energy use, they pose signicant challenges to software. the scientic modeling and simulation community has billions of dollars invested in software based on message passing between serial programs, with only isolated examples of applications that can take advantage of accelerators. shrinking memory size per core is a problem for some applications, and explicit data movement may require signicant code rewriting because it requires careful consideration of which data structures should be allocated in each type of memory, keeping track of memory size limits, and scheduling data movement between memory spaces as needed. further disruptive innovation is on the horizon. for example, processorinmemory technology has been advanced as a way to reduce memory latency and increase bandwidth, and memristors could potentially be used for nonvolatile memory with a very high density and fast access times.future directions for nsf advanced computing infrastructure to support u.s. science and engineering in...copyright national academy of sciences. all rights reserved.background 37the scientic computing community therefore must balance (1) leaving software and programming models unchanged and giving up on opportunities for more computing performance that come from these hardware changes with (2) developing new codes based on new programming models, such as those being researched within the doe exascale initiative, that can exploit the new hardware. some type of energyefcient processing and memory system will be necessary for building an aggregate exascale capability that nsf can afford to operate, whether that is in a single system, in many systems, or partially based on commercial cloud resources. the breadth of nsf™s workload and the number of architectural options complicate this decision. on the surface, the manycore processors may be best suited to computeintensive simulation problems, yet some data analysis workloads, such as image analysis and neural net algorithms, run effectively on gpus, while highly irregular simulation problems so far do not. nonaccelerator manycore options such as the intel phi may provide more familiar programming support and more workload ˚exibility, but may not achieve the same performance benets. further, they are relatively untested and had yet to demonstrate high performance across a wide range of applications at the time this report was prepared. data storage has also undergone its own exponential improvement, with both data densities (bits per unit area) and bit per unit cost doubling every 1 to 2 years. new technologies are providing revolutionary advances and blurring the line between ﬁstorageﬂ and ﬁmemory.ﬂ however, while the technology continues to improve, the rate of improvement has fallen off in recent years. historically, external storage has primarily meant magnetic hard disk drives (hdds) in which data are encoded on spinning platters of magnetic media. the vast majority of the world™s online data (some 12 zettabytes) are stored on hdd, and this is projected to be the case for at least the next 5 years. over the course of six decades and driven in part by advances in fundamental material science, hdds have gone from devices the size of washing machines, storing 3.75 mb, to modern 2.5in. disks holding 8 tb and up. this expansion in capacity is projected to continue. but capacity is just one of several gures of meritšothers include bandwidth, latency, and input/output operations per second (iops), which have all advanced at a much slower pace than capacityšand none are anticipated to advance signicantly over current hdd technologies that have effective bandwidths of circa 1200 mb/s, latencies of a few milliseconds, and iops of 1200. this is in part due to the physical constraints of spinning media, but also because investments are focusing on new technologies that are already delivering 1,000fold advances over hdd in some performance metrics. parallelism to many disks is required to provide very high data rates. latencies have not improved as much; future directions for nsf advanced computing infrastructure to support u.s. science and engineering in...copyright national academy of sciences. all rights reserved.38 future directions for nsf advanced computing infrastructurefor spinning disks, the latencies are dominated by the disk revolutions per minute and head seek time, which have advanced much more slowly than the densities and transfer rates (bandwidth). in contrast, solidstate disks (ssds) provide much lower latency and greater data transfer rates. ssds are presently based on various nonvolatile (meaning data persists even without power) silicon memory technologies that will continue to benet from advances in silicon manufacturing technologies. in the past, ssds were regarded as both small and expensive, but in the past few years, the capacity of ssds has approached that of hdds, and while presently about 3 to 10 times more expensive per byte than hdds, price parity is expected within several years. with new standards for connecting ssds to computer systems (e.g., nonvolatile memory express), ssds are now capable of delivering bandwidths of several gigabytes per second, latencies of a few microseconds, and 100,000 iops. in addition to use in storage, the price, performance, persistence, and power characteristics of nonvolatile memory technologies enable innovations in computer architectures to complement regular dram, such as in the proposed doe preexascale systems. in summary, over the next few years, hdd storage capacity will continue to decrease slowly in cost, but various performance metrics will see revolutionary change as nonvolatile memory technologies become even more price competitive, and eventually storage capacity itself will fall in cost once silicon technologies dominate.advances in storage capacity were critical enablers of the dataintensive nobel prizewinning work of perlmutter (see box 3.2), as well as the discovery of the higgs boson at the large hadron collider by an international collaboration storing and analyzing more than 100 pb of data. diverse other elds of science have been transformed by the ability to manipulate massive data sets from genomics, social networks, video and images, satellite data, and the results of simulations. looking forward, continued advances in capacity and revolutionary advances in other aspects of data technologies promise new revolutions in science across many elds presently constrained by their ability to store, explore, or analyze their data at sufcient scale or speed.because of these relatively high latencies, as well as the limits in bandwidth compared with semiconductor memory, a wide range of memory architectures are being developed with intermediate performance. some of these will be used closer to the compute elements and have been mentioned above. others may be used to boost the apparent performance of disks, for example, by providing a higherbandwidth, lowerlatency, temporary buffer that can absorb bursts of data to write to disk. all of these new input/output (i/o) and memory products will need new software and, in many cases, new algorithms that t their performance characteristics.future directions for nsf advanced computing infrastructure to support u.s. science and engineering in...copyright national academy of sciences. all rights reserved.background 39the last major component of highend computers is the internode interconnect; that is, the network that is used to move data between compute elements or between centralized data storage and the compute system. although the performance of these interconnects has also increased signicantly, with bandwidths for proprietary networks used in hpc systems of 4080 gb/s per link being typical, the latencies have not improved much in recent years, with highperformance interconnects having latencies on the order of 1 microsecond. commodity interconnects are one to two orders of magnitude slower, with link speeds of 1 gb/s being common, and with 10 gb/s available at the high end of commodity interconnects.the manner in which the links are connected is also important. there are three separate but related decisions. one is the topology of the connections. highend supercomputers link nodes directly together in an ndimensional torus. for example, the ibm bluegene/q uses a vedimensional (5d) torus; the cray gemini network uses a threedimensional (3d) torus, with two compute nodes connected to each torus node. a second is the switch radixšhow many ports each switch has. a third is whether the network uses switch notes that are distinct from processor nodes. recently, interconnect design principles from hpc, such as more highly connected networks with better bisection bandwidth and latencies, have been adopted for commercial applications.7 also of importance is widearea networking, which is critical to the success of nsf™s advanced computing, especially in terms of providing access and the infrastructure necessary to bring together data sources and computing resources. the size of some data sets is forcing some data offline or onto remote storage, so storage hierarchies, storage architectures, and wan (wide area network) architectures are increasingly important to overall infrastructure design. nsf has made signicant investments in widearea networking. the internet2 network plays an important role in connecting researchers. it carries multiple petabytes of research data and also connects researchers globally with peering to more than 100 international research and education networks. widearea networks have a distinct set of technical, managerial, and social complexities that are beyond the scope of this report. 7 see, for example, a. singh, j. ong, a. agarwal, g. anderson, a. armistead, r. bannon, et al., ﬁjupiter rising: a decade of clos topologies and centralized control in google™s datacenter network,ﬂ presented at the association for computing machinery special interest group on data communication (sigcomm), 2015, http://conferences.sigcomm.org/sigcomm/2015/pdf/papers/p183.pdf.future directions for nsf advanced computing infrastructure to support u.s. science and engineering in...copyright national academy of sciences. all rights reserved.40 future directions for nsf advanced computing infrastructure2.4.2 softwarealthough computer hardware will to continue to improve, the rate of improvement has been slowing down and producing increasingly disruptive programming features. software for scientic simulations for parallel systems with more than a handful of processing cores has largely been written in a message passing model (e.g., with mpi) using domain decomposition, where the physical domain or other major data structures are divided in pieces assigned to each processor. this works especially well for problems that can be decomposed statically and where communication between processes is predictable, involving a limited number of neighbors along with global operations. the assumptions underlying this model are that (1) locality is critical to scaling, so the application programmer needs to do the data decomposition, (2) the network and processors are reliable, and (3) the performance is roughly uniform across the machine. at the same time, many of the data analysis workloads processed on cloud computing platforms have used a mapreduce style in which independent tasks are spread across nodes and results are aggregated using global communication operations at intermediate points. this model allows for hardware heterogeneity or variablespeed processors, but does not permit pointtocommunication between tasks. both models have proven powerful in their own setting. the relative stability until recently of the hardware platforms has allowed a rich set of libraries and frameworks for simulation to emerge, many supported by nsf (box 2.3). this includes libraries for sparse and dense linear algebra, spectral transforms, and application frameworks for  highperformance computing (hpc) systems have grown enormously in the past have existed for many years. large community codes for modeling problems in materials and climate, for example, have many different models to simulate different conditions, options for algorithm choices, and multiple implementations for libraries and, in many cases, involve multiple languages mixed together. they may use fortran for numerical kernels, c++ for complex data structures, and python libraries that are themselves written in other languages. parallelism is typically expressed using message passing, typically mpi, possibly with threading used for onfuture directions for nsf advanced computing infrastructure to support u.s. science and engineering in...copyright national academy of sciences. all rights reserved.background 41node parallelism. but applications at a large center may take advantage of many different languages, libraries, and programming abstractions as well as tools to help with debugging, performance analysis, data management, and visualization. using hpc. tables 2.3.1 and 2.3.2 show the usage of some of the most popular users and weighted by the number of hours each project uses. these are based on data reported in categories chosen by those responding to the survey. as a result, there is some overlap in categories, and some used a general category category (e.g., upc, a pgas language). these should be used only (1) to see the breadth of libraries, languages, and systems and (2) as a very rough guide to the amount of use of each item.table 2.3.2  programming systems used at one centertierprogramming system1st mpi, fortran, c++, openmp, c2nd shellscript, python3rd posix threads, tcl/tk, java, perl, assembler, charm++, opencl, idl, pgas, shmem4th gasnet, matlab, upc, global arrays, coarray fortran, lua, ruby, upc++, cuda, openclnote: systems are grouped by usage in terms of the number of hours used by the projects that listed the programming system. the tiers are subjective but represent rough clusters of tierlibrary1stlapack, fftw, scalapack, petsc, ncar, hypre, superlu, mumps, chombo, trilinos, root2ndmetis, boost, cernlib, blas, slepc, boxlib, pspline, gsl, chroma, qdp++, mkl, parpack, scorec, gotoblas, fftpacknote: libraries are grouped by usage in terms of number of compute hours used by the projects that listed the library. the tiers are subjective but represent, roughly, clusters of usage. libraries in each tier are used by roughly 10 times the number of compute hours as those in the next tier (measured by the total time used by the project, not necessarily the library). sudip dosanjh, nersc director, personal communication.future directions for nsf advanced computing infrastructure to support u.s. science and engineering in...copyright national academy of sciences. all rights reserved.42 future directions for nsf advanced computing infrastructureclimate modeling, astronomy, ˚uid dynamics, mechanical modeling, and many more. to manage the overall power consumption of larger future systems, it will not be viable to carry out larger computations simply by scheduling threads on more cores. the processors themselves will need to become more energy efcient. as a result, scientic software will need to be revised to take advantage of powerconserving processor features like softwaremanaged memory, wider serial instructions, and multiple data architectures. scientic libraries face these challenges but are also a point of leverage, allowing multiple applications to benet from optimizations to new architectures. looking ahead, substantial investments in software will also be required to take advantage of future hardware, as will research to address new models of concurrency and correctness concerns.the virtual machine abstractions in the commercial cloud have enabled a different class of applications, with complex work˚ows for data analysis built and distributed as an integrated software stack. these are particularly popular in biology and particle physics. 2.4.3 algorithmsthe situation is even more complicated for algorithms, where improvements in algorithmic complexity are harder to predict. not all of the improvements fall into a general category, but some of the common approaches include hierarchical algorithms, exploiting sparseness or symmetry, and reducing data movement. in simulation problems, both the mathematical models of a given physical system and the algorithms to solve them may be specialized to a problem domain, allowing for more efcient computations. the same is true for data analysis, where some preexisting knowledge of the data may permit faster analysis techniques. machine characteristics may also affect the choice of algorithms, as the relative costs of computation, data movement, and data storage continue to change across generations, along with the types and degrees of hardware parallelism. minimizing the total work performed is generally a desirable metric, but on machines with very fast processing and limited bandwidth, recomputation or other seemingly expensive computations may pay off if data movement is reduced, and memory size limits can make some algorithms impractical. future algorithmic innovations will still be essential for addressing more complex simulation problemsšfor example, modeling problems with enormous ranges of time or space scale, or problems that combine multiple physical models into a single computation. they will also be needed for new problems in datadriven science, such as enabling multimodal analysis across disparate types of data, interpreting data with a low signaltonoise ratio, and handling enormous data sets where only samples of the data may be analyzed. new algorithms will future directions for nsf advanced computing infrastructure to support u.s. science and engineering in...copyright national academy of sciences. all rights reserved.background 43also be needed to take advantage of future hardware with its new forms of parallelism and different cost metrics, including algorithms that can detect or tolerate various types of errors. finally, scientic discovery at the boundary of simulation and observation will require new algorithms to measure uncertainties, adjust models dynamically to t observed data, and interpret data that are incomplete or biased.although research into algorithms will continue to have large payoffs in some domains, it does not replace the need for increasingly capable machines. algorithmic improvements have historically gone handinhand with hardware improvements, provided that the algorithmic advances can be effectively implemented on the advanced hardware. machine learning algorithms based on neural networks, for example, are only effective because of the performance of modern hardware, and the massive highthroughput computations of the materials genome initiative would not be possible on the hardware available two decades ago. so while hardware performance gains will be increasingly difcult in the future, substantial algorithmic improvements for some problems are probably impossible. for these problems, decades of work on algorithms have led to optimal solutions, and further improvements must come from hardware and operating system software (box 2.4).  the rate of improvement in hardware performance, whether measured by clock rate or even by concurrency, has been slowing down. although the situation is much more complicated for algorithms, there are cases where yeartoyear one example that is often used to demonstrate the essential contribution of algorithms is the solution of the large, linear systems of equations that arise when approximating the solution to a threedimensional partial differential equation on a grid of size nbynbyn. figure 2.4.1 is a typical example. it shows that the improvement in performance for this problem is comparable to the improvement 1 in other words, for this particular problem with a size of n = 64, using the most modern algorithm on a 35yearold computer system would be as effective (by this simple measure) as running the 35yearold algorithm on a stateoftheart system. this is true, and it emphasizes the tremendous advancements in numerical algorithms. however, note that the most modern algorithm, full there is no longer much room for improvement. full multigrid is an optimal algorithm for this problem at any size; a size of n = 1,000 (i.e., a matrix with a billion continuedfuture directions for nsf advanced computing infrastructure to support u.s. science and engineering in...copyright national academy of sciences. all rights reserved.44 future directions for nsf advanced computing infrastructurerows) is easily handled today. any further improvements in performance can come only from faster hardware or by the relatively small reductions in the constant term in the time complexity of the algorithm.this example is not meant to say that all linear systems can now be solved in optimal time; it applies only to one wellstudied and relatively simple problem. optimal algorithms for solving other types of systems of linear equations have yet to be found, and seeking such algorithms remains an active and important area of research. and for particular problems, alternative formulations may provide a route to a solution without needing to solve this particular linear system of equations. but this example does point out that there is a limit to the use of better algorithmsšin some cases, there is no alternative but to run the current optimal algorithm on faster hardware.1a sciencebased case for largescale simulation, washington, d.c., 2003, p. 32. top: a table of the scaling of the memory and processing requirements for the solution of the electrostatic potential equation on a uniform cubic grid of n × n × n cells for n = 64. bottom:of science, u.s. department of energy, a sciencebased case for largescale simulation, volume 1, washington, d.c., 2003, p. 32.future directions for nsf advanced computing infrastructure to support u.s. science and engineering in...copyright national academy of sciences. all rights reserved.background 452.5 nsf investments in advanced computing since the beginning of nsf™s supercomputing centers program in the 1980s, its division of advanced cyberinfrastructure (aci) and its predecessor organizations have supported computational research across nsf with both supercomputers and other highperformance computers and provided services to a user base that spans work sponsored by all federal research agencies. although a large fraction of the leadershipclass investments have been driven by the missioncritical requirements of doe and dod, nsf has played a pivotal role in moving forward the state of the art in hpc software and systems.aci supports and coordinates a range of activities to develop, acquire, and provision advanced computing and other cyberinfrastructure for science and engineering research together with research and education programs. a signicant fraction of aci™s investments have been for two tiers of advanced computing hardware; a petascale computing system, blue waters, deployed in 2013 at the university of illinois, and a distributed set of systems deployed under the extreme digital program and integrated by the extreme science and engineering discovery environment (xsede). xsede makes eight compute systems located at six sites available to researchers along with a distributed open science grid and visualization, storage, and management services. resource allocations for both tiers are made through competitive processes managed by the petascale computing resource allocations committee (prac) and the xsede resource allocation committee (xrac), respectively. as things stand currently, roughly half of all available computing capacity will shut down in 2018 with the anticipated endoflife decommissioning of blue waters.  one of the major contributions of nsf to computational science has been the development of software: application codes, libraries, and tools. nsf™s implementation of the cyberinfrastructure framework for 21st century science and engineering vision8 identies three classes of software investments: software elements (targeting small groups seeking to advance one or more areas of science), software frameworks (targeting larger, interdisciplinary groups seeking to develop software infrastructure to address common research problems), and software institutes (to establish longterm hubs serving larger or broader research areas). investments at the larger/broader end are supported under the crossfoundation software infrastructure for sustained innovation program, while those at 8 national science foundation, ﬁimplementation of nsf cif21 software vision,ﬂ http://www.nsf.gov/funding/pgmsumm.jsp?pimsid=504817, accessed january 27, 2016.future directions for nsf advanced computing infrastructure to support u.s. science and engineering in...copyright national academy of sciences. all rights reserved.46 future directions for nsf advanced computing infrastructurethe smaller/narrower end are supported by the relevant science and engineering divisions.9not included in the aci portfolio are investments in computer science research infrastructure, such as the geni (global environment for network innovations) testbed. such resources are important research resources but belong more properly to the specic research program within nsf. also not included is basic research into algorithms and software, which while also vital, is supported by other research programs in nsf (both within computer and information science and engineering [cise] and the other science divisions) and at other federal agencies such as doe.trends in the overall investment in advanced computing can be seen by looking at the spending amounts reported by federal agencies to the networking and information technology research and development program™s national coordination ofce. figure 2.1 shows the total federal investment in all categories tracked by networking and information technology research and development (nitrd) including highend computing infrastructure and applications (hecia), a category that shows both longterm growth over the period 20002015 as well as a signicant falloff from a mid2000s investment spike. note that advanced computing systems have a relatively short useful lifetime. however, nsf™s investments in hecia have fallen off from nearly 40 percent to less than 20 percent of the total (figure 2.2ab), even as demand has grown.2.6 demand for and use of nsf advanced computing resourcesthe use of advanced computing resources cuts across research funded by all the divisions of nsf, as shown in figure 2.3. data obtained from xsede indicate that the number of active users has quintupled over the past 8 years, and the use10 grew exponentially through about 2009. use increases less rapidly after that, matching the slower growth in available resources (cf. figure 2.5). the usage patterns over the years indicate signicant usage by all of the nsf directorates, including mathematical and physical sciences, biological sciences, geosciences, engineering, cise, and social, behavioral and economic sciences. notably, use by the direc9 national science foundation, ﬁimplementation of nsf cif21 software vision,ﬂ http://www.nsf.gov/funding/pgmsumm.jsp?pimsid=504817, accessed january 27, 2016.10 xsede use is measured in service units (sus), which are dened locally for each xsede machine and normalized across machines based on highperformance linpack benchmark results. sus do not account for other relevant system parameters such as memory or storage use. also, a large fraction of available sus in the current xsede resources comes from coprocessors that can be used only after signicant changes to software and, sometimes, to algorithms as well. future directions for nsf advanced computing infrastructure to support u.s. science and engineering in...copyright national academy of sciences. all rights reserved.background 47torate for social, behavioral and economic sciences is continuing to grow exponentially and by 2014 exceeded the use by mathematics and physical sciences in 2005, showing the broad growth in the use of computing across the foundation. further, for such infrastructure as xsede, nsf supports a signicant fraction of nonnsf funded users. with xsede, the usage patterns indicate that for large allocations (e.g., over 10 million service units) approximately 47 percent of the allocations are for nonnsf funded users (figure 2.4). that share includes 14 percent in support of research funded by the national institutes of health. although it is difcult to know exactly how much advanced computing is required by the nation™s researchers, one available metric is the amount of computer time requested on the xsede resources. there is a growing gap between the amount requested, which continues to grow exponentially, and the amount available (figure 2.5). the implication is figure 2.1 total federal investment ($ millions) in the networking and information technology research and development program categories. note: csia, cyber security and information assurance; hciim, human computer interaction and information management; hcss, high condence software and systems; hecia, highend computing infrastructure and applications; hecrd, highend computing research and development; lsn, large scale networking; sdp, software design and productivity; sew, social, economic, and workforce implications of it and it workforce development. source: compiled from data provided in annual supplements to the president™s budget request, prepared by the national coordination ofce for the networking and information technology research and development program, https://www.nitrd.gov/publications/supplementsall.aspx. future directions for nsf advanced computing infrastructure to support u.s. science and engineering in...copyright national academy of sciences. all rights reserved.48 future directions for nsf advanced computing infrastructurefigure 2.2 national science foundation investment by networking and information technology research and development category from 2000 to 2016 as (a) a percent of total and (b) in millions of dollars. note: csia, cyber security and information assurance; hciim, human computer interaction and information management; hcss, high condence software and systems; hecia, highend computing infrastructure and applications; hecrd, high end computing research and development; lsn, large scale networking; sdp, software design and productivity; sew, social, economic, and workforce implications of it and it workforce development. source: compiled from data provided in annual supplements to the president™s budget request, prepared by the national coordination ofce for the networking and information technology research and development program, https://www.nitrd.gov/publications/supplementsall.aspx. future directions for nsf advanced computing infrastructure to support u.s. science and engineering in...copyright national academy of sciences. all rights reserved.background 491.e+031.e+041.e+051.e+061.e+071.e+081.e+091.e+102006 q12007 q12008 q12009 q12010 q12011 q12012 q12013 q12014 q1xd sus chargedbiological sciencescomputer and information science and engineeringengineeringgeosciencesmathematical and physical sciencesothersocial, behavioral, and economic sciencesfigure 2.3 use of xsede resources by nsf directorate funding research grantee, 20062014. note: nsf, national science foundation; su, service unit; xsede, extreme science and engineering discovery environment. source: derived from data obtained by querying open xdmod database, university at buffalo. nsf 2113m46%nih 672m15%doe 503m11%none 333m7%nasa 312m7%dod 252m6%other 154m3%all others 226m5%figure 2.4 estimated use in xsede service units of nsf advanced computing by grantees of other federal agencies, based on allocations of xsede resources over calendar year 2014. note: nsf, national science foundation; xsede, extreme science and engineering discovery environment. source: derived from data obtained by querying open xdmod database, university at buffalo (j.t. palmer, s.m. gallo, t.r. furlani, m.d. jones, r.l. deleon, j.p. white, n. simakov, et al., open xdmod: a tool for the comprehensive management of highperformance computing resources, computing in science and engineering 17.4(2015):5262, 2015).future directions for nsf advanced computing infrastructure to support u.s. science and engineering in...copyright national academy of sciences. all rights reserved.50 future directions for nsf advanced computing infrastructurethat insufcient computing resources inhibits the effective execution and constrains the scale of accomplishment of already funded nsf science. 2.7 national strategic computing initiativeas this study was being completed, an executive order11 was issued establishing a national strategic computing initiative. section 3a of the order designates nsf as one of the three lead agencies for the initiative and calls for nsf to ﬁplay a central role in scientic discovery advances, the broader hpc ecosystem for scientic discovery, and workforce development.ﬂ box 2.5 compares items in the executive order with the major themes of this report.11 executive ofce of the president, ﬁexecutive orderšcreating a national strategic computing initiative,ﬂ july 29, 2015, https://www.whitehouse.gov/thepressofce/2015/07/29/executiveordercreatingnationalstrategiccomputinginitiative.figure 2.5 requested xsede resources compared to awarded and available resources, illustrating the gap as well as growing divergence between available and requested resources. note: xsede, extreme science and engineering discovery environment. source: data from open xdmod, university at buffalo (j.t. palmer, s.m. gallo, t.r. furlani, m.d. jones, r.l. deleon, j.p. white, n. simakov, et al., open xdmod: a tool for the comprehensive management of highperformance computing resources, computing in science and engineering 17.4(2015):5262, 2015). custom query by robert l. deleon. future directions for nsf advanced computing infrastructure to support u.s. science and engineering in...copyright national academy of sciences. all rights reserved.background 51box 2.5 1the following are themes in this report as well as the executive order establishing a national strategic computing initiative (nsci):1. highperformance computing (hpc) remains critical for science and industry; if anything, the need and value continue to grow. (nsci section 1)2. ﬁincreasing coherence between the technology base used for modeling and simulation and that used for data analytic computing.ﬂ (nsci section 2.2)3. building on its successes in cyberinfrastructure, the national science foundation (nsf) has an important role to play both in providing hpc (including data and compute) for basic science and in development of the science needed to advance hpc, including the algorithms, software, and hardware for extreme scale computing. (nsci section 3a)4. nsf must also contribute to the development of an hpc workforce. (nsci section 3a)5. publicprivate partnerships should be explored. (nsci section 1.2)6. hpc research must be transitioned into practice. (nsci section 1.4) this to perform research in support of hpc and to support bringing that research into practice as needed by the nsf user community.7. embrace an integrated approach to providing effective hpc, combining hardware, software, and algorithms, as well as address the development of an hpccapable workforce and the whole of hpc, including the midrange as well as the high end. (nsci section 2.4)order:1. although convergence of dataintensive and computeintensive systems is important and will address many needs, some applications require more specialized approaches that may emphasize compute or data. (nsci section 2.2 focuses on convergence)2. the demand for computing continues to outstrip supply; more needs to be done to (a) provide greater resources (especially systems and expertise in using them) and (b) make the best use of these resources (nsci makes no statecoordination). 3. a diversity of platforms and software will be needed to capture the long tail of science. nsci calls for acceleration of the deployment of an exascale class system but says nothing about the acceleration needed for future science needs at all scales. 1executiveordercreatingnationalstrategiccomputinginitiative.future directions for nsf advanced computing infrastructure to support u.s. science and engineering in...copyright national academy of sciences. all rights reserved.3maintaining science leadershipadvanced computing underpins virtually every discipline of science and engineering and is critical to the national science foundation™s (nsf™s) mission ﬁto promote the progress of science; to advance the national health, prosperity, and welfare; to secure the national defense; and for other purposes.ﬂ1 the use of advanced computing enables discoveries in fundamental areas of physical sciences; provides new insights in biological sciences that have implications for national health; leads to improved engineering of devices; enables the development of new materials and systems with both commercial and defense implications; and aids in our understanding of the environment and society. advanced computing has traditionally been used for modeling and simulation to interpret and project the implications of mathematical models of physical phenomena and, increasingly, to analyze the large and complex data sets from observations and experiments. the impacts on science have been both broad and deep. advanced computing supports the education and research of thousands of students and scientists across the country, and it has been essential in some of the most signicant awardwinning scientic discoveries. advanced computing has been used for scientic discoveries across many disciplines, from cosmology and astrophysics to biology and medicine. for example, advanced computing at nsf has been used to understand the forma1 national science foundation, ﬁat a glance,ﬂ http://www.nsf.gov/about/glance.jsp, accessed march 31, 2016.52future directions for nsf advanced computing infrastructure to support u.s. science and engineering in...copyright national academy of sciences. all rights reserved.maintaining science leadership 53tion of the rst galaxies in the early universe, to analyze the impacts of cloudaerosolradiation on regional climate change, and to understand the design and behavior of computing device technology as the end of moore™s law scaling approaches. the use of advanced computing systems, including those designed for dataintensive workloads, has expanded beyond traditional domains to understanding social phenomena captured in realtime video streams, connection properties of social networks, and voter redistricting schemes. other examples of science impacts can be found in box 3.1.advanced computing has been a key to multiple nobel prizes (box 3.2), including the 2013 nobel prize in chemistry awarded jointly to martin karplus, michael levitt, and arieh warshel for ﬁthe development of multiscale models for complex chemical systems.ﬂ the team used nsf teragrid resources for particle simulations to predict the structure of proteins and combine molecular dynamics with quantum mechanical calculations.3.1 critical role of nsfnsf plays a critical role in providing the advanced computational infrastructure, including advanced computing, necessary to keep the united states at the forefront in the areas of science and engineering. according to the networking and information technology research and development (nitrd) reports2 on investments in highend computing infrastructure and applications (hecia), nsf ranked second, behind the department of energy (doe), for 2015 in investments in highend computing facilities. doe invested more than $350 million, while nsf was just over $200 million. with respect to investments in the nitrd highend computing research and development program area, nsf is currently very close to doe, with the department of defense (dod)3 leading with more than $200 million and nsf and doe in the range of $125 million.4 however, nsf investment in hecia has declined signicantly 2 see scal year 2000 through scal year 2016 editions of networking and information technology research and development national coordination ofce. the networking and information technology research and development program: supplement to the president™s budget. 3 dod includes ofce of the secretary of defense, national security agency, and the dod service research organizations.4 note that the interpretation of the nitrd budget numbers is difcult and not consistent across agencies, as has been observed in president™s information technology advisory committee reports (2010) and that doe has been investing in extreme scale research both through its application communities with scientic discovery through advanced computing (scidac) and codesign projects and through research evaluation prototypes, some of which may not have been included in the hecrd (high end computing research and future directions for nsf advanced computing infrastructure to support u.s. science and engineering in...copyright national academy of sciences. all rights reserved.54 future directions for nsf advanced computing infrastructure the southern california earthquake center (scec) and its lead scientist, thomas jordan, use nsf advanced computing resources to improve our understanding of ect is creating more physically realistic, wavebased earthquake simulations using an earthquake model that calculates how earthquake waves ripple through a threedimensional (3d) model of the ground. given detailed information about the geological to calculate how earthquake waves will move through the earth and how strong the ground motions will be when the waves reach the surface. in 2014, the scec team north american plates run into each other at the san andreas fault. in this study, the simulation showed earthquake waves trapped, and reverberating, within the los angeles basin, leading to highshaking ground motions much greater than expected. in 2015, scec used the nsffunded blue waters supercomputer at the national center for supercomputing applications and the department of energyfunded titan supercomputers at the oak ridge leadership computing facility to carry out a simutherefore also doubling the accuracy. even though the number of calculations required increased as the maximum simulated frequency of the earthquake went up, the computing power of blue waters and titan reduced the time needed for these calculations from months to weeks. researchers believe seismic hazard analyses need to simulate earthquake frequencies above 10 hertz to realistically capture the full dynamics of a potential event. physicsbased 3d earthquake simulations at 10 hertz, once a distant dream, are now on the horizon.source: adapted from nsf, ﬁlos angeles basin jiggles like big bowl of jelly in cuttingedge simulations,ﬂ august 20, 2015, http://nsf.gov/discoveries/discsumm.jsp?cntnid=136013.the hiv capsid project, headed by klaus schulten of the university of illinois at capsid and simulated it on blue waters, representing steps toward a better understanding of the interactions of potential drugs and host cell factors with the capsid. the project expanded the frontier of molecular dynamics simulation capabilities from simulating just a few proteins to simulating full organelles. it involved simulations of about 65 million atoms using thousands of nodes on blue waters, and required several years to redesign computer codes to make them more scalable to petascale systems. going from simulating organelles to full cells with 100 billion atoms will require much faster computers. source: adapted from national center for supercomputing applications, ﬁthe computational microscope,ﬂ blue waters highlights, https://bluewaters.ncsa.illinois.edu/future directions for nsf advanced computing infrastructure to support u.s. science and engineering in...copyright national academy of sciences. all rights reserved.maintaining science leadership 55 the avian phylogenomics consortium project published eight papers in science and 20 papers in other journals on its work reconstructing the evolutionary history of birds. the research involved processing hundreds of times more genetic data per species than previous studies. the size of the data sets and the complexity of the analysis required multiple xsede resources: ranger, lonestar, and stampede at the texas advanced computing center (tacc); nautilus at the national institute of computational sciences; and gordon at the san diego supercomputer center. tacc resources as well as a cluster at the university of texas were used to test and validate new computational techniques; nautilus was used to generate phylogenetic trees at the chromosome level for all the bird genomes; and the gordon cluster was used to infer phylogenetic trees at the genome level. the analysis allowed the researchers to realize the existence of new interavian relationships, redrawing the family ﬁtreeﬂ for nearly all of the 10,000 species of birds alive today. source: adapted from extreme science and engineering discovery environment (xsede), xsedehighlights2015.pdf, p. 14.nent. frequent observations of coalescing binaries of two neutron stars, a neutron star and a black hole, and two stellarmass black holes are expected once the detectors  the coalescence and merger of two black holes (binary black holes; bbhs) is the tational waves and no other kind of radiation, observation of merging bbhs makes it possible to probe the predictions of general relativity. numerical relativity simulations that implement general relativity without approximation (other than numerical truncation error) are essential for enabling discovery and testing of general relativity with aligo. codes with exponential convergence (highly supercomputers to generate waveform predictions (ﬁtemplatesﬂ) for aligo. the tremendous challenge is that the bbh parameter space is ninedimensional: mass ratio of argument of periapsis. thousands of numerical relativity simulations, each tracking the holes over many orbits before merger, are needed. these thousands of waveforms will populate carefully chosen discrete nodes that will allow accurate interpolation throughout the continuous parameter space. although a single numerical relativity bbh simulation runs on about 48 cores within 3 months, running thousands is a massive challenge at the interface of highthroughput sweep (hundreds of simulations; only about 40 premerger orbits) through the bbh parameter space with hundreds of simulations. a nextgeneration machine and improved algorithms will be needed to carry out the thousands of very long coalescence simulations that aligo needs for discovery and for testing general relativity.continuedfuture directions for nsf advanced computing infrastructure to support u.s. science and engineering in...copyright national academy of sciences. all rights reserved.56 future directions for nsf advanced computing infrastructuresource: adapted from christian d. ott, email message to bill gropp, september 11, 2015. there is widespread consensus that the modern human lineage evolved in africa, and it has been hypothesized that the cape region of south africa may have been the refuge region for the progenitor lineage of all modern humans during harsh global glacial phases. during this phase of human origins, the economy was based on hunting and gathering, and huntergatherer adaptations are tied to the way that climate and environment shape the food and technological resource base. curtis marean, arizona state university, leads a multinational, multidisciplinary team of researchers in a pioneering application of highperformance computing to the study of these interactions. ﬁour project began as a straight archaeological dig,ﬂ marean says. ﬁthen i realized that we needed much better climate and environmental contextual data to understand the archaeological record we were excavating.ﬂ marean began using mizing atmospheric physics codes for hpc systems, and campus champion fellow eric shook (kent state university) helped port the variableresolution global climate model ccam [commonwealth center for advanced manufacturing] to blacklight and adapt the code to allow veryhighresolution paleoclimate simulations over the cape marean observed that ﬁpeople were totally blown away, and it was so exciting to see something that has never been accomplished beforešthe production of glacial climate from a regional climate model,ﬂ providing a foundation for further study of the climate experienced by early humans. source: adapted from ralph roskies, pittsburgh supercomputing center, email message to robert harrison, october 21, 2015.as a percentage of its total investments in nitrd research and in total dollar amount (figure 2.2), even as the gap between request and available computing resources has grown (figure 2.5).nsf has a critical role with advanced computing because of its mission to initiate and support ﬁbasic scientic research and research fundadevelopment) category. however, the point here is that the levels of investment are roughly comparable. doe also supports industry research in processor and memory design, interconnects, and programming environments through the research and evaluation prototypes program.future directions for nsf advanced computing infrastructure to support u.s. science and engineering in...copyright national academy of sciences. all rights reserved.maintaining science leadership 57mental to the engineering process.ﬂ5 nsf™s division of advanced cyberinfrastructure (aci) and its predecessor organizations have supported computational research across nsf and provided services to a user base that spans work sponsored by all federal research agencies. while a large fraction of the leadershipclass investments have been driven by the missioncritical requirements of doe and dod, nsf has played a pivotal role in moving forward both the state of the art in hpc software and systems and the scale and scope of impacts that are enabled through their use to address key scientic challenges. this is in complement to other agencies such as doe, the national institutes of health, dod, and the defense advanced research projects agency, which are mission driven. currently, nsfsupported advanced computing investment includes a diversity of resources. nsf supports several largescale hardware facilities, together with associated staff expertise (blue waters at the university of illinois, urbanachampaign, stampede at the university of texas, austin, and yellowstone at the national center for atmospheric researchwyoming), longtail and highthroughput resources (comet at the university of california, san diego), dataintensive resources (wrangler at university of texas, austin, and bridges at the pittsburgh supercomputing center), and cloud resources (jetstream at indiana university). in february 2012, nsf published cyberinfrastructure for 21st century science and engineering: advanced computing infrastructure vision and strategic plan.6 the document addressed broadly the cyberinfrastructure needed by science, engineering, and education communities to address complex problems and issues. the cyberinfrastructure framework for 21st century science and engineering (cif21) strategic plan seeks to position and support the entire spectrum of nsffunded communities at the cutting edge of advanced computing technologies, hardware, and software. the cif21 vision is to position nsf to ﬁbe a leader in creating and deploying a comprehensive portfolio of advanced computing infrastructure, programs, and other resources to facilitate cuttingedge foundational research in computational and dataenabled science and engineering (cds&e) and their application to all disciplines.ﬂ7 in addition, the vision calls for nsf to ﬁbuild on its leadership role to promote human capital development and education in cds&e to benet all elds of science and engineering.ﬂ8 after the publication of the strategic plan, 5 national science foundation act of 1950, as amended, and related legislation, 42 u.s.c. 1861 et seq.6 national science foundation (nsf), cyberinfrastructure for 21st century science and engineering: advanced computing infrastructure vision and strategic plan, nsf 12051, http://www.nsf.gov/pubs/2012/nsf12051/nsf12051.pdf, february 2012.7 ibid., p. 4.8 ibid., p. 4.future directions for nsf advanced computing infrastructure to support u.s. science and engineering in...copyright national academy of sciences. all rights reserved.58 future directions for nsf advanced computing infrastructure  california, santa barbara) for his ﬁdevelopment of the density functional theory,ﬂ and john pople (northwestern university) for his ﬁdevelopment of computational methods in quantum chemistry.ﬂ density functional theory is the workhorse of computational chemistry and materials science and is now, perhaps, the central predictive computational tool of the multiagency materials genome initiative. software, pople put powerful simulation tools into the hands of both theoreticians and experimentalists and ushered in the modern era of computational chemistry.in 2013, the nobel prize for chemistry was awarded to martin karplus (university of strasbourg and harvard university), michael levitt (stanford university), and arieh warshel (university of southern california) for ﬁthe development of multiscale models for complex chemical systems.ﬂ their theoretical innovations synthesized classical and quantum models, and by realizing these advances in powerful computer programs they enabled the predictive modeling of chemical reactions in complex systems relevant to combustion, drug design, and biological systems. levitt remarked,1neered with martin karplus and arieh warshel, has certainly grown and matured through access to nsffunded programs like xsede. . . . our 2013 nobel prize in chemistry represents a huge step forward in the perception that highperformance being purely experimental. the importance of xsede lies in its ability to work across many disciplines with a broad spectrum of users extending from novices to the most experienced users and all this at no cost of the researcher.ﬂnsf formed a foundationwide committee with representatives from all directorates to move nsf in the direction of achieving the advanced computing infrastructure vision. 3.2 global issues advanced computing is arguably the most critical ingredient of international science leadership, affecting leadership in other disciplines, providing an essential and often unique role in international experiments, and literally tying international collaborations together through networking, data, and computing infrastructure. nearly every developed country has some type of national program for computing because of its importance to economic growth, science, defense, and society. future directions for nsf advanced computing infrastructure to support u.s. science and engineering in...copyright national academy of sciences. all rights reserved.maintaining science leadership 59in 2011, saul perlmutter (lawrence berkeley national laboratory), brian schmidt (australian national university), and adam riess (johns hopkins university) shared the nobel prize in physics for ﬁthe discovery of the accelerating expansion of the universe through observations of distant supernovae,ﬂ2 the unknown cause of which is termed ﬁdark energy.ﬂ their search for type ia supernovae by an extended survey of thousands of galaxies was enabled by advanced image processing techniques and fast computers.3in this work, perlmutter remarked that ﬁwe need more supercomputers to narrow down the history of expansion because there are subtle differences in the histories lections of thousands of images where each image is many megapixels, or even gigapixels collected by the big mosaic cameras. and then, to analyze the data and compare your results to different cosmological models also requires large computers, as do the monte carlo and all the statistical models you need. finally, 41 national science foundation, ﬁcomputational science takes the nobel stage,ﬂ february 11, 2014, http://www.nsf.gov/discoveries/discsumm.jsp?cntnid=130427.2 royal swedish academy of sciences, ﬁnobel prize in physics 2011,ﬂ press release, october 4, 2011. 3 c. day, ﬁnobel prizes for computational science,ﬂ computing in science and engineering 14(6):88, 2012. 4 r. brueckner, ﬁinterview: universe has some surprises in store, says nobel laureate saul perlmutter,ﬂ november 26, 2013, http://insidehpc.com/2013/11/universesurprisesstorenobellaureatesaulperlmutter/.leadership is difcult to quantify by a single, good benchmark or by analysis of a single system. the top500 list9 ranks computers globally by their performance on the highperformance linpack benchmark. it is sometimes criticized for using this measure, which is much more computeintensive than most modeling and simulation applications and does not re˚ect dataintensive workloads. moreover, it does not contain all advanced computing systems, either because the system is business condential, classied, or, as in the case of the blue waters system, because owners did not wish to take time away from normal operations to run the benchmark. nevertheless, the list is an excellent source of historical data, and taken in the aggregate gives insights into investments in advanced computing internationally. the united states continues to dominate the 9 see the top500 website at http://top500.org, accessed january 27, 2016.future directions for nsf advanced computing infrastructure to support u.s. science and engineering in...copyright national academy of sciences. all rights reserved.60 future directions for nsf advanced computing infrastructurelist, with 45 percent of the aggregate performance across all machines on the july 2015 list, but it has dropped substantially from a peak of over 65 percent in 2008. nsf has had systems either high on the list (e.g., kraken, stampede) or comparable to the top systems (i.e., blue waters), re˚ecting the importance of computing at this level to nsfsupported science. although there are ˚uctuations across other countries, the loss in performance share across this period is mostly explained by the growth in asia, with china™s share growing from 1 percent to nearly 14 percent today and japan growing from 3 to 9 percent. the association for computing machinery™s gordon bell prize may be a better metric of scientic talent and usable performance; it is awarded annually to teams who demonstrate the best performance on a real application. of the 26 awards to date, 20 were awarded to u.s. teams using u.s. systems, some involving participants from other countries. the other 6 were from japanese teams on the earth simulator system in the early 2000s and the k computer in 2013, both customdesigned systems. although nsf systems and staff were involved in some awards, doe laboratory staff and systems have largely dominated the u.s. awards, and the most recent award was to a commercial entity and custom system (d.e. shaw research™s anton 2). china™s tianhe2 supercomputer stands at the top of the top500 list. barely visible in highperformance computing 15 years ago,10 china™s presence on the list has continued to grow. china had, however, not announced at the time this report was being prepared its new 5year plan for highperformance computing, so it is difcult to be precise about its future plans. japan has a long history of strong support for advanced computing in support of both science and industrial competitiveness. by several measures, japan has often deployed and operated the world™s fastest machine for science, most recently with the earth simulator (#1 on the top500 list from 20022004) and the k computer (#1 on the top500 list in 2011). japan has plans for both a powerful, leadershipclass system, called the flagship 2020 project, and a roadmap for nine powerful systems at university centers. the flagship 2020 project is expected to provide roughly 1 exaflop/s (1018 ˚oatingpoint operations per second), although the focus is on a leadershipclass system for science rather than a particular peak performance target. this is likely to be one of the most powerful systems in the world when it becomes operational and a pow10 j. alspector, a. brenner, r.f. leheny, and j.n. richmann, ﬁchinaša new power in supercomputing hardware,ﬂ institute for defense analysis, march 27, 2013, https://www.ida.org/~/media/corporate/files/publications/idadocuments/itsd/idadocumentnsd4857.ashx.future directions for nsf advanced computing infrastructure to support u.s. science and engineering in...copyright national academy of sciences. all rights reserved.maintaining science leadership 61erful advantage for japanese scientists. perhaps more importantly, japan has a roadmap for what might be considered its secondtier systems, which will deploy, by 2020, nine highly capable systems at its university hpc centers, including eight with performance exceeding 10 petaflop/s (figure 3.1). although what japan actually acquires is of course subject to change, the roadmap illustrates the depth of the japanese government™s commitment to advanced computing.the situation in europe is complex. first, although there is a crosseuropean union (eu) consortium, the partnership for advanced computing in europe (prace), most investment decisions are made by individual countries. germany, for example, has made signicant investments to support both basic research and industrial competitiveness. the juqueen system at the juelich research center, an ibm blue gene/q system, is one of the most powerful systems in the world. it is over half the size of the mira system at the argonne national laboratory in illinois, which is one of the two leadershipclass systems operated by doe™s ofce of science. germany has three other systems ranked in the top 25. outside the eu, other european countries have their own powerful systems. for example, the swiss national supercomputing center operates a system ranked #6 by the top500 list in june 2015.in terms of dataintensive computing, the united kingdom™s escience program identied the emergence of dataintensive computingšas a complement to the tradition simulation and modeling research activitiesšas long ago as the early 2000s. it would later prove in˚uential in launching nsf™s cyberinfrastructure program and as an inspiration for other national programs, such as the australian eresearch and the dutch escience programs.11another country that has made signicant recent investments is saudi arabia. both its shaheen system, an ibm blue gene, and its more recent shaheen ii, a cray xc40, both installed at the king abdullah university of science and technology and used for scientic research, have been among the world™s fastest systems for science research. the indian government has approved a 7year supercomputing program worth $730 million (rs. 4,500crore) intended to revitalize its program and raise the nation™s status as a worldclass computing power.leadership means drawing the best talent nationally and internally and supporting training of the next generation of scientists in computer 11 international panel for the 2009 rcuk review of the escience programme, review of escience 2009: building a uk foundation for the transformative enhancement of research and innovation, research councils uk and the royal society, 2009, https://www.epsrc.ac.uk/newsevents/pubs/rcukreviewofescience2009buildingaukfoundationforthetransformativeenhancementofresearchandinnovation/.future directions for nsf advanced computing infrastructure to support u.s. science and engineering in...copyright national academy of sciences. all rights reserved.62fiscal year 2012 2013 2014 2015 2016 2017 2018 2019 2020 2021 2022 2023 hokkaido tohoku tsukuba tokyo tokyo tech. nagoya kyoto osaka kyushu t2k todai (140 tf) 50+ pf (fac) 3mw 100+ pf (ucc + tpc) 4mw post t2k  30 pf  (ucc + tpf) 4mw (manycore system) (700+ tf) hapacs (800 tf) nec sx9 + exp5800 (31tf) 50100 pflops (fac + ucc)   fujitsu fx10 (90.8tf, 31.8 tb/s), cx400(470.6tf, 55 tb/s) fujitsu fx10 (1pflops, 150tib, 408 tb/s),  hitachi sr16000/m1 (54.9 tf, 10.9 tib, 5.376 tb/s) fujitsu m9000(3.8tf, 1tb/s)  hx600(25.6tf, 6.6tb/s) fx1(30.7tf, 30 tb/s) upgrade (3.6pf) 3mw 50 pf (tpf) 2mw 100hz200 pf (fac/tpf + ucc) 4mw hitachi sr16000/m1 (172 tf, 22tb)  cloud system hitachi bs2000 (44tf, 14tb) 10+ pf (cflm/tpf + ucc) 1.5 mw 100 pf 2 mw (cflm/tpf+ucc)  ~1pf ,~1pb/s(cflm) ~2mw 30+pf, 30+pb/s (cfld) ~5.5mw(max) tsubame 3.0 (20~30 pf, 2~6pb/s) 1.8mw (max 3mw) tsubame 4.0 (100~200 pf,  20~40pb/s), 2.3~1.8mw (max 3mw) tsubame 2.5 (5.7 pf, 110+ tb, 1160 tb/s), 1.8mw tsubame 2.0 (2.4pf, 97tb, 744 tb/s)1.8mw cray xc30 (400tf) 600tf 610 pf (fac/tpf + ucc) 1.8 mw 100+ pf (fac/tpf + ucc) 1.82.4 mw cray xe6 (300tf, 92.6tb/s), greenblade 8000 (243tf, 61.5 tb/s) sx8 + sx9 (21.7 tf, 3.3 tb, 50.4 tb/s) 500+ tb/s (cflm) 1.2 mw 5+ pb/s (tpf) 1.8 mw hitachi sr1600(25tf) fujitsu fx10270tf)+fx10(180tf),  cx400/gpgpu (766tf, 183 tb) 510 pf (fac) hitachi ha8000tc/ xeon phi (712tf, 242 tb) , sr16000(8.2tf, 6 tb) 100150 pf (fac/tpf + ucc) 1020 pf (ucc + tpf) 3mw 2.6mw 2.0mw figure 3.1 snapshot of japan™s roadmap for its nine highperformance computing infrastructure university centers. source: highperformance computing infrastructure consortium in japan, available at http://www.hpcic.jp/news/20150527centersummary.pdf.future directions for nsf advanced computing infrastructure to support u.s. science and engineering in...copyright national academy of sciences. all rights reserved.maintaining science leadership 63science, data sciences, and scientic computing. this next generation includes the designers of future computer architectures, systems software, algorithms, and computational tools, as well as the applications. it is difcult to quantify these future impacts, except to point to the historical benets that the united states has seen from computing and the continual demand by industry, government, and academia for experts in the design and use of advanced computing, networking, and data systems. future directions for nsf advanced computing infrastructure to support u.s. science and engineering in...copyright national academy of sciences. all rights reserved.4future nationalscale needsforecasting the future national needs for advanced computing is difcult. the recent revolution in datacentric computing emphasizes both the broad generality of computation and the difculty in forecasting what future needs will emerge as new methods and opportunities arise. in addition, the end of dennard (frequency) scaling and the move to massive parallelism and new computing architectures mean that many existing applications will need to be updated or replaced in order to make effective use of forthcoming systems. this chapter discusses previous approaches for discussing needs that were based primarily on ˚oatingpoint performance and makes recommendations for how to think about the more complex, multidimensional requirements for computing and data systems in the future.4.1 the structure of nsf investments and the branscomb pyramidfor the past 30 years, national science foundation (nsf) investments in advanced computing have focused on the top two levels of the branscomb pyramid (box 4.1): leadershipclass and centerclass machines (figure 4.1). although the branscomb pyramid has been invoked (and revised) for decades, it focuses only on the computational aspects of the portfolio. variations have appeared over the years that include other axes, such as storage and bandwidth (see chapter 5), but the pyramid remains as a useful, albeit incomplete, organizing principle. 64future directions for nsf advanced computing infrastructure to support u.s. science and engineering in...copyright national academy of sciences. all rights reserved.future nationalscale needs 65the pyramid conveys more than just a portfolio of computational capability that spans ve (or more) orders of magnitude in performance. it implies that there is substantial congruence in the programming models up and down the pyramid. moreover, there is an expectation that the number of users roughly scales with the horizontal extent of each level. if the pyramid is to represent resource consumption, then these last two issues must be considered. in the area of programming models, kogge and resnick1 showed that there was a signicant discontinuity in 2004 with a sudden growth in the diversity in architectures in the top500 systems. kogge and resnick noted that this was the result of barriers to the previous several decades of increases in singlecore performance, more memory per chip, memory latency, and interconnect performance. in 2004, there was growth in multicore systems relying on simpler cores and slower clock speeds, slow growth in memory density, and complex interconnects. thus, the advanced computing portfolio began to be a combination of heavyweight architectures (e.g., cray xe6 nodes for national center for supercomputing applications™ blue waters),2 lightweight architectures (e.g., ibm bluegene/q for argonne national laboratory™s mira), hybrid architectures (e.g., cray xk7 for oak ridge national laboratory™s titan or the intel xeon phi nodes on texas advanced computing center™s [tacc™s] stampede), and heterogeneous multicore systems on a chip (e.g., arm cortex). the current expectations are that scientists can use the same codes across a broad range of systems and that u.s. vendors do not develop chips and packages uniquely for the highestend systems. however, future performance improvements to the largestscale systems may require even more exotic technologies to cope with such issues as resilience, power management, and energy efciency. as a result, the highperformance community is currently exploring whether those seeking the very highest performance will have to adopt new programming models, tools, and practices sooner. on the other hand, users of both scientic and commercial systems see considerable value in maintaining a common programming model across the spectrumšfrom the single chip in handheld devices to the largest multiprocessor systems.regarding the number of users at each level of the pyramid, there are economic and cultural pressures that sometimes work to increase the 1 p.m. kogge and d.r. resnick, yearly update: exascale projections for 2013, sandia report sand20139229, sandia national laboratories, october 2013, http://prod.sandia.gov/techlib/accesscontrol.cgi/2013/139229.pdf.2 strictly speaking, blue waters is a hybrid machine, but is predominantly lightweight because only about 16 percent of the nodes include gpus.future directions for nsf advanced computing infrastructure to support u.s. science and engineering in...copyright national academy of sciences. all rights reserved.66 future directions for nsf advanced computing infrastructure the branscomb pyramid was developed as part of a panel report to the 1 and was named after the panel chair, lewis branscomb. it relates three things: computational power (yaxis, more is up), in x direction). it should really be a right triangle (so xaxis represents number of user in some possibly log scale). between the three components described above, and in particular the fact that the more capable and powerful the system (up), the fewer you can afford (also up) and the fewer ﬁpeopleﬂ (or research projects) that can be supported. it was this last relationship that was the important insight graphically presented by the branscomb pyramid. many parts of this picture are still true today, but there are also many changes that, if not rendering the branscomb pyramid obsolete, require a much more careful interpretation of the current situation in computing.there are several, and each is important and addressed in this report:compute power is not simply measured. even in highperformance computing (hpc), it has long been known that aspects such as sustained memory bandwidth (itself very different from peak memory bandwidth) are often far more ond (flop/s). the revolution in data science adds another set of dimensions to this, by adding data volume, bandwidth, latency, etc., to the list.the ability to solve a problem depends on far more than just hardware. as has been noted (see box 2.3) for some applications, using modern algorithms on 35yearold hardware will give a faster solution than using a 35yearold algorithm on modern hardware. the branscomb pyramid does not represent this aspect of computingšthe combination of software, algorithms, and human expertise in solving the problem. since the yaxis captures, in some sense, both the number of systems and the cost of those systems, the cost of this nonhardware expense also needs to be captured. new access modes make it possible for a much greater pool of users to have access to extremely large resources. in the old model, captured in the branscomb pyramid, users of (especially) the peak systems typically used a large of operation for leadershipclass systems around the worldšresearch projects future directions for nsf advanced computing infrastructure to support u.s. science and engineering in...copyright national academy of sciences. all rights reserved.future nationalscale needs 67are allocated tens of millions of nodehours (hundreds of millions to billions of cpu core hours). due to the limited nature of the resource, this implies that there can be only a relatively small number of such projects. however, some research problems may require only a small amount of total time but be infeasible without access to the special capabilities of a leadershipclass system. an example is an application that requires 1 pb of memory to run and has linear complexity in that sort of workload, converged systems would be well suited, as would cloud service models, provided that the necessary data are already colocated with the cloud computing resources.no, the growing gap between the branscomb pyramid and presentday computing has been recognized in the community. for example, in the 2011 report national science foundation advisory committee for cyberinfrastructure: task force on campus bridging,2the cyberinfrastructure environment in the us is now much more complex and varied than the longuseful branscomb pyramid. as regards computational facilities, this is largely due to continued improvements in processing power per unit of money and changes in cpu architecture, continued development of volunteer computing systems, and evolution of commercial infrastructure/platform/software as service (cloud) facilities. data management and access facilities and user communities are also increasingly complex, and not necessarily well described by a pyramid. if anything, this understates the situation.the branscomb pyramid is a convenient way to represent those three original relationships. even with greater access through new service models or application gateways, there will be only enough ﬁleadershipclassﬂ computing for a relatively small number of users and user groups. while this may still span thousands or this picture are understood, it remains a valuable way to express this relationship.1 national science foundation blue ribbon panel on high performance computing, from , national sci2 nsf advisory committee for cyberinfrastructure task force on campus bridging, final report, march 2011, http://www.nsf.gov/od/oci/taskforces/taskforcereportcampusbridging.pdf.future directions for nsf advanced computing infrastructure to support u.s. science and engineering in...copyright national academy of sciences. all rights reserved.68 future directions for nsf advanced computing infrastructurefigure 4.1 the branscomb pyramid that appeared in the national science foundation advisory committee for cyberinfrastructure task force on campus bridging final report, march 2011, http://www.nsf.gov/cise/aci/taskforces/taskforcereportcampusbridging.pdf. note: gf, giga˚op; tr, tera˚op. source: image by francine berman, san diego supercomputer center, ﬁbeyond branscomb,ﬂ presentation at clusters and computational grids for scientic computing, september 1013, 2006, licensed under the creative commons 3.0 unported attribution license (http://creativecommons.org/licenses/byncnd/3.0/legalcode). number of users at the top two levels of the pyramid (leadership class and center class). this can be done either through limiting the scope of resources allocated to a single job (number of processors, run time, etc.) or through explicit programs designed to increase the user base (e.g., the matlab portal at tacc or specic funding solicitations). thus, a modern interpretation of the pyramid needs to clearly distinguish between the usage of services and the provisioning of services. the modern interpretation also needs to recognize that there are signicant pressures to build out the resources that are affordable rather than those that are needed. while budget realism is essential, it can lead to an acquisition process dominated by cost considerations rather than one driven by the science requirements. previous studies of the advanced computing portfolio consistently cite similar science needs (computational ˚uid dynamics, astrophysics, cosmology, materials science, etc.) and the gap between these needs and the available computational resources. this future directions for nsf advanced computing infrastructure to support u.s. science and engineering in...copyright national academy of sciences. all rights reserved.future nationalscale needs 69suggests that there remains a persistent gap between the science requirements and the advanced computing components that are eventually provided.looking forward, the constellation of resources to support science needs is much broader and more diverse than in the past, ranging from cloudbased systems to highspeed wireless networks to the more traditional centers and campusbased facilities. the range of usage models has also widened (box 4.2), and the science community is generating a ˚ood of new data. thus, there are more demands from new user communities. moreover, the adoption timescale is much shorter for new technologies and capabilities. when these forces are coupled, new work˚ows emerge and evolve much more rapidly. in addition, although discussions of computing needs often focus on what systems need to be acquired and operated, the effective use of computing systems, particularly largescale systems that address national needs, requires much more than just computer hardware. expert staff are needed to operate the systems, diagnose performance problems, and help the user community. software needs to be tuned and updated for each generation of system, and community codes, which encourage sharing of effort and efcient use of resources, need to be nurtured. new algorithms are needed to address new problems and to make better use of the hardware. data need to be preserved and curated. these needs must not be forgotten when provisioning computing resources.4.2 dataintensive science and the needs for advanced computingthe current generation of advanced computing infrastructure focuses largely on meeting the requirements of work˚ows for simulation science that has fueled advances across many disciplines over the past two decades. however, the landscape of scientic work˚owsšthe series of computational or data manipulation steps required to carry out a scientic analysisšis evolving rapidly to respond to the remarkable potential that datadriven science (or more colloquially ﬁbig dataﬂ) holds for answering open scientic questions such as ﬁhow do we reliably detect a potential pandemic early enough to intervene?ﬂ or (in combination with simulation) ﬁcan we predict new materials with advanced properties before these materials have ever been synthesized?ﬂ3 advances in sensing and measurement from empirical approaches have resulted in a wealth of scientic data that can be utilized to develop 3 national institute of standards and technology, big data program, see http://bigdatawg.nist.gov/home.php.future directions for nsf advanced computing infrastructure to support u.s. science and engineering in...copyright national academy of sciences. all rights reserved.70 future directions for nsf advanced computing infrastructure computational science needs span a wide spectrum. some applications require a single, large, tightly coupled distributed memory parallel computer (such in box 3.1). others may require large numbers of runs, each of which may require only a few compute nodes but run for hours, days, or even weeks (such as the avian lineage example in box 3.1); the total compute requirement of these applications can be very large. other applications may require realtime or continuous access to computing resourcesšfor example, if computing is required to process data from an active experiment such as a telescope or sensor network.each of these types of computing (and there are others) requires a different service model. for example, applications requiring a large, tightly coupled system need the resources to be made available in a coordinated fashion and may need cesses in the application. applications using large numbers of runs may need to different types of applications on the same systemšfor example, longrunning applications using a few nodes can fragment the available nodes, making it imposin the short term, providers of computing resources can develop different partitioning the computing system into groups of nodes that support development and tuning of applications, realtime applications, longrunning applications requiring only a few nodes, and highly parallel applications (adaptive to support requirements that change with time, such as the need to use an entire system to run a single tightly coupled parallel application).in the longer term, enhancements to the applications could help relax some of the usage constraints. for example, unless one is analyzing a realtime stream from an experiment, there may be no actual requirement that a single code be able to run uninterrupted for weeks. this is an artifact of how the program has been designed and written and the availability of tools that can provide ways to stop and restart an application (e.g., through systemlevel checkpointrestart or through a userlevel software library for checkpointrestart). in the case of highly respect to requirements about when and on which compute nodes the programs and may require expertise that the computational scientists who have developed the applications may not have. more fundamentally, both the algorithms used and the implementations of those algorithms need to be examined. in some cases, the methods were appropriate for smaller problems but not for the size of problems to which they are now being applied.future directions for nsf advanced computing infrastructure to support u.s. science and engineering in...copyright national academy of sciences. all rights reserved.future nationalscale needs 71new models or rene existing models in order to gain new insights. as the costs of sensors continues to decline, experimental and observational data are being generated not only by large instruments (assembled from many small sensors), but also from large arrays of geographically distributed sensors. social media feeds are important new data sources for social science research.more generally, as data accrue from experiments and simulations, and as data from multiple experiments and simulations are integrated, scientic discoveries are increasingly being made from the accumulated and integrated data using advanced computing. this is sometimes known as the ﬁfourth paradigmﬂ of scientic discovery, because it supplements discovery paradigms based on theory, experiment, and simulation.4 further, there are additional opportunities for scientic insights at the interfaces of each of these paradigms of discoveries. a good example is provided by the aspirations of the genomics community. microarray data setsšin which several hundred to several thousand genes were measured under different experimental conditions, resulting in data sets that were megabytes in sizešhave given way to data sets in which the expression level of the entire genome is measured, resulting in data sets that are gigabytes in size. similarly, gene chips produce data sets that are kilobytes in size, while whole genome sequencing is producing data sets that are hundreds of gigabytes in size. as a rough rule of thumb, genomic and related clinical data for a cancer patient (assuming normal tissue is sequenced, a tumor is sequenced, and a tumor after relapse is sequenced) are approximately 1 tb in size. a cohort of 10,000 patients requires 10 petabytes of storage, and a cohort of 1 million patients (a goal for the community over the next several years) would require 1 eb of storage.another example is the large synoptic survey telescope (lsst), which will produce a wideeld astronomical survey of the universe using a 8.4meter telescope and 3gigapixel camera. lsst will collect 15 tb of raw image data every night that will be processed in nearreal time to provide scientists with alerts about new and unexpected astronomical events and reprocessed annually. it will yield a 200 pb data archive by the end of the decadelong survey.today, both scientic researchers and businesses use a wide array of data analytics tools. in some areas, largescale analytics companies such as google and amazonšrather than the scientic communityšare in a leadership position. in some other areas, the needs of science may not 4 t. hey, s. tansley, and k. tolle, eds., the fourth paradigm: dataintensive scientic discovery, microsoft research, 2009, http://research.microsoft.com/enus/collaboration/fourthparadigm/.future directions for nsf advanced computing infrastructure to support u.s. science and engineering in...copyright national academy of sciences. all rights reserved.72 future directions for nsf advanced computing infrastructureoverlap the needs of industry. for example, the statistical analysis of large, in memory data sets (such as the anticipated output from the lsst) is more similar to a scientic computation that to the type of analyses that have generally interested google or amazon. looking ahead, there may be opportunities for researchers to make better use of the tools and concepts developed by industry or for the industrial and scientic communities to partner more effectively where their needs overlap.one challenge with respect to the private sector is that salaries for those trained in data analytics can be far higher in the private sector than in the academic research community. this makes it difcult for academic researchers to stay abreast of emerging technical tools that enable dataintensive science. for nsf, this creates two challenges. the rst challenge is to act strategically to develop the needed workforce to support both science and business applications. the second is to nd ways to keep people with these skills in the science community despite lower salariesšfor example, by offering reasonably secure, stable career paths as well as exciting work.from a technical requirements perspective, infrastructure for dataintensive science needs to consider data acquisition, storage and archiving, search and retrieval, analytics, and collaboration (including publish/subscribe services). recent nsf requirements to submit data management plans as part of proposals signal recognition that access to data is increasingly important for interdisciplinary science and for research reproducibility. although the focus is sometimes on the hardware infrastructure (amount of storage, bandwidth, etc.), the human and software infrastructure is also important. understanding the software frameworks that are enabled within the various cloud services and then mapping scientic work˚ows onto them requires a high level of both technical and scientic insight. moreover, these new services enable a deeper level of collaboration and software reuse that are critical for dataintensive science.when considering cyberinfrastructure requirements, the needs of dataintensive science are often considered as separate from those of the more traditional computationally intensive science problems, such as climate modeling. however, as new massive data sets become available (e.g., the lsst project), the line separating these two types of research becomes blurred. as a specic example, today™s climate models have dramatically increased temporal and spatial resolution compared to models from 10 years ago. although this has greatly improved model performance, many processes that once could be parameterized simply at coarse resolution now must be included explicitly in highresolution models. one example is the representation of cloud processes, where the physics are poorly understood and critical parameters cannot be measured. future directions for nsf advanced computing infrastructure to support u.s. science and engineering in...copyright national academy of sciences. all rights reserved.future nationalscale needs 73the june 2014 issue of philosophical transactions of the royal society5 was devoted to a generation of coupled deterministic/probabilistic models that illustrate a possible convergence between computeintensive models and dataintensive models.4.3 forecasting future requirementsdeveloping science requirements for any large project takes enormous experience and insight (and creativity). establishing requirements that can be achieved within a cost and schedule framework is even harder. by its very nature, science is unbounded. there are always pressures to improve our understanding or to make better predictions.past nsf efforts have, in general, implicitly constrained requirements, either through budget caps or by technical feasibility. obviously, there is some iteration between these elements, although the nsf petascale program6 was driven largely by cost and desired sustained computational performance. there was an implicit assumption that the acquired systems would enable a certain class of scientic models and analyses to be addressed. with this approach, there was a risk that the science requirements could have been only loosely coupled to the systems that were acquired, and key areas of science could have been left unserved.today, with growing demand for computing and constrained budgets, it has become especially important to understand the relative benets and risks of different technical approaches for the science portfolio. this section describes some of the challenges nsf will face in developing science requirements for advanced computing.quantifying science benets. it remains an unsolved (and probably unsolvable) problem to accurately quantify the return on investment in scientic research, and certainly it is not possible to predict the return. but it may be possible to consider the likely costs and risks of different approaches, as well as the possible opportunities, and use these to guide the setting of objectives and priorities.suitable measures of advanced computing performance. it is also important to avoid reducing the requirements to a too simplistic measure, such as peak ˚oatingpoint operations per second (flop/s). the system with the best flop/s per dollar may not provide the best value for science 5 for example, t. palmer, p. düben, and h. mcnamara, stochastic modelling and energyefcient computing for weather and climate prediction, philosophical transactions of the royal society a 372:20140118, 2014, doi:10.1098/rsta.2014.0118.6 national science foundation, ﬁhigh performance computing system acquisition: towards a petascale computing environment for science and engineering,ﬂ program solicitation nsf 050625.future directions for nsf advanced computing infrastructure to support u.s. science and engineering in...copyright national academy of sciences. all rights reserved.74 future directions for nsf advanced computing infrastructureapplications, where sustained, rather than peak, performance is far more important. key areas of science may have different requirements, such as sustained i/o performance for datacentric applications.rapidly evolving science needs.  any sciencedriven requirements process must also confront the issue that the science itself is changing rapidly on the timescale associated with largescale advanced computing acquisition and deployment. past experience has shown that although a procurement can be completed in several years, large systems sometimes take as long as 10 years from initial concept to full availability to users. a rolling decadal roadmapping process could help inform users about plans for the upgrade and replacement of existing systems and, more generally, the performance characteristics of expected future systems. responding to the rapid evolution of datadriven science. for example, new classes of weather forecasting models combine the tools of computational ˚uid dynamics along with datadriven parameterizations to improve forecasts for smallscale (but intense) events. moreover, these datadriven approaches often rely on ensembles of many model runs. as the network of realtime sensors connected through highspeed links to the internet grows, these datadriven models will require new capabilities in regard to computation, storage, and bandwidth. much as with business analytics, these dataintensive methods will be based on near realtime streaming data. moreover, new technologies could have profound benets for dataintensive science. one can envision networks of sensors where each sensor node has local compute capabilities that rival the supercomputer performance of only a few years ago. the impact on adaptive computing and sensing could be signicant, realizing one of jim gray™s admonitions to move computation to the data.7such work˚ows will require autonomous tools to assess data quality and model performance; human intervention and control will not scale up to these new models. planning for these changing (and often poorly formulated) requirements will require considerable insight. these changing scientic work˚ows extend to the human side of scientic computing as well. especially in regards to dataintensive science, reproducibility will be challenging. these requirements will often be as important as the traditional technical requirements of cpu performance, latency, storage, and bandwidth.complex and rapidly changing technology landscape. just a few years ago, mainstream highperformance computing was limited to commodity x86 processors from intel and advanced micro devices and ibm power 7 a. szalay and j.a. blakeley, ﬁgray™s laws: databasecentric computing in science,ﬂ in the fourth paradigm: dataintensive scientic discovery (t. hey, s. tansley, and k. tolle, eds.), microsoft research, redmond, wash., 2009.future directions for nsf advanced computing infrastructure to support u.s. science and engineering in...copyright national academy of sciences. all rights reserved.future nationalscale needs 75processors. today, highperformance computing is making use of generalpurpose graphical processing units and accelerators, and some designs such as intel™s xeon phi are focused on scientic computing. looking ahead, more diversity is likely to come in the form of things like integrated nonvolatile randomaccess memory and processors integrated with memory. at the same time, much of the broader commercial industry is focused on the needs of mobile devices.  the technical landscape now has a range of new service providers beyond the hardware/software companies. much has been made of cloud services, although most of the discussion has focused on its elastic computation and storage model along with an aggressive pricing strategy. however, a key capability of cloud services is the rich software framework that is available for users. not only can these services and frameworks be leveraged to support changing science work˚ows, they can be extended to include new components that can then be made available to other users. the science community rapidly adopts these new ﬁproviders,ﬂ such as dropbox, until a new and improved service appears on the market. along with the challenges of a changing scientic and technical landscape, any requirements process must recognize that there will always be gaps. for example, one cannot predict with any certainty the technical or business directions of the major hardware and software vendors beyond several years. to give another example, the international technology roadmap for semiconductors makes evident the major technical challenges faced by industry in maintaining the pace of performance improvements several years out. widely used proprietary software such as cuda is also subject to rapid change.8 adoption rates of (or resistance to) new technologies is another challenge. the requirements process must at least consider the economic forces that are driving the technology market as well as the political and cultural forces that either speed or resist adoption. moreover, it must also recognize that the science community must be capable of using the advanced computing portfolio, which means one cannot follow a ﬁbuild it and they will comeﬂ approach. 4.4 thinking about a new approach to develop requirements for advanced computingat its heart, there needs to be a rigorous process for development and assessment of the science requirements for advanced computing. the process needs to ensure that these science requirements have substantive 8 language standards such as c++, fortran, and openmp are less subject to unexpected changes over the short term.future directions for nsf advanced computing infrastructure to support u.s. science and engineering in...copyright national academy of sciences. all rights reserved.76 future directions for nsf advanced computing infrastructurefeedbacks between the science, the technical approach, and cost. it also needs to make explicit what research can and cannot be done within a given budget envelope and with a particular set of acquisition decisions. moreover, a clear and bounded vision for the types of science that advanced computing will support is needed. for nsf, this will likely mean developing an understanding of how much of the portfolio can be supported by a more datacapable generalpurpose platform (and what specic data capabilities are needed), and what is left over that either needs specialized advanced computing supported as cyberinfrastructure, or perhaps topical computing supported in part by the science programs. a more productive view than just focusing on the hardware that can be afforded would be to describe and quantify a set of services that are needed to meet a class of science challenges. such an approach would allow a more ˚exible investment strategy (build a specic center, work with cloud service providers, etc.) rather than trying to t everything into a small set of infrastructure assets.a process that relies on documented science objectives and assessment of the progress made toward achieving these objectives, rather than simply statements that greater computational capacity will improve understanding of a specic scientic process or phenomenon, can help improve future decisions. for example, such an assessment might show that the ability to run an ensemble of 1,000 shortterm weather forecasting models will improve the quality of the forecasts by a specic percentage. these science objectives capture the value of the requirement as a function of benet and affordability, where benet is in turn a function of importance, quality, utility, and probability of success. this approach to costbenet analysis would allow science communities to understand the linkages between science, technical requirements, and cost, thus allowing more rational analysis of the trade space of science capability, technical requirements, and cost.the more granularity that can be provided in terms of costs and benets, the better the decisions that can be made when the inevitable trades need to be made between science, technology, and cost. for example, one must consider the full costs of the advanced computing components, including both the acquisition as well as operations and maintenance costs (including hardware, software, and staff costs). in doing so, one must also consider xed costs (staff, maintenance contracts, etc.) as well as marginal costs (elastic costs of cloud services, etc.). this is especially important as nsf and the science community are moving toward a model of buying services as needed rather than recognizing the true xed costs. for example, a scientic programmer may spend only 1 month on a particular project, but the employer needs to provide a full year of salary. the existing supercomputer centers repeatedly note the difculty in maintainfuture directions for nsf advanced computing infrastructure to support u.s. science and engineering in...copyright national academy of sciences. all rights reserved.future nationalscale needs 77ing experienced and highly trained staff as the funding agencies move into a mode of buying talent by the month rather than providing stable support for the expertise the scientic community depends on.another component of the requirements analysis is to identify the linkages and dependencies across nsf™s advanced computing portfolio. such systems engineering across a diverse portfolio will not be simple, but it is essential to developing a resilient portfolio that can support a wide range of science areas. advanced computing requirements should also take into account the science needs (because nsf provides advanced computing to research communities funded by other federal agencies) and contributions of other federal agencies (because some nsffunded researchers make use of advanced computing provided by other federal agencies). today, most users of nsf™s advanced computing infrastructure have no understanding of the value of the resource that they have been granted. while rationed (through the allocation process), advanced computing resources are for the most part ﬁfreeﬂ (there is no charge for them). this leads to a mindset that puts little value on making efcient use of these resources, particularly because there is no way in the current system to trade, for example, computer time for expert help in tuning applications. as a rst step, building an awareness of the value and cost of computing resources may lead to a more holistic and comprehensive approach to using the advanced computing resources. one possible way to do this would be to provide a dollar value of the computational resource granted. there are some dangers in this approach; the goal is to build awareness of the costs and value, not create a chargeback mechanism. section 6.3.8 describes a possible pilot project to explore the benets, risks, and problems with such an approach.this process will yield a much more thorough understanding of the complete costs and technical feasibility of the portfolio in the context of documented science objectives. this will inform an analysis of tradeoffs that will modify the approach to t within economic and political realities. balancing its primary science mission with the need to operate infrastructure will require constant assessment by nsf, as noted in the recent decadal survey of the ocean sciences.9 4.5 roadmappingthe department of energy (doe) has created a roadmap for future advanced scientic computing research systems that provides research9 national research council, sea change: 20152025 decadal survey of ocean sciences, the national academies press, washington, d.c., 2015.future directions for nsf advanced computing infrastructure to support u.s. science and engineering in...copyright national academy of sciences. all rights reserved.78 future directions for nsf advanced computing infrastructureers with a view of what capabilities to expect (figure 4.2). by describing the next 3 to 5 years of leadership computing systems, doe has given the community useful information about the general characteristics and organization of the next doe leadershipclass systems. the longerterm roadmaps are less concrete but still provide information about the general intentions of doe: continue increasing single machine performance, which contrasts with keeping the single machine performance about the same but increasing the total number of machines.nasa and other mission agencies have regularly employed a roadmap process that outlines a small set of science themes that will engage the scientic enterprise over the next few decades.10 these themes then serve as a framework for a series of notional missions or activities that address specic questions in the theme. some of these questions may need to be addressed sequentially (e.g., the approach to one question may depend on the knowledge gained from answering another question), while others may proceed roughly in parallel. taken together, the notional missions lay out a roadmap that is based on scientic progress at each stage. however, unlike a decadal survey, the roadmap also lays out options and multiple pathways and identies the scientic and technical challenges.a fundamental aspect of the roadmapping process is that it is driven by the science themes, rather than simply a quest for a certain level of technical capability. also, the process lays out options and challenges. lastly, it links scientic progress to technological capabilities, rather than a ﬁbuild it and they will comeﬂ approach. maintaining a linkage between science need and technological capability is an important aspect of effective roadmapping.implementing a roadmapping process that re˚ects all of the research supported by nsf advanced computing will not be easy. for one thing, as dennard scaling has fallen off, there is growing pressure to use domainspecic hardware to achieve greater computing performance. for another, the requirements have become more diverse as the range of science using advanced computing has grown. specialized accelerators, storage facilities, or other capabilities may be needed to enable some research objectives efciently, and it will in any event be difcult to roll up requirements into a sufciently small set. it may be necessary to develop separate road10 for example, the endtoend challenges in managing massive research data are considered in nasa earth science technology ofce/advancd information systems technology (esto/aist) big data study roadmap team, ﬁnasa earth science research in data and computational science technologies,ﬂ september 2015, http://ieeebigdataearthscience.jpl.nasa.gov/references/aistbigdatastudydraftsummer2015.future directions for nsf advanced computing infrastructure to support u.s. science and engineering in...copyright national academy of sciences. all rights reserved. 79figure 4.2 advanced scientic computing research (ascr) computing upgrades at a glance. note: acronyms dened in appendix d. source: u.s. department of energy, ofce of science, ﬁascr computing upgrades at a glance,ﬂ http://science.energy.gov/~/media/ascr/pdf/facilities/ascrcomputiingfacilityupgrades.pdf, accessed april 25, 2016. anameplanned installationedisontitanmiracori2016summit20172018theta2016aurora20182019systempeak (pf)2.627 10>30200>8.5180 peak ower (mw)294.8<3.7 13.3 1.713total system memory357 tb710tb768tb~1 pb ddr4 +high bandwidth memory (hbm)+1.5pb persistent memory > 2.4pb ddr4 + hbm + 3.7 pb persistent memory>480 tb ddr4 + hbm>7pb node performance (tf)0.460 1.452 0.204>3>40>3>17 times miranode processorsintel ivy bridge amd opteronnvidiakepler 64bit powerpc a2intelknights landing manycore cpus intel haswellcpu in data partitionmultiple ibm power9 cpus multiple nvidiavoltas gpussystem size (nodes)5,600 nodes18,688nodes49,1529,300 nodes1,900 nodes in data partition~4,600nodes>2,500 nodes>50,000nodessystem nterconnect ariesgemini5d torusariesdual rail edribariesfile ystem7.6 pb168 gb/s,lustre®32 pb1 tb/s,lustre®26 pb300 gb/s 28 pb744 gb/s lustre®120pb1 tb/s10pb, 210 gb/s lustre initial150pb1 tb/slustre®future directions for nsf advanced computing infrastructure to support u.s. science and engineering in...copyright national academy of sciences. all rights reserved.80 future directions for nsf advanced computing infrastructuremaps by science area, and then aggregate similar needs across areas (e.g., the use of unstructured grids and iterative linear methods in simulations). another challenge is determining a good conguration for a computing system that requires more than just a measure of the number of operations per second (e.g., clock speed) or size of data (e.g., disk space). research has shown that simple benchmarks are, individually, rarely predictive of the performance of an application, and even collections of benchmarks give only a rough estimate.11 highly accurate performance estimates, while possible, remain a difcult and timeconsuming process. as a result, the community has relied on a very simple measure of computing performance, based on ˚oatingpoint performance only. for example, xsede allocates resources in service units (sus), which are related to the performance of highperformance linpack. this re˚ects the peak ˚oatingpoint performance of a system but little else. allocations under the prac program for blue waters are in nodehours, which is more easily related to the specic system but is not easily convertible to sus or nodehours for a different system. a rst step would be to gather more information about the needs of applications. relevant measures include memory size and bandwidth, data size and bandwidth, interconnect bandwidth and application sensitivity to interconnect latency, integer and ˚oatingpoint performance, and longterm data storage requirements. some of this information could be gathered by tools designed for this purpose, applied to an application running on a current system, reducing the burden on the computational scientists. an example of what the rst step in this process might be is presented in box 4.3.despite the challenges and the likely imperfections in the roadmaps that are developed, it should be possible to develop roadmaps that provide enough guidance to the community to be worthwhile. by focusing on the overall picture rather than the highresolution details, roadmaps can indicate to the community what the major investments will look like. this will allow researchers to make better decisions about future research. it will also allow researchers to start preparing their software to be ready for future systemsšfor example, by providing advance notice about signicant changes in architecture or conguration. 11 see, for example, l. carrington, m. laurenzano, a. snavely, r. campbell, and l. davis, how well can simple metrics represent the performance of hpc applications?, in proceedings of the acm/ieee sc 2005 conference, 2005, http://ieeexplore.ieee.org/xpl/articledetails.jsp?arnumber=1560000.future directions for nsf advanced computing infrastructure to support u.s. science and engineering in...copyright national academy of sciences. all rights reserved.future nationalscale needs 81 needs of applications, the national science foundation (nsf) could ask that all proposals for research that would require advanced computing include relevant measures such as memory size and bandwidth, data size and bandwidth, interconnect bandwidth and application sensitivity to interconnect latency, integer and this information could be gathered by tools designed for this purpose, applied to an application running on a current system, reducing the burden on the computational scientists. understanding the computational requirements of applications is a complex task. many studies1 have shown that even for the subset of applications that are numerical simulation codes, there is no simple way to predict performance. performance from the processor manufacturer or the rate achieved by the high performance linpack benchmark, is often a very poor way to predict performance. the situation has been made far worse in the past few years with the advent of code rewrites to make use of the accelerators. advanced language, compilation, and execution systems could have transformative impact on both productivity and performance; however, these still largely represent frontiers of research rather than readytodeploy technologies. the situation is further complicated in the case of more dataintensive applications.given this complexity, what data ought to be collected from advanced computing proposals to understand the computational requirements of the applications? this is really an unsolved problem, and one for which the national science foundation could support research. in addition, for the purposes of requirements gathering, the data must be relatively easy for the code developers to provide and would provide more information than is currently collected (the number of service units requested), without requiring a detailed analysis by experts of each application. these examples have been selected because they are either relatively easy to determine, based on the code or algorithm, or they can be measured from a typical run of the application using widely available, open source tools. the data should apply to a typical execution of the application; if there are either multiple applications or widely different execution behaviors, data should be provided for each instance. the list below is targeted at parallel highperformance computing applications, but the approach can be applied to other areas; some items include examples relevant to some data science applications.memory motion, and internode communication.continuedfuture directions for nsf advanced computing infrastructure to support u.s. science and engineering in...copyright national academy of sciences. all rights reserved.82 future directions for nsf advanced computing infrastructure2. application performance characteristics, including code scaling and per3. input/output (i/o) data sizes and number of i/o operations.4. algorithms used by the application; the berkeley 13 motifs2 may be a starting point for a list.5. application implementation, including programming languages and major libraries used.6. total application needs, such as the number of runs for an ensemble study.cludes multiple applications. these may be computationally intensive or they may be used to control other codes. for requests to use a known community code, for example, the data requested should instead be the name of the code and enough information about the running environment, such as the problem size, so that the compute needs in point 1 can be computed.the list above illustrates useful data that could be obtained with relatively little effort. only item 1 requires some code analysis, which can be obtained by running the application with tools such as papi [precision approach path indicator] and darshan (instructions should be provided on how to use these tools; xsede [extreme science and engineering discovery environment] and blue waters could provide tools and support to make this easy for applications that are already running on their systems). the other data are either descriptive or, in the case of this approach provides only the highestlevel description of the application needs. while this would provide valuable data not currently being tracked, especially because it includes memory and i/o needs, as well as suitability for accelcertainly be improved over time. however, it must be easy for the researcher to provide the information and not require a lengthy analysis of the code. if a shorter list was desired, the data in item 2 (application performance characteristics), combined with the number of sus required, would provide valuable guidance in setting requirements for production computing systems.1 see, for example, l. carrington, m. laurenzano, a. snavely, r. campbell, and l. davis, how well can simple metrics represent the performance of hpc applications?, in proceedings of the acm/ieee sc 2005 conference, 2005, http://ieeexplore.ieee.org/xpl/articledetails.jsp?arnumber=1560000.2 k. asanovic, r. bodik, b.c. catanzaro, j.j. gebis, p. husbands, k. keutzer, d.a. patterson, w.l. plishker, j. shalf, s.w. williams, and k.a. yelick, the landscape of parallel computing research: a view from berkeley, technical report no. ucb/eecs2006183, december 18, 2006, http://www.eecs.berkeley.edu/pubs/techrpts/ 2006/eecs2006183.pdf.future directions for nsf advanced computing infrastructure to support u.s. science and engineering in...copyright national academy of sciences. all rights reserved.5investment tradeoffs in advanced computingowing to the success of computing in advancing all areas of science and engineering, advanced computing is now an essential component in the conduct of basic and applied research and development. in a perfect world, investments in advanced computing hardware and software, together with investments in human expertise to support their effective use, would re˚ect the full range and diversity of science and engineering research needs. but, as discussed in chapter 2, the gap between supply and demand is signicant and continuing to grow. in addition, developments in dataintensive science are adding to the demand for advanced computing. from the smallestscale system to the largest leadershipscale system, one of the challenges of advanced computing today is the capacity requirement along two welldifferentiated trajectoriesšnamely, highthroughput computing for ﬁdata volumeﬂdriven work˚ows and high parallel processing for ﬁcompute volumeﬂdriven work˚ows. although converged architectures may readily support requirements at the small and medium scales, at the upper end, leadershipscale systems may have to emphasize some attributes at the expense of others.moreover, within a given budget envelope for hardware, the criteria for future procurements should re˚ect scientic requirements rather than simplistic or unrepresentative benchmarks. there is no single metric by which advanced computing can be measured. although peak ˚oatingpoint operations per second (flop/s) is the most common benchmark, even within the simulation community it has long been known that other 83future directions for nsf advanced computing infrastructure to support u.s. science and engineering in...copyright national academy of sciences. all rights reserved.84 future directions for nsf advanced computing infrastructureaspects of computer performance, including memory bandwidth and latency, are often more important. finally, advanced computing is more than hardware. investments in software, algorithms, and tools can help scientists make more effective use of resources, effectively increasing the computing power available to the community.the tradeoffs to be considered are many, with different impacts on advanced computing cost and capability. this chapter starts by considering tradeoffs associated with the volume of compute and data operations, applies them to investments in systems designed for simulation and dataintensive workload, and considers converged solutions (section 5.1). this example was chosen because in the near term, it is perhaps the most critical tradeoff that the national science foundation (nsf) must consider, as it balances the needs of existing computational users against a rapidly emerging data science community. the chapter then turns to another critical tradeoff, between investments in production and investments to prepare for future needs (section 5.2). several investment tradeoffs faced by nsf in simulation science, along with their impact, are discussed in section 5.3. an example portfolio illustrating how nsf might address these tradeoffs is sketched out in section 5.4.5.1 tradeoffs among compute, data, and communicationssupporting both simulation and datadriven science requires making tradeoffs among compute, data, and communications capabilities. at a conceptual level, work˚ows for the simulations of physicsbased models are typically computevolume driven in that they require a higher number of arithmetic or logical operations per unit of data moved. an illustrative example is manybody simulation of the electronic structure of molecules or materials, which is dominated by the contraction of dense, multidimensional tensors. on the other hand, work˚ows for developing or rening models by utilizing data from experiments or simulations are typically datavolume driven in that they require a larger number of units of data moved per arithmetic or logical operation. examples include the analysis of genomic data from large studies or the analyses of streaming data. further, in areas where scientic advances may be imminent at the con˚uence of both of these approachesšfor example, in earth systems science, where climate and weather models can be coupled to data from observatoriesšwork˚ows will likely exhibit a complex mix of both of these aspects.the communicationvolume dimension refers to the speeds at which data chunks from very small to very large sizes can be moved efciently future directions for nsf advanced computing infrastructure to support u.s. science and engineering in...copyright national academy of sciences. all rights reserved.investment tradeoffs in advanced computing 85within the system. such communication is accomplished through networks that may connect processors directly or via/across memory and storage subsystems; technology trends typically point to one or more orders of magnitude differences in the latencies per operation at a processor, memory, or storage element. consequently, communication networks can be congured to serve efciently the critical set of latencies at appropriate bandwidths. now, high performance for a particular work˚ow will depend not only on how its data and computevolume dimensions tap into the corresponding dimensions, but perhaps even more crucially on how the software implementation and algorithms underlying the work˚ow match the communication dimension. a key question to consider is how the different types of work˚ows can inform advanced computing designs and specications so that they can be provisioned appropriately to advance national priorities for discovery and innovation. here, the major dimensions of advanced computing, as shown in figure 5.1, play a critical role. the computevolume and datavolume dimensions of advanced computing architectures are closely related to the corresponding compute and data dimensions of scientic work˚ows. however, the correspondence to the communicationvolume dimension1 is more complex, and it drives the space of tradeoffs in regard to how desirable levels of performance may be obtained for specic types of work˚ows.5.2 tradeoffs for dataintensive sciencewhen making design and investment tradeoffs for systems that support dataintensive science, one needs to consider the entire work˚ow, from instrument to scientic publication, and optimize the entire infrastructure, not just individual systems. one key tradeoff is that investments in capabilities for data processing and longterm storage need to be balanced against each other accordingly. for example, in a project that collects data over several years, data are analyzed as they are collected and typically continue to be analyzed for several more years. in this case, the project is required to store the data, analyze them, and almost always to reanalyze them as algorithms improve and as new data arrive. as another example, a design that allocates more time to computing capabilities may complete its analysis faster but may not be able to store 1 communication volume is used here as shorthand for the more accurate and complex representation of internode communication, including latency, pointtopoint bandwidth, bisection bandwidth, network topology and routing, and similar characteristics. latency in particular is critical for many applications; some algorithms require high bisection bandwidth. future directions for nsf advanced computing infrastructure to support u.s. science and engineering in...copyright national academy of sciences. all rights reserved.86 future directions for nsf advanced computing infrastructurelength of time data heldbatch analysis of data setsreanalysisstorage onlyintegration of multiple data setsduration of computationnext project / "nearby" projectsnext technologyduration of projectdataintensive computingdigital archiveshpcdata archivesstreaming analyticsduration of ingestionstream computingtype of analysisfigure 5.1 length of time data are held for different forms of computing and types of analysis. sufcient data to carry out the analysis of interest. consider two designs with different allocations between the compute and storage allocated to a project. as figure 5.2 illustrates, some projects may reanalyze all of the raw data throughout the project and so must keep the data online. another important aspect of dataintensive science to keep in mind is that as different large data sets are integrated and the results analyzed, there are usually new types of scientic discoveries that are possible. dataintensive projects often provide data to other projects that may use their data as part of a broader ﬁintegrative analysis.ﬂ the tradeoffs concern balancing how much data can be stored and for how long with how many processors can be used to analyze the data and how the communication network can be optimized for analysis and for efcient redistribution of the data to other interested parties. when instruments, computers, and archival sites are geographically distributed, the data they produce may be processed and consumed at multiple sites, requiring special attention to the widearea networks needed to transfer the data, how data should be staged and consolidated, and so forth. for experiments, deciding how much data to save is a tradeoff between the cost of saving and the cost future directions for nsf advanced computing infrastructure to support u.s. science and engineering in...copyright national academy of sciences. all rights reserved.investment tradeoffs in advanced computing 87figure 5.2 a simplied view of computing, including the three axes of compute performance, input/output (i/o) and le system performance, and internode communication (network) performance. commodity clusters typically use commodity interconnects with performance less than typical highperformance computing (hpc) systems. however, they often include large amounts of fast le systems and large numbers of nodes, giving high aggregate compute performance. the best leadershipclass systems (and the most expensive) will have the highest performance along all three axes. in reality, leadershipclass systems are also a compromise, often trading i/o performance for greater ˚oatingpoint performance. missing from this view are important characteristics such as the type of compute or data architecture. the shapes are meant to capture common tradeoffs in different types of systems and are not meant to be rigorous. for example, the shape for the ﬁcommodity clusterﬂ describes systems where the i/o capacity is roughly proportional to the number of nodes (compute performance), and the system uses interconnects that typically have lower performance than is found in highend hpc systems.future directions for nsf advanced computing infrastructure to support u.s. science and engineering in...copyright national academy of sciences. all rights reserved.88 future directions for nsf advanced computing infrastructureof reproducing, and this is potentially more signicant than the tradeoff between disks and processors.in summary, for dataintensive science projects, one must balance the amount of data that can be stored with the capability and capacity of the work˚ows for analyzing the stored data. second, tradeoffs in the work˚ows themselves must be optimized. for datavolumedriven work˚ows, scientic outcomes are best achieved when the advanced computing is congured for efcient, highthroughput processing at scale with communication attributes directed toward efciencies at the processing and storage layers for continuous updating and reanalysis of petabytesized data sets. consequently, achieving u.s. leadership in this space requires achieving such capabilities at scale through an appropriate balance of advanced computing attributes in the networked storage elements in regard to the datavolume and communicationvolume dimensions and in the processing elements in regard to the computevolume dimension to achieve high throughput of data analyses work˚ows.5.3 tradeoffs for simulation science looking to the future, a variety of tradeoffs will need to be examined with regard to the investments that nsf can make and their potential for enabling highimpact outcomes in simulation science. these tradeoffs concern the scale of highperformance computing (hpc) systems and the fact that scale itself can become a tipping point for enabling new and unprecedented discoveries. the pivotal role of nsf in advancing simulation science and engineering through its hpc investments at different scales is readily demonstrated by using the ncsa blue waters project and the xsede program as illustrative examples. the blue waters project has enabled breakthrough scientic results in a range of areas, including an enhanced understanding of early galaxy formation, accelerating nanoscale device design, and characterizing alzheimer™s complex genetic networks.2 nsf also supports the development and integration of midscale hpc resources through its xsede program, which provides hpc capacities to the broader scientic community along with resources for training, outreach, and visualization and supporting research in such areas as earthquake modeling and the simulation of black hole mergers.3 further tradeoffs concern the maturation of simulation science from oneoff simulations of a select few critical points of a highdimensional modeling space to ensemble calculations that can manage uncertainties 2 see blue waters, ﬁimpact,ﬂ https://bluewaters.ncsa.illinois.edu/impactoverview, accessed january 29, 2016.3 xsede, ﬁimpact,ﬂ https://www.xsede.org/impact, accessed january 29, 2016.future directions for nsf advanced computing infrastructure to support u.s. science and engineering in...copyright national academy of sciences. all rights reserved.investment tradeoffs in advanced computing 89for increases in prediction accuracies4 or enable highdelity modeling, simulation, and analyses for costefcient and innovative digital engineering and manufacturing.5the scientic workloads supported by nsf through the blue waters and xsede programs are largely computevolume and communicationvolume driven, although aggregate memory capacity can be a key enabler for some frontier science applications. a notable point is that blue waters provides leadership capabilities in regard to all these dimensions, as shown in figure 5.2, while some other nsf xsede investments provide capacities for such work˚ows at the midrange. for example, stampede enables high throughput of low to midscale computations. historically, scientic workloads that are computevolume driven have largely driven the balance of tradeoffs in regard to the compute, communication, and storage components of such hpc advanced computing. further, as described earlier, the trend toward multicore nodes with increasing core counts and very high degrees of thread and corelevel parallelism require the use of highbandwidth and lowlatency networks to enable data exchange at the right scale. additionally, the underlying algorithms and software implementations of the associated scientic workloads have been rened to dene a more intricate relationship between how the elements of the advanced computing have to be tuned to provide the right balance along the computevolume and communicationvolume dimensions. even within a single work˚ow, different algorithms with different tradeoffs between compute and communication may be preferable on a given platform. this makes it difcult to evaluate which platforms are best suited for which applications. as a consequence of trends in both hardware and software, including multicore nodes with high degrees of parallelism and sophisticated algorithms that require higher levels of data sharing while reducing the number of operations per unit data, the communicationvolume dimension is a key differentiator in how tradeoffs need to be managed. the advanced systems for simulation science often require that signicant fractions of the cost budget are invested in the form of lowlatency and highbandwidth communication networks to couple multicore processor nodes. for example, much of the budget for a system to support simulation science workloads would be allocated to multicore processor nodes 4 national weather service, ﬁnmme: north american multimodel ensemble,ﬂ http://www.cpc.ncep.noaa.gov/products/nmme/, accessed january 29, 2016.5 national digital engineering and manufacturing consortium, modeling, simulation and analysis, and high performance computing: force multiplier for american innovation, final report to the u.s. department of commerce economic development administration on the national digital engineering manufacturing consortium (ndemc), 2015, http://www.compete.org/storage/documents/ndemcfinalreport030915.pdf.future directions for nsf advanced computing infrastructure to support u.s. science and engineering in...copyright national academy of sciences. all rights reserved.90 future directions for nsf advanced computing infrastructureand the network that connects them to enable fast data exchange as the simulations proceed. in contrast, if the same budget was to be directed to serve streaming data science workloads, the bulk of it would go toward the storage network to enable high levels of data ingestion from storage by the multicore processor nodes. both the wrangler and gordon systems funded by nsf are targeted in this manner toward dataintensive computation. this illustrates the contrasts between how tradeoffs need to be balanced for serving different work˚ows.in summary, for computevolumedriven work˚ows, scientic outcomes are best achieved when the advanced computing is congured for efcient parallel processing at scale for a single analysis or simulation. the elements along the communicationvolume dimension of the advanced computing (i.e., the interconnects) should be congured toward efciencies at the processing and storage layers for continuous data exchange and the highthroughput output of data that are the results or outcomes of the processing. it is natural therefore to interpret performance in these hpc systems as they are traditionally known, to represent high levels of coupled parallel processing for computevolumedriven applications. 5.4 datafocused, simulationfocused, and converged architecturesone of the features of the current era in computing is that there are several distinct architectures for the largest highperformance computers. at the same time, large systems for handling data, especially commercial data systems, are as large or larger in sizešand even raw aggregate computing poweršas the hpc leadershipclass systems. today, this suggests that there are two types of systems: hpc systems focused on simulation and systems focused on data. the true situation is almost certainly more complex. an issue complicating the discussion is that leadershipclass systems for simulation science are operated mainly by research organizations and the government, while leadershipclass systems for data science today are operated mainly by industry. advances in hpc system architectures have generally been shared. in the case of dataintensive systems, some tools have been made open source while others have remained proprietary. stronger ties between the computer science and computational science communities focused on new data analysis tools, techniques, and algorithms would help bridge this gap. some demands can be met by what is sometimes called convergence computing, in which highperformance systems are designed to meet the needs of both highend simulation science and data science work˚ows. indeed, highperformance systems for dataintensive computing in industry are almost always coupled with largescale storage systems future directions for nsf advanced computing infrastructure to support u.s. science and engineering in...copyright national academy of sciences. all rights reserved.investment tradeoffs in advanced computing 91(figure 5.1). this coupling is still relatively infrequent in nsfsponsored projects, and no nsfsupported project has data storage at the scale of a large internetscale company. as a simple example, the total online data storage for blue waters and xsede systems is in aggregate on the order of 100 pb, while online data storage systems at google can be estimated at over tens of exabytes,6 two orders of magnitude larger. in addition, the architectures at internetscale commercial companies are designed for the continuous updating and reanalysis of data sets that can be tens to hundreds of petabytes in size, something that is again rare in the research environment.7 presently costing several hundred million dollars, an exabyte of storage will become affordable for science applications within a few years because both disk and tape storage are still following an exponential increase in density and reduction in cost. however, bandwidth to the data will likely remain expensive, and it must be borne in mind that any signicant analysis of an exabyte data set implies exascale computation. over time, it seems reasonable to expect researchers to adopt industry use patterns as the necessary software is written. for example, researchers might analyze aggregated video streams to understand social behavior, with much of the large volumes of data not being retained for long.5.5 tradeoffs between support for production advanced computing and preparing for future needsgiven the high demand for advanced computing, it will be essential for nsf to focus on and devote the majority of investments to provide production capabilities in support of its advanced computing roadmap. production support is needed for software as well as hardware, to include community software as well as frameworks, shared elements, and other supporting infrastructure. nsf™s software infrastructure for sustained innovation (sisi) program is a good foundation for such investments. however, sisi needs to be grown in partnership with nsf™s science directorates to a scale that matches need, where it can then be sustained essentially indenitely. the united kingdom™s collaborative computational projects (ccps) provide examples of the impact and successful operation of communityled activities that now span nearly four decades. produc6 precise gures are not available, but a plausible estimate can be found in what if?, ﬁgoogle™s datacenters on punch cards,ﬂ https://whatif.xkcd.com/63/, accessed january 29, 2016.7 in highenergy physics analyses, enormous data sets are frequently reanalyzed in their entirety, but typically written only once. future directions for nsf advanced computing infrastructure to support u.s. science and engineering in...copyright national academy of sciences. all rights reserved.92 future directions for nsf advanced computing infrastructuretion support is further needed for data management; curation, preservation, archiving, and support for sharing all need ongoing investment. this balance is re˚ected in the example in section 5.7. however, if nsf invested solely in production, it would miss some key technology shifts, and its facilities would become obsolete quickly. some innovation takes the form of netuning of production systems, but modest, directed investments in exploratory or experimental facilities and services are also needed to create, anticipate, and prepare for technology disruptions. nsf needs to play a leadership role in both dening future advanced computing capabilities and enabling researchers to effectively use those systems. this is especially true in the current hardware environment, where architectures are diverging in order to continue growing computing performance. such investments would include (1) research into how to use and program novel architectures and (2) research into how applications might effectively use future production systems. in the rst category, longerterm, curiositydriven research likely belongs as part of the computer and information science and engineering directorate™s research portfolio rather than nsf™s advanced computing program, which would be focused on roadmapdriven experimentation. leadership by nsf will help ensure that its software and systems remain relevant to its science portfolio, that researchers are prepared to use the systems, and that investments across the foundation are aligned with this future. the range of possible options for advanced computing is growing as new architectures for analyzing data, increasing computing performance, or managing parallelism are introduced. one associated risk is that investments end up spread across too many emerging technologies, fragmenting the investment portfolio and reducing the ability to make investments at the scale needed for production capabilities. another risk is that the criteria used to select among the technologies do not adequately re˚ect realistic science requirements, as can happen when overly simplistic benchmarks are used, leading to acquisition of systems that fall short in serving the research community. as new technologies offering greater performance or other new capabilities begin to mature, decisions must be made about when to shift investments in the new direction. many applications will benet from higher performance, some applications may not need more performance, and one also expects new applications to emerge when higher performance thresholds are reached. it may be worthwhile to push aggressively into higher performance to enable some new applications, even if other applications take a long time to exploit the new architectures, or never do so.today, accelerators, including generalpurpose gpus and other techfuture directions for nsf advanced computing infrastructure to support u.s. science and engineering in...copyright national academy of sciences. all rights reserved.investment tradeoffs in advanced computing 93nologies that are flop/srich but memorypoor and, possibly, hardtoprogram can provide very high performance at reduced cost for a subset of applications. this can create tension between, on the one hand, moving forward aggressively with these technologies to obtain higher performance, thereby putting pressure on researchers to transition their software and algorithms to use the technologies more quickly, and, on the other hand, allowing sufcient time (and resources) for researchers to undertake such transitions. one possible indicator would be the level of active research on how to use a new architecture effectively. a high level might indicate that it is premature to consider the architecture ready for production systems. this indicator is not perfect; for example, there is still active research on how to use cache effectively. more generally, the requirements expressed in the advanced computing roadmaps can serve as a guide to when technologies are ripe for transition from exploratory to production status. a requirements analysis is necessary to reveal the tradeoffs implicit behind any such investment in nsfwide infrastructure.a 10year roadmap would extend well into the exascale era. by focusing on its advanced computing roadmap rather than the rst exascale system, nsf will ensure its investments have longterm benet and will also assist the wider community in understanding and navigating the associated technology transitions. although exascale systems may seem remote or even irrelevant to the majority of (but certainly not all) nsf users, technology advances in areas like energy efciency needed for exascale capability will change the hardware and software landscape and have bearing on the purchase and operational costs of the aggregate capability nsf will need in the future. it will thus be important for nsf and the research users it supports to be involved in the national discussion around exascale and other futuregeneration computing, including through the recently announced national strategic computing initiative, for which nsf has been designated as a lead agency.at the same time, it will be especially important that nsf not only is engaged, but is actually helping to lead the national and international activities that dene and advance future software ecosystems that support simulation and datadriven science. this includes active participation in and coordination of the development of tools and programming paradigms and the software required for exascale hardware technologies. the department of energy (doe) is currently investing heavily in new exascale programming tools that, through the scale of investment and buyin from system manufacturers, could plausibly dene the future of advanced programming even though the design may not re˚ect the needs of all nsf science because the centers and researcher communities it supports are not formally engaged in the specication process. future directions for nsf advanced computing infrastructure to support u.s. science and engineering in...copyright national academy of sciences. all rights reserved.94 future directions for nsf advanced computing infrastructure5.6 configuration choices and tradeoffsthere are many choices and tradeoffs to consider in allocating resources to computing infrastructure. this chapter has discussed several key tradeoffs in detail, but there are many others. whenever considering tradeoffs, it is important to keep in mind that designing for a broader overall work˚ow almost certainly means conguring a system that is not perfect for all individual work˚ows; rather, it is able to run the entire work˚ow more effectively than other congurations. thus, simply maximizing the performance or capability of one aspect, such as ˚oatingpoint performance or data handling capacity, will not provide useful guidance. 5.6.1 capability can be used for capacity but not vice versaperhaps one of the most important items to consider is that not all computing resources are interchangeable. this may seem obvious, but it is often forgotten when computation is described in term of peak flop/s, cores, or memory size. in addition, some computations (again, both computecentric and datacentric computations) are infeasible on systems smaller than a certain size. for example, many simulations require large amounts of memory (in the hundreds of terabytes to 1 petabyte [1 tb = 1012 bytes; 1 pb = 1015 bytes]), frequent exchanges of data, and terabytes to petabytes of data storage for both input and output data. today, such simulations can only be run on leadershipclass systems, such as nsf™s blue waters or doe™s mira and titan systems. a simulation attempting to run on a system with a slower network will spend most of its time waiting on data to arrive (while still occupying most of the system memory); on a smaller system, there will not be enough memory to start the application.8 thus, without a system with these characteristics, such simulations cannot be performed.on the other hand, large, capable systems can be used effectively for applications with smaller requirements. one argument that is sometimes made is that leadershipclass systems ought to be used only for applications that require or can make good use of their unique capabilities. this is overly simplistic and is not looking at the overall objective, which, in essence, is to accomplish the greatest amount and most valuable science within the available budget (or with a minimum of cost and risk). note that while it might be possible to run smaller jobs at slightly lower cost on a less capable system, the cost advantage is likely to be small given the 8 in principle, outofcore techniques, or even virtual memory approaches, could be used to address the lack of sufcient fast memory. but in practice this has the same problem as a tooslow networkšthe application might run, but it would run so inefciently as to be impractical (and costly, since it would tie up the system for a very long time).future directions for nsf advanced computing infrastructure to support u.s. science and engineering in...copyright national academy of sciences. all rights reserved.investment tradeoffs in advanced computing 95signicant economies of scale that can be realized in large systems. the goal is to be costeffective over the entire portfolio of applications, not to optimize for each individual application. moreover, if a large system is running only large jobs, then there are likely to be many unutilized nodes, because a few large numbers of nodes are unlikely to sum up to the total system size. small jobs can improve utilization and, thus, have a small marginal cost.although this report avoids the terms capability (instead referring to leadershipclass systems) and capacity (systems that can run large numbers of jobs, none of which require a leadershipclass system), this point can be most concisely expressed as ﬁcapability can be used for capacity but not vice versa.ﬂ this critical point, re˚ected in recommendation 2.2, calls for nsf to operate at least one leadershipclass system so that the science that requires such systems can continue to be conducted.as discussed above, a system that is optimized for datadriven science that requires processing (and reprocessing) large numbers of mostly independent data records needs capabilities not required on systems optimized for simulation. in particular, there is a greater need for a large amount of persistent storage; it may also be important to prefer higher bandwidth to independent storage devicesšfor example, having large numbers of compute nodes, each with several disks, over a unied system that provides access of all data to all nodes, as is common on ﬁclassicﬂ supercomputers. in a perfect world, nsf could deploy several such systems, each optimized for a different workload. unfortunately, in a budgetconstrained environment, nsf will need to make some tradeoffs and, in particular, consider alternative approaches to provisioning the necessary resources.although there are clearly applications that are dominated either by ˚oatingpointintensive work or by dataintensive work, there are many problems that require a combination of capabilities. for example, some forms of graph analytics require the same sort of lowlatency, highbandwidth interconnect used in leadershipclass hpc systems. in fact, the systems that dominate the graph500 benchmark9 are all large hpc systems, even though this benchmark involves no ˚oatingpoint computation. similarly, there are other features, such as large memory size, high memory bandwidth, and low memory latency, that are desirable in leadershipclass systems for a wide range of problems, be they datacentric or simulation/computecentric. thus, it is best, as illustrated in figure 5.2, to consider leadershipclass systems as a spectrum of systems with different emphases. with enough funds, several largescale systems could be deployed, each making different tradeoffs in this space of con9 see the graph500 website at http://www.graph500.org, accessed january 29, 2016.future directions for nsf advanced computing infrastructure to support u.s. science and engineering in...copyright national academy of sciences. all rights reserved.96 future directions for nsf advanced computing infrastructureguration parameters. one can have several largescale systems making different tradeoffs, or different subsystems of a coupled system that make different tradeoffs. if simulations increasingly ingest experimental data and increasingly require in situ analysis, then the latter solution may work better.arguments like these suggest that an economy of scale implies that all resources should be centralized to gain maximum efciency from the system. however, this is not correct, because it takes more than just hardware to provide effective advanced computing. instead, a balance needs to be struck that provides resources large enough to tackle the critical science problems that the nation™s researchers face while also providing systems tuned for different workloads and the expertise to ensure that these scarce and valuable resources are effectively used. it is also important to have several centers of expertise to ensure that the community has access to several different perspectives. an example of a possible set of tradeoffs is given at the end of this chapter.deploying a ˚exible hardware platform capable of addressing a wide range of datacentric, highperformance, and highthroughput work˚ows is just the rst step. also essential are deploying and supporting the associated software stacks and addressing the challenges and barriers faced by researchers and their communities who will use the systems for their research and education.5.6.2 trading flop/s for data handling and memory size per requirements analysisin the short run, even as it develops a more systematic requirements process, nsf needs to ensure continued access to advanced computing resources (which include both data and compute and the expertise to support the users), informed by feedback from the research communities it supports. in the longer run, it is essential that nsf use a robust requirementsgathering process to guide the selection of system congurations needed to ensure continued access. this will necessarily involve tradeoffs of different capabilities, as each choice will have a signicant cost. while this must be driven by the requirements analysis, one likely tradeoff will be to improve data handling and memory size at the expense of peak flop/s. some nsf systems have already done this; blue waters, with large amounts of memory, high input/output (i/o) performance, and a large number of conventional cpus to support existing applications, is a good example. wrangler is another good example of this tradeoff in practice, although at a much smaller scale than blue waters. note that the conguration of blue waters was guided by a process that required demonstrated performance on full applications, including reading input data and writing results to les, rather than just benchmarks and that this future directions for nsf advanced computing infrastructure to support u.s. science and engineering in...copyright national academy of sciences. all rights reserved.investment tradeoffs in advanced computing 97signicantly in˚uenced the conguration of the system. the roadmapping process recommended in this report would ensure that future systems would be similarly aligned with the needs of the community.5.6.3 tradeoffs associated with rewriting code for new architectureswhen considering these tradeoffs, it is important to consider the tension between maintaining compatibility with legacy applications and providing the highest performance for new applications. note that there is a huge investment in scientic software that is not only written but also tested and (perhaps) understood. this code base cannot be rewritten without a signicant investment in time and money. the nancial cost is real and must be considered when evaluating the cost advantage of a new architecture. at the same time, if a new architecture is likely to persist, then that cost will only need to be paid once. an example of a new architecture that required many applications to be rewritten is the successful adoption of distributed memory parallel computers, along with messagepassing programming, more than 20 years ago, which enabled an entire class of science applications.a related issue is the one of scientist productivity versus achieved application performance balanced with efcient use of expensive, shared computational resources. as this report stresses, the goal is to maximize the science that is enabled and supported by advanced computing. an individual scientist may rightfully be focused on the fastest path to discovery and not be concerned about computational performance unless it is essential to completing the computations with available resources or time, such as is the case for the massively parallel applications running on blue waters. however, efcient utilization and maximum scientic productivity of a fully allocated, shared facility requires that the majority of cycles are consumed by welloptimized software. as systems become increasingly complicated and hard to use effectively, a burden has been put on the science teams as well as the computing facilities to create and maintain application codes that run efciently on a range of systems. many users are concerned about the difculty in moving their codes to new architectures. in the short run, this means that production systems cannot be predicated on users needing to rewrite their applications to use new architectures. they also cannot depend on unproven software technologies to make existing or new applications run efciently on new architectures. this concern with productivity also applies to new applications. not all architectures are easy to use efciently, and some algorithms remain very challenging to parallelizešfor example, parallelization in time.these observations relate to the relatively short run. however, nsf future directions for nsf advanced computing infrastructure to support u.s. science and engineering in...copyright national academy of sciences. all rights reserved.98 future directions for nsf advanced computing infrastructurealso needs to be planning well into the future for a postcmos [complementary metaloxide semiconductor] era. here, the divisions of the computer and information science and engineering directorate other than the division of advanced cyberinfrastructure (aci) can play a role by ensuring that the requirements of the science community are included in computer science and engineering research on future device technologies and architectures. 5.6.4 tradeoffs between investments in hardware and in software and expertisefollowing on the theme of maximizing the science, today™s hardware is challenging to use efciently and, despite many attempts and interesting ideas, this is unlikely to change. nsf has already established several services that support application developers in making better use of the systems, both for xsede and for the prac teams on blue waters. the initial investments in the sisi program are a good start. they have the potential for broad impact if the investments reach sufcient scale, are sufciently focused on the nittygritty of improving the engineering of codes, and are sustained over a sufcient period, if not indenitely, with both external review and communitybased requirements analysis being essential ingredients. recent nsfsponsored work10 points to plausible mechanisms that could be adopted to assess the science impact of software as well as establish directions and locations for future investments. investments in future hardware must continue to be considered together with support for using those systems, and that support must be organized for effective delivery.5.6.5 optimizing the entire science workow, not the individual partsfurthering the topic of getting the most science from the system, it is important to optimize for the entire scientic work˚ow, not just each part separately. this is for two reasons: rst, as is well known, the global optimum is often not made up of a number of local optima. second, it may not be possible to afford an optimal solution for each part of the problem. an example of a common yet incorrect tradeoff is to design a system to meet the ˚oatingpoint performance needs of a benchmark that is thought to represent an application. yet in practice, the full application may require le i/o, memory bandwidth, or other characteristics. in 10 j. howison, e. deelman, m.j. mclennan, r. ferreira da silva, and j.d. herbsleb, understanding the scientic software ecosystem and its impact: current and future measures, research evaluation, 2015, doi:10.1093/reseval/rvv014.future directions for nsf advanced computing infrastructure to support u.s. science and engineering in...copyright national academy of sciences. all rights reserved.investment tradeoffs in advanced computing 99addition, the science may require running pre and postprocessing tools, visualization systems, or data analysis tools. it is critical that the entire work˚ow be considered. note that the blue waters procurement was one of the few for leadershipclass systems that required overall application performance, including i/o, as part of the evaluation criteria; as a result, this system has more i/o capability than most systems with the same level of ˚oatingpoint performance and is, in fact, as powerful for i/o operations as the leadershipclass systems planned by doe for 20162017.5.6.6 generalpurpose versus specialpurpose systemsthere are some applications that on their own use a signicant faction of nsf™s advanced computing resources. it may make sense, based on an assessment of the science impacts, to dedicate a system optimized for those applications (either together or singly) and provide a generalpurpose system that can handle (most/many) of the remaining application areas that require a leadershipclass system. for example, such systems may have smaller pernode memory requirements or pernode i/o performance; they may require simpler communication topologies but place a premium on the lowest possible internode communication latency. similarly, as discussed above, an architecture focused on data volume will devote a much higher part of its cost to i/o than a system focused on compute or communication. while it may still be more cost effective to have a single machine that is good at all aspects of advanced computing (the convergence approach), it is essential that options that consider a small portfolio containing either specialized machines or access to time on specialized systems be considered.5.6.7 midscale versus highend systemsimportant scientic discoveries are made not just at the high end of the compute and dataintensive scales, but also in the midscale and low end. because of the improvement in software applications and tools and accessible training, there is a growing demand for the use of midscale advanced computing infrastructure. work at the midrange produces a large number of scientic publications and supports a large scientic community. the requirements for midscale computing will only grow as improvements in software make it easier and easier to take advantage of these resources. the hadoop software ecosystem provides an interesting example of what is needed for midscale systems to become widely usable. traditional hpc clusters, around since the mid1990s, were challenging to use until the message passing interface (mpi, a standardized messagepassing system that runs on a wide variety of parallel computers) matured, softfuture directions for nsf advanced computing infrastructure to support u.s. science and engineering in...copyright national academy of sciences. all rights reserved.100 future directions for nsf advanced computing infrastructureware was developed that could leverage it, and students were trained to use it. similarly, it was not until hadoop emerged and began to mature that the same clusters could be easily used for dataintensive computing (especially of unstructured data). a hadoop software ecosystem had to be developed and students trained to use it. demand is just beginning to rise as this dataintensive ecosystem matures and researchers are trained to use it. as advanced computing technology advances, midscale users will benet from work to develop easily used software and standardized congurations that can be scaled to different sizes and thus readily reproduced to serve larger communities through foundation, university, and industry partnerships.5.7 example portfolionsf needs to act now in acquiring the next generation of computing systems in order to continue supporting science. the following is just an example of the sort of portfolio for hardware, together with supporting expertise, that nsf could consider, along with some explanations for the choices. this is not a recommendation; rather, it is an illustration of some of the options with the rationale behind them.1. one or two leadershipclass systems, congured to support data science, traditional simulation, and emerging uses of largescale computation. such systems are needed to support current nsf science; by ensuring that there is adequate i/o support, as well as interconnect performance and memory, such a system can also address many data science applications. these systems must include support for experts to ensure that the science teams can make efcient use of these systems. continuity of support for advanced computing expertise is essential because people with these skills are hard to nd, train, and retain. note also that these systems may not be optimal for any one workload, but can be congured to run the required applications more efciently than other choices. also, these systems should not be limited to running only applications that can run nowhere else; to ensure the most effective use of these resources, they should be used for a mixture of what might be called capability and capacity jobs, with priority given to the jobs that cannot be run on any other resources. in the case where funding is extremely tight and only one system is possible, that system must complement other systems that are available to the nation™s scientists, such as those operated by doe, and memoranda of understanding among agencies may help ensure that the aggregate needs of the research community are met. 2. a cooperative arrangement with one or more operators of largescale future directions for nsf advanced computing infrastructure to support u.s. science and engineering in...copyright national academy of sciences. all rights reserved.investment tradeoffs in advanced computing 101clouds. these are likely to be commercial clouds that can provide some access to a system at a different point in the conguration space for a leadershipclass data system. this addresses the need for access to extremely large systems optimized for this class of dataintensive research. conversely, because the commercial sector is rapidly evolving and scaling out these largescale clouds, it makes sense to lease the service rather than attempt to build one at this time.11 note also that the commercial sector is investing heavily in applied research for these platforms, which suggest that nsf emphasize support for basic research.3. a number of smaller systems, optimized for different workloads, including support for the expertise to use them effectively. it is important to have enough providers to provide distributed expertise as well as two types of workforce development: training for staff and training for students. currently, xsede effectively provides this access to smaller systems optimized for different workloads. this capability is essential in supporting the breadth of use of advanced computing in nsf.4. a program to evaluate experimental computer architectures. this effort would acquire small systems (or acquire access to the systems without necessarily taking possession of them) and work with the research community to evaluate the systems in the context of the applications that nsf supports.12 this program will help inform the future acquisitions of the systems in the above three points, as well as inform basic research problems in computer and computational science, such as programming models, developer productivity, and algorithms. this approach differs from research testbeds for basic computer science; while important, those testbeds should be dened by the particular research divisions that need them.5. a sustained sisi program. continue to learn from the sisi program and apply lessons learned to longterm investments in software.11 once nsf is using a large amount of time on a cloud, the cost of contracting with a service provider will need to be compared to the cost of operating its own cloud system. many of the economies of scale that work for the cloud providers are applicable to nsf; the decision should be made based on data about the total costs.12 some centers are already evaluating systems with nsf and external funding. tacc supports hikari (funded by hewlett packard and ntt) for exploration of the effectiveness of direct highvoltage dc in data centers supplied by solar power, and catapult (microsoftfunded) evaluates the effectiveness of a specic eldprogrammable gate arraybased infrastructure for science. other centers are conducting similar activities. the beacon system at the national institute for computational sciences, partly funded by nsf, provided access to intel xeon phi processes before they were deployed in production systems by tacc. the team that proposed beacon included researchers from several scientic disciplines, including chemistry and highenergy physics. future directions for nsf advanced computing infrastructure to support u.s. science and engineering in...copyright national academy of sciences. all rights reserved.6range of operational modelsthe national science foundation™s (nsf™s) current model of cyberinfrastructure, including advanced computing, is based on a mix of centralized and distributed funding, anchored by the division of advanced cyberinfrastructure (aci) within the directorate of computer and information science and engineering (cise). previously, aci was the ofce of cyberinfrastructure (oci), reporting to the director. this central structure currently supports the blue waters facility (a leadingedge facility) and a set of smaller computing and storage resources via the extreme science and engineering discovery environment (xsede). in addition to these centrally funded resources, the geosciences directorate operates advanced computing facilities at the national center for atmospheric research (ncar), and it and other nsf directorates fund cyberinfrastructure via a variety of programs.advanced computing shares many elements of other nsf infrastructure investments, but it also differs in some profound ways. first, unlike advanced telescopes or particle accelerators, where there is no competing commercial market, a vibrant computing industry develops new technologies and products and responds to market needs and opportunities that dwarf computing expenditures in academia and by federal research sponsors. second, computing market shifts and the welldocumented, rapid evolution of computing technology mean that researcher expectations and economically viable computing technologies change every few years. consequently, advanced computing capital assets have a very short operational lifetime, in marked contrast to many other scientic instru102future directions for nsf advanced computing infrastructure to support u.s. science and engineering in...copyright national academy of sciences. all rights reserved.range of operational models 103ments. these shifts, however, do not mean that longterm planning is unnecessary or impossible. businesses and academia regularly develop strategic information technology (it) plans that accommodate technology shifts.third, advanced computing is distinguished by its universality; it is applicable to all scientic and engineering domains, spanning data capture and analysis, simulation and modeling, and communication and collaboration. fourth, and consequently, demand for advanced computing continues to grow rapidly, placing increasing stress on the nancial models and social processes used to support research cyberinfrastructure. although states, universities, and companies have long subsidized the capital and operating costs of nsf™s leadingedge advanced computing, those costs have now reached tens to hundreds of millions of dollars. consequently, the willingness of these parties to engage in ﬁpay to playﬂ (i.e., accept losses in exchange for publicity or collateral institutional advantage) has declined accordingly. 6.1 goals and opportunitiesthe unique attributes of advanced computing create both opportunities and challenges for any nsf strategy, requiring both nimbleness in the face of changing technologies and economics and stability to ensure sustained capabilities and research continuity. the following basic principles will help ensure the sustainability of nsf™s advanced computing strategy:realistic business assessment that exposes the true costs and subsidies of cyberinfrastructure deployment and operation at all scales; identication and tracking of technology trends and economics, along with the research opportunities they create;longterm planning and articulated strategy (a roadmap) that allows the broad research community and service providers to plan accordingly;balanced support for computing hardware, storage systems, and networks, along with professional staff, software and tools, and operating budgets; andnsfwide commitment to cyberinfrastructure investment, strategic directions, and operational processes.three crosscutting aspects of sustainability are particularly crucial: continuity, coverage, and skills. future directions for nsf advanced computing infrastructure to support u.s. science and engineering in...copyright national academy of sciences. all rights reserved.104 future directions for nsf advanced computing infrastructure6.1.1 service continuity and adaptabilityservice continuity encompasses longterm strategic planning and sustainability on a decadal or longer timescale. nsf™s major research equipment and facilities construction (mrefc) projects for scientic infrastructure typically involve years of planning. today, nsf™s cyberinfrastructure facilities are rarely used to support computational modeling and data analysis for mrefc projects. the former have lifetimes of just a few years, making it impractical for mrefc project leaders to reduce overall costs of advanced computing by including nsf™s own cyberinfrastructure facilities on the mrefc operational plan. this must change if common cyberinfrastructure is to support mrefc projects and other longterm community research.historically, most research data has been produced by carefully planned experiments, and it has been both expensive to capture and highly guarded by the researchers who produced it. ubiquitous, inexpensive sensors and a new generation of largescale scientic instruments, including mrefc infrastructure, have changed the economics of data capture and are shifting scientic expectations about data retention and community sharing. although nsf™s recent requirement that all nsffunded research projects have a data management and accessibility plan is an explicit policy recognition of data™s importance, there is no nsfwide cyberinfrastructure strategy or program to support disciplinary or crossdisciplinary data sharing and preservation. hence, much of the data preservation responsibility and nancial burden rests on individual investigators and their home institutions. today, when the cognizant investigators no longer perceive value in retaining the data, those data are often lost. this is increasingly problematic as the longerterm research value of data often accrues to those in other disciplines. 6.1.2 service coverage: breadth and depthin its earliest form, cyberinfrastructure was synonymous with highperformance computing and computational science. today it encompasses not only highperformance computing but also largescale data archiving and analytics, software codes and tools, and human expertise and computingmediated research and discovery. orthogonally, cyberinfrastructure spans the capabilities and needs of individual investigator laboratories, campus sites, regional and national research facilities, and commercial cloud service providers. any comprehensive cyberinfrastructure strategy must include the entire spectrum of services and span the entire range of organizational future directions for nsf advanced computing infrastructure to support u.s. science and engineering in...copyright national academy of sciences. all rights reserved.range of operational models 105scales. it cannot be simply about leadingedge supercomputing platforms or just about big data analytics; it must integrate both at multiple scales. nor can it focus on hardware infrastructure while neglecting both software development and maintenance and training and support of technical expertise. it must balance sustainability against adaptation, recognizing that community needs evolve and technology shifts drive new solutions. the rise of ﬁbig dataﬂ as a cyberinfrastructure challenge that rivals the scale and complexity of advanced scientic computing is indicative of this need for community adaptation. to respond appropriately to this technology shift and opportunity, nsf must adapt its investments and infrastructure. big data will require big infrastructure, just as leadingedge computational science does, and will likely involve a mix of both centralized facilities and decentralized repositories at universities. the australian eresearch initiative and its australian national data service is a relevant example. in this context, the nsf community would benet from a coherent, big data retention and preservation strategy and capability, one that balances investigator and disciplinary differences against communal benet and research collaborations. unfunded mandates for retention and preservation will not be workable. a balanced model is likely to require greater total funding, a better balance of capital and operating budgets, more focus on business practices and return on research investment, and greater coordination across nsf directorates. 6.1.3 skills and workforcesustainable and effective cyberinfrastructure depends critically on the skills and expertise of domain scientists and of committed and welltrained advanced computing professionals. even if they are not directly responsible for code development and work˚ow management, scientists using advanced computing need to be generally knowledgeable about these matters. for their part, technical staff members not only deploy and operate facilities, but also support community toolkits and codes, serve as keepers of institutional knowledge and expertise, and manage and ensure data security and provenance. unlike hardware, with a lifetime of a few years, the human infrastructure of people™s experiences in operating such systems has a lifetime of decades. despite their importance, these staff often lack clear academic career paths and are dependent on an uncertain stream of funding for support. given the global competition of computing and computational science talent, any cyberinfrastructure plan must include mechanisms that recognize and reward professional staff and ensure they have career opportunities that retain their talent within the academic community. one future directions for nsf advanced computing infrastructure to support u.s. science and engineering in...copyright national academy of sciences. all rights reserved.106 future directions for nsf advanced computing infrastructureimportant contribution to retaining and rewarding this skilled workforce is stability in funding for centers, recognizing that developing an expert staff is a longterm process that can be wasted with even a shortterm gap in staff funding. programs are also needed to train future computational science and data analytics experts. the report of the nsf task force on cyberlearning and workforce development1 addressed this issue in depth and includes, more broadly, the use of computerbased approaches in learning and recognizes the need to train both the workforce that supports advanced computing and the practicing scientists who make use of advanced computing. note that the effective use of advanced computing systems requires specialized and advanced training. nsf computing centers and other centers of advanced computing expertise (academic departments involved in advanced computing, national laboratories, and private industry) have leveraged their inhouse expertise to offer such training. examples include training programs for users offered by xsede and blue waters and the argonne training program in extreme scale computing. such programs could benet from a more formal approach and, in particular, longterm support for training materials and resources.the pervasive nsfwide and nationwide nature of advanced computing presents a perhaps unique opportunity, and responsibility, to pursue nsf™s diversity and inclusion goals.2 this includes ensuring the broadest possible benet from and access to nsf™s cyberinfrastructure, as well as translating this participation into creating and sustaining a computationally skilled workforce that re˚ects our nation. xsede has made signicant progress in increasing the number of underrepresented minority and women users and, more notably, principal investigators (pis) with allocations. the successful xsede campus champions program is a human network, which, while pursuing its primary mission of ﬁempowering campus researchers, educators, and students to advance scientic discovery,ﬂ3 also serves other missions including advancing diversity through increased awareness, training, and education. increased access to statistics and metrics, concerning not just pis and users but also those accessing online materials or participating in events or using other services, could better inform and guide actions by nsf, xsede, and 1 national science foundation, advisory committee for cyberinfrastructure, task force on cyberlearning and workforce development final report, march 2011, https://www.nsf.gov/cise/aci/taskforces/frontcyberlearning.pdf.2 national science foundation, diversity and inclusion strategic plan 20122016, http://www.nsf.gov/od/odi/reports/strategicplan.pdf, accessed january 29, 2016.3 xsede, ﬁcampus championsšoverview,ﬂ https://www.xsede.org/campuschampions, accessed january 29, 2016.future directions for nsf advanced computing infrastructure to support u.s. science and engineering in...copyright national academy of sciences. all rights reserved.range of operational models 107the community, and xsede is already working toward increased public access to data. 6.2 organizational challenges and community needsalthough nsf™s current mix of centralized and distributed cyberinfrastructure has had many notable successes, it is not without problems, both for infrastructure providers and for the research community. some of these problems are rooted in history, some are embedded in the nsf culture, and some are consequences of nsf™s organizational structure.6.2.1 competitive challengesfrom its origins, nsf™s advanced computing programsšthe original 1980s supercomputer centers program, the 1990s partnership for advanced computational infrastructure (paci) program, the 2000s distributed and extensible terascale facilities, and now xsedešhave all been based on a repeated cycle of competitions to host and operate largescale cyberinfrastructure. this cycle continues to pit putative operatorsšuniversities and national laboratoriesšagainst one another in irregularly scheduled ﬁwinner take allﬂ competitive battles. in each case, competitors build ad hoc hardware and software vendor alliances to mount proposals. to compete, they also leverage institutional funds to cover facility, hardware, and operations costs (which are capped in the competitions as a percentage of hardware costs). much of this difculty is rooted in the lack of distinction between research and infrastructure funding. each has widely differing timescales and success metrics. not only does repeated infrastructure competition on 2 to 5year cycles create strong disincentives for national collaboration, it convolves performance review, recompetition, and strategic planning in ways that are challenging for all. in addition, it leads to proposals designed to win a competition rather than maximize community scientic returns. for example, it places a premium on sometimes unproven, nextgeneration technology that can serve as a vendormarketing showpiece, rather than on proven, productionquality infrastructure, and researchers have little input into vendor selection, conguration options, or service models. (there is a role for facilities to test novel and risky computing technologies, but it is not in production systems.)researchers whose work depends on access to shared facilities also face a form of ﬁdouble jeopardy.ﬂ the scientic merit of their proposed work is assessed via the standard peer review process. however, if funded, they are still not assured of access to the computing and storage resources future directions for nsf advanced computing infrastructure to support u.s. science and engineering in...copyright national academy of sciences. all rights reserved.108 future directions for nsf advanced computing infrastructurethey need to conduct their research. a separate proposal for shared cyberinfrastructure access is conducted by either the xsede resource allocation committee (xrac) or the petascale computing resource allocations committee (prac) to assess the competence of the researcher and his/her team to use the cyberinfrastructure resources efciently. however, there is little operational followup to ensure the resources are in fact used wisely and efciently. this is especially problematic because the monetary value of computing resource awards continues to increase.finally, as discussed earlier, the current model is structured largely in support of individual investigator and small team projects, with a nominal 3year lifetime. larger disciplinary projects and major scientic instruments (e.g., nsf mrefc projects or crossagency partnerships) with longer production cycles have no mechanism to plan for and request cyberinfrastructure for a 10 or 20year horizon, because there is no guarantee that any of the extant cyberinfrastructure facilities will still be operational. this adversely affects data preservation activities in particular, because, by denition, they target longterm access.6.2.2 structural challengessince the beginning of the nsf supercomputing centers program in the 1980s, nsf aci and its predecessor organizations have supported computational science research across nsf and provided services to a user base that spans all federal research agencies. despite the clear recognition that computational science and data analytics are true peers with theory and experiment in the scientic process, nsfwide coordination and support remain somewhat informal and ad hoc, with directorate participation often a secondary responsibility of the designees.although researchers in all nsf directorates are critically dependent on cyberinfrastructure, at present there are no formal mechanisms for coordinated strategic planning, nor are there ready ways to pool and disburse shared resources. concretely, there are no shared negotiations for discounted infrastructure or services, nor an accepted strategy for prioritizing the balance of individual investigator, campus, and shared infrastructure. nsf would benet from a formal roadmapping committee for cyberinfrastructure with representatives drawn from all directorates and shared responsibility for crossdirectorate resource investment and strategy. in addition, it is crucial that advanced computing be treated as an nsf asset and funded accordingly, regardless of its organizational location. the need is too great and current resources are too limited for loosely coordinated action and reactive processes.one corollary to the need for strategic coordination is scaling and scoping to match available resources. as a decentralized organization, future directions for nsf advanced computing infrastructure to support u.s. science and engineering in...copyright national academy of sciences. all rights reserved.range of operational models 109with frequent rotation of program ofcers, nsf regularly launches new programs and initiatives. for research, this is the distinguishing characteristic of nsf; it is community driven and adaptive. for infrastructure, this is often debilitating, because it leads to a proliferation of small efforts and projects that consume critical resources. when building and operating infrastructure, it is critical to do a small number of things extremely well. successful infrastructure is derived from a sustained strategy and driven by relentless focus. the implication for nsf is clear. given limited cyberinfrastructure resources, it must do a very small number of things extremely well, avoiding mission creep and resource dilution at all costs. a second and equally important corollary is an integrated strategy for highperformance computing and big data analytics and a concomitant rebalancing of investments. big data requires strongly coordinated big infrastructure, just as leadingedge computational science requires advanced computing systems. the lessons of commercial cloud computing are clear; centralization and scale create unprecedented opportunities for innovation and discovery. clear and unambiguous requirements for data deposit and access are also needed. only via such a mechanism, developed in broad community consultation, can the true benets of data analytics be realized.6.3 potential sustainability approachesas the scale and scope of advanced computing demands and associated facilities and services have grown, the irregular, winnertakeall process described above has become more problematic. first, the scale and cost of highend or leadershipclass facilities needed to meet researcher demands is a large fraction of the total currently available in the nsf budget, whether within the aci division budget or the budgets of other directorates. (whether nsf needs a leadershipclass or highend system should be determined by the analysis of science requirements.) nsf could afford to purchase a signicantly larger system than it is currently acquiring, but only by focusing on that investment rather than a larger number of much smaller investments. second, uncertainty regarding the timing and capability of infrastructure upgrades makes community planning difcult, and the timing is often not well matched to vendor hardware and software upgrade cycles. third, the timescales are incompatible with the planning and life cycle of other scientic infrastructure, making use of centrally funded cyberinfrastructure difcult at best and often impossible.current models of funding for advanced computing (based on periodic recompetition) and service block allocations (via committee) create substantial uncertainty regarding service continuity and research access. future directions for nsf advanced computing infrastructure to support u.s. science and engineering in...copyright national academy of sciences. all rights reserved.110 future directions for nsf advanced computing infrastructurethere are several ways to address these shortcomings while retaining the best elements of the current approach. these include approaches as varied as publicprivate partnerships for access to cloud services, federally funded research and development centers (ffrdcs) for organizational sustainability, and mrefc projects for facility construction. many of these are not mutually exclusive and could be combined to address limitations of the current model.6.3.1 a regular cadence of infrastructure investments the cost of leadingedge advanced computing facilities and user support, whether for computational modeling or data analytics, is no longer measured in tens of millions of dollars. rather, the costs are now denominated in hundreds of millions of dollars. indeed, largescale commercial data centers operated by cloud providers now cost over $1 billion each. the mrefc process may be a useful point of departure. although there are some aspects of mrefc projects that match the needs of advanced computing infrastructure, the current mrefc mechanisms may need to be modied and adapted to the unique needs of advanced computing infrastructure, including the general nature of computing and the need for regular refresh of computing equipment.to establish a regular cadence of infrastructure investments, nsf would plan and budget an upgrade every 3 to 5 years, with planning and construction of each generation overlapping the operation of the previous generation. this would clarify and systematize the technology upgrade and refresh process, provide a community mechanism to plan and shape infrastructure transitions, elevate budget planning and prioritization to nsfwide discussion and approval, and provide the level of funding needed to maintain leadingedge capability. as with mrefc projects, nsf would be able to request new funds as a line item in its annual budget request, explicitly acknowledging that that current, internal funding is inadequate to meet burgeoning need and scientic priorities. finally, it would provide an operational instantiation for an nsfwide advanced computing roadmap.6.3.2 leased infrastructure historically, nsf cyberinfrastructure facilities have been operated by academic institutions on nsf™s behalf, typically via cooperative agreements. in turn, the academic institutions have purchased computing, storage, and networking hardware from computing vendors at the start of the cooperative agreement to deliver the committed services. this hardware then depreciates over its nominal 3 to 5year lifetime until its future directions for nsf advanced computing infrastructure to support u.s. science and engineering in...copyright national academy of sciences. all rights reserved.range of operational models 111residual economic value is minimal and its performance and capability are no longer competitive. at that point, only another infusion of capital will ensure service continuity.rather than purchasing hardware at the time of an award, nsf or its awardees might choose to lease the desired hardware from a vendor or a system integrator. in the simplest variation of this model, the hardware remains the property of the vendor but is located at the operator™s facility. from an operational perspective, a simple leasing model is indistinguishable from outright purchase. alternatively, the hardware could be hosted and maintained at a vendor facility, with a division of hardware service and user support between the partners.annual lease payments would smooth the punctuated budget shock of capital acquisitions, allowing amortization across multiple budget years. lease terms at a higher level might also include periodic hardware upgrades to maintain leadingedge capability (e.g., equipment could be upgraded during the life of a cooperative agreement without competition to meet a series of performance targets) as well as quality of service and/or performance guarantees. leases could also include exit clauses for termination, either with or without cause.this is not a new idea. for example, the department of energy (doe) has used this strategy successfully for its leadingedge computing deployments. university supercomputing centers in japan also use leasing, which permits a regular and stable annual funding for each center.6.3.3 commercial cloud service purchasesthe explosive growth of commercial cloud services and their widespread adoption by both large corporations and small startups offers another alternative for provisioning advanced computing but is not a panacea (boxes 6.1 and 6.2). cloud computing now allows large organizations to outsource the provisioning, maintenance, and operation of computing infrastructure and commodity services, allowing them to focus resources and expertise on their core competence and differential value proposition. for smaller companies, the ability to offer services on a payasyougo basis has reduced capital startup requirements and lowered the barrier to market entry. the same could be true of individual laboratory users where computing use is highly episodic, with periods of low and high utilization.the ability to scale services rapidly and dynamically across a wide range of demand is a consequence of the massive scale of cloud service deployment. all of the major cloud service vendors are investing billions of dollars annually to offer advanced computing and data analytics services. in addition, market competition is driving rapid declines in future directions for nsf advanced computing infrastructure to support u.s. science and engineering in...copyright national academy of sciences. all rights reserved.112 future directions for nsf advanced computing infrastructurebox 6.1 cloud computing has recently emerged as an effective way to provide computing to diverse communities. by taking advantage of economies of scale and easy network access, clouds can provide large amounts of computing power as well as convenient access to shared data. a natural question is whether cloud computing can meet the advanced computing needs of segments of the science community. this box considers some of the advantages and disadvantages of commercial cloud services today. the role of clouds for the national science foundation (nsf) will need to be reevaluated frequently because the technology and ecosystem around clouds continues to change rapidly. lowing aspects of cloud computing are relevant to the discussion here:clouds are typically large clusters of computers that exploit economies of scale, providing both computing and data capabilities. the cloud is a shared resource. many users can make use of it; the resource is not just hardware; it includes software and, often, data. it is easy to access cloud services, typically over the internet. the resources available to a single job can vary from a single virtual cpu to a substantial faction of the entire cloud. this characteristic is sometimes to use as much computing power as desired.clouds may provide access to shared data, permitting a diverse user community to share the data and data products.resources with (usually) no longterm commitment.cloud computing provides a number of advantages, particularly for single investigators or small research groups. perhaps the most obvious advantage is that a cloud provides quick and easy access to computing power, and it is just as easy to get 10,000 cores as 1 core. for many users, the fact that access is available on demand within minutes of making the initial request is a major advantage. for others, the availability, if only for a short time, of more resources than they could otherwise afford is the key advantage. for many users, the cost of cloud computing is much lower than the cost of costs, including the oftenneglected cost of maintaining cybersecurity as well as software base, then software must also be developed, sometimes at high cost.future directions for nsf advanced computing infrastructure to support u.s. science and engineering in...copyright national academy of sciences. all rights reserved.range of operational models 113another advantage is the ability to share noncompute resources, such as data or networking. the easyaccess model for clouds makes it simple to share data between users and communities at different institutions, and even different countries. cloud computing also allows researchers to leverage rapid developments in data analytics that are being driven by the private sector and offered by commercial cloud computing providers. for those whose needs are not met by this software, it will be necessary, as with traditional highperformance computing (hpc), for communities to develop custom software. similar services can be offered by nsf centers, although the different allocation and resource model imposes some constraints. in particular, for very large data repositories, it may be impractical for each user to have a copy of the performance, widearea networking.note, however, that some of these advantages are or could be provided by nsfoperated advanced computing resources, which are already elastic. for example, an allocation on the blue waters system can be used for any number of nodes, permitting the use of as little as 32 cores (1 node) and as many as nearly 800,000 cores.the idea of sharing a computing resource to exploit economies of scale is not new. computing centers of all types, including the supercomputing centers operated by nsf, the department of energy, and the department of defense, have done this almost from the beginning of computing. however, there are important differences between cloud computing and conventional time and spacesharing systems, although some of these are a matter of degree rather than being qualitatively different. first, clouds are accessed through a convenient network interface. this network connectivity makes it much easier to provide the resource to anyone on the planet, rather than those with access to but less so than commercial cloud services, which require only a credit card for tion for access.) second, virtualization support has made it much easier to securely standardized apis for web access make it easy to provide interactive access to a computing resource on demand, including access to data repositories. clouds provide many advantages, but it is important to separate cloud myths from realities. clouds are not free. some researchers have been given free cloud applauded, but it is not realistic to expect 5 billion cpucore hours as a gift from a commercial cloud provider (that is a small fraction of just the cpu time nsf consumes in a year and does not include data or network costs). commercial clouds, such as those operated by amazon, google, or microsoft, are often very large in scale, with aggregate compute capacity larger than continuedfuture directions for nsf advanced computing infrastructure to support u.s. science and engineering in...copyright national academy of sciences. all rights reserved.114 future directions for nsf advanced computing infrastructureleadershipclass systems. the very size of these systems gives these vendors and software that goes into these systems. while this gives them some cost advantage, it does not necessarily mean they are cheaper than federally supported hpc centers.costs also include more than a charge for cpu time; in any cost comparison, it is important to include all costs. costs for data handling, such as to/from disk, the cost of cpu time. to further complicate the issue, commercial clouds rarely give enough details to make cost comparisons; for example, a cloud vendor may teristics) are not provided. mercial clouds are more expensive than a traditional supercomputing center,1 marking would help inform an understanding of true costs. to provide an updated roughcost comparison, the committee estimated the cost of providing a leadershipclass system using the amazon elastic cloud (box 6.2).exceeds supply (see figure 2.5); whenever that is the case, the supply must be rationed by some mechanism. nsf currently does this through the allocations process (which introduces various delays). the commercial market does this by adjusting price (not cost). there is no free lunch: ondemand access is, on average, more expensive than scheduled bulk access (although spot markets also offer an opportunity to get lower costs on occasions when demand is lower than supply). cloud availability and cost savings depend, in large part, on uncorrelated use by the different customers. in the end, the only way to address the long queue times is to provide enough capacity. using external clouds, at a higher unit cost, computing. per unit of performance by exploiting commodity computing and the declining cost/performance ratio of its technologies. hpc has a long tradition of doing this. the systems from commodity parts. many believe that this led to broader use of hpc by making systems more widely available. today, many hpc systems are 100 percent commodity hardware, making use of highend, but still commodity, interconnects today, commercial cloud systems employ a mix of commodity and custom hardware. for example, servers used by leading vendors include custom accelerators. both commercial cloud operators and governmentfunded hpc centers exbox 6.1 continuedfuture directions for nsf advanced computing infrastructure to support u.s. science and engineering in...copyright national academy of sciences. all rights reserved.range of operational models 115but not larger than other large hpc systems. for example, currently aws has two systems on top500, but the highest is ranked only 180th on the november 2015 top500 list. windows azure reached number 165 on the 2012 list (but is not currently on the list).clouds also may not match the computing needs of largescale, tightly coupled parallel science applications. most clouds are designed to provide single ﬁcpus,ﬂ possibly in large numbers, to the user. highend hpc applications can among the nodes (e.g., communication every 100 microseconds with a communication overhead of 12 microseconds). this requires (1) a fast interconnect, (2) communication interference with other jobs. (clouds may in fact be distributed can be built to provide these capabilities, but only at additional cost. in short, as a past study2 has shown and as the discussion above further suggests, supercomputing centers already exploit many of the cost advantages some science applications. researchers will need more than access to the cloud services themselves research communities have developed cloudbased applications and software stacks for certain applications, many disciplines lack common tools that reduce the development and management burden on researchers. researchers will also need assistance selecting the appropriate services from a growing range of comsome communities have been looking into taking advantage of clouds and seeing how to take advantage of improving software stacks. a few communities have developed ﬁpointandclickﬂ solutions, but these do not exist for the vast maits hardware acquisition programs, it seems natural to extend the model by helping support researchers who want to further explore using cloud services. indeed, given that cloud services may be of the greatest immediate value in serving the long tail of users, who are less likely to have expertise and experience than larger users, provisioning expertise may be especially important.1 department of energy (doe), the magellan report on cloud computing for science, 2011, http://science.energy.gov/~/media/ascr/pdf/programdocuments/docs/magellanfinalreport.pdf.2 see, for example, doe, the magellan report on cloud computing for science, 2011, page ii, or finding 7, p. iv.future directions for nsf advanced computing infrastructure to support u.s. science and engineering in...copyright national academy of sciences. all rights reserved.116 future directions for nsf advanced computing infrastructurebox 6.2 it is natural to ask whether one could replace a large highperformance computing system with cloud services, especially given that clouds are often viewed as providing verylowcost computing. the 2011 magellan report on cloud computing for science,1 prepared for the department of energy (doe), asked just this question. chapter 12 of the report contains an analysis of the cost of using a cloud to provide the compute and storage capability roughly in line with that at two doe operation, building, and power, as well as the computing equipment. this analysis amazon cloud as well as the introduction of more types of nodes, optimized for different types of computational needs. the authors are also careful to note that the analysis does not take into account the sustained performance on the sort of parallel science application that is common for doe (and the national science foundation [nsf]) supercomputers, nor does it include the performance of the input/output (i/o) system and the cost of i/o operations. in addition, these analyses look solely at the cost of the computing resource and do not take into account the expertise in using these systems and working with computational scientists. the intent was to estimate a lower bound for the cost of a cloud; it is likely that the true cost will be higher.however, 2011 was a long time ago, and cloud technologies and businesses have advanced. are these conclusions still relevant? a check of amazon web services pricing for computing and storage2 suggests that they are. there are now many different tiers of nodes and i/o services, including nodes that provide gpus service costs and frequent service expansions (e.g., in software tools and packages).nsf could make cloud services available to its researchers in one of several ways. all would likely involve nsf negotiating a bulk purchase agreement for data analytics and computing services.individual investigators could request cloud services as part of a standard nsf proposal. the pis of funded proposals could spend awarded funds with the cloud service provider of their choice. this is possible today, although cloud services incur indirect costs that may be more than 50 percent at many institutions, making them signicantly less attractive than they otherwise would be compared to the purchase future directions for nsf advanced computing infrastructure to support u.s. science and engineering in...copyright national academy of sciences. all rights reserved.range of operational models 117and large memory nodes. in addition, substantial discounts are available by purchasing longer term (1 and 3year) reserved instances. the committee compared to an estimate of the cost to nsf of the blue waters supercomputer, using data on january 12, 2016. note that this cost only includes the processors, memory, amazon); the blue waters highperformance, lowlatency interconnect; the blue waters tape library that can hold 320 pb of data; or an hpcoptimized software stack. using 3year reserved instances (which provide the greatest discount) and assuming 100 percent utilization of the amazon resource and an estimate of about 75 percent utilization for blue waters, the cloud was still two to three times more expensive, depending on the exact choice of node type. using 1year reserved instances increases the cloud cost by about 50 percent.this analysis does not mean that clouds must be more expensive; for example, nsf could negotiate a better (lower price) deal with a cloud provider. rather, the point of this analysis is twofold. first, clouds are not necessarily cheaper than public supercomputing centers. second, the costs must be very carefully analyzed able rather than peak performance available to the applications on the respective systems. for this reason, it will be important for nsf and the science community to continue to monitor the opportunities in cloud computing and to take advantage of them where it makes sense, but to also be aware that clouds are not necessarily cheaper than supercomputing centers and to be very careful in comparing costs.1 department of energy, the magellan report on cloud computing for science, 2011, http://science.energy.gov/~/media/ascr/pdf/programdocuments/docs/magellanfinalreport.pdf.2 k. asanovic, r. bodik, b.c. catanzaro, j.j. gebis, p. husbands, k. keutzer, d.a. patterson, w.l. plishker, j. shalf, s.w. williams, and k.a. yelick, the landscape of parallel computing research: a view from berkeley, technical report no. ucb/eecs2006183, december 18, 2006, http://www.eecs.berkeley.edu/pubs/techrpts/ 2006/eecs2006183.pdf.of computing hardware, which presently seems inequitable because the cost to an institution for purchasing cloud services is more akin to that of a recurring credit card charge or a subcontract. by bulk purchasing, nsf could eliminate this additional cost as well, potentially receiving more favorable rates than single investigators could obtain. alternatively, mechanisms to reduce the indirect cost rate charged on cloud services can be explored.the current computing allocation review process could be expanded to include award of cloud services. approved users would receive a budget to be spent with their chosen cloud provider. this would ensure centralized assessment of the appropriateness and likely efciency future directions for nsf advanced computing infrastructure to support u.s. science and engineering in...copyright national academy of sciences. all rights reserved.118 future directions for nsf advanced computing infrastructureof the request, albeit with the double jeopardy of separate research and computing reviews. nsf could negotiate an agreement with one or more commercial cloud service providers (e.g., amazon, google, or microsoft) and then operate a virtual facility on behalf of its users. in this model, user and application support would still rest with a noncommercial entity (e.g., via a cooperative agreement with an academic institution), and the cloud vendor would provide computing and storage services. nsf could leverage the internet2 organization™s net+ initiative, which has selected commercial cloud services for its members and negotiated pricing and other terms. all of these approaches would help take advantage of the rapid evolution of cloud services, the vibrant software ecosystem for cloud data analytics, the ability to use resources at massive scale, and the presence of large, shared data sets. to address the structural disparity in the cost of cloud services compared to hardware acquisition, nsf would need to address the facilities and administrative (f&a) costs now charged for purchase of cloud services. today, researchers can include cloud services as direct costs in research proposals, but these services are not excluded from the modied total direct cost (mtdc) on which f&a is computed. in contrast, capital equipment costs (e.g., computing equipment exceeding $5,000) are excluded from mtdc. the result is that $1 of cloud service costs $1.xx, where xx is the f&a rate at the researcher™s institution. in contrast, the equivalent service on computing equipment purchased by an investigator on a research award costs only $1. in addition, power, cooling, and space for equipment are included in f&a, further skewing the incentive toward equipment purchase rather than service purchase. removing this inequity would allow a more direct comparison and researcher selection based on perceived research value.6.3.4 cooperative agreement extensionany funding and organizational structure must balance organizational stability and sustainability against responsiveness to technological change and customer needs. as noted earlier, nsf has long supported leadingedge cyberinfrastructure via a series of solicitations and open competitions. although this has stimulated intellectual competition and increased nsf™s nancial leverage, it has also made deep and sustainable collaboration difcult among frequent competitors. individual awardees quite rationally often focus more on maximizing their longterm probfuture directions for nsf advanced computing infrastructure to support u.s. science and engineering in...copyright national academy of sciences. all rights reserved.range of operational models 119ability of continued funding, rather than adapting and responding to community needs.frequent competitions have also made it more difcult for nsffunded service providers to recruit and retain talented staff when the horizon for funding is only 2 to 5 years. this is especially true when the competition for it and computational science expertise with industry is so great. periodic review and rigorous performance assessment need not be coupled with ﬁlife or deathﬂ proposal competition and cooperative agreement funding. other federal agencies regularly review the performance of their service facilities, providing strategic and tactical guidance, without coupling those reviews to a facility termination decision. for example, doe operates its national energy research scientic computing center (nersc) in this model. hardware acquisition decisions, management reviews, and service priorities are subject to stringent reviews, but nersc itself is not subject to termination review each time a new system is acquired. this also allows more honest and forthright discussion of problems, without existential fears.nsf could consider designating one or more cyberinfrastructure centers as a core facility with a nominal lifetime of a decadešfor example, as part of an extended cooperative agreement. working with nsf and under regular review, the center would deploy and operate cyberinfrastructure on nsf™s behalf. this would ensure organizational lifetime and planning horizons more similar to those of other nsf mrefc projects, which often last 10 to 20 years. in addition, longer horizons would also let nsf and its service providers evolve services and stafng in response to changing community needs and business partnerships. as extant examples, nsf™s national radio astronomy observatory and national optical astronomy observatory play these roles in the astronomy community.6.3.5 federally funded research and development centersas noted above, continuity is crucial to strategic planning, staff retention, and crossdomain partnerships. cooperative agreements, whether for mrefc projects or other initiatives, provide one mechanism for collaborative planning and management. implicit in all such approaches is a presumption that the project has a bounded lifetime. in turn, that presumption profoundly and adversely affects strategic planning and a commitment to sustainability within nsf and the community.the centrality of advanced computing to research suggests that nsf treat it as a longterm, indenite commitment that more clearly delineates the distinction between performance review and accountability and organizational continuity and service capabilities. such separation future directions for nsf advanced computing infrastructure to support u.s. science and engineering in...copyright national academy of sciences. all rights reserved.120 future directions for nsf advanced computing infrastructurewould allow service providers to work more collaboratively with nsf on responses to community needs and would encourage interorganizational collaboration. an ffrdc is an excellent example of this balance. ffrdcs are independent nonprot entities sponsored and funded by the u.s. government to meet specic longterm technical needs in areas of national interest. they operate as longterm strategic partners with their sponsoring government agencies. many ffrdcs, such as doe laboratories, include multiple programs spanning many areas of science and engineering research. nsf already uses an ffrdc, ncar, as an integral part of nsf™s cyberinfrastructure service strategy for the geoscience community; it can budget and plan new equipment acquisitions, and it offers staff career paths and continuity. nsf could consider establishing one or more ffrdcs to support national cyberinfrastructure for research. working with nsf, industry, and academia, such cyberinfrastructure ffrdcs could develop a strategic plan for cyberinfrastructure that meets evolving community needs, tracks technology developments, and provides a roadmap for nsf™s directorates. the ffrdcs would also deploy and operate general or domainspecic cyberinfrastructure for the national community.6.3.6 partnerships with other agencies nsf could explore partnerships with other federal agencies. for example, nsf could coordinate complementary leadershipclass system congurations with doe, especially with doe systems that are used to support the doe innovative and novel computational impact on theory and experiment program. the purpose of this partnership is not to shift the responsibility for providing cycles from nsf to doe; rather, it is in recognition of the fact that there is not a simple onedimensional conguration space for advanced cyberinfrastructure. such a partnership would develop a way to fairly serve special needs from the population supported by each agency. for example, today nsf operates a system with more memory than any doe system; conversely, doe operates a system with more gpus and peak ˚oatingpoint operations per second (flop/s) than any nsf system. currently, computational scientists request time on a variety of resources, taking advantage of doe, nsf, and other providers of advanced computing infrastructure to the science community. but there is no formal coordination between agencies of the systems that they acquire, and tradeoffs are made independently. partnerships with other agencies could help ensure that the full spectrum of advanced cyberinfrastructure is available to the science community.future directions for nsf advanced computing infrastructure to support u.s. science and engineering in...copyright national academy of sciences. all rights reserved.range of operational models 1216.3.7 strategic publicprivate partnershipsas the demand for cyberinfrastructure continues to rise, the costs for deployment and operation rise commensurately. this is true for both aggregate demandšlaboratory and institutional capabilitiesšand leadingedge computing and data storage systems. supercially, this may seem paradoxical, given the dramatic increases in computing capability and storage capability regularly delivered by the computing industry. however, those same computing advances have birthed new sensors and scientic instruments and a torrent of new digital data, as well as new simulation models and expectations for everlarger computing capability.4 rising demands for computing and storage (endtoend capabilities, not just hardware) now challenge the nances and social processes of both nsf and its academic grantees. simply put, the rising cost of leadingedge facilities (nsf track 1 and track 2 systems) is not sustainable under the current partnership model and may not be sustainable under any governmentfunded model. put another way, the perceived return on investment for a facility costing hundreds of millions of dollars must be substantial, particularly when the equipment has a useful lifetime of only 3 to 5 years.nsf might consider alternative publicprivate partnership models that create nancial incentives for privatesector partners to operate largescale cyberinfrastructure facilities on the research community™s behalf. these necessarily require more ˚exible approaches than traditional feeforservice models and might include such options as access to university intellectual property in exchange for cyberinfrastructure services. precisely how such arrangements might work would depend on the willingness of the academic community to agree on, for example, vendor exclusivity and intellectual property sharing.6.3.8 userdriven acquisition and allocationall of the operational strategies described above are based on some variant of central planning and resource management. alternatively, nsf could decentralize cyberinfrastructure acquisition and support and rely on social and economic forces to dene and optimize community cyberinfrastructure. one rst step in this process would be denominating all services in dollars, rather than the abstract, normalized service units (sus) or storage allocations used today. sus play an important role by attempting to enable the comparison of allocations on computers that may differ widely in both architecture (e.g., conventional processors or 4 the end of dennard scaling and limits of future microprocessor performance increases mean the ﬁfree lunchﬂ of performance doubling will bring new and sobering economic constraints. larger capability will require larger capital infusions.future directions for nsf advanced computing infrastructure to support u.s. science and engineering in...copyright national academy of sciences. all rights reserved.122 future directions for nsf advanced computing infrastructuregraphical processing units) and time of deployment. for instance, the use of sus makes more quantitative the assessment in figure 2.5 of resources over the past decade. however, despite their merit, sus obscure from users the actual costs associated with requests and allocations, and the use of sus also distances the nsf programs and the user community from the prioritization processes about how the underlying funding is allocated. moreover, the conversion factor between actual wall time on a computational resource and sus is established by each site based on highperformance linpack benchmark results, which is just a single and outdated metric that does not capture the diversity of factors controlling the capability (which is more than just performance) of individual applications mapped to different architectures. recently, xsede has started notifying both users and associated nsf program managers of the actual dollar value associated with an allocation, and there seem to be multiple signicant potential benets in making users even more cognizant of and ultimately responsible for the actual costs and effective use of resources.realizing these benets can certainly start with increasing user awareness of costs and engaging users in resource planning and acquisition. in a more extensive realization of this model, however, individual researchers or research teams would be allowed to spend awarded cyberinfrastructure dollars at their discretion. this cyberinfrastructure marketplace might include the following options:purchasing local computing infrastructure, services, or staff support for use within the individual researcher™s laboratory;contributing dollars to a university pool that operates a campus facility under a ﬁcampus condominiumﬂ model;5pooling research dollars to purchase and operate shared regional or national facilities; andpurchasing commercial cloud services, exploiting the properties of elasticity and ondemand access.all of these variants allow individual researchers and research teams to make separate decisions on how best to advance their research. they also remove researchers from double jeopardy, where they must compete separately for research funding and for computing resources. in addition, the options expose the costs of each option in a common currency. how5 under a condominium model, a university purchases a baseline computing and storage infrastructure and allows individual researchers to purchase and contribute nodes and storage to the shared pool. researchers receive access priority in proportion to their nancial contribution.future directions for nsf advanced computing infrastructure to support u.s. science and engineering in...copyright national academy of sciences. all rights reserved.range of operational models 123ever, the risk is that the sum of the local research optimizations may not be globally optimal for the national community. moreover, some form of such a model may provide an effective mechanism to encourage and formalize investments and responsibilities of researchers, institutions, and regions in private and shared local or national infrastructure. nsf already recognizes that there are signicant computing resources ﬁat the edgesﬂ (meaning within campuses and states) and that there is a clear need to coordinate and leverage investments. programs such as campus cyberinfrastructurešdata, networking, and innovation program (cc*dni) and major research instrumentation help develop this infrastructure, and elements of xsede, such as campus champions, are directed toward tying both communities and cyberinfrastructure together. however, the same economic and technological forces driving the decisions on national computing infrastructure are eroding the abilities of campuses to purchase and operate their own cyberinfrastructure, and especially challenging are the cost and complexity of managing research data. thus, smaller institutions are now choosing to invest in infrastructure operated by larger neighbors or at national centers, which can provide both cost and other advantages compared to attempting to use the commercial cloud. however, in the absence of a scalable national model, such partnerships are presently ad hoc. the nsf big data regional innovation hubs (bd hubs) program is potentially a powerful catalyst to drive regional synergy, but this still needs to be tied to a national narrative that includes all aspects of advanced cyberinfrastructure. variations of this economic model have been explored in the past. then called the ﬁgreen stampsﬂ model of resource allocation, it was analyzed in the 1995 report of the task force on the future of the nsf supercomputer centers program.6 the report notedthe key concept in a green stamp mechanism is the use of the stamps to represent both the total allocation of dollars to the centers and the allocation of those resources to individual pi™s. nsf could decide a funding level for the centers, which based on the ability of the centers to provide resources, would lead to a certain number of stamps, representing those resources, being available. individual directorates could disperse the stamps to their pi™s, which could then be used by the researchers to purchase cycles. multiple stamp colors could be used to represent different sorts of resources that could be allocated.  the major advantages raised for this proposal are the ability of the di6 national science foundation, report of the task force on the future of the nsf supercomputer centers program, nsf9646, september 15, 1995, https://www.nsf.gov/publications/pubsumm.jsp?odskey=nsf9646.future directions for nsf advanced computing infrastructure to support u.s. science and engineering in...copyright national academy of sciences. all rights reserved.124 future directions for nsf advanced computing infrastructurerectorates to have some control over the size of the program by expressing interest in a certain number of stamps, improvement in efciency gained by having the centers compete for stamps, and improvements in the allocation process, which could be made by program managers making normal awards that included a stamp allocation. other than the mechanics of overall management, most of the disadvantages of such a scheme have been raised in the previous sections. in particular, such a mechanism (especially when reduced to cash rather than stamps) makes it very difcult to have a centralized highend computing infrastructure that aggregates resources and can make longterm investments in largescale resources.nsf could conduct a pilot project to evaluate the power of market forces in allocating limited cyberinfrastructure support. among the issues to evaluate is whether such an approach would exacerbate the problem of buying resources by the hour (see section 5.5) without recognizing the xed costs, such as the cost of retaining staff and supporting the use of new architectures. independently of any pilot projects, nsf will benet by expressing in dollars the true cost of large cyberinfrastructure resource allocations (i.e., those now made by the xsede resource allocation committee [xrac] and petascale computing resource allocation committees [prac]). first, it would allow researchers to identify the value of cyberinfrastructure awards to their institutions. second, and equally important, it would make clear that such large allocations have true costs, encouraging wise and efcient use.future directions for nsf advanced computing infrastructure to support u.s. science and engineering in...copyright national academy of sciences. all rights reserved.appendixesfuture directions for nsf advanced computing infrastructure to support u.s. science and engineering in...copyright national academy of sciences. all rights reserved.future directions for nsf advanced computing infrastructure to support u.s. science and engineering in...copyright national academy of sciences. all rights reserved.alist of individuals, research groups, and organizations that submitted commentsjay alameda, national center for supercomputing applications, university of illinois, urbanachampaignrichard b. arthur, ge global researchdan atkins, university of michigannadine aubry, northeastern universitytroy baer, university of tennesseejim belak, lawrence livermore national laboratoryfrancine berman, rensselaer polytechnic instituteprentice bisbal, rutgers universityalan blatecky, rti internationaladam bowser, on behalf of the university corporation for advanced internet development (internet2)robert f. brammer, brammer technology, llcjames g. brasseur, pennsylvania state universitydanielle chandler, university of illinois, urbanachampaign, on behalf of the theoretical and computational biophysics groupcoalition for academic scientic computation, comments collected from senior u.s. cyberinfrastructure facility directorscoalition for academic scientic computation, comments from members (individual opinions)ronald cohen, carnegie institutioncomputing community consortium, computing research associationgeorge w. crabtree, argonne national laboratoryalan crosswalk, columbia universitytimothy alden davis, texas a&m university127future directions for nsf advanced computing infrastructure to support u.s. science and engineering in...copyright national academy of sciences. all rights reserved.128 future directions for nsf advanced computing infrastructurecarleton detar, university of utahthom dunning, pacic northwest national laboratory and university of washingtonrodolfo barniol duran, purdue universitybruce g. elmegreen, ibm t.j. watson research centerian foster, university of chicagolars grabow, university of houstonvictor hazlewood, university of tennesseehendrik heinz, university of akron on behalf of faculty of the college of polymer science and engineeringtony hey, university of washington escience institutealvin kennedy, morgan state universityrubin h. landau, oregon state universityrandall leveque, university of washingtonzachary h. levine, national institute of standards and technology david a. lifka, cornell universityyangzheng lin, carnegie institutionglenn k. lockwood, 10x genomicspaul b. mackenzie, fermi national accelerator laboratory, on behalf of the u.s. lattice quantum chromodynamics collaborationjan mandel, university of colorado, denverthomas a. manz, new mexico state universityj. andrew mccammon, university of california, san diegojonathan c. mckinney, university of marylandcharles meneveau, johns hopkins universityblake mertz, west virginia universityrajat mittal, johns hopkins universitycolin morningstar, carnegie mellon universitylawrence murakami, university of alaskaannick pouquet, university of colorado, boulderjeff f. pummel, university of arkansasralph roskies, pittsburgh supercomputing center, university of pittsburghbarry i. schneider, national institute of standards and technologybill schultz, university of michiganjerome soller, cognitech corporationjames m. stone, princeton universityalexander tchekhovskoy, university of california, berkeleygreg van anders, university of michiganchris van de walle, university of california, santa barbara nancy wilkinsdiehr, university of california, san diegowalt wright, check twelve leadershipp.k. yeung, georgia institute of technologypeijun zhang, carnegie institutionfuture directions for nsf advanced computing infrastructure to support u.s. science and engineering in...copyright national academy of sciences. all rights reserved.binformationgathering meetingsapril 15, 2014, by telephonebriengs from irene qualters, national science foundation (nsf), and peter arzberger, nsfmay 16, 2014, washington, d.c.briengs from michael norman, san diego supercomputer center; michael vogelius, nsf; bogdan mihaila, nsf; jeryl mumpower, nsf; and eva zanzerkia, nsfnovember 19, 2014, birdsofafeather session at sc14, new orleans, louisianadecember 1617, 2014, workshop in mountain view, californiaparticipants: christian ott, caltech; thomas cheatham, university of utah; tom jordan, university of southern california; steven gottlieb, indiana university; tony hey, microsoft, by telephone; ilkay altintas, san diego supercomputer center; jacek becla, slac national accelerator laboratory; victoria stodden, university of illinois, urbanachampaign; and ed lazowska, university of washington129future directions for nsf advanced computing infrastructure to support u.s. science and engineering in...copyright national academy of sciences. all rights reserved.130 future directions for nsf advanced computing infrastructurefebruary 19, 2015briengs from jim kurose, nsf; irene qualters, nsf; rudi eigenmann, nsf; and steven binkley, department of energy ofce of sciencefuture directions for nsf advanced computing infrastructure to support u.s. science and engineering in...copyright national academy of sciences. all rights reserved.cbiosketches of committee memberswilliam d. gropp, cochair, is the thomas m. siebel chair in computer science at the university of illinois, urbanachampaign, where he is also founding director of the parallel computing institute. he held the positions of assistant (19821988) and associate (19881990) professor in the computer science department at yale university. in 1990, he joined the numerical analysis group at argonne national laboratory (anl), where he was a senior computer scientist in the mathematics and computer science division, a senior scientist in the department of computer science at the university of chicago, and a senior fellow in the argonnechicago computation institute. from 2000 through 2006, he was deputy director of the mathematics and computer science division at anl. in 2007, he joined the university of illinois, urbanachampaign, as the paul and cynthia saylor professor in the department of computer science. in 2008, he was appointed deputy director for research for the institute of advanced computing applications and technologies at the university of illinois. his research interests are in parallel computing, software for scientic computing, and numerical methods for partial differential equations. he has played a major role in the development of the mpi messagepassing standard, is one of the designers of the petsc parallel numerical library, and has developed efcient and scalable parallel algorithms for the solution of linear and nonlinear equations. dr. gropp is a fellow of the association for computing machinery (acm), the institute of electrical and electronics engineers (ieee), and the society for industrial and applied mathematics (siam) and a member of the national academy 131future directions for nsf advanced computing infrastructure to support u.s. science and engineering in...copyright national academy of sciences. all rights reserved.132 future directions for nsf advanced computing infrastructureof engineering. he received the sidney fernbach award from the ieee computer society in 2008 and the technical committee on scalable computing award for excellence in scalable computing in 2010. dr. gropp received his b.s. in mathematics from case western reserve university, an m.s. in physics from the university of washington, and a ph.d. in computer science from stanford university.robert j. harrison, cochair, is director, institute of advanced scientic computing, stony brook university, and director, computational science center, brookhaven national laboratory. the core mission of the new stony brook institute is to advance the science of computing and its applications to solving complex problems in the physical sciences, the life sciences, medicine, sociology, industry, and nance. the institute works closely with the brookhaven center, which specializes in dataintensive computing. dr. harrison™s research interests are focused on scientic computing and the development of computational chemistry methods for the world™s most technologically advanced supercomputers. from 2002 to 2012, he was director of the joint institute of computational science and professor of chemistry and corporate fellow at the university of tennessee and oak ridge national laboratory. prior positions were at the environmental molecular sciences laboratory, pacic northwest laboratory, and anl. he has a prolic career in highperformance computing, with more than 100 publications on the subject, as well as extensive service on national advisory committees. he received his b.a. from churchill college, university of cambridge, and his ph.d. in organic and theoretical chemistry from the university of cambridge.mark r. abbott is president and director of the woods hole oceanographic institution. he was previously dean of the college of earth, ocean, and atmospheric sciences at oregon state university (osu). prior to his appointment at osu, he served as a member of the technical staff at the jet propulsion laboratory (jpl) and as a research oceanographer at scripps institution of oceanography. dr. abbott™s research focuses on the interaction of biological and physical processes in the upper ocean and relies on both remote sensing and eld observations. he is a pioneer in the use of satellite ocean color data to study coupled physical/biological processes. as part of a nasa earth observing system interdisciplinary science team, dr. abbott led an effort to link remotely sensed data of the southern ocean with coupled ocean circulation/ecosystem models. his eld research included the rst deployment of an array of biooptical moorings in the southern ocean as part of the u.s. joint global ocean flux study. dr. abbott was a member of the national science board from 2006 to 2012 and served as a consultant to the board until 2013. he is the future directions for nsf advanced computing infrastructure to support u.s. science and engineering in...copyright national academy of sciences. all rights reserved.appendix c 133vice chair of the oregon global warming commission. he is currently a member of the board of trustees for the consortium for ocean leadership and the board of trustees of neon, inc. his past advisory posts include chairing the coastal ocean applications and science team for noaa and chairing the u.s. joint global flux study science steering committee. he has also been a member of the director™s advisory council for jpl and nasa™s modis and seawifs science teams and the earth observing system investigators working group. he was the 2011 recipient of the jim gray escience award, presented by microsoft research. dr. abbott is a national associate member of the national academies of sciences, engineering, and medicine and is currently a member of the space studies board, chair of the committee on earth science and applications from space, a member of the committee to advise the u.s. global change research program, and a member of the panel on the review of the draft 2013 national climate assessment (nca) report. as part of his prolic service to the academies, dr. abbott served on the committee on evaluating nasa™s strategic direction, the committee on the assessment of nasa™s earth science programs, the committee on the role and scope of missionenabling activities in nasa™s space and earth science missions, and the panel on landuse change, ecosystem dynamics and biodiversity for the 2007 earth science and applications from space decadal survey. dr. abbott received his b.s. in conservation of natural resources from the university of california, berkeley, and his ph.d. in ecology from the university of california.robert l. grossman is a faculty member at the university of chicago. he is the director of the center for data intensive science, a senior fellow and core faculty in the computation institute and the institute for genomics and systems biology, and a professor of medicine in the section of genetic medicine. he also serves as the chief research informatics ofcer for the biological sciences division. his research group focuses on data intensive computing, data science, and bioinformatics. he is the founder and a partner of open data group, which provides analytic services to help companies build predictive models over big data, and is the director of the notforprot open cloud consortium, which provides cloud computing infrastructure to support the research community. he was elected a fellow of the american association for the advancement of science in 2013. dr. grossman earned his ph.d. in applied mathematics at princeton university and an a.b. in mathematics from harvard university.peter m. kogge is a professor of computer science and engineering and concurrent professor of electrical engineering at the university of notre dame. dr. kogge was with ibm, federal systems division, from future directions for nsf advanced computing infrastructure to support u.s. science and engineering in...copyright national academy of sciences. all rights reserved.134 future directions for nsf advanced computing infrastructure1968 until 1994, and was appointed an ieee fellow in 1990 and an ibm fellow in 1993. in 1977, he was a visiting professor in the ece department at the university of massachusetts, amherst. from 1977 through 1994, he was also an adjunct professor in the computer science department of the state university of new york at binghamton. in 1994, he joined the university of notre dame as rst holder of the endowed mccourtney chair in computer science and engineering (cse). starting in the summer of 1997, he has been a distinguished visiting scientist at the center for integrated space microsystems at jpl. he is also the research thrust leader for architecture in notre dame™s center for nano science and technology. for the 20002001 academic year, he was the interim schubmehlprein chairman of the cse department at notre dame. from august 2001 until december 2008, he was the associate dean for research, college of engineering; since fall 2003, he has been a concurrent professor of electrical engineering. his current research areas include massively parallel processing architectures, advanced vlsi and nanotechnologies and their relationship to computing systems architectures, non von neumann models of programming and execution, parallel algorithms and applications, and their impact on computer architecture. while at ibm, one of his groups designed the rst multiprocessor pim device with signicant dram memory that may also be the world™s rst multicore chip. a paper on its architecture received the daniel slotnick award at the 1994 international conference on parallel processing. dr. kogge also designed and built the rtais parallel processor. prior parallel machines included the ibm 3838 array processor and the space shuttle input/output processor (iop), which probably represents the rst true parallel processor to ˚y in space and is one of the earliest examples of multithreaded architectures. dr. kogge received the ieee seymour cray award in 2012 and the ieee charles babbage award in 2014. he received his b.s. in electrical engineering from the university of notre dame, his m.s. in systems and engineering from syracuse university, and his ph.d. in electrical engineering from stanford university.padma raghavan is the associate vice president for research and director of strategic initiatives at the pennsylvania state university, where she is also a distinguished professor of computer science and engineering. dr. raghavan is the founding director of the penn state institute for cyberscience, the coordinating unit on campus for developing interdisciplinary computation and dataenabled science and engineering. prior to joining penn state in 2000, she served as an associate professor in the department of computer science at the university of tennessee. her research is in the area of highperformance computing and computational science and engineering. she has more than 95 peerreviewed publications in three major areas, including scalable parallel computing; future directions for nsf advanced computing infrastructure to support u.s. science and engineering in...copyright national academy of sciences. all rights reserved.appendix c 135energyaware supercomputing (i.e., performance and power scalability of advanced computer systems); and computational modeling, simulation, and knowledge extraction. dr. raghavan currently serves on the editorial boards of the siam book series computational science and engineering and software, environments and tools, the journal of parallel and distributed computing, the journal of computational science, and ieee transactions on parallel and distributed systems. she serves on the program committees of major conferences sponsored by acm, ieee, and siam, and she cochaired technical papers for supercomputing 2012 and the 2011 siam conference on computational science and engineering. dr. raghavan also serves on various advisory and review boards, including the academies™ panel on digitization and communication science, the network for earthquake engineering simulation, and the computer research association™s (cra™s) committee on the status of women in computing research. she is a fellow of the ieee, and she received an nsf career award and the maria goeppertmayer distinguished scholar award from the university of chicago and anl for her research on parallel sparse matrix computations. dr. raghavan received her ph.d. in computer science from penn state.daniel a. reed is currently vice president for research and economic development, as well as a professor of computer science, electrical and computer engineering, and medicine at the university of iowa. he also holds the university computational science and bioinformatics chair at iowa. dr. reed was a corporate vice president at microsoft from 2009 to 2012, responsible for global technology policy and extreme computing, and director of scalable and multicore computing at microsoft from 2007 until 2009. prior to microsoft, he was the founding director of the renaissance computing institute at the university of north carolina, chapel hill, where he also served as chancellor™s eminent professor and vice chancellor for information technology. before joining the university of north carolina, chapel hill, in 2003, dr. reed was director of the national center for supercomputing applications (ncsa), and gutgsell professor and head of the department of computer science at the university of illinois, urbanachampaign. he was appointed to the president™s council of advisors on science and technology (pcast) by president bush in 2006 and served on the president™s information technology advisory committee (pitac) from 2003 to 2005. as chair of pitac™s computational science subcommittee, he was lead author of the report computational science: ensuring america™s competitiveness. on pcast, he cochaired the networking and information technology subcommittee (with george scalise of the semiconductor industry association) and coauthored a report on the networking and information technology research and future directions for nsf advanced computing infrastructure to support u.s. science and engineering in...copyright national academy of sciences. all rights reserved.136 future directions for nsf advanced computing infrastructuredevelopment (nitrd) program called leadership under challenge: information technology r&d in competitive world. he is past chair of the board of directors of cra and currently serves on its government affairs committee. cra represents the research interests of the university, national laboratory, and industrial research laboratory communities in computing across north america. dr. reed received his b.s. from the university of missouri, rolla, and his m.s. and ph.d. degrees from purdue university, all in computer science.valerie taylor is the senior associate dean of academic affairs in the dwight look college of engineering and the regents professor and royce e. wisenbaker professor in the department of computer science and engineering at texas a&m university. in 2003, she joined texas a&m as the department head of computer science and engineering, where she remained in that position until 2011. prior to joining texas a&m, dr. taylor was a member of the faculty in the electrical engineering and computer sciences department at northwestern university for 11 years. she has authored or coauthored more than 100 papers in the area of highperformance computing. she is also the executive director of the center for minorities and people with disabilities in it. dr. taylor is an ieee fellow and has received numerous awards for distinguished research and leadership, including the 2001 ieee harriet b. rigas award for a woman with signicant contributions in engineering education, the 2002 outstanding young engineering alumni award from the university of california, berkeley, the 2002 cra nico habermann award for increasing the diversity in computing, and the 2005 tapia achievement award for scientic scholarship, civic science, and diversifying computing. dr. taylor is a member of acm. she earned her b.s. in electrical and computer engineering and m.s. in computer engineering from purdue university and a ph.d. in electrical engineering and computer sciences from the university of california, berkeley.katherine a. yelick is a professor of electrical engineering and computer sciences at the university of california, berkeley, and the associate laboratory director for computing sciences at lawrence berkeley national laboratory. dr. yelick is known for her research in parallel languages, compilers, algorithms, and libraries. she coinvented the upc and titanium languages and developed analyses, optimizations, and runtime systems for their implementation. she has also done research on memory hierarchy optimizations, communicationavoiding algorithms, and automatic performance tuning, including developing the rst autotuned sparse matrix library. in her current role as associate laboratory director, she manages an organization that includes the national energy research future directions for nsf advanced computing infrastructure to support u.s. science and engineering in...copyright national academy of sciences. all rights reserved.appendix c 137scientic computing center (nersc), the energy science network (esnet), and the computational research division. she was the director of nersc from 2008 to 2012. dr. yelick has received multiple research and teaching awards, including the athena award, and she is an acm fellow and an ieee senior member. she is a member of the california council on science and technology, the academies™ computer science and telecommunications board, and the science and technology committee overseeing research at los alamos and lawrence livermore national laboratories. she earned her ph.d. in electrical engineering and computer science from the massachusetts institute of technology.future directions for nsf advanced computing infrastructure to support u.s. science and engineering in...copyright national academy of sciences. all rights reserved.dacronyms and abbreviations3d threedimensional  aci division of advanced cyberinfrastructure (nsf)aligo advanced laser interferometer gravitational wave observatories bbh binary black holebd hub big data regional innovation hubblas basic linear algebra subprograms cc*dni campus cyberinfrastructurešdata, networking, and innovation program ccam commonwealth center for advanced manufacturingccp collaborative computational projectcif21 cyberinfrastructure framework for 21st century science and engineeringcise directorate for computer and information science and engineeringcmos complementary metaloxide semiconductorcpu central processing unitcsia cyber security and information assurance dod department of defensedoe department of energy138future directions for nsf advanced computing infrastructure to support u.s. science and engineering in...copyright national academy of sciences. all rights reserved.appendix d 139dram dynamic randomaccess memory eb exabyteecss extended collaborative support serviceeu european union f&a facilities and administrativeffrdc federally funded research and development centerfftpack fastest fourier transform in the west packagefftw fastest fourier transform in the westflop/s ˚oatingpoint operations per secondfpga eldprogrammable gate array gb gigabytegeni global environment for network innovationsgpu graphical processing unitgsl gnu scientic library gteps gigatraversed edges per second hcss high condence software and systemshdd hard disk drivehecia highend computing infrastructure and applicationshecrd highend computing research and developmenthpc highperformance computinghpcg highperformance conjugate gradient i/o input/outputiops input/output operations per secondit information technology lapack linear algebra packagelsn large scale networkinglsst large synoptic survey telescope mb megabytemkl math kernel librarympi message passing interfacemrefc major research equipment and facilities constructionmtdc modied total direct costmumps massachusetts general hospital utility multiprograming systems future directions for nsf advanced computing infrastructure to support u.s. science and engineering in...copyright national academy of sciences. all rights reserved.140 future directions for nsf advanced computing infrastructurenasa national aeronautics and space administrationncar national center for atmospheric researchncsa national center for supercomputing applicationsnersc national energy research scientic computing centernitrd networking and information technology research and developmentnsci national strategic computing initiativensf national science foundationntt nippon telegraph and telephone oci ofce of cyberinfrastructure paci partnership for advanced computational infrastructurepapi precision approach path indicatorparpack parallel arnoldi packagepb petabytepetsc portable, extensible toolkit for scientic computationpgas partitioned global address spacepitac president™s information technology advisory committeeprac petascale computing resource allocations committeeprace partnership for advanced computing in europepsc pittsburgh supercomputing centerpspline princeton spline and hermite cubic interpolation routines r&d research and development scalapack scalable linear algebra packagescec southern california earthquake centerscidac scientic discovery through advanced computingscorec scientic computation research centersdp software design and productivitysew social, economic, and workforce implications of it and it workforce developmentsisi software infrastructure for sustained innovation programslepc scalable library for eigenvalue problem computationsssd solidstate disksu service unit tacc texas advanced computing centertb terabyte  future directions for nsf advanced computing infrastructure to support u.s. science and engineering in...copyright national academy of sciences. all rights reserved.appendix d 141upc universal product code wan wide area network xdmod xd metrics on demandxrac xsede resource allocation committeexsede extreme science and engineering discovery environment future directions for nsf advanced computing infrastructure to support u.s. science and engineering in...copyright national academy of sciences. all rights reserved.
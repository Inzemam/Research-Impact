detailsdistribution, posting, or copying of this pdf is strictly prohibited without written permission of the national academies press. (request permission) unless otherwise indicated, all materials in this pdf are copyrighted by the national academy of sciences.copyright © national academy of sciences. all rights reserved.the national academies pressvisit the national academies press at nap.edu and login or register to get:œ œ 10% off the price of print titlesœ special offers and discountsget this bookfind related titlesthis pdf is available at sharecontributorshttp://nap.edu/11148getting up to speed: the future of supercomputing306 pages | 6 x 9 | paperbackisbn 9780309095020 | doi 10.17226/11148susan l. graham, marc snir, and cynthia a. patterson, editors; committee on thefuture of supercomputing; computer science and telecommunications board;division on engineering and physical sciences; national research councilgetting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.susan l. graham, marc snir, and cynthia a. patterson, editorscommittee on the future of supercomputingcomputer science and telecommunications boarddivision on engineering and physical sciencesgettingettingettingettingetting up tg up tg up tg up tg up to speedo speedo speedo speedo speedthe future ofthe future ofthe future ofthe future ofthe future ofsupersupersupersupersupercccccomputinomputinomputinomputinomputingggggthe national academies presswashington, d.c.www.nap.edugetting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.the national academies press500 fifth street, n.w.washington, dc 20001notice: the project that is the subject of this report was approved by the governing board of the national research council, whose members are drawn fromthe councils of the national academy of sciences, the national academy of engineering, and the institute of medicine. the members of the committee responsiblefor the report were chosen for their special competences and with regard for appropriate balance.support for this project was provided by the department of energy under sponsor award no. deat0103na00106. any opinions, findings, conclusions, orrecommendations expressed in this publication are those of the authors and donot necessarily reflect the views of the organizations that provided support for theproject.international standard book number 0309095026 (book)international standard book number 0309546796 (pdf)library of congress catalog card number 2004118086cover designed by jennifer bishop.cover images (clockwise from top right, front to back)1. exploding star. scientific discovery through advanced computing(scidac) center for supernova research, u.s. department of energy, office ofscience.2. hurricane frances, september 5, 2004, taken by goes12 satellite, 1 kmvisible imagery. u.s. national oceanographic and atmospheric administration.3. largeeddy simulation of a rayleightaylor instability run on the lawrencelivermore national laboratory mcr linux cluster in july 2003. the relative abundance of the heavier elements in our universe is largely determined by fluid instabilities and turbulent mixing inside violently exploding stars.4. threedimensional model of the structure of a ras protein. human genomeprogram, u.s. department of energy, office of science.5. test launch of minuteman intercontinental ballistic missile. vandenbergair force base.6. a sample of liquid deuterium subjected to a supersonic impact, showingthe formation of a shock front on the atomic scale. the simulation involved 1,320atoms and ran for several days on 2,640 processors of lawrence livermore national laboratoryõs asci white. it provided an extremely detailed picture of theformation and propagation of a shock front on the atomic scale. accelerated strategic computing initiative (asci), department of energy, lawrence livermorenational laboratory.7. isodensity surfaces of a national ignition facility ignition capsule bounding the shell, shown at 200 picosec (left), 100 picosec (center), and near ignitiontime (right). an example of asci white threedimensional computer simulationsbased on predictive physical models. asci, lawrence livermore national laboratory.copies of this report are available from the national academies press, 500 fifthstreet, n.w., lockbox 285, washington, dc 20055; (800) 6246242 or (202) 3343313 in the washington metropolitan area; internet, http://www.nap.educopyright 2005 by the national academy of sciences. all rights reserved.printed in the united states of americagetting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.the national academy of sciences is a private, nonprofit, selfperpetuating society of distinguished scholars engaged in scientific and engineering research, dedicated to the furtherance of science and technology and to their use for the generalwelfare. upon the authority of the charter granted to it by the congress in 1863,the academy has a mandate that requires it to advise the federal government onscientific and technical matters. dr. bruce m. alberts is president of the nationalacademy of sciences.the national academy of engineering was established in 1964, under the charterof the national academy of sciences, as a parallel organization of outstandingengineers. it is autonomous in its administration and in the selection of its members, sharing with the national academy of sciences the responsibility for advising the federal government. the national academy of engineering also sponsorsengineering programs aimed at meeting national needs, encourages education andresearch, and recognizes the superior achievements of engineers. dr. wm. a.wulf is president of the national academy of engineering.the institute of medicine was established in 1970 by the national academy ofsciences to secure the services of eminent members of appropriate professions inthe examination of policy matters pertaining to the health of the public. the institute acts under the responsibility given to the national academy of sciences by itscongressional charter to be an adviser to the federal government and, upon itsown initiative, to identify issues of medical care, research, and education. dr.harvey v. fineberg is president of the institute of medicine.the national research council was organized by the national academy of sciences in 1916 to associate the broad community of science and technology withthe academyõs purposes of furthering knowledge and advising the federal government. functioning in accordance with general policies determined by theacademy, the council has become the principal operating agency of both the national academy of sciences and the national academy of engineering in providing services to the government, the public, and the scientific and engineering communities. the council is administered jointly by both academies and the instituteof medicine. dr. bruce m. alberts and dr. wm. a. wulf are chair and vice chair,respectively, of the national research council.www.nationalacademies.orggetting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.committee on the future of supercomputingsusan l. graham, university of california, berkeley, cochairmarc snir, university of illinois at urbanachampaign, cochairwilliam j. dally, stanford universityjames w. demmel, university of california, berkeleyjack j. dongarra, university of tennessee, knoxville, and oakridge national laboratorykenneth s. flamm, university of texas, austinmary jane irwin, pennsylvania state universitycharles koelbel, rice universitybutler w. lampson, microsoft corporationrobert f. lucas, university of southern californiapaul c. messina, distinguished senior computer scientist,consultantjeffrey m. perloff, university of california, berkeleywilliam h. press, los alamos national laboratoryalbert j. semtner, naval postgraduate schoolscott stern, northwestern universityshankar subramaniam, university of california, san diegolawrence c. tarbell, jr., technology futures office, eaglealliancesteven j. wallach, chiaro networksstaffcynthia a. patterson, study directorphil hilliard, research associate (through may 2004)margaret marsh huynh, senior program assistantherbert s. lin, senior scientistivgetting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.computer science and telecommunications boarddavid liddle, u.s. venture partners, cochairjeannette m. wing, carnegie mellon university, cochaireric benhamou, benhamou global ventures, llcdavid d. clark, massachusetts institute of technology, cstb chairemerituswilliam dally, stanford universitymark e. dean, ibm almaden research centerdeborah estrin, university of california, los angelesjoan feigenbaum, yale universityhector garciamolina, stanford universitykevin kahn, intel corporationjames kajiya, microsoft corporationmichael katz, university of california, berkeleyrandy h. katz, university of california, berkeleywendy a. kellogg, ibm t.j. watson research centersara kiesler, carnegie mellon universitybutler w. lampson, microsoft corporation, cstb memberemeritusteresa h. meng, stanford universitytom m. mitchell, carnegie mellon universitydaniel pike, gci cable and entertainmenteric schmidt, google inc.fred b. schneider, cornell universitywilliam stead, vanderbilt universityandrew j. viterbi, viterbi group, llccharles n. brownstein, directorkristen batch, research associatejennifer m. bishop, program associatejanet briscoe, manager, program operationsjon eisenberg, senior program officerrenee hawkins, financial associatemargaret marsh huynh, senior program assistantherbert s. lin, senior scientistlynette i. millett, senior program officerjanice sabuda, senior program assistantgloria westbrook, senior program assistantbrandye williams, staff assistantfor more information on cstb, see its web site at <http://www.cstb.org>, writeto cstb, national research council, 500 fifth street, n.w., washington, dc 20001,call (202) 3342605, or email the cstb at cstb@nas.edu.vgetting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.getting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.prefacehighperformance computing is important in solving complexproblems in areas from climate and biology to national security.several factors have led to the recent reexamination of the rationale for federal investment in research and development in support ofhighperformance computing, including (1) continuing changes in thevarious component technologies and their markets, (2) the evolution ofthe computing market, particularly the highend supercomputing segment, (3) experience with several systems using the clustered processorarchitecture, and (4) the evolution of the problems, many of them missiondriven, for which supercomputers are used.the department of energyõs (doeõs) office of science expressed aninterest in sponsoring a study by the computer science and telecommunications board (cstb) of the national research council (nrc) thatwould assess the state of u.s. supercomputing capabilities and relevantresearch and development. spurred by the development of the japanesevectorbased earth simulator supercomputer, the senateõs energy andwater development appropriations committee directed the advancedsimulation and computing (asc) program of the national nuclear security administration (nnsa) at doe to commission, in collaboration withdoeõs office of science, a study by the nrc. congress also commissioned a study by the jasons1 to identify the distinct requirements ofthe stockpile stewardship program and its relation to the asc acquisitionstrategy.1formed in 1959, the jasons are a select group of scientific advisors who consult with thefederal government, chiefly on classified research issues.viigetting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.viiiprefacecstb convened the committee on the future of supercomputing toassess prospects for supercomputing technology research and development in support of u.s. needs, to examine key elements of contextñthehistory of supercomputing, the erosion of research investment, the changing nature of the problems demanding supercomputing, and the needs ofgovernment agencies for supercomputing capabilitiesñand to assess opportunities for progress. the 18 distinguished members of the study committee (see appendix a for their biographies) were drawn from academia,industry, and government research organizations in the united states.several committee members have had previous government and/or industry service. their collective expertise includes software, computer architecture, performance assessment, applications using supercomputing,economics, and policy matters.the committee did its work through its own expert deliberations andby soliciting input from key officials in its sponsoring agency (doe) andnumerous experts in both the united states and japan, including government officials, academic researchers, supercomputer manufacturers, software vendors, supercomputer center managers, and application users ofsupercomputing systems (see appendix b). in addition to meeting sixtimes, the committee hosted a workshop attended by more than 20 scientists from a broad range of disciplines to explore the supercomputingneeds and opportunities of key scientific domains in the coming decadeand to discuss the supercomputing technologies that will facilitate supercomputer use in these domains. many of the workshop participants provided white papers (see appendix c for a list) expressing their views oncomputational challenges in supercomputing, which informed both theworkshop and this report.the committee also visited five doe supercomputer centers and thenational security agencyõs (nsaõs) supercomputer center (see appendix b). a subset of the committee received classified briefings from thedepartment of energy on stockpile stewardship and from the nsa onsignals intelligence that helped illuminate how these mission requirements drive supercomputing needs now and in the future. given that asignificant fraction of government funding of supercomputing is for classified national security programs, the committee believed such briefingswere needed to ensure that its report would be useful for the entiresupercomputing community. having received the briefings, the committee believes that the needs of the classified supercomputing applicationsreinforce, but do not change, the committeeõs findings and recommendations for the future of supercomputing. this unclassified report does nothave a classified annex, nor is there a classified version.to facilitate communication within the broader community, the committee hosted a town hall meeting at the annual 2003 supercomputinggetting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.prefaceixconference in phoenix, arizona. in addition, a subset of the committeespent one week in japan meeting with senior colleagues from the japanese government, industry, and academia to discuss scientific, technical,and policy issues of mutual interest and to better understand both thesimilarities and the differences in how the two countries approach supercomputing. they visited several sites in japan, including the earth simulator; the government ministry responsible for funding the earth simulator; a university supercomputer center; japanõs aerospace explorationagency; and an auto manufacturer. on the committeeõs behalf, the national academy of engineering cosponsored with the engineering academy of japan a 1day forum in tokyo on the future of supercomputing.twentyfive japanese supercomputing experts participated in the forum.the sharing of ideas in those meetings provided important perspectivesthat contributed to the completeness and accuracy of this report. it is thehope of the committee that activities such as the tokyo forum will lead tofuture collaboration between japan and the united states in areas thatwill advance supercomputing in both countries.in july 2003, the committee released an interim report2 that provideda highlevel description of the state of u.s. supercomputing, the needs ofthe future, and the factors that contribute to meeting those needs. thatreport generated a number of comments that helped to guide the committee in its work for this final report. additional inputs helpful to committee members and staff came from professional conferences, the technicalliterature, and government reports.the committee is grateful to the many people who contributed to thiscomplex study and its comprehensive report. first and foremost, the committee thanks the sponsors, doeõs office of science (fred johnson anddan hitchcock) and doeõs nnsa (dimitri kusnezov, edgar lewis, andjos” muœoz), not only for their financial support but also for their help infacilitating meetings with people with whom its members wished tospeak.the committee appreciates the thoughtful testimony received frommany individuals at its plenary sessions (see appendix b for a completelist of briefers). the nsa and doe site visits provided critical input to thecommittee deliberations. these site visits would not have been possiblewithout the assistance of people at each locale. the committee and staffthank the following people for their help: gary d. hughes (nsa), lynnkissel (lawrence livermore national laboratory), james s. peery (losalamos national laboratory),horst d. simon (lawrence berkeley na2national research council (nrc). 2003. the future of supercomputing: an interim report.washington, d.c.: the national academies press.getting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.xprefacetional laboratory), robert thomas (sandia national laboratories), rickstevens (argonne national laboratory), and thomas zacharia (oak ridgenational laboratory).the committee thanks the workshop participants for the insights theycontributed through their white papers (see appendix c for a list of papers), discussions, breakout sessions, and subsequent interactions. thecommittee is particularly grateful to warren washington (national center for atmospheric research), charles mcmillan (lawrence livermorenational laboratory), jeffrey saltzman (merck research laboratory), andphillip colella (lawrence berkeley national laboratory) for their thoughtful plenary presentations.many people were instrumental in making the trip to japan a success.the committee is extremely grateful to kenichi miura (fujitsu fellow) andtadashi watanabe (nec) for their assistance before and during the trip.the 1day japanðu.s. forum on the future of supercomputing would nothave been possible without the support of the engineering academy ofjapan and the national academy of engineering. the committee learneda lot from insightful presentations and discussions from all the japaneseforum participants. the committee and staff also thank the individuals ateach site who took time to meet with the committee. in particular, theythank tetsuya sato at the earth simulator center and harumasa miura atthe ministry of education, culture, sports, science, and technology. makiharaga provided excellent translation services and logistical help for thecommitteeõs entire trip.the committee was fortunate to receive many thoughtful and perceptive comments from the reviewers as well as from the monitor and thecoordinator of this report. these comments were instrumental in helpingthe committee to sharpen and improve its report.finally, the committee thanks the various members of the nrc staffwho helped to move this report from vision to reality. cynthia pattersonprovided continuing wisdom, guidance, encouragement, and friendship,in concert with her hard work on the report. margaret huynhõs skills inorganizing the committeeõs meetings and supporting its efforts and philhilliardõs research support were key contributions to the work of the committee. liz fikre edited the final manuscript for publication. kevin haleand machelle reynolds successfully facilitated the security clearances andsecurity review necessary to complete this study in a timely manner.janice mehler and liz panos were very helpful in facilitating and expediting the review process.susan l. graham and marc snir, cochairscommittee on the future of supercomputinggetting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.acknowledgment of reviewersxithis report has been reviewed in draft form by individuals chosenfor their diverse perspectives and technical expertise, in accordancewith procedures approved by the national research councilõs(nrcõs) report review committee. the purpose of this independent review is to provide candid and critical comments that will assist the institution in making the published report as sound as possible and to ensurethat the report meets institutional standards for objectivity, evidence, andresponsiveness to the study charge. the review comments and draftmanuscript remain confidential to protect the integrity of the deliberativeprocess. we wish to thank the following individuals for their review ofthis report:mark e. dean, ibm,steven gottlieb, indiana university,shane mitchell greenstein, northwestern university,sidney karin, university of california, san diego,ken kennedy, rice university,richard loft, national center for atmospheric research,j. andrew mccammon, university of california, san diego,kenichi miura, national institute of informatics,michael norman, university of illinois, urbanachampaign,richard proto, national security agency (retired),daniel a. reed, university of north carolina, chapel hill,ahmed sameh, purdue university,gary smaby, smaby group, inc.,getting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.burton j. smith, cray inc.,allan edward snavely, san diego supercomputing center,william w. stead, vanderbilt university, andpaul tackley, university of california, los angeles.although the reviewers listed above have provided many constructive comments and suggestions, they were not asked to endorse the conclusions or recommendations, nor did they see the final draft of the reportbefore its release. the review of this report was overseen by elsa garmireof dartmouth university and samuel h. fuller of analog devices, inc.appointed by the nrc, they were responsible for making certain that anindependent examination of this report was carried out in accordance withinstitutional procedures and that all review comments were carefully considered. responsibility for the final content of this report rests entirelywith the authoring committee and the institution.xiiacknowledgement of reviewersgetting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.contentsxiiiexecutive summary11introduction and context11study context, 12,computenik, 15,about the interim report, 18,organization of the report, 19,2explanation of supercomputing203brief history of supercomputing28the prehistory of u.s. supercomputing, 28,supercomputers emerge as a market, 31,control data and cray, 33,enter japan, 36,innovation in supercomputing, 38,recent developments in supercomputing, 43,the u.s. highperformance computing industry today, 44,an industrial revolution, 53,impacts, 61direct contributions, 62,spillover effects, 65,getting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.xivcontents4the demand for supercomputing67compelling applications for supercomputing, 70common themes and synergies across applications areas, 72selected application areas, 74stockpile stewardship, 74signals intelligence, 76defense, 78climate modeling, 79plasma physics, 84transportation, 85bioinformatics and computational biology, 89societal health and safety, 92earthquakes, 93geophysical exploration and geoscience, 94astrophysics, 96materials science and computational nanotechnology, 97human/organizational systems studies, 100projected computing needs for applications, 1015todayõs supercomputing technology104supercomputer architecture, 105scaling of technology, 105types of supercomputers, 111performance issues, 113tradeoffs, 118trends in supercomputer architecture, 121supercomputing algorithms, 125solving partial and ordinary differential equations, 126mesh generation, 128dense linear algebra, 129sparse linear algebra, 129discrete algorithms, 130fast transforms, 131new algorithmic demands arising from supercomputing, 131disciplinary needs, 131interdisciplinary needs, 132synthesis, sensitivity analysis, and optimizationreplacing analysis, 133huge data sets, 133changing machine models, 133supercomputing software, 134operating systems and management software, 135programming models, programming languages, and tools, 138getting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.contentsxvlibraries, 142applications software, 142reliability and fault tolerance, 145performance estimation, 146performance benchmarks, 146performance monitoring, 148performance modeling and simulation, 148performance estimation and the procurement process, 150the imperative to innovate and barriers to innovation, 151systems issues, 151issues for algorithms, 153an example from computational fluid dynamics, 154software issues, 1556supercomputing infrastructures and157institutionssupercomputing ecosystem creation and maintenance, 161how ecosystems get established, 162potential barriers for new ecosystems, 166ecosystem workforce, 170consumer institutions, 174supercomputing centers, 174industrial supercomputing, 1767supercomputing abroad180japan, 182similarities, 182differences, 183the earth simulator, 183other japanese centers, 187china, 188europe, 189united kingdom, 189germany, 190france, 190spain, 191application software, 1918a policy framework192the government as the leading user and purchaser ofsupercomputer technology, 193supercomputer technology investments as public goods, 194potential costs of government intervention, 195getting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.xvicontentsalternative modes for government intervention, 196government incentives, 197government research, 199competing government objectives, 201coordination versus diversification, 201commitment versus flexibility, 201secrecy versus spillovers, 2029stewardship and funding of supercomputing206satisfying current supercomputing needs, 208ensuring future supercomputing leadership, 209the need for hardware and software producers, 209the need for stability, 210the need for a continuum from research to production, 211the need for money, 216the need for people, 218the need for planning and coordination, 219a supercomputing roadmap, 220responsibility and oversight, 22210the future of supercomputingñconclusions225and recommendationsconclusions, 225recommendations, 230appendixesacommittee member and staff biographies249bspeakers and participants at meetings and site visits263clist of white papers prepared for the applications workshop276dglossary and acronym list278getting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.1executive summarysupercomputing is very important to the united states for conducting basic scientific research and for ensuring the physical and economic wellbeing of the country. the united states has a proud history of leadership in supercomputing, which has contributed not only toits international standing in science and engineering and to national healthand security but also to the commercial strength of many industries, including the computing industry. supercomputing has become a majorcontributor to the economic competitiveness of our automotive, aerospace,medical, and pharmaceutical industries.the discovery of new substancesand new techniques, as well as cost reduction through simulation ratherthan physical prototyping, will underpin progress in a number of economically important areas. the use of supercomputing in all of these areas is growing, and it is increasingly essential to continued progress.however, in recent years our progress in supercomputing has slowed,as attention turned to other areas of science and engineering. the advancesin mainstream computing brought about by improved processor performance have enabled some former supercomputing needs to be addressedby clusters of commodity processors. yet important applications, somevital to our nation’s security, require technology that is only available inthe most advanced custombuilt systems. we have been remiss in attending to the conduct of the longterm research and development we will oneday need and to the sustenance of the industrial capabilities that will alsobe needed. the japanese earth simulator has served as a wakeup call,reminding us that complacency can cause us to lose not only our competigetting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.2getting up to speedtive advantage but also, and more importantly, the national competencethat we need to achieve our own goals.to maintain our level of achievement in supercomputing and its applications, as well as to keep us from falling behind relative to other nations and to our own needs, a renewed national effort is needed. thateffort must have the following components:•government leadership in maintaining a national planning activity that is sustained, ongoing, and coordinated and that drives investmentdecisions.•continuing progress in creating hardware, software, and algorithmic technologies that enable the application of supercomputing to important domainspecific problems. such progress will require continuing government investment.•international collaborations in all aspects of supercomputing except those that would demonstrably compromise national security.supercomputing has always been a specialized form of computing atthe cutting edge of technology. as the computing field has grown andmatured, computing has become broader and more diverse. from an economic perspective, there are large new markets that are distinct fromsupercomputing—for example, personal computing devices of variouskinds, computers invisibly embedded in many kinds of artifacts, and applications that use large amounts of computing in relatively undemanding ways. as a consequence, potential providers of supercomputing systems and software and potential creators of future supercomputingtechnology are fewer in number than they once were. in the face of continuing need and the competing demands that weaken supply, the committee recommends that the following actions and policies be initiated.overall recommendation:to meet the current and future needs ofthe united states, the government agencies that depend on supercomputing, together with the u.s. congress, need to take primaryresponsibility for accelerating advances in supercomputing andensuring that there are multiple strong domestic suppliers of bothhardware and software.the government is the primary user of supercomputing. governmentfunded research that relies on supercomputing is pushing the frontiers of knowledge and bringing important societal benefits. becausesupercomputing is essential to maintain u.s. military superiority, toachieve the goals of stockpile stewardship, and to maintain national security, the government must ensure that the u.s. supercomputing infrastructure advances sufficiently to support our needs in the coming years.getting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.executive summary3these needs are distinct from those of the broad information technologyindustry. they involve platforms and technologies that are unlikely ontheir own to have a broad enough market in the short term to satisfygovernment needs.to guide the government agencies and congress in assuming thatresponsibility, the committee makes eight recommendations.recommendation 1.to get the maximum leverage from the national effort, the government agencies that are the major users ofsupercomputing should be jointly responsible for the strength andcontinued evolution of the supercomputing infrastructure in theunited states, from basic research to suppliers and deployed platforms. the congress should provide adequate and sustained funding.a small number of government agencies are the primary users ofsupercomputing, either directly, through acquisitions, or indirectly, byawarding contracts and grants to other organizations that purchasesupercomputers. at present, those agencies include the department ofenergy (doe), including its national nuclear security administration(nnsa) and its office of science; the department of defense (dod), including its national security agency (nsa); the national aeronauticsand space administration (nasa); the national oceanic and atmospheric administration (noaa); and the national science foundation(nsf). (the increasing use of supercomputing in biomedical applicationssuggests that the national institutes of health (nih) should be added tothe list.) although the agencies have different missions and differentneeds, they could benefit from the synergies of coordinated planning andacquisition strategies and coordinated support for r&d. for instance,many of the technologies, in particular the software technology, need tobe broadly available across all platforms. therefore, those agencies mustbe jointly responsible and jointly accountable. moreover, for the agenciesto meet their own mission responsibilities and also take full advantage ofthe investments made by other agencies, collaboration and coordinationmust become much more long range. the agencies that are the biggestusers of supercomputing must develop and execute an integrated plan.the committee emphasizes the need for developing an integrated planrather than coordinating distinct supercomputing plans through a diffuseinteragency structure. an integrated plan is not an integrated budget.such a plan would not preclude agencies from individual activities, norwould it prevent them from setting their own priorities. also, it must notbe used to the exclusion of unanticipated needs and opportunities. rather,the intent is to identify common needs at an early stage, and to leverageshared efforts for meeting those needs, while minimizing duplicative efgetting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.4getting up to speedforts. different agencies should pick the activities that best match theirmissions; for example, longterm basic research best matches nsf’s mission, while industrial supercomputing r&d is more akin to the mission ofthe defense advanced research projects agency (darpa).recommendation 2.the government agencies that are the primaryusers of supercomputing should ensure domestic leadership inthose technologies that are essential to meet national needs.current u.s. investments in supercomputing and current plans arenot sufficient to provide the supercomputing capabilities that our countrywill need. it needs supercomputers that satisfy critical requirements inareas such as cryptography and stockpile stewardship, as well as systemsthat will enable breakthroughs for the broad scientific and technologicalprogress underlying a strong and robust u.s. economy. the committee isless concerned that the topranked computer in the top500 list (as of june2004) was located in japan. u.s. security is not necessarily endangered if acomputer in a foreign country is capable of doing some computationsfaster than u.s.based computers. the committee believes that had theunited states at that time made an investment similar to the japaneseinvestment in the earth simulator, it could have created a powerful andequally capable system. the committee’s concern is that the united stateshas not been making the investments that would guarantee its ability tocreate such a system in the future.leadership is measured by the ability to acquire and exploit effectively machines that can best reduce the time to solution of importantcomputational problems. from this perspective, it is not the earth simulator system per se that is worrisome but rather the fact that the construction of this system might turn out to have been a singular event. it appears that custom highbandwidth processors such as those used by theearth simulator are not viable products without significant governmentsupport. two of the three japanese companies that were manufacturingsuch processors do not do so anymore, and the third (nec) may also bowto market realities in the nottoodistant future, since the japanese government seems less willing now to subsidize the development of cuttingedge supercomputing technologies. only by maintaining national leadership in these technologies can the u.s. government ensure that keysupercomputing technologies, such as custom highbandwidth processors, will be available to satisfy its needs. the u.s. industrial base mustinclude suppliers on whom the government can rely to build custom systems to solve problems arising from the government’s unique requirements. since only a few units of such systems are ever needed, there is nobroad market for the systems and hence no commercial offtheshelf suppliers. domestic supercomputing vendors can become a source of bothgetting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.executive summary5the components and the engineering talent needed for building these custom systems.recommendation 3.to satisfy its need for unique supercomputingtechnologies such as highbandwidth systems, the governmentneeds to ensure the viability of multiple domestic suppliers.supercomputers built out of commodity components satisfy a largefraction of supercomputing applications. these applications benefit fromthe fast evolution and low cost of commodity technology. but commoditycomponents are designed for the needs of large markets in data processing or personal computing and are inadequate for many supercomputingapplications. the use of commodity clusters results in lower sustainedperformance and higher programming costs for some demanding applications. this is especially true of some securityrelated computationswhere shorter time to solution is of critical importance, justifying the useof custombuilt, highbandwidth supercomputers even at a higher costper solution.it is important to have multiple suppliers for any key technology inorder to maintain competition, to prevent technical stagnation, to providediverse supercomputing ecosystems that will address diverse needs, andto reduce risk. however, it is unrealistic to expect that such narrow markets will attract a large number of vendors. as is true for many militarytechnologies, there may be only a few suppliers.to ensure their continuing existence, domestic suppliers must followa viable business model. for a public company, that means having a predictable and steady revenue stream recognizable by the financial market.a company cannot continue to provide leadership products without r&d.at least two models have been used successfully in the past: (1) an implicit guarantee for the steady purchase of supercomputing systems, giving the companies a steady income stream with which to fund ongoingr&d and (2) explicit funding of a company’s r&d. stability is a key issue.suppliers of such systems or components are often small companies thatcan cease to be viable; additionally, uncertainty can mean the loss ofskilled personnel to other sectors of the computing industry or the loss ofinvestors. historically, government priorities and technical directionshave changed more frequently than would be justified by technology lifetimes, creating market instabilities. the chosen funding model must ensure stable funding.recommendation 4.the creation and longterm maintenance of thesoftware that is key to supercomputing requires the support of thoseagencies that are responsible for supercomputing r&d. that software includes operating systems, libraries, compilers, software development and data analysis tools, application codes, and databases.getting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.6getting up to speedsupercomputer software is developed and maintained by the nationallaboratories, by universities, by vertically integrated hardware vendors,and by small independent companies. an increasing amount of the software used in supercomputing is developed in an open source model.many of the supercomputing software vendors are small and can disappear from the marketplace. the open source model may suffer from having too few developers of supercomputing software with too many otherdemands on their time.the successful evolution and maintenance of complex software systems are critically dependent on institutional memory—that is, on the continuous involvement of the few key developers that understand the software design. stability and continuity are essential to preserve institutionalmemory. whatever model of support is used, it should be implementedso that stable organizations with lifetimes of decades can maintain andevolve the software. at the same time, the government should not duplicate successful commercial software packages but should instead investin new technology. when new commercial providers emerge, the government should purchase their products and redirect its own efforts toward technology that it cannot otherwise obtain.barriers to the replacement of application programming interfaces arevery high, owing to the large sunk investments in application software.any change that significantly enhances our nation’s ability to programvery large systems will entail the radical, coordinated change of manytechnologies, creating a new ecosystem. to facilitate this change, the government needs longterm, coordinated investments in a large number ofinterlocking technologies.recommendation 5.the government agencies responsible forsupercomputing should underwrite a community effort to developand maintain a roadmap that identifies key obstacles and synergiesin all of supercomputing.the challenges in supercomputing are very significant, and theamount of ongoing research is limited. to make progress, it is importantto identify and address the key roadblocks. furthermore, technologies indifferent domains are interdependent: progress on a new architecture mayalso require specific advances in packaging, interconnects, operating system structures, programming languages and compilers, and the like. thus,investments need to be coordinated. to drive decisions, one needs aroadmap of all the technologies that affect supercomputing. the roadmapneeds to have quantitative and measurable milestones. its creation andmaintenance should be an open process that involves a broad community. it is important that a supercomputing roadmap be driven both topdown by application needs and bottomup by technology barriers andgetting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.executive summary7that mission needs as well as science needs be incorporated. it shouldfocus on the evolution of each specific technology and on the interplaybetween technologies. it should be updated annually and undergo majorrevision at suitable intervals.the roadmap should be used by agencies and by congress to guidetheir longterm research and development investments. those roadblocksthat will not be addressed by industry without government interventionmust be identified, and the needed research and development must beinitiated. metrics must be developed to support the quantitative aspectsof the roadmap. it is important also to invest in some highrisk, highreturn research ideas that are not indicated by the roadmap, to avoid being blindsided.recommendation 6.government agencies responsible forsupercomputing should increase their levels of stable, robust, sustained multiagency investment in basic research. more research isneeded in all the key technologies required for the design and useof supercomputers (architecture, software, algorithms, and applications).the peak performance of supercomputers has increased rapidly inthe last decades, but their sustained performance has lagged, and the productivity of supercomputing users has lagged. over the last decade theadvance in peak supercomputing performance was largely due to the advance in microprocessor performance driven by increased miniaturization, with some contribution from increased parallelism. perhaps becausea large fraction of supercomputing improvements resulted from theseadvances, few novel technologies were introduced in supercomputer systems, and supercomputing research investments decreased. however,many important applications have not benefited from these advances inmainstream computing, and it will be harder for supercomputing to benefit from increased miniaturization in the future. fundamental breakthroughs will be needed that will require an increase in research funding.the research investments should be informed by the supercomputingroadmap but not constrained by it. it is important to focus on technologies that have been identified as roadblocks and that are beyond the scopeof industry investments in computing. it is equally important to supportlongterm speculative research in potentially disruptive technical advances. the research investment should also be informed by the “ecosystem” view of supercomputing—namely, that progress is often needed ona broad front of interrelated technologies rather than as individual breakthroughs.research on supercomputing hardware and software should includea mix of small, medium, and large projects. many small individual projectsgetting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.8getting up to speedare necessary for the development of new ideas. a smaller number oflarge projects that develop technology demonstrations are needed to bringthese ideas to maturity and to study the interaction between various technologies in a realistic environment. such demonstration projects (whichare different from product prototyping activities) should not be expectedto be stable platforms for exploitation by users, because the need to maintain a stable platform conflicts with the ability to use the platform forexperiments. it is important that the development of such demonstrationsystems have the substantial involvement not only of academic researchers but also of students, to support the education of the new generation ofresearchers and to increase the supercomputing workforce. it is also important that the fruits of such projects not be proprietary. the committeeestimated the necessary investments in such projects at about $140 million per year. this estimate does not include investments in the development and use of applicationspecific software.in its early days, supercomputing research generated many ideas thateventually became broadly used in the computing industry. such influences will continue in the future. many of the technical roadblocks facedtoday by supercomputing are roadblocks that will affect all computingover time. there can be little doubt that solutions developed to solve thisproblem for supercomputers will eventually influence the broader computing industry, so that investment in basic research in supercomputingis likely to be of widespread benefit to all of information technology.recommendation 7.supercomputing research is an internationalactivity; barriers to international collaboration should be minimized.research has always benefited from the open exchange of ideas andthe opportunity to build on the achievements of others. the national leadership advocated in these recommendations is enhanced, not compromised, by earlystage sharing of ideas and results. in light of the relativelysmall community of supercomputing researchers, international collaborations are particularly beneficial. the climate modeling community, forone, has long embraced that view.collaboration with international researchers must include givingthem access to domestic supercomputing systems; they often spend timein the united states to work closely with resident scientists. many of thebest u.s. graduate students come from other countries, although they often remain as permanent residents or new citizens. access restrictionsbased on citizenship hinder collaboration and are contrary to the openness that is essential to good research.restrictions on the import of supercomputers to the united stateshave not benefited the u.s. supercomputing industry and are unlikely togetting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.executive summary9do so in the future. some kinds of export controls—on commodity systems, especially—lack any clear rationale, given that such systems arebuilt from widely available commercial components. it makes little senseto restrict sales of commodity systems built from components that are notexport controlled. because restrictions on the export of supercomputingtechnology may damage international collaboration, the benefit of usingexport controls to prevent potential adversaries or proliferators from accessing key supercomputing technology has to be carefully weighedagainst that damage.since supercomputer systems are multipurpose (nuclear simulations,climate modeling, and so on), their availability need not compromise thedomestic leadership needed for national defense, so long as safeguardsare in place to protect critical applications.recommendation 8.the u.s. government should ensure that researchers with the most demanding computational requirementshave access to the most powerful supercomputing systems.access to the most powerful supercomputers is important for the advancement of science in many disciplines. a model in which top supercomputing capabilities are provided by different agencies with differentmissions is healthy. each agency is the primary supporter of certain research or missiondriven communities; as such, each agency should havea longterm plan and budget for the acquisition of the supercomputingsystems that are needed to support its users. the planning and fundingprocess followed by each agency must ensure stability from the viewpoint of its users.the users should be involved in the planning process and should beconsulted in setting budget priorities for supercomputing. the mechanisms for allocating supercomputing resources must ensure that almostall of the computer time on capability systems is allocated to jobs for whichthat capability is essential. budget priorities should be reflected in thehighend computing plan proposed in recommendation 1. in chapter 9,the committee estimates the cost of a healthy procurement process at about$800 million per year. such a process would satisfy the capability supercomputing needs (but not the capacity needs) of the main agencies usingsupercomputing and would include the platforms primarily used for research. it would include both platforms used for missionspecific tasksand platforms used to support science.the nsf supercomputing centers have traditionally provided openaccess to a broad range of academic users. however, some of these centershave increased the scope of their activities in order to support highspeednetworking and grid computing and to expand their education mission.the increases in scope have not been accompanied by corresponding ingetting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.10getting up to speedcreases in funding, so less attention is paid to supercomputing, and support for computational scientists with capability needs has been diluted.it is important to repair the current situation at nsf, in which thecomputational science users of supercomputing centers appear to havetoo little involvement in programmatic and budgetary planning. all theresearch communities in need of supercomputing capability have a sharedresponsibility to provide direction for the supercomputing infrastructurethey use and to ensure that resources are available for sustaining thesupercomputing ecosystems. funding for the acquisition and operationof the research supercomputing infrastructure should be clearly separatedfrom funding for computer and computational science and engineeringresearch. it should compete on an equal basis with other infrastructureneeds of the science and engineering disciplines. that is not now the case.getting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.111introduction and contextsupercomputers are used to solve complex problems, including thesimulation and modeling of physical phenomena such as climatechange, explosions, and the behavior of molecules; the analysis ofdata such as national security intelligence, genome sequencing, and astronomical observations; and the intricate design of engineered products.their use is important for national security and defense, as well as forresearch and development in many areas of science and engineering.supercomputers can advance knowledge and generate insight that wouldnot otherwise be possible or that could not be captured in time to beactionable. supercomputer simulations can augment or replace experimentation in cases where experiments are hazardous, expensive, or evenimpossible to perform or to instrument; they can even enable virtual experiments with imaginary worlds to test theories beyond the range of observable parameters. further, supercomputers have the potential to suggest entirely novel experiments that can revolutionize our perspective ofthe world. they enable faster evaluation of design alternatives, thus improving the quality of engineered products. most of the technical areasthat are important to the wellbeing of humanity use supercomputing infundamental and essential ways.as the uses of computing have increased and broadened, supercomputing has become less dominant than it once was. many interesting applications require only modest amounts of computing, by todayõs standards. yet new problems have arisen whose computational demands forscaling and timeliness stress even our current supercomputers. many ofgetting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.12getting up to speedthose problems are fundamental to the governmentõs ability to addressimportant national issues. one notable example is the department ofenergyõs (doeõs) computational requirements for stockpile stewardship.the emergence of mainstream solutions to problems that formerlyrequired supercomputing has caused the computer industry, the researchand development community, and some government agencies to reducetheir attention to supercomputing. recently, questions have been raisedabout the best ways for the government to ensure that its supercomputingneeds will continue to be satisfied in terms of both capability and costeffectiveness. at the joint request of the doeõs office of science and theadvanced simulation and computing1 (asc) program of the nationalnuclear security administrations (nnsa) at doe, the national researchcouncilõs (nrcõs) computer science and telecommunications board convened the committee on the future of supercomputing to conduct a 2year study to assess the state of supercomputing in the united states. specifically, the committee was charged to do the following:¥examine the characteristics of relevant systems and architectureresearch in government, industry, and academia and the characteristics ofthe relevant market.¥identify key elements of context such as the history of supercomputing, the erosion of research investment, the needs of governmentagencies for supercomputing capabilities, and historical or causal factors.¥examine the changing nature of problems demanding supercomputing (e.g., stockpile stewardship, cryptanalysis, climate modeling,bioinformatics) and the implications for systems design.¥outline the role of national security in the supercomputer marketand the longterm federal interest in supercomputing.¥deliver an interim report in july 2003 outlining key issues.¥make recommendations in the final report for government policyto meet future needs.study contextmuch has changed since the 1980s, when a variety of agencies invested in developing and using supercomputers. in the 1990s the highperformance computing and communications initiative (hpcci) wasconceived and subsequently evolved into a broader and more diffuse pro1asc was formerly known as the accelerated strategic computing initiative (asci). thisreport uses asc to refer collectively to these programs.getting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.introduction and context13gram of computer science research support.2 over the last couple of decades, the government sponsored numerous studies dealing withsupercomputing and its role in science and engineering research.3following the guidelines of the report of the panel on large scale computing in science and engineering (the lax report),4 the national sciencefoundation (nsf) established three supercomputer centers and one advanced prototype in 1985 and another center in 1986. major projects oninnovative supercomputing systems were fundedñfor instance, thecaltech cosmic cube, the new york university (nyu) ultracomputer,and the illinois cedar project. the other recommendations of the report(to increase research in the disciplines needed for an effective and efficient use of supercomputers and to increase training of people in scientific computing) had only a modest effect. following the renewal of fourof the five nsf supercomputer centers in 1990, the national science board(nsb) commissioned the nsf blue ribbon panel on high performancecomputing to investigate future changes in the overall scientific environment due to rapid advances in computers and scientific computing.5 thepanelõs report, from desktop to teraflop: exploiting the u.s. lead in highperformance computing (the branscomb report), recommended a significant expansion in nsf investments, including accelerating progress inhighperformance computing through computer science and computational science research. the impact of these recommendations on fundingwas small.in 1991 congress passed the high performance computing act (p.l.102194),6 which called for the president to establish a national programto set goals for federal highperformance computing research and development in hardware and software and to provide for interagency cooperation.2the proliferation of pcs and the rise of the internet commanded attention and resources,diverting attention and effort from research in highend computing. there were, however,efforts into the 1990s to support highperformance computing. see, for example, nsf, 1993,from desktop to teraflop: exploiting the u.s. lead in high performance computing, nsf blueribbon panel on high performance computing, arlington, va.: nsf, august.3the committeeõs interim report provides a more detailed summary of several key reports.4national science board. 1982. report of the panel on large scale computing in science andengineering. washington, d.c., december 26 (the lax report).5nsf. 1993. from desktop to teraflop: exploiting the u.s. lead in high performance computing. nsf blue ribbon panel on high performance computing. arlington, va.: nsf, august.6bill summary and status are available online at <http://thomas.loc.gov/cgibin/bdquery/r?d102:fld002:@1(102+194)>.getting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.14getting up to speednsf formed a task force in 1995 to advise it on the review and management of the supercomputer centers program. the chief finding of thereport of the task force on the future of the nsf supercomputer centers program (the hayes report)7was that the advanced scientific computingcenters funded by nsf had enabled important research in computationalscience and engineering and had also changed the way that computational science and engineering contribute to advances in fundamentalresearch across many areas. the recommendation of the task force was tocontinue to maintain a strong advanced scientific computing centersprogram. congress asked the nrcõs computer science and telecommunications board (cstb) to examine the hpcci.8 cstbõs 1995 reportevolving the high performance computing and communications initiative tosupport the nationõs infrastructure (the brooks/sutherland report)9 recommended the continuation of the hpcci, funding of a strong experimental research program in software and algorithms for parallel computing machines, and hpcci support for precompetitive research incomputer architecture.in 1997, following the guidelines of the hayes report, nsf establishedtwo partnerships for advanced computational infrastructure (pacis),one with the san diego supercomputer center as a leadingedge site andthe other with the national center for supercomputing applications as aleadingedge site. each partnership includes participants from other academic, industry, and government sites. the paci program ended on september 30, 2004. the reports did not lead to increased funding, and nomajor new projects resulted from the recommendations of the brooks/sutherland report.in 1999, the presidentõs information technology advisorycommitteeõs (pitacõs) report to the president. information technology research: investing in our future (the pitac report) made recommendations7nsf. 1995. report of the task force on the future of the nsf supercomputer centers program.september 15.8hpcci was formally created when congress passed the highperformance computingact of 1991 (p.l. 102194), which authorized a 5year program in highperformance computing and communications. the goal of the hpcci was to òaccelerate the development offuture generations of highperformance computers and networks and the use of these resources in the federal government and throughout the american economyó (federal coordinating council for science, engineering, and technology (fccset), 1992, grand challenges:highperformance computing and communications, fy 1992 u.s. research and developmentprogram, office of science and technology policy, washington d.c.). the initiative broadened from four primary agencies addressing grand challenges such as forecasting severeweather events and aerospace design research to more than 10 agencies addressing nationalchallenges such as electronic commerce and health care.9nrc. 1995. evolving the high performance computing and communications initiative to support the nationõs infrastructure. washington, d.c.: national academy press.getting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.introduction and context15similar to those of the lax and branscomb reports.10 pitac found thatfederal information technology r&d was too heavily focused on nearterm problems and that investment was inadequate. the committeeõsmain recommendation was to create a strategic initiative to support longterm research in fundamental issues in computing, information, and communications. in response to this recommendation, nsf developed the information technology research (itr) program. this program, which wasonly partly successful in meeting the needs identified by pitac, is nowbeing phased out.in 2000, concern about the diminishing u.s. ability to meet nationalsecurity needs led to a recommendation by the defense science boardthat dod continue to subsidize a cray computer development programas well as invest in relevant longterm research.11the defense advanced research projects agency (darpa) launchedthe high productivity computing systems (hpcs) program in 2002 toprovide a new generation of economically viable, highproductivity computing systems for the national security and industrial user community in20072010. the goal is to address the gap between the capability needed tomeet mission requirements and the current offerings of the commercialmarketplace. hpcs has three phases: (1) an industrial concept phase (nowcompleted), in which cray, silicon graphics, inc. (sgi), ibm, hewlettpackard, and sun participated; (2) an r&d phase that was awarded tosun, cray, and ibm in july 2003 and lasting until 2006; and (3) fullscaledevelopment, to be completed by 2010, ideally by the two best proposalsfrom the second phase.in summary, while successive reports have emphasized the importance of increased investments in supercomputing and the importance oflongterm, strategic research, investments in supercomputing seem not tohave grown, and the focus has stayed on shortterm research, one generation ahead of products. research on the base technologies used forsupercomputing (architecture, programming languages, compilers, operating systems, etc.) has been insufficient.computenikin the spring of 2002 the japanese installed the earth simulator (es), asupercomputer to be used for geosciences applications. for over 2 years,10pitac. 1999. report to the president. information technology research: investing in our future. february.11defense science board. 2000. report of the defense science board task force on dodsupercomputing needs. washington, d.c.: office of the under secretary of defense for acquisition and technology. october 11.getting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.16getting up to speedthe top500 list12,13 has ranked it as the fastest performing supercomputerin the world. the es was designed to use custom multiprocessor vectorbased nodes and to provide good support for applications written in highperformance fortranñtechnologies that were all but abandoned in theunited states in favor of commodity scalar processors and message passing libraries. the emergence of that system has been fueling recent concerns about continued u.s. leadership in supercomputing. experts haveasserted that the earth simulator was made possible through longterm,sustained investment by the japanese government. the u.s. congress andseveral government agencies began to question what should be done toregain the supercomputing lead. while some experts have argued thatmaintaining an absolute lead in supercomputing (as measured by thetop500 list) should not be an overriding u.s. policy objective, the earthsimulator nonetheless offers important lessons about investment in, management of, and policy toward supercomputing.the defense appropriations bill for fy 2002 directed the secretary ofdefense to submit a development and acquisition plan for a comprehensive, longrange, integrated, highend computing (ihec) program. theresulting report, high performance computing for the national security community,14 released in the spring of 2003 and known as the ihec report,recommends an applied research program to focus on developing the fundamental concepts in highend computing and creating a pipeline of newideas and graduatelevel expertise for employment in industry and thenational security community. the report also emphasizes the importanceof highend computing laboratories that will test system software on dedicated largescale platforms; support the development of software toolsand algorithms; develop and advance benchmarking, modeling, andsimulations for system architectures; and conduct detailed technical re12the top500 project was started in 1993 to provide a reliable basis for tracking and detecting trends in highperformance computing. twice a year, a list of the sites operating the500 most powerful computer systems is assembled and released. the best performance onthe linpack benchmark is used for ranking the computer systems. the list contains a varietyof information, including the system specifications and its major application areas (see<http://www.top500.org> for details).13when jack dongarra, one of the people who maintains the top500 list (an authoritativesource of the worldõs 500 most powerful supercomputers), announced that the earth simulator was the worldõs fastest supercomputer, the new york times quoted him as saying, òinsome sense we have a computenik on our handsó (john markoff, 2002, òjapanese computeris worldõs fastest, as u.s. falls back,ó the new york times, april 20, page a1, c14).14available online at <http://www.hpcc.gov/hecrtfoutreach/bibliography/200302hec.pdf>.getting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.introduction and context17quirements analysis. the report suggests $390 million per year as thesteadystate budget for this program. the program plan consolidates existing darpa, doe/nnsa, and national security agency (nsa) r&dprograms and features a joint program office with director of defenseresearch and engineering (ddr&e) oversight.at the request of congress, doe commissioned (in addition to thisstudy) a classified study by the jasons to identify the distinct requirements of the stockpile stewardship program and its relation to the ascacquisition strategy. roy schwitters, the study leader, said that the report,released in 2003, concluded that òdistinct technical requirements placevalid computing demands on asc that exceed present and planned computing capacity and capability.ó15the 2003 scales report, a sciencebased case for largescale simulation,16presents a sciencebased case for balanced investment in numerous areasñsuch as algorithms, software, innovative architecture, and peopleñto ensure that the united states benefits from advances enabled by computational simulations.the highend computing revitalization task force (hecrtf) of thenational science coordination office for information technology research and development (nitrd) was chartered under the national science and technology council to develop a plan and a 5year roadmap toguide federal investments in highend computing starting with fy 2005.the report, federal plan for highend computing,17 released in may 2004,noted that the 1990s approach of building systems based on commercialofftheshelf (cots) components may not be suitable for many applications of national importance. it recommends research in alternative technologies to ensure u.s. leadership in supercomputing. the report alsocalls for an interagency collaborative approach.the senate committee on energy and natural resources held a hearing in june 2004 on the highend computing revitalization act of 2004(s. 2176).18 this bill calls for the secretary of energy to implement a research and development program in supercomputing and establish ahighend software development center. on july 8, 2004, the house passedand referred to the senate committee on commerce, science, and transportation a similar bill, the department of energy highend computing15presentation to the committee on december 3, 2003.16doe, office of science. 2003. òa sciencebased case for large scale simulation.ó scalesworkshop report, vol. 1. july. available online at <http://www.pnl.gov/scales/>.17available online at <http://www.hpcc.gov/pubs/2004hecrtf/20040702hecrtf.pdf>.18see <http://thomas.loc.gov/cgibin/query/z?c108:s.2176:>.getting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.18getting up to speedrevitalization act of 2004 (h.r. 4516), which omits the call for the software development center.19 the senate passed an amended version ofh.r. 4516 on october 11, 2004; the house is expected to consider the legislation in late november 2004.20 the house also passed and sent to thesenate the highperformance computing revitalization act of 2004 (h.r.4218),21 which amends the highperformance computing act of 1991 anddirects the president to establish a program to provide for longterm research on highperformance computing, including the technologies toadvance the capacity and capabilities of highperformance computing. italso calls for the director of the office of science and technology policy todevelop and maintain a roadmap for highperformance computing.about the interim reportan interim report was presented in july 2003, approximately 6 monthsafter the start of the study.22 the report provides a preliminary outline ofthe state of u.s. supercomputing, the needs of the future, and the factorsthat will contribute to meeting those needs. the report notes that theunited states had the lead, on the june 2003 top500 list, in the use andmanufacture of supercomputers.23 however, to meet the security anddefense needs of our nation and to realize the opportunities to use supercomputing to advance knowledge, progress in supercomputing must continue. an appropriate balance is needed for investments that evolve current supercomputing architectures and software and investments thatexploit alternative approaches that may lead to a paradigm shift. balanceis also needed between exploiting costeffective advances in widely usedhardware and software products and developing custom solutions thatmeet the most demanding needs. continuity and stability in the government funding of supercomputing appear to be essential to the wellbeingof supercomputing in the united states.organization of the reportin this report the committee first examines the requirements of different classes of applications and the architecture, software, algorithm, and19see <http://thomas.loc.gov/cgibin/bdquery/z?d108:hr04516:>.20see <http://thomas.loc.gov/cgibin/query/r?r108:fld001:s61181>.21see <http://thomas.loc.gov/cgibin/query/d?c108:3:./temp/~c108qnbgq9::>.22nrc. 2003. the future of supercomputing: an interim report. washington, d.c.: the national academies press.23based on the june 2003 top500 list at <http://www.top500.org>.getting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.introduction and context19cost challenges and tradeoffs associated with these application classes.the report addresses not only presentday applications and technology,but also the context provided by history, by institutions and communities, and by international involvement in supercomputing. chapter 2 defines supercomputing. chapter 3 outlines a brief history of supercomputing. chapter 4 describes many compelling applications that placeextreme computational demands on supercomputing. chapter 5 discussesthe design of algorithms, computing platforms, and software environments that govern the performance of supercomputing applications. theinstitutions, computing platforms, system software, and the people whosolve supercomputing applications can be thought of collectively as anecosystem. chapter 6 outlines an approach to supercomputing ecosystemcreation and maintenance. chapter 7 discusses the international dimension of supercomputing. chapter 8 offers a framework for policy analysis.chapter 9 describes the role of the government in ensuring that supercomputing appropriate to our needs is available both now and in the future. chapter 10 contains the committeeõs conclusions and recommendations for action to advance highend computing.getting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.202explanation of supercomputingthe term òsupercomputeró refers to computing systems (hardware,systems software, and applications software) that provide close tothe best currently achievable sustained performance on demanding computational problems. the term can refer either to the hardware/software system or to the hardware alone. two definitions follow:¥from landau and fink1: òthe class of fastest and most powerfulcomputers available.ó¥from the academic press dictionary of science and technology: ò1.any of a category of extremely powerful, largecapacity mainframe computers that are capable of manipulating massive amounts of data in anextremely short time. 2. any computer that is one of the largest, fastest,and most powerful available at a given time.óòsupercomputingó is used to denote the various activities involved inthe design, manufacturing, or use of supercomputers (e.g., òsupercomputing industryó or òsupercomputing applicationsó). similar terms areòhighperformance computingó and òhighend computing.ó the latterterms are used interchangeably in this report to denote the broader rangeof activities related to platforms that share the same technology assupercomputers but may have lower levels of performance.1rubin h. landau and paul j. fink, jr. 1993. a scientistõs and engineerõs guide to workstations and supercomputers. new york, n.y.: john wiley & sons.getting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.explanation of supercomputing21the meaning of the terms supercomputing or supercomputer is relative to the overall state of computing at a given time. for example, in 1994,when describing computers subject to export control, the department ofcommerceõs bureau of export administration amended its definition ofòsupercomputeró to increase the threshold level from a composite theoretical performance (ctp) equal to or exceeding 195 million theoreticaloperations per second (mtops) to a ctp equal to or exceeding 1,500mtops.2 current examples of supercomputers are contained in thetop500 list of the 500 most powerful computer systems as measured bybest performance on the linpack benchmarks.3supercomputers provide significantly greater sustained performancethan is available from the vast majority of installed contemporary mainstream computer systems. in applications such as the analysis of intelligence data, weather prediction, and climate modeling, supercomputersenable the generation of information that would not otherwise be available or that could not be generated in time to be actionable. supercomputing can accelerate scientific research in important areas such as physics, material sciences, biology, and medicine. supercomputer simulationscan augment or replace experimentation in cases where experiments arehazardous, expensive, or even impossible to perform or to instrument.they can collapse time and enable us to observe the evolution of climateover centuries or the evolution of galaxies over billions of years; they canexpand space and allow us to observe atomic phenomena or shrink spaceand allow us to observe the core of a supernova. they can save lives andmoney by producing better predictions on the landfall of a hurricane orthe impact of an earthquake.in most cases, the problem solved on a supercomputer is derived froma mathematical model of the physical world. approximations are madewhen the world is represented using continuous models (partial differential equations) and when these continuous models are discretized. validated approximate solutions will provide sufficient information to stimulate human scientific imagination or to aid human engineering judgment.as computational power increases, fewer compromises are made, andmore accurate results can be obtained. therefore, in many applicationdomains, there is essentially no limit to the amount of compute power2federal register, february 24, 1994, at <http://www.fas.org/spp/starwars/offdocs/940224.htm>.3the top500 list is available at <http://www.top500.org>. the linpack benchmark solvesa dense system of linear equations; in the version used for top500, one picks a system sizefor which the computer exhibits the highest computation rate.getting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.22getting up to speedthat can be usefully applied to a problem. as the committee shows inchapter 4, many disciplines have a good understanding of how theywould exploit supercomputers that are many orders of magnitude morepowerful than the ones they currently use; they have a good understanding of how science and engineering will benefit from improvements insupercomputing performance in the years and decades to come.one of the principal ways to increase the amount of computingachievable in a given period of time is to use parallelismñdoing multiplecoordinated computations at the same time. some problems, such assearches for patterns in data, can distribute the computational workloadeasily. the problem can be broken down into subproblems that can besolved independently on a diverse collection of processors that are intermittently available and that are connected by a lowspeed network suchas the internet.4 some problems necessarily distribute the work over ahighspeed computational grid5 in order to access unique resources suchas very large data repositories or realtime observational facilities. however, many important problems, such as the modeling of fluid flows, cannot be so easily decomposed or widely distributed. while the solution ofsuch problems can be accelerated through the use of parallelism, dependencies among the parallel subproblems necessitate frequent exchangesof data and partial results, thus requiring significantly better communication (both higher bandwidth and lower latency) between processors anddata storage than can be provided by a computational grid. both computational grids and supercomputers hosted in one machine room are components of a cyberinfrastructure, defined in a recent nsf report as òtheinfrastructure based upon distributed computer, information and communication technology.ó6 this report focuses mostly on systems hostedin one machine room (such systems often require a large, dedicated room).to maintain focus, it does not address networking except to note its importance. also, the report does not address specialpurpose hardware accelerators. specialpurpose hardware has always played an important but4a good example is seti@home: the search for extraterrestrial intelligence, <http://setiathome.ssl.berkeley.edu>.5a computational grid is a hardware and software infrastructure that provides dependable, consistent, pervasive, and inexpensive access to highend computational capabilities (i.foster and c. kesselman, 2003, the grid 2: blueprint for a new computing infrastructure, 2nded., san francisco, calif.: morgan kaufman).6nsf. 2003. revolutionizing science and engineering through cyberinfrastructure. nsf blueribbon advisory panel on cyberinfrastructure.getting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.explanation of supercomputing23limited role in supercomputing.7 the committee has no evidence that thissituation is changing. while it expects specialpurpose hardware to continue to play an important role, the existence of such systems does notaffect its discussion of generalpurpose supercomputers.supercomputers in the past were distinguished by their unique (vector) architecture and formed a clearly identifiable product category. today, clusters of commodity computers that achieve the highest performance levels in scientific computing are not very different from clustersof similar size that are used in various commercial applications. thus, thedistinction between supercomputers and mainstream computers hasblurred. any attempt to draw a clear dividing line between supercomputers and mainstream computers, e.g., by price or level of performance, will lead to arbitrary distinctions. rather than attempting to drawsuch distinctions, the discussion will cover the topmost performing systems but will not exclude other highperformance computing systems thatshare to a significant extent common technology with the topperformingsystems.virtually all supercomputers are constructed by connecting a numberof compute nodes, each having one or more processors with a commonmemory, by a highspeed interconnection network (or switch). supercomputer architectures differ in the design of their compute nodes, theirswitches, and their nodeswitch interface. the system software used onmost contemporary supercomputers is some variant of unix; most commonly, programs are written in fortran, c, and c++, augmented withlanguage or library extensions for parallelism and application libraries.global parallelism is most frequently expressed using the mpi message passing library,8 while openmp9 is often used to express parallelismwithin a node. libraries and languages that support global arrays10 are7the grape (short for gravity pipe) family of specialpurpose systems for astrophysicsis one example. see the grape web site for more information: <http://grape.astron.s.utokyo.ac.jp/grape/>.8mpi: a messagepassing interface standard; see <http://www.mpiforum.org/docs/mpi11html/mpireport.html>.9leonardo dagum and ramesh menon. 1998. òopenmp: an industrystandard api forsharedmemory programming.ó ieee journal of computational science and engineering (5)1.10tarek a. elghazawi, william w. carlson, and jesse m. draper, upc language specification (v 1.1.1), <http://www.gwu.edu/~upc/docs/upcspec1.1.1.pdf>; robert w. numrichand john reid, 1998, òcoarray fortran for parallel programming,ó sigplan fortran forum17(2), 131; j. nieplocha, r.j. harrison, and r.j. littlefield, 1996, òglobal arrays: a nonuniform memory access programming model for highperformance computers,ó journal ofsupercomputing 10, 197220; katherine yelick, luigi semenzato, geoff pike, carletonmiyamoto, ben liblit, arvind krishnamurthy, paul hilfinger, susan graham, david gay,philip colella, and alexander aiken, 1998, òtitanium: a highperformance java dialect,óconcurrency: practice and experience 10, 825836.getting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.24getting up to speedbecoming more widely used as hardware support for direct access to remote memory becomes more prevalent.most of the traditional algorithm approaches must be modified sothat they scale effectively on platforms with a large number of processors. supercomputers are used to handle larger problems or to introducemore accurate (but more computationally intensive) physical models.both may require new algorithms. some algorithms are very specializedto a particular application domain, whereas othersñfor example, meshpartitionersñare of general use.two commonly used measures of the overall productivity of highend computing platforms are capacity and capability. the largest supercomputers are used for capability or turnaround computing where themaximum processing power is applied to a single problem. the goal is tosolve a larger problem, or to solve a single problem in a shorter period oftime. capability computing enables the solution of problems that cannototherwise be solved in a reasonable period of time (for example, by moving from a twodimensional to a threedimensional simulation, using finergrids, or using more realistic models). capability computing also enablesthe solution of problems with realtime constraints (e.g., intelligence processing and analysis). the main figure of merit is time to solution. smalleror cheaper systems are used for capacity computing, where smaller problems are solved. capacity computing can be used to enable parametricstudies or to explore design alternatives; it is often needed to prepare formore expensive runs on capability systems. capacity systems will oftenrun several jobs simultaneously. the main figure of merit is sustainedperformance per unit cost. there is often a tradeoff between the two figures of merit, as further reduction in time to solution is achieved at theexpense of increased cost per solution; different platforms exhibit different tradeoffs. capability systems are designed to offer the best possiblecapability, even at the expense of increased cost per sustained performance, while capacity systems are designed to offer a less aggressive reduction in time to solution but at a lower cost per sustained performance.11a commonly used unit of measure for both capacity systems and capability systems is peak floatingpoint operations (additions or multiplications) per second, often measured in teraflops (tflops), or 1012 floatingpoint operations per second. for example, a 2003 report by the jasons1211note that the capacity or capability of a system depends on the mix of application codesit runs. the seti@home grid system provides more sustained performance for its application than is possible on any single supercomputer platform; it would provide very low sustained performance on a weather simulation.12jason program office. 2003. requirements for asci. july 29.getting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.explanation of supercomputing25estimated that within 10 years a machine of 1,000 tflops (1 petaflops)would be needed to execute the most demanding advanced simulationand computing (asc) application (compared to the thenexisting ascplatforms of highest capability, the white at lawrence livermore nationallaboratory and the q at los alamos national laboratory, of 12.3 tflopsand 20 tflops, respectively). although peak flops are a contributing factorto performance, they are only a partial measure of supercomputer productivity because performance as delivered to the user depends on muchmore than peak floatingpoint performance (e.g., on local memory bandwidth and latency or interconnect bandwidth and latency).a system designed for high capability can typically be reconfiguredinto multiple virtual lowercapacity machines to run multiple less demanding jobs in parallel. there is much discussion about the use of custom processors for capacity computing (see box 2.1). commodity clustersare frequently used for capacity computing because they provide bettercost/performance. however, for many capability applications, customprocessors give faster turnaroundñeven on applications for which theyare not the most costeffective capacity machines.a supercomputer is a scientific instrument that can be used by manydisciplines and is not exclusive to one discipline. it can be contrasted, forexample, with the hubble space telescope, which has immense potentialfor enhancing human discovery in astronomy but little potential for designing automobiles. astronomy also relies heavily on supercomputingto simulate the life cycle of stars and galaxies, after which results fromsimulations are used in concert with hubbleõs snapshots of stars and galaxies at various evolutionary stages to form consistent theoretical viewsof the cosmos. it can be argued that supercomputing is no less importantthan the hubble telescope in achieving the goal of understanding theuniverse. however, it is likely that astronomers paid much less attentionto ensuring that supercomputing resources would be available than theypaid to carefully justifying the significant cost of the telescope. in astronomy, as in other disciplines, supercomputing is essential to progressbut is not disciplinespecific enough to marshal support to ensure that it isprovided. nevertheless, as the committee heard, the net contributions ofsupercomputing, when summed over a multitude of disciplines, are noless than monumental in their impact on overall human goals. therefore,supercomputing in some sense transcends its individual uses and can bea driver of progress in the 21st century.getting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.26getting up to speedbox 2.1 custom processors and commodity processorsmost supercomputers are built from commodity processors that are designed for a broad market and are manufactured in large numbers. a smallnumber of supercomputers use custom processors that are designed toachieve high performance in scientific computing and are manufactured insmall numbers. commodity processors, because they benefit from economies of scale and sophisticated engineering, provide the shortest time tosolution (capability) and the highest sustained performance per unit cost(capacity) for a broad range of applications that have significant spatial andtemporal locality and therefore take good advantage of the caches provided by commodity processors. a small set of important scientific applications, however, has almost no locality. they achieve shorter time to solution and better sustained performance per unit cost on a custom processorthat provides higher effective local memory bandwidth on access patternshaving no locality. for a larger set of applications with low locality, customprocessors deliver better time to solution but at a higher cost per unit ofsustained performance.commodity processors are often criticized because of their low efficiency (the fraction of peak performance they sustain). however, peak performance, and hence efficiency, is the wrong measure. the system metricsthat matter are sustained performance (on applications of interest), time tosolution, and cost.the rate at which operands can be transferred to/from the processor isthe primary performance bottleneck for many scientific computing codes.1,2custom processors differ primarily in the effective memory bandwidth thatthey provide on different types of access patterns. whether a machine hasa vector processor, a scalar processor, or a multithreaded processor is asecondary issue. the main issue is whether it has efficient support for irregular accesses (gather/scatter), high memory bandwidth, and the abilityto hide memory latency so as to sustain this bandwidth. vector processors,for example, typically have a short (if any) cache line and high memorybandwidth. the vectors themselves provide a latency hiding mechanism.such features enable custom processors to more efficiently deliver the rawmemory bandwidth provided by memory chips, which often dominate system cost. hence, these processors can be more cost effective on applications that are limited by memory bandwidth.commodity processors are manufactured in high volume and hencebenefit from economies of scale. the high volume also justifies sophisticated engineeringñfor example, the clock rate of the latest intel xeon processor is at least four times faster than the clock rate of the cray x1. acommodity processor includes much of its memory system but little of itsmemory capacity on the processor chip, and this memory system is adaptedfor applications with high spatial and temporal locality. a typical commodity processor chip includes the level 1 and 2 caches on the chip and anexternal memory interface. this external interface limits sustained localmemory bandwidth and requires local memory accesses to be performedin units of cache lines (typically 64 to 128 bytes in length3). accessinggetting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.explanation of supercomputing27memory in units of cache lines wastes a large fraction (as much as 94percent) of local memory bandwidth when only a single word of the cacheline is needed.many scientific applications have sufficient spatial and temporal locality that they provide better performance per unit cost on commodity processors than on custom processors. some scientific applications can besolved more quickly using custom processors but at a higher cost. someusers will pay that cost; others will tolerate longer times to solution orrestrict the problems they can solve to save money. a small set of scientificapplications that are bandwidthintensive can be solved both more quicklyand more cheaply using custom processors. however, because this application class is small, the market for custom processors is quite small.4in summary, commodity processors optimized for commercial applications meet the needs of most of the scientific computing market. for themajority of scientific applications that exhibit significant spatial and temporal locality, commodity processors are more cost effective than customprocessors, making them better capability machines. for those bandwidthintensive applications that do not cache well, custom processors are morecost effective and therefore offer better capacity on just those applications.they also offer better turnaround time for a wider range of applications,making them attractive capability machines. however, the segment of thescientific computing marketñbandwidthintensive and capabilityñthatneeds custom processors is too small to support the free market development of such processors.the above discussion is focused on hardware and on the current state ofaffairs. as the gap between processor speed and memory speed continuesto increase, custom processors may become competitive for an increasingrange of applications. from the software perspective, systems with fewer,more powerful processors are easier to program. increasing the scalabilityof software applications and tools to systems with tens of thousands orhundreds of thousands of processors is a difficult problem, and the characteristics of the problem do not behave in a linear fashion. the cost of using,developing, and maintaining applications on custom systems can be substantially less than the comparable cost on commodity systems and maycancel out the apparent cost advantages of hardware for commoditybasedhighperformance systemsñfor applications that will run only on customsystems. these issues are discussed in more detail in chapter 5.1l. carrington, a. snavely, x. gao, and n. wolter. 2003. a performance prediction framework for scientific applications. iccs workshop on performance modeling and analysis(pma03). melbourne, june.2s. goedecker and a. hoisie. 2001. performance optimization of numerically intensivecodes. philadelphia, pa.: siam press.3the ibm power 4 has a 512byte level 3 cache line.4this categorization of applications is not immutable. since commodity systems are cheaperand more broadly available, application programmers have invested significant effort in adapting applications to these systems. bandwidthintensive applications are those that are not easilyadapted to achieve acceptable performance on commodity systems. in many cases the difficultyseems to be intrinsic to the problem being solved.getting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.283brief history of supercomputingthis chapter touches on the role, importance, and special needs ofsupercomputing.1it outlines the history of supercomputing, theemergence of supercomputing as a market, the entry of the japanese supercomputing manufacturers, and the impact of supercomputingon the broader computer market and on progress in science and engineering. it focuses on hardware platforms and only touches on other supercomputing technologies, notably algorithms and software. a more detailed discussion of current supercomputing technologies is provided inchapter 5.the prehistory of u.s. supercomputingthe development of computer technology in the united states wasinextricably linked to u.s. government funding for research on cryptanalysis, nuclear weapons, and other defense applications in its first several decades.2 arguably, the first working, modern, electronic, digitalcomputer was the colossus machine, put into operation at bletchley park,1an expanded version of much of the analysis in this chapter will be found in òan economic history of the supercomputer industry,ó by kenneth flamm, 2004.2in chapter 3, òmilitary roots,ó of creating the computer: government, industry, and hightechnology (brookings institution press, 1988), kenneth flamm lays out the entire panoramaof governmentfunded projects in the late 1940s and 1950s that essentially created the earlyu.s. computer industry. another good but less comprehensive source ends in the very early1950s, when highvolume production was 20 machines: n. metropolis, j. howlett, and giancarlo rota, a history of computing in the twentieth century (academic press, 1980).getting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.brief history of supercomputing29in the united kingdom, in 1943. although it was designed and employedto break a specific german cipher system, this machine was in fact a trueelectronic computer and could be used, in principle, on a range of problems. the existence of this machine was classified until the 1970s.u.s. personnel working with bletchley park during world war iiplayed a major role in creating the early u.s. computer industry in thedecade following the war. in particular, u.s. engineers at the naval computing machinery laboratory (a national cash register plant in dayton,ohio, deputized into the war effort) were building copies or improvedversions of bletchley park electronic cryptanalysis machines, as well ascomputers of their own design. american engineers involved in this effort included william norris and howard engstromñnorris laterfounded engineering research associates (era), then control data;engstrom was later deputy director of the national security agency(nsa)ñand ralph palmer who was principal technical architect of ibmõsmove into electronic computers in the 1950s. of the 55 people in the founding technical group at era, where seymour cray had his first design jobin computers, 40 came from navy communications intelligence in washington, 5 from the navy lab in dayton, and 3 from the naval ordnancelaboratory.3the eniac, built in 1945 at the university of pennsylvania and oftencredited as the first functioning electronic computer, was a larger, plugprogrammable computer designed to compute artillery ballistics tables.4ironically, it came into existence, indirectly, as a result of the codebreaking efforts of the u.s. intelligence community. the u.s. armyõs ballisticresearch laboratory (brl) had originally funded a ballistics computerproject at national cash register and had turned down a competing proposal from j. presper eckert and john mauchly at the university of pennsylvania. brl reconsidered this decision after the national cash registerdayton group was drafted into producing cryptanalysis machines for thenavy and finally decided to fund the eniac project.3see flamm, 1988, pp. 3641, 4345.4as is the case for many other technologies, there has been a heated debate about whoshould be credited as the inventor of the first digital computer. in addition to the colossusand the eniac, the following are worth mentioning: konrad zuse, working in germany,built a relaybased automatic digital computer in germany in 19391941. a similar system,the automatic sequence controlled calculator (ascc), also called the mark i, was conceived by howard aiken and designed and built by ibm in 19391944. john vincentatanasoff and clifford berry started building an electronic digital computer at iowa stateuniversity in 19371942. although the project was not completed, atanasoff and berry wona patent case against eckert and mauchly in 1973, invalidating the patent of the latter oneniac as the first automatic electronic computer.getting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.30getting up to speedprinceton mathematician and war department consultant john vonneumann heard about the existence of the eniac project at the brl andinvolved himself in the project.5 it is reported that some of the early atomicbomb calculations (in which von neumann was involved) made use ofthe eniac even before it was formally delivered to the army. the linkbetween both cryptanalytical and nuclear design applications and highperformance computing goes back to the very first computers.eniacõs designers, eckert and mauchly, built the first working storedprogram electronic computer in the united states in 1949 (the binac)and delivered it to northrop aircraft, a defense contractor. a number ofadvanced machines had been built in britain by that timeñbritain wasactually leading in the construction of working electronic computers inthe late 1940s. a massive u.s. government investment in computer technology in the 1950s was critical to the rapid rise of u.s. companies as theundisputed leaders in the field.the second and third computers in the united states were the seac(built for the national bureau of standards, now renamed nist) and theera 1101 (built for predecessors to the national security agency). bothwent into operation in 1950, runnersup in the united states to the eckertmauchly binac.the first eckert and mauchlydesigned computer targeting a commercial market, the univac, was delivered to the census bureau in 1951.the experimental mit whirlwind computer, built with navy and laterair force funding, also went into operation in 1951.von neumann, who had brought british computing theoretician alanturing to princeton in the 1930s and was much influenced by this contact,began work on the conceptual design of a generalpurpose scientific computer for use in calculations of military interest in 1946, but a workingmachine was not completed until 1951. this machine was intended to be atool for scientists and engineers doing numerical calculations of the sortneeded in nuclear weapons design. versions of the first machine installedat the institute of advanced studies in princeton, the ias machine, werebuilt and installed at los alamos (the maniac i) in 1952 and oak ridge(the oracle) in 1953; these were the first computers installed at thenuclear weapons laboratories.6 the nuclear weapons labssponsored iasdesign was highly influential. but the laboratories were so pressed forcomputing resources before these machines were delivered that they did5nancy stern. 1981. from eniac to univac: an appraisal of the eckertmauchly computers.digital press.6the argonne national laboratory built avidac (argonneõs version of the instituteõsdigital automatic computer), which was operational prior to ias.getting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.brief history of supercomputing31their calculations on the seac at the national bureau of standards andran thermonuclear calculations on the floor of the univac factory inphiladelphia.volume computer production did not begin until 1953. in that year,the firstera 1103 was delivered to the cryptanalysts in the intelligencecommunity, as was the first ibm 701 defense calculator. twenty era1103s and 19 ibm 701s were built; all were delivered to dod customers.nsa was the primary sponsor of highperformance computingthrough most of the post1103 1950s era. it sponsored the philco 210 andthe philco 211 and cosponsored the ibm 7030 stretch as part of its supportfor the harvest system. dod supported the development of the ibm 7090for use in a ballistic missile early warning system.energy labsponsored computers did not play a leading role at thefrontiers of highperformance computing until the late 1950s. the atomicenergy commission (aec) set up a formal computer research program in1956 and contracted with ibm for the stretch system and with sperry rand(which acquired both the eckertmauchly computer group and era inthe 1950s) for the livermore advanced research computer (larc). thecosponsorship of the stretch system by nsa and aec required ibm tomeet the needs of two different customers (and applications) in one system. it was said that balancing those demands was an important factor inthe success of ibmõs system 360.supercomputers emerge as a marketwith the emergence of specific models of computers built in commercial volumes (in that era, the double digits) in the 1950s, and the dawningrealization that computers were applicable to a potentially huge range ofscientific and business data processing tasks, smaller and cheaper computers began to be produced in significant numbers. in the early 1950s,machines produced in volume were typically separated by less than anorder of magnitude in speed. by the late 1950s, the fastest, most expensivecomputers were three to four orders of magnitude more powerful thanthe smallest models sold in large numbers. by the early 1970s, that rangehad widened even further, with a spread now exceeding four orders ofmagnitude in performance between highest performance machines andsmall business or scientific computers selling in volume (see figure 3.1).in the late 1950s, the u.s. government, motivated primarily by national security needs to support intelligence and nuclear weapons applications, institutionalized its dominant role in funding the development ofcuttingedge highperformance computing technology for these two setsof military applications. arguably, the first supercomputers explicitly intended as such, designed to push an order of magnitude beyond the fastgetting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.32getting up to speedfigure 3.1early computer performance. included in this figure are the bestperforming machines according to value of installations, number of installations,and millions of operations per second (mops). source: kenneth flamm. 1988.creating the computer: government, industry, and high technology. washington,d.c.: brookings institution press.est available commercial machines, were the ibm 7030 stretch and sperryrand univac larc, delivered in the early 1960s.7these two machines established a pattern often observed in subsequent decades: the governmentfunded supercomputers were producedin very limited numbers and delivered primarily to government users.but the technology pioneered in these systems would find its way into theindustrial mainstream a generation or two later in commercial systems.for example, one typical evaluation holds that òwhile the ibm 7030 wasnot considered successful, it spawned many technologies incorporated infuture machines that were highly successful. the transistor logic was thebasis for the ibm 7090 line of scientific computers, then the 7040 and 1400lines. multiprogramming, memory protection, generalized interrupts, the7the term òsupercomputeró seems to have come into use in the 1960s, when the ibm 7030stretch and control data 6600 were delivered.getting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.brief history of supercomputing338bit byte were all concepts later incorporated in the ibm 360 line of computers as well as almost all thirdgeneration processors and beyond. instruction pipelining, prefetch and decoding, and memory interleavingwere used in later supercomputer designs such as the ibm 360 models 91,95, and 195, as well as in computers from other manufacturers. these techniques are now used in most advanced microprocessors, such as the intelpentium and the motorola/ibm powerpc.ó8 similarly, larc technologies were used in sperry randõs univac iii.9yet another feature of the supercomputer marketplace also becameestablished over this period: a high mortality rate for the companies involved. ibm exited the supercomputer market in the mid1970s. sperryrand exited the supercomputer market a few years after many of itssupercomputer designers left to found the new powerhouse that came todominate u.s. supercomputers in the 1960sñthe control data corporation (cdc).control data and crayfrom the mid1960s to the late 1970s, the global u.s. supercomputerindustry was dominated by two u.s. companies: cdc and its offspring,cray research. both companies traced their roots back to era, which hadbeen absorbed by sperry rand in 1952. a substantial portion of this talentpool (including seymour cray) left to form a new company, cdc, in 1957.cdc was to become the dominant manufacturer of supercomputers fromthe mid1960s through the mid1970s. government users, particularly theintelligence community, funded development of cdcõs first commercialoffering, the cdc 1604. in 1966 cdc shipped its first fullscalesupercomputer, the cdc 6600, a huge success. in addition to offering anorder of magnitude jump in absolute computational capability (see figure 3.1), it did so very cost effectively. as suggested by figure 3.2, computing power was delivered by the 6600 at a price comparable to or lowerthan that of the best cost/performance in mainstream commercial machines.108historical information on the ibm 7030 is available online from the wikipedia at<http://en.wikipedia.org/wiki/ibm7030>.9see <http://en.wikipedia.org/wiki/ibm7030>; g. gray, òthe univac iii computer,óunisys history newsletter 2(1) (revised 1999), <http://www.cc.gatech.edu/gvu/people/randy.carpenter/folklore/v2n1.html>.10the benchmarks, the performance metrics, and the cost metrics used for that figure areconsiderably different from those used today, but the qualitative comparison is generallyaccepted.getting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.34getting up to speed0.11011001000100001000001e+061e+071e+081e+091e+101e+111e+121e+131940195019601970198019902000eniacunivac 1cdc 6600mits altair 8800dec pdp 11/45dec pdp 11/20apple macintoshosborne 1ibm pc/xt w/ndpdell386/16sun ss1dell 433pdell p60dell p3 600dec pdp8cray 1apple iiintel ascii redmcr linux clusterascii qnec esfigure 3.2cost/performance over time. based on data collected by johnmccallum at <http://www.jcmit.com/cpuperformance.htm>. nec earth simulator cost corrected from $350 million to $500 million. note that ònormalizedómips (millions of instructions per second) is constructed by combining a varietyof benchmarks run on these machines over this 50year period, using scores onmultiple benchmarks run on a single machine to do the normalization.at this point, there was no such thing as a commodity processor. allcomputer processors were custom produced. the high computational performance of the cdc 6600 at a relatively low cost was a testament to thegenius of its design team. additionally, the software tools that were provided by cdc made it possible to efficiently deliver this performance tothe end user.although the 6600 gave cdc economic success at the time, simplydelivering theoretical computational power at a substantially lower priceper computation was not sufficient for cdc to dominate the market. then,as now, the availability of applications software, the availability of specialized peripherals and storage devices tailored for specific applications,and the availability of tools to assist in programming new software werejust as important to many customers.the needs of the government users were different. because the specific applications and codes they ran for defense applications were oftensecret, frequently were tied to specialpurpose custom hardware and peripherals built in small numbers, and changed quickly over time, the availgetting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.brief history of supercomputing35ability of lowcost, commercially available peripherals and software wereoften unimportant. the defense agencies typically invested in creating thesoftware and computing infrastructure they needed (for example,nastran11 and dyna12). when some of that software became availableto commercial customers after it had been made available to the first government customers, these supercomputers became much more attractiveto them.in 1972, computer designer seymour cray left cdc and formed anew company, cray research. although cdc continued to produce highperformance computers through the remainder of the 1970s (e.g.,star100), cray quickly became the dominant player in the highest performance u.s. supercomputer arena.13 the cray1, first shipped to losalamos national laboratory in 1976, set the standard for contemporarysupercomputer design. the cray1 supported a vector architecture inwhich vectors of floatingpoint numbers could be loaded from memoryinto vector registers and processed in the arithmetic unit in a pipelinedmanner at much higher speeds than were possible for scalar operands.14vector processing became the cornerstone of supercomputing. like thecdc 6600, the cray1 delivered massive amounts of computing power ata price competitive with the most economical computing systems of theday. figure 3.2 shows that the cost of sustained computing power on thecray1 was roughly comparable to that of the cost/performance champion of the day, the apple ii microcomputer.during this period, ibm retreated from the supercomputer market,instead focusing on its fastgrowing and highly profitable commercialcomputer systems businesses. apart from a number of larger companiesflirting with entry into the supercomputer business by building experimental machines (but never really succeeding) and several smaller com11nastran (nasa structural analysis) was originally developed at goddard spaceflight center and released in 1971 (see <http://www.sti.nasa.gov/tto/spinoff2002/goddard.html>). there are now several commercial implementations.12dyna3d was originally developed in the 1970s at the lawrence livermore nationallaboratory to simulate underground nuclear tests and determine the vulnerability of underground bunkers to strikes by nuclear missiles. its successor, lsdyna, which simulatesvehicle crashes, is commercially available.13cdc ultimately exited the supercomputer business in the 1980s, first spinning off itssupercomputer operations in a new subsidiary, eta, and then shutting down eta a fewyears later, in 1989.14vector processing first appeared in the cdc star100 and the texas instruments asc,both announced in 1972. much of the vector processing technology, including vectorizingcompilers, originated from the illiac iv project, developed at illinois.getting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.36getting up to speedpanies that successfully pioneered a lowerend, costoriented òminisupercomputeró market niche, u.s. producers cdc and cray dominatedthe global supercomputer industry in the 1970s and much of the 1980s.although it was not widely known or documented at the time, inaddition to using the systems from cdc and cray, the defense community built specialpurpose, highperformance computers. most of thesecomputers were used for processing radar and acoustic signals and images. these computers were often òmilspecõedó (designed to function inhostile environments). in general, these systems performed arithmeticoperations on 16 and 32bit data. fast fourier transforms and digital filters were among the most commonly used algorithms. many of the commercial array processor companies that emerged in the late 1970s werespinoffs of these efforts.the commercial array processors, coupled with minicomputers fromdigital equipment corporation and data general, were often used assupercomputers. the resulting hybrid system combined a commodityhost with a custom component. unlike most other supercomputers of theperiod, these systems were aircooled.the 1970s also witnessed the shipment of the first simple, singlechipcomputer processor (or microprocessor) by the intel corporation, in november 1971. by the early 1980s, this technology had matured to the pointwhere it was possible to build simple (albeit relatively lowperformance)computers capable of òseriousó computing tasks. the use of lowcost,massproduced, highvolume commodity microprocessors was to transform all segments of the computer industry. the highest performance segment of the industry, the supercomputer, was the last to be transformedby this development.enter japanby the mid1980s, with assistance from a substantial governmentsubsidized r&d program launched in the 1970s and from a history oftrade and industrial policy that effectively excluded foreign competitorsfrom japanese markets, japanese semiconductor producers had pushedto the technological frontier in semiconductor manufacturing. historically, the rationale for japanese government support in semiconductorshad been to serve as a steppingstone for creating a globally competitivecomputer industry, since the semiconductor divisions of the large japanese electronics companies had also produced computers sold in a protected japanese market. aided by their new capabilities in semiconductors and a successful campaign to acquire key bits of ibmõs mainframetechnology, by the mid1980s japanese computer companies were shipgetting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.brief history of supercomputing37ping costeffective commercial computer systems that were competitivewith, and often compatible with, ibmõs mainframes.15thus it was that the united states viewed with some concern japanõsannouncement of two governmentfunded computer r&d programs inthe early 1980s explicitly intended to put japanese computer producers atthe cutting edge in computer technology. one was the fifth generationcomputer system project, which was primarily focused on artificial intelligence and logic programming. the other was the high speed computing system for scientific and technological uses project, also called thesuperspeed project, which focused on supercomputing technology.16 atroughly the same time, the three large japanese electronics companiesmanufacturing mainframe computers began to sell supercomputers athome and abroad. the japanese vendors provided good vectorizing compilers with their vector supercomputers. although the fifth generationproject ultimately would pose little threat to u.s. computer companies, itstimulated a substantial government effort in the united states to accelerate the pace of highperformance computing innovation. in the 1980s thiseffort, led by darpa, funded the large strategic computing initiative(sci), which transformed the face of the u.s. supercomputer industry.the prospect of serious competition from japanese computer companies in mainstream markets also led to a series of trade policy responsesby u.s. companies and their supporters in the u.s. government (see thediscussion of trade policies in chapter 8, box 8.1). by the 1980s, fujitsu,hitachi, and nec were all shipping highly capable supercomputers competitive with crayõs products, dominating the japanese market and beginning to make inroads into european and american markets. the vastmajority of japanese supercomputers were sold outside the united states.there were some minimal sales to the united states in areas such as thepetroleum industry but few sales to u.s. government organizations. significant obstacles faced the sales of u.s.made supercomputers in japan aswell. responding to these market limitations in the 1980s, u.s. trade negotiators signed agreements with the japanese government designed toopen up government procurement in japan to u.s. supercomputer producers. (in japan, as in the united states, the government dominated themarket for supercomputers.) in the mid1990s, the u.s. government alsosupported u.s. supercomputer makers in bringing an antidumping case15a good reference for the survey of supercomputer development in japan is y. oyanagi,1999, òdevelopment of supercomputers in japan: hardware and software,ó parallel computing 25:15451567.16d.k. kahaner. 1992. òhigh performance computing in japan: supercomputing.ó asiantechnology information program. june.getting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.38getting up to speedagainst japanese supercomputer makers in the u.s. market. that case ultimately forced japanese companies out of the u.s. market until 2003,when a suspension agreement was signed.innovation in supercomputingwhile one part of the u.s. government reacted by building wallsaround the u.s. market, darpa and its strategic computing initiative(sci), in concert with other government agencies and programs, took theopposite tack, attempting to stimulate a burst of innovation that wouldqualitatively alter the industry.17 computing technology was regarded asthe cornerstone of qualitative superiority for u.s. weapons systems. it wasargued that the united states could not regain a significant qualitativelead in computing technology merely by introducing faster or cheapercomputer components, since japanese producers had clearly achievedtechnological parity, if not some element of superiority, in manufacturingthem. furthermore, many technologists believed that continued advancesin computer capability based on merely increasing the clock rates of traditional computer processor designs were doomed to slow down as inherent physical limits to the size of semiconductor electronic componentswere approached. in addition, amdahlõs law was expected to restrict increases in performance due to an increase in the number of processorsused in parallel.18the approach to stimulating innovation was to fund an intense effortto do what had not previously been doneñto create a viable new architecture for massively parallel computers, some of them built around commodity processors, and to demonstrate that important applications couldbenefit from massive parallelism. even if the individual processors wereless efficient in delivering usable computing power, as long as the parallelarchitecture was sufficiently scalable, interconnecting a sufficient number17investments in highperformance computing were only one area funded by the sci,which funded over $1 billion in r&d from 1983 to 1993. there are no available data thatbreak out this investment by technology area. other areas were electronic components, artificial intelligence and expert systems, and largescale prototype development of advancedmilitary systems intended to explore new technology concepts. the committee is not awareof any objective assessment of the success and utility of the program as a whole. an excellenthistory of the program may be found in alex roland and phillip shiman, 2002, strategiccomputing: darpa and the quest for machine intelligence, 19831993, cambridge, mass.: mitpress.18amdahlõs law states that if a fraction of 1/s of an execution is sequential, then parallelism can reduce execution time by at most a factor of s. conventional wisdom in the early1980s was that for many applications of interest amdahlõs law will restrict gains in performance from parallelism to factors of tens or low hundreds.getting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.brief history of supercomputing39of processors might potentially provide a great deal of computing capability. once the hardware architectural details of how to scale up thesesystems were determined, very large parallel machines could be put towork, and supercomputers that were orders of magnitude faster wouldgive the government agencies charged with national security new qualitative technological advantages. it was assumed that appropriate softwaretechnology would follow.this was the dream that motivated the architects of the u.s.governmentõs supercomputer technology investments in the late 1980s.dozens of new industrial flowers bloomed in darpaõs strategic computing hothouse from the mid1980s through the early 1990s. old playersand new ones received substantial support for experiments with new,parallel architectures.19it has become commonplace to point to the high mortality rate amongu.s. supercomputer manufacturers in the 1990s and the large amount ofresources invested by the u.s. government in now defunct massively parallel supercomputer makers. many critics believe that the darpa program was a failure that harmed the market.20 most of these startup companies went bankrupt or were sold.over this period, however, some important lessons were learned. onewas the importance of node performance; another was the importance ofhighbandwidth, lowlatency, scalable interconnects. the evolution of thethinking machines products from the cm1 (with bit serial processorsand a relatively lowperforming, singlestage bitserial network) to thecm5 (with a powerful sparc node enhanced with a vector unit and apowerful, scalable multistage network) is a typical example. over time,19gordon bellõs list of experiments includes att/columbia (non von), bbn labs, belllabs/columbia (dado), cmu (production systems), cmu warp (ge and honeywell),encore, esl, ge (like connection machine), georgia tech, hughes (dataflow), ibm (rp3),mit/harris, mit/motorola (dataflow), mit lincoln labs, princeton (mmmp),schlumberger (faim1), sdc/burroughs, sri (eazyflow), university of texas, thinkingmachines (connection machine). see gordon bell, òpact 98,ó a slide presentation availableat <http://www.research.microsoft.com/barc/gbell/pact.ppt>.20a list of failed industrial ventures in this area, many inspired by sci, includes alliant,american supercomputer, ametek, amt, astronautics, bbn supercomputer, biin, cdc/eta systems, chen systems, columbia homogeneous parallel processor, cogent, craycomputer, culler, cydrome, denelcor, elxsi, encore, e&s supercomputers, flexible,goodyear, gould/sel, intel supercomputer division, ipm, ipsystems, kendall square research, key, multiflow, myrias, pixar, prevec, prisma, saxpy, scs, sdsa, stardent (stellarand ardent), supercomputer systems inc., suprenum, synapse, thinking machines, trilogy, vitec, vitesse, wavetracer (e. strohmaier, j.j. dongarra, and h.w. meuer, 1999, òmarketplace of highperformance computing,ó parallel computing 25(13):15171544.getting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.40getting up to speeddarpa shifted its emphasis from hardware alone to complementary investments in software that would make the newly developed parallelhardware easier to program and use in important applications. these investments included modest support for the port of industrial codes to thenew scalable architectures.in the commercial supercomputing arena, there continued to be vector architectures as well as the increasing presence of scalable systemsbased on commodity processors. there were many common attributesamong the supercomputers of this period. among them were these:¥device technology shifted to complementary metal oxide semiconductor (cmos), both for commoditybased systems and for custom systems. as a result, custom systems lost the advantage of faster technology.¥the increase in clock and memory speeds coincided with mooreõslaw.¥the reduction of the size of the processor resulted in smallscalemultiprocessor systems (two to four processors) being used as nodes inscalable systems; larger sharedmemory configurations appeared as highend technical servers.¥vendors began supplying vectorizing and (in some cases)parallelizing compilers, programming tools, and operating systems(mostly unixbased), which made it easier to program.the common architectural featuresñvector processing, parallelshared memory, and, later, message passingñalso encouraged third parties to develop software for this class of computers. in particular, standard numerical libraries such as the blas21 evolved to supply commonhighlevel operations, and important scientific and engineering applications such as nastran appeared in vectorized and parallelized versions.the development of this software base benefited all supercomputer manufacturers by expanding the total market for machines. similarly, the availability of common software and a shared programming model benefitedthe entire user community, both government and commercial.by accident or by design, the course correction effected by sci hadsome important and favorable economic implications for the u.s.supercomputer industry. suppose that technology were available to per21c.l. lawson, r.j. hanson, d.r. kincaid, and f.t. krogh, 1979, òbasic linear algebrasubprograms for fortran usage,ó acm transactions on mathematical software 5:308325; j.j.dongarra, j. du croz, s. hammarling, and r.j. hanson, 1988, òan extended set of fortranbasic linear algebra subprograms,ó acm transactions on mathematical software 14(1):1ð17;j.j. dongarra, j. du croz, s. hammarling, and i.s. duff, òa set of level 3 basic linear algebra subprograms,ó acm transactions on mathematical software 16(1):117.getting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.brief history of supercomputing41mit large numbers of inexpensive, highvolume commodity microprocessors to divide up the work of a given computing task. then the continuingsteep declines in the cost of commodity processors would eventually makesuch a system a more economic solution for supplying computing capability than a system designed around much smaller numbers of very expensive custom processors that were falling in cost much less rapidly. if aricher and more portable software base became available for these systems, the cost of their adoption would be reduced. if so, the difference inprice trends between custom and commodity processors would eventually make a parallel supercomputer built using commodity components avastly more economically attractive proposition than the traditional approach using custom processors.in the late 1980s and early 1990s, darpa shifted more of its supercomputer investments into systems based on commercially available processors, at thinking machines (what was to become the cm5, usingsparc processors), at intel (what was to become its paragon supercomputer line, using intelõs ipsc processor), and at cray (its t3d system, using decõs alpha processor).22the net impact of this shift benefited thedevelopment and sales of commoditybased systems. this outcome wasparticularly important given the increasing and highly competent japanese competition in the market for traditional vector supercomputers.rather than backing an effort to stay ahead of the competition in an established market in which competitors had seized the momentum, researchand experiencerich u.s. companies threw the entire competition onto awhole new battlefield, where they had a substantial advantage over theircompetitors.some of these hardware and software characteristics also found theirway into a new generation of supercomputers, called òminisupercomputersó (e.g., convex, alliant, multiflow). unlike the products from crayand cdc, the minisupercomputers were aircooled, had virtual memoryoperating systems, and sold for under $1 million. the minisupercomputer systems included unix operating systems and automaticvectorizing/parallelizing compilers. this new generation of software systems was based on prior academic research. with unix came a wealth ofdevelopment tools and software components (editors, file systems, etc).the systems also made extensive use of open standards used for i/o bus22the myrinet commodity interconnects used in a number of commodity supercomputersystems were also developed with darpa support at about this time (alex roland andphilip shiman, 2002. strategic computing: darpa and the quest for machine intelligence, 19831993, cambridge, mass.: mit press. pp. 308317; darpa, technology transition, 1997, pp.42, 45.getting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.42getting up to speedses, peripheral devices, and networking (e.g., tcp/ip). this standardization made it easier for users and independent software vendors (isvs) tomove from one platform to another. additionally, the client/server modelevolved through the use of ethernet and tcp/ip. the nsffunded supercomputer centers helped promote the adoption of unix for supercomputers; the cray systems at those centers were required to run unix. also,darpa required the use of unix as a standard operating system formany of the supercomputing projects it funded. ultimately every supercomputer platform supported unix. that in turn increased the use of theprogramming language c, which became widely used to write numerically intense applications. the newer generation of compilers enabledapplications written in standard fortran and c to be optimized and tunedto the contemporary supercomputers. this led to the widespread conversion of isvdeveloped software and consequently the widespread adoption of supercomputing by the commercial (nongovernmentsponsored)marketplace.mooreõs law continued to hold, and to a large degree it changed theface of supercomputing. the systems built in the 1980s were all built fromcmos or from ecl gate arrays. as the density of cmos increased, itbecame possible to put an entire processor on one die, creating a microprocessor. this led to the attack of òkiller micros.ó23 the killer micro permitted multiple microprocessors to be coupled together and run in parallel. for applications that could be parallelized (both algorithmically andby localizing data to a particular processor/memory system), a coupledsystem of killer micros could outperform a customdesigned supercomputer. just as important, the singleprocessor scalar performance of a killermicro often exceeded the singleprocessor scalar performance of asupercomputer. this next generation of supercomputer resulted in achange of architectures. highperformance vector systems began to bereplaced by parallel processing, often massiveñhundreds and thousandsof microprocessors.thus, although it is true that there was an extraordinarily high mortality rate among the companies that developed parallel computer architectures in the 1980s and early 1990s, much was learned from the technical failures as well as the successes. important architectural andconceptual problems were confronted, parallel systems were made towork at a much larger scale than in the past, and the lessons learned were23the term òkiller microó was popularized by eugene brooks in his presentation to theteraflop computing panel, òattack of the killer micros,ó at supercomputing 1989 in reno,nev. (see also <http://jargon.watsonnet.com/jargon.asp?w=killer%20micro>).getting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.brief history of supercomputing43absorbed by other u.s. companies, which typically hired key technicalstaff from defunct parallel supercomputer pioneers. subsequently, therewere five major new u.s. entrants into the highperformance computing(hpc) market in the 1990sñibm, sgi, sun, dec/compaq (recentlymerged into hewlettpackard), and convex/hpñwhich today have survived with the lionõs share (as measured in numbers of systems) of thehpc marketplace.though dreams of effortless parallelism seem as distant as ever, thefact is that the supercomputer marketplace today is dominated by a newclass of useful, commodityprocessorbased parallel systems thatñwhilenot necessarily the most powerful highperformance systems availableñare the most widely used. the commercial center of gravity of the supercomputer market is today dominated by u.s. companies marketing commodityprocessor parallel systems that capitalize on technologyinvestments made by the u.s. government in largescale parallel hardware (and to a lesser extent, software) technology in the 1980s and 1990s.recent developments in supercomputingto some extent, the reasons for the dominance of commodityprocessor systems are economic, as illustrated by the hardware costs shown infigure 3.2. contemporary distributedmemory supercomputer systemsbased on commodity processors (like linux clusters) appear to be substantially more cost effectiveñby roughly an order of magnitudeñin delivering computing power to applications that do not have stringent communication requirements. however, there has been little progress, andperhaps even some regress, in making scalable systems easy to program.software directions that were started in the early 1980s (such as cmfortran and highperformance fortran) were largely abandoned. the payoffto finding better ways to program such systems and thus expand the domains in which these systems can be applied would appear to be large.the move to distributed memory has forced changes in the programming paradigm of supercomputing. the high cost of processortoprocessor synchronization and communication requires new algorithms thatminimize those operations. the structuring of an application forvectorization is seldom the best structure for parallelization on these systems. moreover, despite some research successes in this area, withoutsome guidance from the programmer, compilers are not generally able todetect enough of the necessary parallelism or to reduce sufficiently theinterprocessor overheads. the use of distributed memory systems has ledto the introduction of new programming models, particularly the message passing paradigm, as realized in mpi, and the use of parallel loops inshared memory subsystems, as supported by openmp. it also has forcedgetting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.44getting up to speedsignificant reprogramming of libraries and applications to port onto thenew architectures. debuggers and performance tools for scalable systemshave developed slowly, however, and even today most users consider theprogramming tools on parallel supercomputers to be inadequate.the u.s. highperformancecomputing industry todaytoday, the current tension in the industry is between a large numberof applications that make acceptable use of relatively inexpensive supercomputers incorporating large numbers of lowcost commodity processors and a small number of highly important applications (predominantlythe domain of government customers) in which the required performancecan currently be provided only by highly tuned systems making use ofexpensive custom components. this tension is at the root of many of thepolicy issues addressed by this report.despite the apparent economic weakness of the sole remaining u.s.vectorbased supercomputer maker, cray (which makes supercomputersbased on custom processors), available data on the supercomputer marketplace (based on the top500 list of june 2004) show it is dominated byu.s. companies today. international data corporation (idc) numberspaint a similar picture: in 2000, u.s. vendors had 93 percent of the highperformance technical computing market (defined to include all technicalservers) and 70 percent of the capability market (defined as systems purchased to solve the largest, most performancedemanding problems). in2002 the numbers were 95 percent and 81 percent and in 2003 they were98 percent and 88 percent, respectively, showing a continued strengthening of u.s. vendors. ninetyfour percent of technical computing systemsselling for more than $1 million in 2003 were u.s. made.24 it may be alegitimate matter of concern to u.s. policymakers that the fastest computer in the world was designed in japan and has been located there forthe last 2 years. but it would be inaccurate to assert that the u.s. supercomputer industry is in trouble. indeed, the competitive position of u.s.supercomputer producers is as strong as it has been in decades, and allsigns point to continued improvement.to characterize the current dynamics of the u.s. industry, the committee turned to a detailed analysis of the top500 data (using the june2004 list), which are available for the period 19932003.25 while the top50024source: earl joseph, program vice president, highperformance systems, idc; email exchanges, phone conversations, and inperson briefings from december 2003 tooctober 2004.25for details and the data used in the analysis that follows, see <http://www.top500.org>.getting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.brief history of supercomputing45lists are the best publicly available source of information on supercomputing trends, it is important to keep in mind the limitations of this sourceof information.the rmax linpack metric used by the top500 ranking does not correlate well with performance on many reallife workloads; this issue is further discussed in the section on metrics in chapter 5. while no one number can characterize the performance of a system for diverse workloads, itis likely that the rmax metric exaggerates by at least a factor of 2 the realperformance of commodity platforms relative to custom platforms. similarly, custom highperformance systems are significantly more expensivethan commodity systems relative to their performance as measured byrmax. thus, if rmax is used as a proxy for market share, then the top500list greatly exaggerates the dollar value of the market share of commoditysystems. the top500 data merit analyzing because the changes and theevolution trends identified in the analysis are real. however, one shouldnot attach too much significance to the absolute numbers.some large deployed systems are not reported in the top500 list. insome cases, organizations may not want to have their computer powerknown, either for security or competitiveness reasons. thus, companiesthat sell mainly to classified organizations may see their sales underreported in the top500 lists. in other cases, organizations may not seevalue in a top500 listing or may consider that running a benchmark istoo burdensome. this is especially true for companies that assemble clusters on their own and need to provide continuous availability. althoughweb search or web caching companies own the largest clusters, they donot usually appear in the top500 lists. many large clusters used by service companies and some large clusters deployed in academia are alsomissing from the top500 list, even though they could be there. it is reasonable to assume that this biases the top500 listing toward underreporting of commercial systems and overreporting of research systems,supporting the argument that use of highperformance computing platforms in industry does not seem to be declining. while custom systemswill be underreported because of their heavier use in classified applications, clusters will be underreported because of their use in large servicedeployments. it is not clear whether these two biases cancel each otherout.top500 provides information on the platform but not on its usage.the size of deployed platforms may not be indicative of the size of parallel applications run on these platforms. industry often uses clusters ascapacity systems; large clusters are purchased to consolidate resources inone place, reducing administration costs and providing better securityand control. on the other hand, computing tends to be less centralized inacademia, and a cluster often serves a small number of top users. thus, agetting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.46getting up to speedgood penetration of top500 platforms in industry does not necessarilyindicate that applications in industry have scaled up in proportion to thescaling of top500 platforms over the years; the size of academic platforms is a better indicator of the scale of applications running on them.keeping those caveats in mind, many things can be learned fromstudying the top500 data.there has been continuing rapid improvement in the capability ofhighperformance systems over the last decade (see figure 3.3).26 meanlinpack performance has improved fairly steadily, by roughly an order ofmagnitude every 4 years (about 80 percent improvement annually). theperformance of the very fastest machines (as measured by the rmax27ofthe machine) has shown much greater unevenness over this period but onaverage seems roughly comparable. interestingly, the performance of theleast capable machines on the list has been improving more rapidly than0.1110100100010000100000jun 93dec 93jun 94dec 94jun 95dec 95jun 96dec 96jun 97dec 97jun 98dec 98jun 99dec 99jun 00dec 00jun 01dec 01jun 02dec 02jun 03dec 03jun 04asci redearth simulatorasci whitermaxmax mean min rmaxfigure 3.3top500 linpack performance.26asci white and asci red are two supercomputers installed at doe sites as part of theasc strategy. information on all of the asc supercomputers is available at <http://www.llnl.gov/asci/platforms/platforms.html>.27the rmax is the maximal performance achieved on the linpack benchmarkñfor any sizesystem of linear equations.getting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.brief history of supercomputing47mean performance, and the ratio between the least capable and the listmean is substantially smaller now than it was back in 1993. this reflectsthe fact that performance improvement in lowcost commodity microprocessors (used in lowerend top500 systems) in recent years has exceededthe already impressive rates of performance improvement in custom processors used in the higherend systems; it also reflects the fact that in recent years, the average size of commonly available clusters has increasedmore rapidly than the size of the most powerful supercomputers.there is no evidence of a longterm trend to widening performancegaps between the least and most capable systems on the top500 list (seefigure 3.4). one measure of this gap is the relative standard deviation ofrmax of machines on this list, normalized by dividing by mean rmax in anygiven year. there was a significant jump in this gap in early 2002, whenthe earth simulator went operational, but it has since diminished to priorlevels as other somewhat less fast machines made the list and as the leastcapable machines improved faster than the mean capability. essentiallythe same story is told if one simply measures the ratio between greatestperformance and the mean.0102030405060708090jun 93dec 93jun 94dec 94jun 95dec 95jun 96dec 96jun 97dec 97jun 98dec 98jun 99dec 99jun 00dec 00jun 01dec 01jun 02dec 02jun 03dec 03jun 04rmax/mean00.511.522.533.54standard deviation/meanasci redasci whiteearth simulatorrmax/meansd/meanfigure 3.4rmax dispersion in top500.getting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.48getting up to speedincreasingly, a larger share of highend systems is being used by industry and a smaller share by academia. there has been a rapid increasein the share of top500 machines installed in industrial locations (see figure 3.5). in the last several years, roughly 40 to 50 percent of the top500systems (number of machines) have been installed in industry, as compared with about 30 percent in 1993. this contrasts with the situation inacademia, which had a substantially smaller share of top500 systems inthe late 1990s than in the early 1990s. there has been some increase inacademic share in the last several years, accounted for mainly by linuxclustertype systems, often selfbuilt. it is tempting to speculate that theproliferation of relatively inexpensive, commodityprocessorbased hpcsystems is driving this development. there is one qualification to this picture of a thriving industrial market for highend systems, however: thegrowing qualitative gap between the scale and types of systems used byindustry and by cuttingedge government users, with industry using lessand less of the most highly capable systems than it used to. there havebeen no industrial users in the top 20 systems for the last 3 years, contrast0%10%20%30%40%50%60%70%80%90%100%jun 93dec 93jun 94dec 94jun 95dec 95jun 96dec 96jun 97dec 97jun 98dec 98jun99dec 99jun 00dec 00jun 01dec 01jun 02dec 02jun 03dec 03jun 04vendoracademicres&govtindustrialfigure 3.5top500 by installation type.getting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.brief history of supercomputing49ing with at least one industrial user in the top 20 at least once in each ofthe previous 9 years (see figure 3.6).u.s. supercomputer makers are performing strongly in globalsupercomputer markets. their global market share has steadily increased,from less than 80 percent to more than 90 percent of top500 units sold(see figure 3.7). measuring market share by share of total computing capability sold (total rmax) is probably a better proxy for revenues and presents a more irregular picture, but it also suggests a significant increase inmarket share, by about 10 percentage points (see figure 3.8). the conclusion also holds at the regional level. u.s. computer makersõ share of european and other (excluding japan) supercomputer markets also increasedsignificantly, measured by either machines (figure 3.9) or the capabilityproxy for revenues (figure 3.10).u.s. supercomputer capabilities are strong and competitive in thehighest performing segment of the supercomputer marketplace (see figure 3.11). even if we consider only the 20 fastest computers in the worldevery year, the share manufactured by u.s. producers has been increasingsteadily since the mid1990s and is today about where it was in 1993ñ01234567891011121314151617181920jun 93dec 93jun 94dec 94jun 95dec 95jun 96dec 96jun 97dec 97jun 98dec 98jun 99dec 99jun 00dec 00jun 01dec 01jun 02dec 02jun 03dec 03jun 04industrialvendorres&govtacademicfigure 3.6top 20 machines by installation type.getting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.50getting up to speedfigure 3.7share of top500 machines by country of maker.0%10%20%30%40%50%60%70%80%90%100%jun 93dec 93jun 94dec 94jun 95dec 95jun 96dec 96jun 97dec 97jun 98dec 98jun 99dec 99jun 00dec 00jun 01dec 01jun 02dec 02jun 03dec03jun 04otherjapanu.s.figure 3.8rmax share of top500 machines by maker.0%10%20%30%40%50%60%70%80%90%100%jun 93dec 93jun 94dec 94jun 95dec 95jun 96dec 96jun 97dec 97jun 98dec 98jun 99dec 99jun 00dec 00jun 01dec 01jun 02dec 02jun 03dec 03jun 04otherjapanusgetting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.brief history of supercomputing510%10%20%30%40%50%60%70%80%90%100%jun 93dec 93jun 94dec 94jun 95dec 95jun 96dec96jun 97dec 97jun 98dec 98jun 99dec 99jun 00dec 00jun 01dec 01jun 02dec 02jun 03dec 03jun 04u.s.japaneuropeotherfigure 3.9u.s. maker share of top500 installations in each geographical area.0%10%20%30%40%50%60%70%80%90%100%jun 93dec 93jun 94dec 94jun 95dec 95jun 96dec 96jun 97dec 97jun 98dec 98jun 99dec 99jun 00dec 00jun 01dec 01jun 02dec 02jun 03dec 03jun 04u.s.japaneuropeotherfigure 3.10u.s. maker share of total top500 rmax in each geographical area.getting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.52getting up to speed02468101214161820jun 93dec 93jun 94dec 94jun 95dec 95jun 96dec 96jun 97dec 97jun 98dec 98jun 99dec 99jun 00dec 00jun 01dec 01jun 02dec 02jun 03dec 03jun 04u.s. madejapan madeus installedjapan installedfigure 3.11top 20 machines by maker and country of installation.with 17 of the top 20 machines worldwide made by u.s. producers. thistrend reverses a plunge in the u.s. maker share of the fastest machinesthat took place in 1994 to 8 of the top 20 machines. japanese producerperformance is a mirror image of the u.s. picture, rising to 12 of the top 20in 1994 and then falling steadily to 2 in 2003. the japanese earth simulator was far and away the top machine from 2002 through mid2004, butmost of the computers arrayed behind it were americanmade, unlike thesituation in 1994.a similar conclusion holds if we consider access by u.s. users to thefastest computers (figure 3.11). of the top 20, 14 or 15 were installed inthe united states in the last 3 years, compared with lows of 7 or 8 observed earlier in the 1990s (a sharp drop from 1993, the initial year of thetop500, when 16 or 17 of the top500 were on u.s. soil). again, japan is amirror image of the united states, with 1 or 2 of the top 20 machinesinstalled in japan in 1993, peaking at 10 in 1994, then dropping fairlysteadily, to 2 or 3 over the last 3 years.getting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.brief history of supercomputing53there are indications that national trade and industrial policies maybe having impacts on behavior in global markets. u.s. supercomputermakers now have effectively 100 percent of their home market, measuredby machines (figure 3.9) or capability (figure 3.10). no machines on thetop500 have been sold by japan (the principal technological competitorof the united states) to this country since 2000, and only a handful on thelist were sold in prior years going back to 1998. this contrasts with between 2 and 5 machines on the lists in years prior to 1998. (about half ofthe top500 systems currently installed in japan are u.s. made.)these data coincide with a period in which formal and informal barriers to purchases of japanese supercomputers were created in the unitedstates. conversely, u.s. producer market share in japan, measured in either units or capability, began to fall after the same 1998 watershed intrade frictions. while this analysis does not so prove, one might suspect adegree of retaliation, official or not, in japan. given that u.s. producershave been doing so well in global markets for these products, it is hard toargue that policies encouraging the erection of trade barriers in this sectorwould have any beneficial effect on either u.s. producers or u.s.supercomputer users. this is a subject to which the committee will return.an industrial revolutionfrom the mid1960s to the early 1980s, the supercomputer industrywas dominated by two u.s. firmsñfirst cdc, then cray. the productthese companies producedñhighly capable, very expensive, customdesigned vector supercomputers, with individual models typically produced in quantities well under 100ñwas easily identified and categorized.this small, largely american world underwent two seismic shifts in thelate 1980s.figure 3.12 sketches out the first of these changes. as described earlier, capable japanese supercomputer vendors for the first time began towin significant sales in international markets. the japanese vendors sawtheir share of vector computer installations double, from over 20 percentto over 40 percent over the 6 years from 1986 to 1992.28the second development was the entry of new types of productsñforexample, nonvector supercomputers, typically massively parallel ma28these data are taken from h.w. meuer, 1994, òthe mannheim supercomputer statistics19861992,ó top500 report 1993, j.j. dongarra, h.w. meuer, and e. strohmaier, eds., university of mannheim, pp. 115. see also erich strohmaier, jack j. dongarra, hans w. meuer, andhorst d. simon, 1999, òthe marketplace of high performance computing,ó parallel computing 25(1314):15171544.getting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.54getting up to speed0%20%40%60%80%100%nec281018273540hitachi891117273644fujitsu3136567287108141cdc303445622400cray1181481782352482683051986198719881989199019911992figure 3.12share of vector supercomputers installed.chines built using large numbers of processors interconnected within asingle system. one impetus for the development of these systems wasdarpaõs strategic computing initiative in the 1980s, in part a reaction tothe data depicted in figure 3.12, discussed earlier, and other u.s. government initiatives that coordinated with and followed this initial effort.these new forms of supercomputing systems are not tracked in figure3.12.the new types of supercomputing systems were initially built entirelyfrom customdesigned and manufactured components used only in theseproprietary supercomputer architectures. in the early 1990s, however, reacting to the high fixed costs of designing and manufacturing specializedprocessors that were only going to be used in machines to be built, in themost wildly optimistic estimate, in volumes in the hundreds, some of thesemachines began to make use of the most capable commercially availablemicroprocessors and confined the proprietary elements of thesesupercomputer designs to the overall system architecture and interconnection components. to engineer a system that could be offered with moreattractive cost/performance characteristics, there was a shift from a purelycustom approach to building a highperformance machine, to a hybridapproach making use of cots processor components.over the last 4 years, the highend computing marketplace has ungetting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.brief history of supercomputing55dergone another fairly radical transformation, leaving makers of traditional supercomputers in an increasingly weakened position economically. the impetus for this transformation has been the growing availability of commodity highperformance interconnections, which, coupled tomassproduced, highvolume commodity microprocessors, are now being used to build true commodity supercomputers: systems built entirelyfrom cots hardware components. although not commonly appreciated,over the last several years such commodity supercomputers have rapidlycome to dominate the supercomputer marketplace. to see this, the committee has categorized highend systems intothree groups. first, there are the socalled commodity systems, systemsbuilt using cots microprocessors and cots interconnections. the firstsuch commodity system appeared on the top500 list in 1997.29 second,there are machines using custom interconnections linking cots microprocessors, or machines making use of customized versions of cots microprocessor chips. these systems are labeled as hybrid systems. finally,there are machines using both custom processors and custom interconnects. these are labeled as full custom systems. all traditional vectorsupercomputers fall into this category, as do massively parallel systemsusing custom processors and interconnects.using this taxonomy, all supercomputers on the top500 list from juneof 1993 through june of 2004 were categorized.30 the results are summarized in figure 3.13, which shows changes in mean rmax for each of thesesystem types from 1993 to 2004. commodity systems showed the greatest29this was the experimental university of california at berkeley network of workstations(now).30the categorization used the following rules: all ap1000, convex, cray, fujitsu, hitachi,hitachi sr8000, ibm 3090, kendall square, maspar, ncube, nec, and thinking machinescm2 processorbased systems were categorized as custom. all amd processorbased systems were categorized as commodity. all alpha processor systems were commodity exceptthose made by cray, dec/hp alphaserver 8400 systems, and alphaserver 8400, 4100, and300 clusters, which were categorized as hybrid. all intel processorbased systems were commodity, except those made by intel (sandia asc red, delta, xp, other ipsc 860), meiko,cray, hp superdome itanium systems, and sgi altix systems, which were categorized ashybrid. all power processor systems were categorized as hybrid except ibm pseries, forwhich use of commodity connections was noted in the top500 database, and the parampadma cluster, which were categorized as commodity. all sparc processor systems werehybrid except those that were òselfmadeó and categorized as commodity. all hewlettpackard processor systems were categorized as hybrid. all mipsbased systems were hybrid, except for sgi origin systems, for which the use of ethernet interconnects was noted.the ibm blue gene system using the power pc processor was hybrid; selfmade and eserverblade systems using this processor were commodity.getting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.56getting up to speedannual growth rates in performance; hybrid systems showed the leastgrowth in linpack performance. trend lines fitted to figure 3.13 haveslopes yielding annual growth rates in rmax of 111 percent for commoditysystems, 94 percent for custom systems, and 73 percent for hybrid systems.31 this is considerably faster than annual growth rates in singleprocessor floatingpoint performance shown on other benchmarks, suggesting that increases in the number of processors and improvements in theinterconnect performance yielded supercomputer performance gains significantly greater than those due to component processor improvementalone for both commodity and custom systems. hybrid system performance improvement, on the other hand, roughly tracked singleprocessorperformance gains.nonetheless, the economics of using much less expensive cots microprocessors was compelling. hybrid supercomputer systems rapidlyreplaced custom systems in the early 1990s. custom supercomputer sys31a regression line of the form ln rmax = a + b time was fit, where time is a variableincremented by one every half year, corresponding to a new top500 list. annualized trendgrowth rates were calculated as exp(2b) ð 1.figure 3.13mean rmax by system type.110100100010000jun 93dec 93jun 94dec 94jun 95dec 95jun 96dec 96jun 97dec 97jun 98dec 98jun 99dec 99jun 00dec 00jun 01dec 01jun 02dec 02jun 03dec 03jun 04commod.hybridcustommean rmaxgetting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.brief history of supercomputing570%20%40%60%80%100%jun 93dec 93jun 94dec 94jun 95dec 95jun 96dec 96jun 97dec 97jun 98dec 98jun 99dec 99jun 00dec 00jun 01dec 01jun 02dec 02jun 03dec 03jun 04customhybridcommod.figure 3.14 share of top500 by system type.tems, increasingly, were being used only in applications where softwaresolutions making use of massively parallel hybrid systems were unsatisfactory or unavailable, or where the need for very high performance warranted a price premium.commodity highperformance computing systems first appeared onthe top500 list in 1997, but it was not until 20012002 that they began toshow up in large numbers. since 2002, their numbers have swelled, andtoday commodity systems account for over 60 percent of the systems onthe list (see figure 3.14). just as hybrid systems replaced many customsystems in the late 1990s, commodity systems today appear to be displacing hybrid systems in acquisitions. a similar picture is painted by data onrmax, which, as noted above, is probably a better proxy for systems revenues. figure 3.15 shows how the distribution of total top500 systemperformance between these classes of systems has changed over time.furthermore, the growing marketplace dominance of commoditysupercomputer systems is not just at the low end of the market. a similargetting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.58getting up to speed0%10%20%30%40%50%60%70%80%90%100%jun 93dec 93jun 94dec 94jun 95dec 95jun 96dec 96jun 97dec 97jun 98dec 98jun 99dec 99jun 00dec 00jun 01dec 01jun 02dec 02jun 03dec 03jun 04customhybridcommod.figure 3.15share of top500 rmax by system type.pattern has also been evident in the very highest performance systems.figure 3.16 shows how the numbers of top20 systems in each of thesecategories has changed over time. a commodity system did not appear inthe top 20 highest performing systems until mid2001. but commoditysupercomputers now account for 12 of the 20 systems with the highestlinpack scores. as was true with the entire top500 list, custom systemswere replaced by hybrid systems in the 1990s in the top 20, and the hybridsystems in turn have been replaced by commodity systems over the last 3years.this rapid restructuring in the type of systems sold in the marketplace has had equally dramatic effects on the companies selling supercomputers. in 1993, the global hpc marketplace (with revenues againproxied by total rmax) was still dominated by cray, with about a third ofthe market, and four other u.s. companies, with about another 40 percentof the market (three of those four companies have since exited the industry). the three japanese vector supercomputer makers accounted for another 22 percent of top500 performance (see figure 3.17).getting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.brief history of supercomputing59024681012141618jun 93sep 93dec 93mar 94jun 94sep 94dec 94mar 95jun 95sep 95dec 95mar 96jun 96sep 96dec 96mar 97jun 97sep 97dec 97mar 98jun 98sep 98dec 98mar 99jun 99sep 99dec 99mar 00jun 00sep 00dec 00mar 01jun 01sep 01dec 01mar 02jun 02sep 02dec 02mar 03jun 03sep 03dec 03mar 04jun 04customhybridcommod.figure 3.16share of top 20 machines by system type.of the five u.s. companies with significant market share on this chart,two (intel and thinking machines, second only to cray) were buildinghybrid systems and three (cray, hewlettpackard, and kendall squareresearch) were selling custom systems.32 the makers of traditional custom vector supercomputers (cray and its japanese vector competitors)have about half of the market share shown if only vector computers areconsidered (compare to figure 3.12). clearly, the hpc marketplace wasundergoing a profound transformation in the early 1990s.a decade later, after the advent of hybrid systems and then of commodity highend systems, the players have changed completely (see figure 3.18). a company that was not even present on the list in 1993 (ibm,marketing both hybrid and commodity systems) now accounts for overhalf of the market, hewlettpackard (mainly hybrid systems) now has32although some of the thinking machines systems counted here were using older proprietary processors, most of the thinking machines supercomputers on this chart werenewer cm5 machines using commodity sparc processors.getting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.60getting up to speedfujitsu9%hitachi1%nec12%cray35%hp2%intel8%kendall square3%thinking machines28%maspar1%all others1%figure 3.17top500 market share (rmax) by company, june 1993.cray2%dell3%hp19%ibm51%linux networx3%selfmade1%sgi3%sun1%fujitsu2%hitachi1%nec6%all others8%figure 3.18top500 market share (rmax) by company, june 2004.getting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.brief history of supercomputing61roughly the same market share as all three japanese producers did back in1993, and other entirely new, purecommodity u.s. vendors in this product space (dell, linux networks) are now larger than two of the threetraditional japanese supercomputer vendors. the most successful japanese producer, nec, has about half of the top500 market share it had in1993. cray is a shadow of its former market presence, with only 2 percentof installed capability. two other u.s. hpc vendors (sun and sgi), whichgrew significantly with the flowering of hybrid systems in the late 1990s,have ebbed with the advent of commodity systems and now have sharesof the market comparable to the pure commodity supercomputer vendorsand selfmade systems.over the last 15 years, extraordinary technological ferment has continuously restructured the economics of this industry and the companiessurviving within its boundaries. any policy designed to keep neededsupercomputing capabilities available to u.s. government and industrialusers must recognize that the technologies and companies providing thesesystems are living through a period of extremely rapid technological andindustrial change.impactsthroughout the computer age, supercomputing has played two important roles. first, it enables new and innovative approaches to scientificand engineering research, allowing scientists to solve previously unsolvable problems or to provide superior answers. often, supercomputershave allowed scientists, engineers, and others to acquire knowledge fromsimulations. simulations can replace experiments in situations where experiments are impossible, unethical, hazardous, prohibited, or too expensive; they can support theoretical experiments with systems that cannotbe created in reality, in order to test the prediction of theories; and theycan enhance experiments by allowing measurements that might not bepossible in a real experiment. during the last decades, simulations onhighperformance computers have become essential to the design of carsand airplanes, turbines and combustion engines, silicon chips or magneticdisks; they have been extensively used in support of petroleum exploration and exploitation. accurate weather prediction would not be possiblewithout supercomputing. according to a report by the lawrence berkeley national laboratory (lbnl) for doe, òsimulation has gained equalfooting to experiments and theory in the triad of scientific process.ó33 in33lbnl. 2002. doe greenbookñneeds and directions in highperformance computing for theoffice of science. prepared for the u.s. department of energy. april, p. 1.getting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.62getting up to speeddeed, a significant fraction of the articles published in top scientific journals in areas such as physics, chemistry, earth sciences, astrophysics, andbiology, depend for their results on supercomputer simulations.the second major effect supercomputing technology has had on computing in general takes place through a spillover effect. todayõs desktopcomputer has the capability of the supercomputers of a decade ago.direct contributionssupercomputers continue to lead to major scientific contributions.supercomputing is also critical to our national security. supercomputingapplications are discussed in detail in chapter 4. here the committee highlights a few of the contributions of supercomputing over the years.the importance of supercomputing has been recognized by many reports. the 1982 lax report concluded that largescale computing was vitalto science, engineering, and technology.34 it provided several examples.progress in oil reservoir exploitation, quantum field theory, phase transitions in materials, and the development of turbulence were all becomingpossible by combining supercomputing with renormalization group techniques (p. 5). aerodynamic design using a supercomputer resulted in thedesign of an airfoil with 40 percent less drag than the design using previous experimental techniques (p. 5). supercomputers were also critical fordesigning nuclear power plants (p. 6). the lax report also praised supercomputers for helping to find new phenomena through numerical experiments, such as the discovery of nonergodic behavior in the formation ofsolitons and the presence of strange attractors and universal features common to a large class of nonlinear systems (p. 6). as supercomputers become more powerful, new applications emerge that leverage their increased performance. recently, supercomputer simulations have beenused to understand the evolution of galaxies, the life cycle of supernovas,and the processes that lead to the formation of planets.35 such simulations provide invaluable insight into the processes that shaped our universe and inform us of the likelihood that lifefriendly planets exist. simulations have been used to elucidate various biological mechanisms, such34national science board. 1982. report of the panel on large scale computing in science andengineering. washington, d.c., december 26 (the lax report).35òsimulation may reveal the detailed mechanics of exploding stars,ó asc/alliancescenter for astrophysical thermonuclear flashes, see <http://flash.uchicago.edu/website/home/>; òplanets may form faster than scientists thought,ó pittsburgh supercomputercenter, see <http://www.psc.edu/publicinfo/news/2002/planets20021211.html>; j.dubinski, r. humble, u.l. pen, c. loken, and p. martin, 2003, òhigh performance commodity networking in a 512cpu teraflops beowulf cluster for computational astrophysics,ó paper submitted to the sc2003 conference.getting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.brief history of supercomputing63as the selective transfer of ions or water molecules through channels incellular membranes or the behavior of various enzymes.36 climate simulations have led to an understanding of the longterm effects of humanactivity on earthõs atmosphere and have permitted scientists to exploremany whatif scenarios to guide policies on global warming. we havenow a much better understanding of ocean circulation and of globalweather patterns such as el niœo.37 lattice quantum chromodynamics(qcd) computations have enhanced our basic understanding of matterby exploring the standard model of particle physics.38 box 3.1 highlightsthe value of having a strong supercomputing program to solve unexpected critical national problems.codes initially developed for supercomputers have been critical formany applications, such as petroleum exploration and exploitation (threedimensional analysis and visualization of huge amounts of seismic dataand reservoir modeling), aircraft and automobile design (computationalfluid mechanics codes, combustion codes), civil engineering design (finiteelement codes), and finance (creation of a new market in mortgagebackedsecurities).39much of the early research on supercomputers occurred in the laboratories of doe, nasa, and other agencies. as the need for supercomputingin support of basic science became clear, the nsf supercomputing centerswere initiated in 1985, partly as a response to the lax report. their mission has expanded over time. the centers have provided essentialsupercomputing resources in support of scientific research and havedriven important research in software, particularly operating systems,compilers, network control, mathematical libraries, and programming languages and environments.40supercomputers play a critical role for the national security community according to a report for the secretary of defense.41 that report iden36benoit roux and klaus schulten. 2004. òcomputational studies of membrane channels.ó structure 12 (august): 1.37national energy research scientific computing center. 2002. ònersc helps climatescientists complete firstever 1,000year run of nationõs leading climatechange modeling application.ó see <http://www.lbl.gov/sciencearticles/archive/nersc1000yearclimatemodel.html>.38d. chen, p. chen, n.h. christ, g. fleming, c. jung, a. kahler, s. kasow, y. luo, c.malureanu, and c.z. sui. 1998. ò3 lattice quantum chromodynamics computations.ó thispaper, submitted to the sc1998 conference, won the gordon bell prize in the category priceperformance.39nrc. 1995. evolving the high performance computing and communications initiative to support the nationõs information infrastructure. washington, d.c.: national academy press, p. 35.40ibid., p. 108.41office of the secretary of defense. 2002. report on high performance computing for thenational security community.getting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.64getting up to speedbox 3.1 sandia supercomputers aid inanalysis of columbia disastersandia national laboratories and lockheed martin offered sandiaõstechnical support to nasa immediately after the february 1, 2003, breakupof the space shuttle columbia. sandia personnel teamed with analysts fromfour nasa centers to provide timely analysis and experimental results tonasa johnson space center accident investigators for the purpose of either confirming or closing out the possible accident scenarios being considered by nasa. although sandiaõs analysis capabilities had been developed in support of doeõs stockpile stewardship program, they containedphysical models appropriate to the accident environment. these modelswere used where they were unique within the partnership and wheresandiaõs massively parallel computers and asc code infrastructure wereneeded to accommodate very large and computationally intense simulations. sandia external aerodynamics and heat transfer calculations weremade for both undamaged and damaged orbiter configurations using rarefied direct simulation monte carlo (dsmc) codes for configurations flyingat altitudes above 270,000 ft and continuum navierstokes codes for altitudes below 250,000 ft. the same computational tools were used to predict jet impingement heating and pressure loads on the internal structure,as well as the heat transfer and flow through postulated damage sites intoand through the wing. navierstokes and dsmc predictions of heatingrates were input to sandiaõs thermal analysis codes to predict the timerequired for thermal demise of the internal structure and for wire bundleburnthrough. experiments were conducted to obtain quasistatic and dynamic material response data on the foam, tiles, strain isolation pad, andreinforced carboncarbon wing leading edge. these data were then used insandia finite element calculations of foam impacting the thermal protection tiles and wing leading edge in support of accident scenario definitionand foam impact testing at southwest research institute.the supercomputers at sandia played a key role in helping nasa determine the cause of the space shuttle columbia disaster. sandia researchersõanalyses and experimental studies supported the position that foam debrisshed from the fuel tank and impacting the orbiter wing during launch wasthe most probable cause of the wing damage that led to the breakup of thecolumbia.note: the committee thanks robert thomas and the sandia national laboratories staff fortheir assistance in drafting this box.getting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.brief history of supercomputing65tified at least 10 defense applications that rely on highperformance computing (p. 22): comprehensive aerospace vehicle design, signals intelligence, operational weather/ocean forecasting, stealthy ship design,nuclear weapons stockpile stewardship, signal and image processing, thearmyõs future combat system, electromagnetic weapons, geospatial intelligence, and threat weapon systems characterization.spillover effectsadvanced computer research programs have had major payoffs interms of technologies that enriched the computer and communication industries. as an example, the darpa vlsi program in the 1970s had major payoffs in developing timesharing, computer networking, workstations, computer graphics, windows and mouse user interface technology,very large integrated circuit design, reduced instruction set computers,redundant arrays of inexpensive disks, parallel computing, and digitallibraries.42 todayõs personal computers, email, networking, data storageall reflect these advances. many of the benefits were unanticipated.closer to home, one can list many technologies that were initially developed for supercomputers and that, over time, migrated to mainstreamarchitectures. for example, vector processing and multithreading, whichwere initially developed for supercomputers (illiac iv/star100/ti ascand cdc 6600, respectively), are now used on pc chips. instructionpipelining and prefetch and memory interleaving appeared in early ibmsupercomputers and have become universal in todayõs microprocessors.in the software area, program analysis techniques such as dependenceanalysis and instruction scheduling, which were initially developed forsupercomputer compilers, are now used in most mainstream compilers.highperformance i/o needs on supercomputers, particularly parallelmachines, were one of the motivations for redundant array of inexpensive disks (raid)43 storage, now widely used for servers. scientific visualization was developed in large part to help scientists interpret the results of their supercomputer calculations; today, even spreadsheets candisplay threedimensional data plots. scientific software libraries such aslapack that were originally designed for highperformance platformsare now widely used in commercial packages running on a large range of42nrc. 1995. evolving the high performance computing and communications initiative to support the nationõs information infrastructure. washington, d.c.: national academy press, pp.1718.43raid is a disk subsystem consisting of many disks that increases performance and/orprovides fault tolerance.getting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.66getting up to speedplatforms. in the application areas, many application packages that areroutinely used in industry (e.g., nastran) were initially developed forsupercomputers. these technologies were developed in a complex interaction involving researchers at universities, the national laboratories, andcompanies. the reasons for such a spillover effect are obvious and stillvalid nowadays: supercomputers are at the cutting edge of performance.in order to push performance they need to adapt new hardware and software solutions ahead of mainstream computers. and the high performance levels of supercomputers enable new applications that can be developed on capability platforms and then used on an increasingly broaderset of cheaper platforms as hardware performance continues to improve.getting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.674the demand for supercomputingthe committee now turns to a discussion of some of the key application areas that are using supercomputing and are expected to continue to do so. as each of these areas is reviewed, it is important torecognize that many of the areas are themselves supported by government research funds and contribute to broader societal objectives, ranging from national defense to the ability to make more informed decisionson climate policy. also, as is discussed further in chapter 5, the precisetechnology requirements for these different application areas differ. additionally, several of them are subject to at least some degree of secrecy.as will be discussed in chapter 8, a key issue in the effective managementof and policy toward supercomputing involves understanding and choosing the degree of commitment, the degree of diversification, and the degree of secrecy associated with the technology.supercomputers are tools that allow scientists and engineers to solvecomputational problems whose size and complexity make them otherwise intractable. such problems arise in almost all fields of science andengineering. although mooreõs law and new architectural innovationsenable the computational power of supercomputers to grow, there is noforeseeable end to the need for ever larger and more powerful systems.in most cases, the problem being solved on a supercomputer is derived from a model of the physical world. an example is predictingchanges that earthõs climate might experience centuries into the future.approximations are made when scientists use partial differential equations to model a physical phenomenon. to make the solution feasible,compromises must be made in the resolution of the grids used to discretizegetting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.68getting up to speedthe equations. the coefficients of the matrices are represented as numbersexpressed in scientific notation.1 therefore, the computation does not precisely emulate the real phenomenon but, rather, simulates it with enoughfidelity to stimulate human scientific imagination or to aid human engineering judgment. as computational power increases, the fidelity of themodels can be increased, compromises in the methods can be eliminated,and the accuracy of the computed answers improves. an exact solution isnever expected, but as the fidelity increases, the error decreases and results become increasingly useful.this is not to say that exact solutions are never achieved. many problems with precise answers are also addressed by supercomputing. examples are found in discrete optimization, cryptography, and mathematical fields such as number theory. recently a whole new discipline,experimental mathematics, has emerged that relies on algorithms suchas integer relation detection. these are precise calculations that requirehundreds or even thousands of digits.2,3 at the hardware level, these operations are most efficiently done using integer arithmetic. floatingpointarithmetic is sometimes used, but mostly to perform whole numberoperations.by studying the results of computational models, scientists are able toglean an understanding of phenomena that are not otherwise approachable. often these phenomena are too large and complex or too far away intime and space to be studied by any other means. scientists model turbulence inside supernovae and material properties at the center of earth.they look forward in time and try to predict changes in earthõs climate.they also model problems that are too small and too fast to observe, suchas the transient, atomicscale dynamics of chemical reactions. material scientists can determine the behavior of compounds not known to exist innature.supercomputers not only allow people to address the biggest andmost complex problems, they also allow people to solve problems faster,even those that could fit on servers or clusters of pcs. this rapid time tosolution is critical in some aspects of emergency preparedness and national defense, where the solutions produced are only valuable if they canbe acted on in a timely manner. for example, predicting the landfall of a1ieee standard 754, available at <http://cch.loria.fr/documentation/ieee754/#sgiman>.2jonathan m. borwein and david h. bailey. 2004. mathematics by experiment: plausiblereasoning in the 21st century. natick, mass.: a.k. peters.3jonathan m. borwein, david h. bailey, and roland girgensohn. 2004. experimentalmathematics: computational paths to discovery. natick, mass.: a.k. peters.getting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.the demand for supercomputing69hurricane allows evacuation of the coastline that will be impacted (savinglives), while not disturbing the surrounding area (saving money). rapidtime to solution in a commercial arena translates into minimizing the timeto market for new products and services. the ability to solve many problems in a reasonable time frame allows engineers to explore design spacesbefore committing to the time and expense of building prototypes.an important phenomenon that cannot be underestimated is how thepotential for making a scientific discovery can encourage human creativity. few advances in science and technology are unplanned or unexpected,at least in hindsight. discoveries almost always come in the wake of workthat inspires or enables them. when one discovery opens up the possibility of another, the leading intellects of our time will focus tremendoustime and energy on developing the algorithms needed to make a discovery that appears tantalizingly close. supercomputing expands the spacewithin which such new algorithms can be found by maximizing the resources that can be brought to bear on the problem.supercomputing allows pioneering scientists and engineers to inventsolutions to problems that were initially beyond human ability to solve.often, these are problems of great national importance. dimitri kusnezov,director of the nnsa, put it this way when he testified before the u.s.senate in june 2004:4 òsimulating the time evolution of the behavior of anexploding nuclear device is not only a mammoth scientific enterprise froma computational perspective, it probably represents the confluence ofmore physics, chemistry and material science, both equilibrium and nonequilibrium, at multiple length and time scales than almost any other scientific challenge.óover time and with increasing experience, the algorithms mature andbecome more efficient. furthermore, smaller computing systems such asservers and personal computers become more powerful. these two trendsmake problems that were once addressable only by nation states now addressable by large research and engineering enterprises and, given enoughtime, eventually by individual scientists and engineers. consider an example from mechanical dynamics. starting in the 1950s, scientists at thenuclear weapons laboratories pioneered the use of explicit finite elementprograms to simulate the propagation of shocks through the devices theywere developing. these codes became available to industrial users in the1980s. through the 1980s and into the 1990s, automotive companies ran4testimony of dimitri kusnezov, director, office of advanced simulation and computing, nnsa, u.s. department of energy, before the u.s. senate committee on energy andnatural resources, subcommittee on energy, june 22, 2004.getting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.70getting up to speedtheir occupant safety problems on the same type of supercomputers usedby the national laboratories. as the power of servers and pcs continuedto increase, many of those engineering problems were able to move todepartmentalscale systems in the late 1990s, and even to individual pcstoday. without the development of algorithms and software on supercomputers in the 1980s and 1990s, such codes would not be available forbroad use on servers and pcs today.the example above should not be construed to suggest that there isno longer a need for supercomputing in mechanical engineering. on thecontrary, while todayõs codes are very useful tools for supporting designand analysis, they are by no means predictive. one software vendor believes that his users in the automotive industry could productively employ computing power at least seven orders of magnitude greater thanwhat they have today. there are many such examples, some of which aregiven later in the chapter.the above discussion has focused on supercomputers as tools forresearch performed in other disciplines. by their very nature, supercomputers push the boundaries of computer engineering in terms ofscale. to effectively solve the most challenging problems requires thatsupercomputers be architected differently than standard pcs and servers. as the underlying technology (semiconductors, optics, etc.) fromwhich they are constructed evolves, the design space for supercomputerschanges rapidly, making supercomputers themselves objects of scientificcuriosity. this last point will be taken up in chapter 5.compelling applications for supercomputingthe committee on the future of supercomputing has extensively investigated the nature of supercomputing applications and their presentand future needs. its sources of information have included its own membership as well as the many experts from whom it heard in committeemeetings. the committee has talked with the directors of many supercomputing centers and with scientists and engineers who run applicationprograms at those centers. subcommittees visited doe weapons laboratories, doe science laboratories, the national security agency, and thejapanese earth simulator. in addition, the committee held a 2day applications workshop in santa fe, new mexico, in september 2003, duringwhich approximately 20 experts discussed their applications and theircomputing requirements. what follows is a consensus summary of theinformation from all of those sources.many applications areas were discussed either at the santa fe workshop or in presentations to the committee. in addition to furthering basicscientific understanding, most of these applications have clear practicalgetting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.the demand for supercomputing71benefits. a capsule summary of the areas is given first, followed by adetailed description. this is a far from complete list of supercomputingapplications, but it does represent their broad range and complexity. several other recent reports give excellent summaries of the highend computational needs of applications. among those reports are the hecrtfworkshop report,5 the scales report,6 the ihec report,7 and the hecrtffinal report.8¥stockpile stewardship. several of the most powerful computers in theworld are being used as part of doeõs advanced simulation and computing (asc) to ensure the safety and reliability of the nationõs stockpileof nuclear weapons. franceõs cea (atomic energy commission) has asimilar project.¥intelligence/defense. very large computing demands are made by thedod, intelligence community agencies, and related entities in order toenhance the security of the united states and its allies, including anticipating the actions of terrorists and of rogue states.¥climate prediction. many u.s. highend computational resourcesand a large part of the japanese earth simulator are devoted to predictingclimate variations and anthropogenic climate change, so as to anticipateand be able to mitigate harmful impacts on humanity.¥plasma physics. an important goal of plasma physics will be to produce costeffective, clean, safe electric power from nuclear fusion. verylarge simulations of the reactions in advance of building the generatingdevices are critical to making fusion energy feasible.¥transportation. whether it be an automobile, an airplane, or a spacecraft, large amounts of supercomputer resources can be applied to understanding and improving the vehicleõs airflow dynamics, fuel consumption, structure design, crashworthiness, occupant comfort, and noisereduction, all with potential economic and/or safety benefits.¥bioinformatics and computational biology. biology has huge emergingcomputational needs, from dataintensive studies in genomics to5nitrd high end computing revitalization task force (hecrtf). 2003. report of theworkshop on the roadmap for the revitalization of highend computing. daniel a. reed, ed. june1620, washington, d.c.6doe, office of science. 2003. òa sciencebased case for largescale simulation,ó scalesworkshop report, vol. 1, july.7department of defense, national security agency. 2002. report on high performancecomputing for the national security community. july 1.8nitrd hecrtf. 2004. federal plan for high end computing. may.getting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.72getting up to speedcomputationally intensive cellular network simulations and largescalesystems modeling. applications promise to provide revolutionary treatments of disease.¥societal health and safety. supercomputing enables the simulation ofprocesses and systems that affect the health and safety of our society (forinstance, pollution, disaster planning, and detection of terrorist actionsagainst local and national infrastructures), thereby facilitating governmentand private planning.¥earthquakes. supercomputing simulation of earthquakes showspromise for allowing us to predict earthquakes and to mitigate the risksassociated with them.¥geophysical exploration and geoscience. supercomputing in solidearth geophysics involves a large amount of data handling and simulation for a range of problems in petroleum exploration, with potentiallyhuge economic benefits. scientific studies of plate tectonics and earth as ageodynamo require immense supercomputing power.¥astrophysics. supercomputer simulations are fundamental to astrophysics and play the traditional scientific role of controlled experimentsin a domain where controlled experiments are extremely rare or impossible. they allow vastly accelerated time scales, so that astronomical evolution can be modeled and theories tested.¥materials science and computational nanotechnology. the simulationof matter and energy from first principles is very computationally intensive. it can lead to the discovery of materials and reactions having largeeconomic benefitsñfor instance, superconductors that minimize transmission loss in power lines and reduce heating in computers.¥human/organizational systems studies. the study of macroeconomicsand social dynamics is also amenable to supercomputing. for instance,the behavior of large human populations is simulated in terms of the overall effect of decisions by hundreds of millions of individuals.common themes and synergies across applications areasthe committee was struck by the many similarities across applicationareas in the importance of supercomputing to each scientific domain, thepresent use of computational equipment, and projected futuresupercomputing needs. most of the applications areas use supercomputersimulations in one of three ways: (1) to extend the realization of complexnatural phenomena so that they can be understood scientifically; (2) totest, via simulation, systems that are costly to design or to instrument,saving both time and money; or (3) to replace experiments that are hazardous, illegal, or forbidden by policies and treaties. the use ofsupercomputing provides information and predictions that are beneficialgetting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.the demand for supercomputing73to the economy, to health, and to society at large. the applications areasall use supercomputing to accomplish tasks that are uneconomicalñoreven impossibleñwithout it.whether the task is cracking a cryptographic code, incorporating newphysics into a simulation, or detecting elusive targets, the real value ofsupercomputing is increased insight and understanding. time to solutionincludes getting a new application up and running (the programmingtime), waiting for it to run (the execution time), and, finally, interpretingthe results (the interpretation time). applications areas have productivityproblems because the time to program new supercomputers is increasing.while application codes and supercomputing systems have both becomemore complex, the compilers and tools that help to map application logiconto the hardware have not improved enough to keep pace with that complexity. the recent darpa high productivity computing systems(hpcs) initiative, having recognized this problem, has a strong focus onimproving the programmability of supercomputers and on developingproductivity metrics that will provide a measure of this improvement.9it is well known that computational techniques span application areas. for example, astrophysics, aircraft design, climate modeling, and geophysics all need different models of fluid flow. computational modelingused in applications that seek fundamental understanding enhances applications that solve realworld needs. thus, basic understanding ofplasma physics and materials facilitates stockpile stewardship, while basic results in weather prediction can facilitate climate modeling. theseexamples are illustrative, not a complete story.in july 2003, raymond orbach, director of the doe office of science,testified before the u.s. house of representatives committee on science.he saidthe tools for scientific discovery have changed. previously, science hadbeen limited to experiment and theory as the two pillars for investigationof the laws of nature. with the advent of what many refer to as òultrascaleó computation, a third pillar, simulation, has been added to thefoundation of scientific discovery. modern computational methods aredeveloping at such a rapid rate that computational simulation is possibleon a scale that is comparable in importance with experiment and theory.the remarkable power of these facilities is opening new vistas for scienceand technology. previously, we used computers to solve sets of equations representing physical laws too complicated to solve analytically.9for more information on the hpcs program, see <http://www.darpa.mil/ipto/programs/hpcs/index.htm>.getting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.74getting up to speednow we can simulate systems to discover physical laws for which thereare no known predictive equations.10dr. orbach also remarked that computational modeling and simulation were among the most significant developments in the practice of scientific inquiry in the latter half of the 20th century. supercomputing hascontributed to essentially all scientific research programs and has provedindispensable to doeõs missions. computerbased simulation can bridgethe gap between experimental data and simple mathematical models, thusproviding a means for predicting the behavior of complex systems.selected application areasstockpile stewardshipin june 2004, dimitri kusnezov, director of the office of advancedsimulation and computing at doeõs national nuclear security administration, testified before the u.s. senate committee on energy and naturalresources. he said òsince the dawn of the nuclear age, computation hasbeen an integral part of the weapons program and our national security.with the cessation of testing and the advent of the sciencebased stockpilestewardship program, asc simulations have matured to become a critical toolin stockpile assessments and in programs to extend the life of thenationõs nuclear deterrent.ó11even with simple, lowresolution physics models, weapons simulations have given insight and information that could not be obtained inother ways.12 thus, the doe nuclear weapons laboratories have alwaysbeen at the forefront of supercomputing development and use. the hugechallenge of nuclear weapons simulation is to develop the tools (hardware, software, algorithms) and skills necessary for the complex, highlycoupled, multiphysics calculations needed for accurate simulations. under the doe/nnsa stockpile stewardship program, several of the larg10testimony of raymond l. orbach, director, office of science, u.s. department of energy, before the u.s. house of representatives committee on science, july 16, 2003.11testimony of dimitri kusnezov, director, office of advanced simulation and computing, nnsa, u.s. department of energy, before the u.s. senate committee on energy andnatural resources, subcommittee on energy, june 22, 2004.12this subsection is based on white papers by charles f. mcmillan et al., llnl, òcomputational challenges in nuclear weapons simulation,ó and by robert weaver, lanl, òcomputational challenges to supercomputing from the los alamos crestone project: a personal perspective.ó both papers were prepared for the committeeõs applications workshopat santa fe, n.m., in september 2003.getting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.the demand for supercomputing75est supercomputers in the world are being developed and used as part ofthe nnsa asc program to ensure the safety and reliability of the nationõsstockpile of nuclear weapons.one of the fundamental problems that the national laboratories areattempting to solve with extremely complex (and obviously classified)codes is the simulation of the full physical operation of the nuclear weapons in the u.s. stockpile. this problem is important in order to continue tocertify to the nation that the nuclear deterrent stockpile is safe and reliablein the absence of testing. prior to the development of the current generation of national laboratory codes, weapons designers had to rely on amore empirical solution to the complex, nonlinear coupled physics thatoccurs in a nuclear weapon. this procedure had to be augmented by anexperimental test of the design.in the absence of nuclear testing, the simulation codes must rely lesson empirical results and must therefore be more refined. the simulationshave evolved from twodimensional models and solutions to threedimensional ones. that evolution has required a more than 1,000fold increasein computational resources. to achieve that capability, the simulationsare developed and run on the most advanced platformsñsystems that areprototype machines with few users. these platforms often lack the idealinfrastructure and stability, leading to new and unanticipated challenges,with the largest runs taking many months to a year to complete. dr.kusnezov noted that stockpile simulations òcurrently require heroic,nearly yearlong calculations on thousands of dedicated processors. it isessential that we provide the designers with the computational tools thatallow such simulations to be completed in a reasonable time frame forsystematic analysis. this is one of the requirements that drive us well intothe petascale regime for future platforms.ó13during the last 5 years, asc has acquired a number of increasinglypowerful supercomputing systems and plans to continue such acquisition. the vendors of these systems include intel, ibm, silicon graphics,cray, and hewlettpackard. the number of processors in these systemsranges from about 2,000 to a proposed 131,000, with peak performancesranging from 3 trillion floatingpoint operations per second (tflops) to aproposed 360 tflops. portability of applications among the systems hasbecome relatively smooth because of commitment in general to standardlanguages and programming models and avoidance of processorspecificoptimizations. these practices have allowed the asc community to begintaking advantage of new processor technology as it becomes available.13testimony of dimitri kusnezov, director, office of advanced simulation and computing, u.s. department of energy, before the u.s. senate committee on energy and naturalresources, subcommittee on energy, june 22, 2004.getting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.76getting up to speedthe asc programming environment stresses software developmenttools because of the scale of the hardware architecture, software complexity, and the need for compatibility across asc platforms. a multiphysicsapplication code may take 4 to 6 years to become useful and may thenhave a lifespan of several decades. thus, it is important that code development focus on present and future supercomputing systems. almost allasc applications, for example, use a combination of three programmingmodels: the serial model, symmetric multiprocessing using openmp, andmessage passing using mpi. programming is typically done in ansi c,c++, and fortran 90. algorithm development attempts to balance the (often competing) requirements of highfidelity physics, short executiontime, parallel scalability, and algorithmic scalability14; not surprisingly, itis in some ways influenced by target architectures. it is interesting to notethat, even with all this effort, codes running on the asc white systemtypically attain from 1 percent to 12 percent of theoretical peak performance. it is not uncommon for complex scientific codes run on other platforms to exhibit similarly modest percentages. by contrast, the somewhatmisleading linpack benchmarks run at 59 percent of peak on that system.signals intelligencethe computational challenges posed by the signals intelligence mission of the nsa are enormous.15 the essence of this mission is to interceptand analyze foreign adversariesõ communications signals, many of whichare protected by encodings and other complex countermeasures. nsamust collect, process, and disseminate intelligence reports on foreign intelligence targets in response to intelligence requirements set at the highest levels of the government. the signals intelligence mission targets capabilities, intentions, and activities of foreign powers, organizations, orpersons. it also plays an important counterintelligence role in protectingagainst espionage, sabotage, or assassinations conducted for or on behalfof foreign powers, organizations, persons, or international terrorist groupsor activities.the context and motivation that the signals intelligence mission pro14parallel scalability means nearlinear decrease in execution time as an increasing number of processors are used; algorithm scalability means moderate (nearlinear) increase incomputer time as problem size increases.15this subsection is based on excerpts from the white paper òcomputational challengesin signals intelligence,ó prepared by gary hughes, nsa, and william carlson and francissullivan, institute for defense analyses, center for computational science, for thecommitteeõs santa fe, n.m., applications workshop, september 2003.getting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.the demand for supercomputing77vides are essential to understanding its demands on supercomputing. twocharacteristics are key: problem choice and timeliness of solutions. thehighest priority problems to be solved are chosen not by nsa itself butrather by the very entities that pose the greatest danger: foreign adversaries. they do this when they choose communication methods. this singlecharacteristic puts phenomenal demands on both the development of solutions and their deployment on available computing platforms. solutionsmust also be timelyñthe intelligence derived from the communicationòattack at dawnó is, to say the least, far less valuable at noon. timelinessapplies to both the development of solutions and their deployment. whilethese specific missiondriven requirements are unique to the signals intelligence mission, their effect is seen across a fairly broad spectrum ofmission agencies, both inside and outside the defense community. this isin contrast to computing that targets broad advances in technology andscience. in this context, computations are selected more on the basis oftheir match to available resources and codes.there are two main uses of supercomputing driven by the signalsintelligence mission: intelligence processing (ip) and intelligence analysis(ia). intelligence processing seeks to transform intercepted communications signals into a form in which their meaning can be understood. thismay entail overcoming sophisticated cryptographic systems, advancedsignal processing, message reconstruction in the presence of partial orcorrupted data, or other complex signaling or communications subsystems. intelligence analysis begins with the output of ip and seeks totransform the blizzard of communication messages into a complete mosaic of knowledge so that adversariesõ intentions can be discerned andactionable intelligence provided to national leadership and others with aneed to know.the key computational characteristics of signals intelligence problemsdiffer greatly from those of the other scientific problems discussed in thissection. there is extensive use of bit operations and operations in nonstandard algebraic systems; floating point is used on only a tiny percentage of problems. a significant portion of the problem space is easily amenable to all forms of parallel processing (e.g., òembarrassingly paralleló)techniques. yet another significant portion of the problem space uses computations needing random access to extremely large data sets in memoryand sustained, but unpredictable, interprocessor communication. in fact,the designers of cryptographic systems do their best to ensure there is noway to segment the codebreaking problem. additionally, the knowledgediscovery problem requires the understanding of extremely large graphnetworks with a dynamic collection of vertices and edges. the scale ofthis knowledge discovery problem is significantly larger than the largestcommercial data mining operations.getting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.78getting up to speedcomputational systems for signals intelligence include workstations,workstation farms, beowulf clusters, massively parallel supercomputers,vector supercomputers, òhandmadeó fpgaenhanced systems, and others. operating systems used are mainly unix and linux and programming is done mainly in c and universal parallel c (upc).16 interprocessorcommunication is essential for the most demanding computations, yetmpi and related message passing models are not used because the addedoverhead of message passing systems is much too high a price to pay.instead, shmem, a messagepassing library developed for the cray t3eand related systems, is employed.defensea mitre corporation survey documented in june 200117 listed 10 dodapplications for supercomputing, which are still valid today:¥weather and ocean forecasting.¥planning for dispersion of airborne/waterborne contaminants.¥engineering design of aircraft, ships, and other structures.¥weapon (warhead/penetrators) effect studies and improved armordesign.¥cryptanalysis.¥survivability/stealthiness.¥operational intelligence, surveillance, and reconnaissance (isr).¥signal and image processing research to develop new exploitation.¥national missile defense.¥test and evaluation.many of these defense applications require computational fluid dynamics (cfd), computational structural mechanics (csm), and computational electromagnetics (cem) calculations similar to those needed by16tarek a. elghazawi, william w. carlson, and jesse m. draper, òupc language specification (v 1.1.1),ó <http://www.gwu.edu/~upc/docs/upcspec1.1.1.pdf>; robert w.numrich and john reid, 1998, òcoarray fortran for parallel programming,ó sigplan fortran forum 17(2):131; j. nieplocha, r.j. harrison, and r.j. littlefield, 1996, òglobal arrays: anonuniform memory access programming model for highperformance computers,ó journal of supercomputing 10:197220; katherine yelick, luigi semenzato, geoff pike, carletonmiyamoto, ben liblit, arvind krishnamurthy, paul hilfinger, susan graham, david gay,philip colella, and alexander aiken, 1998, òtitanium: a highperformance java dialect,óconcurrency: practice and experience 10:825836.17richard games. 2001. survey and analysis of the national security high performance computing architectural requirements. mitre corp. june 4.getting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.the demand for supercomputing79other supercomputing applications areas discussed in this report. onedefense application that relies critically on supercomputing is comprehensive aerospace vehicle design, such as the design of the f35 joint strikefighter, and this reliance will only accelerate. future aerospace development programs will involve hypersonic capabilities requiring more comprehensive physics models for accurate simulation in these harsh flightregimes. two distinct types of computational science are required. cfd isused in the engineering design of complex flow configurations, includingexternal airflow, and for predicting the interactions of chemistry with fluidflow for combustion and propulsion. cem is used to compute electromagnetic signatures of tactical ground, air, sea, and space vehicles. currently, we have the capability to model the external airflow, propulsionperformance, vehicle signature, and materials properties in vehicle design with reasonable predictive accuracy on current systems, providedthat these aspects are computed independently. but what is desired is theability to combine these independent modeling efforts into an interactivemodeling capability that would account for the interplay among modelcomponents. for example, engineers could quickly see the effect of proposed changes in the propulsion design on the vehicleõs radar and infrared signature. exceptional supercomputing performance and exceptionalprogrammability are jointly required to enable a finegrained, fullairframe combined cfd and cem simulation of a vehicle like the joint strikefighter.18climate modelingcomprehensive threedimensional modeling of the climate has always required supercomputers.19 to understand the role of supercomputing in climate modeling, it is important to first describe the composition of a climate model. presentday climate models are made up of severalmajor components of the climate system. in a sense they are now reallyearth system models designed to deal with the issue of global change.the standard components are an atmosphere model, an ocean model, acombined landvegetationriver transport (hydrological) model (which issometimes a part of the atmospheric model), and a sea ice model. some of18highend crusader. 2004. òhec analysis: the highend computing productivity crisis.ó hpc wire 13(15).19this subsection is based on white papers by warren m. washington, ncar, òcomputer architectures and climate modeling,ó and by richard d. loft, ncar, òsupercomputing challenges for geoscience applications,ó both prepared for the committeeõs applications workshop in santa fe, n.m., in september 2003.getting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.80getting up to speedthe climate models have embedded chemical cycles such as carbon, sulfate, methane, and nitrogen cycles, which are treated as additional aspectsof the major components. indeed, climate modeling is similar to astrophysics and plasma physics in that it is a multiscale and multiphysicaldiscipline. although all relevant processes ultimately interact at the10,000km scale of the planet, the most important and leastparameterizable influence on climate change is the response of cloud systems; clouds are best treated by embedding explicit submodels with gridsizes down to 1 km into a coarser climate grid. similarly, the most important aspect of the oceanic part of climate change deals with changes in thegulf stream and the associated thermohaline overturning in the northatlantic, where horizontal grid spacing in the hydrodynamics is requiredto be only a few kilometers in order to resolve the fundamental lengthscales. southern ocean processes, which involve both the seaice cover asit affects marine biological productivity and the stability of the antarcticice cap as it affects global sea level, also occur mainly at this small spacescale. land component models should represent the biological propertiesof multiple types of vegetation and soil at a resolution of 1 km, and models of the global carbon cycle must represent the complex chemical andbiological reactions and processes in the free atmosphere, the land surface, and the fulldepth ocean. even then, some processes must be prescribed separately on the basis of laboratory and process studies into suchphenomena as cloud microphysics, smallscale ocean mixing, chemicalreactions, and biological interactions.even with the highest performing supercomputers available today,climate simulations of 100 to 1,000 years require thousands of computational hours. climate modeling requires multithousandyear simulationsto produce equilibrium climate and its signals of natural variability, multihundredyear simulations to evaluate climate change beyond equilibrium(including possible abrupt climatic change), many tens of runs to determine the envelope of possible climate changes for a given emission scenario, and a multitude of scenarios for future emissions of greenhousegases and human responses to climate change. however, these extendedsimulations require explicit integration of the nonlinear equations usingtime steps of only seconds to minutes in order to treat important phenomena such as internal waves and convection.during each time step of a climate model, there is a sizeable amountof floatingpoint calculation, as well as a large amount of internal communication within the machine. much of the spatial communication derivesinherently from the continuum formulations of atmospheric and oceanicdynamics, but additional communication may arise from numerical formulations such as atmospheric spectral treatment or oceanic implicit freesurface treatments. because of the turbulent nature of the underlying flugetting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.the demand for supercomputing81ids, large volumes of model output must be analyzed to understand theunderlying dynamics; this requires large external storage devices and efficient means of communicating with them.as already indicated, an important aspect of the climate model is thegrid resolution, both vertically and horizontally. in particular, the presence of moisture leads to a new class of smallscale fluid motionsñnamely, moist convectionñwhich requires very high horizontal and vertical resolution (on the order of a kilometer) to resolve numerically. toresolve moist convection, the governing equations must includenonhydrostatic effects. this set of governing equations is considerablymore difficult to solve than the hydrostatic primitive equations traditionally used in lower resolution atmospheric models. while direct numericalsimulation at a global 1km grid scale remains impractical for the foreseeable future, even socalled super parameterizations that attempt to realistically capture the subgridscale properties of the underlying moist dynamics are dramatically more computationally expensive than currentphysics packages in operational models.resolution increases in hurricane modeling made possible by supercomputing upgrades since 1998 have improved the ability to forecast hurricane tracks, cutting the track error in half and providing advance information to reduce loss of life and property in threatened areas.20 resolutionincreases will improve predictions of climate models, including the statistics of severe events in the face of climatic change.all of the above considerations point to a massive need for increasedcomputational resources, since current climate models typically have gridsizes of hundreds of kilometers, have few components and oversimplified parameterizations, have rarely reached equilibrium, and have rarelysimulated future climate changes beyond a century. moreover, they areseldom run in ensembles or for multipleemission scenarios. today, theclimate modeler must make compromises in resolution in order to perform a realistic set of simulations. as advances in technology increase thespeed of the supercomputers, history shows that the model complexitygrows correspondingly, bringing both improved treatment of physicalprocesses (such as clouds, precipitation, convection, and boundary layerfluxes) and the need for finer grid resolution.recently, climate models running on the most advanced u.s. supercomputers have approached grid sizes of 100 km. in particular, simulations with the community climate system model (ccsm),21 running20cnn. 2004. òsupercomputers race to predict storms.ó september 16.21see <http://www.ccsm.ucar.edu/> for more information.getting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.82getting up to speedmainly at the national center for atmospheric research (ncar) in support of the important fourth assessment of the intergovernmental panelon climate change (ipcc),22 have used a 100km ocean grid and an atmospheric grid of about 140 km. very computeintensive oceanonlysimulations have been carried out at 10 km for simulated periods of onlya few decades at several doe and dod sites and for longer periods on theearth simulator, and the results show a striking increase in the realism ofstrong currents like the gulf stream. also, the initialization of the oceancomponent of climate models using fourdimensional data assimilationhas used enormous amounts of supercomputer time at ncar and thesan diego supercomputer center while still being carried out at relativelycoarse resolution. notwithstanding the implied need for high internal bandwidth andeffective communication with external storage, the requirement for sustained computational speed can be taken as a measure of computing needsfor climate modeling. a 100 to a 1,000fold increase in compute powerover the next 5 to 10 years would be used very effectively to improveclimate modeling. for example, the embedding of submodels of cloudsystems within climate model grids removes much of the uncertainty inthe potential climatic response to increasing greenhouse gases but increases the computing time by a factor of 80. ocean components of climate models should be run a few thousand years at the desired 10kmresolution to test their ability to simulate longterm equilibrium conditions from first principles. additional aspects of atmospheric chemistryand oceanic chemistry and biology are needed to move toward a propertreatment of the global carbon cycle and its vulnerability to greenhousegases and industrial pollutants.continuing progress in climate prediction can come from further increases in computing power beyond a factor of 1,000. one detailed studyof computational increases needed for various facets of climate modelinghas shown the need for an ultimate overall increase in computer power ofat least a billionfold.23 (such a large increase could also be used for complex systems in plasma physics and astrophysics.) the breakdown of ultimate needs for increased computing power in climate modeling is as follows:22more information is available at <http://www.ipcc.ch/about/about.htm>.23robert malone, john drake, philip jones, and douglas rotman. in press. òhighendcomputing in climate modeling.ó a sciencebased case for largescale simulation.d. keyes, ed. philadephia, pa.: siam press.getting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.the demand for supercomputing83¥increase the spatial resolution of the grids of the coupled modelcomponents. the resolution targets are about 10 km in both the atmosphere and ocean, but for different reasons. it has been demonstrated that10km resolution is needed to resolve oceanic mesoscale eddies. a similarresolution is needed in the atmospheric component to obtain predictionsof surface temperature and precipitation in sufficient detail to analyze theregional and local implications of climate change. this increases the totalamount of computation by a factor of 1,000.¥increase the completeness of the coupled model by adding to eachcomponent model important interactive physical, chemical, and biological processes that heretofore have been omitted owing to their computational complexity. inclusion of atmospheric chemistry, both troposphericand stratospheric, and biogeochemistry in the ocean are essential for understanding the ecological implications of climate change. this increasescomputation by a factor of 100.¥increase the fidelity of the model by replacing parameterizations ofsubgrid physical processes by more realistic and accurate treatments asour understanding of the underlying physical processes improves, oftenas the result of observational field programs. this increases computationby a factor of 100.¥increase the length of both control runs and climatechangescenario runs. longer control runs will reveal any tendency for the coupledmodel to drift and will also improve estimates of model variability. longerclimatechangescenario runs will permit examination of critical issuessuch as the potential collapse of the global thermohaline circulation thatmay occur on time scales of centuries in global warming scenarios. computation increases by a factor of 10.¥increase the number of simulations in each ensemble of controlruns or climatechangescenario runs. increase the number of climatechange scenarios investigated. these issues are both examples of perfectlyparallel extensions of presentday simulations: each instance of anotherscenario or ensemble member is completely independent of every otherinstance. ensemble members are distinguished by small perturbations intheir initial conditions, which are quickly amplified by the nonlinearity ofthe equations. the use of ensembles provides an important measure ofthe range of variability of the climate system. computation increases by afactor of 10.2424ibid.getting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.84getting up to speedplasma physicsa major goal of plasma physics research is to produce costeffective,clean, safe electric power from nuclear fusion.25 very large simulations ofthe reactions in advance of building the generating devices can save billions of equipment dollars. plasmas comprise over 99 percent of the visible universe and are rich in complex, collective phenomena. fusion energy, the power source of the sun and other stars, occurs when forms ofthe lightest atom, hydrogen, combine to make helium in a very hot (~100million degrees centigrade) ionized gas, or òplasma.ó the development ofa secure and reliable energy system that is environmentally and economically sustainable is a truly formidable scientific and technological challenge facing the world in the 21st century. this demands basic scientificunderstanding that can enable the innovations to make fusion energypractical. fusion energy science is a computational grand challenge because, in addition to dealing with space and time scales that can spanmore than 10 orders of magnitude, the fusionrelevant problem involvesextreme anisotropy; the interaction between largescale fluidlike (macroscopic) physics and finescale kinetic (microscopic) physics; and the needto account for geometric detail. moreover, the requirement of causality(inability to parallelize over time) makes this problem among the mostchallenging in computational physics.supercomputing resources can clearly accelerate scientific researchcritical to progress in plasma science in general and to fusion research inparticular. such capabilities are needed to enable scientific understandingand to costeffectively augment experimentation by allowing efficient design and interpretation of expensive new experimental devices (in themultibilliondollar range). in entering the exciting new physics parameter regimes required to study burning fusion plasmas, the associatedchallenges include higher spatial resolution, dimensionless parameterscharacteristic of higher temperature plasmas, longer simulation times, andhigher model dimensionality. it will also be necessary to begin integrating these models together to treat nonlinear interactions of different phenomena. various estimates indicate that increases in combined computational power by factors of 1,000 to 100,000 are needed. associatedchallenges include advancing computer technology, developing algorithms, and improving theoretical formulationñall of which will contribute to better overall timetosolution capabilities.25this subsection is based on excerpts from the white paper òplasma science,ó preparedby w.m. tang, princeton university, for the committeeõs santa fe, n.m., applications workshop, september 2003.getting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.the demand for supercomputing85transportationhighperformance computing contributes to many aspects of transportation product engineering. it provides many benefits, such as reducedtime to market, reduced requirements for physical prototypes, the abilityto explore a larger design space, and a deeper understanding of vehiclebehavior. the main problems addressed by highperformance computinginclude occupant safety (crash), noise, vibration, and harshness (nvh),durability, airflow, and heat transfer. these problems vary in time to solution from a few hours to days to weeks. the general goal is to achieveovernight turnaround times for all types of problems, which trades off thecomplexity of the models being run with the ability of engineers to utilizethe results. the models need to have sufficient detail to provide a highdegree of confidence in the accuracy of the results. todayõs machines arenot fast enough to compensate for the scaling limitations of many of theseproblems.26transportation manufacturers drastically reduce their developmentexpenses and time to market by replacing physical models and car crasheswith virtual tests run on supercomputers. according to bob kruse, gmõsexecutive director for vehicle integration,27 supercomputing will enablehis company to shorten its product development cycle from the 48 monthsof a few years ago to 15 months. the company is performing fewer roundsof vehicle prototyping, which has reduced engineering costs by 40 percent. kruse went on to say that gm has eliminated 85 percent of its realworld crash tests since moving to modeling crashes on its supercomputer.in theory, the company could do away with its $500,000 crash tests, butthe national highway traffic safety administration still requires finalrealworld crash testing.there is a long history of using highperformance computing in theautomotive industry (see box 4.1). automotive computeraided engineering (cae) may well be the largest privatesector marketplace for suchsystems. in the 1980s and early 1990s, automotive companies worldwidedeployed the same cray vector supercomputers favored by governmentlaboratories and other mission agencies. this changed in the late 1990s,when governmentfunded scientists and engineers began migrating todistributed memory systems. the main cae applications used in the automotive industry contain millions of lines of code and have proven very26based on excerpts from the white paper òhigh performance computing in the autoindustry,ó by vincent scarafino, ford motors, prepared for the committeeõs santa fe, n.m.,applications workshop, september 2003.27john gartner. 2004. òsupercomputers speed car design.ó wired news. april 26.getting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.86getting up to speedbox 4.1 automotive companies and their use ofhighperformance computersone of the main commercial users of supercomputing is the automotiveindustry. the largest car manufacturers in the united states, europe, andthe far east all use supercomputing in one form or another for the designand validation cycle. this market segment is called mechanical computeraided engineering (mcae).the use of computing in the automotive industry has come about inresponse to (1) the need to shorten the design cycle and (2) advances intechnology that enable such reduction. one advance is the availability oflargescale highperformance computers. the automotive industry was oneof the first commercial segments to use highperformance computers. theother advance is the availability of thirdparty application software that isoptimized to the architecture of highperformance computers. both advances operate in other industries as well; the existence of thirdparty application software for mcae, electrical cad, chemistry, and geophysicshas increased the market for highperformance computers in many industries.since the use of supercomputing is integrated within the overall vehicledesign, the time to solution must be consistent with the overall design flow.this requirement imposes various time constraints. to be productive, designers need two simulation runs a day (one in the morning and one overnight) or three (one in the morning, one in the afternoon, and one overnight). to meet that need, typical computer runs must complete in 4 to 8hours or, at most, overnight. in many situations, the fidelity of the input ismatched to this requirement. as additional compute power is added, thefidelity of the models is increased and additional design features simulated.demand for computing doubles every year. one measure of demand isthe size of the model. current models process 1 million elements. largermodels are not run now for two reasons: (1) processor capability is notpowerful enough to process more than 1 million elements with manageable timetosolution characteristicsñthat is, the single job takes too longto complete subject to operational requirementsñand (2) the companiesdo not have adequate tools such as visualization to help them understandthe outputs from larger simulations.difficult to port to the distributed memory computational model. as aresult, automotive companies have tended not to purchase capability systems in the last decade. instead they have increased capacity and reducedtheir costs by replacing vector mainframes with shared memory multiprocessor (smp) servers and, more recently, clusters of pcs.getting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.the demand for supercomputing87this is not to say that there is no longer a demand for supercomputersin the automobile industry. in march of 2000, toyota purchased 30vpp5000 vector processors from fujitsu. at the time, this was arguablythe most powerful privately owned system in the world. as independentsoftware vendor (isv) cae codes have matured to the point where they almost all software is supplied by thirdparty, independent softwarevendors (isvs). there are several de facto standard codes that are used,among them the following:¥msc/nastran (structural analysis). nastran generally runs onone processor and is i/o bound. large jobs are run with limitedparallelization on small smp systems (four to eight processors).¥pamcrash/lsdyna/radioss (crash analysis). these codes usemodest degrees of parallelism, ranging from 12 to 100 processors in production automotive calculations today. at that scale, crash codes workwell on clusters. while at least one of these codes has run on 1,024 processors,1 load imbalances limit the effective scaling of these codes for todayõsautomotive calculations.in the past 10 years there has been considerable evolution in the use ofsupercomputing in the automotive industry. ten years ago, cae was usedto simulate a design. the output of the simulation was then compared withthe results from physical tests. simulation modeled only one component ofa vehicleñfor example, its brake systemñand only one òdisciplineó withinthat subsystem (for example, temperature, weight, or noise). there has beena transition to the current ability to do design verificationñthat is, a component is designed by human engineers but the properties of the designcan be checked before the component is built and tested. in some casesmultidisciplinary verification is possible. the longerterm goal is to automate the design of a vehicle, namely, to move from single subsystems to anintegrated model, from single disciplines to multidisciplinary analysis, andfrom verifying a human design to generating the design computationally.design definition will require optimization and firstorder analysis basedon constraints. attaining this objective will reduce design cycle times andincrease the reliability and safety of the overall design.note: the committee is grateful to vince scarafino, ford motor company, for his assistancein developing this box. in addition, the committee thanks the several auto manufacturers whokindly provided anonymous input.1roger w. logan and cynthia k. nitta. 2002. òverification & validation (v&v) methodology and quantitative reliability at confidence (qrc): basis for an investment strategy.ó doepaper ucrlid150874.getting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.88getting up to speedcan effectively exploit hundreds of processors, automotive companieshave responded by purchasing larger systems. gm recently announcedthe purchase of a large ibm system rated at 9 tflops peak, which wouldplace it within the top 20 systems in the june 2004 top500 list.28 frankroney, a managing director at ibm, said gmõs supercomputer would mostlikely be the most powerful computer owned by a private company. inmay 2004, the japan agency for marineearth science and technologyannounced that it would make the earth simulator available to the japanese automobile industry association starting in summer 2004.29 according to a june 2004 report from top500.org, automotive companies, including ford, gm, renault, vw, bmw, opel, and daimler chrysler (threecompanies are anonymous), own 13 of the 500 fastest supercomputers inthe world. the same report indicates that automakers dedicate nearly 50percent of their supercomputing hours to crash test simulations.automotive engineers are continually expanding their computationalrequirements to exploit both available computing power and advances insoftware. finiteelement models of automobiles for crash simulation usemesh spacing of about 5 mm, resulting in problems that have as many as1 million elements. the automotive engineering community would like toreduce the mesh size to 1 mm, resulting in 100 million elements. todayõscrash test models typically include multiple dummies, folded front andside airbags, and fuel in the tanks. deployment of airbags and sloshing offuel are modeled with cfd. engineers in the future will expect cae toolsto automatically explore variations in design parameters in order to optimize their designs. john hallquist of livermore software technologycorporation believes that fully exploiting these advances in automotivecae will require a sevenorderofmagnitude increase beyond the computing power brought to bear today.30 this would allow, among otherthings, much greater attention to occupant safety requirements, includingaspects of offset frontal crash, side impact, outofposition occupants, andmore humanlike crash dummies.while the use of supercomputers has historically been the most aggressive in the automotive industry, supercomputing facilitates engineer28ibid.29summary translation of an article from nihn keizai newspaper, may 24, 2004, providedby the nsf tokyo regional office.30based on excerpts from the white paper òsupercomputing and mechanical engineering,ó by c. ashcraft, r. grimes, j. hallquist, and b. maker, livermore software technologycorporation, prepared for the committeeõs santa fe, n.m., applications workshop, september 2003.getting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.the demand for supercomputing89ing in many other aspects of transportation. according to ray orbach, thedoe office of scienceõs research accomplishments in transportation simulation have received accolades from corporations such as ge and gm.31when the office of science met with the vice presidents for research ofthese and other member companies of the industrial research institute, itlearned, for example, that ge is using simulation very effectively to detectflaws in jet engines. if the engine flaws identified by simulation were togo undetected, the life cycle of those ge engines would be reduced by afactor of 2, causing ge a loss of over $100,000,000. for example, the evaluation of a design alternative to optimize a compressor for a jet enginedesign at ge would require 3.1 × 1018 floatingpoint operations, or over amonth at a sustained speed of 1 tflops, which is near todayõs state of theart in supercomputing. to do this for the entire jet engine would requiresustained computing power of 50 tflops for the same period. this is to becompared with many millions of dollars, several years, and many designsand redesigns for physical prototyping.32in summary, transportation companies currently save hundreds ofmillions of dollars using supercomputing in their new vehicle design anddevelopment processes. supercomputers are used for vehicle crash simulation, safety models, aerodynamics, thermal and combustion analyses,and new materials research. however, the growing need for higher safetystandards, greater fuel efficiency, and lighter but stronger materials demands dramatic increases in supercomputing capability that will not bemet by existing architectures and technologies. some of these problemsare relatively well understood and would yield to more powerful computing systems. other problems, such as combustion modeling inside pistons, are still open research challenges. nevertheless, a supercomputingcapability that delivered even 100 tflops to these applications would savebillions of dollars in product design and development costs in the commercial transportation sector.33bioinformatics and computational biologythe past two decades have witnessed the emergence of computationand information technology as arguably the most important disciplines31testimony of raymond l. orbach, director, office of science, u.s. department of energy, before the u.s. house of representatives committee on science, july 16, 2003.32ibid.33ibid.getting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.90getting up to speedfor future developments in biology and biomedicine.34 the explanationof biological processes in terms of their underlying chemical reactions isone of the great triumphs of modern science and underlies much of contemporary medicine, agriculture, and environmental science. an excitingconsequence of this biochemical knowledge is that computational modeling methods developed to study fundamental chemical processes cannow, at least in principle, be applied to biology. many profound biological questions, such as how enzymes exhibit both exquisite selectivity andimmense catalytic efficiency, are amenable to study by simulation. suchsimulations could ultimately have two goals: (1) to act as a strong validation that all relevant features of a biochemical mechanism have been identified and understood and (2) to provide a powerful tool for probing orreengineering a biochemical process.computation also is essential to molecular biology, which seeks tounderstand how cells and systems of cells function in order to improvehuman health, longevity, and the treatment of diseases. the sheer complexity of molecular systems, in terms of both the number of moleculesand the types of molecules, demands computation to simulate and codifythe logical structure of these systems. there has been a paradigm shift inthe nature of computing in biology with the decoding of the human genome and with the technologies this achievement enabled. equationsofphysicsbased computation is now complemented by massivedatadrivencomputations, combined with heuristic biological knowledge. in additionto deployment of statistical methods for data processing, myriad datamining and pattern recognition algorithms are being developed and employed. finding multiple alignments of the sequences of hundreds ofbacterial genomes is a computational problem that can be attempted onlywith a new suite of efficient alignment algorithms on a petaflopssupercomputer. largescale gene identification, annotation, and clustering expressed sequence tags are other largescale computational problemsin genomics.in essence, computation in biology will provide the framework forunderstanding the flow of information in living systems. some of thegrand challenges posed by this paradigm are outlined below, along withthe associated computational complexity:34this subsection is based in part on excerpts from the white papers òquantum mechanical simulations of biochemical processes,ó by michael colvin, llnl, and òsupercomputingin computational molecular biology,ó by gene myers, uc berkeley, both prepared for thecommitteeõs santa fe, n.m., applications workshop, september 2003.getting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.the demand for supercomputing91¥deciphering the genome continues to be a challenging computational problem. one of the largest hewlettpackard cluster systems everdeveloped was used to assemble the human genome. in the annotation ofthe genome (assigning functional roles), the computation can be extraordinarily complex. multiple genome comparisons, which are practicallyimpossible with current computers, are essential and will constitute a significant challenge in computational biomedicine for the future.¥there are typically a few hundred cell types in a mammal, andeach type of cell has its own repertoire of active genes and gene products.our understanding of human diseases relies heavily on figuring out theintracellular components and the machinery formed by the components.the advent of dna microarrays has provided us with a unique ability torapidly map the gene expression profiles in cells experimentally. whileanalysis of a single array is not a supercomputing problem, the collectiveanalysis of a large number of arrays across time or across treatment conditions explodes into a significant computational task.¥genes translate into proteins, the workhorses of the cell. mechanistic understanding of the biochemistry of the cell involves intimate knowledge of the structure of these proteins and details of their function. thenumber of genes from various species is in the millions, and experimentalmethods have no hope of resolving the structures of the encoded proteins. computational modeling and prediction of protein structures remain the only hope. this problem, called the proteinfolding problem, isregarded as the holy grail of biochemistry. even when knowledgebasedconstraints are employed, this problem remains computationally intractable with modern computers.¥computer simulations remain as the only approach to understanding the dynamics of macromolecules and their assemblies. early simulations were restricted to small macromolecules. in the past three decades,our ability to compute has helped us to understand large macromolecularassemblies like membranes for up to tens of nanoseconds. these simulations that scale as n2 are still far from capable of calculating motions ofhundreds of thousands of atoms for biologically measurable time scales.¥understanding the characteristics of protein interaction networksand proteincomplex networks formed by all the proteins of an organismis another large computational problem. these networks are smallworldnetworks, where the average distance between two vertices in the network is small relative to the number of vertices. smallworld networksalso arise in electric power networks and semantic networks for intelligence analysis and in models of the web; understanding the nature ofthese networks, many with billions of vertices and trillions of edges, iscritical to making them invulnerable to attacks. simulations of smallworld networks fall into three categories: topological, constraintdriven,getting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.92getting up to speedand dynamic. each of these categories involves complex combinatorial,graph theoretic, and differential equation solver algorithms and challenges any supercomputer. current algorithmic and computational capabilities will not be able to address computational needs for even the smallest microorganism, haemophilus influenza. there is an imminent needfor the development of novel methods and computing technology.¥the achievement of goals such as a cure for cancer and the prevention of heart diseases and neurovascular disorders continue to drive biomedicine. the problems involved were traditionally regarded asnoncomputational or minimally computational problems. however, withtodayõs knowledge of the genome and intracellular circuitry, we are in aposition to carry out precise and targeted discovery of drugs that, whilecuring the pathology, will only minimally perturb normal function. thisis rapidly emerging as a serious computational task and will become thepreeminent challenge of biomedicine.¥much of our knowledge of living systems comes from comparativeanalysis of living species. phylogenetics, the reconstruction of historicalrelationships between species or individuals, is now intensely computational, involving string and graph algorithms. in addition to being an intellectual challenge, this problem has a significant practical bearing onbioterrorism. computation is the fastest and currently the only approachto rapidly profiling and isolating dangerous microorganisms.in conclusion, we are at the threshold of a capability to perform predictive simulations of biochemical processes that will transform our ability to understand the chemical basis of biological functions. in addition toits value to basic biological research, this will greatly improve our abilityto design new therapeutic drugs, treat diseases, and understand themechanisms of genetic disorders. societal health and safetycomputational simulation is a critical tool of scientific investigationand engineering design in many areas related to societal health and safety,including aerodynamics; geophysics; structures; manufacturing processeswith phase change; and energy conversion processes. insofar as thesemechanical systems can be described by conservation laws expressed aspartial differential equations, they may be amenable to analysis usingsupercomputers. trillions of dollars of economic output annually and thehealth and safety of billions of people rest on our ability to simulate suchsystems.incremental improvements in the accuracy and reliability of simulations are important because of huge multipliers. a very small (perhapsgetting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.the demand for supercomputing93even 1 percent) improvement in the efficiency of heat exchangers or gasturbines could have a significant impact on the global environment andeconomy when aggregated over the lifetime of many such devices.35the problem of monitoring the quality of air, water, and other utilitynetworks has gained prominence in the wake of terrorist events liketokyoõs subway incident and londonõs poison gas bomb plot. one example of a computational problem of this type is optimizing the placement of sensors in municipal water networks to detect contaminants injected maliciously. traditionally, this type of problem was studied usingnumerical simulation tools to see how a water supply network is impactedby the introduction of contaminant at a given point. recently, combinatorial optimization formulations have been proposed to compute optimalsensor locations. optimal sensor placement is desirable to ensure adequatecoverage of the networkõs flow for detection and remediation of contaminants. the objective of one model is to minimize the expected fraction ofthe population that is at risk for an attack. an attack is modeled as therelease of a large volume of harmful contaminant at a single point in thenetwork with a single injection. for any particular attack, assume that allpoints downstream of the release point can be contaminated. in general,one does not know a priori where this attack will occur, so the objective isto place sensors to provide a compromise solution across all possible attack locations. depending on the size of the water network, the amount ofcomputation needed can be extremely large and can certainly requiresupercomputing performance for timely results, especially in an emergency.36earthquakesan important application in geophysical exploration is earthquakemodeling and earthquake risk mitigation. when an earthquake occurs,some areas the size of city blocks are shaken, while other areas are stableand not shaken. this effect is caused by the focusing or deflection of seismic waves by underground rock structures. if the underground rock struc35based on excerpts from the white paper òsupercomputing for pdebased simulationsin mechanics,ó by david keyes, columbia university, prepared for the committeeõs santafe, n.m., applications workshop, september 2003.36based on excerpts from the white paper òsupercomputing and discrete algorithms: asymbiotic relationship,ó by william hart, bruce hendrickson, and cindy phillips, sandianational laboratories, prepared for the committeeõs santa fe, n.m., applications workshop,september 2003.getting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.94getting up to speedture of an area in an earthquakeprone region could be simulated or imaged, damage mitigation strategies could include identifying dangerousareas and avoiding building on them and simulating many typical earthquakes, noting which areas are shaken and identifying dangerous areas.using forward simulation, one can match seismic simulation resultswith observed seismographic data. then an image of the undergroundrock in a region can be deduced by repeatedly simulating the error fromforward simulation by adjoint methods.37current earthquake simulation codes running at the california institute of technology and the pittsburgh supercomputer center use frequencies up to 1 hz, which equates to a resolution of several miles of rock.seismographs can collect data up to 20 hz or more, which yields a resolution of hundreds of feet of rock. this is a useful resolution for risk mitigation, since buildings are hundreds of feet in size. however, the computingpower needed to process such data is on the order of 1 exaflops, or 1,000pflops (25,000 times the power of the earth simulator). for useful earthquake risk mitigation, the algorithms exist, the codes are written and debugged, and the input data exist. the consequence of not proceeding iscontinued loss of life and extensive property damage in earthquakeproneregions of the world.38geophysical exploration and geosciencethe simulation of petroleum reservoirs is a large consumer ofsupercomputing resources in this application area.39 all of the major oilcompanies simulate petroleum reservoirs to predict future oil and gas production from the subsurface of earth, where porous sandstone or limestone formations may hold oil and gas. predictions are made using differential equations that represent flow in porous media in three dimensions.in addition to the simple case of flow of oil, water, and gas in the reservoirs, it is often necessary to include the phase behavior of multicomponent hydrocarbon fluids for enhancedrecovery processes and/or thermal effects for steam injection or in situ combustion recovery techniques.37erik p. debenedictus. 2004. òcompleting the journey of mooreõs law,ó presentation atthe university of illinois, may 5.38ibid.39this subsection is based on excerpts from the white paper òhigh performance computing and petroleum reservoir simulation,ó by john killough, landmark graphics corporation, prepared for the committeeõs santa fe, n.m., applications workshop, september 2003.getting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.the demand for supercomputing95the overall goal of the simulations is to maximize hydrocarbon liquidand gas recovery and net present value.the motivation for using supercomputing in reservoir simulation hasalways existed. from the earliest simulation models, computing resources have been severely taxed simply because the level of complexitydesired by the engineer almost always exceeded the speed and memoryof the hardware. the highspeed vector processors of the late 1970s andearly 1980s led to orders of magnitude improvement in the speed of computation and led to production models of several hundred thousand cells.the relief brought by these models was short lived. the desire for increased physics of compositional modeling and the introduction ofgeostatistically/structurally based geological models led to increases incomputational complexity even beyond the largescale models of the vector processors. tens of millions of cells with complete reservoir parameters now became available for use by the engineer. although upscalingor lumping provided a tool to dramatically reduce model sizes, the inherent assumptions of the upscaling techniques left the engineer with astrong desire to incorporate all of the available data in studies.scientific studies of earthõs interior are heavily dependent on supercomputer power. two examples are illustrative. one is the geodynamoñi.e., an understanding of how earthõs magnetic field is generated by complicated magnetohydrodynamic convection and turbulence in its outercore, a longstanding grand challenge in fluid dynamics. supercomputersimulations have enabled major breakthroughs in the last decade, including the first selfconsistent dynamo solution and the first simulated magnetic reversal, both of which occurred in 1995. however, these simulateddynamos are still many orders of magnitude away from the òcorrectó parameter range. the second example comes from the need to understandthe dynamics of earthõs plate tectonics and mantle convection, whichdrives continental drift, mountain building, etc. to do this simulationproperly requires incorporating the correct multirheological behavior ofrocks (elastic, brittle, viscous, plastic, historydependent, and so forth),which results in a wide range of length scales and time scales, into a threedimensional, spherical model of the entire earth, another grand challengethat will require substantially more computing power to address.4040for more information, see <http://sdcd.gsfc.nasa.gov/ess/olson.finalreport/finalreport.html>. a more general article is p.j. tackley, j.r. baumgardner, g.a. glatzmaier, p.olson, and t. clune, 1999, òthreedimensional spherical simulations of convection inearthõs mantle and core using massivelyparallel computers,ó advanced simulations technologies conference, san diego, pp. 95100.getting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.96getting up to speedastrophysicsobservation has always been fundamental to astronomy, but controlled experiments are extremely rare.41 thus, astronomical computersimulations have assumed the traditional scientific role of controlled experiments by making it possible to test scenarios when the underlyingphysical laws are known. observations still provide a check, but theyshow the results of processes that cannot be controlled in a laboratory.furthermore, the evolutionary time scales for most astronomical systemsare so long that these systems seem frozen in time. constructing evolutionary models purely from observation is therefore difficult. by observing many different systems of the same type (e.g., stars or galaxies), wecan see many different stages of development and attempt to put theminto a logical order, but we cannot watch a single system evolve. asupercomputer simulation is usually required to provide the evolutionary model that ties the different observed stages together using knownphysical laws and properties of matter.stellar evolution theory provides an excellent example of why astrophysicists have been forced to rely on computer simulation. although onecan perform laboratory experiments to determine the properties of thegaseous constituents in a star like the sun, one cannot build an experimental star in the laboratory and watch it evolve. that must be done bycomputer simulation. although one can make some simple argumentsand estimates without using a computer, the physics involved in stellarevolution theory is complex and nonlinear, so one does not get very far indeveloping the theory without a computer.supercomputing power can be used to literally add a spatial dimension, turning a twodimensional simulation of a supernova explosion intothreedimensional simulation, or it can be used to add treatments of newand important phenomena into a simulation. for example, magnetic fieldscould be added to global simulations of solar convection to address theoperation of the dynamo that drives the sunspot cycle. for some problems, such as the development of largescale structure in the expandinguniverse, simply getting more of the system under study into the computational problem domain by dramatically increasing the size of the computational grid should have a significant impact on scientific discovery.alternatively, one might choose to simulate the same size system, usingsupercomputing power to treat structures on a much wider range of41this subsection is based on excerpts from the white paper òfuture supercomputingneeds and opportunities in astrophysics,ó by paul woodward, university of minnesota,prepared for the committeeõs santa fe, n.m., applications workshop, september 2003.getting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.the demand for supercomputing97length and time scales. an excellent example is the cosmological problem,since it contains scales of interest ranging from that of a single star to thatof a large cluster of galaxies.physicists trying to determine whether our universe will continue toexpand or eventually collapse have gathered data from dozens of distantsupernovae. by analyzing the data and simulating another 10,000 supernovae on supercomputers at nersc, they have concluded that the universe is expandingñand at an accelerating rate.42materials science and computational nanotechnologythe emerging fields of computational materials science examine thefundamental behavior of matter at atomic to nanometer length scales andpicosecond to millisecond time scales in order to discover novel properties of bulk matter for numerous important practical uses.predictive equations take the form of first principles electronic structure molecular dynamics (fpmd) and quantum monte carlo (qmc) techniques for the simulation of nanomaterials. the qmc methods are highlyparallel across multiple processors but require high bandwidth to localmemory, whereas the fpmd methods are demanding of both local andglobal bandwidth. the computational requirements of a materials scienceproblem grow typically as the cube of the number of atoms in any simulation even when the newest and best computational algorithms are usedñmaking the area an almost unlimited consumer of future increases in computer power. the most beneficial simulations in terms of practicalapplications require large numbers of atoms and long time scalesñfarmore than presently possible in both of those aspects. for example, fpmdsimulations are currently limited to a few hundred atoms for a few picoseconds. the promise of revolutionary materials and processes from materials science will routinely require several petaflops of computer powerin the not too distant future.as the committee on the future of supercomputing heard in numerous presentations during its site visits, computational materials science isnow poised to explore a number of areas of practical importance. algorithms are well tested that will exploit 100 to 1,000 times the computingpower available today. materials scientists in a number of universities aswell as in doe laboratories are already targeting the largest future con42testimony of raymond l. orbach, director, office of science, u.s. department of energy, before the u.s. house of representatives committee on science, july 16, 2003.getting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.98getting up to speedfigurations of cray x1 and ibm blue gene/l in order to advance theirapplications.the promise of new materials and processes covers a wide variety ofeconomically important areas. among the most important are these:¥better electronic equipment. materials with superconducting properties are most useful when they can function at temperatures well aboveabsolute zero. the negligible power loss of superconductors makes themideal for constructing a range of devices from mri machines to microprocessors, when cooling can be provided by relatively inexpensive liquidnitrogen (as opposed to more expensive liquid helium systems). a computational search is well under way for superconductors with higher critical temperatures than substances already found in the laboratory.¥improved power transmission. it is possible that computational methods will discover synthetic materials with much better conducting properties at room temperatures than those presently available. the possibility of nearly lossfree power transmission has major economicimplications. even supercomputing itself would benefit greatly.¥highdensity data storage. some supercomputing applications willrequire magnetic storage densities of terabits per square inch in the relatively near future. the information will need to be stored in nanometerscale particles or grains. a detailed understanding of the magnetism innanometer particles will have to come from computational studies thatwill be validated with selected experiments. this is a new way to approachthe science involving magnetic storage and constitutes a major opportunity for petaflopsscale computing.43¥photoelectric devices. in selectivelightabsorbing materials for solarenergy, for photothermal energy conversion, or for optical sensors, theactive semiconductor particles will contain millions of atoms to ensuresharp enough lines. with clever techniques exploiting special features toreduce the computational burden, the optical properties of such particlescan be accurately evaluated, and even charging effects from electron excitations can be accounted for. such calculations can now be performedonly by using very large allocations of time on the most powerful computers available in the united states. to be useful for designing new structures and devices, such simulations need to be run almost routinely forconfigurations that do not have the special features currently being exploited.4443thomas schulthess. 2004. òabinitio monte carlo for nanomagnetism.ó ornl whitepaper.44òaccelerating the revolution in computational materials science,ó 2002, <http://www.ultrasim.info/doedocs/accmatsci.pdf>.getting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.the demand for supercomputing99¥electric motors. scientists have recently achieved breakthroughquantum mechanical simulations of magnetic moments at high temperatures. such simulations were limited to a few thousand atoms of pureiron. understanding more complex substances is the key to designingmaterials for stronger magnets in order to build more efficient and powerful electrical generators and motors. for simulations to accurately modelthe dynamics of magnetic domains in more complex materials, muchlarger simulation sizes will be required. awardwinning algorithms ofhigh quality exist, so the issue now is having a computing platform capable of sustaining the level of computation necessary to carry out thescience.45¥catalysts. the u.s. chemical, biochemical, and pharmaceutical industries are the worldõs largest producer of chemicals, ranging from wonder drugs to paints to cosmetics to plastics to new, more efficient energysources. a key ingredient in nearly all such industrial processes is a typeof chemical called a catalyst. the true computational design of practicalcatalysts for industrial and commercial applications will require the ability to predict, at the molecular level, the detailed behavior of the large,complex molecules and materials involved in catalytic processes. thislevel of detail is not available from experiments, and it is not feasible oncurrently available computer hardware. for example, to simulate the platinum catalyst in a carõs catalytic converter requires the model to includehundreds to tens of thousands of platinum atoms. a realistic simulationof the actual process in a car engine would take decades on todayõs computer hardware. the design of new catalysts simply cannot wait this longif the u.s. chemical and pharmaceutical industries are to remain competitive. new computational capabilities will revolutionize the chemical industry, turning the art of catalysis creation into the science of catalystdesign.46¥bioengineering. within the biology arena, the use of supercomputerswill enable microscopic modeling of dna repair mechanisms and drug/dna interactions, effectively bringing quantum simulations into therealm of biology. in particular, nearly exact qmc results will representvaluable theoretical benchmarks that may help overcome some of the current limitations of experimental biology.4745ibid.46òcomputational design of catalysts: building the science case for ultrascale simulations,ó 2002, <http://www.ultrasim.info/doedocs/catalysisredux2.pdf>.47f. gygi, g. galli, j.c. grossman, and v. bulatov. 2002. òimpact of earthsimulatorclasscomputers on computational nanoscience and materials science.ó doe ultrascale simulation white paper.getting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.100getting up to speedin summary, computational materials science is emerging as an important factor in providing the designer materials and processes that willunderlie the economic progress of the nation in the coming decades. simulating the complexity of large numbers of atoms and molecules over increasingly long time periods will challenge supercomputers of petaflopspower and beyond.human/organizational systems studiesthe study of macroeconomics and social dynamics is amenable tosimulation and study using supercomputing. in such applications, thebehavior of large human populations is simulated in terms of the overalleffect of decisions by hundreds of millions of individuals. the simulations can model physical or social structures with hundreds of thousands,or maybe even millions, of actors interacting with one another in a complex fashion. supercomputing makes it possible to test different interactor(or interpersonal) relations to see what macroscopic behaviors can ensue.simulations can determine the nature of the fundamental forces or interactions between actors. some logistical examples include airline crewscheduling, inventory management, and package delivery scheduling (thefedex problem).48sociotechnical systems of 106 to 109 agents (people, packets, commodities, and so on) with irregular interactions on time scales of seconds toyears can be simulated using supercomputers at institutions like losalamos national laboratory. however, the customers for such simulations are often organizations such as metropolitan planning offices, whichdo not generally have access to sophisticated supercomputing systemsand therefore are limited to manipulating the amount of data that can behandled by cots technology such as linux clusters. over the comingyears, researchers will expand existing simulations of transportation, electricity distribution and markets, epidemiology, and mobile telecommunications on scales ranging from that of a city the size of portland, oregon(1.6 million people) to national scale. sociotechnical simulations in thefuture will require coupling many large, heterogeneous, irregular simulation systems, which will require advanced supercomputing power to accomplish.4948testimony of raymond l. orbach, director, office of science, u.s. department of energy, before the u.s. house of representatives committee on science, july 16, 2003.49based on excerpts from the white paper òthe future of supercomputing forsociotechnical simulation,ó by stephen eubank, lanl, prepared for the committeeõs santafe, n.m., applications workshop, september 2003.getting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.the demand for supercomputing101projected computing needs for applicationsthe scientific and engineering applications that use supercomputingare diverse both in the nature of the problems and in the nature of thesolutions. most of these applications have unsatisfied computationalneeds. they were described in expert briefings to the committee as computinglimited at present and very much in need of 100 to 1,000 timesmore computing power over the next 5 to 10 years. increased computingpower would be used in a variety of ways:¥to cover larger domains, more space scales, and longer time scales;¥to solve timecritical problems (e.g., national security ones) inshorter times;¥to include more complete physics and/or biogeochemistry;¥to use more sophisticated mathematical algorithms with desirablelinear scaling; and¥to add more components to models of complex systems.various experts made estimates of the longrange computing powerneeded for their disciplines in units of petaflops. most of the applicationsareas discussed would require a minimum sustained performance of 10pflops to begin to solve the most ambitious problems and realize practicalbenefits. to move toward a full solution of these problems would requirecapabilities of 100 pflops and beyond.the overall computing style in important application areas appears tobe evolving toward one in which community models are developed andused by large groups. the individual developers may bring diverse backgrounds and expertise to modeling a complex natural system such as theclimate system or to a daunting engineering effort like the developmentof a fusion power generator. in addition, the applications are moving toward firstprinciples methods, in which basic physical and biochemicalrelations are used as much as possible instead of ad hoc parameterizationsinvolving approximations and poorly known constants. both trends willgreatly increase the amount of computing power required in various applications.a common computational characteristic is the demand for both capacity and capability. typically, each disciplinary area does many smallersimulations and parameter studies using machine capacity prior to largesimulations that require machine capability, followed by analysis studiesthat use capacity. many application areas could each use at least one largecomputing center almost continuously to attack multiple problems in thisway.another computational characteristic is that each application area hasgetting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.102getting up to speeda rather high degree of problem complexity. there may be multiple timeand space scales, different component submodels (e.g., magnetic, hydrodynamic, or biochemical), different types of equations (e.g., nonlinear partial differential equations and ordinary differential equations), and different algorithms (spectral, finitedifference, finiteelement, algebraic)covering a range of problems being studied in each area.it is clear from the summary above that a 1,000fold increase in computing power is needed almost immediately and a 1,000,000fold increasewill ultimately be needed by the current major applications. some of thisincrease can be expected on the basis of mooreõs law and greater numbersof processors per machine. any increase in raw computing power in termsof raw flops will have to be accompanied by larger memories to accommodate larger problems, and internal bandwidth will have to increasedramatically. as problems become more dataoriented, more effectiveparallel i/o to external devices will be needed, which will themselveshave to be larger than todayõs disks and mass storage systems.table 4.1 summarizes six supercomputing system bottlenecks that often limit performance on important applications and gives examples ofthe applications. it should be noted that the limitations/bottlenecks inapplication areas are heavily dependent on the problemsolving strategies and the algorithms used.the ability of applications to be mapped onto hardware effectively iscritically dependent on the software of the overall system, including boththe operating system and the compilers. application programmers andusers will need software that exploits the features of any given machinewithout heroic efforts on the programmerõs part. software ideally shouldtable 4.1 six limitations of supercomputing systemslimitation/bottlenecktypical areas of applicationfloatingpoint performanceastrophysics, defense radar crosssections, climatemodeling, plasma physicsmemory sizeintelligence, materials science, genomics, automobilenoise, vibration, and harshnessmemory bandwidthintelligence, climate modeling, materials science,astrophysics, biological systems modelingmemory latencyintelligence, nuclear simulation, climate modeling,astrophysics, biological systems modelinginterconnect bandwidthintelligence, climate modeling, materials science,astrophysics, biological systems modelinginterconnect latencyintelligence, nuclear simulation, climate modeling,astrophysics, biological systems modelinggetting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.the demand for supercomputing103promote effective parallel processor usage and efficient memory use whilehiding many of the details. ideally, software should allow portability ofwelldesigned application programs between different machine architectures, handle dynamic load balancing, and also have fault tolerance.there is also a need for better ability to deal with locality while maintaining some type of global addressing in a way that can be mapped efficiently by compilers and runtime systems onto diverse hardware architectures. for lack of alternatives, many supercomputing applications arewritten in fortran 90 and c. the use of highperformance fortran (hpf)on the earth simulator is one of only a few examples of using higher levelprogramming languages with better support for parallelism. more versatile, higherlevel languages would need to exploit architectures efficientlyin order to attract a critical mass of followers that would sustain the language and its further development. in regard to memory access beyondan individual processor, most communication between and even withinnodes uses mpi and sometimes openmp, again because of the lack ofother choices. many of the application areas are hampered by the software overheads of existing methods and would benefit significantly frommore efficient tools to maximize parallel utilization with minimal programming effort. chapter 5 discusses the hardware and software issuesfrom a technology perspective.getting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.1045todayõs supercomputing technologythe preceding chapter summarized some of the application areas inwhich supercomputing is important. supercomputers are used toreduce overall time to solutionñthe time between initiating theuse of computing and producing answers. an important aspect of theiruse is the cost of solutionñincluding the (incremental) costs of owningthe computer. usually, the more the time to solution is reduced (e.g., byusing more powerful supercomputers) the more the cost of solution isincreased. solutions have a higher utility if provided earlier: a weatherforecast is much less valuable after the storm starts. the aggressiveness ofthe effort to advance supercomputing technology depends on how muchadded utility and how much added cost come from solving the problemfaster. the utility and cost of a solution may depend on factors other thantime takenñfor instance, on accuracy or trustworthiness. determining thetradeoff among these factors is a critical task. the calculation depends onmany thingsñthe algorithms that are used, the hardware and softwareplatforms, the software that realizes the application and that communicates the results to users, the availability of sufficient computing in atimely fashion, and the available human expertise. the design of the algorithms, the computing platform, and the software environment governsperformance and sometimes the feasibility of getting a solution. the committee discusses these technologies and metrics for evaluating their performance in this chapter. other aspects of time to solution are discussedlater.getting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.todayõs supercomputing technology105supercomputer architecturea supercomputer is composed of processors, memory, i/o system,and an interconnect. the processors fetch and execute program instructions. this execution involves performing arithmetic and logical calculations, initiating memory accesses, and controlling the flow of programexecution. the memory system stores the current state of a computation.a processor or a group of processors (an smp) and a block of memory aretypically packaged together as a node of a computer. a modernsupercomputer has hundreds to tens of thousands of nodes. the interconnect provides communication among the nodes of the computer, enablingthese nodes to collaborate on the solution of a single large problem. theinterconnect also connects the nodes to i/o devices, including disk storage and network interfaces. the i/o system supports the peripheral subsystem, which includes tape, disk, and networking. all of these subsystems are needed to provide the overall system. another aspect ofproviding an overall system is power consumption. contemporarysupercomputer systems, especially those in the top 10 of the top500, consume in excess of 5 megawatts. this necessitates the construction of a newgeneration of supercomputer facilities (e.g., for the japanese earth simulator, the los alamos national laboratory, and the lawrence livermorenational laboratory). nextgeneration petaflops systems must considerpower consumption in the overall design.scaling of technologyas semiconductor and packaging technology improves, different aspects of a supercomputer (or of any computer system) improve at different rates. in particular, the arithmetic performance increases much fasterthan the local and global bandwidth of the system. latency to localmemory or to a remote node is decreasing only very slowly. when expressed in terms of instructions executed in the time it takes to communicate to local memory or to a remote node, this latency is increasing rapidly. this nonuniform scaling of technology poses a number of challengesfor supercomputer architecture, particularly for those applications thatdemand high local or global bandwidth.figure 5.1 shows how floatingpoint performance of commodity microprocessors, as measured by the specfp benchmark suite, has scaledover time.1 the trend line shows that the floatingpoint performance of1material for this figure was provided by mark horowitz (stanford university) and stevenwoo (rambus). most of the data were originally published in microprocessor report.getting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.106getting up to speed0.1110100100010000jan 88jan 90jan 92jan 94jan 96jan 98jan 00jan 02jan 04specfp performance (mflops)figure 5.1processor performance (specfp mflops) vs. calendar year of introduction.microprocessors improved by 59 percent per year over the 16year periodfrom 1988 to 2004. the overall improvement is roughly 1,000fold, fromabout 1 mflops in 1988 to more than 1 gflops in 2004.this trend in processor performance is expected to continue, but at areduced rate. the increase in performance is the product of three factors:circuit speed (picoseconds per gate), pipeline depth (gates per clock cycle),and instructionlevel parallelism (ilp) (clock cycles per instruction). eachof these factors has been improving exponentially over time.2 however,increases in pipeline depth and ilp cannot be expected to be the source offurther performance improvement, leaving circuit speed as the driver ofmuch of future performance increases. manufacturers are expected tocompensate for this drop in the scaling of singleprocessor performanceby placing several processors on a single chip. the aggregate performanceof such chip multiprocessors is expected to scale at least as rapidly as thecurve shown in figure 5.1.figure 5.2 shows that memory bandwidth has been increasing at a2w.j. dally. 2001. the last classical computer. information science and technology (isat)study group, sponsored by the institute for defense analyses and darpa.getting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.todayõs supercomputing technology107much slower rate than processor performance. over the entire period from1982 to 2004, the bandwidth of commodity microprocessor memory systems (often called the frontside bus bandwidth) increased 38 percent peryear. however, since 1995, the rate has slowed to only 23 percent per year.this slowing of memory bandwidth growth is caused by the processorsbecoming limited by the memory bandwidth of the dram chips. thelower line in figure 5.2 shows that the bandwidth of a single commoditydram chip increased 25 percent per year from 1982 to 2004. commodityprocessor memory system bandwidth increased at 38 percent per yearuntil it reached about 16 times the dram chip bandwidth and has beenscaling at approximately the same rate as dram chip bandwidth sincethat point. the figure gives bandwidth in megawords per second, wherea word is 64 bits.we are far from reaching any fundamental limit on the bandwidth ofeither the commodity microprocessor or the commodity dram chip. in2001, chips were fabricated with over 1 tbit/sec of pin bandwidth, over26 times the 38 gbit/sec of bandwidth for a microprocessor of the sameyear. similarly, dram chips also could be manufactured with substantially higher pin bandwidth. (in fact, special gddr drams made forgraphics systems have several times the bandwidth of the commodity1101001000jan 82jan 84jan 86jan 88jan 90jan 92jan 94jan 96jan 98jan 00jan 02bandwidth (mword/sec)frontside bus bwfrontside bus bw (since 1995)dram chip bwexpon. (frontside bus bw)expon. (frontside bus bw (since 1995))expon. (dram chip bw)figure 5.2bandwidth (mword/sec) of commodity microprocessor memory interfaces and dram chips per calendar year.getting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.108getting up to speed110100100010000jan 88jan 90jan 92jan 94jan 96jan 98jan 00jan 02memory bw (mword/sec)mflopsdram chip bw (mword/sec)figure 5.3arithmetic performance (mflops), memory bandwidth, and dramchip bandwidth per calendar year.chips shown here.) the trends seen here reflect not fundamental limitsbut market forces. these bandwidths are set to optimize cost/performancefor the highvolume personal computer and enterprise server markets.building a dram chip with much higher bandwidth is feasible technically but would be prohibitively expensive without a volume market todrive costs down.the divergence of about 30 percent per year between processor performance and memory bandwidth, illustrated in figure 5.3, poses a majorchallenge for computer architects. as processor performance increases,increasing memory bandwidth to maintain a constant ratio would requirea prohibitively expensive number of memory chips. while this approachis taken by some highbandwidth machines, a more common approach isto reduce the demand on memory bandwidth by adding larger, and oftenmultilevel, cache memory systems. this approach works well for applications that exhibit large amounts of spatial and temporal locality. however, it makes application performance extremely sensitive to this locality. applications that are unable to take advantage of the cache will scalein performance at the memory bandwidth rate, not the processor performance rate. as the gap between processor and memory performance continues to grow, more applications that now make good use of a cache willbecome limited by memory bandwidth.the evolution of dram row access latency (total memory latencygetting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.todayõs supercomputing technology109is typically about twice this amount) is shown in figure 5.4. comparedwith processor performance (59 percent per year) or even dram chipbandwidth (25 percent per year), dram latency is improving quiteslowly, decreasing by only 5.5 percent per year. this disparity results in arelative increase in dram latency when expressed in terms of instructions processed while waiting for a dram access or in terms of dramwords accessed while waiting for a dram access.the slow scaling of memory latency results in an increase in memorylatency when measured in floatingpoint operations, as shown in figure5.5. in 1988, a single floatingpoint operation took six times as long as thememory latency. in 2004, by contrast, over 100 floatingpoint operationscan be performed in the time required to access memory.there is also an increase in memory latency when measured inmemory bandwidth, as shown in figure 5.6. this graph plots the frontside bus bandwidth of figure 5.2 multiplied by the memory latency offigure 5.4. the result is the number of memory words (64bit) that mustsimultaneously be in process in the memory system to sustain the frontside bus bandwidth, according to littleõs law.3 figure 5.6 highlights the10100jan 88jan 90jan 92jan 94jan 96jan 98jan 00jan 02time (nsec)dram row access timeexpon. (dram row access time)figure 5.4decrease in memory latency (in nanoseconds) per calendar year.3littleõs law states that the average number of items in a system is the product of theaverage rate of arrival (bandwidth) and the average holding time (latency).getting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.110getting up to speed0.11.010.0100.01000.0jan 88jan 90jan 92jan 94jan 96jan 98jan 00jan 02time (nsec)memory latency (nsec)time per floatingpoint operation (nsec)figure 5.5decrease in dram latency and time per floatingpoint operation percalendar year.figure 5.6increase in the number of simultaneous memory operations in flightneeded to sustain frontside bus bandwidth.110100jan 84jan 86jan 88jan 90jan 92jan 94jan 96jan 98jan 00jan 02memory operations in flightexpon. (memory operations in flight)getting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.todayõs supercomputing technology111need for latency tolerance. to sustain close to peak bandwidth on a modern commodity machine, over 100 64bit words must be in transfer simultaneously. for a custom processor that may have 5 to 10 times the bandwidth of a commodity machine, the number of simultaneous operationsneeded to sustain close to peak bandwidth approaches 1,000.types of supercomputerssupercomputers can be classified by the degree to which they usecustom components that are specialized for highperformance scientificcomputing as opposed to commodity components that are built forhighervolume computing applications. the committee considers threeclassificationsñcommodity, custom, and hybrid:¥a commodity supercomputer is built using offtheshelf processors developed for workstations or commercial servers connected by anofftheshelf network using the i/o interface of the processor. such machines are often referred to as òclustersó because they are constructed byclustering workstations or servers. the big mac machine constructed atvirginia tech is an example of a commodity (cluster) supercomputer.commodity processors are manufactured in high volume and hence benefit from economies of scale. the high volume also justifies sophisticatedengineeringñfor example, the fullcustom circuits used to achieve clockrates of many gigahertz. however, because commodity processors areoptimized for applications with memory access patterns different fromthose found in many scientific applications, they realize a small fractionof their nominal performance on scientific applications. many of thesescientific applications are important for national security. also, the commodity i/oconnected network usually provides poor global bandwidthand high latency (compared with custom solutions). bandwidth and latency issues are discussed in more detail below.¥a custom supercomputer uses processors that have been specialized for scientific computing. the interconnect is also specialized and typically provides high bandwidth via the processormemory interface. thecray x1 and the nec earth simulator (sx6) are examples of customsupercomputers. custom supercomputers typically provide much higherbandwidth both to a processorõs local memory (on the same node) andbetween nodes than do commodity machines. to prevent latency fromidling this bandwidth, such processors almost always employ latencyhiding mechanisms. because they are manufactured in low volumes, custom processors are expensive and use less advanced semiconductor technology than commodity processors (for example, they employstandardcell design and static cmos circuits rather than fullcustom degetting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.112getting up to speedsign and dynamic domino circuits). consequently, they now achieve clockrates and sequential (scalar) performance only one quarter that of commodity processors implemented in comparable semiconductor technology.¥a hybrid supercomputer combines commodity processors with acustom highbandwidth interconnectñoften connected to the processormemory interface rather than the i/o interface. hybrid supercomputersoften include custom components between the processor and the memorysystem to provide latency tolerance and improve memory bandwidth.examples of hybrid machines include the cray t3e and asc red storm.such machines offer a compromise between commodity and custom machines. they take advantage of the efficiency (cost/performance) of commodity processors while taking advantage of custom interconnect (andpossibly a custom processormemory interface) to overcome the global(and local) bandwidth problems of commodity supercomputers.custom interconnects have also traditionally supported more advanced communication mechanisms, such as direct access to remotememory with no involvement of a remote processor. such mechanismslead to lower communication latencies and provide better support for aglobal address space. however, with the advent of standard interconnectssuch as infiniband4 the òsemantic gapó between custom interconnects andcommodity interconnects has shrunk. still, direct connection to a memoryinterface rather than an i/o bus can significantly enhance bandwidth andreduce latency.the recently announced ibm blue gene/light (bg/l) computer system is a hybrid supercomputer that reduces the cost and power per nodeby employing embedded systems technology and reducing the pernodememory. bg/l has a highly integrated node design that combines twoembedded (ibm 440) powerpc microprocessor cores, two floatingpointunits, a large cache, a memory controller, and network routers on a singlechip. this bg/l chip, along with just 256 mbyte of memory, forms asingle processing node. (future bg/l configurations may have morememory per node; the architecture is designed to support up to 2 gbyte,although no currently planned system has proposed more than 512mbyte.) the node is compact, enabling 1,024 nodes to be packaged in asingle cabinet (in comparison with 32 or 64 for a conventional clustermachine).4see <http://www.infinibandta.org/home>.getting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.todayõs supercomputing technology113bg/l is a unique machine for two reasons. first, while it employs acommodity processor (the ibm 440), it does not use a commodity processor chip but rather integrates this processor as part of a system on a chip.the processor used is almost three times less powerful than with singlechip commodity processors5 (because it operates at a much lower clockrate and with little instructionlevel parallelism), but it is very efficient interms of chip area and power efficiency. by backing off on absolute singlethread processor performance, bg/l gains in efficiency. second, bychanging the ratio of memory to processor, bg/l is able to realize a compact and inexpensive node, enabling a much higher node count for a givencost. while custom supercomputers aim at achieving a given level of performance with the fewest processors, so as to be able to perform well onproblems with modest amounts of parallelism, bg/l targets applicationswith massive amounts of parallelism and aims to achieve a given level ofperformance at the lowest power and area budget.performance issuesthe rate at which operands can be brought to the processor is theprimary performance bottleneck for many scientific computing codes.6,7the three types of supercomputers differ primarily in the effective localand global memory bandwidth that they provide on different access patterns. whether a machine has a vector processor, a scalar processor, or amultithreaded processor is a secondary issue. the main issue is whether ithas high local and global memory bandwidth and the ability to hidememory latency so as to sustain this bandwidth. vector processors typically have high memory bandwidth, and the vectors themselves providea latency hiding mechanism. it is this ability to sustain high memory bandwidth that makes the more expensive vector processors perform better formany scientific computations.a commodity processor includes much of its memory system (butlittle of its memory capacity) on the processor chip, and this memory system is adapted for applications with high spatial and temporal locality. atypical commodity processor chip includes the level 1 and level 2 caches5a comparison of bg/l to the 3.06ghz pentium xeon machine at ncsa yields a nodeperformance ratio of 1:2.7 on the tpp benchmark.6l. carrington, a. snavely, x. gao, and n. wolter. 2003. òa performance predictionframework for scientific applications.ó international conference on computational scienceworkshop on performance modeling and analysis (pma03). melbourne, june.7s. goedecker and a. hoisie. 2001. performance optimization of numerically intensive codes.philadelphia, pa.: siam press.getting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.114getting up to speedon the chip and an external memory interface that limits sustained localmemory bandwidth and requires local memory accesses to be performedin units of cache lines (typically 64 to 128 bytes in length8). scientific applications that have high spatial and temporal locality, and hence makemost of their accesses from the cache, perform extremely well on commodity processors, and commodity cluster machines represent the mostcosteffective platforms for such applications.scientific applications that make a substantial number of irregularaccesses (owing, for instance, to sparse memory data organization thatrequires random access to noncontiguous memory words) and that havelittle data reuse are said to be scattergather codes. they perform poorlyon commodity microprocessors, sustaining a small fraction of peak performance, for three reasons. first, commodity processors simply do nothave sufficient memory bandwidth if operands are not in cache. for example, a 3.4ghz intel xeon processor has a peak memory bandwidth of6.4 gbyte/sec, or 0.11 words per flops; in comparison, an 800mhz crayx1 processor has a peak memory bandwidth of 34.1 gbyte/sec per processor, or 0.33 words per flops; and a 500mhz nec sx6 has a peakmemory bandwidth of 32 gbyte/sec, or 0.5 words per flops. second, fetching an entire cache line for each word requested from memory may waste15/16 of the available memory bandwidth if no other word in that cacheline is usedñsixteen 8byte words are fetched when only one is needed.finally, such processors idle the memory system while waiting on longmemory latencies because they lack latencyhiding mechanisms. eventhough these processors execute instructions out of order, they are unableto find enough independent instructions to execute to keep busy whilewaiting hundreds of cycles for main memory to respond to a request.note that low data reuse is the main impediment to performance on commodity processors: if data reuse is high, then the idle time due to cachemisses can be tolerated, and scattergather can be performed in software,with acceptable overhead.there are several known techniques that can in part overcome thesethree limitations of commodity memory systems. however, they are notemployed on commodity processors because they do not improve cost/performance on the commercial applications for which these processorsare optimized. for example, it is straightforward to build a wider interface to memory, increasing the total bandwidth, and to provide a shortor sectored cache line, eliminating the cache line overhead for irregularaccesses.8the ibm power 4 has a 512byte level 3 cache line.getting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.todayõs supercomputing technology115a latencyhiding mechanism is required to sustain high memorybandwidth, and hence high performance, on irregular applications. sucha mechanism allows the processor to initiate many memory referencesbefore the response to the first reference is received. in short, it allows theprocessor to fill the memory system pipeline. without a latencyhidingmechanism, the processor idles waiting for a response from memory, andmemory bandwidth is wasted, since no new requests are initiated duringthe idle period.common approaches to latency hiding, including multithreading andvectors (or streams), use parallelism to hide latency. a multithreaded processor uses threadlevel parallelism to hide latency. when one threadneeds to wait for a response from memory, the processor switches to another thread. while some commodity processors provide limited multithreading, they fall short of the tens to hundreds of threads needed tohide main memory latencyñcurrently hundreds of cycles and growing.vectors or streams use data parallelism9 to hide latency. each vector loadinstruction loads a vector (e.g., up to 64 words on the cray x1), allowing asmall number of instructions to initiate a large number of memory references, filling the memory pipeline.architectural organizations that enhance locality reduce bandwidthdemand, complementing a highbandwidth memory system. two suchorganizations are currently being actively studied: processorinmemory(pim) and stream processing. a pim machine integrates processors nearor on the memory chips, allowing data to be operated on locally inmemory. this approach is advantageous if there are large amounts of spatial localityñdata can be operated on in place rather than having to bemoved to and from a remote processor, reducing demand on bandwidth.current research is focused on developing compilation techniques to exploit this type of spatial locality and on quantifying this locality advantage for programs of interest.stream processors exploit temporal locality by providing a large (100kbyte or more) softwaremanaged memory, the stream register file, andreordering programs so that intermediate results are stored in the streamregister file and then immediately consumed without ever being writtento memory. shortcircuiting intermediate results through this large register file greatly reduces demand on the memory system. there is somecurrent software research on compilation techniques to take advantage of9in data parallelism the same operation is applied to multiple elements of a data structureñusually a vector. this is less general than multithreading or control parallelism, wheredistinct threads can execute distinct sequences of instructions.getting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.116getting up to speedexplicit data staging and on organizations to integrate softwaremanagedmemory with hardwaremanaged caches.global bandwidth issues are similar to local bandwidth issues butalso involve the interconnection network and network interface. becausethe cost of bandwidth increases with distance it is prohibitively expensiveto provide flat memory bandwidth across a supercomputer. even the bestcustom machines have a bandwidth taper with a local to global bandwidth ratio of about 10:1. similarly, latency increases across a machine. inthe past, welldesigned custom machines exhibited global latencies thatwere only a few times local latency (e.g., 600 cycles to access globalmemory and 200 cycles to access local memory). similar ratios will become harder to support in the future as the physical size of current systems increases and the absolute speed of light bounds global latency to beat least a few hundreds of nanoseconds.most commodity cluster machines employ offtheshelf interconnect(such as gigabit ethernet) that is connected to the i/o buses of the processing nodes. this results in very low global bandwidth and high globallatency (for instance, 10,000 cycles is not unusual). moreover, softwarelibraries are used to initiate message passing data transfers between processing nodes. the overhead of executing these library calls is sufficientlyhigh that transfers must be aggregated into large units, often thousands ofbytes, to amortize the overhead. this aggregation complicates the programming of these machines for programs where the natural transfer sizeis a few words.as with local bandwidth, there are several known techniques to address global bandwidth and latency. these techniques are not typicallyemployed in commodity interconnects but can be used in hybrid machines. such machines cannot widen the memory interface of a commodity microprocessor. however, they can provide an external memory interface that has a wide path to the actual memory chips, supports efficientsingleword access, and hides latency by allowing many remote accessesto be initiated in parallel (as with t3e eregisters). it is quite straightforward to interface the interconnection network to the processormemoryinterface. the network interface can generate automatically a network request message for each memory access request to a remote address (global address space); it can process arriving requests and generate replymessages with no involvement from the main processor.a wealth of technologies exists for building fast interconnection networks. highspeed electrical and optical signaling technology enableshigh raw bandwidth to be provided at reasonable cost. highradix routers enable tens of thousands of nodes to be connected with just a fewhops, resulting in both low cost and low latency. however, the softwaredriven, i/obusconnected interfaces of commodity cluster machines aregetting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.todayõs supercomputing technology117unable to take advantage of the bandwidth and latency that can be provided by stateoftheart networks.the local and global memory bandwidth bottleneck is expected tobecome a more serious problem in the future due to the nonuniform scaling of technology, as explained in the preceding section. memory latencyhiding is becoming increasingly important as processor speed increasesfaster than memory access time. global latency hiding is becoming increasingly important as global latency becomes constrained by the speedof light (see table 5.1), while processor speeds continue to increase. thecost and power of providing bandwidth between chips, boards, and cabinets is decreasing more slowly than the cost and power of providing logicon chips, making the cost of systems bandwidth dominated by the cost ofglobal bandwidth.another trend is the increased complexity of supercomputers and theincreased variety of supercomputing platforms. a vector supercomputerwill have at least three levels of parallelism: vector parallelism within aprocessor, thread parallelism across processors within an smp, and internode parallelism. the synchronization and communication mechanismswill have very different performance and semantics at each level. performance of commodity processors is affected by their cache hierarchy, whichoften includes three levels of caches, each with a different structure, aswell as a translation lookaside buffer to cache page table entries. the processor performance is also affected by the performance of mechanismssuch as branch prediction or cache prefetching, which attempt to hidevarious latencies. many supercomputing applications stretch the capabilities of the underlying hardware, and bottlenecks may occur in manydifferent parts of the system. as a result, small changes in the applicationtable 5.1parallel hardware trendsannualtypical valuetypical valuetypical valuechange (%)in 2004in 2010in 2020no. of processors204,00012,00074,000general bandwidth26652602,600(mword/sec)(= 0.03(= 0.008(= 0.0008word/flops)word/flops)word/flops)general latency(28)2,000280200(nsec)(= 4,000 flops)(= 9,000 flops)(= 670,000 flops)mpi bandwidth26652602,600(mword/sec)mpi latency (nsec)(28)3,000420300getting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.118getting up to speedcode can result in large changes in performance. similarly, the same application code may exhibit a very different behavior on two fairly similarhardware platforms.the largest supercomputers today include many thousands of processors, and systems with close to 100,000 processors are being built. commodity processors are often designed to have a mean time to failure(mttf) of a few yearsñthere is no incentive to have the mttf muchlonger than the average lifetime of a processor. systems consisting of thousands of such processors have an mttf that is measured in hours, so thatlongrunning applications have to survive multiple failures of the underlying hardware. as hundreds of thousands of such processors are assembled in one supercomputer, there is a risk that the mttf of a largesupercomputer will be measured in minutes, creating a significant problem for a commodity supercomputer. hardware mechanisms can be usedto provide transparent recovery from such failures in custom supercomputers and, to a lesser extent, in hybrid supercomputers.tradeoffsit is important to understand the tradeoffs among various supercomputer architectures. the use of custom processors with higher memorybandwidth and effective latencyhiding mechanisms leads to higher processor performance for the many scientific codes that have poor temporaland spatial locality. one can compensate for lower node performance incommodity systems by using more nodes. but the amount of parallelismavailable in a problem of a given size is limited; for example, in an iterative mesh algorithm, the level of parallelism is bounded by the number ofpoints in the mesh. furthermore, the parallel efficiency of computationsdecreases as one increases the number of processors used (each additionalprocessor contributes slightly less).one reason for decreasing returns from larger amounts of parallelismis amdahlõs law, which states that if a fraction s of a programõs executiontime is serial, then the maximum potential speedup is 1/s. for example, if1 percent of the code is serial, then there is very little gain from usingmore than 100 processors.another reason is that the relative overhead for communication between processors increases as more processors are used. many computations proceed in alternating computation and communication phases; processors compute independently during the computation phase andsynchronize and exchange data during the communication phase. as thenumber of processors is increased, the amount of computation done byeach processor during a computation phase decreases, and the synchronization overhead becomes a higher fraction of the total execution time.getting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.todayõs supercomputing technology119many computations exhibit a surfacetovolume behavior that leadsto relatively more data being exchanged when the computation is splitamong a larger number of processors. thus, an iterative algorithm on athreedimensional cartesian mesh is parallelized by allocating to eachprocessor a subcube; communication involves exchanges between gridpoints at the boundary of the subcubes. the number of points per subcube,hence the number of operations performed in a computation phase, decreases in proportion to the number p of processors used. but the surfaceof the subcubes, hence the amount of data exchanged between subcubes,decreases in proportion to p2/3.load balance becomes more of an issue as the number of nodes isincreased. as fewer data points are processed per node, the variance inexecution time across nodes increases. this variance causes many nodesto idle while waiting for the most heavily loaded nodes to complete execution.other factors reduce the relative performance or increase the relativecost of very large clusters. having more nodes often results in higher failure rates. to compensate, one needs more frequent checkpoints, whichtake time. more frequent checkpoints and restarts increase the relativeoverhead for error tolerance. the cost of some components of the system(in particular, the interconnect) increases faster than linearly with thenumber of nodes. the performance of various system services and toolsmay decrease: for example, it may take longer to load and start a job;debuggers and performance tools may not scale. total power consumption may be higher, and the need for more floor space may be a practicalobstacle.custom supercomputers are a good way to achieve lower timetosolution performance for applications that have poor temporal and spatial locality and for applications that have limited amounts of parallelismor fastdecreasing parallel efficiency. because of their limited volumes,custom processors are significantly more expensive than commodity processors. thus, in many cases, the reduction in execution time is achievedat the expense of an increase in cost per solution.the use of fewer, more powerful processors also typically reducesprogramming effort. consider, for example, a weather code that simulates the atmosphere by discretizing the simulated atmosphere into cubiccells. if more processors are used, then each processor is allocated fewercells. a code that partitions the cells in one dimension (longitude) is simpler than a code that partitions them in two dimensions (longitude andlatitude), and such a code is simpler than a code that partitions cells inthree dimensions (longitude, latitude, and altitude). if finer partitioning isneeded, partitioning along more dimensions will be required. if it is acceptable to run the code only on a custom supercomputer, or to use agetting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.120getting up to speedcustom supercomputer for the more performancedemanding runs, thenthe programming time is reduced. (weather codes are now adapted torun on each type of supercomputer platform; however, many codes runby intelligence agencies are customized to one platform.)the advantages of a custom interconnect and custom interconnect interface can be understood in a similar way. if the interconnect has highereffective bandwidth and lower latency, then the synchronization and communication overheads are smaller, parallel efficiency increases, and it becomes possible to apply efficiently a greater number of processors on aproblem of a given size in situations where performance does not scalewell because of communication costs. one can more easily dynamicallyload balance a computation by allowing idle processors to process datapoints stored on other nodes. in addition, a custom interconnect simplifies programming because one need not aggregate communications intolarge messages: a custom interconnect and custom interface will typicallyprovide better support for shared name space programming models,which are generally accepted to reduce programming overheads. (hereagain, the reduction is most significant for codes that will only run onmachines with custom interconnects.)in summary,¥commodity supercomputers have a cost advantage for many scientific computing applications; the advantage weakens or disappears forapplications with poor temporal and spatial locality or for applicationswith stringent timetosolution requirements, where custom supercomputers do better by reducing both programming time and execution time.as the memory gap continues to increase, the relative performance ofcommodity supercomputers will further erode.¥many applications will scale up with better efficiency on hybridsupercomputers than on commodity supercomputers; hybrid supercomputers can also support a more convenient programming model.the preceding discussion was processorcentric. a slightly differentperspective is achieved by a memorycentric view of parallel computations. for codes where data caches are not effective, performance is determined by the rate at which operands are brought from memory. the mainmemory of custom processors has similar latency to the main memory ofcommodity processors; in order to achieve a given level of performance,both need to sustain the same number of concurrent memory accesses.from the memory perspective, custom architectures do not reduce theamount of parallelism needed to support a given level of performance butenable more memory parallelism per processor; interprocessor parallelism is replaced by intraprocessor parallelism, where one processor supgetting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.todayõs supercomputing technology121ports a larger number of concurrent memory operations. an importantadvantage is that synchronization and communication among operationsexecuted on the same processor are much faster than synchronization andcommunication across processors. the faster synchronization and communication also enable finergrained parallelism to be efficiently exploited, in effect exposing more parallelism than is available internode.thus, the shift to custom processors can help speed up computations thathave enough intrinsic parallelism (significantly more than the number ofcustom processors used) and that exhibit a surfacetovolume behavior sothat most communications and synchronization are intranode. anotheradvantage is better utilization of the processor and memory bus on applications with low cache reuse.the memorycentric discussion does not change the basic conclusionsreached on the relative advantages of custom or hybrid supercomputers,but it introduces some caveats: to take advantage of custom supercomputers, one needs problems where the level of intrinsic parallelism available is much higher than the number of processors and where mostcommunications are local. one often needs a multilevel problem decomposition and different mechanisms for extracting intranode and internode parallelism. furthermore, vector processors support only a restrictedform of intranode parallelismñnamely, data parallelism where the sameoperation is applied to all the components of a vector. codes need to beamenable to this form of parallelism in order to take advantage ofintranode parallelism.trends in supercomputer architecturesupercomputer evolution is driven by many forces.10 mooreõs lawprovides semiconductor components with exponentially increasing numbers of devices. as semiconductor technology evolves, commodity microprocessors improve in performance. the different scaling rates of components (e.g., processors improving faster than memory and interconnectbandwidth) create a need for novel architectures and software to compensate for the gap. at the same time, new applications drive demands forprocessing, global and local bandwidth, and i/o bandwidth.some evolution is parametricñthat is, just a scaling of existing architecture and software. replacing the processors in a machine with a new10the time horizon in this subsection is 2020. the committee does not expect fundamentally new technologies, such as quantum computing, to be deployed in this time frame.therefore, the discussion is based on an extrapolation of trends in current microelectronictechnologies.getting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.122getting up to speedgeneration of faster processors and the memories with a new generationof larger memories is an example of parametric evolution. this evolutionis relatively simple (all that is required is integration), no new hardwareor software technology needs to be developed, and old software runs with,at most, minor changes.as different parts of the system scale at different rates, new bottlenecks appear. for example, if processor speed increases but the interconnect is not improved, then global communication may become a bottleneck. at some point, parametric evolution breaks down and qualitativechanges to hardware and software are needed. for example, as memorylatency (measured in processor cycles) increases, at some point a latencyhiding mechanism is needed to sustain reasonable performance onnonlocal applications. at this point, vectors, multithreading, or some othermechanism is added to the architecture. such a change is complex, requiring a change in software, usually in both systems software (including compilers) and applications software. similarly, increased latency may necessitate different software mechanisms, such as dynamic load balancing.table 5.2 shows expected parametric evolution of commodity components used in supercomputersñsummarizing the trends shown earlier infigure 5.1 through figure 5.6.11 as explained previously, the annual 59percent improvement in single processor speed is expected to decrease intable 5.2hardware trendsannualtypical valuetypical valuetypical valuechange (%)in 2004in 2010in 2020singlechip floating59 2 32 3,300point performance(gflops)frontside bus23 13.527bandwidth(= 0.5(= 0.11(= 0.008(gword/sec)word/flops) word/flops)word/flops)dram bandwidth25 100380 3,600(mword/sec)(= 0.05(= 0.012(= 0.0011word/flops) word/flops)word/flops)dram latency (nsec)(5.5)705028(= 140 flops(= 1,600 flops(= 94,000 flopsor 70 loads)or 170 loads)or 780 loads)11these extrapolations are for explanatory purposes only and do not represent detailedtechnology assessments. in particular, physical limits, such as the electromagnetic radiationthat a 400 gflops chip might emit, are not considered.getting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.todayõs supercomputing technology123the future. however, the committee expects that processor chips will compensate for that by putting several processors on each chip to continue toscale the performance per chip at 59 percent annually. the numbers intable 5.2 for 2010 and 2020 reflect this scaling of chip multiprocessors.table 5.2 highlights the divergence of memory speeds and computation speeds that will ultimately force an innovation in architecture. by2010, 170 loads (memory reads) will need to be executed concurrently tokeep memory bandwidth busy while waiting for memory latency, and1,600 floatingpoint arithmetic operations can be performed during thistime. by 2020, 780 loads must be in flight, and 94,000 arithmetic operations can be performed while waiting on memory. these numbers are notsustainable. it is clear that systems derived using simple parametric evolution are already greatly strained and will break down completely by2020. changes in architecture and/or programming systems are requiredeither to enhance the locality of computations or to hide large amounts oflatency with parallelism, or both.it is not clear if commodity processors will provide the required innovations to overcome this òmemory wall.ó while the pc and server applications for which commodity processors are tuned also suffer from theincreased gap between arithmetic and memory performance, they exhibitsufficient spatial and temporal locality so that aggressive cache memorysystems are largely sufficient to solve the problem. if commodity processors do not offer latencyhiding and/or localityenhancing mechanisms,it is likely that a smaller fraction of scientific applications will be adequately addressed by these processors as the processormemory performance gap grows.figure 5.7 shows the increase in the number of processors for highend systems. at the high end, the number of processors is increasing approximately 20 percent per year. the committee sees no technology limitsthat would cause this trend to change. extrapolating this trend to 2020indicates a number of processors in the 100,000 range; since each of themwill have significant amounts of concurrency for latency hiding, systemswill run tens of millions of concurrent operations.figures 5.8 and 5.9 show measured latency (in microseconds) andbandwidth (in megabytes per second) for mpi programs between twonodes in a variety of commodity and hybrid supercomputer systems.1212the numbers were collected by k. yelick from the following sources: l. oliker et al., inpress, òscientific computations on modern parallel vector systems,ó supercomputing 2004;c. bell et al., 2003, òan evaluation of current highperformance networks,ó 17th international parallel and distributed processing symposium; d.e. culler et al., 1996, òperformanceassessment of faster network interfaces,ó ieee micro, february; and j. dongarra and t.dunigan, 1997, òmessagepassing performance of various computers,ó concurrency: practice and experience 9(10):915926.getting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.124getting up to speed5001,0001,5002,0002,5003,0003,5004,000sep 91jan 93jun 94oct 95mar 97jul 98dec 99apr 01sep 02jan 04may 05datenumber of processorsfigure 5.7median number of processors of the 10 leading top500 systems.(the committee considers mpi measurements because the algorithmicmodels below are based on message passing programs.) leastsquaresfits to the data show an annual improvement of 28 percent in latency and29 percent in bandwidth, albeit with substantial variation. (r2 values forthe formulas are 0.83 and 0.54, respectively.) the improvement rates forlowerlevel communication systems (e.g., shmem on the cray t3e) aresimilarñ28 percent for latency and 26 percent for bandwidth.the committee summarized the expected evolution of parallel systems in table 5.1. a later section will discuss these extrapolations in moredetail. for now, the committee simply points out that even if the individual components continue to improve parametrically, the overall system will see radical changes in how they are balanced. parametric evolution of the system as a whole is unsustainable, and current machinesarguably have already moved into a problematic region of the designspace.the numbers in table 5.1 should be taken with a grain of salt, as theyintegrate factors such as software overheads and transmission delays thatevolve at different rates. furthermore, light traverses 60 m in 200 nsec,less than the diameter of the largest supercomputer installations; the decrease in general latency will slow down as one approaches this limit.however, even the numbers are grossly inaccurate; they clearly show thata parametric evolution of current communication architectures is not sustainable.getting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.todayõs supercomputing technology125supercomputing algorithmsan algorithm is the sequence of basic operations (arithmetic, logic,branches, and memory accesses) that must be performed to solve theuserõs task. to be useful, an algorithm must solve the userõs problem withsufficient accuracy and without using too much time or memory (exactlyhow much accuracy, time, or memory is enough depends on the applica110100199019921994199619982000200220042006year (approximate)latency (msec)1101001,00010,000199019921994199619982000200220042006year of introductionmbyte/secfigure 5.8latency.figure 5.9bandwidth.getting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.126getting up to speedtion). improvements in algorithms can sometimes improve performanceas much as or more than improvements in hardware and software do. forexample, algorithms for solving the ubiquitous linear system arising fromthe poisson equation13on a regular threedimensional grid with n gridpoints have improved over time from needing o(n7/3) to o(n) arithmeticoperations.14 such algorithmic improvements can contribute as much toincreased supercomputer performance as decades of hardware evolution,15 even when the o(n) algorithms run at a much lower fraction ofpeak machine speed than the older o(n7/3) algorithms. while such dramatic breakthroughs are hard to predict, the rewards can be significant.further research can lead to such breakthroughs in the many complicateddomains to which supercomputers are applied.there was considerable discussion of algorithms at the committeeõsapplications workshop, as well as at site visits and in the recent reports ofother study groups.16 the presenters and reports concur that, althoughmuch is known about algorithms for solving scientific problems usingsupercomputing, a great deal more knowledge is needed. for some fields,the algorithms now in use will not solve the most challenging problems,even if they are run on the most capable systems expected to be availablein a foreseeable future. for other fields, satisfactory algorithms of any kindremain to be developed. while these algorithmic needs arise from quitedifferent application areas, they often have much in common.the committee first describes the nature of the algorithms in commonuse, including their demands on the underlying hardware, and then summarizes some of their shortcomings and future challenges.solving partial and ordinary differential equationsdifferential equations are the fundamental equations for many problems governed by the basic laws of physics and chemistry. traditionally,13a poisson equation is an equation that arises in models of many physical systems, including heat flow, fluid flow, diffusion, electrostatics, and gravity.14note on o(.) notation: we say that an algorithm uses o(n) arithmetic operations, or runsin time o(f(n)), on a problem of size n if the number of arithmetic operations is bounded bysome constant multiple of f(n) or if it runs in a number of seconds bounded by some constantmultiple of f(n). an algorithm that runs in time o(n2) will be much slower than an algorithmthat runs in time o(n) once n is large enough, no matter what their respective constants are,which is why we use the o(.) notation to compare the asymptotic speed of algorithms.15doe, office of science. 2003. òa sciencebased case for largescale simulation.ó scalesworkshop report, vol. 1. july.16for example, the hecrtf report, the scales workshop report, vol. 1, the ihec report,and the doe greenbook (doe, nersc, 2002, the doe greenbookñneeds and directions inhighperformance computing for the office of science, april).getting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.todayõs supercomputing technology127much algorithmic research has been devoted to methods for their solution. these continuous equations are typically discretized by replacingthem by algebraic equations for a (large) set of discrete variables corresponding to points or regions on a mesh approximating the physical and/or time domain of the continuous equations. (alternatively, the solutioncould be represented by a collection of particles, vortices, or other discreteobjects.) these equations arise, for example, in fusion, accelerator design,nuclear physics, weapons design, global climate change, reactive chemistry, astrophysics, nanotechnology, contaminant transport, material science, drug design, and related fields. a more recent variation on thistheme is stochastic differential equations, where one or more of the termsrepresent a random process of some kind, like diffusion. in this case thegoal is to compute certain statistics about the set of possible solutions.included in this category of algorithms is work on new ways to discretizethe equations and work on fast solution methods, such as multigrid andother multilevel methods, which use a hierarchy of meshes.the demands these algorithms place on hardware depend both onthe method and on the differential equation. elliptic partial differentialequations (pdes), of which the aforementioned poisson equation is thecanonical example, have the property that the solution at every meshpoint depends on data at every other mesh point, which in turn placesdemands on memory and network bandwidth. their discretizations often use socalled òimplicit difference schemes,ó which lead to large sparsesystems of equations to be solved. on the other hand, the data at distantmesh points can often be compressed significantly without losing muchaccuracy, ameliorating bandwidth needs (a property exploited both bymultigrid methods and by some of the fast transforms discussed below).in contrast to elliptic equations, timedependent equations may (e.g.,parabolic pdes arising in diffusion or heat flow or their approximationsby systems of ordinary differential equations [odes]) or may not (e.g.,hyperbolic pdes arising in electromagnetics or, again, some odes) havethe same global dependence at every time step and corresponding bandwidth need. in the case without global dependence, often discretized using socalled òexplicit difference schemes,ó communication only occursbetween mesh points at processor boundaries, so that a surfacetovolume effect determines bandwidth needs. some timedependent equations(e.g., òstiffó odes) must be solved using communicationintensive implicit methods in order to avoid extremely small time steps. even without global dependence, a timedependent equation with a rapidly changing solution solved with a mesh that adapts to the solution may againhave high bandwidth demands in order to support load balancing (seebelow). finally, if the equation has a lot of òlocal physicsó (e.g., as woulda nuclear weapons simulation requiring the solution of complicated equagetting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.128getting up to speedtions of state at each mesh point), then the correspondingly higher ratioof floatingpoint operations to memory operations makes performanceless sensitive to bandwidth. this variety of behaviors can be found inmany of the asc codes.17longstanding open problems include overcoming the need for tiny(femtosecond) time steps in molecular dynamics simulations18 and finding better anisotropic radiation transport algorithms than fluxlimited diffusion, discrete ordinates (sn), or monte carlo,19 among many others. thedesire to solve larger systems of equations describing more complicatedphenomena (not all of which may be represented or discretized the sameway) on more complicated domains spurs ongoing innovation in this area.mesh generationthe committee considered both generating the abovementioned initial mesh and adapting it during the solution phase. as for time to solution, it is often the process of generating the initial mesh that takes themost time. this is because it often requires a great deal of human intervention to create a suitable geometric model of a complicated physicalsystem or object. even when those models are available (as in the case ofnasaõs space shuttle), creating a mesh suitable for simulation may takemonths using traditional methods. the shuttle in particular has benefitedfrom recent breakthroughs in mesh generation,20 but many problems remain in producing threedimensional meshes with guaranteed geometricand mathematical properties and in doing so efficiently in parallel or whenmemory is limited.in addition to generating the initial mesh, hierarchies of meshes areneeded for multigrid and multilevel methods, and producing these hierarchies in an automatic fashion so as to appropriately approximate thesolution at each level of resolution is challenging. when the mesh represents a deforming material, algorithms are needed to deform the mesh as17based on excerpts from the white paper òcomputational challenges in nuclear weapons simulation,ó by charles f. mcmillan et al., llnl, prepared for the committeeõs santafe, n.m., applications workshop, september 2003.18molecular dynamic simulations use time steps of a few femtoseconds; some phenomena, such as protein folding, take many milliseconds.19expert group on 3d radiation transport benchmarks, nuclear energy agency of theorganisation for economic cooperation and development (oecd), <http://www.nea.fr/html/science/eg3drtb>.20nasa, office of aerospace technology commercial technology division. 2003. òfasteraerodynamic simulation with cart3d.ó spinoff 2003, p. 56.getting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.todayõs supercomputing technology129well. meshes are also sometimes adapted during the solution process tohave higher resolution (more points) in regions where the solution is complicated and fewer points in simple regions. the complicated region canmove during solution; an example is the intricate flame front betweenburnt and unburnt gas in an internal combustion engine.21 using a staticmesh fine enough everywhere to resolve the solution would take ordersof magnitude more work than using it only in complicated regions. effective use of large numbers of parallel processors in these algorithms is anongoing challenge, because the workload and load (im)balance changesunpredictably with the position of the complicated region.dense linear algebrathis class of algorithms for solving linear systems of equations, leastsquares problems, and eigenvalue problems in which all equations involve all or most variables, is epitomized by the linpack benchmark discussed elsewhere in this report. these algorithms are among the least sensitive to memory and network bandwidth of any discussed here, providedthe problems are large enough. dense linear algebra still forms a significant fraction (but not majority) of the workload at some supercomputercenters. for example, nersc reports that materials science applicationsrepresenting 15 percent of their total cycles spend 90 percent of their timein dense linear algebra routines today.22 recent research has focused onexploiting structure, in effect finding and using sparse representationsòhiddenó inside certain dense problems. it is worth noting that even inthis relatively mature field, only a relatively small fraction of the algorithms with good sequential software implementations have good parallel software implementations.sparse linear algebrathe discrete equations on a mesh arising in a discretized differentialequation are typically sparse (i.e., most equations involve just a few variables). it is critical to exploit this mathematical structure to reduce memoryand arithmetic operations, rather than using dense linear algebra. idealalgorithms scale linearlyñthat is, they take time proportional to nnz/p,21doe, office of science. 2003. òa sciencebased case for largescale simulation,ó scalesworkshop report, vol. 1. july.22based on presentations and discussions at the committeeõs site visit to doeõs nationalenergy research scientific computing center in lawrence berkeley national laboratory injanuary 2004.getting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.130getting up to speedwhere nnz (ònumber of nonzerosó) is the total number of appearances ofvariables in all equations and p is the number of processors. in otherwords, an ideal algorithm performs just a constant amount of work pernonzero and communicates very little. whether in fact a reasonably efficient (let alone ideal) algorithm can be found depends strongly on thestructure of the equations (namely, which variables appear and with whatcoefficients), so there is a large set of existing algorithms corresponding tothe large variety of problem structures.23 these algorithms are generallylimited by memory and network bandwidth and are the bottlenecks inpde solvers mentioned earlier, for pdes where the solution at each pointdepends on data at all mesh points. general solution techniques (e.g.,sparse gaussian elimination) have been parallelized, but they are limitedin scalability, especially for linear systems arising from threedimensionalpdes. however, they remain in widespread use because of their reliability and ease of use. iterative methods, which typically rely on the morescalable operation of matrix vector multiplication, can be much faster butoften require careful problemdependent design to converge in a reasonable number of iterations. as new exploitable problem structures ariseand computer architectures change, algorithmic innovation is ongoing.discrete algorithmsdiscrete algorithms are distinguished from others in this summary byhaving few, if any, floatingpoint numbers required to define the inputsor outputs to the problem. discrete algorithms can involve a wide array ofcombinatorial optimization problems arising in computational biology(for instance, looking for nearly matching sequences), the analysis of largedata sets (finding clusters or other patterns in highdimensional data sets),or even other parallel computing algorithms (balancing the workload orpartitioning a sparse matrix among different parallel processors). manyof these problems are nphard (nondeterministic polynomialtime hard),meaning that an optimal solution would take impractically long to compute on any foreseeable computer, so that heuristic approximations arerequired. again, the diversity of problems leads to a diversity of algorithms (perhaps involving floating point) and an ongoing potential forinnovation.23r. barrett, m. berry, t.f. chan, j. demmel, j. donato, j. dongarra, v. eijkhout, r. pozo,c. romine, and h. van der vorst. 1994. templates for the solution of linear systems: buildingblocks for iterative methods. philadelphia, pa.: siam press; zhaojun bai, james demmel, jackdongarra, axel ruhe, and henk van der vorst. 2000. templates for the solution of algebraiceigenvalue problems: a practical guide. philadelphia, pa.: siam press.getting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.todayõs supercomputing technology131other discrete algorithms involve number theory (arising incryptanalysis), symbolic algorithms for exact solutions to algebraic equations (arising in the intelligence community and elsewhere), and discreteevent simulation and agentbased modeling (arising in traffic, epidemiology, and related simulations). it appears that relatively little work (at leastwork that has been made public) has been done to parallelize symbolicalgorithms.fast transformsthere are a variety of widely used fast transform methodsñsuch asthe fast fourier transform (fft), wavelets, the fast multipole method, kernels arising in quantum chemistry, and their numerous variationsñwherea clever reformulation changes, for example, an o(n2) algorithm into ano(n log n) algorithm. these reformulations exploit the underlying mathematical or physical structure of the problem to represent intermediateresults in compressed forms that are faster to compute and communicate.a recent big advance is o(n) methods in electronic structures calculations.it is an ongoing challenge to adapt these methods to new problem structures and new computer architectures. some of these algorithms (e.g., thefast multipole method) limit their bandwidth requirements by compressing and approximating distant data before sending them, whereas others(e.g., the fft) need to communicate more intensively and so require morebandwidth to scale adequately. fastest fourier transform in the west(fftw)24 is a successful example of a system for automatically adaptingan fft algorithm to perform well on a particular problem size and a particular computer.new algorithmic demands arising from supercomputingin addition to opportunities to improve algorithms (as describedabove in the categories of differential equations, mesh generation, linearalgebra, discrete algorithms, and fast transforms), there are new, crosscutting algorithmic needs driven by supercomputing that are common tomany application areas.disciplinary needsone reason for needing increased supercomputer performance is thatmany applications cannot be run using realistic parameter ranges of spa24see <http://www.fftw.org>.getting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.132getting up to speedtial resolution and time integration. for many such applications, applyingmore computer power with substantially the same algorithms can significantly increase simulation quality. for example, mesh resolution can beincreased. but the need for higherresolution analyses may also lead tothe need for faster algorithms. for example, solving a problem 10 timeslarger than currently possible would require 10 times as powerful a machine using an algorithm with complexity o(n) but 100 times as powerfula machine using an algorithm with complexity o(n2). it is sometimes possible to use physicsbased algorithms (like the fast multipole method) orphysicsbased preconditioners that exploit particular properties of theequations being solved. one important area needing research is scalableadaptive methods, where the computational work adapts depending onthe complexity of the physical solution, making load balancing difficult asthe solution changes over time. but in other applications, increased meshresolution may require the development of new physics or algorithms toresolve or approximate phenomena at tiny scales. in some cases,submodels of detailed processes may be required within a coarser mesh(e.g., cloudresolving submodels embedded within a larger climate modelgrid). sometimes completely different physical models may be required(e.g., particle models instead of continuum models), which in turn requiredifferent algorithms. in some problems (such as turbulence), physicallyunresolved processes at small length or time scales may have large effectson macroscopic phenomena, requiring approximations that differ fromthose for the resolved processes. a similar example arises in moleculardynamics, where the molecular motions at the shortest time scales mustcurrently be computed at intervals of 10ð15 seconds to resolve reactionsthat may take a second or more; a new algorithm is needed to avoid thecurrent bottleneck of 1015 sequential steps.interdisciplinary needsmany realworld phenomena involve two or more coupled physicalprocesses for which individual models and algorithms may be known(clouds, winds, ocean currents, heat flow inside and between the atmosphere and the ocean, atmospheric chemistry, and so on) but where thecoupled system must be solved. vastly differing time and length scales ofthe different disciplinary models frequently makes this coupled modelmuch harder to solve. emerging application areas also drive the need fornew algorithms and applications. bioinformatics, for example, is drivingthe need to couple equationdriven numerical computing with probabilistic and constraintdriven computing.getting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.todayõs supercomputing technology133synthesis, sensitivity analysis, and optimization replacing analysisafter one has a model that can be used to analyze (predict) the behavior of a physical system (such as an aircraft or weapons system), it is oftendesirable to use that model to try to synthesize or optimize a system sothat it has certain desired properties, or to discover how sensitive the behavior is to parameter changes. such a problem can be much more challenging than analysis alone. as an example, a typical analysis computes,from the shape of an airplane wing, the lift resulting from airflow over thewing by solving a differential equation. the related optimization problemis to choose the wing shape that maximizes lift, incorporating the constraints that ensure that the wing can be manufactured. solving that problem requires determining the direction of change in wing shape thatcauses the lift to increase, either by repeating the analysis as changes toshape are tried or by analytically computing the appropriate change inshape. similar optimization problems can arise in any manufacturing process, as can parameter identification problems (e.g., reconstructing biological images or earthõs structure from measurements of scatteredwaves), finding stable molecular configurations, and optimizing control.this transition to synthesis, sensitivity analysis, and optimization requiresimproved algorithms in nonlinear solvers, mathematical optimizationtechniques, and methods for quantifying uncertainty.huge data setsmany fields (e.g., biology) that previously had relatively few quantitative data to analyze now have very large amounts, often of varying type,meaning, and uncertainty. these data may be represented by a diversityof data structures, including tables of numbers, irregular graphs, adaptive meshes, relational databases, two or threedimensional images, text,or various combined representations. extracting scientific meaning fromthese data requires coupling numerical, statistical, and logical modelingtechniques in ways that are unique to each discipline.changing machine modelsa machine model is the set of operations and their costs presented tothe programmer by the underlying hardware and software. algorithmicresearch has traditionally sought to minimize the number of arithmetic(or logical) operations. however, the most expensive operation on a machine is not arithmetic but, rather, fetching data from memory, especiallyremote memory. furthermore, the relative costs of arithmetic and fetching data can change dramatically between machines and over time. thisgetting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.134getting up to speedhas profound implications for algorithm design. sometimes this meansthat the fastest algorithm must compress data that are needed far awaybefore communicating them; this compression often involves approximations (which one must carefully bound) that rely on the detailed physicsor other mathematical structure of the problem. the fast multipole methodand multigrid algorithms are celebrated and widely used examples of thistechnique. in these examples, reducing arithmetic and reducing data fetching go hand in hand. but there are yet other examples (e.g., certain sparsematrix algorithms) where one must increase the amount of arithmetic substantially from the obvious algorithm in order to reduce memory fetchesand so speed up the algorithm.25 as the machine model changes betweentechnology generations or among contemporaneous platforms, an algorithm will probably have to be changed to maintain performance andscalability. this optimization process could involve adjusting a few parameters in the algorithm describing data layouts, running a combinatorial optimization scheme to rebalance the load, or using a completely different algorithm that trades off computation and communication indifferent ways. successful tuning by hand is typically a tedious processrequiring familiarity with everything from algorithms to compilers tohardware. some success has been achieved in automating this process,but only for a few important algorithmic kernels, such as atlas26 formatrixmatrix multiplication or fftw for fast fourier transforms. workis needed on these adaptive algorithms to make them more broadly applicable and available to more users.supercomputing softwarethe software used for computing in general and supercomputing inparticular has multiple purposes. the system softwareñthe operatingsystem, the scheduler, the accounting system, for exampleñprovide theinfrastructure for using the machine, independently of the particular applications for which it is used. the programming languages and tools helpthe user in writing and debugging applications and in understanding theirperformance. the applications codes directly implement the application.the software system is sometimes described as a stack of abstractions, inthe sense that the operating system is the lowest level, programming lan25richard vuduc, james w. demmel, katherine a. yelick, shoaib kamil, rajesh nishtala,and benjamin lee. 2002. òperformance optimizations and bounds for sparse matrix vectormultiply.ó proceedings of the acm/ieee sc2002. november 1622.26see <http://mathatlas.sourceforge.net>.getting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.todayõs supercomputing technology135guages and tools sit on top of the operating system, and the applicationsform the top layer. each of the conceptual layers is important in the overall system, and each layer in a supercomputer system has special characteristics that distinguish it from the layers in other kinds of computingsystems.supercomputing software has many requirements in common withsoftware for other computing systems. layered abstractions providehigherlevel operations for most users, allowing them to reuse complexoperations without needing the deep knowledge of the specialists writingthe lower levels. portability is essential, since many programs outlast theiroriginal platforms. in the supercomputing arena, a computer has a typicaluseful lifetime of 5 years, while manydecadesold applications codes arestill in daily use. execution efficiency is important in all areas, particularlyfor supercomputers, because of the high cost of the systems and the heavydemands of the applications. ensuring correct results, a problem on allcomputers, is of course especially difficult on a large, complex system likea supercomputer.other issues are unique to supercomputer software. foremost amongthese is the requirement for excellent scalability at all levels of the software. to benefit from parallel hardware, the software must provideenough concurrent operations to use all the hardware. for example, asupercomputer with a thousand processors needs many thousands ofoperations available for execution at all timesñor many tens of thousandsif custom processors are used. todayõs largest systems typically have onthe order of 10,000 processors to keep busy concurrently. future systemsmay push this degree of concurrency to 100,000 or 1 million processorsand beyond, and the concurrency level within each processor will need toincrease in order to hide the larger memory latency. in addition to havinga high level of concurrency, scalable software needs to avoid sequentialbottlenecks so as not to suffer from the consequences of amdahlõs law,and it needs to manage the global communication and synchronizationefficiently in order to reduce communication overheads.operating systems and management softwareoperating systems manage the basic resources of the system, such asthe memory, the network interfaces, the processors, and the i/o devices.they provide services such as memory and process management to enable multiple executing programs to share the system and abstractionssuch as interfaces and file systems that both facilitate the programminglayers above and reduce hardware dependence. other key services theyprovide are security and protection, logging, and fault tolerance. closelyassociated with those operating system roles is the management softwaregetting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.136getting up to speedthat provides interfaces for servicing users. key components include useraccounts, queuing systems, system monitors, and configuration management.in the operating system arena, virtually all supercomputers today usesome variant of unix, including such systems as aix (from ibm), irix(sgi), linux (open source), superux (nec), tru64 (hewlettpackard),unicos (cray), and macos x (apple). a few projects have createdsupercomputerclass clusters running versions of microsoft windows; aprominent example of such a system is at the cornell theory center, 146thon the june 2004 top500 list.management software for supercomputing is quite varied. for example, just within the top 10 machines on the top500 list are found atleast four batch job submission systems (lsf, batch priority scheduler,distributed production control system, and loadleveler). even amongsites that use the same management tools, the configurationsñfor instance, the number of queues and the policies that control themñdiffersubstantially. although there are open source versions of some of thesetools, most production sites use proprietary management software even ifthey use open source software such as linux for other software components. this is probably due to limitations of the open source tools. forexample, portable batch system (openpbs) supports up to 32 processors,not nearly enough for supercomputing use. management software forsupercomputing typically uses straightforward extensions or improvements to software for smaller systems, together with policies tailored totheir user community.it is challenging to scale an operating system to a large number ofprocessors. a modern operating system is a complex multithreaded application with asynchronous, eventdriven logic, many sequential bottlenecks, and little data locality. it is hard to scale such an application, andeven harder to do so while maintaining full compatibility with a broadlyused commercial operating system such as linux or windows. many ofthe operating system services (and the programming tools) need to scaleas the number of concurrent threads that are created. thus, custom systems that achieve a given level of performance with fewer concurrentthreads facilitate the scaling of these subsystems.large supercomputers are typically managed by multiple operatingsystem images, each controlling one node. a singlesystem image (common file space, single login, single administrative point of control, etc.) isprovided by a set of distributed services that coordinate and integrate themultiple kernels into one system in a way that provides scalability andfault tolerance. this approach creates a fundamental mismatch betweenthe virtual machine provided by the operating system, which is looselygetting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.todayõs supercomputing technology137coupled, and the application running atop this virtual machine, which istightly coupled.a key manifestation of this mismatch is the lack of concurrent scheduling. most existing parallel programming models implicitly assume thatthe application controls a dedicated set of processors executing at the samespeed. thus, many parallel codes consist of an alternation of computephases, where an equal amount of computation work is performed byeach process and by global communication and synchronization phases.but a computation that frequently uses global synchronizations cannottolerate nonsynchronized variations in computation speed that are due,for example, to asynchronous system activities (daemons, page misses,and so on). for example, suppose that each node spends 1 percent of itstime handling system events and each event requires five times as long asit takes to execute a barrier (a synchronization of all active processes). ifthese system events occur simultaneously at all nodes, then the globalloss of performance is 1 percent (as one might expect). however, if theyoccur at random times in a 100node computation, then each barrier isstatistically expected to be preceded by a system event, effectively raisingthe synchronization cost 500 percent. the effect is smaller on smaller systems, but still significant; for example, a 50node system in the same circumstances would see a 250 percent synchronization cost increase. a programmer without detailed knowledge of the underlying operating systemwould be unable to design an appropriate program to compensate for thisvariation. most supercomputer manufacturers (ibm, hewlettpackard,cray) were surprised to encounter this problem on their systems, andmost resolved it by various ad hoc means.some supercomputers run a microkernel on the compute nodes thatreroutes many system functions to a service node running the fullfunction operating system. this approach reduces asynchronous systemevents on the compute nodes and also reduces the frequency of softwarefailures. the implicit assumption in this approach is that page faults canbe virtually eliminated.the òcrystallineó model of a parallel computer, where all processesexecute the same quantity of work at the same speed, is harder and harderto maintain as the number of processors increases and lowprobabilityevents (in particular, recoverable failures) are more likely to disturb thesmooth progress of individual processes. the model is increasingly inappropriate for complex, dynamic, heterogeneous applications. changes inoperating system structures to reduce asynchrony or in programmingmodels to tolerate asynchrony (or, likely, both) will be required. indeed,the more recent programming languages described in the next sectiontend to allow looser synchronization. however, it remains for applicagetting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.138getting up to speedtions and algorithms to utilize this freedom to improve their realworldperformance.programming models, programming languages, and toolsa programming model is an abstract conceptual view of the structureand operation of a computing system. for example, a uniform sharedmemory (or a global addressing) model supports the abstraction that thereis one uniformly addressable storage (even though there may be multiplephysical memories being used). the use of a given programming modelrequires that the operating system, the programming languages, and thesoftware tools provide the services that support that abstraction. in thecontext of this discussion, the programming languages at issue are theones in which applications are written, not the ones in which the systemssoftware is written (although the tools that support the applications programming language must provide appropriate interfacing with systemssoftware). programming tools provide a means to create and run programs. key tools include compilers, interpreters, debuggers, and performance monitors.the programming languages and tools for supercomputing are diverse. many applications, in fact, use components written in more thanone language. a useful taxonomy of languages might be based on theparts of the supercomputer under the control of language operations. sequential imperative languages, such as c and fortran, are commonly usedto program individual processing elements (which may be singleprocessor nodes or threads in multithreaded systems). nodes consisting of several processors with a shared memory are typically programmed usingmodest extensions to these languages, such as the openmp27 extensions,which have bindings for c and fortran. collections of nodes (or processors) that do not share memory are programmed using calls to runtimesystem libraries for message passing, such as mpi, or other communications paradigms (for instance, onesided communication). there has beensome progress in the use of betterintegrated parallel languages. as theirnames suggest, highperformance fortran (hpf)28 and coarray fortran29 are parallel dialects of fortran, and upc30 is a parallel version of c.there are also research languages based on java, such as titanium.3127see <http://www.openmp.org>.28see <http://dacnet.rice.edu/depts/crpc/hpff/index.cfm>.29see <http://www.coarray.org/>.30see <http://upc.gwu.edu/>.31see <http://www.cs.berkeley.edu/projects/titanium/>.getting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.todayõs supercomputing technology139these languages support a shared memory model, in the sense that asingle partitioned global name space allows all executing threads to access large shared data stored in shared (but distributed) arrays. at thefullsystem level, scripting languages (e.g., python and perl) are often usedto link components written in all of the languages mentioned above. objectoriented languages such as c++ and component frameworks such ascommon component architecture (cca)32 and cactus33 are also used toprovide a layer of abstraction on components written in lowerlevel languages. of course, each language requires its own compiler and development tools. some sharing of tools is possible in principle but less commonin practice. one exception is the totalview debugger,34 which supportsfortran, c, openmp, and mpi.parallel programming languages and parallel programming modelsare necessarily compromises between conflicting requirements. althoughmany of the current compromises are deemed to be inadequate, it is notclear what a better solution should be. the use of dialects of fortran and cstems, in part, from a desire to migrate legacy software and tools and toexploit existing user expertise. the ecosystem complexity described inchapter 6 makes it difficult to experiment with new approaches.to improve programmer productivity it would be desirable to havelanguages with a higher level of abstraction. a higher level of abstractionand/or a more restricted model of parallelism are essential in order to beable to comprehend the behavior of a large parallel code, debug it, andtune it. it is not possible to understand the behavior of 10,000 concurrentthreads that may interact in unexpected ways. although many bugs canbe found on smaller runs, some problems only manifest themselves atlarge scales; therefore, the asc program since at least 1998 has listed support for thousands of processors as one of its top requirements for paralleldebuggers.35 however, controlling concurrency and communication areessential activities in parallel algorithm design; a language that does notexpress parallelism and communication explicitly forces the programmerto reverseengineer the implementation strategy used, so as to guess howmuch concurrency or how much communication will be generated by agiven program. for example, a compiler for a sequential language thatgenerates code for a vector machine may be very sensitive to exactly howthe program is written, whereas a language with vector operations makesthat form of parallelism explicit. even if the compiler determines that a32see <http://www.ccaforum.org/>.33see <http://www.cactuscode.org/>.34see <http://www.etnus.com/>.35see <http://www.lanl.gov/projects/asci/pse/ascidebug.html/>.getting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.140getting up to speedparticular vector operation is not profitable (e.g., owing to short vectorlength), the notation may still help optimization (e.g., by improving theprogram analysis information available).it is desirable to have portability across platforms. portability isneeded both to leverage the variety of platforms that a community mayhave access to at a given point in time and to handle hardware evolution.supercomputer applications often outlive the hardware they were designed for: a typical application may be used for 20 years, while the useful life of a supercomputer is more often 5 years. (currently, the oldestmachine on the june 2004 top500 list is 7 years old.) by the same token,one wants good performance on each platform. parallel platforms are distinguished not only by lowlevel details such as the precise instruction setof processors; they also may have very different performance characteristics, with numbers of processors ranging from just a few to 100,000, withglobal latency ranging from hundreds of cycles to more than 10,000 cycles,and so on. in some cases, different algorithms are needed to accommodate such large differences. in general, the more disparate the set of targetplatforms, the harder it is for a single computer program to be mappedefficiently onto all target platforms by compiler and run time; some userhelp is needed. in practice, supercomputer codes written to port to a multiplicity of platforms contain multiple versions tuned for different platforms. it is not clear how one improves this situation in general. someresearch projects have attempted this in specific cases, including packages that generate tuned versions of library codes such as atlas36 andfftw,37 domainspecific program generators such as the tensor contraction engine38 and spiral,39 and dynamic code generators such as tcc.40supercomputer platforms differ not only in the relative performanceof their interconnects but also in the communication mechanisms sup36r. clint whaley, antoine petitet, and jack dongarra. 2001. òautomated empirical optimization of software and the atlas project.ó parallel computing 27(12):335.37see <http://www.fftw.org/>.38g. baumgartner, a. auer, d.e. bernholdt, a. bibireata, v. choppella, d. cociorva, x.gao, r.j. harrison, s. hirata, s. krishnamoorthy, s. krishnan, c. lam, m. nooijen, r.m.pitzer, j. ramanujam, p. sadayappan, and a. sibiryakov. in press. òsynthesis of highperformance parallel programs for a class of ab initio quantum chemistry models.ó proceedings of the ieee.39manuela veloso, and robert w. johnson. 2004. òspiral: a generator for platformadaptedlibraries of signal processing algorithms.ó journal of high performance computing and applications 18(1):2145.40massimiliano poletto, wilson c. hsieh, dawson r. engler, and m. frans kaashoek.1999. òc and tcc: a language and compiler for dynamic code generation.ó acm transactions on programming languages and systems 21(2):324369.getting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.todayõs supercomputing technology141ported by their hardware. clusters may not support direct access to remote memories; with no such hardware support, it is challenging to provide efficient support for a shared memory programming model. if support for shared memory is deemed important for good softwareproductivity, then it may be necessary to forsake porting to clusters thatuse lan interconnects.41different forms of parallelism operate not only on differentsupercomputers but at different levels within one supercomputer. for instance, the earth simulator uses vector parallelism on one processor,shared memory parallelism within one node, and message passing parallelism across nodes.42 if each hardware mechanism is directly reflectedby a similar software mechanism, then the user has to manage three different parallel programming models within one application and managethe interaction among these models, a difficult task. if, on the other hand,a common abstraction such as multithreading is used at all levels, thenthe mapping from user code to hardware may be less efficient. twolevel(or multilevel) problem decomposition is probably unavoidable, since onchip parallelism will increasingly be the norm and onchip communication will continue to be much faster than offchip communication. mechanisms for combining different programming models are not wellunderstood and are a topic for research.to the extent that onchip or intrachip parallelism can be handledautomatically by compilers, the need for distinct programming modelscan be reduced. compilers have been fairly successful with automaticvectorization of code and reasonably successful with automaticparallelization in situations where there are relatively few implementation threads, where communication among implementation threads isvery efficient, and where the applicationõs dependences can be automatically analyzed (for instance, data parallelism, where the same operation isapplied to multiple data items). compilers have been less successful withautomatic parallelization, where communication and synchronizationamong threads is relatively expensive or where data access is more irregular. automatic parallelization is seldom, if ever, used to map sequential codes onto large supercomputers. it is still very much a research issueto find out the best division of labor between programmer, programmingenvironment, and run time in managing parallelism at the different levelsof a complex modern supercomputer.41new emerging san interconnects, such as infiniband, do support remote memory access (see, for example, the infiniband trade association web site at <http://www.infinibandta.org/specs>). however, it is not clear that they will do so with the lowlatency necessary for the efficient support of shared memory programming models.42the design of the earth simulator system is summarized in chapter 7.getting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.142getting up to speedlibrariessince the earliest days of computing, software libraries have been developed to provide commonly used components in a convenient form tofacilitate reuse by many programs. indeed mathematical software libraries are a standard example of successful software reuse. key examplesused for supercomputing include mathematical libraries such aslapack43 for linear algebra, templates such as c++ standard templatelibrary,44 runtime support such as mpi for message passing, and visualization packages such as the visualization tool kit (vtk).45the libraries of most interest to supercomputing involve mathematical functions, including linear algebra (e.g., lapack and its kin), fouriertransforms (e.g., fftw and other packages), and basic functions. owingto the needs of modern scientific software, advanced data structures (e.g.,the c++ standard template library), data management (e.g., hdf),46 andvisualization (e.g., vtk) are all vital for full application development aswell. both the mathematical and computer science libraries are typicallyrequired in both sequential and parallel form; sequential forms solve subproblems on a single processor, while the parallel variants are used forthe global computation.applications softwareapplications software provides solutions to specific science and engineering problems. such software is necessarily domain or problemspecific and ranges from small codes maintained by a single researcher (forinstance, a studentõs dissertation work) through large community codesserving a broad topic (for instance, mm5 for atmospheric research47) tocommercial codes such as the nastran structural engineering package.48large community codes can have hundreds of thousands of sourcelines; commercial packages can have many millions of lines, written overdecades. such codes are hard to port to new hardware platforms or newprogramming languages because of their size, the possible lack of struc43see <http://www.netlib.org/lapack/>.44david r. musser, gillmer j. derge, and atul saini. 2001. stl tutorial and reference guide,2nd ed.: c++ programming with the standard template library. boston, mass.: addisonwesley.45see <http://public.kitware.com/vtk/index.php>.46see <http://hdf.ncsa.uiuc.edu/>.47see <http://www.mmm.ucar.edu/mm5/>.48see <http://www.mscsoftware.com/products/productsdetail.cfm?pi=7>.getting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.todayõs supercomputing technology143ture due to repeated accretions, and the difficulty of verifying majorchanges. in the case of important codes such as nastran, it is the platform that has to adapt to the application, not vice versa; compilers have tocontinue supporting obsolete language features, and obsolete architectures may continue having a market due to their support of importantpackages.49 thus, while the accelerated evolution of supercomputer architectures and programming environments satisfies important missionrequirements of agencies and may accelerate scientific discovery, it alsoaccelerates the obsolescence of important packages that cannot take advantage of the larger scale and improved cost/performance of newsupercomputers.scalability of applications is a major challenge. one issue already discussed is that of appropriate programming languages and programmingmodels for the development of supercomputing applications. most application developers would like to focus their attention on the domainaspects of their applications. although their understanding of the problem will help them in finding potential sources of concurrency, managing that concurrency in more detail is difficult and error prone. the problem is further compounded by the small size of the supercomputermarket, the cost of large supercomputers, and the large variety of supercomputer applications and usage models. because of the small size of thesupercomputer market, commercial software vendors are unlikely to invest in stateoftheart application development environments (ades) forparallel computing. indeed, supercomputer users have to use ades thatare less well integrated and less advanced than those used by commercial programmers. the high cost of supercomputers implies that achieving close to the best possible hardware performance is often paramount.even on sequential processors one can often get a fivefold or better improvement in performance by playing close attention to hardware andsystem parameters (cache sizes, cache line sizes, page size, and so on)and tuning code for these parameters. the reward for platformspecifictuning on supercomputers can be much larger. but such code tuning isvery laborious and not well supported by current ades. the variety ofsupercomputing applications implies that it is not sufficient to tune a fewkey subsystems and libraries: most supercomputer programmers haveto deal with performance issues. finally, supercomputer applicationsrange from codes with a few thousands of lines of source code that aredeveloped in days by one person and run once, to codes with millions of49this is, of course, true in the broad commercial market as well.getting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.144getting up to speedlines that are developed over many years by teams with tens of programmers and used over decades; it is not clear that the same programminglanguages and models can fit these very different situations.fortunately, software abstractions simplify these tasks and in somecases automate them. dataparallel languages like hpf provide the ability to do the same (or very similar) operations on all (or many) elements ofa data structure, with implicit synchronizations between these array operations. other abstractions make the concurrency explicit but simplifyand standardize the synchronization and communications. languageswith a loosely synchronous model of computation proceed in alternating,logically synchronized computation and communication steps. most mpiprograms follow this paradigm, although mpi does not require it. similarly, many operating system operations encapsulate resources to avoidinappropriate concurrent operations.all of these abstractions, however, represent tradeoffs. for example,the global synchronizations used in a loosely synchronous model cancause onerous overheads. alternatively, programs may allow moreasynchrony between concurrent threads, but then the user has to understand the effect of arbitrary interleaved executions of interacting threadsand use proper synchronization and communication, which is often complex. compilers for dataparallel languages have had difficulty achievinggood parallel efficiency owing to the difficulty of minimizing synchronization and communication from finegrain operations. successfully usingany of these approaches on a large machine is a difficult intellectual exercise. worse yet, the exercise must often be repeated on new machines,which often have radically different costs for the same operations or donot support some abstractions at all. perhaps worst of all, even apparentlyminor inefficiencies in software implementation can have a devastatingeffect on scalability; hence, effectively programming these systems in away that allows for software reuse is a key challenge.finally, implementation effort is a major consideration given the limited resources available for hpc software. one important reason that mpiis so successful is that simple mpi implementations can be created quicklyby supplying device drivers for a publicdomain mpi implementation likempich.50 moreover, that mpi implementation can be improved incrementally by improving those drivers and by tuning higherlevel routinesfor the particular architecture. on the other hand, an efficient implementation of a full language like hpf may require many tens, if not hundreds,50see <http://wwwunix.mcs.anl.gov/mpi/mpich/>.getting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.todayõs supercomputing technology145of person years. development teams for hpf at various companies havebeen significantly larger than development teams for mpi; yet hpf implementations are not as mature as mpi implementations. implementationcost and, hence, quality of implementation is also a big problem for toolsand libraries: todayõs supercomputing tools frequently do not address theproblems of interest to application programmers, do not function as advertised, and/or do not deliver a significant fraction of the performanceavailable from the computer.reliability and fault toleranceanother area where there is a complex interplay between hardwareand software is reliability and fault tolerance. as systems become larger,the error rate increases and the mean time between failures (mtbf) decreases. this is true both of hardware failures and software failures. hardware failures on some large asc supercomputers are sufficiently frequentso as to cause 1,000 node computations to suffer about two unrecoverablefailures a day. (this corresponds to an mtbf of about 3 years per node.)this problem is overcome by frequently checkpointing parallel applications and restarting from the last checkpoint after a failure occurred. atcurrent failure rates, the fraction of performance loss due to checkpointsand restarts is modest. but, extrapolating todayõs failure rates to a machine with 100,000 processors suggests that such a machine will spendmost of its time checkpointing and restarting. worse yet, since many failures are heat related, the rates are likely to increase as processors consume more power. this will require new processor technologies to enablecoolerrunning chips, or even more support for fault tolerance.there is little incentive to reduce failure rates of commodity processors to less than one error per few years of operations. failure rates can bereduced using suitable faulttolerant hardware in a custom processor orby using triplicated processors in hybrid supercomputers.in many supercomputers the majority of failures are due to systemsoftware. again, there is little incentive for commercial operating systemproducers to reduce failure rates to the level where a system with 100,000copies of linux or windows will fail only once or twice a day. failurescan be reduced by using a specially designed operating system and, inparticular, by using a reducedfunction microkernel at the compute nodes.alternatively, higher error rates can be tolerated with better softwarethat supports local rather than global fault recovery. this, however, mayrequire more programmer effort and may require a shift in programmingmodelsñagain toward a programming model that is more tolerant ofasynchrony.getting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.146getting up to speedperformance estimationmost assertions about the performance of a supercomputer system orthe performance of a particular implementation of an application arebased on metricsñeither measurements that are taken on an existing system or models that predict what those measurements would yield.supercomputing metrics are used to evaluate existing systems for procurement or use, to discover opportunities for improvement of softwareat any level of the software stack, and to make projections about futuresources of difficulty and thereby to guide investments. system measurement is typically done through the use of benchmark problems that provide a basis for comparison. the metrics used to evaluate systems areconsiderably less detailed than those used to find the performance bottlenecks in a particular application.ideally, the metrics used to evaluate systems would extend beyondperformance metrics to consider such aspects of time to solution as program preparation and setup time (including algorithm design effort, debugging, and mesh generation), programming and debugging effort, system overheads (including time spent in batch queues, i/o time, time lostdue to job scheduling inefficiencies, downtime and handling system background interrupts), and job postprocessing (including visualization anddata analysis). the ability to estimate activities involving human effort,whether for supercomputing or for other software development tasks, isprimitive at best. metrics for system overhead can easily be determinedretrospectively, but prediction is more difficult.performance benchmarksperformance benchmarks are used to measure performance on a givensystem, as an estimate of the time to solution (or its reciprocal, speed) ofreal applications. the limitations of current benchmarking approachesñfor instance, the degree to which they are accurate representatives, thepossibilities for tuning performance to the benchmarks, and so forthñarewell recognized. the darpafunded high productivity computing systems (hpcs) program is one current effort to improve the benchmarks incommon use.industry performance benchmarks include linpack, spec, nas, andstream, among many others.51 by their nature they can only measure lim51see <http://www.netlib.org/benchweb>. other industrial benchmark efforts includereal applications on parallel systems (raps) (see <http://www.cnrm.meteo.fr/aladin/meetings/raps.html>) and mm5 (see <http://www.mmm.ucar.edu/mm5/mpp/helpdesk/20030923.html>).getting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.todayõs supercomputing technology147ited aspects of system performance and cannot necessarily predict performance on rather different applications. for example, lapack, an implementation of the linpack benchmark, produces a measure (rmax) that isrelatively insensitive to memory and network bandwidth and so cannotaccurately predict the performance of more irregular or sparse algorithms.stream measures peak memory bandwidth, but slight changes in thememory access pattern might result in a far lower attained bandwidth ina particular application due to poor spatial locality. in addition to notpredicting the behavior of different applications, benchmarks are limitedin their ability to predict performance on variant systemsñthey can atbest predict the performance of slightly different computer systems orperhaps of somewhat larger versions of the one being used, but not ofsignificantly different or larger future systems. there is an effort to develop a new benchmark, called the hpc challenge benchmark, whichwill address some of these limitations.52as an alternative to standard benchmarks, a set of applicationspecific codes is sometimes prepared and optimized for a particular system,particularly when making procurement decisions. the codes can rangefrom fullscale applications that test endtoend performance, includingi/o and scheduling, to kernels that are small parts of the full applicationbut take a large fraction of the run time. the level of effort required forthis technique can be much larger than the effort needed to use industrystandard benchmarking, requiring (at a minimum) porting of a large code,detailed tuning, rerunning and retuning to improve performance, and rewriting certain kernels, perhaps using different algorithms more suited tothe particular architecture. some work has been done in benchmarkingsystemlevel efficiency in order to measure features like the job scheduler,job launch times, and effectiveness of rebooting.53 the darpa hpcs program is attempting to develop metrics and benchmarks to measure aspects such as ease of programming. decisions on platform acquisition52the hpc challenge benchmark consists of seven benchmarks: linpack, matrix multiply, stream, randomaccess, ptrans, latency/bandwidth, and fft. the linpack andmatrix multiply tests stress the floatingpoint performance of a system. stream is a benchmark that measures sustainable memory bandwidth (in gbytes/sec), randomaccess measures the rate of random updates of memory. ptrans measures the rate of transfer forlarge arrays of data from the multiprocessorõs memory. latency/bandwidth measures (asthe name suggests) latency and bandwidth of communication patterns of increasing complexity between as many nodes as is timewise feasible. ffts stress low spatial and hightemporal locality. see <http://icl.cs.utk.edu/hpcc> for more information.53adrian t. wong, leonid oliker, william t.c. kramer, teresa l. kaltz and david h.bailey. 2000. òesp: a system utilization benchmark.ó proceedings of the acm/ieee sc2000.november 410.getting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.148getting up to speedhave to balance the productivity achieved by a platform against the totalcost of ownership for that platform. both are hard to estimate.54performance monitoringthe execution time of a large application depends on complicated interactions among the processors, memory systems, and interconnectionnetwork, making it challenging to identify and fix performance bottlenecks. to aid this process, a number of hardware and software tools havebeen developed. many manufacturers supply hardware performancemonitors that automatically measure critical events like the number offloatingpoint operations, hits and misses at different levels in the memoryhierarchy, and so on. hardware support for this kind of instrumentationis critical because for many of these events there is no way (short of verycareful and slow simulation, discussed below) to measure them withoutpossibly changing them entirely (a heisenberg effect). in addition, somesoftware tools exist to help collect and analyze the possibly large amountof data produced, but those tools require ongoing maintenance and development. one example of such a tool is papi.55 other software toolshave been developed to collect and visualize interprocessor communication and synchronization data, but they need to be made easier to use tohave the desired impact.the limitation of these tools is that they provide lowlevel, systemspecific information. it is sometimes difficult for the application programmer to relate the results to source code and to understand how to use themonitoring information to improve performance.performance modeling and simulationthere has been a great deal of interest recently in mathematicallymodeling the performance of an application with enough accuracy to predict its behavior either on a rather different problem size or a rather different computer system, typically much larger than now available. performance modeling is a mixture of the empirical (measuring theperformance of certain kernels for different problem sizes and using curvefitting to predict performance for other problem sizes) and the analytical54see, for example, larry davis, 2004, òmaking hpc system acquisition decisions is anhpc application,ó supercomputing.55s. browne, j. dongarra, g. ho, n. garner, and p. mucci. 2000. òa portable programminginterface for performance evaluation on modern processors.ó international journal of highperformance computing applications: 189204.getting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.todayõs supercomputing technology149(developing formulas that characterize performance as a function of system and application parameters). the intent is that once the characteristics of a system have been specified, a detailed enough model can be usedto identify performance bottlenecks, either in a current application or afuture one, and so suggest either alternative solutions or the need for research to create them.among the significant activities in this area are the performance models that have been developed for several full applications from the ascworkload56,57,58 and a similar model that was used in the procurementprocess for the asc purple system, predicting the performance of thesage code on several of the systems in a recent competition.59 alternative modeling strategies have been used to model the nas parallel benchmarks, several small petsc applications, and the applications parallelocean program, navy layered ocean model, and cobal60, across multiple compute platforms (ibm power 3 and power 4 systems, a compaqalpha server, and a cray t3e600).60,61 these models are very accurateacross a range of processors (from 2 to 128), with errors ranging from 1percent to 16 percent.performance modeling holds out of the hope of making a performanceprediction of a system before it is procured, but currently modeling hasonly been done for a few codes by experts who have devoted a great dealof effort to understanding the code. to have a wider impact on the procurement process it will be necessary to simplify and automate the modeling process to make it accessible to nonexperts to use on more codes.56a. hoisie, o. lubeck, and h. wasserman. 2000. òperformance and scalability analysisof teraflopscale parallel architectures using multidimensional wavefront applications.óthe international journal of high performance computing applications 14(4).57d.j. kerbyson, h. alme, a. hoisie, f. petrini, h. wasserman, and m. gittings. 2001.òpredictive performance and scalability modeling of a largescale application.ó proceedings of the acm/ieee sc2001, ieee. november.58m. mathis, d. kerbyson, and a. hoisie. 2003. òa performance model of nondeterministic particle transport on largescale systems.ó workshop on performance modeling andanalysis, 2003 iccs. melbourne, june.59a. jacquet, v. janot, r. govindarajan, c. leung, g. gao, and t. sterling. 2003. òan executable analytical performance evaluation approach for early performance prediction.óproceedings of ipdpsõ03.60l. carrington, a. snavely, n. wolter, and x. gao. 2003. òa performance predictionframework for scientific applications.ó workshop on performance modeling and analysis,2003 iccs. melbourne, june.61a. snavely, l. carrington, n. wolter, j. labarta, r. badia, and a. purkayastha. 2002. òaframework for performance modeling and prediction.ó proceedings of the acm/ieee sc2002,november.getting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.150getting up to speedultimately, performance modeling should become an integrative part ofverification and validation for highperformance applications.supercomputers are used to simulate large physical, biological, oreven social systems whose behavior is too hard to otherwise understandor predict. a supercomputer itself is one of these hardtounderstand systems. some simulation tools, in particular for the performance of proposednetwork designs, have been developed,62 and computer vendors haveshown significant interest.measuring performance on existing systems can certainly identifycurrent bottlenecks, but it not adequate to guide investments to solve future problems. for example, current hardware trends are for processorspeeds to increasingly outstrip local memory bandwidth (the memorywall63), which in turn will increasingly outstrip network bandwidth.therefore, an application that runs efficiently on todayõs machines maydevelop a serious bottleneck in a few years either because of memorybandwidth or because of network performance. performance modeling,perhaps combined with simulation, holds the most promise of identifyingthese future bottlenecks, because an application (or its model) can be combined with the hardware specifications of a future system. fixing thesebottlenecks could require investments in hardware, software, or algorithms. however, neither performance modeling nor simulation are yetrobust enough and widely enough used to serve this purpose, and bothneed further development. the same comments apply to software engineering, where it is even more difficult to predict the impact on softwareproductivity of new languages and tools. but since software makes upsuch a large fraction of total system cost, it is important to develop moreprecise metrics and to use them to guide investments.performance estimation and the procurement processthe outcome of a performance estimation process on a set of currentand/or future platforms is a set of alternative solution approaches, eachwith an associated speed and cost. cost may include not just the cost ofthe machine but the total cost of ownership, including programming, floorspace, power, maintenance, staffing, and so on.64 at any given time, there62see <http://simos.stanford.edu>.63wm. a. wulf and s.a. mckee. 1995. òhitting the wall: implications of the obvious.ócomputer architecture news 23(1):2024.64national coordination office for information technology research and development.2004. federal plan for highend computing: report of the highend computing revitalizationtask force (hecrtf). may.getting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.todayõs supercomputing technology151will be a variety of lowcost, lowspeed approaches based on cots architectures and software, as well as highcost, highspeed solutions based oncustom architectures and software. in principle, one could then applyprinciples of operations research to select the optimal systemñfor example, the cheapest solution that computed a solution within a hard deadline in the case of intelligence processing, or the solution that computedthe most solutions per dollar for a less timecritical industrial application,or the number of satisfied users per dollar, or any other utility function.65the most significant advantage of commodity supercomputers is theirpurchase cost; less significant is their total cost of ownership, because ofthe higher programming and maintenance costs associated with commodity supercomputers. lower purchase cost may bias the supercomputingmarket toward commodity supercomputers if organizations do not account properly for the total cost of ownership and are more sensitive tohardware cost.the imperative to innovate andbarriers to innovationsystems issuesthe committee summarizes trends in parallel hardware in table 5.1.the table uses historical data to project future trends showing that innovation will be needed. first, for the median number of processor chips toreach 13,000 in 2010 and 86,000 in 2020, significant advances will be required in both software scalability and reliability. the scalability problemis complicated by the fact that by 2010 each processor chip is likely to be achip multiprocessor (cmp) with four to eight processors, and each of theseprocessors is likely to be 2 to 16way multithreaded. (by 2020 these numbers will be significantly higher: 64 to 128 processors per chip, each 16 to128way multithreaded.) hence, many more parallel threads will need tobe employed to sustain performance on these machines. increasing thenumber of threads by this magnitude will require innovation in architecture, programming systems, and applications.a machine of the scale forecast for 2010 is expected to have a rawfailure rate of several failures per hour. by 2020 the rate would be severalfailures per minute. the problem is complicated because there are bothmore processors to fail and because the failure rate per processor is expected to increase as integrated circuit dimensions decrease, making cir65marc snir and david a. bader. 2003. a framework for measuring supercomputer productivity. technical report. october.getting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.152getting up to speedcuitry more vulnerable to energetic particle strikes. in the near future, softerrors will occur not just in memory but also in logic circuits. such failurerates require innovation in both fault detection and fault handling to givethe user the illusion of a faultfree machine.the growing gap between processor performance and global bandwidth and latency is also expected to force innovation. by 2010 globalbandwidth would fall to 0.008 words/flops, and a processor would needto execute 8,700 flops in the time it takes for one communication to occur.these numbers are problematic for all but the most local of applications.to overcome this global communication gap requires innovation in architecture to provide more bandwidth and lower latency and in programming systems and applications to improve locality.both locally (within a single processor chip) and globally (across amachine), innovation is required to overcome the gaps generated by nonuniform scaling of arithmetic local bandwidth and latency, and globalbandwidth and latency. significant investments in both basic and appliedresearch are needed now to lay the groundwork for the innovations thatwill be required over the next 15 years to ensure the viability of highendsystems. lowend systems will be able, for a while, to exploit onchipparallelism and tolerate increasing relative latencies by leveraging techniques currently used on highend systems, but they, too, will eventuallyrun out of steam without such investments.innovations, or nonparametric evolution, of architecture, programming systems, and applications take a very long time to mature. this isdue to the systems nature of the changes being made and the long timerequired for software to mature. the introduction of vector processing isa good example. vectors were introduced in the early 1970s in the texasinstruments asc and cdc star. however, it took until 1977 for a commercially successful vector machine, the cray1, to be developed. the lagging balance between scalar performance and memory performance prevented the earlier machines from seeing widespread use. one could evenargue that the systems issues were not completely solved until the introduction of gatherandscatter instructions on the cray xmp and the convex and alliant minisupercomputers in the 1980s. even after the systemsissues were solved, it took additional years for the software to mature.vectorizing compilers with advanced dependence analysis did not emergeuntil the mid 1980s. several compilers, including the convex and thefujitsu fortran compilers, permitted applications that were written instandard fortran 77 to be vectorized. applications software took a similaramount of time to be adapted to vector machines (for example, by restructuring loops and adding directives to facilitate automatic vectorization ofthe code by the compiler).a major change in architecture or programming has farreaching efgetting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.todayõs supercomputing technology153fects and usually requires a number of technologies to be successful. introducing vectors, for example, required the development of vectorizingcompilers; pipelined, banked memory systems; and masked operations.without the supporting technologies, the main new technology (in thiscase, vectors) is not useful. the main and supporting technologies are typically developed via research projects in advance of a first fullscale system deployment. fullscale systems integrate technologies but rarely pioneer them. the parallel computers of the early 1990s, for example, drewon research dating back to the 1960s on parallel architecture, programming systems, compilers, and interconnection networks. chapter 6 discusses the need for coupled development in more detail.issues for algorithmsa common feature of algorithms research is that progress is tied toexploiting the mathematical or physical structure of the application. generalpurpose solution methods are often too inefficient to use. thus,progress often depends on forming interdisciplinary teams of applications scientists, mathematicians, and computer scientists to identify andexploit this structure. part of the technology challenge is to facilitate theability of these teams to address simultaneously the requirements imposed by the applications and the requirements imposed by thesupercomputer system.a fundamental difficulty is the intrinsic complexity of understandingand describing the algorithm. from the application perspective, a concisehighlevel description in which the mathematical structure is apparent isimportant. many applications scientists use matlab66 and frameworkssuch as petsc67 to rapidly prototype and communicate complicated algorithms. yet while parallelism and communication are essential issues inthe design of parallel algorithms, they find no expression in a highlevellanguage such as matlab. at present, there is no highlevel programmingmodel that exposes essential performance characteristics of parallel algorithms. consequently, much of the transfer of such knowledge is done bypersonal relationships, a mechanism that does not scale and that cannotreach a large enough user community. there is a need to bridge this gapso that parallel algorithms can be described at a high level.it is both infeasible and inappropriate to use the full generality of acomplex application in the process of designing algorithms for a portionof the overall solution. consequently the cycle of prototyping, evaluating,66<http://www.mathworks.com/products/matlab/>.67<http://wwwunix.mcs.anl.gov/petsc/petsc2/>.getting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.154getting up to speedand revising an algorithm is best done initially by using benchmark problems. it is critical to have a suitable set of test problems easily available tostimulate algorithms research. for example, the collection of sparse matrices arising in real applications and made available by harwell and boeingmany years ago spawned a generation of research in sparse matrix algorithms. yet often there is a dearth of good benchmarks with which towork.68 such test sets are rare and must be constantly updated as problem sizes grow.an example from computational fluid dynamicsas part of the committeeõs applications workshop, phillip colella explained some of the challenges in making algorithmic progress. he wroteas follows:69success in computational fluid dynamics [cfd] has been the result of acombination of mathematical algorithm design, physical reasoning, andnumerical experimentation. the continued success of this methodologyis at risk in the present supercomputing environment, due to the vastlyincreased complexity of the undertaking. the number of lines of coderequired to implement the modern cfd methods such as those describedabove is far greater than that required to implement typical cfd software used twenty years ago. this is a consequence of the increased complexity of both the models, the algorithms, and the highperformancecomputers. while the advent of languages such as c++ and java withmore powerful abstraction mechanisms has permitted us to manage software complexity somewhat more easily, it has not provided a completesolution. lowlevel programming constructs such as mpi for parallelcommunication and callbacks to fortran kernels to obtain serial performance lead to code that is difficult to understand and modify. the netresult is the stifling of innovation. the development of stateofthearthighperformance cfd codes can be done only by large groups. even inthat case, the development cycle of designimplementtest is much moreunwieldy and can be performed less often. this leads to a conservatismon the part of developers of cfd simulation codes: they will make dowith lessthanoptimal methods, simply because the cost of trying outimproved algorithms is too high. in order to change this state of affairs, acombination of technical innovations and institutional changes areneeded.68doe. 2003. doe science networking challenge: roadmap to 2008. report of the june 35science networking workshop, conducted by the energy sciences network steering committee at the request of the office of advanced scientific computing research of the doeoffice of science.69 from the white paper òcomputational fluid dynamics for multiphysics and multiscaleproblems,ó by phillip colella, lbnl, prepared for the committeeõs santa fe, n.m., applications workshop, september 2003.getting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.todayõs supercomputing technology155as dr. colellaõs discussion suggests, in addition to the technical challenges, there are a variety of nontechnical barriers to progress in algorithms. these topics are discussed in subsequent chapters.software issuesin extrapolating technology trends, it is easy to forget that the primary purpose of improved supercomputers is to solve important problems better. that is, the goal is to improve the productivity of users, including scientists, engineers, and other nonspecialists in supercomputing.to this end, supercomputing software development should emphasizetime to solution, the major metric of value to highend computing users.time to solution includes time to cast the physical problem into algorithmssuitable for highend computing; time to write and debug the computercode that expresses those algorithms; time to optimize the code for thecomputer platforms being used; time to compute the desired results; timeto analyze those results; and time to refine the analysis into an improvedunderstanding of the original problem that will enable scientific or engineering advances. there are good reasons to believe that lack of adequatesoftware is today a major impediment to reducing time to solution andthat more emphasis on investments in software research and development (as recommended by previous committees, in particular, pitac) isjustified. the main expense in large supercomputing programs such asasc is software related: in fy 2004, 40 percent of the asc budget wasallocated for application development; in addition, a significant fractionof the acquisition budget also goes, directly or indirectly, to software purchase.70 a significant fraction of the time to solution is spent developing,tuning, verifying, and validating codes. this is especially true in the nsaenvironment, where new, relatively short hpc codes are frequently developed to solve new emerging problems and are run once. as computingplatforms become more complex,and as codes become much larger andmore complex, the difficulty of delivering efficient and robust codes in atimely fashion increases. for example, several large asc code projects,each involving tens of programmers, hundreds of thousands of lines ofcode, and investments from $50 million to $100 million had early milestones that proved to be too aggressive.71 many supercomputer users feel70advanced simulation and computing program plan, august 2003.71see douglass post, 2004, òthe coming crisis in computational sciences,ó workshop onproductivity and performance in highend computing, february; and d. post and r.kendall, 2003, òsoftware project management and quality engineering practices for complex, coupled multiphysics, massively parallel computation simulations: lessons learnedfrom asci,ó doe software quality forum, march.getting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.156getting up to speedthat they are hampered by the difficulty of developing new hpc software. the programming languages, libraries, and application development environments used in hpc are generally less advanced than thoseused by the broad software industry, even though the problems are muchharder. a software engineering discipline geared to the unique needs oftechnical computing and highperformance computing is yet to emerge.in addition, a common software environment for scientific computationencompassing desktop to highend systems will enhance productivitygains by promoting ease of use and manageability of systems.extrapolating current trends in supercomputer software, it is hard tosee whether there will be any major changes in the software stack used forsupercomputers in the coming years. languages such as upc, caf, andtitanium are likely to be increasingly used. however, upc and caf donot support object orientation well, and all three languages have a staticview of parallelism (the crystalline model) and give good support to onlysome application paradigms. the darpa hpcs effort emphasizes software productivity, but it isvendor driven and hardware focused and hasnot generated a broad, coordinated community effort for new programming models. meanwhile, larger and more complex hardware systemscontinue to be put in production, and larger and more complex application packages are developed. in short, there is an oncoming crisis in hpcsoftware created by barely adequate current capabilities, increasing requirements, and limited investment in solutions.in addition to the need for software research, there is a need for software development. enhanced mechanisms are needed to turn prototypetools into welldeveloped tools with a broad user base. the core set oftools available on supercomputersñoperating systems, compilers,debuggers, performance analysis toolsñis not up to the standards of robustness and performance expected for commercial computers. tools arenonexistent or, even worse, do not work. parallel debuggers are an oftencited example. parallel math libraries are thought to be almost as bad,although math libraries are essential for building a mature applicationsoftware base for parallel computing. thirdparty commercial and publicdomain sources have tried to fill the gaps left by the computer vendorsbut have had varying levels of success. many active research projects arealso producing potentially useful tools, but the tools are available only inprototype form or are fragmented and buried inside various applicationefforts. the supercomputer user community desperately needs bettermeans to develop these technologies into effective tools.although the foregoing discussion addresses the need for technicalinnovation and the technical barriers to progress, there are significantpolicy issues that are essential to achieving that progress. these topics aretaken up in subsequent chapters.getting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.1576supercomputinginfrastructures and institutionssupercomputing is not only about technologies, metrics, and economics; it is also about the people, organizations, and institutions thatare key to the further progress of these technologies and about thecomplex web that connects people, organizations, products, and technologies. to understand supercomputing, one needs to understand the structure of the supercomputing community and the structure of the supercomputing landscape of concepts and technologies. such a structuralistapproach to supercomputing is necessarily less quantitative, more subjective, and more speculative than approaches that are more congenial toeconomists or engineers. however, it provides a necessary corrective fora study that might otherwise measure the trees but might not view theforest. the committee presents such an approach to supercomputing inthis chapter.it is useful to think of supercomputing infrastructure as an ecosystem. the encarta dictionary defines an ecosystem as òa localized group ofinterdependent organisms together with the environment that they inhabit and depend on.ó a supercomputer ecosystem is a continuum ofcomputing platforms, system software, and the people who know how toexploit them to solve supercomputing applications such as those discussed in chapter 4.in supercomputing ecosystems, the òorganismsó are the technologiesthat mutually reinforce one another and are mutually interdependent.examples include the following:getting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.158getting up to speed¥vector architectures, vectorizing compilers, and applications tunedfor the use of vector hardware;¥shared memory architectures, scalable operating systems,openmpcompliant compilers and runtime systems, and applicationsthat can take advantage of shared memory; and¥message passing architectures, parallel software tools and libraries, and applications that are designed to use this programming model.the organism space tends to group according to the architecture class,the programming language and models used on the system, the algorithms, the set of applications and how the code is being tuned (e.g., vector version versus cache version), what application program packages areavailable, and so on. the success of supercomputer architectures is highlydependent on the organisms that form around them.the architecture and the balance among its key configuration parameters (such as number of processors or memory size) are the dominantfactors in determining the nature of the technologies in the ecosystem. forexample, early supercomputers such as the cdc 7600 had rather smallmemories compared with their computing speed and the requirements ofthe applications that users wanted to run. that characteristic led to thedevelopment of system tools to reuse memory during execution (overlaymanagement software) and to the use of different algorithms in certaincases, and it excluded certain classes of applications, together with a partof the user community. a second example is the speed of i/o to localdisk, which can have a major impact on the design of application programs. for example, in a number of chemistry applications, if the ratio ofi/o speed to computation performance is below a certain (wellunderstood) level, the application will run faster by recomputing certain quantities instead of computing them once, writing them to disk, and thenreading them in subsequent phases of the job. some widely used chemistry programs use this recompute strategy. another common example ofthe impact of system performance characteristics on programming is thata message passing programming style is most often used when sharedmemory performance is below some threshold, even if shared memoryprogramming tools are provided.because all current supercomputers have highly parallel architectures,the system software and the algorithms used have to be designed oradapted to function on such machines. as discussed in chapter 5, thecharacteristics of the processors and the interconnection network (latencyand bandwidth of access to memories, local and remote) are key featuresand determine to a large extent the algorithms and classes of applicationsthat will execute efficiently on a given machine. low latency and highbandwidth access to memory not only yield higher performanceñmuchgetting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.supercomputing infrastructures and institutions159higher on some applicationsñbut enable the use of less complex algorithms and ease the programming task. however, these features also leadto higher hardware costs and are typically available on systems that useless advanced device technology and run at a slower clock rate. even formachines with such hardware features, software tools or compiler directives are often needed to achieve high fractions of peak speed. in short,the micro and macroarchitecture of a supercomputer determine to alarge extent the complexities of the other technologies in its ecosystem.grid computing environments may provide a way to integrate many components of the supercomputing ecosystem. but they will also create evenmore complex ecosystems.without software, the hardware is useless; hence, another importantpart of the ecosystem is the system software. by system software is meantthe operating system components as well as tools such as compilers,schedulers, runtime libraries, monitoring software, debuggers, file systems, and visualization tools. but in supercomputing ecosystems, the existence of software with certain functionality is not sufficient. unlike thesituation with pcs, almost all of which use the same type of processor, insupercomputing environments the mere existence of system software isnot enough to create an effective supercomputing ecosystem. for example,if the supercomputer configuration has thousands of processors but theoperating system is designed for systems with only a few processors,many operating system tasks will run unacceptably slowly or even fail toexecute. this inadequacy was observed for computers from several different vendors, and until those gross inefficiencies were removed, the systems saw little use. additional examples of software technology that maybe required for a supercomputing ecosystem to be effective are globalparallel file systems and fault tolerance.libraries are also part of the ecosystem. examples include messagepassing libraries (e.g., mpi) and numerical libraries that embody algorithms that are efficient on the supercomputerõs architecture and that areimplemented appropriately (e.g., petsc and scalapack).to enable effective use of a supercomputer, the system software andlibraries must be tailored to the particular supercomputer that is the focusof the ecosystem. as pointed out above, some of this software, such ascompilers and runtime libraries, may require extensive customization,while others, such as networking software, might require relatively little.the nature of the userõs interface to the systemñfor example, the programming languages or the job schedulerñis also part of the ecosystem.if the technologies that make up a supercomputing ecosystem constitute the òorganism,ó the environment that they inhabit and depend onincludes people with the relevant skills (such as expertise in parallel algorithms) and projects with certain requirements (for instance, researchgetting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.160getting up to speedwhose conduct requires supercomputers). the people with the expertiseto produce the software, design the algorithms, and/or use the supercomputer are determinants of the scope (or sphere of influence) of theecosystem associated with that supercomputer.1many industrial users depend on commercial software packages suchas msc nastran or gaussian. if those packages run poorly or not at allon a given supercomputer, the industrial users will be missing from theecosystem, reducing the financial viability of that supercomputer. on theother hand, at national laboratories and research universities, almost allapplication programs are developed by individual research teams that donot depend on the availability of commercial software packages. as a result, a given installation may become favored by users of one or a fewapplication classes, creating an ecosystem that is essentially a òtopical centeró because of the expertise, libraries, configuration details (such as storage and i/o), and visualization capabilities that are suitable for that classof applications. expertise and sharing of methodologies might be as big afactor in creating the topical ecosystem as the use of the same softwarepackage by many projects.ecosystems are stable over several generations of a computer family,sometimes for one or two decades. as supercomputing hardware has become more complex, the barriers to creating a new ecosystem have risen,mostly due to the large effort required to develop a robust and fullfeatured software environment for complex architectures. hence, creating anew ecosystem requires a significant, protracted effort and often also research and development of new software technologies. vectorizing compilers took decades to mature and relied on the results of many academicresearch projects. parallelizing compilers are still in their infancy morethan 20 years after parallel computers came into use. the cost and timeñand research and development in many different areas (compiler technology, operating system scalability, hardware, etc.)ñmake it very difficultto mount projects to introduce supercomputers with novel architecturalfeatures, especially if they require new programming paradigms for theirefficient use.among the challenges in introducing new architectural features isfinding ways to make them usable through languages and concepts thatusers can easily relate to familiar tools. if the learning curve for using anew system is too steep, there will be little development of system software and especially of applications, and users will simply not attempt tomodify their applications to run on it. this is why programming languages persist for decades. for instance, fortran 77 (a modest evolution offortran 66) is still heavily used 27 years after it first became a standard.1the workforce portion of the supercomputer ecosystem is discussed later in this chapter.getting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.supercomputing infrastructures and institutions161the large cost in human effort and time to adopt new programming languages and programming paradigms raises barriers that further stabilizethe ecosystem. many supercomputer applications are very complex, entail hundreds of thousands of lines of code, and require multidisciplinaryteams with tens of people. like supertankers, they cannot change direction quickly.another factor that contributes to the stability of ecosystems is thatdifferent technologies have different lifetimes and different startingpoints. it is almost never the case that one is afforded a cleansheet starting point, where hardware, software, applications, and interfaces can allbe designed from scratch. this reality calls for longterm ecosystem planning: application packages have to be ported and maintained, new systems have to interoperate with old ones, the same people have to operatethe new and the old systems, and so on. a lack of continuity in platformsand a lack of platform independence in software raise the costs of hiringand retaining appropriate personnel. longterm planning is also neededbecause buildings cost money and can often account for a substantial portion of the total costs of procurement. to amortize that investment, it isdesirable that a given facility be designed to serve for several ecosystemgenerations.supercomputing ecosystemcreation and maintenancesupercomputing system evolution is not all that different from generic computing system evolution, with the same patterns of horizontallyintegrated ecosystems (for example, the wintel ecosystem) and verticallyintegrated ecosystems (such as the ecosystem created by ibm mainframes)and with the same high cost of change. from this process point of view,there is very little difference between supercomputing systems and generic computing systems except that, since the architectural platform differences are so radical, it can be much more expensive to port applicationsin the supercomputing ecosystem than in the generic ecosystem. that expense, coupled with the very small number of supercomputers sold,greatly inhibits the development and porting of commercial softwarepackages to supercomputer platforms.designing, developing, and deploying a truly radical new computingplatform is a very difficult and expensive project. an example of the heightof the barriers is illustrated by the ibm blue gene (bg) project.2 the bg/2see <http://www.research.ibm.com/bluegene/index.html>. the 8,192processor bg/lprototype at ibm rochester was 4th on the june 2004 top500 list, while the 4,096 prototypeat ibm watson was 8th.getting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.162getting up to speedl system that is currently being developed and built is substantially lessinnovative than the initial bg concept. the instruction set architecture is aknown one, so that compilers, many parts of the operating system, andmany software tools do not have to be written ab initio and so that userswill find a somewhat familiar environment. even so, for bg/l a largeconsortium is being formed of academic groups and research laboratoriesthat are users and developers of very high end applications to build acommunity around this class of hpc architectures, which, in its largestconfigurations, will have an orderofmagnitude more nodes than previous multiple instruction, multiple data (mimd) systems. the consortiumis being formed to provide additional human capital to study the newarchitecture in the context of many applications, to develop some of thesoftware tools or to migrate existing open source tools, and to provideinput on hardware and software improvements that should be made forfuture generations of the blue gene family. the forming of a consortiumreflects the fact that even a very large and profitable company like ibmdoes not have enough inhouse expertise in all the necessary areas andcannot justify the investments needed to hire such experts. it is furtherevidence of the complexity of current supercomputer environments compared with those of only 20 years ago. formation of the consortium is alsoan explicit attempt to quickly create an ecosystem around the blue geneplatforms.how ecosystems get establishedtraditionally, supercomputing ecosystems have grown up around aparticular computer vendorõs family of products, e.g., the cray researchfamily of vector computers, starting with the cray1 and culminating inthe t90, and the ibm sp family of parallel computers. while a givenmodelõs lifetime is but a few years, the similarity of the architecture ofvarious generations of hardware provides an opportunity for systems andapplication software to be developed and to mature. cray1 serial number one was delivered in 1976 with no compiler and an extremely primitive operating system. twenty years later, there were good vectorizingcompilers, reliable and efficient operating systems, and thick books thatcatalogued the hundreds of commercial application software packagesavailable on the cray vector machines.the excellent access to memory of such highbandwidth systems andthe availability of good optimizing compilers and reliable libraries tunedto the systemõs architecture can yield much higher performance (by a factor of as much as 30) on some applicationsñand with less programmingeffortñthan can commodity clusters. even though less programming effort may be required, expertise is still quite important. people who weregetting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.supercomputing infrastructures and institutions163proficient in the efficient use of cray vector computers became valuedmembers of projects that used those systems, and small consulting firmswere established to sell such expertise to groups that did not have it. theexistence of a cadre of people with the expertise to use a particularsupercomputer family further strengthens the corresponding ecosystemand adds to its longevity.one can see the benefits of these two ecosystems in the more recentearth simulator. as the committee observed in chapter 5, the earth simulator has three kinds of parallelism, requiring multiple programmingmodels to be used in an application. however, once the applications developer does multilevel problem decomposition, there is an hpf compiler that performs vectorization, sharedmemory parallelization, and distributedmemory communication, thus partially shielding the applicationprogrammer from those issues.more recently, commodity cluster systems that use open, de facto software standards have become a significant component of the highendcomputing scene, and modest ecosystems are forming around them. inmany cases, cluster hardware is based on microprocessors that use theintel x86 instruction set, providing some stability for the hardware andnode architecture. open source software suited to large clusters and highend computing is becoming available, such as versions of the linux operating system, parallel file systems such as pvfs, messagepassing libraries such as mpich, and visualization toolkits such as vtk. reinforcingthe trend toward clusters are factors such as these:¥the low entry cost, which enables even small university groups toacquire them;¥their proliferation, which provides a training ground for manypeople, some of whom will use them as a development platform for software tools, libraries, and application programs, thus adding technologiesto the ecosystem;¥the local control that a group has over its cluster, which simplifiesmanagement and accounting;¥the relative ease of upgrades to new processor and interconnection technologies; and¥their cost effectiveness for many classes of applications.software challenges remain for the nascent cluster ecosystem. sincecommunication is more expensive on clusters, minimizing communication plays a big role in achieving good performance. the distribution ofdata and computation across nodes has to be planned carefully, as part ofalgorithm design, particularly because there is no hardware support forremote data access and because load balancing is more expensive. tasksgetting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.164getting up to speedthat are done by hardware in a highbandwidth system have to be done insoftware in a cluster. current vector systems further reduce programmingeffort because they have highquality compilers available, facilitating theimmediate use of legacy codes. compilers for clusters have a much shorterhistory of development and are much less mature, even when they usevectorization technology.the computer industry has shifted over the last decades from a modelof vertically integrated systems to one of horizontally integrated systems.in a vertically integrated model, vendors develop the technology requiredacross most of the layers of the system. thus, ibm designed and manufactured almost everything associated with a mainframe: it designed andmanufactured the chips and their package, the disks, the operating system, the compilers and tools, the databases, and many other specializedapplications. the same was largely true of supercomputing manufacturers such as cray. today the dominant model is that of horizontal integration. thus, intel designs and manufactures chips, microsoft develops operating systems and applications running on those and other chips, anddell integrates both into one system. the same evolution has happenedwith highend systems. cray is still responsible for the design of mosthardware and software components of the cray x1; most of the value ofsuch a product is produced by cray. on the other hand, cluster systemsare based on integration of technologies provided by many vendors, andthe cluster integrators contribute only a small fraction of the productvalue.a vertically integrated, vendordriven ecosystem has the advantagesthat there are fewer variables to contend with and that there is centralizedcontrol of the hardware architecture and most of the software architecture. if the supercomputer vendor is successful financially, then commercial applications software is likely to emerge, lowering the barriers foruse. on the other hand, a vertically integrated ecosystem might becomefragile if the vendor encounters financial difficulties, switches to a newhardware architecture, or abandons its own proprietary operating systemas a result of increasingly high costs. the tight integration that (1) ensuredsmooth functioning of the software on the hardware and (2) enabled thedevelopment of proprietary features that application developers and users came to rely on will now make it much more expensive to transition toanother system, either from the same vendor or a different one.horizontal integration can provide a less arduous migration pathfrom one supercomputer platform to another and thus a longerlived,though less tightly coupled, ecosystem. those advantages are gainedthrough the use of portable software environments and less reliance onthe highly specific characteristics of the hardware or proprietary vendorsoftware. such portability has its costña smaller fraction of the potentialgetting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.supercomputing infrastructures and institutions165performance will be achieved, perhaps much smaller. a second disadvantage of the horizontal integration approach is that the various software components will not have been designed to be used in concert withone another; independently designed and implemented components willbe less compatible and the integration less cohesive and more fragile. thefragility results from the independence of the efforts that produce eachcomponent; changes may be made to one component without consideringwhether the new version will still interface with other software in theecosystem.in a horizontal market, the role of integrators that assemble the various technologies into a coherent product becomes more important. suchintegrators (for example, linux netwox3) have appeared in the clustermarket, but their small size relative to that of the makers of the components they assemble implies that they have little clout in ensuring thatthese components fit well together. furthermore, an integrator may nothave the scope to provide the kind of ongoing customer support that wasavailable from vertically integrated companies.an example of a vertically integrated ecosystem that did not survivefor very long is the thinking machines cm5, a product of thinking machines corporation (tmc). the cm5 had a unique proprietary architecture and a software environment with highly regarded components: languages, compilers, mathematical software libraries, debuggers, and soforth. the largest cm5 configuration was worldleading for a while.when tmc went out of business, users had to migrate to different systems that had less sophisticated software as well as different hardwarearchitecture. one component of the ecosystem that adapted quickly todifferent systems was the group of highly skilled tmc employees. manyof the people who produced the cm5 software environment were hireden masse by other computer companies because of their expertise, although their new employers have not attempted to produce as sophisticated an environment for their own systems.message passing libraries are an example of software technology thatcan stimulate the evolution of an ecosystem around an architecture family, in this case, processormemory nodes connected by a network of somesort. message passing has been the dominant programming model forparallel computers with distributed memory since the mid1980s. clusters fall into this class of computers. while the functionality provided indifferent systems was similar, the syntax and semantics were not. as earlyas the mid1980s there were attempts to develop de facto standards for3see <http://www.laurentconstantin.com/en/netw/netwox/>.getting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.166getting up to speedmessage passing libraries, but it was not until the mpi effort in 19931994that a standard was developed and was widely adopted. it is worth noting that argonne national laboratoryõs mpich was the first availableimplementation of the mpi standard and served as the basis for almost allmpi libraries produced by computer manufacturers. the wide availability of robust, open source mpi implementations was an important factorin the rapid and widespread proliferation of the commodity clustersupercomputer.potential barriers for new ecosystemseach ecosystem has a critical mass below which it cannot survive; if itis too close to the critical mass, there is a high risk that a catastrophicevent may wipe it out. a supercomputing ecosystem that is too small isnot economically viable and cannot evolve fast enough to compete. evenif it is viable, but barely so, a few wrong decisions by company managersor national policy makers may destroy it. the demise of companies suchas tmc or kendall square research and the near demise of cray clearlyillustrate these points.there are good reasons to believe that the critical mass needed to sustain a computer ecosystem has increased over the last decades. computersystems have become more complex, so they are more expensive to develop. this complexity arises at all levels of current systems. it has beenasserted that the development cost of intel microprocessors has increasedby a factor of 200 from the intel 8080 to the intel p6. operating systems,compilers, and libraries are larger and more complexñthe code size ofoperating systems has grown by two orders of magnitude in two decades.application codes are larger and more complex, in part because of thecomplexity of the platforms on which they sit, but mostly because of theincreased sophistication of the applications themselves. the increases canbe explained, in large part, by the growth of the computer industry, whichcan support and justify larger investments in computer technologies. asmall supercomputing niche will not be able to support the developmentof complex hardware and software and will be handicapped by a lack ofperformance or function relative to commodity computer products. as aresult the critical mass of a viable supercomputing ecosystem is greaterthan it once was. this is an obstacle to the establishment of new supercomputing ecosystems.horizontal integration is another obstacle to the establishment of newecosystems. it is not only that small agile vessels have been replaced bylarge supertankers that are hard to steer; they have been replaced by flotillas of supertankers that sail in tight formation and that all need to besteered in a new direction in order to achieve change.getting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.supercomputing infrastructures and institutions167horizontal integration admits more specialization and provides alarger market for each component maker, thus allowing more r&d investments and faster technology evolution. this has clearly benefited themainstream computer industry, in which vertically integrated servers,such as ibm mainframes, have become an anomaly. on the other hand,horizontal integration solidifies the boundaries across components andtechnologies provided by different vendors. systemlevel changes thatrequire coordinated changes of multiple components provided by multiple vendors are less likely to occur. in other words, one trades off fasterprogress within a paradigm (the paradigm defined by agreedupon interfaces at the various layers) against more frequent changes of the paradigm.there are many examples of the difficulty of effecting such coordinated changes. for example, softwarecontrolled cache prefetching is awellknown, useful mechanism for hiding memory latency. the instruction sets of several modern microprocessors have been modified to support a prefetch instruction. however, to take advantage of software controlled prefetching, one needs significant changes in compilers andlibraries, and perhaps also application codes. as long as prefetch instructions are unavailable on the large majority of microprocessors, compilerwriters and, a fortiori, application writers have limited incentives tochange their code so as to take advantage of prefetching. as long as software and applications do not take advantage of prefetch instructions, suchinstructions add to hardware complexity but bring no benefit. therefore,these instructions have been allowed, in one or more cases, to wither intoa noop implementation (i.e., the instructions have no effect), since thatsimplifies the microprocessor design and does not affect perceived performance. software controlled prefetching did not catch on, because onecould not coordinate multiple microprocessor designers and multiplecompiler providers.there are clear historical precedents for vertically integrated firmssuccessfully introducing a new design (for instance, the introduction ofthe cray), as well as many examples of such ambitions failing (which areamply documented in this report). in the present horizontal environmentit is difficult for entrepreneurs with the ambition to effect radical changesto coordinate the changes across many areas of computing. instead,todayõs supercomputing entrepreneurs tend to accept many componentdesigns as given and try to improve in their own niche. thus, a particulartype of innovative design that might benefit supercomputing users doesnot get done. it is conceivable that the benefit of faster progress on eachcomponent technology (e.g., faster progress on commodity microprocessors) more than compensates for the lack of coordinated changes across aspectrum of technologies. however, it is also conceivable that supercomgetting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.168getting up to speedputing is stuck in a local optimum and is unable to move out of it becauseof the high costs and high risks and the small size of the market (andhence the small size of the reward even if a better global design point canbe reached).the biggest barrier to the evolution of a supercomputing ecosystemaround a new computing platform is the effort and time required to develop the necessary software (for performance especially, but also forfunctionality). new software does not get used until it is refined to thepoint of actual productivity. (fortran 90, for example, may not yet havereached that point. some widely used applications still use fortran 77,even though f90 compilers are widely available, because there are stillvendors with significant market share whose f90 compilers produce significantly slower executables than their f77 compilers.) the generatedcode must be efficient for supercomputing applications; achieving thatefficiency requires large investments and years of evolution as experienceis gained.if a new language is involved, the barrier is even higher. adoptionwill be slow not only until there are reliable compilers that produce efficient code but also until that language is available on systems from a number of vendors. until the new language is widely adopted there is littleincentive for vendors to support it, thus creating a chickenandegg problem. this is another instance of the barriers introduced by standardizedinterfaces, this time the interface between application designers and thesystem on which they run their application. for example, while the limitations of mpi as a programming model are widely acknowledged, theasc program leadership has expressed little enthusiasm for architecturalimprovements that would require forfeiting the use of mpi, because ofthe large existing investment in mpi codes and the likelihood that mpiwill continue to be needed on many of the asc platforms.the barriers to acceptance extend to algorithms as well. users areoften justifiably wary about replacing a known and trusted, if suboptimal, algorithm with a new one. there need to be better quality vettingand communication mechanisms for users to discover and evaluate newalgorithms.users resist migrating to a new system, either hardware or software,that may survive only a few years; most supercomputing application programs evolve and are used over decades. in other words, another barrierto be surmounted is the need to guarantee longevity. computers withdifferent architectures from sources new to supercomputing may not besuccessful in the marketplace and thus will no longer be available. thiscan be true even for new models from established vendors. the effortrequired to adapt most supercomputer application programs to new environments is substantial. therefore, code developers are reluctant to ingetting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.supercomputing infrastructures and institutions169vest the effort to migrate to different systems, even when they are likely todeliver a better than twofold increase in absolute performance or in cost/performance. codes will run for decades, so developers are risk averse.the effort required to port applications to new systems is a majorbarrier as well. few supercomputer users can afford to halt their primaryactivity for months in order to move their application software to a newenvironment. they need to finish their papers or their ph.d. dissertationsor to produce a result for their mission within a fixed time frame. the costto the end user of introducing new systems must be reduced. that is quitedifficult to accomplish while preserving high performance.wider use of technologies for more automated code generation andtuning will also facilitate migration between ecosystems. a vision of thefuture is to develop supercomputing ecosystems that are broader andmore general, so that they can support a variety of supercomputers withdifferent characteristics. this would have the advantages of economy ofscale, more years for system software and tools to mature, and a largerbase of installations to pay for the continued enhancement of the software. users of new supercomputers that fit into one of those ecosystemswould also benefit by not having to learn everything new (although newmachines will always have differences from previous generations of machines that affect their use and performance). open source software thatcan be ported to a variety of systems might be able to engender thosemore general supercomputing ecosystems. in this scenario, many aspectsof the operating system and tools could be made to improve monotonically over time, and much of the software could be reused in new machines. parallel file systems are a case in point. projects to develop themrequire specialized expertise and take years to complete. open source attempts such as pfvs4 and lustre5 to develop an open, fairly portable parallel file system may eventually reduce the effort required to provide aparallel file system for new platforms.similarly, one can envision strategies for application programs thatwould lower the barriers for new supercomputing ecosystems to evolve.an example is the relatively new type of application programs known ascommunity codes. these programs address a particular class of applications such as chemical reactions (nwchem)6 or climate modeling(ccsm).7 because community codes are implemented by large teams, often at different institutions and having complementary expertise, they are4see <http://www.parl.clemson.edu/pvfs/>.5see < http://www.lustre.org/>.6see <http://www.emsl.pnl.gov/docs/nwchem/nwchem.html>.7see <http://www.ccsm.ucar.edu/>.getting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.170getting up to speedcarefully designed for modularity (so that better models and algorithmscan be inserted with moderate effort) and portability (so that porting themto new systems is not too onerous). their modularity promises to reducethe effort to migrate the code to different platforms, since relatively fewmodules may need to be redesigned to use different algorithms orreimplemented to exploit new hardware features. designing and developing such programs is still a challenging research topic, but there aresuccess stories (nwchem, ccsm, sierra,8 pyre,9 and others).a related approach is to develop higher level codes or parameterizedcodes that can run well on a broader range of platforms, codes that areprogrammed for performance portability. such codes would adapt to thechanges in hardware parameters that result from different exponents inthe rate of change of different technologies, such as the much faster increase in processor speeds than in memory speeds. the ratios of hardware component speeds determine to a large extent the performance thatis achieved. automated library tuning and domainspecific code generators are discussed in chapter 5. although this is a good research topic, itis hard (it has been around for some time). we do not yet do a goodenough job mapping higherlevel programming languages onto onesingletarget platform; it is even more difficult to map them well onto abroad range of platforms.ecosystem workforceas has already been stated, one of a supercomputing ecosystemõsmost important investments is its investment in people. the technology ismaintained, exploited, and enhanced by the collective knowhow of a relatively small population of supercomputing professionalsñfrom thosewho design and build the hardware and system software to those whodevelop the algorithms and write the applications programs. their expertise is the product of years of experience. as supercomputing becomes asmaller fraction of research and development in information technology,there is a greater chance that those professionals will move out ofsupercomputingrelated employment into more lucrative jobs. (for example, their systems skills could be reused at google10 and their applications/algorithms skills would be useful on wall street.) in companies such8see <http://www.sandia.gov/asci/apps/sierra.html>.9see <http://www.cacr.caltech.edu/projects/pyre/>.10google has been aggressively recruiting computer science graduates with advanced degrees and advertising openings at top conferences, such as the international symposium oncomputer architectures, the top computer architecture conference.getting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.supercomputing infrastructures and institutions171as ibm, hewlettpackard, and sun, which design and build not onlysupercomputers but also other commercial systems, people with uniqueskills could migrate to those other parts of the company in response tochanges in company priorities or in pursuit of personal opportunities. ifthe funding stream of an academic or national laboratoryõs supercomputercenter is unstable (e.g., nasa or nsfõs partnerships for advanced computational infrastructure (paci)), their professionals will seek greener,more stable pastures elsewhere, often outside supercomputing. as seniorprofessionals move out of supercomputing, it becomes harder to maintain the knowledge and skill levels that come from years of experience.at the other end of the people pipeline are the graduate students whowill eventually become the next generation of senior supercomputing researchers and practitioners. according to the 20012002 taulbee survey ofthe computing research association (the most recent survey available), atotal of 849 ph.d. degrees were awarded in 2002 by the 182 respondingcomputer science and computer engineering departments, the lowestnumber since 1989. of the 678 reporting a specialty area, only 35 were inscientific computing, the smallest total for any specialty area. (the nextsmallest group was 46, in programming languages/compilers, while thelargest was 147, in artificial intelligence/robotics.) a review of researchgrants and publications in computer architecture shows a steady decreasein the number of grants and publications related to parallel architectures,leading to a decrease in the number of ph.d. dissertations in computerarchitecture research that is relevant to hpc. for example, while nsf cisefunded about 80 grants a year in areas related to parallel processing in themid1990s, that number had shrunk to about 20 a year at the beginning ofthe 2000s.11 during the same period, the number of papers containingterms such as òsupercomputer,ó òparallel computing,ó òhighperformance computing,ó and òparallel architectureó shrank by a factor of 2 ormore.12of course, many of the new ph.d.õs entering the supercomputingemployment market receive degrees from departments other than computer science and computer engineering departments (for example, aerospace engineering, mechanical engineering, chemistry, physics, molecular biology, and applied mathematics), so the true number of new peopleentering the field is difficult to know. there are a handful of interdisciplinary graduate or certificate programs targeted specifically at educatingthe nextgeneration supercomputing professional, for example princetonõs11number of new grants matching the keyword òparalleló and funded by cise.12based on a search of the inspec publication index.getting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.172getting up to speedprogram in integrative information, computer and application sciences(picasso) (http://www.cs.princeton.edu/picasso) and the computational science and engineering program at the university of california,santa barbara (http://www.cse.ucsb.edu/index.html), both of which arefunded, in part, by nsf igertõs computational science and engineeringgraduate option program at the university of illinois at urbanachampaign (http://www.cse.uiuc.edu/) and pennsylvania stateuniversityõs graduate minor in computational science and engineering(http://www.psu.edu/dept/ihpca). such programs are typically open tointerested graduate students from a wide range of science and engineering majors.13 doeõs computational science graduate fellowship andhigh performance computer science fellowship programs support approximately 80 graduate students a year (http://www.krellinst.org/work/workhome.html). these programs promise to help increase the supply of new supercomputing professionals, although the committee believes that some faculty members, like faculty in other interdisciplinaryfields, may have concerns about their career paths. (the academic valuesystem often fails to reward òmereó code developers with tenure even ifthey are critical to a project and have made important computational orcomputer science advances.) even with these programs, the supply of newpeople well educated in computational science is rather small and may bebelow the replacement rate for the current population.to maintain knowledge and skills at the senior level, it is important tomake sure that incentives are provided to keep the senior professionals inthe field. the first step is to determine the key institutions (from academia,industry, and the national laboratories) that are the repositories of thisinstitutional memory. often the software is built by a team of academicresearchers, national laboratory employees, and government agency staff.next, strategies must be developed that provide these institutions withthe mission and the stability necessary to retain supercomputing professionals. also important is enough flexibility in the supercomputing ecosystem so that people can move within it as the money moves.the key institutions in academia have been the nsf centers and partnerships (currently with leadingedge sites at illinois, san diego, andpittsburgh and with partners at many universities) that together providea national, highend computational infrastructure for academic super13more information on computational science and engineering graduate programs can befound in siamõs working group on cse education, at <http://epubs.siam.org/sambin/dbq/article/37974>. information on the elements that make up computational science andengineering education can be found at <http://epubs.siam.org/sambin/dbq/article/40807>.getting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.supercomputing infrastructures and institutions173computing researchers and for advanced education in supercomputing.the paci program evolved out of the nsf supercomputing program.through these programs nsf has provided a reasonably stable fundingbase for academic supercomputing infrastructure (equipment and professionals) for over 15 years. the centers have brought together computerscientists, computational scientists, and scientists from a broad array ofdisciplines that use computer simulations, together with their researchstudents, promoting fertile interdisciplinary interaction. however, nsffunding for the paci program stayed flat despite major increases innsfõs budget. the paci program ended in september 2004, and the formand level of future support are uncertain. a recent report from the nsfadvisory panel on cyberinfrastructure14 makes numerous recommendations for ways to continue to provide supercomputing infrastructure forthe academic community. for instance, the report says, òsubject to appropriate review, we anticipate that they [the pacis] will play a continuing but evolving substantial role in the greatly enlarged activity wepropose.ó however, funding for nsfõs cyberinfrastructure program ispending. the leadingedge sites will receive some funding for the next 3years, but no plans for technology refresh have been announced, andfunding for the partners at other sites has been discontinued. both thepartnerships and the leadingedge sites are already in danger of losingkey senior professionals.industrial institutions that have had the most success in keeping theirprofessional employees are those that are specialized and physically located where there is little or no competition (for instance, cray). keepingtheir supercomputing professionals is easiest for those national laboratories and institutions (for instance, lanl or the institute for defenseanalyses (ida)) that have a longterm commitment to particular applications for which they have unique or nearunique responsibilityñthatis, those laboratories and institutions whose òmissionó is protected. however, even in those organizations, uncertainties surrounding fundingcan cause professional employees to look elsewhere for perceived jobsecurity.while it is important to keep senior professionals in the field, it is alsoimportant to continue to produce nextgeneration professionals. fundingmodels that encourage and support the education of the next generation,as well as those that provide the supercomputing infrastructure needed14daniel e. atkins. 2003. revolutionizing science and engineering through cyberinfrastructure:report of the national science foundation blueribbon advisory panel on cyberinfrastructure.january.getting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.174getting up to speedfor their education, are necessary. it is also important that students preparing for a career in highperformance computing have confidence thatattractive employment opportunities will continue to exist.consumer institutionsconsumers of supercomputers can be roughly divided into nationallaboratory and academic researchers and commercial users. supercomputing òcentersó have evolved for the use of national lab and academicresearchers. these centers provide access to supercomputers and support(socalled cycle shops), or they can offer a fuller complement of services,including advancing the state of the art in supercomputing software andworkforce infrastructure. some (e.g., ncar or the doe weapons laboratories) are targeted at a single application domain; others (e.g., nsfõs paciand doeõs nersc) serve multiple domains. supercomputing exists inforprofit companies when it can give them a competitive advantage.15supercomputing centerssupercomputing centers provide a community of users, sometimesfrom the same organization and sometimes not, with shared access to oneor more supercomputers. a center normally employs professional staff tohelp run the installation as well as to help users run and improve theirapplication codes to best effect. supercomputing centers are typicallyhoused in specialpurpose facilities that provide the needed physicalplant, notably floor space, structural support, cooling, and power. theyalso provide working space for the operational staff. thanks to theinternet, users normally need not be physically present.the computing infrastructure provided by a center includes morethan computing hardware. typically, the users also share access to licensed or purchased software and, increasingly, to very large quantitiesof archival data. thus a supercomputing center leverages its investmentby gaining enough users to keep the system in constant use, by using thesystem well, and by sharing essential software, data, and expertise thatfacilitate the applications. most centers also provide expertise on effectiveuse of the supercomputers and software packages they support, throughconsulting and training services and occasionally by loaning programmers with relevant expertise to the application projects.the primary organizations that provide supercomputing centers aregovernment mission agencies such as the department of energy and the15cf. chapter 4.getting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.supercomputing infrastructures and institutions175department of defense, and the national science foundation. somecenters are sponsored directly by universities; some are managed by companies. the main purpose of a mission agency center is to support thecomputing related to its mission. the users that support that missionsometimes come from outside the agencyñfor example, from academiaor from companies that contract with the agency. in most instances, a supercomputing center is part of a larger organization that includes researchers who use the computational facilities, computational science software developers, and education and traininggroups. having local users provides continuing dialogue for improvingthe centerõs offerings and provides the justification for the host institutionto house the facility. having inhouse software developers also facilitatesbetter use of these systems.the committee met with center directors from both nsf and doe(see appendix b). the center directors described a variety of difficultiesthey have had in planning supercomputing procurements, in ensuringthat users can take full advantage of capability systems, and in balancingpresentday needs against future demands.¥centers are often under pressure to raise funds to cover both operating costs and technology refresh. center directors find it difficult to dolongrange planning in light of the yeartoyear uncertainties that surround both capital and operating budgets.¥centers are under pressure to use capability systems for capacitycomputing: (1) to respond to required measures of usage (such as havinglarge numbers of jobs run and servicing large numbers of distinct users),(2) to satisfy influential users with noncapability needs, and (3) to makeup for the lack of adequate capacity availability. such use is increasinglyhard to justify, in an era where capacity can be provisioned using cheapdepartmental clusters. supercomputing centers have attempted to lessenthe severity of this problem by developing software that facilitates theestablishment and maintenance of departmental clusters. suitably usedgrid computing infrastructure should further facilitate this shift.¥nsf centers in particular have experienced mission creepñtheyare expected to move into new areas such as very high capacity networking, grid computing, and so on without adequate additional funding oradequate consideration of the effect on their capability computing responsibilities. the expectations come both from users and from external program reviewers.¥procurement is both very expensive and somewhat prolonged. because of the time lags, it is speculative, in the sense that the deliveredsystem may not meet expectations or requirements. (it is also expensiveand difficult for the suppliers.) procurement overheads cannot be amorgetting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.176getting up to speedtized over multiple platform acquisitions, and current processes do notfacilitate the creation of longterm relationships with a vendor.industrial supercomputingin the context of this report, industrial supercomputing refers to thepurchase and use of a supercomputer by a forprofit corporation. supercomputers give commercial users capabilities similar to those that theygive to defense and other government researchers. they enable scientistsand engineers to study phenomena that are not readily observable such asthe transient dynamics of semiconductor switching. commercial supercomputers allow relatively inexpensive simulations to replace costlyexperiments, saving both time and money. an example is the crash testing of automobiles. another driver for the commercial use of supercomputers is government regulations. examples include the structural analysis of airplane frames, nox/sox analysis for combustion engines, andelectromagnetic radiation for electronic devices.supercomputers can offer companies a competitive advantage by, forinstance, enabling the discovery of new drugs or other technologies, resulting in lucrative intellectual property rights. accurately modeling theyield of an oil field can impact lease prices by hundreds of millions ofdollars. engineering analysis can also allow reducing the cost ofprototyping new products and reducing the time to market.many of todayõs commercial supercomputer applications were pioneered by scientists and engineers working on problems of great nationalimportance. over time, the technology they developed was transitionedto other uses (e.g., nastran,16 kiva17). as mooreõs law steadily reducedthe cost and increased the performance of computers, a problem that wasfirst only tractable as a critical nationalscale problem then became approachable for a large corporation, then for an engineering department,and eventually for anyone with a desktop computer. this is not to suggestthat industry no longer needs supercomputers. as pointed out in chapter4, industrial users not only are making more extensive use of highperformance computing than ever before, but they are also making more use oflowend supercomputers than ever before. just as in defense and science,new problems arise that require increasingly higher fidelity and shorterturnaround times. john hallquist, for one, contends that a 107 increase in16see <http://www.mscsoftware.com/products/productsdetail.cfm?pi=7>.17see <http://www.eere.energy.gov/vehiclesandfuels/pdfs/success/modelofcombprocessmar2001.pdf>.getting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.supercomputing infrastructures and institutions177delivered computing power could be used for crash testing. however, itseems that the rate at which new codes are being developed or existingcodes are being scaled up has slowed down. one way to help understandsupercomputing use by the commercial sector is to partition it accordingto the application/industry sector. another way is to partition it by market sectorñconsumer or capital equipment or government/defense industry. there are users in many fields who have applications that theywould like to run on larger data sets, with less turnaround time. theseusers constitute the potential commercial market for supercomputers.figure 6.1 shows the relative share of the various sectors of the technical computing market in 19982003, the importance of the scientific research and classified defense sectors, the relative growth of new sectorssuch as biosciences, and the relative stability of sectors such as mechanical engineering. it also shows that no market is so large as to dominate all0%10%20%30%40%50%60%70%80%90%100%199819992000200120022003othertechnical management andsupportsimulationscientific research and r&dmechanicaldesign/engineering analysismechanical design anddraftingimaginggeoscience and geoengineeringelectrical design/engineeringanalysiseconomics/financialdigital content creation anddistributionclassified defensechemical engineeringbiosciencesfigure 6.1revenue share of industry/applications segments, 19982003.source: earl joseph, program vice president, highperformance systems, idc;email exchanges, phone conversations, and inperson briefings from december2003 to october 2004.getting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.178getting up to speed$0$1b$2b$3b$4b$5b$6b$7b0%10%20%30%40%50%60%hpc private sectorhpc public sectorfraction public199819992000200120022003figure 6.2worldwide hpc market. source: earl joseph, program vice president, highperformance systems, idc; email exchanges, phone conversations,and inperson briefings from december 2003 to october 2004.others. as a result, large computer manufacturers have to develop systems that perform well on a very broad range of problems. this maximizes the potential return on investment when developing a product buthas the unfortunate effect of delivering suboptimal performance to thedifferent end users.figures 6.2 and 6.3 show the evolution of the worldwide technicalcomputing market from 1998 to 2003. they indicate that the overall size ofthe market is about $5 billion, with less than $1 billion being spent oncapability systems. the market exhibits significant fluctuations; the capability segment has been moving 10 or 20 percent up or down almost everyyear. supercomputing vendors are hampered both by the small size of thehighend market and by the large yeartoyear variations. the charts alsoindicate the significant impact of public acquisitions on this marketñover50 percent of the hpc market is in the public sector, as is over 80 percentof the capability market. public sector purchases are very volatile, withlarge changes from year to year.industrial use is changing, and for reasons of competitive advantage,that industrial use is often not revealed. in fact, the ability of small groupsgetting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.supercomputing infrastructures and institutions179to assemble pc clusters from existing office equipment means that today,managers of large commercial enterprises are often unaware of thesupercomputers within their own companies. the overall decline in thetechnical computing market indicated by these charts may be due to thiseffect.$0$200m$400m$600m$800m$1,000m$1,200m1998199920002001200220030%10%20%30%40%50%60%70%80%90%capability privatesectorcapability publicsectorfraction publicfigure 6.3worldwide capability markets. source: earl joseph, program vicepresident, highperformance systems, idc; email exchanges, phone conversations, and inperson briefings from december 2003 to october 2004.getting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.1807supercomputing abroadthe committee devoted most of its attention to supercomputing inthe united states. a subcommittee made a visit to japan in march2004, but there were no visits to other countries. however, mostcommittee members have significant contact with supercomputing experts in other countries, and there is considerable literature about supercomputing activities abroad. a very useful source is a recent survey bythe organisation associative du parallelisme (orap)1 in france. the committee drew on all those sources when it considered the state of supercomputing and its future abroad.supercomputing is an international endeavor and the research community is international. many countries have significant supercomputinginstallations in support of science and engineering, and there is a significant exchange of people and technology. however, the united statesclearly dominates the field. of the top500 systems in june 2004, 255, or 51percent, are installed in the united states, which also has 56 percent of thetotal compute power of the systems on that list. the next country, japan,has 7 percent of the systems and 9 percent of the total compute power. asfigure 7.1 shows,2 this situation has not changed significantly in the last1orap. 2004. promouvoir le calcul haute performance 19942004.2in contrast to the data presented in chapter 3, figure 3.7, which are based on manufacturer, the data in figure 7.1 present the percent of worldwide supercomputing systems thatare installed in a given country regardless of manufacturer. this figure was generated at thetop500 web site.getting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.supercomputing abroad181decade: no particular trend emerges, except the progressive broadeningof the òotheró category, indicating the progressive democratization ofsupercomputing, attributable to the advent of relatively low cost commodity clusters.the dominance is even more striking when one looks at manufacturers: 91 percent of the top500 systems are manufactured in the unitedstates (see figure 3.7). many of the remaining systems use u.s.manufactured commodity parts. the software stack of supercomputing systemsused worldwide (operating systems, compilers, tools, libraries, application codes, etc.) was also largely developed in the united states, with significant contributions from researchers in other countries.however, this is no reason for complacency. since late 2001, the system that heads the top500 list has been the earth simulator (es), installedin japan. even more important than being the most powerful system, thees, because of its use of custom vector processors, achieves higher sustained performance on application codes of interest than many of the othertopperforming machines. while the es is likely to lose its top position on05010015020025030035040045050006/199311/199406/199611/199706/199911/200006/200211/2003otherschinaitalyfranceunited kingdomgermanyjapanunited statesfigure 7.1top500 by country.getting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.182getting up to speedthe top500 list soon,3 it is likely to continue providing significantly betterperformance than competing systems on climate codes and the other applications it runs. (at present, the es is 5 to 25 times faster than largeu.s. systems on the various components of climate models used atncar.) idc estimates that in the last few years, the north america, europe, and the asianpacific regions each purchased about onethird of thetotal dollar value of the capability systems sold. another trend that hasbeen much in evidence in recent years is the ability of many countries tobuild topperforming systems using commodity parts that are widelyavailable. this reduces the usefulness of export restrictions and enablesmany countries to reduce their dependence on the united states and itsallies for supercomputing technology. china is vigorously pursuing apolicy of selfsufficiency in supercomputing.next, the committee presents highlights of supercomputing activitiesin various countries.japanthe committee found both similarities and differences insupercomputing in japan and in the united states.4similaritiesin many areas the issues and concerns about hpc are broadly similarin japan and in the united states. hpc continues to be critical for manyscientific and engineering pursuits. many are common to the united statesand japan, for example, climate modeling, earthquake simulation, andbiosystems. however, japan does not have the kind of defense missions,such as stockpile stewardship, that have historically been drivers for u.s.supercomputing.the hpc community is small in both countries relative to the scienceand engineering community overall and may not have achieved a criticalmassñin both countries it is hard to attract top young researchers withthe needed skills in simulation and highperformance computing. the3on september 29, 2004, ibm announced that the blue gene/l system, which is beingassembled for llnl, had surpassed the performance of the earth simulator according to thestandard linpack benchmark. on october 26, 2004, silicon graphics announced that thecolumbia system installed at nasa ames had surpassed the earth simulator. as a result, itis expected that the earth simulator will lose the top spot on the november 2004 top500 list.4the subcommittee participated in a 1day joint naeðengineering academy of japanforum and visited six supercomputing sites in japan (see appendix b for a complete list ofsites and participants).getting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.supercomputing abroad183committee had a lively technical exchange at the 1day joint forum, whereits members learned of several japanese research projects with which theyhad not been familiar. more international collaboration on research wouldclearly be beneficial to both countries.the commercial viability of traditional supercomputing architectureswith vector processors and highbandwidth memory subsystems is problematic. commodity clusters are increasingly replacing such traditionalsystems and shrinking their market. it has become harder to identify attractive payoffs for investments in the development of vector architectures. the large investments needed to continue progress on custom hpcsystems, as well as the opportunity costs, are increasingly difficult to justify. however, at least one large company in japan (nec) continues to becommitted to traditional vector architectures.continuity is a problem in both countries. the es project was officially proposed in 1996 and started in 1997,5 at a time when japanõseconomy and politics were different. in the current japanese economicand political climate, it has become harder to allocate significant funds ona continuous basis for large, innovative projects in hpc. similar pressures exist in the united states.hpc usage is also constrained in both countries by the lack of suitablesoftware and by the difficulty of using less expensive machines with lowermemory bandwidth.differencesthere were some notable differences between the united states andjapan. traditional supercomputer architectures (vector, pseudo vector,etc.) play a larger role in japan. top nec, fujitsu, and hitachi machinesare still the mainstay of academic supercomputing centers and nationallaboratories. as a result, there is more reliance on vendorprovided software than on thirdparty or open source software, which is less available.however, the trend is toward increased use of clusters and open sourcesoftware. also, since japan does not have a military rationale for hpc, ithas to be justified on the basis of its ultimate economic and societal benefits for a civil society.the earth simulatorthe earth simulator was developed as a national project by three government agencies: the national space development agency of japan5see <http://www.es.jamstec.go.jp/esc/eng/es/birth.html>.getting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.184getting up to speed(nasda), the japan atomic energy research institute (jaeri), and thejapan marine science and technology center (jamstec). the es (seefigure 7.2) is housed in a specially designed facility, the earth simulatorcenter (approximately 50 m × 65 m × 17 m). the fabrication and installation of the es at the earth simulator center of jamstec was completedat the end of february 2002. the es is now managed by jamstec, underthe ministry of education, culture, sports, science and technology(mext).the system consists of 640 processor nodes, connected by a 640 by 640singlestage crossbar switch. each node is a shared memory multiprocessor with eight vector processors, each with a peak performance of 8gflops. thus, the total system has 5,120 vector processors and a peak performance of 40 tflops. most codes are written using mpi for global communication and openmp or microtasking for intranode parallelism. somecodes use hpf for global parallelism. as shown in figure 7.3, the sustained performance achieved by application codes is impressive: the esachieved 26.58 tflops on a global atmospheric code; 14.9 tflops on a threedimensional fluid simulation code for fusion written in hpf; and 16.4tflops on a turbulence code.the es, with its focus on earth sciences, was one of the first missionoriented projects of the science and technology agency.6 although thefigure 7.2 earth simulator center. this figure is available at the earth simulatorweb site, <http://www.es.jamstec.go.jp/esc/eng/es/hardware.html>.6mext took over the es after the merger of the ministry of education and the science andtechnology agency.getting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.supercomputing abroad185u.s. ncar is also missionoriented for earth sciences, it is perceived thatin the united states òmissionorientedó usually implies ònational security.ó the es also might turn out to be a singular event: mext officialswith whom the committee met stated that as of march 2004 there were noplans to build topical supercomputing centers in support of japanese priority science areas (biotechnology, nanotechnology, the environment, andit), nor were there plans to build a second es. tetsuya sato, directorgeneral of the earth simulator center, has plans for another very powerful system and is trying to marshal the necessary support for it. plans forresearch on technology for an es successor with 25 times the performanceof the es were recently announced.7the launch of the earth simulator created a substantial amount ofconcern in the united states that this country had lost its lead in high15001000500014001300120011009000800070006000400030002000100000100200300400500600gflopsnodeocean and atmospheresolid earthcomputational scienceepochmaking30%peakfigure 7.3 2002 earth simulator performance by application group.7according to an article in the august 27, 2004, issue of the newspaper nihon keizai, mextwill request ´2 billion (about $20 million) in fy 2005 to fund an industryuniversitygovernment collaboration on lowpower cpu, optical interconnect, and operating system. participants include nec, toshiba, and hitachi.getting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.186getting up to speedperformance computing. while there is certainly a loss of national pridebecause a supercomputer in the united states is not first on a list of theworldõs fastest supercomputers, it is important to understand the set ofissues that surround that loss of first place. the development of the esrequired a large investment (approximately $500 million, including thecost of a special facility to house the system) and a commitment over along period of time. the united states made an even larger investment inhpc under the asc program, but the money was not spent on a singleplatform. other important differences are these:¥the es was developed for basic research and is shared internationally, whereas the asc program is driven by national defense and may beused only for domestic missions.¥a large part of the es investment supported necõs developmentof its sx6 technology. the asc program has made only modest investments in industrial r&d.¥es uses custom vector processors; the asc systems use commodity processors.¥the es software technology largely comes from abroad, althoughit is often modified and enhanced in japan. for example, a significantnumber of es codes were developed using a japaneseenhanced versionof hpf. virtually all software used in the asc program has been developed in the united states.surprisingly, the earth simulatorõs number one ranking on thetop500 list is not a matter of national pride in japan. in fact, there isconsiderable resentment of the earth simulator in some sectors of the research community in japan. some japanese researchers feel that the es istoo expensive and drains critical resources from other science and technology projects. owing to the continued economic crisis in japan and thelarge budget deficits, it is becoming more difficult to justify governmentprojects of this kind.computing time on the earth simulator is allocated quite differentlyfrom the way it is done by nsf in the u.s. supercomputer centers. mostprojects are sponsored by large consortia of scientists, who jointly decidewhich projects are of most interest to the science community. the directorhas a discretionary allocation of up to 20 percent that can be used, forexample, to bring in new user communities such as industry or to supportinternational users. japanese private sector companies are permitted touse the resources of the governmentfunded supercomputer. (for example, auto manufacturers recently signed a memorandum of understanding for use of the es.)the machine cannot be accessed remotely, although that policy maygetting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.supercomputing abroad187change within japan. collaborators must be on site to run on the es. theymay not use the machine unless they can demonstrate on a small subsystem that their codes scale to achieve a significant fraction of peak performance. because of the custom highbandwidth processors used in esand the user selection policy, the codes running on the es achieve, onaverage, a sustained performance that is 30 percent of the peak. thus thesystem is used to advantage as a capability machine, but at the politicalcost of alienating scientists who are unable to exploit that capability. thereare several international collaborations being conducted at the es, including a joint effort between ncar and the central research institute of theelectric power industry (criepi), which involves porting and runningthe ncar ccsm on the es, and a joint research effort with scientistsfrom the california institute of technology in earthquake simulation.8other japanese centersother large supercomputer installations in japan are found in university supercomputer centers, in national laboratories, and in industry. inthe june 2004 top500 list, japan appears again in 7th place with a fujitsusystem at the institute of physical and chemical research (riken); in19th place with an opteron cluster at the grid technology research center at the national institute of advanced industrial science and technology (aist); in 22nd place with a fujitsu system at the national aerospacelaboratory (jaxa); and also further down the list. japanese manufacturers are heavily represented. commodity clusters are becoming moreprevalent.the university supercomputer centers were until recently directlyfunded by the government. funding was very stable, and each center hada longterm relationship with a vendor. the centers have been managedmostly as òcycleshopsó (i.e., centers that do not advance the state of theart but, rather, maintain the status quo) in support of a research user community. for example, at the university of tokyo center, the main applications are climate modeling and earthquake modeling. there appear to beless software development and less user support than the nsf centersprovide in our country.since april 1, 2004, universities in japan have been granted greaterfinancial autonomy. funds will be given to a university, which will decide how to spend the money. universities are being encouraged to emulate the american model of seeking support from and fostering collabora8see <http://www.es.jamstec.go.jp/esc/images/journal200404/index.html> for moreinformation on the caltech research project at the es.getting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.188getting up to speedtion with industry. this change could have a dramatic effect on existinguniversity supercomputing centers because the government will no longerearmark money for the supercomputer centers.there is widespread concern on the part of many in japan regardingthe quality of students. both industry and government agencies (such asjaxa) expressed concern that students have no practical experience. universities have been encouraged to provide more practical training anddecrease the emphasis on academic study. jaxa has a comprehensive 2to 3year program to train graduate students before hiring them; a constraint to participation is that the students are not paid while training.chinachina is making significant efforts to be selfsufficient in the area ofhighperformance computing. its strategy is based on the use of commodity systems, enhanced with homebrewed technology, in an effort to reduce its dependence on technologies that may be embargoed. china hadlittle or no representation on the top500 list until recently. it reached 51stplace in june 2003, 14th in november 2003, and 10th in june 2004. theaccumulated top500 performance has been growing by a factor of 3 every 6 months since june 2003. today, china has a cumulative performanceroughly equal to that of france, making it the fifth largest performer.the toplisted chinese system has a peak performance of 11 tflops. itis a cluster of 2,560 opteron multiprocessors (640 fourway nodes) connected by a myrinet switch. the system was assembled and installed atthe shanghai supercomputing center by the chinese dawning company.9this company markets server and workstation technologies developedby the chinese academy of science (casict), the national researchcenter for intelligent computing systems (ncic), and the national research center for high performance computers.another topranked system (in 26th place) is the deepcomp 6800, a1,024processor itanium cluster with a quadrics qsnet interconnect thatis used by the casict. the system was assembled by the chinese lenovogroup limited.10 casict is the main shareholder of lenovo, an important pc manufacturer.china is also developing its own microprocessor technology: thedragon godson microprocessor is a lowpower, mipslike chip; the god9see <http://www.dawning.com.cn>.10see <http://www.legendgrp.com>.getting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.supercomputing abroad189sonii runs at 500 mhz and consumes 5 w. dawning has announced plansto build clusters using this microprocessor.europecollectively, the european union countries had 113 of the top500systems as of june 2004; this amounts to 23 percent of the top500 listedsystems and 19 percent of their total compute power. however, it is notclear that one should treat the european union as a single entity. in thepast, the european union made significant coordinated investments inhpc research: the 19951998 fourth eu framework program for researchand technological development11 included 248 million for highperformance computing and networking (hpcn). however, hpc is not identified as a separate area in the fifth or sixth framework programs.12 thethematic areas are life sciences, information technology, nanotechnology,aeronautics and space, food quality and safety, sustainable development,and citizens and governance. while some of the funding under these headings supports the use of supercomputing systems, it is quite clear thathpc is driven in europe by national policies rather than eu initiatives.united kingdomthe united kingdom is the largest european supercomputer user,with two large academic centersñthe edinburgh parallel computing center (epcc) and the csar consortium at manchester. recently, it announced a large escience initiative with a total budget of £213 million.the budget funds a national center at edinburgh, nine regional centers,and seven centers of excellence. the escience vision promoted by thisinitiative is similar to the cyberinfrastructure vision promoted by theatkins report;13 it includes significant funding for supercomputers as partof a grid infrastructure.some u.k. users have recently moved from vector systems to commoditybased systems. the european center for mediumrange weatherforecasts, which was a major fujitsu user, now has an ibm power 4based11the fourth framework program is available online at <http://europa.eu.int/comm/research/fp4.html>.12the sixth framework program, the current program, is available online at <http://europa.eu.int/comm/research/fp6/pdf/howtoparticipateen.pdf>.13daniel e. atkins. 2003. revolutionizing science and engineering through cyberinfrastructure:report of the national science foundation blueribbon advisory panel on cyberinfrastructure.january.getting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.190getting up to speedsystem that was ranked 6th on the june 2004 top500 list. the centerõsoperational forecasts are carried out in ensembles of up to 100 simultaneous runs, which require large computing capacity rather than capability. (ncar in the united states has also moved to an ibm system, butunwillingly, as a result of the antidumping case against nec; see box 8.1.)on the other hand, many weather and climate centers, including the u.k.meteorology office and dkrz, the german hpc center for climate andearth system research, prefer to use custom sx6 systems with 120 and192 processors, respectively. epcc was a heavy cray t3e user and nowhosts the 18th place system (owned by the hpcx consortium); also, power4based csar deploys large shared memory machines with origin andaltix processors.an interesting aspect of u.k. hpc is the use of longterm contracts forprocurements. both epcc and csar have 6year service contracts withtheir platform suppliers that include an initial platform delivery and a 3year refresh. plans are made to allow such contracts to be extensible forup to 10 years, with periodic hardware refresh; 2year extensions can begranted subject to a òcomprehensive and rigorous review.ó14germanygermany has almost as many listed supercomputers as the unitedkingdom. many of the systems are hosted in regional centers that arelocally funded by provincial authorities and by federal programs. theremunich. the centers at stuttgart and munich host several large customsystems: a 48processor nec sx6 at stuttgart and a 1,344processorhitachi sr8000f1 and a 52processor fujitsu vpp700/52 vectorsupercomputer at munich.francefrance has fallen behind germany and the united kingdom insupercomputing. the largest french supercomputer is operated by thefrench atomic energy commission (ceadam) and is 28th on thetop500 list. it supports the french equivalent of the asc program and issimilar to (but smaller than) the ascq system at lanl. unlike the doe14u.k. engineering and physical sciences research council. 2004. a strategic frameworkfor highend computing, may, <http://www.epsrc.ac.uk/content/publications/other/astrategicframeworkforhighendcomputing.htm>.getting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.supercomputing abroad191centers, the french center is partly open and supports a collaboration withfrench industrial partners and other agencies (power, edf; space,onera; engines, snecma; and turbines, turbomeca). franceõs nexttwo largest systems are industrial and commercial (petroleum, totalfinaelf; and banking, soci”t” g”n”rale). france has two academicsupercomputing centers: cines (55 people, yearly budget of about 10million) and idris (44 people, yearly budget of about 1 million).spainspain recently announced its plan to build a 40tflops cluster systemin barcelona using ibm power g5 technology. the spanish governmentwill invest 70 million in the national centre for supercomputing over 4years. this will significantly enhance the compute power available in thatcountry.application softwaregenerally, the type of research performed in these various centers issimilar to the research performed in the united states; similar software isbeing used, and there is significant sharing of technology. however, bothin japan and in europe there seem to be more targeted efforts to develophighperformance application software to support industry. japanõs frontier simulation software project for industrial science is a 5year programto develop parallel software in support of industrial applications, fundedat about $11 million per year. the expectation is that the program, onceprimed, will be able to support itself from revenues produced by commercial software use. in joint university/industry projects, it is anticipatedthat universitydeveloped software will be available through open sourcelicensing, although industrydeveloped software will probably be proprietary. various european countries, in particular france, have significantprograms with industrial participation for the development of engineering codes. for example, the french salome project aims at the development of a large open source framework for cad and numeric simulation;currently available code is distributed and maintained by the french opencascade company. edf, eads (aerospace) and other french companiesare partners in the project. darpa invested in similar projects as part ofthe sci program, but that support seems to have disappeared. furthermore, from the committeeõs visits to doe sites, members got the clearimpression that there are no incentives for the transfer of codes developedat those sites to industrial use and no significant funding to facilitate thetransfer.getting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.1928a policy frameworkin this chapter the committee discusses a policy framework for government activities in supercomputing. it does so in general terms,without going into the specifics of current or proposed policies. concrete government policies in supercomputing in areas such as acquisitions, research funding, and support of industrial r&d are discussed inchapter 9.the federal government has been involved in the development andadvancement of supercomputing since the advent of computers. althoughthe mechanisms and levels of support have varied over time, there hasbeen a longstanding federal commitment to encourage technical progressand the diffusion of highperformance computing systems. (key aspectsof this history are summarized in chapter 3.) effective policy must bepremised on a clear understanding of the rationale for intervention andan analysis of how intervention might be tailored to adapt to a changingeconomic and technological environment. in the absence of a compellingrationale for intervention, economists are generally reluctant to see government intervene in highly competitive markets, where the costs of disruption to wellfunctioning and efficient private sector allocation mechanisms are likely to be high. however, there are two broad and widelyaccepted rationales for government involvement in supercomputing: (1)the government is the primary customer and (2) supercomputing technology is beneficial to the country as a whole.getting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.a policy framework193the government as the leading user and purchaserof supercomputer technologymuch technological innovation is, at least initially, directed to applications dominated by government involvement and purchasing. mostnotably, defense and national security needs have often been the specificsetting in which new technologiesñincluding supercomputingñwerefirst developed and applied. even when commercial firms are the locus ofresearch and development for new technology, governments are often thelargest single customer for the resulting innovations.government demand for advanced information technologyñincluding supercomputersñis not static. historically, government demand hasbeen quite responsive to current technological capabilities. as technicalprogress over time relaxes a given set of constraints, key governmentsupercomputer purchasers have not simply taken advantage of a fixedlevel of performance at a lower cost; instead they spur continuing technical progress by demanding ever higher levels of technical performance.the use of supercomputing allows missionoriented governmentagencies to achieve their objectives more effectively, with the consequencethat the federal government has a strong interest in ensuring a healthyrate of technological progress within supercomputing. the u.s. government remains the single largest purchaser of supercomputers in the world,and most federal supercomputer procurement is justified by the requirements of missions like national security and climate modeling.for example, the justification for the original asci program was topromote supercomputing technology not for its own sake but for the sakeof ensuring confidence in the nuclear stockpile in the absence of nucleartesting. doe tried to achieve this objective by two means: the aggressiveprocurement of supercomputers throughout the 1990s and funding of thepathforward development program, which attempted to accelerate technical progress in the types of supercomputers used by the asci program.other defense and national security agencies have also been aggressive users of supercomputing technology. (see chapter 4 for a descriptionof specific applications.) for example, the timely calculation of areas ofenemy territory where enemy radars are not able to spot our airplanes(such calculations were performed during the first gulf war) can be crucial.1 design and refurbishment of nuclear weapons depends critically onsupercomputing calculations, as does the design of nextgeneration armament for the armyõs future combat system.1william r. swart. 1991. keynote address. sc1991, albuquerque, n.m., november 20.getting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.194getting up to speedit is likely that supercomputing will be increasingly important tohomeland security. examples include micrometeorology analysis to combat biological terrorism and computer forensic analysis in the wake ofterrorist bombings. the federal government must be able to guaranteethat such systems do what they are intended to do. moreover, these programs must ensure that, while supercomputers are available to u.s. security agencies with no hindrance and with capabilities that satisfy theirneeds, other countries can be prevented from achieving key capabilities insupercomputing. to achieve this balancing act, the relevant federal agencies and research laboratories must often be closely involved in criticalaspects of supercomputing r&d, even when the research and development are carried out in the private sector.as the social custodian of welldefined government missions and thelargest and most aggressive customer for new technology related to thesemissions, the government has an incentive to ensure appropriate and effective funding for innovative supercomputing investments so as to guarantee that the technology progresses at a rate and in a direction that servethe missions.supercomputer technology investmentsas public goodsthe public goods nature of supercomputer investment is a secondbroad rationale for government intervention. in contrast to purely privategoods (such as hot dogs or pencils, which only one person owns and consumes), public goods are nonrival (many consumers can take advantageof the good without diminishing the ability of other consumers to enjoy it)and nonexcludable (suppliers cannot prevent some people from using thegood while allowing others to do so). national defense is an importantexample of a public good. even though the national defense protects oneperson, it can still protect others (nonrival), and the national defense cannot protect some people without also protecting others (nonexcludable).when a market involves goods that are both nonrival and nonexcludable, innovators are unable to capture the full value of their inventions, sothe incentive for an individual firm to undertake investment is less thanthe socially optimal level of incentive. in the absence of government intervention or coordinated action, the underinvestment problem tends to bemost serious for basic research, fundamental scientific discoveries, technologies that serve as steppingstones for followon research by others,and software.both policymakers and economists have emphasized the public goodsrationale for government intervention in areas like supercomputing technology. in large part, and as discussed in more detail in chapter 3 (andgetting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.a policy framework195elsewhere), a number of technologies and innovations first implementedin supercomputers played an important role in shaping the architectureand performance of mainstream computers today (from workstations topersonal computers). moreover, initiatives funded in the context ofsupercomputers have influenced the ability to commercialize innovations,from workstation architecture to the latest intel processor. algorithms andcodes initially developed for supercomputers in areas such as computational fluid dynamics, solid modeling, or signal processing are nowbroadly used by industry. as well, many of the most important applications of supercomputing technology, such as national security and climate modeling, are themselves public goods. given these conditions, it isnot surprising that both policymakers and economists have long justifiedinvestments in supercomputing technology on the basis of their status aspublic goods.several perceived shortcomings of the environment for supercomputing may reflect the public goods problem. for example, supercomputerusers suffer from a lack of accessible and wellmaintained software. moreover, the development of better programming interfaces would greatlyenhance productivity. while such initiatives would benefit all supercomputer users, no individual programmer or team has sufficient incentivesto develop such complementary software and interface technologies. similar to the more comprehensive approach to software development that isbeing attempted in recent projects such as the earth system modelingframework at multiple institutions, overcoming these deficiencies requires either government intervention to provide direct support for thedevelopment of these technologies or a mechanism for coordinated actionacross groups involved in supercomputing technology.2potential costs of government interventionbecause the federal government is the main purchaser of supercomputing technology, and supercomputer hardware and software development is a public good, the federal government has played a leading andcrucial role in the development and procurement of supercomputing technology. as discussed in chapter 3, the federal government is not simply apassive consumer in these markets but has actively sought to influence2some people have also attempted to justify government intervention on the grounds ofinternational competitiveness. according to this argument, government intervention canensure that u.s. products are superior and thus benefit u.s. economy. most economistsreject this type of argument, and the committee found no reason to endorse it forsupercomputing.getting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.196getting up to speedthe rate and precise direction of technological change, with consequencesfor both the supercomputer market and the evolution of computing moregenerally.it is important to emphasize that federal intervention in a technologically dynamic industry can be costly and disruptive, substantially limiting the efficiency and incentives provided by competitive markets. manyeconomists question the desirability of government involvement in thedevelopment and commercialization of new technology; government intervention can often be a far from benign influence on the market for newtechnologies.3 first, attempts to promote standardization through procurement can result in inadequate diversity, reducing the degree of technological experimentation. inadequate experimentation with a variety ofnew technologies can be particularly costly in areas like supercomputing,where much of the realized value of a given technology is only realizedover time through user experience and learning. second, individual firmsand vendors supporting specific supercomputer architectures may attempt to exert political influence over the procurement process itself.when such rent seeking occurs, government purchasing decisions may bebased on the political influence of a firm rather than on its ability to meetthe needs of government agencies in terms of performance and cost.given that government intervention may come with substantial costs,it is important to consider the types of interventions that the governmentcan undertake and some of the key tradeoffs that policymakers mightconsider as they develop and implement policy towards supercomputing.alternative modes for government interventionalmost by definition, government intervention in the supercomputerindustry influences the allocation of resources toward supercomputingtechnology. however, the government has wide latitude in choosing theform of its intervention, and each type of intervention has its own costsand benefits. in large part, the governmentõs optimal choice of intervention and involvement depends on the balance between the specific missionoriented objectives of individual agencies and the broader goal ofencouraging technological progress in supercomputing (and informationtechnology more generally).the government has two main avenues for increasing innovation in3linda cohen and roger noll. 1991. the technology pork barrel. washington, d.c.:brookings institution press.getting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.a policy framework197supercomputers. it can either provide incentives to the nongovernmentsector or it can conduct the research itself.government incentivesgovernment policy can provide broad encouragement to private industry to develop and commercialize supercomputing technology andaffect the broader information technology marketplace. the governmentcan influence the private sector by providing incentives for innovationand development investments, including grants or other subsidies, taxincentives, and intellectual property protection.the government may subsidize private r&d activities. for example,a pervasive form of federal support for scientific and engineering researchis grant and research contract programs, ranging from the peerreviewedgrant systems maintained by the national science foundation and otherinstitutions to research contracts awarded by mission agencies such asdarpa. such programs are particularly effective when the governmentwould like to encourage basic research in specific areas but has limitedinformation or knowledge about the precise nature of the outputs fromresearch in that area. for example, grants and subsidies to the supercomputer center at the university of illinois during the early 1990s were theprincipal form of support underlying the development of the mosaicbrowser technology, an enormously beneficial innovation whose preciseform, features, or impact could not have been forecast prior to its invention.4alternatively, r&d tax credits can provide important incentives forinnovative investment. r&d tax credit programs provide direct incentives to private firms at a relatively low administrative burden.5 how4a somewhat similar approach is for the government, a nonprofit organization, or even aprivate firm to offer a prize. this approach has been tried throughout history with mixedresults. for example, in 1795, the french directory offered a prize of 12,000 francs òto anyfrenchman who could devise a method of ensuring fresh, wholesome food for his armiesand navies.ó the prize was awarded by napoleon bonaparte to nicholas appret, who invented a method for preservation by sealing foods in airtight bottles and immersing them inboiling water for varying periods, which led to modernday canning. sobel provides anextremely rich description of the deficiencies and politics of governmentsponsored prizesin his history of a prize for longitude at sea (david sobel, 1995, longitude: the true story of alone genius who solved the greatest scientific problem of his time, new york, n.y.: walkerand company). recent examples of prizes range from the efforts by u.s. electrical companies to encourage research on a refrigerator that runs on 25 percent less electricity to theansari x prize, which awarded $10 million to the first privately sponsored spacecraft toreach 100 km above earthõs surface (www.xprize.org).5the costs and delays in grant review are often cited as the reason private firms are unwilling to apply for government subsidy programs.getting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.198getting up to speedever, tax credit programs have often been criticized for subsidizing private research that would have taken place even in the absence of a taxcredit program.6 moreover, it is difficult to use tax credits to specificallyencourage research in specialized technical areas such as supercomputing.while tax credit programs are an appropriate tool to achieve broad r&dinvestment objectives, they are often too blunt to influence the precisedirection of technical advance.finally, the patent system provides an indirect incentive system toencourage the development and commercialization of new supercomputing technology. underinvestment in research and development willoccur if others can copy a new idea or invention. a patent for a new invention gives the inventor monopoly rights to the invention for a fixedperiod of time, currently 20 years, so that the inventor can capture a relatively large proportion of the gains from innovation.7 unlike fixed subsidies, patents lead to distortions from monopoly pricing; however, a principal rationale for the patent system is that the shortrun loss from highprices is (hopefully) more than compensated for by the enhanced incentives for innovative investment. perhaps the chief benefit of the patentsystem is its inherent flexibility: rather than having the government determine in advance the types of innovations and discoveries to be encouraged, the patent system provides a marketbased incentive availableacross a wide range of technologies and industries. however, the influence of the patent system on innovation incentives is subtle, and there isan ongoing debate about its use, particularly in areas of science and technology that might also benefit from subsidies or other mechanisms.8each of these mechanisms provides incentives for innovation but6b. hall and j. van reenen. 2000. òhow effective are fiscal incentives for r&d? a newreview of the evidence.ó research policy 29(45):449469.7an alternative method whereby firms can avoid the problem of underinvestment is forthe firms in an industry to engage in research joint ventures, where they agree to share thecost of development as well as the benefits. however, firms may fear that such joint researchactivity may lead to antitrust prosecutions. the national cooperative research act of 1984tried to reduce firmsõ fears of antitrust penalties by lowering the damages a joint venturemust pay if it is convicted of an antitrust violation. international joint ventures are increasingly common. for example, in 1992, toshiba, ibm, and siemens announced they wouldcollaborate in developing advanced memory chips, and on the same day, fujitsu and advanced micro devices said they would jointly manufacture flash memories, which are usedfor data storage instead of disk drives. from april 1991 to july 1992, at least seven technology alliances to produce memory chips were formed between u.s. and japanese firms.8see, for example, nancy gallini and suzanne scotchmer, 2002, òintellectual property:when is it the best incentive mechanism?ó innovation policy and the economy, adam b. jaffe,josh lerner, and scott stern, eds., cambridge, mass.: mit press.getting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.a policy framework199places few restrictions on the ultimate output of the r&d investment orthe use made of the technology and discoveries resulting from that investment. as such, these mechanisms will be inadequate when the government would like to maintain close control over the precise developmentof a technology or keep a given technology secret. when the governmenthas clear technical objectives and an interest in maintaining precise control, the government can staff and fund intramural research and evenimplement prototyping and development programs.government researchsince the beginning of the computer era, national laboratories andother government agencies have conducted supercomputer research and,in some cases, been responsible for building individual machines. a keybenefit of internal development is that the government can maintain extensive control over the evolution of the technology and, when needed,maintain a high level of secrecy for the technology. maintaining such control may be important in those cases where the technology is being developed for very specific government missions within very narrow parameters and where secrecy and continued control over the technology ismuch more important than cost or the ability to build on a diverse set ofalready existing technologies. the degree of control and secrecy that arefeasible even under internal development should not be overstated. government employees can move to private industry (or even start their owncompanies), and as long as individual components or subsystems are being procured from the private sector, it is difficult to maintain completesecrecy over the technology choices and capabilities of large governmentprojects.most important, large intramural technology development projectsare likely to be extremely costly, relative to what could be achievedthrough procurement from the private sector. indeed, while overall government science and technology expenditures are predominantly fundedthrough grants and tax credits, a high share of supercomputer investmentis implemented through procurement contracts with private firms. underideal conditions, procurement allows the government to acquire specifictypes of advanced technology while taking advantage of competition between firms on the basis of cost and performance. the government canindeed take advantage of these benefits when it is a relatively small playerin an otherwise competitive market. for example, over the past two decades, the government has been able to take advantage of the rapid paceof technical advance and the high level of competition in the market forpersonal computers as it acquires desktop pcs for nearly all governmentfunctions.getting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.200getting up to speedhowever, reaping the benefits of competition through a procurementsystem is more challenging when the government is the principal (or evensole) demander and the development requires substantial sunk investments. in this case, procurement decisions themselves shape the degree ofcompetition in the marketplace. for example, the government can chooseto deal with one, two, or more firms in the market over time. by committing to one firm, the government may be able to encourage the firm tomake large sunk investments to take advantage of economies of scale andalso maintain a relatively high level of secrecy. a single vendor captures alarger share of the benefits from innovation and customizing the softwareto work well with the hardware than would two vendors. the single firmgains from economies of scale in producing more units. however, a singlevendor will exercise market power, setting a price above marginal costand hence reducing demand for its product. by dealing with several firmsover time, the procurement environment will be more competitive, leading to greater technological diversity, greater technological experimentation, and less risk. the probability of discovering a superior technologymay be greater if more independent groups are involved. because thegovernment buys or funds most supercomputer purchases, its approachto procurement largely determines the degree of competition in this industry.when a single government agency such as a single branch of the department of defense has faced this type of procurement environment, itoften uses a òcommitted supplieró approach. when procuring technologies such as those for advanced fighter aircraft, the government choosesto engage (and commit) to a few firms (sometimes as few as two or three)over a relatively long horizon. by so doing, the government gives eachfirm a relatively strong incentive to make large investments, while maintaining at least a degree of flexibility and competition over time.9 at thebroadest level, committing to several firms would probably be effectivefor supercomputing if there were a single coordinated approach tosupercomputing procurement across the government. however, in contrast to the environment facing the department of defense, governmentprocurement of supercomputing is dispersed across multiple governmentagencies and facilities, many of which are engaged in (at least tacit) competition with one another. since technology changes rapidly, it is not possible to specify deliverables in detail beyond a short horizon. therefore,9for a review of the literature related to government sourcing, see w.n. washington,1997, òa review of the literature: competition versus sole source procurements,ó acquisition review quarterly 10:173187.getting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.a policy framework201contracts are short term. in the current institutional structure, any givenagency cannot commit to a longterm relation with a vendor. doing sowould require an accord and coordinated procurement across agenciesand a different procurement model.finally, it is important to emphasize that government policy can, byitself, substantially limit the degree of competition available for futureprocurement. for example, if the u.s. government contracts with onlyone company, it virtually guarantees that there will be a monopoly (or atleast a dominant firm) in the u.s. supercomputer market. in addition, byenacting trade barriers (see box 8.1), the government may benefit a smallnumber of domestic firms at the expense of government agencies andother consumers of supercomputers in the future, who may have to bearmuch higher prices or make do with inferior equipment.competing government objectivesoverall, optimal government policy toward supercomputing musttherefore balance competing objectives, including serving the requirements of missionoriented agencies and encouraging technologicalprogress more broadly. as a practical matter, these objectives are balancedthrough the procurement process, which is discussed in detail in chapter9. in managing the procurement process, the government faces three keytradeoffs: coordination versus diversification, commitment versus flexibility, and secrecy versus spillovers.coordination versus diversificationgovernment agencies can coordinate or they can act independently,obtaining diverse individual solutions. by coordinating (e.g., buying thesame equipment and using common software), the agencies benefit fromeconomies of scale. however, individual agencies would not necessarilyobtain the best solution for their individual needs. a central planner(supercomputer czar) would be more likely to obtain the benefits of coordination at the expense of not fully satisfying the diverse needs of individual agencies. on the other hand, if each individual agency makes independent decisions, it probably will forgo the benefits from coordination(local versus global maximization).commitment versus flexibilitythe government may commit or maintain flexibility. for example, thegovernment may commit to a particular vendor (a particular piece of hardware or software) or a particular approach (parallel versus vector magetting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.202getting up to speedchines) for a given period of time. by so doing, it would benefit fromeconomies of scale and leverage investments by others. however, it wouldlose flexibility. if the government backs the òwrong horse,ó the rate offuture advances might be slowed.at least in part, the tradeoff between commitment and flexibility reflects mandates to maintain a procurement process with high integrity.the government intentionally layers the procurement process with enormous amounts of auditing (and other legal constraints) in order to eliminate corruption. while such mandates serve the purpose of avoiding favoritism, they inevitably slow down the process of acquiring a newsystem, adopting frontier technology, and coordinating across differentbidding processes.10 they may also make it harder to weigh intangiblessuch as a good, continued relation between government and a vendor.secrecy versus spilloversbecause the government has many missions that depend on secrecy,such as code breaking and weapons development, it often sacrificesspillover benefits. a national defense agency may develop superior hardware or software that would benefit other government agencies or otherusers around the world by allowing them to avoid òreinventing thewheel.ó however, to maintain secrecy for reasons of national security, thegovernment does not share these innovations. obviously there are manycases where secrecy is paramount, but there may be many cases at themargin, where the cost of reducing secrecy (at least to the degree of allowing government agencies to share information) would be justified by thespillover benefits to others.secrecy also reduces spillovers in the reverse direction. if much of theresearch on certain forms of supercomputing is done in a classified environment, then one creates two distinct supercomputing research communities; an academic one that is open to foreigners and a classified one. thetwo communities have a limited ability to interact, thus reducing the inflow of people and research ideas from universities to classifiedsupercomputing. such a separation is more hurtful in areas where technology changes rapidly.overall, managing each of these tradeoffs requires a detailed understanding of the specific needs and requirements of different agencies andinstitutions, as well as the environment and infrastructure in which10see, for example, steven kelman, 1990, procurement and public management: the fear ofdiscretion and the quality of government performance, washington, d.c.: american enterpriseinstitute press; shane greenstein, 1993, òprocedural rules and procurement regulations:complexity creates tradeoffs,ó journal of law, economics, and organizations, pp. 159180.getting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.a policy framework203supercomputing technology will be developed and deployed. it requiresa clear understanding of the critical points of control. for example, thereis no practical way to prevent foreign countries from assembling powerful clusters out of commodity components, but it is practical to restrictaccess to critical application codes.box 8.1 trade policiesseveral u.s. government policies affect international trade, such as antidumping laws, subsidies for sales in third markets, restrictions on imports(quotas or tariffs, if allowed under international agreements), and exports(export restrictions). using these policies, the united states has effectivelybanned japanese supercomputers from the u.s. supercomputer market. theevents leading up to this ban follow.as summarized in chapter 3, japanese firms started manufacturing highperformance vector machines in the early 1980s. by the late 1980s, usingvector designs based on highperformance custom processor chips, thesemanufacturers posed a substantial competitive threat to u.s. producers.they benefited substantially from procurement by the japanese government and the educational system and also received direct government subsidies for related research and development. it has also been alleged thatlarge japanese private customers that received substantial government funding were under pressure to buy japanese supercomputers. the u.s. government pressured japan to open its markets. in 1996, nec developed the sx4, a fast and relatively inexpensive cmosbased vector supercomputer.on may 17, 1996, the federally funded university corporation for atmospheric research (ucar) decided to lease a supercomputer made by ajapanese company, the first such decision by a public entity.1 it awarded a$35 million, 5year leasing contract for a supercomputer to the u.s.basedintegrator company federal computer corporation (fcc), which had outbid two other finalists for the contractñfujitsu america, inc., and crayresearch of eagan, minnesotañto supply a supercomputer to the nationalcenter for atmospheric research (ncar) for modeling weather and climate. the heart of fccõs proposal was four nec sx4 machines, to beprovided by hnsx supercomputing, the u.s.based subsidiary of nec.within 2 months, a domestic firm, sgi/cray research, which had submitted a bid to ucar, filed an antidumping complaint.continuedgetting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.204getting up to speedin 1997, the international trade administration (ita) of the departmentof commerce determined in ònotice of final determination of sales at lessthan fair value: vector supercomputers from japanó (a588841) that vector supercomputers from japan were being sold in the united states at lessthan fair value. in its determination,2the ita concluded that dumping hadoccurred and calculated dumping margins using relatively indirect evidence:manufacturer/producer exportermargin percentagefujitsu ltd.173.08nec454.00all others313.54on september 26, 1997, a second u.s. agency, the international tradecommission, made the dumping charge final with its determination thatcray research had suffered material injury, even though ncar argued thatthe hardware cray proposed did not meet its minimum specifications.3the punitive tariffs of between 173 percent and 454 percent on allsupercomputers imported from japan established a barrier so high that iteffectively prevented imports and excluded japanese supercomputers fromthe u.s. market.4nec and fujitsu were, however, able to sell manysupercomputers outside the united states.nec filed suit with the court of international trade (cit) seeking suspension of the antidumping investigation. the suit, which was unsuccessful, alleged that the u.s. actions were politically motivated and reportedthat, prior to its findings, the department of commerce had arranged fivemeetings between it and government agencies, meetings that were attendedby highranking officials.5on may 3, 2001, the commerce department revoked the duties onvector supercomputers made by nec and fujitsu ltd., retroactive to october 1, 2000. ironically, cray requested this action as part of crayõs distribution and service agreement with nec, whereby cray became the exclusivedistributor of necõs vector supercomputers in north america and a nonexclusive distributor in the rest of the world other than certain accounts infrance. however, it has not yet sold any nec sx6 machines in the unitedstates.this u.s. policy has had adverse effects on u.s. scientific computing.for example, as a consequence of the initial cit action, ncar was unableto upgrade its supercomputing capability for almost 2 years and suffered aserious delay in research.6 in addition, because the ncar climate codeswere heavily oriented toward a vector architecturebased supercomputer,they could easily have been ported to the powerful nec system. subsebox 8.1 continuedgetting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.a policy framework205quent reprogramming of the major climate models to allow them to run oncommodity equipment caused additional delays during which very littlescience could be undertaken.the new cray t90 vector supercomputer was generally considered tobe overpriced. many of the u.s. supercomputer users that would havepreferred a japanese vector machine turned instead to commodity microprocessorbased clusters from various vendors. applications such as thoseat ncar, which require high machine capability and broad memory access, were hampered by the small caches and slow interconnects of thecommodity products. after a number of years of optimization efforts, theefficiency of the ncar applications taken as a whole is only 4.5 percenton a large system of the 32processor ibm power 4 nodes and 5.7 percenton a large system of the 4processor ibm power 3 nodes.7 only recently,and with substantial u.s. development funding, has cray research successfully developed the x1, a vector supercomputer comparable in powerto those produced in japan.commoditybased systems are now increasingly used for weather simulations, since the problem has become one of capacity. many independentsimulations are carried in an ensemble study and each can now be performed on a relatively modest number of nodes, even on a commoditysystem. while efficiency is low, these systems seem to offer good cost/performance. however, custom systems are still needed for climate simulations, since climate studies require that a few scenarios be simulated overlong time periods, and scientists prefer to study scenarios one at a time.commodity systems cannot complete the computation of one scenario in areasonable time. the same consideration applies to large fluid problemssuch as the long global ocean integrations with 10km or finer horizontalgrids that will be needed as part of climate simulationsñsuch problemsrequire the scalability and capability of large systems that can only be provided by hybrid or fully custom architectures.1christopher m. dumler. 1997. òantidumping laws trash supercomputer competition.ócato institute briefing paper no. 32. october 14.2federal register, vol. 62, no. 167, august 28, 1997:45636.3see <http://www.scd.ucar.edu/info/itc.html>.4see <http://www.computingjapan.com/magazine/issues/1997/jun97/0697indnews.html>.5ibid.6bill buzbee, director of the scientific computing division at ncar during that antidumping investigation, argued in 1998 that the decision gave a significant computational advantage toall earth system modelers outside the united states and that it would still be 1 to 2 years beforeu.s. commoditybased supercomputers were powerful enough to carry out the ncar researchsimulations that could be done on the nec system in 1996 (national research council, 1998,capacity of u.s. climate modeling to support climate change assessment activities, washington, d.c.: national academy press).7the underlying hardware reasons for these numbers are discussed in an online presentation by rich loft of ncar, available at <http://www.scd.ucar.edu/dir/cas2k3/cas2k3%20presentations/mon/ loft.ppt>.getting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.2069stewardship and funding ofsupercomputingchapters 1 through 8 of this report described in some detail thecurrent state of supercomputing and provided some context basedon history, policy considerations, and institutions. the situationwe find ourselves in at the present time can be summarized as follows:¥in the united states, the government is the primary user of supercomputing (directly or indirectly). supercomputing is used for many public goods, including national defense, pollution remediation, improvedtransportation, and improved health care. it is used for governmentsponsored basic research in many areas of science and engineering. althoughu.s. industry uses supercomputing as well, companies report that thereare major inhibitors to greater use.1¥many of the most computationally demanding applications havegreat societal benefit. health care, defense, climate and earthquake modeling, clean air, and fuel efficiency are examples of public goods that arefacilitated by the applications discussed earlier.¥u.s. leadership in supercomputing is essential. supercomputingplays a major role in stockpile stewardship, in intelligence collection andanalysis, and in many areas of national defense. for those applications,the government cannot rely on external sources of technology and expertise. more broadly, leadership in science and engineering is a national1earl joseph, christopher g. williard, and allison snell. 2004. council on competitivenessstudy of u.s. industrial hpc users. international data corporation. july.getting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.stewardship and funding of supercomputing207priority.2 leadership in supercomputing is an important component ofoverall leadership in science and engineering.¥by its very nature, supercomputing has always been characterizedby higher performance than mainstream computing. however, as theprice of computing has dropped, the cost/performance gap betweenmainstream computers and toppriced supercomputers has increased. thecomputer market has grown most vigorously at the bottom end (cheappcs and lowend servers). the share of that market devoted to supercomputing has diminished, and its importance in economic terms to hardware and software vendors has decreased. even within supercomputing,the relative weight of the most challenging systems, those based on custom components, has decreased as an increasing number of supercomputer users are having their needs met by highend commodity systems.yet some essential needs can only be met by custom components. consequently, market forces are less and less natural drivers of advances insupercomputingspecific technologies.¥supercomputer systems are highly complex. supercomputing is,almost exclusively, parallel computing, in which parallelism is availableat all hardware and software levels of the system and in all dimensions ofthe system. the coordination and exploitation of those aspects of parallelism is challenging; achieving balance among the aspects is even more challenging.¥ecosystem creation is both long term and expensive. the amalgamof expertise, technology, artifacts, and infrastructure that constitutes asupercomputing ecosystem is developed over a significant period of time.to get all the necessary components in place, a lot of effort is required.the nurturing of human talent, the invention of new ideas and approaches, and the use of those ideas and approaches in hardware andsoftware artifacts all require significant investment. given the lead timeneeded, and the fact that a given ecosystem has a bounded lifetime, investment in future ecosystems is needed to sustain leadership.given that leadership in supercomputing is essential to the government, that supercomputing is expensive, and that market forces alone willnot drive progress in supercomputingdirected technologies, it is the roleof the government to ensure that supercomputing appropriate to ourneeds is available both now and in the future. that entails both having thenecessary activities in place in an ongoing fashion and providing the funding to support those activities.2national coordination office for information technology research and development.2004. federal plan for highend computing: report of the highend computing revitalizationtask force (hecrtf). may.getting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.208getting up to speedthe government needs to be concerned with both the producers ofsupercomputingñthe researchers who create new technology, the hardware and software designers, the manufacturers and service organizationsñand the consumers of supercomputingñthe academic, government, and industrial users.satisfying current supercomputing needsvirtually every group consulted by the committee had concerns aboutaccess to supercomputing. supercomputer center directors in academicsettings and in both unclassified and classified missionoriented centerswere concerned about two things: (1) the large amount of time and effortrequired for procurement decisions and (2) the long time (up to 3 years)between the initial decision to acquire a new system and its actual installation. the recent report by the jasons3 noted the need for increasedcapacity computing for the doe/nnsa stockpile stewardship program.(as pointed out previously, users of capability computing are also usersof capacity computing.) demand for time on nsf supercomputing centerresources greatly exceeds supply;4 at the same time, the performance gapbetween those resources and the highest capability systems is increasing.5academic access to doe/dod missionoriented centers is limited by thepriority assigned to the mission and, in some cases, by the constraints onaccess by noncitizens.at the same time, many users complained about the difficulties inusing supercomputer systems to full advantage, the problems caused bymoving to a new system, and the absence of supercomputing systems ofsufficiently high performance to solve their problems. those communities able to draw on hero programmers worry that the supply of suchindividuals is too small.some of these immediate needs can be satisfied by additional funding. capacity computing is a commodity that can be purchased. additional staffing could help with migration to new systemsñhigher salariesmight help increase the supply of such staff. however, the difficulties ofusing current systems and the absence of more powerful systems are notfixed so quickly.3jason program office. 2003. requirements for asci. july.4the national resource allocations committee (nrac) awards access to the computational resources in the nsf paci program. information is available at <http://www.npaci.edu/allocations/ alloctxt.html>.5see top500 rankings.getting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.stewardship and funding of supercomputing209ensuring future supercomputing leadershipthe need for hardware and software producersthe need for the government to ensure that there are suppliers tomeet national needs is not unique to supercomputing. the committeeõsearlier discussion suggests some possible modes of government intervention. in the case of supercomputing, the discussion of ecosystems has illustrated the interdependency of hardware, system software, and applications software. nevertheless, different forms of intervention might bepossible in different cases.in the committeeõs view, it is necessary that there be multiple suppliers of both hardware and software. as it discussed previously, differentapplications (and different problems within those applications) have different computational needs. there is no single architecture or architectural family that will satisfy all needs. in the foreseeable future, some ofthe needed architectures will come from systems built from custom processors. among the possible hardware suppliers are vertically integratedsupercomputer vendors, such as cray used to be,6 vertically integratedsupercomputer product lines within larger companies such as ibm orhewlettpackard, and systems created from products of horizontal vendors that produce components (e.g., commodity microprocessors fromintel, amd, and apple/ibm and switches from lan vendors, myricomor quadrics).vertically integrated companies usually provide system software aswell as hardware. however, the committee also believes it is possible tohave nonprofit software organizations that develop and maintain community codes, software tools, or system software. these organizationsmight have a single physical location, or they might be geographicallydistributed. their products might be open source, or they might haveother licensing agreements. they would likely draw on contributions fromthe larger research and development community, much as linux effortsdo today. they might be broad in scope or more narrowly specialized.historically, supercomputing software has also been supplied by isvs.however, participants in many such companies say that there is no longera successful profitmaking business model, in part because highly skilledsoftware professionals are so attractive to larger companies. for example,many companies that were developing compilers, libraries, and tools forhighperformance computing went out of business, were bought, or no6the recent development of the x1 was largely vertically integrated, but the developmentof other cray products such as red storm is not.getting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.210getting up to speedlonger focus on highperformance computing (e.g., kai, pgi, pallas, apr,and parasoft). no new companies have entered this field to replace thosethat left.in all of these possible modes of intervention, one thing is clear. success in creating the suppliers depends on longterm, stable, predictableacquisitions and on the fruits of longterm, governmentfunded r&d.the need for stabilitythe committee heard repeatedly from people with whom membersspoke about the difficulties and the disincentives caused by the lack oflongterm planning and the lack of stability in government programs. inorder to undertake ambitious projects, retain highly skilled people,achieve challenging goals, and create and maintain complex ecosystems,organizations of all kinds need to be able to depend on predictable government commitmentsñboth to programs and to ongoing funding forthose programs.7 if that stability is absent, companies will go out of business or move in other directions, researchers will shift to other topics, newprofessionals will specialize in other skills, corporate memory is lost, andprogress on hard problems slows or stops. once interruptions occur, itmay be difficult and expensive, or even impossible, to recover from lostopportunities or discarded activities.8ongoing commitments are not entitlements; the government shoulddemand accountability and performance. however, priorities and longterm objectives need to be sufficiently clear that when funded efforts areperforming well, they have stability.the committee heard of many areas where stability has been lost. following are a few examples.7for example, approximately 80 percent of crayõs sales in 2003 were to the u.s. government. crayõs revenue dropped from over $100 million in 2003 to less than $20 million in 2004due to a drop in a defense appropriation and a delay in doeõs oak ridge national laboratory project (see lawrence carrel, 2004, òcrunch time at cray,ó available online at<http://yahoo.smartmoney.com/onedaywonder/index.cfm?story=20040727>).8the same issue has been studied for other longterm government procurements. for example, a rand study examined in 1993 the costs and benefits of postponing submarineproduction; even though no new submarines were needed until 2006, the cost of the lost ofexpertise was believed to outweigh the savings from postponing production by 10 years(j.l. birkler, j. schank, giles k. smith, f.s. timson, james r. chiesa, marc d. goldberg,michael g. mattock, and malcolm mackinnon, 1994, òthe u.s. submarine production base:an analysis of cost, schedule, and risk for selected force structures,ó rand documentmr456osd; summary at <http://www.rand.org/publications/rb/rb7102/>).getting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.stewardship and funding of supercomputing211¥architecture.darpa built up an impressive body of national expertise in supercomputer architecture in the 1980s and 1990s, which wasthen allowed to languish and atrophy. doe sponsored the acquisitionand evaluation of experimental architectures in the 1980s, but such experimentation has largely disappeared.¥software.nasa actively supported the development and maintenance of libraries, benchmarks, and applications software, but support formany projects and organizations that would have continuing value hasdisappeared.¥collaborations.the nsf grand challenge program of the early1990s produced some strong collaborative interdisciplinary teams that hadno followon program in which to continue. more recently, the nsf itrprogram has again led to the creation of successful collaborations, buttheir expertise seems destined to be lost.it is difficult to achieve stability in the face of local decisions that havean unpredictable collective effect. each of the inauspicious outcomes mentioned above has an explanation. some outcomes stem from the turnoverof government personnel and concomitant shifts in budget priorities. others come from the nearuniversal desire to start something new without,however, waiting to extract the best aspects of the previous programs.still others ensue when agencies decide to stop sponsoring an importantactivity without finding other sponsorship. the net effect is that u.s. leadership in supercomputing suffers.the need for a continuum from research to productionas the discussion in chapter 5 makes clear, research in supercomputing has to overcome many hard, fundamental problems in order forsupercomputing to continue to progress. the dislocations caused by increasing local and remote memory latencies will require fundamentalchanges in supercomputer architecture; the challenge of running computations with many millions of independent operations will require fundamental changes in programming models; the size of the machines and thepotential increase in error rates will require new approaches to faulttolerance; and the increased complexity of supercomputing platforms andthe increased complexity of supercomputing applications will require newapproaches to the process of mapping an application to a platform andnew paradigms for programming languages, compilers, runtime systems,and operating systems. restoring a vigorous, effective research programis imperative to address these challenges.research and development in an area such as supercomputing requires the interactions of many organizations and many modes of activitygetting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.212getting up to speed(see box 9.1 and figure 9.1). it also requires its own instrumentation. research in applications requires stable production platforms. in contrast,research in technologies requires experimental platforms that are not usedfor production. while production platforms for applications research arein short supply today, experimental platforms are largely absent.to ensure that there will be new technologies to form the basis forsupercomputing in the 5 to 15year time frame (a typical interval betweenbox 9.1 the researchtoproduction continuumbasic research is generally done in small projects where many differentideas can be explored. the research can be integrated with graduate education if conducted in academia, thus ensuring a steady supply of professionals. experimental systems research and applied research projects canfurther validate ideas emerging from basic research and will often (but notalways) involve larger groups, whether in academia or in national or corporate research laboratories. before a fundamentally new design can become a product, it is often necessary to develop a large òverticaló prototypethat integrates multiple technologies (e.g., new architecture, a new operating system, new compilers) and validates the design by showing the interplay of these technologies. such a prototype can lead to a vertical product,where one vendor develops and provides much of the hardware and software stack of a system. however, part of the supercomputing market isserved by horizontal vendors that provide one layer of the system stack formany different systemsñfor example, companies such myricom or etnusproduce, respectively, switches and debuggers for many platforms. to thesame extent, some large applied research or prototyping efforts are bestorganized horizontallyñfor example, an effort where a group develops anew library to be widely available on many supercomputer platforms. thetechnology developed by such a group may migrate to a horizontal vendoror be adapted and turned into a product for a specific platform by a verticalvendor.the free dissemination of ideas and technologies is essential for thisresearch enterprise to succeed, because a relatively small group of peoplehave to ensure rapid progress of complex technologies that have complexinteractions. the model is not a simple pipeline or funnel model, wheremany ideas flourish at the basic research level, to be downselected into afew prototypes and one or two winning products. rather, it is a spiral evolution with complex interactions whereby projects inspire one another;whereby ideas can sometimes migrate quickly from basic research to products and may sometimes require multiple iterations of applied research;and whereby failures are as important as successes in motivating new basicresearch and new products.getting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.stewardship and funding of supercomputing213hardware,architecturesystemsoftwarecompilers,tools,librariesalgorithms,applicationssoftwarebasicresearchappliedresearchprototypeproductsmallprojectsmallprojectsmallprojectsmallprojectlargeverticalprojectlarge horizontalprototypelargeverticalprototypelarge horizontalprojectverticalproducthorizontalproductuniversitieslabsindustrysmallprojectfigure 9.1the researchtoproduction continuum.research innovation and commercial deployment in the computer industry), a significant and continuous investment in basic research (hardwarearchitecture, software, and numerical methods) is required. historically,such an investment in basic research has returned large dividends in theform of new technology. the need for basic research in supercomputing isparticularly acute. although there has been basic research in generalpurpose computing technologies with broad markets, and there has been significant expenditure in advanced development efforts such as the ascprogram and the teragrid, there has been relatively little investment inbasic research in supercomputing architecture and software over the pastgetting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.214getting up to speeddecade, resulting in few innovations to be incorporated into todayõssupercomputer systems.our country has arrived at this point by a series of investment decisions. in the 1990s, hpcci supported the development of several newcomputer systems. in retrospect, we did not recognize the critical importance of longterm, balanced investment in all of hardware, software, algorithms, and applications for achieving high performance on complexscientific applications. instead, missionoriented government agencies (including noncomputerscience directorates of nsf) focused their investments on their mission applications and algorithms tailored for themrather than on broadbased improvements. this was noted in a 1999pitac report9 and, more obliquely, in pitacõs review of the fy 2000 information technology for the 21st century (it2) budget initiative.10 arecent report card on the pitac implementation11 listed òhpc softwarestill not getting enough attentionó as one of three toplevel concerns.research in supercomputer architecture, systems software, programming models, algorithms, tools, mathematical methods, and so forth isnot the same as research in using supercomputing to address challengingapplications. both kinds of research are important, but they require different kinds of expertise; they are, in general, done by different people,and it is a mistake to confuse them and to fail to support both.basic longrange research is the exploration of ideas. it is not the sameas advanced development, although such development often ensues. basic research projects should not be required to produce products ordeliverables other than exposition, demonstration, and evaluation. a valuable benefit of basic research is that it can combine research and educationñhelping to create the next generation of supercomputing professionals. the benefits of that education outweigh the occasional delays inprogress stemming from inexperience. an important attribute of wellrunresearch projects is that they make room for serendipity. many importantdiscoveries arise from satisfying needs along the way to the main goalñan oftencited example is the ncsa mosaic browser, which was an unintended consequence of ncsaõs interest in web access to scientific data.performance tools are another example.as the discussion in chapter 5 made clear, supercomputing is headed9presidentõs information technology advisory committee (pitac). 1999. informationtechnology research: investing in our future. report to the president. february. available at<http://www.hpcc.gov/pitac/report/>.10available at <http://www.hpcc.gov/pitac/pitacit2review.pdf>.11ken kennedy. 2004. òpitac recommendations: a report card.ó presentation to thepresidentõs information technology advisory committee. june 17. available at <http://www.hpcc.gov/pitac/meetings/2004/20040617/20040617kennedy.pdf>.getting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.stewardship and funding of supercomputing215for major problems as systems continue to scale up; it is not clear thatincremental research will solve these problems. while incremental research has to be pursued so as to continue the flow of improvements incurrent platforms, there must also be room for outsidethebox thinkingñthat is, for projects that propose to solve the problems of supercomputingin an unorthodox manner.an important stage in the transfer of research results to deploymentand products is the creation and evaluation of prototypes. not all basicresearch leads to prototypes, but prototypes are essential to migrating research results into practice. prototyping provides an essential opportunity to explore the usefulness and the usability of approaches before committing to product development. for example, prototype systems serve toidentify research issues associated with the integration of hardware andsoftware and to address systemlevel problems such as system scalabilityand i/o performance in highperformance computing.ultimately, the purpose of the technology research is to facilitate theuse of supercomputing. prototyping is an appropriate stage at which tosupport technology and applications partnerships, in which applicationsresearchers become early adopters of prototypes and evaluate themagainst their applications. successful partnerships are those from whichboth the technology researchers and the applications researchers benefitñthe technology researchers by getting feedback about the quality and utility of their results; the applications researchers by advancing their application solutions. as part of the transfer of research to production,prototyping activities should normally include industrial partners andpartners from government national laboratories. the building of prototypes and hardening of software require the participation of professionalstaffñthey cannot be done solely by researchers.prototypes may range from experimental research systems to moremature advanced development systems to early examples of potentialproducts. because both industry representatives and professional staff areinvolved, there is often considerable pressure for prototyping projects toyield products. that is not their purposeñthe primary purpose is experience and evaluation. however, organizations sometimes find it difficultto take that view when there is strong pressure for shortterm delivery ofproducts or deliverables from users. government investment to supportprototyping is needed in all contributing sectors, including universities,national laboratories, and vendors.mechanisms are needed to create productive partnerships of this kindand to sustain them. both the nsf grand challenge program and the nsfpaci program have stimulated such partnerships. the most successfulpartnerships are those organized around research problems, not aroundfunding opportunities.getting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.216getting up to speedthe need for moneyprogress in supercomputing depends crucially on a sustained investment by the government in basic research, in prototype development, inprocurement, and in ensuring the economic viability of suppliers. erraticor insufficient funding stifles the flow of new ideas and cuts off technology transfer, inevitably increasing aggregate costs.basic research support requires a mix of small science projects andlarger efforts that create significant experimental prototypes. large numbers of small individual projects are often the best way of exploring newconcepts. a smaller number of technology demonstration systems candraw on the successes of basic research in architecture, software, and applications concepts, demonstrate their interplay, and validate conceptsahead of their use in preproduction or production systems. these wouldtypically be the sorts of projects centered at universities or research laboratories.it is difficult to determine the u.s. government investment in supercomputing research at the present time, in terms of either money or thenumber of projects. the current blue book12 has a category called highend computing research and development. (this annual publication is asupplement to the presidentõs budget submitted to congress that trackscoordinated it research and development, including hpc, across the federal government.13) from the description of the programs in various agencies, one sees that the category includes efforts that are in developmentand research efforts, as well as research in topics outside the scope of thisdiscussion (such as quantum computing or astronaut health monitoring).the recent hecrtf report14 estimates 2004 funding for basic and appliedresearch in highend computing to be $42 million.a search of the number of funded nsf projects with the word òparalleló in the title or abstract (admittedly an imperfect measure) showsthat there were an average of 75 projects per year in the 1990s, but only25 from 2000 to 2003.15 the committee does not have numbers for otheragencies, but its experience suggests that there were decreases at least as12national coordination office for information technology research and development.2004. advanced foundations for american innovation: supplement to the presidentõs budget. available online at <http://www.hpcc.gov/pubs/blue04/>.13an archive of these documents is at <http://www.hpcc.gov/pubs/bb.html>.14nitrd high end computing revitalization task force (hecrtf). 2003. report of theworkshop on the roadmap for the revitalization of highend computing. daniel a. reed, ed. june1620, washington, d.c.15these projects include some that entail only equipment or workshop sponsorship and afew that have nothing to do with supercomputing. on the other hand, there are undoubtedly supercomputing projects that have not been described using the word òparallel.ógetting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.stewardship and funding of supercomputing217great at other agencies. decreases in ph.d. production and publicationof supercomputing research papers are consistent with this falloff insupport.the committee estimates the necessary investment in these projects atapproximately $140 million per year, with approximately 35 to 45 projectsof 3 to 5year duration initiated each year and funded at $300,000 to$600,000 per year and three or four technology demonstration projectsaveraging 5 years in length initiated each year, each at between $3 millionand $5 million per year. even the smaller projects need professional staff,which becomes more expensive as the number of staff members increases.the demonstration projects will likely involve larger, multidisciplinaryteams and may require the manufacture of expensive hardware and thedevelopment of large, complex software systems. both small and largeprojects will often require more expensive infrastructure and more extensive and expensive personnel than similar nsfsupported projects in computer science and computer engineering; the underlying platforms arelarge, complex, and expensive, and most of the difficult problems are atthe integration level. the limited supercomputing industrial base precludes the industrial supportñin particular, equipment donationsñthatoften supplements federal research funding in other areas in computerscience and computer engineering.that estimate does not include support for applications research thatuses supercomputingñit includes only support for research that directlyenables advances in supercomputers themselves. also, it does not includeadvanced development, testbeds, and prototyping activities that are closerto product creation (such as darpaõs hpcs program). the estimate isnecessarily approximate but would bring us part of the way back to thelevel of effort in the 1990s. as one data point, to increase the number ofph.d.õs to 50 a year would require approximately $15 million a year justfor their direct support (assuming an average of $60,000 per year and 5years per student), and that education would come only in the context ofprojects on which they worked. not all projects are conducted inacademia, and not all projects produce ph.d. students in any given year.prototypes closer to production would normally be produced not byresearch groups but by companies and advanced development organizations (usually with research collaborators). the first two phases of thedarpa hpcs program are sponsoring activities of that kind, at a levelof about $60 million per year. this level for the three projects seems reasonable.by way of comparison, the atkins report16 (chapter 6) proposes a16daniel e. atkins. 2003. revolutionizing science and engineering through cyberinfrastructure:report of the national science foundation blueribbon advisory panel on cyberinfrastructure.january.getting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.218getting up to speedyearly budget of $60 million for fundamental and applied research to advance cyberinfrastructure and a yearly budget of $100 million for òresearch into applications of information technology to advance science andengineering research.ó taking into account the fact that cyberinfrastructure includes more than supercomputing and that the categories are different, the atkins committeeõs estimate is similar to this committeeõs.the sustained cost of providing a supply of productionquality software depends in part on the funding model that is assumed. the cost of anonprofit software organization of the kind described earlier would be$10 million to $15 million per year, but such an organization would provide only a fraction of the needed software. a vertically integrated supercomputer vendor would provide some system software as part of the delivered system. the development cost for such a supplier is on the orderof $70 million per year, some of which would come from the purchase ofsystems and some from direct investment in r&d.these estimates do not include the cost of procuring capability supercomputers to satisfy government missions (except indirectly as customersof vendors). assuming a cost of between $100 million and $150 millionper procurement and six or seven procurements per year by organizations such as doe (the national nuclear security administration and theoffice of science), dod (including nsa), nsf, nih, noaa, and nasa,the procurement cost for capability supercomputers would be approximately $800 million per year. this estimate does not include the cost ofmeeting capacity computing needs.the need for peoplethe report presented in chapter 6 some results from the most recenttaulbee survey, which showed that only 35 people earned ph.d.õs in scientific computing in 2002. this is not an anomaly, as the chart in figure9.2 shows.17 the average yearly number of ph.d.õs awarded in scientificcomputing in the last 10 years was 36; on average, government laboratories hire only three of them a year. these numbers are extremely low.while it is hard to collect accurate statistics, the same situation seemsto hold for other areas of supercomputing. for example, few studentsstudy supercomputer architecture. increased and stable research fundingis needed not only to ensure a steady flow of new ideas into supercomputing but also, and perhaps more importantly, to ensure a steady flow ofnew people into supercomputing.17taulbee survey data are available at <http://www.cra.org/statistics/>.getting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.stewardship and funding of supercomputing219the need for planning and coordinationgiven the long lead time that is needed to create an ecosystem, itseems obvious that planning for technological progress would be advisable. given that there are commonalities in supercomputing systems usedfor many different purposes, it is equally obvious that coordination amonggovernment agencies, as well as within government agencies, would be agood thing. not surprisingly, many previous studies have noted the benefits of planning and coordination and have made recommendationsalong those lines. there has also been legislation for that purpose. forinstance, finding 5 of the highperformance computing act of 1991 statedas follows: òseveral federal agencies have ongoing high performancecomputing programs, but improved longterm interagency coordination,cooperation, and planning would enhance the effectiveness of these programs.ó18 among its provisions, the act directed the president to òimple051015202530354045501994199519961997199819992000200120022003ph.d.õshired by governmentfigure 9.2number of ph.d.õs in scientific computing and number hired by government laboratories.18the housesenate compromise version of s. 272, the highperformance computing act,passed the house on november 20, 1991, the senate on november 22, 1991, and was signedby the president on december 9, 1991.getting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.220getting up to speedment a national highperformance computing program, which shall (a)establish the goals and priorities for federal highperformance computing research, development, networking, and other activities; and (b) provide for interagency coordination of federal highperformance computing research, development, networking, and other activities undertakenpursuant to the program.óyet the need for planning and coordination remains. the committeegave particular attention to two aspects of planning and coordination:what needs to be done? who needs to take responsibility for it? a coordinated way to figure out what needs to be done would be to create andmaintain a supercomputing roadmap. the issue of responsibility mustsatisfy the identified needs of hardware and software producers for stability over time, for a researchtoproduction continuum, and for the continuing allocation of adequate funding.a supercomputing roadmaproadmaps are one kind of planning mechanism. a roadmap startswith a set of quantitative goals, such as the target time to solution forcertain weapons simulations or the target cost per solution for certain climate simulations. it identifies the components required to achieve thesegoals, along with their quantitative properties, and describes how theywill enable achievement of the final quantitative goals. for example, certain classes of technologies might enable certain processor and memoryspeeds. in order to evaluate progress, conduct rational short and mediumterm planning, and accommodate increasing scientific demands, theroadmap should specify not just a single performance goal (like petaflops)at a distant point in time but a sequence of intermediate milestones aswell. the roadmap also identifies the activities (for instance, work onhigher bandwidth networks or work on higher performance optimizationtools) and the resources (such as widgets, money, or people) needed foreach goal. a roadmap is periodically updated to reflect current progressand needs. the roadmap needs to be quantitative to allow rational investment decisions and instill confidence that the ultimate goal will bereached.one wellknown roadmap activity is that by the semiconductor industry,19 which spends approximately $1 million per year on the effort.19w.j. spencer and t.e. seidel. in press. òinternational technology roadmaps: the u.s.semiconductor experience.ó productivity and cyclicality in semiconductors: trends, implications, and questions. dale w. jorgenson and charles w. wessner, eds., washington, d.c.:the national academies press.getting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.stewardship and funding of supercomputing221many recent supercomputingrelated reports identify the need forroadmaps and attempt to provide them.20 however, these reports onlypartially quantify their goals: they do not identify all the necessary components and almost universally do not quantitatively explain how thecomponents will enable reaching the final goal. for this reason, they cannot yet be used for rational investment decisions.in the case of the semiconductor industry roadmap, because the participants all share common economic goals and motivations, they arestrongly motivated to cooperate. this is not the case in supercomputing,where the participants include (at least) the national security establishment, academic scientists, industrial users, and the computer industry.the computer industryõs commercial goals of building costeffective computers for popular commercial applications and the national securityestablishmentõs goal of weapons simulation may or may not be wellaligned. the goals of climate modeling, weapons simulation, cryptography, and the like may all require somewhat different supercomputer systems for their attainment. but they will all share certain components, likescalable debuggers. so a supercomputing roadmap will necessarily besomewhat different from the semiconductor industry roadmap.in particular, some components of the roadmap will be inputs fromthe computer industry, basically a set of different technology curves (suchas for commercial processors and for custom interconnects) with associated performances and costs as functions of time. other components willbe application codes, along with benchmarks, performance models, andperformance simulations that measure progress toward the final goals.activities will include better software tools and algorithms, whose contribution is notoriously hard to quantify because of the difficulties of software engineering metrics and the unpredictability of algorithm breakthroughs but whose availability is nonetheless essential.for each application, the roadmap will investigate a set of technological solutions (combinations of algorithms, hardware, and software) andfor each one estimate as carefully as possible both the time to solution (orits reciprocal, speed) and the total cost of ownership. (these were bothdiscussed in more detail in the section on metrics in chapter 5.) finally,given a utility function, which could be the cheapest solution that meets acertain hard deadline, or the maximum number of solutions per dollar, orwhatever criteria are appropriate, it might be possible to choose the optimal technological solution.20for example, the nitrdõs hecrtf report, the asc curves and barriers report (òascitechnology prospectus,ó doe/dp/ascatp001, july 2001), and nasaõs earth scienceenterprise report.getting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.222getting up to speedthe above process is a typical rational industrial planning process. aunique feature of supercomputing that makes it difficult is the technicalchallenge of estimating the time to solution of a complicated problem on afuture hardware and software platform that is only partially defined. hereare some possible outcomes of this roadmap process:¥performance models will show that some applications scale oncommoditycluster technology curves to achieve their goals. for theseapplications, no special government intervention is needed.¥for other applications, it may be the case that the algorithms usedin the application will not scale on commoditycluster technology curvesbut that known alternative algorithms will scale. supporting these applications may require investment in algorithms and software but not hardware.¥for yet other applications, commodity processors will be adequate,but only with custom interconnects. in this case, government investmentin supercomputer interconnection network technology will be required,in addition to the investment in associated software and related costs.¥for some applications, only fullcustom solutions will work. in thiscase longterm technology r&d and òsubmarineóstyle procurement willbe required.it is likely that this roadmap process will identify certain commontechnologies that different applications can use, such as software tools,and it will be fortunate if this turns out to be so. indeed, in order to leverage government investment, the roadmap process must be coordinated atthe top in order to identify as many common solutions as possible.responsibility and oversightin response to the highperformance computing act of 1991, thenational coordination office for high performance computing and communications (nco/hpcc) was established in september 1992. (it has hadseveral name changes subsequently.) that office has done an excellent jobover the years of fostering information exchange among agencies, facilitating interagency working groups, and increasing human communication within the government concerning highend computing. however,its role has been coordination, not longrange planning.the highperformance computing act of 1991 also directed the president to establish an advisory committee on highperformance computing.that committee, the presidentõs information technology advisory committee (pitac), which was not established until 1997 under a somewhatgetting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.stewardship and funding of supercomputing223broader mandate, issued a report in february 1999, in which it recommended that a senior policy official be appointed and that a seniorlevelpolicy and coordination committee be established for strategic planningfor information technology r&d.21 neither recommendation has beenfollowed.in may 2004, an interagency high end computing revitalization taskforce (hecrtf) report again recommended an interagency governanceand management structure. the report suggests some forms that such astructure might take.22 legislation has been proposed to implement thatrecommendation.the nsf is a primary sponsor of basic research in science and engineering and thus has the responsibility to support both the engineeringresearch needed to drive progress in supercomputing as well as the infrastructure needs of those using supercomputing for their scientific research.however, a study of research grants in areas such as computer architecture shows a steady decrease in research focused on highperformancecomputing in the last decade: nsf has essentially ceased to support newhpcmotivated research in areas such as computer architecture or operating systems. in computational sciences, reduced nsf support for longterm basic research is not compensated for by an increase in doe supportthrough the scidac program, because the latterõs 5year project goals arerelatively near term. the significant darpa investment in the hpcs program has not extended to the support of basic research. there is at presenta gap in basic research in key supercomputing technologies.nsf supported supercomputing infrastructure through the paci program, which ended in september 2004. there is some uncertainty aboutfollowon programs. supercomputing infrastructure at nsf is the responsibility of the division of shared cyberinfrastructure (sci) within the directorate for computer and information science and engineering (cise);most of the users of this infrastructure are supported by other disciplinary directorates in nsf, or by nih. the role of supercomputing in thelarger cyberinfrastructure is not yet clear. this uncertainty continues tohurt supercomputing centers: it leads to a loss of talent as the more creative and entrepreneurial scientists move to areas that seem to offer morepromising opportunities, and it leads to a conservative strategy of diversifying into many different directions and small projects to reduce risk,21pitac. 1999. report to the president: information technology research: investing in our future. february.22national coordination office for information technology research and development.2004. federal plan for highend computing: report of the highend computing revitalizationtask force (hecrtf). may.getting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.224getting up to speedrather than placing a few large bets on projects that could have an important impact.this chapter has focused on the tangible aspects of supercomputingand the actions needed to improve them. however, one should not neglect the intangible assets of the supercomputing enterprise. supercomputing has attracted the brightest minds and drawn broad support because of the reality as well as the perception that it is a cuttingedge,worldchanging endeavor. the reality has not changed. there are difficultfundamental computer science and engineering problems that need to besolved in order to continue pushing the performance of supercomputersat the current rate. clearly, fundamental changes will be needed in theway supercomputers are built and programmed, to overcome these problems. supercomputers are becoming essential to research in an evergrowing range of areas; they are solving fundamental scientific problems andare key to progress on an increasing range of societal issues. computational science is becoming an increasingly challenging intellectual pursuitas the ad hoc use of numerical recipes is replaced by a deeper understanding of the relation between the physical world and its discrete representation. the reality is there, but, arguably, the perception has dimmed. assome uses of highperformance computing become easier and more common, it becomes easier to forget the incredibly difficult and immenselyimportant challenges of supercomputing.initiatives to buttress the research on supercomputing technologiesand the use of supercomputers in science and engineering should addressthe perception as well as the reality. it is important that research programs be perceived as addressing grand challenges: the grand engineering challenge of building systems of incredible complexity that are at theforefront of computer technology and the grand scientific challenges addressed by these supercomputers. it is also important that governmentagencies, supercomputing centers, and the broad supercomputing community do not neglect cultivating an image they may take too much forgranted.getting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.22510the future of supercomputingñconclusions and recommendationschapters 1 through 9 describe a long and largely successful historyof supercomputing, a present state of turmoil, and an uncertainfuture. in this chapter the committee summarizes what it haslearned during this study and what it recommends be done.conclusionssupercomputing has a proud history in the united states. ever sincethe 1940s our nation has been a leader in supercomputing. although earlyapplications were primarily military ones, by the 1960s there was a growing supercomputer industry with many nonmilitary applications. theonly serious competition for u.s. vendors has come from japanese vendors. while japan has enhanced vectorbased supercomputing, culminating in the earth simulator, the united states has made major innovationsin parallel supercomputing through the use of commodity components.much of the software running on the earth simulator and on supercomputer platforms everywhere originates from research performed in theunited states.conclusion:since the inception of supercomputing, the unitedstates has been a leader and an innovator in the field.ever since the 1960s, there have been differences between supercomputing and the broader, more mainstream computing market. one difference has been the higher performance demanded (and paid for) by supercomputer users. another difference has been the emphasis ofgetting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.226getting up to speedsupercomputer users on the mathematical aspects of software and on thedata structures and computations that are used in scientific simulations.however, there has always been interplay between advances in supercomputing (hardware and software) and advances in mainstream computing.there has been enormous growth in the dissemination and use ofcomputing in the united states and in the rest of the world since the 1940s.the growth in computing use overall has been significantly greater thanthe growth in the use of supercomputing. as computing power has increased, some former users of supercomputing have found that theirneeds are satisfied by computing systems closer to the mainstream.conclusion:supercomputing has always been a specialized format the cutting edge of computing. its share of overall computing hasdecreased as computing has become ubiquitous.supercomputing has been of great importance throughout its historybecause it has been the enabler of important advances in crucial aspects ofnational defense, in scientific discovery, and in addressing problems ofsocietal importance. at the present time, supercomputing is used to tacklechallenging problems in stockpile stewardship, in defense intelligence, inclimate prediction and earthquake modeling, in transportation, in manufacturing, in societal health and safety, and in virtually every area of basicscience understanding. the role of supercomputing in all of these areas isbecoming more important, and supercomputing is having an evergreaterinfluence on future progress. however, despite continuing increases incapability, supercomputer systems are still inadequate to meet the needsof these applications. although it is hard to quantify in a precise mannerthe benefits of supercomputing, the committee believes that the returnson increased investments in supercomputing will greatly exceed the costof these investments.conclusion:supercomputing has played, and continues to play, anessential role in national security and in scientific discovery. theability to address important scientific and engineering challengesdepends on continued investments in supercomputing. moreover,the increasing size and complexity of new applications will requirethe continued evolution of supercomputing for the foreseeablefuture.supercomputing benefits from many technologies and products developed for the broad computing market. most of the top500 listed systems are clusters built of commodity processors. as commodity processors have increased in speed and decreased in price, clusters havebenefited. there is no doubt that commoditybased supercomputing sysgetting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.conclusions and recommendations227tems are cost effective in many applications, including some of the mostdemanding ones.however, the design of commodity processors is driven by the needsof commercial data processing or personal computing; such processorsare not optimized for scientific computing. the linpack benchmark thatis used to rank systems in the top500 list is representative of supercomputing applications that do not need high memory bandwidth (becausecaches work well) and do not need high global communication bandwidth. such applications run well on commodity clusters. many important applications need better local memory bandwidth and lower apparent latency (i.e., better latency hiding), as well as better global bandwidthand latency. technologies for better bandwidth and latency exist. betterlocal memory bandwidth and latency are only available in custom processors. better global bandwidth and latency are only available in custominterconnects with custom interfaces. the availability of local and globalhigh bandwidth and low latency improves the performance of the manycodes that leverage only a small fraction of the peak performance of commodity systems because of bottlenecks in access to local and remotememories. the availability of local and global high bandwidth can alsosimplify programming, because less programmer time needs to be spentin tuning memory access and communication patterns, and simpler programming models can be used. furthermore, since memory access time isnot scaling at the same rate as processor speed, more commodity clusterusers will become handicapped by low effective memory bandwidth. although increased performance must be weighed against increased cost,there are some applications that cannot achieve the needed turnaroundtime without custom technology.conclusion:commodity clusters satisfy the needs of many supercomputer users. however, some important applications need thebetter main memory bandwidth and latency hiding that are available only in custom supercomputers; many need the better globalbandwidth and latency interconnects that are available only in custom or hybrid supercomputers; and most would benefit from thesimpler programming model that can be supported well on customsystems. the increasing gap between processor speed and communication latencies is likely to increase the fraction of supercomputing applications that achieve acceptable performance only oncustom and hybrid supercomputers.supercomputing systems consist not only of hardware but also of software. there are unmet needs in supercomputing software at all levels,from the operating system to the algorithms to the applicationspecificsoftware. these unmet needs stem from both technical difficulties andgetting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.228getting up to speeddifficulties in maintaining an adequate supply of people in the face ofcompeting demands on software developers. particularly severe needsare evident in software to promote productivityñthat is, to speed the solution process by reducing programmer effort or by optimizing executiontime. while many good algorithms exist for problems solved on supercomputers, needs remain for a number of reasons: (1) because the problems being attempted on supercomputers have difficulties that do notarise in those being attempted on smaller platforms, (2) because new modeling and analysis needs arise only after earlier supercomputer analysespoint them out, and (3) because algorithms must be modified to exploitchanging supercomputer hardware characteristics.conclusion:advances in algorithms and in software technology atall levels are essential to further progress in solving applicationsproblems using supercomputing.supercomputing software, algorithms, and hardware are closelybound. as architectures change, new software solutions are needed. ifarchitectural choices are made without considering software and algorithms, the resulting system may be unsatisfactory. because a supercomputing system is a kind of ecosystem, significant changes are both disruptive and expensive. attention must therefore be paid to all aspects of theecosystem and to their interactions when developing future generationsof supercomputers.educated and skilled people are an important part of the supercomputing ecosystem. supercomputing experts need a mix of specializedknowledge in the applications with which they work and in the varioussupercomputing technologies.conclusion:all aspects of a particular supercomputing ecosystem,be they hardware, software, algorithms, or people, must be strong ifthe ecosystem is to function effectively.computer suppliers are by nature economically opportunistic andmove into areas of greatest demand and largest potential profit. becauseof the high cost of creating a supercomputing ecosystem and the relatively small customer base, the supercomputing market is less profitableand riskier. custom systems form a small and decreasing fraction of thesupercomputer market and are used primarily for certain governmentapplications. the commercial demand for such systems is not sufficient tosupport vendors of custom supercomputers or a broad range of commercial providers of software for highperformance science and engineeringapplications. as the commodity market has grown, and as the costs ofdeveloping commodity components have risen, government missions aregetting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.conclusions and recommendations229less able to influence the design of commodity products (they might notsucceed, for example, in having certain features included in instructionsets). although spillovers from solutions to the technical problems facingsupercomputing will eventually benefit the broader market, there is notsufficient shortterm benefit to motivate commercial r&d.the government has always been the primary consumer and funderof supercomputing. it has sponsored advances in supercomputing in order to ensure that its own needs are met. it is a customer both directly,through purchases for government organizations, and indirectly, throughgrants and contracts to organizations that in turn acquire supercomputers.although supercomputing applications could be very important to industry in areas such as transportation, energy sources, and product design, industry is not funding the development of new supercomputer applications or the major scaling of current applications.conclusion:the supercomputing needs of the government will notbe satisfied by systems developed to meet the demands of thebroader commercial market. the government has the primary responsibility for creating and maintaining the supercomputing technology and suppliers that will meet its specialized needs.the dod has to assure the development and production of cuttingedge weapons systems such as aircraft and submarines, which are notdeveloped or produced for the civilian market. to do this, it continuouslyundertakes to analyze which capabilities are needed in the defense industrial base, and it maintains these capabilities and has an ongoing longterm investment strategy to guarantee that there will always be suppliersto develop and produce these systems. similarly, to ensure its access tospecialized custom supercomputers that would not be produced withoutgovernment involvement, dod needs the same kind of analysis of capabilities and investment strategy. the strategy should aim at leveragingtrends in the commercial computing marketplace as much as possible, butin the end, responsibility for an effective r&d and procurement strategyrests with the government agencies that need the custom supercomputers.however, the analogy with aircraft and submarines breaks down inone essential aspect: not only are custom supercomputers essential to oursecurity, they can also accelerate many other research and engineeringendeavors. the scientific and engineering discovery enabled by suchsupercomputers has broad societal and economic benefits, and government support of the r&d for these supercomputers may broaden theiruse by others outside the government. broader use by industry is desirable and should be encouraged, because of the positive impact on u.s.competitiveness and the positive impact on supercomputing vendors.getting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.230getting up to speedconclusion:government must bear primary responsibility formaintaining the flow of resources that guarantees access to the custom systems it needs. while an appropriate strategy will leveragedevelopments in the commercial computing marketplace, the government must routinely plan for developing what the commercialmarketplace will not, and it must budget the necessary funds.for a variety of reasons, the government has not always done a goodjob in its stewardship role. predictability and continuity are importantprerequisites for enhancing supercomputing performance for use in applications. unstable government funding and a nearterm planning focuscan result in (and have resulted in) high transition costs, limiting the exploitation of supercomputing advances for many applications. unevenand unpredictable acquisition patterns have meant fewer industrial suppliers of hardware and software, as companies have closed or moved intoother areas of computing. insufficient investment in longterm basic r&dand in research access to supercomputers has eroded opportunities tomake major progress in the technical challenges facing supercomputing.conclusion:the government has lost opportunities for importantadvances in applications using supercomputing, in supercomputingtechnology, and in ensuring an adequate supply of supercomputingecosystems in the future. instability of longterm funding and uncertainty in policies have been the main contributors to this loss.recommendationstaken together, the conclusions reached from this study lead to anoverall recommendation:overall recommendation:to meet the current and future needs ofthe united states, the government agencies that depend on supercomputing, together with the u.s. congress, need to take primaryresponsibility for accelerating advances in supercomputing andensuring that there are multiple strong domestic suppliers of bothhardware and software.the government is the primary user of supercomputing. governmentfunded research is pushing the frontiers of knowledge and bringing important societal benefits. advances in supercomputing must be accelerated to maintain u.s. military superiority, to achieve the goals of stockpilestewardship, and to maintain national security. continued advances insupercomputing are also vital for a host of scientific advancements in biology, climate, economics, energy, material science, medicine, physics,getting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.conclusions and recommendations231and seismology. because all of these are, directly or indirectly, the responsibility of the government, it must ensure that the supercomputing infrastructure adequately supports the nationõs needs in coming years. theseneeds are distinct from those of the broader information technology industry because they involve platforms and technologies that are unlikelyon their own to have a broad enough market any time soon to satisfy theneeds of the government.to facilitate the governmentõs assumption of that responsibility, thecommittee makes eight recommendations.recommendation 1.to get the maximum leverage from the national effort, the government agencies that are the major users ofsupercomputing should be jointly responsible for the strength andcontinued evolution of the supercomputing infrastructure in theunited states, from basic research to suppliers and deployedplatforms. the congress should provide adequate and sustainedfunding.a small number of government agencies are the primary users ofsupercomputing, either directly, by themselves acquiring supercomputerhardware or software, or indirectly, by awarding contracts and grants toother organizations that purchase supercomputers. these agencies arealso the major funders of supercomputing research. at present, thoseagencies include the department of energy (doe), including its nationalnuclear security administration and its office of science; the departmentof defense (dod), including its national security agency (nsa); the national aeronautics and space administration (nasa); the national oceanic and atmospheric administration (noaa); and the national sciencefoundation (nsf). (the increasing use of supercomputing in biomedicalapplications suggests that nih should be added to the list.) although theagencies have different missions and different needs, they benefit fromthe synergies of coordinated planning and acquisition strategies and coordinated support for r&d. in short, they need to be part of the supercomputing ecosystem. for instance, many of the technologies, in particularthe software, need to be broadly available across all platforms. if the agencies are not jointly responsible and jointly accountable, the resources spenton supercomputing technologies are likely to be wasted as efforts are duplicated in some areas and underfunded in others.achieving collaborative and coordinated government support forsupercomputing is a challenge that many previous studies have addressedwithout effecting much improvement in daytoday practice. what isneeded is an integrated plan rather than the coordination of distinctsupercomputing plans through a diffuse interagency coordination structure. such integration across agencies has not been achieved in the past,getting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.232getting up to speedand interagency coordination mechanisms have served mostly to communicate independently planned activities. a possible explanation is thatalthough each agency needs to obtain supercomputing for its own purposes, no agency has the responsibility to ensure that the necessary technology will be available to be acquired.today, much of the coordination happens relatively late in the planning process and reflects decisions rather than goals. in order for the agencies to meet their own mission responsibilities and also take full advantage of the investments made by other agencies, collaboration andcoordination must become much more long range. to make that happen,the appropriate incentives must be in placeñcollaboration and coordination must be based on an alignment of interests, not just on a threat ofvetoes from higherlevel management.one way to facilitate that process is for the agencies with a need forsupercomputing to create and maintain a joint 5 or 10year written planfor highend computing (hec) based on both the roadmap that is thesubject of recommendation 5 and the needs of the participating agencies.that hec plan, which would be revised annually, would be increasinglyspecific with respect to development and procurement as the time remaining to achieve particular goals decreased. included in the plan would be aclear delineation of which agency or agencies would be responsible forcontracting and overseeing a large procurement, such as a customsupercomputer system or a major hardware or software component ofsuch a system. the plan would also include cost estimates for elements ofthe plan, but it would not be an overall budget. for example, planning forthe development and acquisition of what the hecrtf report calls òleadership systemsó would be part of this overall hec plan, but the decisionsabout what to fund would not be made by the planners. each new versionof the plan would be critically reviewed by a panel of outside experts andupdated in response to that review.appropriate congressional committees in the house and senatewould have the funding and oversight responsibility to ensure that thehec plan meets the longterm needs of the nation. both the house andsenate authorization and appropriation subcommittees and the office ofmanagement and budget would require (1) that every budget request concerning supercomputing describe how the request is aligned with the hecplan and (2) that an agency budget request does not omit a supercomputing investment (for which it has responsibility according to the hecplan) on which other agencies depend. similarly, house and senate appropriation committees would ensure (1) that budgets passed into laware consistent with the hec plan and (2) that any negotiated budget reductions do not adversely affect other investments dependent on them.consistency does not imply that every part of every request would be ingetting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.conclusions and recommendations233the plan. mission agencies sometimes face shortterm needs to meet shortterm deliverables that cannot be anticipated. new disruptive technologies sometimes provide unanticipated opportunities. however, revisionsto the plan would be responsive to those needs and opportunities.the use of an hec plan would not preclude agencies from individualactivities, nor would it prevent them from setting their own priorities.rather, the intent is to identify common needs at an early stage and toleverage shared efforts to meet those needs, while minimizing duplicativeefforts. for example,¥research and development in supercomputing will continue to bethe responsibility of the agencies that fund research and also use supercomputing, notably nsf, doe (the national nuclear security administration and the office of science), dod, nsa, nasa, noaa, and nih. asubset of these agencies, working in loose coordination, will focus on longterm basic research in supercomputing technologies. another subset ofthese agencies, working in tighter coordination, will be heavily involvedin industrial supercomputing r&d.¥each agency will continue to be responsible for the development ofthe domainspecific technologies, in particular domainspecific applications software, that satisfy its needs.¥the acquisition of supercomputing platforms will be budgeted forby each agency according to its needs. joint planning and coordination ofacquisitions will increase the efficiency of the procurement processes fromthe government viewpoint and will decrease variability and uncertaintyfrom the vendor viewpoint. in particular, procurement overheads anddelays can be reduced with multiagency acquisition plans whereby oncea company wins a procurement bid issued by one agency, other agenciescan buy versions of the winning system.¥tighter integration in the funding of applied research and development in supercomputing will ease the burden on application developers and will enhance the viability of domestic suppliers.until such a structure is in place, the agencies whose missions rely onsupercomputing must take responsibility for the future availability ofleading supercomputing capabilities. that responsibility extends to thebasic research on which future supercomputing depends. these agenciesshould cooperate as much as they canñleveraging one anotherõs efforts isalways advantageousñbut they must move ahead whether or not a formal longterm planning and coordination framework exists. more specifically, it continues to be the responsibility of the nsf, dod, and doe, asthe primary sponsors of basic research in science and engineering, to support both the research needed to drive progress in supercomputing andgetting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.234getting up to speedthe infrastructure needs of those using supercomputing for their research.similarly, it is the responsibility of those agencies whose mission is thesafety and security of the nation or the health and wellbeing of its citizens to plan for future supercomputing needs essential to their missions,as well as to provide for presentday supercomputing needs.recommendation 2.the government agencies that are the primaryusers of supercomputing should ensure domestic leadership inthose technologies that are essential to meet national needs.some critical government needs justify a premium for faster and morepowerful computation that most or all civilian markets cannot justify commercially. many of these critical needs involve national security. becausethe united states may want to be able to restrict foreign access to somesupercomputing technology, it will want to create these technologies hereat home. even if there is no need for such restrictions, the united stateswill still need to produce these technologies domestically, simply becauseit is unlikely that other countries will do so given the lack of commercialmarkets for many of these technologies. u.s. leadership in unique supercomputing technologies, such as custom architectures, is endangered byinadequate funding, inadequate longterm plans, and the lack of coordination among the agencies that are the major funders of supercomputingr&d. those agencies should ensure that our country has the supercomputers it needs to satisfy critical requirements in areas such as cryptography and nuclear weapon stewardship as well as for systems that willprovide the breakthrough capabilities that bring broad scientific and technological progress for a strong and robust u.s. economy.the main concern of the committee is not that the united states isbeing overtaken by other countries, such as japan, in supercomputing.rather, it is that current investments and current plans are not sufficientto provide the future supercomputing capabilities that our country willneed. that the firstplace computer in the june 2004 top500 list was located in japan is not viewed by this committee as a compelling indicationof loss of leadership in technological capability. u.s. security is not necessarily endangered if a computer in a foreign country is capable of doingsome computations faster than u.s.based computers. the committee believes that had our country made an investment similar to japanõs at thesame time, it could have created a powerful and equally capable system.the committeeõs concern is that the united states has not been making theinvestments that will guarantee its ability to create such a system in thefuture.leadership is measured by a broad technological capability to acquireand exploit effectively machines that can best reduce the time to solutionof important computational problems. from this perspective, it is not thegetting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.conclusions and recommendations235earth simulator system that is worrisome but rather the fact that its construction was such a singular event. it seems that without significant government support, custom highbandwidth processors are not viable products. two of the three japanese companies that were manufacturing suchprocessors do not do so anymore, and the third (nec) may also bow tomarket realities in a not too distant futureñsince the japanese government seems less willing now to subsidize the development of leadingsupercomputing technologies. the software technology of the earth simulator is at least a decade old. the same market realities prevail here athome. no fundamentally new highbandwidth architecture has emergedas a product in the last few years in either japan or the united states. nosignificant progress has occurred in commercially available supercomputing software for more than a decade. no investment that would matchthe time scale and magnitude of the japanese investment in the earthsimulator has been made in the united states.the agencies responsible for supercomputing can ensure that keysupercomputing technologies, such as custom highbandwidth processors, will be available to satisfy their needs only by maintaining ournationõs world leadership in these technologies. recommendations 3through 8 outline some of the actions that need to be taken by these agencies to maintain this leadership.recommendation 3.to satisfy its need for unique supercomputingtechnologies such as highbandwidth systems, the governmentneeds to ensure the viability of multiple domestic suppliers.the u.s. industrial base must include suppliers on whom the government can rely to build custom systems to solve problems that are uniqueto the government role. since only a few units of such systems are everneeded, there is no broad market for them and hence no commercial, offtheshelf suppliers. domestic supercomputing vendors are a source ofboth the components and the engineering talent necessary to constructlowvolume systems for the government.to ensure their continuing existence, the domestic suppliers must beable to sustain a viable business model. for a public company, that meanshaving predictable and steady revenue recognizable by the financial market. a company cannot continue to provide cuttingedge products without r&d. at least two models of support have been used successfully: (1)an implicit guarantee of a steady purchase of supercomputing systems,giving the companies a steady income stream with which to fund ongoingr&d and (2) explicit funding for a companyõs r&d. stability is a key issue. suppliers of such systems or components often are small companiesthat can easily lose viability; uncertainty can mean the loss of skilled personnel to other sectors of the larger computing industry or the loss ofgetting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.236getting up to speedinvestors. historically, government priorities and technical directionshave changed more frequently than would be justified by technology lifetimes, creating market instabilities. the chosen funding model must ensure stability. the agencies responsible for supercomputing might consider the model proposed by the british ukhec initiative, wherebygovernment solicits and funds proposals for the procurement of three successive generations of a supercomputer family over 4 to 6 years.it is important to have multiple suppliers for any key technology, inorder to maintain competition, to prevent technical stagnation, to providediverse supercomputing ecosystems to address diverse needs, and to reduce risk. (the recent neardeath experience of cray in the 1990s is a goodexample of such risk.) on the other hand, it is unrealistic to expect thatsuch narrow markets will attract a large number of vendors. as happensfor many military technologies, one may typically end up with only a fewsuppliers. the risk of stagnation is mitigated by the continued pressurecoming from commodity supercomputer suppliers.the most important unique supercomputing technology identified inthis report is highbandwidth, custom supercomputing systems. the vector systems developed by cray have been the leading example of thistechnology. cray is now the only domestic manufacturer of such systems.the r&d cost to cray for a new product has been estimated by idc to beclose to $200 million; assuming a 3year development cycle, this results inan annual r&d cost of about $70 million, or about $140 million per yearfor two vendors. note that cray has traditionally been a vertically integrated company that develops and markets a product stack that goes fromchips and packaging to system software, compilers, and libraries. however, cray seems to be becoming less integrated, and other suppliers ofhighbandwidth systems may choose to be less integrated, resulting in adifferent distribution of r&d costs among suppliers. other suppliers mayalso choose highbandwidth architectures that are not vector.another unique supercomputing technology identified in this reportis that of custom switches and custom, memoryconnected switch interfaces. companies such as cray, ibm, and sgi have developed such technologies and have used them exclusively for their own productsñthecray red storm interconnect is a recent example. myricom (a u.s. company) and quadrics (a european company) develop scalable, highbandwidth, lowlatency interconnects for clusters, but use a standard i/o bus(pcix) interface and support themselves from the broader cluster market. the r&d costs for such products are likely to be significantly lowerthan for a full custom supercomputer.these examples are not meant to form an exhaustive list of leadershipsupercomputing technologies. the agencies that are the primary users ofsupercomputing should, however, establish such a list, aided by thegetting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.conclusions and recommendations237roadmap described in recommendation 5, and should ensure that thereare viable domestic suppliers.similar observations can be made about software for highperformance computing. our ability to efficiently exploit leading supercomputing platforms is hampered by inadequate software support. the problem is not only the lack of investment in research but also, and perhapsmore seriously, the lack of sustained investments needed to promote thebroad adoption of new software technologies that can significantly reduce time to solution at the high end but that have no viable commercialmarket.recommendation 4.the creation and longterm maintenance of thesoftware that is key to supercomputing requires the support of thoseagencies that are responsible for supercomputing r&d. that software includes operating systems, libraries, compilers, software development and data analysis tools, application codes, and databases.the committee believes that the current lowlevel, uncoordinated investment in supercomputing software significantly constrains the effectiveness of supercomputing. it recommends larger and better targeted investments by those agencies that are responsible for supercomputingr&d.the situation for software is somewhat more complicated than thatfor hardware: some softwareñin particular, application codesñis developed and maintained by national laboratories and universities, and somesoftware, such as the operating system, compiler, and libraries, is provided with the hardware platform by a vertically integrated vendor. thesame type of software, such as a compiler or library, that is packaged andsold by one (vertical) vendor with the hardware platform is developedand maintained by a (horizontal) vendor as a standalone product that isavailable on multiple platforms. additionally, an increasing amount ofthe software used in supercomputing is developed in an open sourcemodel. the same type of software, such as a communication library, maybe freely available in open source and also available from vendors undera commercial license.different funding models are needed to accommodate these differentsituations. a key goal is to ensure the stability and longevity of organizations that maintain and evolve software. the successful evolution andmaintenance of complex software systems are critically dependent on institutional memoryñthat is, on the continuous involvement of the fewkey developers that understand the software design. stability and continuity are essential to preserve institutional memory. whatever model ofsupport is used, it should be implemented so that a stable organizationwith a lifetime of decades can maintain and evolve the software. many ofgetting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.238getting up to speedthe supercomputing software vendors are very small (tens of employees)and can easily fail or be bought out, even if they are financially viable. forexample, several vendors of compilers and performance tools for supercomputing were acquired by intel in the last few years. as a result, developers who were working on highperformance computing productsshifted to work on technologies with a broader market. the open sourcemodel is not, per se, a guarantee of stability, because it does not ensurecontinuing stable support for the software.it is also important to provide funding for software integration, as it isoften a major source of function and performance bugs. such integrationwas traditionally done by vertically integrated vendors, but new modelsare needed in the current, less integrated world of supercomputing.as it invests in supercomputing software, the government must carefully balance its need to ensure the availability of software against thepossibility of driving its commercial suppliers out of business by subsidizing their competitors, be they in government laboratories or in othercompanies. the government should not duplicate successful commercialsoftware packages but should instead invest in technology that does notyet exist. when new commercial providers emerge, the governmentshould purchase their products and redirect its own efforts toward technology that it cannot acquire off the shelf. hpss and totalview are examples of successful partnerships between government and the supercomputing software industry. nastran and dyna are examples ofgovernmentfunded applications that were successfully transitioned tocommercial suppliers.barriers to the replacement of application programming interfaces arevery high owing to the large sunk investments in application software.any change that significantly enhances our ability to program very largesystems will entail a radical, coordinated change of many technologies,creating a new ecosystem. to make this change, the government needslongterm coordinated investments in a large number of interlocking technologies.recommendation 5.the government agencies responsible forsupercomputing should underwrite a community effort to developand maintain a roadmap that identifies key obstacles and synergiesin all of supercomputing.a roadmap is necessary to ensure that investments in supercomputingr&d are prioritized appropriately. the challenges in supercomputing arevery significant, and the amount of ongoing research is quite limited. tomake progress, it is important to identify and address the key roadblocks.furthermore, technologies in different domains are interdependent:getting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.conclusions and recommendations239progress on a new architecture may require, in addition to computer architecture work, specific advances in packaging, interconnects, operatingsystem structures, programming languages and compilers, and so forth.thus, investments need to be coordinated. to drive decisions, one needs aroadmap of the technologies that affect supercomputing. the roadmapneeds to have quantitative and measurable milestones.some examples of roadmaplike planning activities are the semiconductor industryõs roadmap, the asc curves and barriers workshops, andthe petaflops workshops. however, none of these is a perfect model. it isimportant that a supercomputing roadmap be driven both topdown byapplication needs and bottomup by technology barriers and that missionneeds as well as science needs be incorporated. its creation and maintenance should be an open process that involves a broad community. thatcommunity should include producersñcommodity as well as custom,components as well as full systems, hardware as well as softwareñandconsumers from all user communities. the roadmap should focus on theevolution of each specific technology and on the interplay between technologies. it should be updated annually and undergo major revisions atsuitable intervals.the roadmap should be used by agencies and by congress to guidetheir longterm research and development investments. those roadblocksthat will not be addressed by industry without government interventionneed to be identified, and the needed research and development must beinitiated. metrics must be developed to support the quantitative aspectsof the roadmap. it is important also to invest in some highrisk, highreturn research ideas that are not indicated by the roadmap, to avoid being blindsided.recommendation 6.government agencies responsible for supercomputing should increase their levels of stable, robust, sustainedmultiagency investment in basic research. more research is neededin all the key technologies required for the design and use of supercomputers (architecture, software, algorithms, and applications).the top performance of supercomputers has increased rapidly in thelast decades, but their sustained performance has lagged, and the productivity of supercomputing users has lagged as well.1 during the lastdecade the advance in supercomputing performance has been largely due1see, for example, figure 1 in the hecrtf report, at <http://www.hpcc.gov/pubs/2004hecrtf/20040702hecrtf.pdf>.getting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.240getting up to speedto the advance in microprocessor performance driven by increased miniaturization, with limited contributions from increasing levels of parallelism.2it will be increasingly difficult for supercomputing to benefit fromimprovements in processor performance in the coming decades. for reasons explained in chapter 5, the rate of improvement in singleprocessorperformance is decreasing; chip performance is improved mainly by increasing the number of concurrent threads executing on a chip (an increase in parallelism). additional parallelism is also needed to hide theincreasing relative memory latency. thus, continued improvement insupercomputer performance at current rates will require a massive increase in parallelism, requiring significant research progress in algorithmsand software. as the relative latencies of memory accesses and global communications increase, the performance of many scientific codes willshrink, relative to the performance of more cache friendly and moreloosely coupled commercial codes. the cost/performance advantage ofcommodity systems for these scientific codes will erode. as discussed inchapter 5, an extrapolation of current trends clearly indicates the need forfundamental changes in the structure of supercomputing systems in a nottoo distant future. to effect these changes, new research insupercomputing architecture is also needed.perhaps as a result of the success of commoditybased systems, thelast decade saw few novel technologies introduced into supercomputersystems and a reduction in supercomputing research investments. thenumber and size of supercomputingrelated grants in computer architecture or computer software have decreased. as the pressure for fundamental changes grows, it is imperative to increase investments in supercomputing research.the research investments should be balanced across architecture, software, algorithms, and applications. they should be informed by thesupercomputing roadmap but not constrained by it. it is important to focus on technologies that have been identified as roadblocks and that arebeyond the scope of industry investments in computing. it is equally important to support longterm speculative research in potentially disruptive technical advances. the research investment should also be informedby the òecosystemó view of supercomputingñnamely, that progress mustcome on a broad front of interrelated technologies rather than in the formof individual breakthroughs.one of the needs of an ecosystem is for skilled and welleducated2surprisingly, the number of processors of top supercomputers did not scale up much inthe last decade: the top system in the june 1994 top500 list had 3,680 processors; the topsystem in the june 2004 list had 5,120 processors.getting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.conclusions and recommendations241people. opportunities to educate and train supercomputing professionalsshould be part of every research program. steady funding for basic research at universities, together with opportunities for subsequent employment at research institutions and private companies, might attract morestudents to prepare for a career in supercomputing.research should include a mix of small, medium, and large projects.many small individual projects are necessary for the development of newideas. a smaller number of large projects that develop technology demonstrations are needed to bring these ideas to maturity and to study theinteraction between various technologies in a realistic environment. suchdemonstrations projects (which are different from product prototypingactivities) should not be expected to be stable platforms for exploitationby users, because the need to maintain a stable platform conflicts with theability to use the platform for experiments. it is important that the development of such demonstration systems have the substantial involvementof academic researchers, particularly students, to support the educationof the new generation of researchers, and that the fruits of such projectsnot be proprietary. in chapter 9, the necessary investments in suchprojects were estimated at about $140 million per year. this does not include investments in the development and use of application specific software.largescale research in supercomputing can occur in a vertical model,whereby researchers from multiple disciplines collaborate to design andimplement one technology demonstration system. or, it can occur in ahorizontal model, in a center that emphasizes one discipline or focuses onthe technology related to one roadblock in the supercomputing roadmap.a large effort focused on a demonstration system brings together peoplefrom many disciplines and is a good way of generating unexpected breakthroughs. however, such an effort must be constructed carefully so thateach of the participants is motivated by the expectation that the collaboration will advance his or her research goals.in its early days, supercomputing research generated many ideas thateventually became broadly used in the computing industry. pipelining,multithreading, and multiprocessing are familiar examples. the committee expects that such influences will continue in the future. many of theroadblocks faced today by supercomputing are roadblocks that affect allcomputing, but affect supercomputing earlier and to a more significantextent. one such roadblock is the memory wall,3 which is due to the slowerprogress in memory speeds than in processor speeds. supercomputers3wm. a. wulf and s.a. mckee. 1995. òhitting the wall: implications of the obvious.ócomputer architecture news 23(1):2024.getting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.242getting up to speedare disproportionately affected by the memory wall owing to the moredemanding characteristics of supercomputing applications. there can belittle doubt that solutions developed to solve this problem for supercomputers will eventually influence the broad computing industry, so thatinvestments in basic research in supercomputing are likely to be of broadbenefit to information technology.recommendation 7.supercomputing research is an internationalactivity; barriers to international collaboration should be minimized.research has always benefited from the open exchange of ideas andthe opportunity to build on the achievements of others. the national leadership advocated in these recommendations is enhanced, not compromised, by earlystage sharing of ideas and results. in light of the relativelysmall community of supercomputing researchers, international collaborations are particularly beneficial. the climate modeling community, forone, has long embraced that view.research collaboration must include access to supercomputing systems. many research collaborations involve colocation. many of the bestu.s. graduate students are foreigners, many of whom ultimately becomecitizens or permanent residents. access restrictions based on citizenshiphinder collaboration and are contrary to the openness that is essential togood research. such restrictions will reduce the ability of research andindustry to benefit from advances in supercomputing and will restrict thetransfer of the most talented people and the most promising ideas to classified uses of supercomputing.restrictions on the import of supercomputers to the united stateshave not benefited the u.s. supercomputing industry and are unlikely todo so in the future. restrictions on the export of supercomputers havehurt supercomputer manufacturers by restricting their market. somekinds of export controlsñon commodity systems, especiallyñlack anyclear rationale, given that such systems are in fact built from widely available cots components, most of which are manufactured overseas. itmakes little sense to restrict sales of commodity systems built from components that are not export controlled.although the supercomputing industry is similar in ways to somemilitary industries (small markets, small ecosystems, and critical importance to government missions), there are significant differences that increase the benefits and decrease the risks of a more open environment.a faster computer in another country does not necessarily endangeru.s. security; u.s. security requires a broad technological capability toacquire and exploit effectively machines that can best reduce the time tosolution of important computational problems. such technological capagetting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.conclusions and recommendations243bility is embodied not in one platform or one code but in a broad community of researchers and developers in industry, academia, and government who collaborate and exchange ideas with as few impediments aspossible.the computer and semiconductor technologies are (still) moving at afast pace and, as a result, supercomputing technology is evolving rapidly.the development cycles of supercomputers are only a few years long compared with decadeslong cycles for many weapons. to maintain its vitality, supercomputing r&d must have strong ties to the broad, open research community.the supercomputing market shares some key hardware and softwarecomponents with the much larger mainstream computing markets. if thesupercomputing industry is insulated from this larger market, there willbe costly reinvention and/or costly delays. indeed, the levels of investment needed to maintain a healthy supercomputing ecosystem pale whenthey are compared with the cost of a major weapon system. a more segregated r&d environment will inevitably lead to a higher price tag if fastprogress is to be maintained.supercomputers are multipurpose (nuclear simulations, climate modeling, and so on). in particular, they can be used to support scientific research, to advance engineering, and to help solve important societal problems. if access to supercomputers is restricted, then important publicbenefits would be lost. moreover, the use of supercomputers for broaderapplications in no way precludes their use for defense applications.finally, advances in supercomputing technology can benefit thebroader it industry; application codes developed in national laboratoriescan benefit industrial users. any restriction of this technology flow reduces the competitiveness of the u.s. industry.restrictions on the export of supercomputing technology may hamper international collaboration, reduce the involvement of the open research community in supercomputing, and reduce the use of supercomputers in research and in industry. the benefit of denying potentialadversaries or proliferators access to key supercomputing technology hasto be carefully weighed against the damage that export controls do toresearch within the united states, to the supercomputing industry, and tointernational collaborations.recommendation 8.the u.s. government should ensure that researchers with the most demanding computational requirementshave access to the most powerful supercomputing systems.access to the most powerful supercomputers is important for the advancement of science in many disciplines. the committee believes that amodel in which top supercomputing capabilities are provided by differgetting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.244getting up to speedent agencies with different missions is a healthy model. each agency is theprimary supporter of certain research or missiondriven communities;each agency should have a longterm plan and budget for the acquisitionof the supercomputing systems that are needed to support its users. theusers should be involved in the planning process and should be consultedin setting budget priorities for supercomputing. budget priorities shouldbe reflected in the hec plan proposed in recommendation 1. in chapter9, the committee estimated at about $800 million per year the cost of ahealthy procurement process that would satisfy the capability supercomputing needs (but not their capacity needs) of the major agencies usingsupercomputing and that would include the platforms primarily used forresearch. this estimate includes both platforms used for missionspecifictasks and platforms used to support science.the nsf supercomputing centers have traditionally provided openaccess to a broad range of academic users. they have been responsive totheir scientific users in installing and supporting software packages andproviding help to both novice and experienced users. however, some ofthe centers in the paci program have increased the scope of their activities, even in the face of a flat budget, to include research in networkingand grid computing and to expand their education mission. the focus oftheir activity has shifted as their mission has broadened. the increases inscope have not been accompanied by sufficient increases in funding. theexpanded mission and the flat budget have diluted the centersõ attentionto the support of computational scientists with capability needs. similardifficulties have arisen at doeõs nersc.it is important to repair the current situation at nsf, in which thecomputational science users of supercomputing centers appear to havetoo little involvement in programmatic and budgetary planning. all theresearch communities in need of supercomputing have a continuing responsibility to help to provide direction for the supercomputing infrastructure that is used by scientists of a particular discipline and to participate in sustaining the needed ecosystems. these communities shouldprioritize funding for the acquisition and operation of the research supercomputing infrastructure against their other infrastructure needs. further,such funding should clearly be separated from funding for computer andcomputational science and engineering research. users of doe and dodcenters have a similar responsibility to provide direction. this does notmean that supercomputing centers must be disciplinary. indeed, multidisciplinary centers provide incentives for collaborations that would notoccur otherwise, and they enable the participation of small communities.a multidisciplinary center should be supported by the agencies (such asnsf or nih) that support the disciplines involved, but with serious commitment from the user communities supported by these agencies.getting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.conclusions and recommendations245the planning and funding process followed by each agency must ensure stability from the usersõ viewpoint. many research groups end upusing their own computer resources, or they spend time ensuring thattheir codes run on a wide variety of systems, not necessarily because it isthe most efficient strategy but because they believe it minimizes the riskof depending on systems they do not control. this strategy traps usersinto a lowestcommondenominator programming model, which in turnconstrains the performance they might otherwise achieve by using morespecialized languages and tools. more stability in the funding and acquisition process can ultimately lead to a more efficient use of resources. finally, the mechanism used for allocating supercomputing resources mustensure that almost all of the computer time on capability systems is allocated to jobs for which that capability is essential. the earth simulatorusage policies are illustrative. supercomputers are scarce and expensiveresources that should be used not to accommodate the largest number ofusers but to solve the largest, most difficult, and most important scientificproblems.getting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.getting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.appendixesgetting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.getting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.249acommittee member andstaff biographiescommittee memberssusan l. graham (nae), cochair, is the pehong chen distinguishedprofessor of electrical engineering and computer science at the university of california, berkeley, and the chief computer scientist of the national partnership for advanced computational infrastructure (npaci).her research spans many aspects of programming language implementation, software tools, software development environments, and highperformance computing. as a participant in the berkeley unix project, sheand her students built the berkeley pascal system and the widely usedprogram profiling tool gprof. their paper on that tool was selected for thelist of best papers from 20 years of the conference on programming language design and implementation (19791999). she has done seminal research in compiler code generation and optimization. she and her students have built several interactive programming environments, yieldinga variety of incremental analysis algorithms. her current projects includethe titanium system for language and compiler support of explicitly parallel programs and the harmonia framework for highlevel interactivesoftware development. dr. graham received an a.b. in mathematics fromharvard university and m.s. and ph.d. degrees in computer science fromstanford university. she is a member of the national academy of engineering and a fellow of the association for computing machinery (acm),the american association for the advancement of science (aaas), andthe american academy of arts and sciences. in 2000 she received theacm sigplan career programming language achievement award. ingetting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.250getting up to speedaddition to teaching and research, she has been an active participant inthe development of the computer science community, both nationally andinternationally, over the past 25 years. she was the founding editor inchief of acm transactions on programming languages and systems, whichcontinued under her direction for 15 years. she has also served on theexecutive committee of the acm special interest group on programminglanguages and as a member and chair of the acm turing award committee. dr. graham has served on numerous national advisory committees,boards, and panels, including the national research councilõs (nrcõs)computer science and telecommunications board, the nrcõs commission on physical sciences, mathematics, and applications, the advisorycommittee for the nsf science and technology centers, and the advisorycommittee of the nsf center for molecular biotechnology. dr. graham isa former member of the presidentõs information technology advisorycommittee (pitac).marc snir, cochair, is michael faiman and saburo muroga professorand head of the department of computer science at the university ofillinois at urbanachampaign. dr. snirõs research interests include largescale parallel and distributed systems, parallel computer architecture, andparallel programming. he received a ph.d. in mathematics from the hebrew university of jerusalem in 1979 and worked at new york universityon the ultracomputer project from 1980 to 1982; at the hebrew universityof jerusalem from 1982 to 1986; and at the ibm t.j. watson research center from 1986 to 2001. at ibm he headed research that led to the ibmscalable parallel (sp) system; contributed to power 4 and intel server architecture; and initiated the blue gene project. dr. snir has publishedmore than a hundred papers on computational complexity, parallel algorithms, parallel architectures, interconnection networks, compilers, andparallel programming environments; he was a major contributor to thedesign of mpi. dr. snir is an acm fellow and a fellow of the institute ofelectrical and electronics engineers (ieee). he serves on the editorialboards of parallel processing letters and acm computing surveys.william j. dally received a b.s. degree in electrical engineering fromvirginia polytechnic institute, an m.s. degree in electrical engineeringfrom stanford university, and a ph.d. degree in computer science fromthe california institute of technology (caltech). he is currently thewillard and inez bell professor of engineering at stanford university,where his group developed the imagine processor, which introduced theconcepts of stream processing and partitioned register organizations, andchair of the computer science department. dr. dally and his group havedeveloped the system architecture, network architecture, signaling, routgetting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.appendix a251ing, and synchronization technology that can be found in most large parallel computers today. while at bell telephone laboratories he contributed to the design of the bellmac32 microprocessor and designed themars hardware accelerator. at caltech, he designed the mossim simulation engine and the torus routing chip, which pioneered wormholerouting and virtualchannel flow control. while a professor of electricalengineering and computer science at the massachusetts institute of technology (mit), his group built the jmachine and the mmachine, experimental parallel computer systems that pioneered the separation of mechanisms from programming models and demonstrated very low overheadsynchronization and communication mechanisms. dr. dally has workedwith cray research and intel to incorporate many of these innovations incommercial parallel computers and with avici systems to incorporate thistechnology into internet routers. he cofounded velio communications tocommercialize highspeed signaling technology, and he cofoundedstream processors, inc., to commercialize stream processing. he is a fellow of the ieee and a fellow of the acm and has received numeroushonors, including the acm maurice wilkes award. he currently leadsprojects on highspeed signaling, computer architecture, and network architecture. he has published over 150 papers in these areas and is an author of the textbooks digital systems engineering (cambridge universitypress, 1998) and principles and practices of interconnection networks (morgan kaufmann, 2003).james w. demmel (nae) joined the computer science division andmathematics department at the university of california, berkeley, in 1990,where he holds a joint appointment as the dr. richard carl dehmel distinguished professor. he is also the chief scientist of the center for information technology research in the interest of society (citris), an interdisciplinary research center dedicated to applying information technologyto societalscale problems such as energy efficiency, disaster response,environmental monitoring, transportation, health care, and education. dr.demmel is an expert on software and algorithms to facilitate computational science, having contributed to the software packages lapack,scalapack, blas, and superlu. he is an acm fellow and an ieeefellow and has been an invited speaker at the international congress ofmathematicians. he received a b.s. in mathematics from caltech in 1975and a ph.d. in computer science from the university of california, berkeley, in 1983.jack j. dongarra (nae) is a university distinguished professor ofcomputer science in the computer science department at the universityof tennessee, a distinguished research staff member in the computergetting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.252getting up to speedscience and mathematics division at oak ridge national laboratory, andan adjunct professor in the computer science department at rice university. he specializes in numerical algorithms in linear algebra, parallel computing, use of advanced computer architectures, programming methodology, and tools for parallel computers. his research includes thedevelopment, testing, and documentation of highquality mathematicalsoftware. he has contributed to the design and implementation of thefollowing open source software packages and systems: eispack,linpack, the blas, lapack, scalapack, netlib, pvm, mpi, netsolve,top500, atlas, and papi. he has published approximately 300 articles,papers, reports, and technical memoranda and is coauthor of severalbooks. he is a fellow of the aaas, the acm, and the ieee. he earned ab.s. in mathematics from chicago state university in 1972. a year later hefinished an m.s. in computer science from the illinois institute of technology. he received his ph.d. in applied mathematics from the university ofnew mexico in 1980. he worked at the argonne national laboratory until 1989, becoming a senior scientist.kenneth s. flamm is a professor and dean rusk chair in international affairs at the university of texas lyndon b. johnson (lbj) schoolof international affairs. he joined the lbj school in 1998, is a 1973 honorsgraduate of stanford university, and received a ph.d. in economics frommit in 1979. from 1993 to 1995, dr. flamm served as principal deputyassistant secretary of defense for economic security and as special assistant to the deputy secretary of defense for dualuse technology policy.defense secretary william j. perry awarded him the dodõs distinguishedpublic service medal in 1995. prior to and after his service at dod, hespent 11 years as a senior fellow in the foreign policy studies program atthe brookings institution. dr. flamm has been a professor of economicsat the instituto tecnolšgico a. de m”xico in mexico city, the universityof massachusetts, and george washington university. he is also currently a member of the national academy of scienceõs science, technology, and economic policy board, its steering group on measuring andsustaining the new economy, and its committee on capitalizing on science, technology, and innovation. he has served as member and chair ofthe nato science committeeõs panel for science and technology policyand organization, as a member of the federal networking council advisory committee, the oecdõs expert working party on high performancecomputers and communications, various advisory committees and studygroups of the national science foundation, the council on foreign relations, the defense science board, and the u.s. congress office of technology assessment and as a consultant to government agencies, internationalorganizations, and private corporations. dr. flamm teaches classes ingetting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.appendix a253microeconomic theory, international trade, and defense economics, haspublished extensively on the economic impacts of technological innovation in a variety of hightechnology industries, and has analyzed economicpolicy issues in the semiconductor, computer, communications, and defense industries.mary jane irwin (nae) has been on the faculty at the pennsylvaniastate university since 1977 and currently holds the a. robert noll chairin engineering in the department of computer science and engineering.her research and teaching interests include computer architecture, embedded and mobile computing systems design, poweraware design, andelectronic design automation. her research is supported by grants fromthe marco gigascale systems research center, the national sciencefoundation, the semiconductor research corporation, and the pennsylvania pittsburgh digital greenhouse. she received an honorary doctoratefrom chalmers university, sweden, in 1997 and the penn state engineering societyõs premier research award in 2001. she was named an ieeefellow in 1995 and an acm fellow in 1996 and was elected to the nationalacademy of engineering in 2003. dr. irwin is currently serving as a member of the technical advisory board of the army research lab, as the coeditor in chief of acmõs journal of emerging technologies in computing systems, as a member of acmõs publication board, and on the steeringcommittee of the computing research associationõs (craõs) committeeon the status of women in computer science and engineering. in the pastshe served as an elected member of the craõs board of directors, of theieee computer societyõs board of governors, of acmõs council, and asvice president of acm. she also served as the editor in chief of acmõstransactions on design automation of electronic systems from to 1999 to 2004and as chair of the nsf/cise advisory committee from 2001 to 2003. dr.irwin has served in leadership roles for several major conferences, including as general chair of the 1996 federated computing conference, generalcochair of the 1998 cra conference at snowbird, general chair of the36th design automation conference, general cochair of the 2002 international symposium on low power electronics and design, and general cochair of the 2004 conference on compilers, architecture, and synthesisfor embedded systems. dr. irwin received her m.s. (1975) and ph.d.(1977) degrees in computer science from the university of illinois, urbanachampaign.charles koelbel is a research scientist in the computer science department at rice university. dr. koelbelõs area of expertise is in languages,compilers, and programming paradigms for parallel and distributed systemsñin laymanõs terms, developing computer languages and algorithmsgetting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.254getting up to speedthat let several computers talk to each other and work together efficiently.he has contributed to many research projects while at rice, mostlythrough the center for research on parallel computation, an nsffundedscience and technology center with the mission to make parallel computation usable by scientists and engineers. these projects include the national computational science alliance technology deployment partnersprogram, the dodõs highperformance computing modernization program, and the fortran d programming language project. he was executive director of the high performance fortran forum, an effort to standardize a language for parallel computing. more recently, he served for3years as a program director at the national science foundation, wherehe was responsible for the advanced computational research programand helped coordinate the information technology research program.he is coauthor of the high performance fortran handbook (mit press, 1993)and many papers and technical reports. he received his ph.d. in computer science from purdue university in 1990.butler w. lampson (nae) is a distinguished engineer at microsoftcorporation and an adjunct professor of computer science and electricalengineering at the massachusetts institute of technology. he was on thefaculty at the university of california, berkeley; at the computer sciencelaboratory at xerox parc; and at digitalõs systems research center. dr.lampson has worked on computer architecture, local area networks, raster printers, page description languages, operating systems, remote procedure call, programming languages and their semantics, programmingin large, faulttolerant computing, transaction processing, computer security, and wysiwyg editors. he was one of the designers of the sds 940timesharing system, the alto personal distributed computing system; thexerox 9700 laser printer; twophase commit protocols; the autonet lan;microsoft tablet pc software; and several programming languages. hereceived an a.b. from harvard university, a ph.d. in electrical engineering and computer science from the university of california at berkeley,and honorary science doctorates from the eidgenoessische technischehochschule, zurich, and the university of bologna. dr. lampson holds anumber of patents on networks, security, raster printing, and transactionprocessing. he is a former member of the nrcõs computer science andtelecommunications board. he has served on numerous nrc committees, including the committee on high performance computing andcommunications: status of a major initiative. he is a fellow of the acmand the aaas. he received acmõs software systems award in 1984 forhis work on the alto, ieeeõs computer pioneer award in 1996, the turingaward in 1992, the von neumann medal in 2001, and the naeõs draperprize in 2004.getting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.appendix a255robert f. lucas is the director of the computational sciences divisionof the university of southern californiaõs information sciences institute(isi). he manages research in computer architecture, vlsi, compilers, andother software tools. prior to joining isi, he was the head of the highperformance computing research department in the national energyresearch scientific computing center (nersc) at lawrence berkeleynational laboratory. he oversaw work in scientific data management,visualization, numerical algorithms, and scientific applications. prior tojoining nersc, dr. lucas was the deputy director of darpaõs information technology office. he also served as darpaõs program manager forscalable computing systems and dataintensive computing. from 1988 to1998, he was a member of the research staff of the institute for defenseanalysesõ (idaõs) center for computing sciences. from 1979 to 1984, hewas a member of the technical staff of the hughes aircraft company. dr.lucas received b.s., m.s., and ph.d. degrees in electrical engineering fromstanford university in 1980, 1983, and 1988, respectively.paul c. messina retired in april 2002 from caltech, where he wasassistant vice president for scientific computing, director of caltechõs center for advanced computing research, and faculty associate in scientificcomputing. he also served as principal investigator for the distributedterascale facility and extensible terascale facility projects at caltech andwas coprincipal investigator of the national virtual observatory project.from 2002 to 2004, dr. messina was a distinguished senior computer scientist (part time) at argonne national laboratory and until june 2003 wasa senior advisor on computing to the director general of cern, in geneva.during a leave from caltech from january 1999 to december 2000, he wasdirector of the office of advanced simulation and computing for defense programs in the nnsa at doe. in that capacity he had responsibility for managing asci, the worldõs largest scientific computing program,which is defining the state of the art in that field. he held the position ofchief architect for the npaci, a partnership established by the nsf andled by the university of california, san diego. his recent interests focuson advanced computer architectures, especially their application to largescale computations in science and engineering. he has also been active inhighspeed networks, computer performance evaluation, and petaflopscomputing issues. prior to his assignment at doe, he led the computational and computer science component of caltechõs research project,funded by the academic strategic alliances program (asap) of the asc.in the mid1990s he established and led the scalable i/o (sio) initiative.in the early 1990s, he was the principal investigator and project managerof the casa gigabit network testbed. during that period he also conceived, formed, and led the consortium for concurrent supercomputing,getting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.256getting up to speedwhose 13 members included several federal agencies, the national laboratories, universities, and industry. that consortium created and operatedthe intel touchstone delta system, which was the worldõs most powerfulscientific computer for 2 years. he also held a joint appointment at the jetpropulsion laboratory as manager of highperformance computing andcommunications from 1988 to 1998. dr. messina received a ph.d. in mathematics in 1972 and an m.s. in applied mathematics in 1967, both from theuniversity of cincinnati, and a b.a. in mathematics in 1965 from the college of wooster. in 1997 he was granted an honorary ph.d. in computerengineering by the university of lecce, italy. he is a member of the ieeecomputer society, the aaas, the acm, the society for industrial andapplied mathematics, and sigma xi. he is coauthor of four books onscientific computing and editor of more than a dozen others.jeffrey m. perloff is the chair of and a professor in the departmentof agricultural and resource economics at the university of california atberkeley. his economics research covers industrial organization and antitrust, labor, trade, and econometrics. his textbooks are modern industrialorganization (coauthored with dennis carlton) and microeconomics. hehas been an editor of industrial relations and associate editor of the american journal of agricultural economics and is an associate editor of the journal of productivity analysis. he has consulted with nonprofit organizations and government agencies (including the federal trade commissionand the departments of commerce, justice, and agriculture) on topicsranging from a case of alleged japanese television dumping to the evaluation of social programs. he has also conducted research in psychology.dr. perloff is a fellow of the american agricultural economics association and a member of the board of directors of the national bureau ofeconomic research. he received his b.a. in economics from the university of chicago in 1972 and his ph.d. in economics from mit in 1976. hewas previously an assistant professor in the department of economics atthe university of pennsylvania.william h. press (nas) is a senior fellow at los alamos nationallaboratory (lanl). from 1998 to 2004 he served as deputy laboratorydirector for science and technology. before joining lanl in 1998, he wasprofessor of astronomy and physics at harvard university and a memberof the theoretical astrophysics group of the harvardsmithsonian centerfor astrophysics. he is also the coauthor and comaintainer of the numerical recipes series of books on scientific computer programming. dr.press was assistant professor of physics at princeton university and richard chace tolman research fellow in theoretical physics at caltech, wherehe received a ph.d. in physics in 1972. he is a member of the nationalgetting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.appendix a257academy of sciences and was a founding member of its computer andinformation sciences section. he has published more than 140 papers inthe areas of theoretical astrophysics, cosmology, and computational algorithms. he is also a fellow in the aaas, a member of the council on foreign relations, and a past recipient of an alfred p. sloan foundation fellowship and the helen b. warner prize of the american astronomicalsociety. dr. press is a past cochair of the commission on physical sciences, mathematics, and applications of the nrc and a past member ofthe chief of naval operationsõ executive panel, the u.s. defense scienceboard, nrcõs computer science and telecommunications board, the astronomy and astrophysics survey committee, and a variety of otherboards and committees. he has led national studies in subjects includinghighbandwidth telecommunications (the global grid), national scienceand technology centers (especially for computational science), and a widevariety of national security issues. dr. press serves as a scientific advisorto the david and lucille packard foundation and other foundations. heis a member of the board of trustees of the ida and serves on its executivecommittee and on the external advisory committees of its ccs and ccrdivisions. he serves on the defense threat reduction agencyõs scienceand technology panel.albert j. semtner is a professor of oceanography at the naval postgraduate school in monterey, california. he received a b.s. in mathematics from caltech and a ph.d. in geophysical fluid dynamics fromprinceton. his prior professional positions were in uclaõs meteorologydepartment and in the climate change research section of the ncar.his interests are in global ocean and climate modeling and in supercomputing. dr. semtner has written extensive oceanographic codes in assembly language for shipboard use. he produced the first vectorized (fortran) version of a standard ocean model in 1974 and the firstparallelvector version (in collaboration with robert chervin of ncar) in1987. he interacted with lanl scientists on transitioning the parallelvector code to massively parallel architectures in the early 1990s. underthe leadership of warren washington of ncar, he participated in thedevelopment of the doe parallel climate model using the los alamosparallel ocean program and a parallel seaice model from the naval postgraduate school. that climate model has been ported to numerous parallel architectures and used as a workhorse climate model in numerous scientific applications. dr. semtner has been an affiliate scientist with ncarfor the last 12 years and simultaneously a member (and usually chair) ofthe advisory panel to the ncar scientific computing division. he is awinner (with r. chervin) of a 1990 gigaflop performance award (for thevectorparallel code) and the 1993 computerworldsmithsonian leadergetting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.258getting up to speedship award in breakthrough computational science (for global oceanmodeling studies that included ocean eddies for the first time). dr.semtner is an associate editor of ocean modeling and of the journal of climate. he is also a fellow of the american meteorological society.scott stern graduated with a b.a. degree in economics from newyork university. after working for a consulting company in new york,he attended stanford university and received his ph.d. in economics in1996. from 1995 to 2001, dr. stern was assistant professor of managementat the sloan school at mit. also, from 2001 to 2003, dr. stern was a nonresident senior fellow of the brookings institution. since september 2001,dr. stern has been an associate professor in the kellogg school of management at northwestern university and a faculty research fellow of thenational bureau of economic research. he is also a coorganizer of theinnovation policy and the economy program at the national bureau ofeconomic research. dr. stern explores how innovationñthe productionand distribution of ideasñdiffers from the production and distribution ofmore traditional economic goods and the implications of these differencesfor both business and public policy. often focusing on the pharmaceuticaland biotechnology industries, this research is at the intersection of industrial organization and economics of technological innovation. specifically,recent studies examine the determinants of r&d productivity, the impactof incentives on r&d organization, the mechanisms by which firms earneconomic returns from innovation, and the consequences of technologicalinnovation for product market competition. a key conclusion from thisresearch is that translating ideas into competitive advantage requires adistinct and nuanced set of resources and strategies. effective management of innovation therefore requires careful attention to the firmõs internal ability to develop truly distinct technologies and to subtle elements ofthe firmõs external development and commercialization environment.shankar subramaniam is a professor of bioengineering, chemistry, and biochemistry and biology and director of the bioinformaticsgraduate program at the university of california at san diego. he alsoholds adjunct professorships at the salk institute for biological studiesand the san diego supercomputer center. prior to moving to the university of california, san diego, dr. subramaniam was a professor of biophysics, biochemistry, molecular and integrative physiology, chemicalengineering, and electrical and computer engineering at the university ofillinois at urbanachampaign (uiuc). he was also the director of thebioinformatics and computational biology program at the national center for supercomputing applications and codirector of the w.m. keckgetting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.appendix a259center for comparative and functional genomics at uiuc. he is a fellowof the american institute for medical and biological engineering and is arecipient of smithsonian foundation and association of laboratory automation awards. dr. subramaniam has played a key role in raising national awareness of training and research in bioinformatics. he served asa member of the national institutes of health (nih) directorõs advisorycommittee on bioinformatics, which produced the report biomedical information science and technology initiative (bisti). the report recognizedthe dire need for trained professionals in bioinformatics and recommended the launching of a strong nih funding initiative. dr.subramaniam serves as the chair of an nih bisti study section. he alsoserved on bioinformatics and biotechnology advisory councils for virginiatech, the university of illinois at chicago, and on the scientific advisoryboard of several biotech and bioinformatics companies. dr. subramaniamserved as review panel member of the center for information technology(cit) at nih, and his focus was on how cit should respond to the bistiinitiative. dr. subramaniam has served as a member of illinoisõsgovernorõs initiative in biotechnology and as advisor and reviewer ofnorth carolinaõs initiative in biotechnology. dr. subramaniam has published more than a hundred papers in the interdisciplinary areas of chemistry/biophysics/biochemistry/bioinformatics and computer science.lawrence c. tarbell, jr., is the deputy director of the technologyfutures office for eagle alliance, a company formed in 2001 by the computer sciences corporation and northrop grumman to outsource part ofthe it infrastructure (workstations, local area networks, servers, and telephony) for the nsa. his primary area of responsibility is it enterprisemanagement, with secondary responsibility in collaboration, distributedcomputing, and storage. mr. tarbell spent the previous 35 years at nsawith responsibilities for research and development of highperformanceworkstations, networks, computer security, mass storage systems, massively parallel processing systems, and systems software. for over 13years, he managed and led supercomputing architecture research anddevelopment for nsa, sponsoring highperformance computing andmass storage research (both independently and jointly with darpa andnasa) at many u.s. companies and universities. in 1990, he cochairedfrontiers of supercomputing ii, sponsored jointly by nsa and lanl. for3 years, he managed the development and procurement of thesupercomputing and mass storage architecture at nsa. mr. tarbell received his m.s. in electrical engineering from the university of marylandand his b.s. in electrical engineering (magna cum laude) from louisianastate university.getting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.260getting up to speedsteven j. wallach (nae) is vice president of technology for chiaronetworks, an advisor to centerpoint venture partners, and a consultantto the u.s. department of energy asc program. chiaro networks provides major disruptive technologies in a highend routing platform forreliability, scalability, and flexibility. before that, he was cofounder, chieftechnology officer, and senior vice president of development of convexcomputers. after hewlettpackard bought convex, mr. wallach becamethe chief technology officer of hpõs large systems group. he was a visiting professor at rice university from 1998 to 1999 and manager of advanced development at data general from 1975 to 1981. he was the principal architect of the 32bit eclipse mv supercomputer and, as part ofthis effort, participated in the design of the mv/6000, mv/8000, andmv/10000 (chronicled in the pulitzer prizewinning book the soul of anew machine, by tracy kidder). mr. wallach was an engineer at raytheonfrom 1971 to 1975, where he participated in various hardware design efforts, including the computer used to control the launching of the patriotmissile system and various signal processors. he had primary responsibility for the design of the allapplications digital computer (aadc),which was intended for military specification airborne applications andwas made up of gate arrays (one of the first such systems) and a vectorinstruction set based on apl. mr. wallach holds 33 patents. he was amember of pitac. he is also a fellow of the ieee. mr. wallach holds ab.s. in engineering from the polytechnic institute of brooklyn, an m.s.e.e.from the university of pennsylvania, and an m.b.a. from boston university.staffcynthia a. patterson is a study director and program officer withthe computer science and telecommunications board of the nationalacademies. before the current study on the future of supercomputing,she completed several projects, including a study on critical informationinfrastructure protection and the law, a study that outlined a researchagenda at the intersection of geospatial information and computer science, and a joint study with the board on earth sciences and resourcesand the board on atmospheric sciences and climate on publicprivatepartnerships in the provision of weather and climate services. she alsohas been involved with a study on telecommunications research and development and a congressionally mandated study on internet searchingand the domain name system. prior to joining cstb, ms. patterson completed a m.sc. from the sam nunn school of international affairs at thegeorgia institute of technology. in a previous life, ms. patterson wasemployed by ibm as an it consultant for both federal government andgetting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.appendix a261private industry clients. her work included application development,database administration, network administration, and project management. she received a b.sc. in computer science from the university of missourirolla.phil hilliard (through may 2004) was a research associate with cstb.he provided research support as part of the professional staff and workedon projects focusing on telecommunications research, supercomputing,and dependable systems. before joining the national academies, mr.hilliard worked at bellsouth in atlanta, georgia, as a competitive intelligence analyst and at ncr as a technical writer and trainer. he earned anm.b.a. from georgia state university (2000) and a b.s. in computer andinformation technology from the georgia institute of technology (1986).he is currently working on a masterõs degree in library and informationscience through florida state universityõs online program.margaret marsh huynh, senior program assistant, has been withcstb since january 1999 supporting several projects. she is currently supporting, in addition to the present project, wireless technology prospectsand policy options, internet navigation and the domain name system,and whither biometrics. she previously worked on the projects that produced the reports beyond productivity: information technology, innovation,and creativity, it roadmap to a geospatial future, building a workforce for theinformation economy, and the digital dilemma: intellectual property in theinformation age. ms. huynh also assisted with the project exploring information technology issues for the behavioral and social sciences (digitaldivide and democracy). she assists on other projects as needed. prior tocoming to the national academies, ms. huynh worked as a meeting assistant at management for meetings, from april 1998 to august 1998, andas a meeting assistant at the american society for civil engineers, fromseptember 1996 to april 1998. ms. huynh has a b.a. (1990) in liberal studies with minors in sociology and psychology from salisbury state university (maryland).herbert s. lin (may 2004 through december 2004) is senior scientistand senior staff officer at cstb, where he has been study director of majorprojects on public policy and information technology. these studies include a 1996 study on national cryptography policy (cryptographyõs rolein securing the information society), a 1991 study on the future of computerscience (computing the future), a 1999 study of defense department systems for command, control, communications, computing, and intelligence(realizing the potential of c4i: fundamental challenges), a 2000 study onworkforce issues in hightechnology (building a workforce for the informagetting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.262getting up to speedtion economy), and a 2002 study on protecting kids from internet pornography and sexual exploitation (youth, pornography, and the internet). priorto his nrc service, he was a professional staff member and staff scientistfor the house armed services committee (19861990), where his portfolioincluded defense policy and arms control issues. he received his doctorate in physics from mit.getting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.263bspeakers and participantsat meetings and site visitsmeeting 1march 67, 2003washington, d.c.george cotter, national security agency (nsa)john crawford, intel fellow, intel corporationrobert graybill, program manager, information processing technologyoffice, defense advanced research projects agency (darpa)john grosh, senior staff specialist (computing and software),information systems directorate, office of the deputy undersecretary of defense (science and technology)daniel hitchcock, office of advanced scientific computing research,department of energy (doe)gary hughes, nsadavid kahaner, asian technology information programjacob v. maizel, jr., chief of the laboratory of experimental andcomputational biology, national cancer institutejos” muœoz, office of advanced simulation and computing, doeclay sell, clerk, senate subcommittee on energy and waterdevelopmentdavid turek, vice president, ibmgetting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.264getting up to speedmeeting 2may 2123, 2003stanford, californiagreg astfalk, chief technical officer, hewlettpackardgordon bell, senior researcher, microsoft researchdebra goldfarb, vice president, idcjames gray, senior researcher, microsoft researchjohn levesque, senior technologist, cray inc.john lewis, technical fellow, boeingscott mcclellan, hewlettpackardwilliam reed, director (retired), office of advanced simulation andcomputing, doemark seager, principle investigator, lawrence livermore nationallaboratory (llnl)burton smith, chief scientist, cray inc.applications workshop and meeting 3september 2426, 2003santa fe, new mexicokeynote speakersphillip colella, senior mathematician, lawrence berkeley nationallaboratory (lbnl)charles mcmillan, defense and nuclear technologies directorate,llnljeffrey saltzman, senior director, merck research laboratorywarren washington, senior scientist, national center for atmosphericresearch (ncar)participantscleve ashcraft, research mathematician, livermore softwaretechnology corporationwilliam carlson, research staff, institute for defense analyses (ida)center for computing sciencesmichael colvin, senior biomedical scientist, llnlstephen eubank, los alamos national laboratory (lanl)robert harrison, principal architect, oak ridge national laboratory(ornl)getting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.appendix b265bruce hendrickson, technical staff and acting manager, sandianational laboratoriesgary hughes, nsaanthony jameson, professor, stanford universityjohn killough, senior research fellow, landmark graphics corporationrichard loft, application engineer, ncargene myers, professor, university of california, berkeleyvincent scarafino, manager, ford motor companyfrancis sullivan, director, ida center for computing scienceswilliam tang, associate director, princeton universitypriya vashishta, professor, university of southern californiarobert weaver, physicist, lanlpaul woodward, professor, university of minnesotatown hall birds of a feather sessionnovember 19, 2003supercomputing conference 2003phoenix, arizonanumerous conference attendees participated in the session and providedcomments to the committee.national security agency site visitdecember 2, 2003fort meade, marylandsuzanne banghartwilliam carlsoncandice culhanedave harriseric haseltinegary hughesbill johnsonboyd livingstonmike merrillbaron millsdave muzzytom pagesteve roznowskigetting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.266getting up to speedmeeting 4december 34, 2003washington, d.c.donald j. becker, founder and chief technical officer, scyldcomputing corporationfrancine berman, director, san diego supercomputer centermatt dunbar, principal development engineer, abaqus, inc.earl joseph, program vice president, highperformance systems, idckenichi miura, fujitsu fellow, professor and project leader, center forgrid research and development, national institute of informaticscleve moler, chairman and chief scientist, the mathworks, inc.daniel reed, director, national center for supercomputingapplicationsroy f. schwitters, s.w. richardson foundation regental professor ofphysics and chair of the department of physics, university oftexas, austinhorst d. simon, director, national energy research scientificcomputing (nersc), lbnlsrinidhi varadarajan, director, virginia polytechnic institute and stateuniversitymichael wolfe, st fellow, stmicroelectronics, inc.lawrence livermore national laboratory site visitjanuary 9, 2004livermore, californiawelcome, security briefing, and overviewthomas f. adams, associate bprogram leader for computationalphysics, defense and nuclear technologies directorate (dntd)lynn kissel, deputy program leader, ascmichel g. mccoy, program leader, ascjames a. rathkopf, associate aprogram leader for computationalphysics, dntdcode development round tablekatie lewis, dntdmarty marinak, dntdthomas l. mcabee, dntdrob neely, dntdbrian pudliner, dntdmichael zika (facilitator), dntdgetting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.appendix b267materials and physics modeling roundtablegrant bazan, dntdlaurence e. fried (facilitator), chemistry and materials sciencedirectoraterandolph q. hood, physics and advanced technologies directorate(patd)stephen b. libby, patdchristian mailhiot, chemistry and materials science directorateandrew k. mcmahan, patdpaul l. miller, dntdalbert l. osterheld, patdjohn e. reaugh, patderic r. schwegler, patdchristine j. wu, patddesignersõ roundtablerobert e. canaan, dntdtodd j. hoover, dntdjuliana j. hsu (facilitator), dntdomar a. hurricane, dntdcynthia k. nitta, dntdpeter w. rambo, dntdmultiprogrammatic capability cluster in production (tour and demo)robin goldstone, linux system project lead, integrated computing andcommunications department (iccd)cyber infrastructure roundtablerob falgout, asc institute for terascale simulation leaderrandy frank, visualization project leader, iccdmark gary, data storage group leader, iccdrobin goldstone, linux system project lead, iccdjohn gyllenhaal, code development computer scientist, iccdsteve louis, asc data and visualization science leader, iccdjohn may, asc performance optimization and modeling project leadermark seager, assistant department head for advanced technologyand terascale computing, asc platform leader, iccdjean shuler, iccd services and development deputy division leader,iccdgetting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.268getting up to speeddave wiltzius, iccd networks and services division leader, ascdiscom leader, iccdmary zosel, asc problem solving environment leader (facilitator),iccdlawrence berkeley national laboratory site visitjanuary 14, 2004berkeley, californiaoverview of computing sciences at lbnlhorst d. simon, associate laboratory director for computing sciences;director, computational research division (crd); director, nersccenter divisionnersc issuesbill kramer, nersc center general manager and department head forhighpower computing (hpc)new technology introduction at nerscjim craw, group leader, computational systems, advanced systems,and pdsf systems, nerscbill saphir, chief architect, high performance computing department,nerscfrancesca verdier, group leader, user services, nersccomputing on the earth simulatorandrew canning, computer scientist, scientific computing group,crdnersc user paneljohn bell, group leader, center for computational science andengineering, crdjulian borrill, computer scientist, scientific computing group, crdwilliam lester, professor, department of chemistry, university ofcalifornia, berkeleydoug rottman, llnl, vice chair of nersc user grouprob ryne, lbnl, chair of nersc user groupmichael wehner, computer scientist, scientific computing group, crdgetting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.appendix b269programming/language issueskathy yelick, professor, computer science division, university ofcalifornia, berkeleyscientific discovery through advanced computing (scidac)juan meza, department head, high performance computing research,crdesmond ng, group leader, scientific computing, crdarie shoshani, group leader, scientific data management, crdsandia national laboratories site visitfebruary 26, 2004albuquerque, new mexiconuclear weapons program overviewtom bickel, director, engineering sciencesgeorge novotony, technical assistant to the vice president, weaponsystems divisionjoe polito, director, stockpile systems programart ratzelpaula schoeneman, protocol officerrobert thomas, manager, advanced simulation and computingprogrammichael vahle, director, simulation enabled product realizationprogrammicrosystems/science applications for the stockpiledon cook, director, microsystems and engineering sciencesapplications program officecode development strategiesken alvin, code developersteve bova, code developerarne gullerud, code developermike mcglaun (speaker/facilitator), level ii manager, systemstechnologygarth reese, code developergetting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.270getting up to speedsolution verification for hyperbolic equationsjames stewart, manager, production computing/sierra architecturemodel/code validationchris garasi, analystjoel lash, analystlen lorence, analystmarty pilch (speaker/facilitator), manager, validation and uncertaintyquantification processsandiaõs longterm computer architecture strategiesbill camp, director, computation, computers, and mathsupercomputer issues, including operating system software, algorithms,capability/capacity strategiesrob leland, level ii manager, computer and software systemssierra frameworkscarter edwards, advanced computational mechanicslos alamos national laboratory site visitfebruary 27, 2004los alamos, new mexicolanl overview and strategic directionsjames s. peery, deputy associate director, weapon physicssummary of requirements/drivers for predictive capabilitypaul j. hommert, division leader, applied physicsperformance modelingadolfy hoisie, group leader, modeling, algorithms, and informaticsgetting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.appendix b271science applianceronald g. minnich, team leader, cluster researchq lessons learnedjohn f. morrison, division leader, computing, communications, andnetworksvisualizationbob tomlinsonarchitecture, partnerships, technology riskskyran b. kemper (chris), deputy division leader, computing,communications and networksflop driversjim morelargonne national laboratory site visit,with participation by oak ridge nationallaboratory staffmarch 2, 2004argonne national laboratoryargonne, illinoisadvanced computing researchrick stevens, director, mathematics computer science, argonnenational laboratory (anl); professor of computer science,university of chicagoscalability studies of selected applicationsandrew seige, anlprogramming models and development environments for hpcewing lusk, anlgetting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.272getting up to speedleadership class computing for sciencethomas zacharia, ornloverview and status of cray x1 evaluation at ccspat worley, ornlapplications in astrophysics and materialstony mezzacappa, ornljeff nichols, ornlthomas schulthess, ornlhighperformance information technology infrastructure requirements for thenational academic research communitymichael levine, pittsburgh supercomputer centerralph roskies, pittsburgh supercomputer centermeeting 5march 34, 2004argonne, illinoispeter freeman, assistant director, nsfshane greenstein, elinor and wendell hobbs professor, kellogg schoolof management, northwestern universitydavid mowery, milton w. terrill professor of business, walter a. haasschool of business, university of california, berkeleyjapan site visitnational academy of engineeringðengineering academy of japanjoint forum on the future of supercomputingmarch 23, 2004u.s. cochairssusan l. graham (nae), pehong chen distinguished professor,electrical engineering and computer science, university ofcalifornia, berkeleymarc snir, michael faiman and saburo muroga professor and head ofdepartment of computer science, university of illinois, urbanachampaigngetting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.appendix b273u.s. speakersjack j. dongarra (nae), distinguished professor of computer science,computer science department, university of tennesseealbert j. semtner, professor, oceanography department, navalpostgraduate school in montereyscott stern, associate professor, kellogg school of management,northwestern universitysteven j. wallach (nae), vice president and cofounder, chiaronetworksu.s. participantsmaki haraga, interpretercynthia patterson, study director and program officer, nationalresearch counciljapan cochairskenichi miura, professor and project leader, center for grid researchand development, national institute of informaticstsuneo nakahara, engineering academy of japan (eaj); vice president,advisor, sumitomo electric industries, ltd.japan speakershironori kasahara, professor, department of computer science,waseda universitychisachi kato, professor, institute of industrial science, university oftokyokeiichiro uchida, professor, department of information and computerscience, science faculty, kanagawa universityjapan participantsmutsumi aoyagi, professor, network computing research division,computing and communication center, kyushu universitytaisuke boku, associate professor, institute of information sciences andelectronics, center for computational physics, university oftsukubakozo fujii, professor, department of space transportation engineering,institute of space and astronautical science, japan aerospaceexploration agencygetting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.274getting up to speedyoshinari fukui, information technology based laboratory, projectleader, technology development unit, advanced center forcomputing and communication, institute of physical and chemicalresearch; vice president, japan society for industrial and appliedmathematicsryutaro himeno, head, computer and information division, advancedcomputing center, institute of physical and chemical researchkohichiro hotta, director, core technologies department, softwaretechnology development division, software group, fujitsu ltd.kozo iizuka, eaj; president, japan association for metrology promotionmasanori kanazawa, professor, academic center for computing andmedia studies, kyoto universitysumio kikuchi, deputy general manager, enterprise business planning,software division, hitachi, ltd.toshio kobayashi, eaj; president, japan automobile research institute;professor emeritus, university of tokyo; member, science councilof japankoki maruyama, senior research scientist and director, principalresearch program on global warming prediction and measure,abiko research laboratory, central research institute of theelectric power industryyuichi matsuo, computation and network infrastructure laboratory,computational fluid dynamics technology center, nationalaerospace laboratory of japansatoshi matsuoka, professor, global scientific information andcomputing center and department of mathematical andcomputing sciences, tokyo institute of technologymasao sakauchi, eaj; deputy director general, national institute ofinformatics; professor, institute of industrial science, university oftokyotetsuya sato, director general, earth simulator center, japan marinescience and technology centersatoshi sekiguchi, director, grid technology research center, nationalinstitute of advanced industrial science and technologymasaru tsukada, professor, department of physics, graduate school ofscience, university of tokyotadashi watanabe, vice president, high performance computing, neccorporationgenki yagawa, eaj; professor, school of engineering, department ofquantum engineering and systems science, university of tokyo;getting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.appendix b275director, center for promotion of computational science andengineering, japan atomic energy research instituteikuo yamada, eaj, executive directoruniversity of tokyo, supercomputerresearch centermarch 24, 2004obinata kazuojapan aerospace exploration agencymarch 24, 2004toshiyuki iwamiya, director, information technology centeryuichi matsuo, engineer, information technology centerauto manufacturermarch 25, 2004names withheld on request.earth simulator centermarch 25, 2004tetsuya sato, director generalkunihiko watanabe, program director, simulation science andtechnology researchuniversity of tokyo, grape groupmarch 26, 2004lab tour.ministry of education, culture, sports,science and technologymarch 26, 2004toshihiko hoshino, director, office for information science andtechnologyharumasa miura, director, information divisionhiroshi sato, director, office of earth and environmental science andtechnologymasaya toma, director, office for science information infrastructuredevelopmentgetting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.276clist of white papers prepared for theapplications workshopashcraft, cleve, roger grimes, john hallquist, and b. maker. òsupercomputing and mechanical engineering.ó livermore software technology corporation.colella, phillip. òcomputational fluid dynamics for multiphysics andmultiscale problems.ó computing sciences directorate, lawrenceberkeley national laboratory.colvin, michael. òquantum mechanical simulations of biochemical processes.ó lawrence livermore national laboratory.eubank, stephen. òthe future of supercomputing for sociotechnicalsimulation.ó computer and computational sciences division, losalamos national laboratory.hendrickson, bruce, william e. hart, and cindy phillips. òsupercomputing and discrete algorithms: a symbiotic relationship.ó discretealgorithms and math department, sandia national laboratories.hughes, gary d., william w. carlson, and francis e. sullivan. òcomputational challenges in signals intelligence.ó national security agency(hughes) and ida center for computing sciences (carlson andsullivan).keyes, david e. òsupercomputing for pdebased simulations in mechanics.ó department of applied physics and applied mathematics, columbia university.killough, john. òhigh performance computing and petroleum reservoirsimulation.ó landmark graphics corporation.getting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.appendix c277loft, richard d. òsupercomputing challenges for geoscience applications.ó scientific computing division, national center for atmospheric research.mcmillan, charles f., thomas f. adams, michel g. mccoy, randy b.christensen, brian s. pudliner, michael r. zika, patrick s. brantley,jeffrey s. vetter, and john m. may. òcomputational challenges innuclear weapons simulation.ó lawrence livermore national laboratory.myers, gene. òsupercomputing and computational molecular biology.óuniversity of california, berkeley.saltzman, jeffrey. òpharmaceutical high performance computing challenges.ó merck & co., inc.scarafino, vincent. òhigh performance computing in the auto industry.óford motor company.tang, william m. òplasma science.ó princeton university.washington, warren m. òcomputer architectures and climate modeling.ó national center for atmospheric research.weaver, robert. òcomputational challenges to supercomputing from thelos alamos crestone project: a personal perspective.ó los alamosnational laboratory.woodward, paul. òfuture supercomputing needs and opportunities inastrophysics.ó university of minnesota.getting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.278dglossary and acronym listasc.advanced simulation and computing program, the current namefor the program formerly known as asci.asci.accelerated strategic computing initiative, which provides simulation and modeling capabilities and technologies as part of doe/nnsa stockpile stewardship program.automatic parallelization.the automatic creation of parallel code fromsequential code by a compiler.bandwidth.the amount of data that can be passed along a communications channel in a unit of time. thus, memory bandwidth is theamount of data that can be passed between processor and memory ina unit of time and global communication bandwidth is the amount ofdata that can be passed between two nodes through the interconnectin a unit of time. both can be a performance bottleneck. bandwidth isoften measured in megabytes (million bytes) per second (mbyte/sec)or gigabytes (billion bytes) per second (gbyte/sec) or in megawords(million words) per second (mword/sec). since a word consists (inthis context) of 8 bytes, then 1 gbyte/sec = 125 mword/sec = 1,000mbyte/sec.benchmark.an experiment that enables the measurement of somemeaningful property of a computer system; a program or a computational task or a set of such programs or tasks that is used to measurethe performance of a computer.blas.basic linear algebra subprograms, a set of subprograms commonly used to solve dense linear algebra problems. level 1 blas includes vectorvector operations, level 2 blas includes vectormatrixgetting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.appendix d279operations, and level 3 blas includes matrixmatrix operations. blassubroutines are frequently optimized for each specific hardware platform.bg/l.blue gene/light (ibm).cache.a small, fast storage area close to the central processing unit(cpu) of a computer that holds the most frequently used memorycontents. caches aim to provide the illusion of a memory as large asthe main computer memory with fast performance. they succeed indoing so if memory accesses have good temporal locality and goodspatial locality.cache line.the unit of data that is moved between cache and memory.it typically consists of 64 or 128 consecutive bytes (8 or 16 consecutivedouble words).cache memory system.modern computers typically have multiple levels of caches (named level 1, level 2, and so on) that are progressivelylarger and slower; together they comprise the cache memory system.cae.computeraided engineering. the construction and analysis of objects using virtual computer models. this may include activities ofdesign, planning, construction, analysis, and production planning andpreparation.capability computing.the use of the most powerful supercomputers tosolve the largest and most demanding problems, in contrast to capacity computing. the main figure of merit in capability computing istime to solution. in capability computing, a system is often dedicatedto running one problem.capacity computing.the use of smaller and less expensive highperformance systems to run parallel problems with more modest computational requirements, in contrast to capability computing. the mainfigure of merit in capacity computing is the cost/performance ratio.ccsm.community climate system model.cdc.control data corporation.circuit speed.time required for a signal to propagate through a circuit,measured in picoseconds per gate. it is a key aspect of processor performance.cise.the nsf directorate for computing and information science andengineering. this directorate is responsible for nsffundedsupercomputing centers.clock rate or clock speed.the frequency of the clock that drives theoperation of a cpu, measured in gigahertz (ghz). clock rate and instructions per cycle (ipc) determine the rate at which a cpu executesinstructions.cluster.a group of computers connected by a highspeed network thatwork together as if they were one machine with multiple cpus.getting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.280getting up to speedcmos.complementary metal oxide semiconductor. cmos is the semiconductor technology that is currently used for manufacturing processors and memories. while other technologies (silicongermaniumand galliumarsenide) can support higher clock rates, their higher costand lower integration levels have precluded their successful use insupercomputers.commodity processor.a processor that is designed for a broad marketand manufactured in large numbers, in contrast to a custom processor.commodity supercomputer.a supercomputer built from commodityparts.communication.the movement of data from one part of a system toanother. local communication is the movement of data between theprocessor and memory; global communication is the movement ofdata from one node to another.composite theoretical performance.ctp is a measure of the performance of a computer that is calculated using a formula that combinesvarious system parameters. ctp is commonly measured in millionsof theoretical operations per second (mtops). systems with a ctpabove a threshold (currently 190,000 mtops) are subject to stricterexport controls. the threshold is periodically raised. while ctp isrelatively easy to compute, it bears limited relationship to actual performance.computational fluid dynamics (cfd).the simulation of flows, such asthe flow of air around a moving car or plane.computational grid.originally used to denote a hardware and softwareinfrastructure that enables applying the resources of many computersto a single problem. now increasingly used to denote more broadly ahardware and software infrastructure that enables coordinated resource sharing within dynamic organizations consisting of individuals, institutions, and resources.control parallelism.parallelism that is achieved by the simultaneous execution of multiple threads.cost/performance ratio.the ratio between the cost of a system and theeffective performance of the system. this ratio is sometimes estimatedby the ratio between the purchase cost of a computer and the performance of the computer as measured by a benchmark. a more accurate but hard to estimate measure is the ratio between the total cost ofownership of a platform and the value contributed by the platform.cots.commercial, offtheshelf.cpu.central processing unit, the core unit of a computer that fetchesinstructions and data and executes the instructions. often used as asynonym for processor.getting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.appendix d281cstb.the computer science and telecommunications board is part ofthe national research council.custom processor. a processor that is designed for a narrow set of computations and is manufactured in small numbers; in particular, a processor designed to achieve highperformance in scientific computing.custom supercomputer.a supercomputer built with custom processors.cyberinfrastructure.an infrastructure based on grids and on applicationspecific software, tools, and data repositories that support research in a particular discipline.darpa.defense advanced research projects agency, the central research and development organization of the department of defense(dod).data parallelism.parallelism that is achieved by the application of thesame operation to all the elements of a data aggregate, under the control of one instruction. vector operations are the main example of dataparallelism.dense linear algebra.linear algebra computations (such as the solutionof a linear system of equations) that involve dense matrices, wheremost entries are nonzero.discretization.the process of replacing a continous system of differential equations by a finite discrete approximation that can be solved ona computer.distributed memory parallel system.a parallel system, such as a cluster, with hardware that does not support shared memory.dod.department of defense.doe.department of energy. doe is a major funder and user ofsupercomputing, through the asc program and the various scienceprograms of the office of science.dram.dynamic random access memory. the technology used in themain memory of a computer; dram is denser, consumes less power,and is cheaper but slower than sram. two important performancemeasures are memory capacity, measured in megabytes or gigabytes,and memory access time, or memory latency. the memory access timedepends on the memory access pattern; row access time (or row access latency) is the worstcase access time, for irregular accesses.effective performance.the rate at which a processor performs operations (for a particular computation), often measured in operations persecond. often used as a shorthand for effective floatingpoint performance. more generally, the rate at which a computer system computes solutions.efficiency or processor efficiency.the ratio between the effective performance of a processor and its peak performance.getting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.282getting up to speedes.earth simulator, a large custom supercomputer installed in japan inearly 2002 in support of earth sciences research. the es topped thetop500 list from its intallation to june 2004 and still provides significantly better performance than the largest u.s. supercomputers onmany application.fft.fast fourier transform.fftw.fastest fourier transform in the west.floatingpoint operations.additions and multiplications involvingfloatingpoint numbers, i.e., numbers in scientific notation.floatingpoint performance.the rate at which a computer executesfloatingpoint performance, measured in floatingpoint operations persecond. in particular, peak floatingpoint performance and effectivefloatingpoint performance.flops.floating point operations per second. flops is used as a metric fora computerõs performance.frontside bus (fsb).the connection of a microprocessor to the memorysubsystem.gigahertz (ghz).1,000,000,000 cycles per second, often the unit used tomeasure computer clock rates.grid.a synonym for computational grid.grid computing.the activity of using a computational grid.hecrtf.high end computing revitalization task force, a task forceestablished in march 2003 to develop a roadmap for highend computing (hec) r&d and discuss issues related to federal procurementof hec platforms.highbandwidth processors.a custom processor designed to providesignificantly higher effective memory bandwidth than commodityprocessors normally provide.highend computing (hec).a synonym for hpc.highperformance computing (hpc).computing on a highperformance machine. there is no strict definition of highperformance machines, and the threshold for high performance will change over time.systems listed in the top500 or technical computing systems sellingfor more than $1 million are generally considered to be highperformance.hpcci.high performance computing and communications initiative,which was established in the early 1990s as an umbrella for federalagencies that support research in computing and communication, including hpc.hpcs.high productivity computing systems, a darpa programstarted in 2002 to support r&d on a new generation of hpc systemsthat reduce time to solution by addressing performance, programmability, portability, and robustness.getting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.appendix d283hpf.highperformance fortran, a language designed in the early 1990sas an extension of fortran 90 to support data parallelism on distributed memory machines. the language was largely discarded in theunited states but continue to be used in other countries and is usedfor some codes on the earth simulator.hybrid supercomputer.a supercomputer built with commodity processors but with a custom interconnect and a custom interface to theinterconnect.idc.international data corporation.ihec report.formally, the report on high performance computing for thenational security community, a report requested by the house of representatives from the secretary of defense and nominally submittedin july 2002. it describes an integrated highend computing program.instructionlevel parallelism.the concurrent execution of multiple instructions in a processor.instructions per cycle (ipc).average number of instructions executedper clock cycle in a processor. ipc depends on the processor designand on the code run. the product of ipc and clock speed yields theinstruction execution rate of the processor.interconnect or interconnection network.the hardware (cables andswitches) that connect the nodes of a parallel system and support thecommunication between nodes. also known as a switch.irregular memory access.a pattern of access to memory where successively accessed words are not equally spaced.isv.independent software vendor.lanl.los alamos national laboratory.lapack.linear algebra package, a package that has largely superseded linpack. the linpack library makes heavy use of the blassubroutines.latency.a measure of delay. memory latency is the time needed to access data in memory; global communication latency is the time neededto effect a communication between two nodes through the interconnect. both can be a performance bottleneck.lbnl.lawrence berkeley national laboratory.linpack.a linear algebra software package; also a benchmark derivedfrom it that consists of solving a dense system of linear equations. thelinpack benchmark has different versions, according to the size of thesystem solved. the top500 ranking uses a version where the chosensystem size is large enough to get maximum performance.linux.a version of the unix operating system initially developed bylinus torvalds and now widely used. the linux code is freely available in open source.llnl.lawrence livermore national laboratory.getting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.284getting up to speedmemory wall.faster increase in processor speed relative to memory access time. it is expected to hamper future improvements in processorperformance.mesh partitioners.a program that partitions a mesh into submeshes ofroughly equal size, with few edges between submeshes. such a program is needed to map a computation on a mesh to a parallel computer.message passing.a method of communication between processes thatinvolves one process sending data and the other process receiving thedata, via explicit send and receive calls.microprocessor.a processor on a single integrated circuit chip.mips.millions of instructions per second. a measure of a processorõsspeed.mpi.message passing interface, the current de facto standard libraryfor message passing.mtops.millions of theoretical operations per second; the unit used tomeasure the composite theoretical performance (ctp) of highperformance systems.mttf.mean time to failure, the time from when a system or an application starts running until it is expected to fail.multigrid.a technique for the numerical solution of the linear systemsthat often arise from differential equations. it alternates the use ofgrids of various resolutions, achieving faster convergence than computations on fine grids and better accuracy than computations oncoarse grids.multilevel.numerical simulations that use multiple levels ofdiscretization for a given domain, mixing coarser and finerdiscretizations; multigrid is an example of multilevel.multiphysics.a simulation that combines various physical models. forexample, a simulation of combustion that combines a fluid model witha model of chemical reactions.multiprocessor.a system comprising multiple processors. each processor executes a separate thread. a singlechip multiprocessor is a system where multiple processors reside on one chip.multithreaded processor.a processor that executes concurrently or simultaneously multiple threads, where the threads share computational resources (as distinct from a multiprocessor, where threads donot share computational resources). a multithreaded processor canuses its resources better that a multiprocessor: when a thread is idling,waiting for data to arrive from memory, another thread can executeand use the resources.multithreading.a form of parallelism where multiple threads run concurrently and communicate via shared memory.getting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.appendix d285nas.nasaõs advanced supercomputing division (previously knownas the numerical aerospace simulation systems division). the nasbenchmarks are a set of benchmarks that were developed by nas torepresent numerical aerospace simulation workloads.nasa.national aeronautics and space administration.nastran.a structural analysis package developed in the mid1960sat nasa and widely used by industry. it is now available both inopen source and as a supported product.ncar.national center for atmospheric research.ncsa.national center for supercomputing applications at the university of illinois at urbanachampaign, one of three extant nsfsupercomputing centers.nersc.national energy research scientific computation center, asupercomputing center maintained by doe at the lawrence berkeleynational laboratory to support basic scientific research.netlib.an online repository of mathematical software maintained bythe university of tennesse at knoxville and by the oak ridge national laboratory.nih.national institutes of health, the focal point for federally fundedhealth research.nitrd.networking and information technology r&d, a federal program. the program includes (among other areas) the high end computing program component area. this involves multiple federalagencies (nsf, nih, nasa, darpa, doe, the agency for healthcareresearch and quality (ahrq), nsa, nist, noaa, epa, the officeof the director of defense research and engineering (oddr&e), andthe defense information systems agency (disa)). the national coordination office for information technology research and development (nco/it r&d) coordinates the programs of the multiple agencies involved in nitrd.nnsa.national nuclear security administration, the organizationwithin doe that manages the stockpile stewardship program that isresponsible for manufacturing, maintaining, refurbishing, surveilling,and dismantling the nuclear weapons stockpile.noaa.national oceanic and atmospheric administration.node.the building block in a parallel machine that usually consists of aprocessor or a multiprocessor, memory, an interface to the interconnect and, optionally, a local disk.nonexcludable goods.goods that suppliers cannot prevent some peoplefrom using while allowing others to use them.nonrival goods.goods that each consumer can enjoy without diminishing anyone elseõs ability to enjoy them.getting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.286getting up to speednrc.the national research council is the operating arm of the national academies.nsa.national security agency, americaõs cryptologic organization.nsa is a major user of supercomputing.nsf.national science foundation, an independent federal agency withresponsiblity for scientific and engineering research. nsf funds research in computer science and engineering and supports three national supercomputing centers that serve the science community.nwchem.a computation chemistry package developed at the doe pacific northwest national laboratory (pnnl).ode.ordinary differential equation.open source.software that is available to users in source form and canbe used and modified freely. open source software is often createdand maintained through the shared efforts of voluntary communities.paci.partnership for advanced computational infrastructure at nsf.parallel efficiency.the ratio between the speedup achieved with p processors and the number of processors p. parallel efficiency is an indication of scalability; it normally decreases as the number of processors increases, indicating a diminishing marginal return as moreprocessors are applied to the solution of one problem.parallel speedup.the ratio between the time needed to solve a problemwith one processor and the time needed to solve it with p processors,as a function of p. a larger parallel speedup indicates that parallelismis effective in reducing execution time.parallel file system.a file system designed to support efficiently a largenumber of simultaneous accesses to one file initiated by distinct processes.parallelism.the concurrent execution of operations to achieve higherperformance.pde.partial differential equation.peak performance.highest performance achievable by a system. oftenused as a shorthand for peak floatingpoint performance, the highestpossible rate of floatingpoint operations that a computer system cansustain. often estimated by considering the rate at which the arithmetic units of the processors can perform floatingpoint operationsbut ignoring other bottlenecks in the system. thus, it is often the casethat no program, and certainly no program of interest, can possiblyachieve the peak performance of a system. also known as nevertoexceed performance.petsc.a package for the parallel solution of sparse linear algebra andpde problems developed at doeõs argonne national laboratory(anl).getting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.appendix d287pim.processor in memory, a technique that combines dram and processor on the same chip to avoid the memory wall problem.pitac.presidentõs information technology advisory committee, chartered by congress in 1991 and 1998 as a federal advisory committee toprovide the president with independent, expert advice on federal information technology r&d programs.prefetching.the moving of data from memory to cache in anticipationof future accesses by the processor to the data, so as to hide memorylatency.process.an executing program that runs in its own address space. aprocess may contain multiple threads.processor.see cpu.programming model.an abstract conceptual view of the structure andoperation of a computing system.public goods.goods that are nonrival and nonexcludable. publiclyavailable software that is not protected by a copyright or patent is anexample of a public good.put/get.a model of communication between processes that allow oneprocess to read from (get) or write to (put) the memory of anotherprocess with no involvement of the other process.r&d.research and development.scalar processor.a processor that operates only on scalar (i.e., singleword) operands; see vector processor.scatter/gather.a type of memory access where multiple words areloaded from distinct memory locations (gather) or stored at distinctlocations (scatter). vector processors typically support scatter/gatheroperations. similarly, a global communication where data are received from multiple nodes (gather) or sent to multiple nodes (scatter).sci.strategic computing initiative, a large program initiated bydarpa in the 1980s to foster computing technology in the unitedstates.shared memory multiprocessor (smp).a multiprocessor where hardware supports access by multiple processors to a shared memory. theshared memory may be physically distributed across processors.shmem.a message passing library developed for the cray t3e andnow available on many systems that support put/get communicationoperations.sparc.scalable processor architecture.sparse linear algebra.linear algebra computations (such as the solution of a linear system of equations) that involve sparse matrices,where many entries are zero. sparse linear algebra codes use datastructures that store only the nonzero matrix entries, thus saving storgetting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.288getting up to speedage and computation time but resulting in irregular memory accessesand more complex logic.spatial locality.the property that data stored near one another tend tobe accessed closely in time. good (high) spatial locality ensures thatthe use of multiple word cache lines is worthwhile, since when a wordin a cache line is accessed there is a good chance that other words inthe same line will be accessed soon after.specfp.set of benchmarks maintained by the standard performanceevaluation corporation (spec); see <http://www.spec.org>. specfpis the floatingpoint component of the spec cpu benchmark thatmeasures performance for computeintensive applications (the othercomponent is specint). the precise definition of the benchmark hasevolvedñthe official name of the current version is spec cfp2000.the changes are small, however, and the mean flops rate achieved onthe benchmarks is a good measure of processor performance evolution.speedup.see parallel speedup.sram.static random access memory. sram is faster but consumesmore power and is less dense and more expensive than dram. sramis usually used for caches, while dram is used for the main memoryof a computer.stockpile stewardship program.a program established at doe by thefy 1994 defense authorization act to develop sciencebased tools andtechniques for assessing the performance of nuclear weapon systems,predicting their safety and reliability, and certifying their functionality in the face of a halt in nuclear tests. the program includes computer simulation and modeling (asc) as well as new experimentalfacilities.supercomputer.refers to those computing systems (hardware, systemssoftware, and applications software) that provide close to the bestcurrently achievable sustained performance on demanding computational problems.supercomputing.used to denote the various activities involved in thedesign, manufacture, or use of supercomputers.switch.see interconnect.synchronization.communication between threads with the effect ofconstraining the relative order that the threads execute code.temporal locality.the property that data accessed recently in the pastare likely to be accessed soon again. good (high) temporal localityensures that caches can effectively capture most memory accesses,since most accesses will be to data that were accessed recently in thepast and that reside in the cache.getting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.appendix d289thread.the basic unit of program execution.time to solution.total time needed to solve a problem, including getting a new application up and running (the programming time), waiting for it to run (the execution time), and, finally, interpreting theresults (the interpretation time).top500.a list, generated twice a year, of the sites operating the 500most powerful computer systems in the world, as measured by thelinpack benchmark. while the list is often used for rankingsupercomputers (including in this study), it is widely understood thatthe top500 ranking provides only a limited indication of the abilityof supercomputers to solve real problems.total cost of ownership.the total cost of owning a computer, includingthe cost of the building hosting it, operation and maintenance costs,and so on. total cost of ownership can be significantly higher than thepurchase cost, and systems with a lower purchase cost can havehigher total cost of ownership.unix.an operating system (os) developed at bell laboratories in thelate 1960s. unix is the most widely used os on highend computers.different flavors of unix exist, some proprietary and some opensource, such as linux.upc.universal parallel c.vector operation.an operation that involves vector operands (consisting of multiple scalars), such as the addition of two vectors, or theloading from memory of a vector. vector loads and stores can be usedto hide memory latency.vector processor.a processor that supports vector operations.vtk.visualization toolkit.getting up to speed: the future of supercomputingcopyright national academy of sciences. all rights reserved.
detailsdistribution, posting, or copying of this pdf is strictly prohibited without written permission of the national academies press. (request permission) unless otherwise indicated, all materials in this pdf are copyrighted by the national academy of sciences.copyright © national academy of sciences. all rights reserved.the national academies pressvisit the national academies press at nap.edu and login or register to get:œ œ 10% off the price of print titlesœ special offers and discountsget this bookfind related titlesthis pdf is available at sharecontributorshttp://nap.edu/4761virtual reality: scientific and technological challenges556 pages | 6 x 9 | hardbackisbn 9780309051354 | doi 10.17226/4761nathaniel i. durlach and anne s. mavor, editors; committee on virtual realityresearch and development, national research councilvirtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.virtual realityscientific and technologicalchallengesnathaniel i. durlach and anne s. mavor, editorscommittee on virtual reality research and developmentcommission on behavioral and social sciences and educationcommission on physical sciences, mathematics, and applicationsnational research councilnational academy presswashington, d.c. 1995ivirtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.national academy press 2101 constitution ave., n.w. washington, dc 20418notice: the project that is the subject of this report was approved by the governing board of thenational research council, whose members are drawn from the councils of the national academyof sciences, the national academy of engineering, and the institute of medicine. the members ofthe committee responsible for the report were chosen for their special competences and with regardfor appropriate balance.this report has been reviewed by a group other than the authors according to proceduresapproved by a report review committee consisting of members of the national academy of sciences, the national academy of engineering, and the institute of medicine.the work of the committee on virtual reality research and development is supported bydepartment of the army contract no. daad0592c0087 issued by the u.s. aberdeen provingground support activity. the views and opinions, and findings contained in this report are those ofthe author(s) and should not be construed as an official department of the army position, policy, ordecision, unless so designated by other official documentation.library of congress cataloginginpublication datavirtual reality : scientific and technological challenges / nathaniel i. durlach and anne s.mavor, editors.p. cm."committee on virtual reality research and development, commission on behavioraland social sciences and education, commission on physical sciences, mathematics, andapplications, national research council."includes bibliographical references and index.isbn 03090513551. humancomputer interaction. 2. virtual reality. i. durlach, nathaniel i. ii. mavor,anne s. iii. national research council (u.s.). committee on virtual reality researchand development.qa76.9.h85v6 1994006šdc20 9437695cipcopyright 1995 by the national academy of sciences. all rights reserved.printed in the united states of americaiivirtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.committee on virtual reality research anddevelopmentnathaniel durlach (chair), department of electrical engineering andcomputer science, research laboratory of electronics, massachusettsinstitute of technologysteve bryson, nasaames research center, moffett field, californianorman hackerman, robert a. welch foundation, houston, texasjohn n. hollerbach, department of biomedical engineering, mcgilluniversityjames r. lackner, ashton graybiel spatial orientation lab, brandeisuniversityj. michael moshell, institute for simulation and training, the universityof central floridarandy pausch, department of computer science, university of virginiarichard w. pew, bbn laboratories, inc., cambridge, massachusettswarren robinett, virtual reality games, inc., chapel hill, north carolinajoseph rosen, dartmouth hitchcock medical center, lebanon, newhampshiremandayam a. srinivasan, department of mechanical engineering,research laboratory of electronics, massachusetts institute of technologyjames j. thomas, battelle pacific northwest laboratory, richland,washingtonandries van dam, computer science department, brown universityelizabeth wenzel, nasaames research center, moffett field,californiaandrew witkin, school of computer science, carnegie mellon universityeugene wong, department of electrical engineering and computerscience, university of california, berkeleymichael zyda, department of computer science, naval postgraduateschoolanne s. mavor, study directorherbert s. lin, senior staff officer, computer science andtelecommunications boardcindy s. prince, project assistantiiivirtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.ivvirtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.contents preface vii executive summary 1part i overview 13part ii research and technology 911 some psychological considerations 932 the visual channel 1113 the auditory channel 1344 haptic interfaces 1615 position tracking and mapping 1886 wholebody motion, motion sickness, and locomotioninterfaces 2057 speech, physiology, and other interface components 2318 computer hardware and software for the generation ofvirtual environments 2479 telerobotics 30410 networking and communications 36211 evaluation of synthetic environment systems 373contentsvvirtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.part iii applications 38112 specific applications of se systems 387 references 444 appendixes a biographical sketches 519b contributors 526 index 529contentsvivirtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.prefacethe committee on virtual reality research and development wasestablished by the national research council in 1992 at the request of aconsortium of federal government agencies: the advanced research projectsagency, the air force office of scientific research, the human research andengineering directorate of the army research laboratory, the crew systemsdirectorate and the human resources directorate of the armstrong laboratory,the army natick rd&e center, the national aeronautics and spaceadministration, the national science foundation, the national securityagency, and the sandia national laboratory. as a group, these agencies soughtguidance and direction regarding the federal investment in research anddevelopment in the area of virtual reality.this report constitutes the committee's response to the charge to"recommend a national research and development agenda in the area of virtualreality" to guide government research and development over the nextgeneration. although the charge refers only to virtual reality systems (systemsin which the world the human operator interacts with is generated by acomputer), the committee has also considered teleoperator systems. such anextension is required not only for logical and scientific reasons, but alsobecause many of the examples cited in the charge are teleoperator systems.the purpose of the agenda is to provide a technical rationale to federalagencies for the allocation of their resources in support of research andtechnology developmentšand thereby to help define and shape the field. inkeeping with this purpose, the recommendations in this reportprefaceviivirtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.have been generated by a committee consisting primarily of computer scientists,engineers, and psychologists, some of whom have extensive experience infederal science and technology policy. the committee, in addition to drawingon the expertise of its own members, has made substantial use of advisers andconsultants. overall, this group's expertise is well matched to its task ofsurveying the scientific and technological state of the field, the potential ofcurrent and future technology for improving performance in various applicationareas, and the types of research and development required to realize thispotential. in addition, it is important to note that most members of thecommittee have a vested interest in seeing the field of virtual reality andsynthetic environments flourish because much of their professional work isrelated to it. indeed, serious involvement in and knowledge about the field wereamong the main criteria used to form the committee.ultimately, however, the federal research and development agenda mustalso take account of societal needs, the potential of various research anddevelopment accomplishments for fulfilling these needs, and the kinds andmagnitudes of attention and support various programs are likely to receivewithout special efforts. furthermore, given a research and developmentprogram that receives high ratings according to these criteria, it is important tospecify the kinds of infrastructure that will facilitate the carrying out of thisprogram. we recognize that the full range of expertise required to determineand judge many of these factors goes well beyond the competencies representedon the committee. in particular, both the formulation and evaluation of societalneeds and the specification of appropriate societal infrastructure to ensureefficient performance in carrying out federal research and developmentprograms require inputs from individuals with expertise not only in computerscience, engineering, and psychology, but also in sociology, economics,business, social policy, and government.we have attempted, to the extent possible, to avoid basing ourrecommendations on assumptions concerning the relative importance of varioussocietal needs. also, to the extent possible, we have avoided basing ourrecommendations on assumptions concerning the extent to which and themanner in which given technological advances will influence societyšthehistory of such predictions is extremely humbling. however, given theimportance of assigning some sort of priorities to different possible researchand development programs (a laundry list of all possible programs would be oflittle use to anyone), these issues cannot be avoided entirely. what we havetried to do is to make our assumptions explicit. we also suggest that individualswith additional expertise be asked to follow up this work. finally, with regardto the problem of ensuring appropriate infrastructures to carry out therecommended researchprefaceviiivirtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.and development programsša problem that is perhaps more difficult to solvethan the problem of selecting the programs themselvesšwe point out some ofthe factors that we believe need to be considered in the followup work.this research and development agenda for the federal investment in virtualenvironments and teleoperation is rooted in a careful review and analysis of thecurrent state of research and technology and of the steps required to reach apoint at which significant applications can be fully realized. our perspective oninfrastructure takes cognizance of the need for additional studies of issuesrelated to market forces and societal design as this agenda is further shaped andimplemented. in the committee's view, our scientific and technical assessmentrepresents only the first step in a multistage process that would appropriatelylead to a more fully developed federal research agenda based on considerationsof societal as well as scientific and technical issues.although our report is directed primarily to the federal agencies whorequested it, we have attempted to make it useful to a wide variety of readers,including students and professionals working in academia and industry. thematerial is structured in the following way. the executive summary provides abrief synopsis of the main points that can easily be read in a few minutes. theoverview is intended to provide a general understanding of the field and of ourrecommendations; it can be read in a few hours. the remainder of the reportprovides the detailed analysis as well as numerous bibliographical citations.as in most reports prepared by large committees, there has been a struggleto find an appropriate balance between coherence and uniformity of style onone hand and free and full expression by individual experts on the other. as isevident when reading the report, different chapters have been prepared bydifferent people. in other words, with the exception of the executive summaryand the overview, we have leaned toward full and free expression rather thanuniformity. it should also be noted that the differences among the chaptersreflect not only the different writing styles of the various contributors but alsothe different cultures associated with the topics being discussed. for example,even within the single domain of humanmachine interfaces, discussions of thevisual, auditory, and haptic channels would require rather disparate treatmenteven if written by the same individual.many individuals have made a significant contribution to the committee'sthinking and to various sections of this report by serving as presenters,consultants, and advisers. a complete list of contributors and their affiliations ispresented in appendix b. although all of these individuals provided us withvaluable information, a few played a more direct role in the preparation of thismanuscript and deserve special mention.prefaceixvirtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.we extend our gratitude to kurt akeley for his contribution on computerarchitecture; to walter aviles for his work on visual technology and remote,teleoperated vehicles; to norman badler for his contributions on modelingvirtual worlds and virtual actors; to paul dizio for his work on fullbodymotion interfaces; to blake hannaford and thomas sheridan for theirdiscussions of teleoperator systems; to michael macedonia for his work onnetworking; to john makhoul and kenneth stevens for their contributions onspeech communication; to barbara shinncunningham for her work on theauditory channel; to thomas wiegand and richard held for their discussions ofthe status of research on the human visual system; to david zeltzer for his workon visual technology and computer modeling; and to george zweig for hissustained interest in our study and his extremely valuable critical comments ona wide variety of crucial issues.in the course of preparing this report, each member of the committee tookan active role in drafting sections of chapters, leading discussions, and readingand commenting on successive drafts. even so, john hollerbach and michaelzyda deserve special acknowledgment for their heroic efforts throughout thestudy. john hollerbach assumed major responsibility for the work ontelerobotics as well as contributing extensively to several other sections of thereport. michael zyda took responsibility for obtaining, summarizing, andintegrating material on the computer generation of virtual environments andnetworking. other committee members contributing work on the computergeneration of virtual environments include steve bryson for scientificvisualization, randy pausch for techniques for interacting in threedimensionalenvironments, andrew witkin for modeling software, and andries van dam forhis overall review and insightful comments. committee members whocontributed their expertise to the humanmachine interface sections of the reportwere james lackner for wholebody motion and illusions; elizabeth wenzel forthe auditory channel; and mandayam srinivasan for the haptic channel and forthe discussion of hardware and software requirements for multimodalinterfaces. those who provided expertise in the various application domains ofvirtual environments include norman hackerman, michael moshell, richardpew, warren robinett, joseph rosen, james thomas, and eugene wong.staff of the national research council made important contributions toour work in many ways. we would like to extend a special note of thanks toharold van cott and herbert lin. harold van cott was instrumental in theoriginal thinking about this project and the formation of the committee; he hasbeen of immeasurable assistance throughout the process in providing advice anddraft materials. herbert lin, a senior staff officer to the committee from thecomputer science and telecommunicationsprefacexvirtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.board, contributed extensively to the applications section of the report. further,we would like to express our appreciation to marjory blumenthal, director ofthe board, for her contributions in the early stages of project development, andto alexandra wigdor, director of the division on education, labor, and humanperformance, for her valuable insight and support. thanks are also due to cindyprince, the committee's administrative assistant, who was indispensable inorganizing meetings, arranging travel, and compiling agenda materials. we arealso indebted to carolyn sax and nora luongo for their work on themanuscript, and to christine mcshane, who edited and significantly improvedthe report.to our sponsors we are most grateful for their interest in the topic of thisreport and for their many useful contributions to the committee's deliberations.we are particularly appreciative of the early efforts of bernard corona, of thehuman research and engineering directorate, u.s. army research laboratory,and james jenkins, of the national aeronautics and space administration,which led to the formation of the committee.finally, it is a deep pleasure to acknowledge the remarkable feelings thatappear to have been generated by this cooperative, interdisciplinary study.these feelings can be best summarized by noting the following two ideasexpressed at the last meeting: whatever the reception of the report, the field of synthetic environmentswas significantly advanced by the education of the committee members,along with their consultants and advisers, that took place during the study; many of the committee members intend to continue meeting on a regularbasis to continue the spirited interdisciplinary dialogue that was initiatedduring the study.nathaniel i. durlach, chairanne s. mavor, study directorcommittee on virtual reality research and developmentprefacexivirtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.preface xiivirtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.virtual reality xiiivirtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved. xivvirtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.executive summaryat the request of a consortium of federal government agencies, thecommittee on virtual reality research and development was established toprovide guidance and direction on the allocation of resources for a coordinatedfederal program in the area of virtual reality. in responding to this charge, thecommittee has included both virtual environments and teleoperation in itsassessment of the field.this report includes recommendations and extensive background materialconcerning systems popularly referred to by such terms as virtual reality,1cyberspace, virtual environments, teleoperation, telerobotics, augmentedreality, and synthetic environments. in all such systems, the basic componentsare a human operator, a machine, and a humanmachine interface linking thehuman operator to the machine.in a teleoperator system, the machine is an electromechanical toolcontaining sensors and actuators (i.e., a telerobot) that effectively extend theoperator's sensorimotor system and thereby allow him or her to sense andmanipulate the real environment in new ways. in a virtual environment (ve)system, the machine is an appropriately programmed computer1 our use of the term virtual reality in the title of the book differs from that in thereport itself. in the title it is intended, as is often the case in the popular press, toencompass the entire field by including both teleoperator and virtual environmentsystems. in the text, virtual reality and virtual environment are used synonymously anddo not include teleoperation. the term we use to refer to systems of this general kindwhen we have no need to distinguish among different types is synthetic environments.executive summary1virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.that generates or synthesizes virtual worlds with which the operator can interact.whereas the purpose of a teleoperator system is to sense and transform the realworld (as in removal of hazardous waste by teleoperation), the purpose of avirtual environment system is to alter the state of the human operator or thecomputer (as in the use of virtual environment systems for training, designing,marketing, or scientific modeling). in many systems, such as teleoperatorsystems that make use of virtual environment systems to help plan futureactions, teleoperator systems and virtual environment systems are combined. inan augmentedreality system, the operator's interaction with the real world(either directly or via a teleoperator system) is enhanced by overlaying theassociated realworld information with information stored in the computer(generated from models, derived previously from other sensing systems, etc.).in general, we refer to all systems of the types just described as syntheticenvironment (se) systems.virtual environment systems differ from traditional simulator systems inthat they rely much less on physical mockups for simulating objects withinreach of the operator and are much more flexible and reconfigurable. virtualenvironment systems differ from other previously developed computercenteredsystems in the extent to which realtime interaction is facilitated, the perceivedvisual space is threedimensional rather than twodimensional, the humanmachine interface is multimodal, and the operator is immersed in the computergenerated environment.in recent years, synthetic environment systems, particularly virtualenvironment systems, have generated both great excitement and greatconfusion. these factors are evident in the extensive material published in thepopular press; in the unrealistic expectations on the part of the public; in theinadequate terminology being used; in the deluge of conferences, articles,books, and demonstrations occurring; in the difficulties being experienced incommunicating across disciplinary boundaries even by individuals whoseprofessional work lies within the domain of synthetic environment systems; andin the frenetic pace at which most of the individuals concerned with syntheticenvironments are working.in this book, we attempt to describe the current state of research andtechnology that is relevant to the development of synthetic environmentsystems, provide a summary of the application domains in which such systemsare likely to make major contributions, and outline a series of recommendationsthat we believe are crucial to rational and systematic development of thesynthetic environment field. inasmuch as the ''bottom line" of the committee'swork is our recommendations (presented in the final section of the overview),the remainder of this executive summary focuses on these recommendations.they are summarized underexecutive summary2virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.the headings of applications, psychological considerations, technology,evaluation, and government policy and infrastructure.in discussing these areas, it should be noted that the recommendationshave not been prioritized in any detailed manner. this is due primarily to ourjudgment that successful development and application of se systems dependson an entire matrix of interrelated factors. we nevertheless feel that it isimportant to stress the crucial need for improved hardware technologies toenable development of improved interface devices and improved computergeneration of multimodal images. unlike the situation in the area ofteleoperation, in the area of ves there are relatively few individuals who haveprimary interests or backgrounds in hardware; most individuals in the ve areaare involved primarily in the software end of computer science, incommunication or entertainment media, and in human perception andperformance. thus, the importance of adequate hardware, without which theve field will never come close to realizing its potential, tends to be underplayedby the ve community. a somewhat similar comment concerns the issue of usercomfort. to date, a very large fraction of ve usage has occurred in the contextof short demonstrations, a context in which the degree of comfort is relativelyunimportant. however, if the comfort of ve systems (particularly headmounted displays) cannot be radically improved, the practical usage of thesesystems will be limited to emergency situations or to very short time periods. inother words, adequate comfort, as well as technically adequate hardware, areessential to realizing the potential of the se field.finally, it should also be noted that our thoughts about government policyand infrastructure are stated as comments and suggestions rather than asrecommendations. they are based solely on the experience and judgment of thecommittee members.recommendationsapplicationssignificant research and development is taking place in a wide variety ofapplication domains, and in some cases the results of this work are beginning tobe applied on an experimental basis. although it is not yet clear which taskswill eventually gain the most from the use of se systems, the committee hasidentified four application domains that show particular promise: (1) design,manufacturing, and marketing; (2) medicine and health care; (3) hazardousoperations; and (4) training.other important application domains that are assigned lower priority areeducation, information visualization, and telecommunications and teletravel.the application domain of education is of critical concern;executive summary3virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.however, in our judgment, the overwhelming issues in this domain are social,political, and economic rather than technological. because the domains ofinformation visualization and telecommunications and teletravel cut across allothers, we expect them to receive substantial attention in connection with workthat is primarily addressed to the other domains. training has the same crosscutting property; however, it was judged to be so important and so well matchedto the technology that it was nevertheless given high priority.psychological considerationsbecause human beings constitute an essential component of all sesystems, there are very few areas of knowledge about human behavior that arenot relevant to the design, use, and evaluation of these systems.the committee recommends that work in this area be organized around thefollowing objectives: (1) development of a comprehensive review of theory anddata on human performance characteristics from the viewpoint of syntheticenvironment systems; (2) development of a theory that facilitates quantitativepredictions of human responses to alterations in sensorimotor loops; (3)development of cognitive models that facilitate effective design of ve systemsfor purposes of education, training, and information visualization; and (4)development of improved understanding of possible deleterious effects ofspending substantial portions of time in synthetic environments.technologydespite the enthusiasm and the "hype" surrounding the se field, there is asubstantial gap between the technology that is available and the technology thatis needed to realize the potential of se systems envisioned in the variousapplication domains. two partial exceptions are the virtual environmenttechnology used in the entertainment industry and the teleoperator technologyused for hazardous operations such as waste removal. for most applications tobe truly successful, however, the development of substantially improvedtechnology is a major requirement. in our review, we divide the relevanttechnology into four general categories: humanmachine interfaces, thecomputer generation of virtual environments, telerobotics, and networks.humanmachine interfaceshumanmachine interfaces for se systems include all the devices used topresent information to human users or to sense the human actions orexecutive summary4virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.responses that control the machine in question. in this book we examine theneeds for human research studies and technology development for the visual,auditory, and haptic channels; for whole body motion and locomotion displays;for position tracking; and for speech communication, physiological, olfactory,and gustatory interfaces. after careful analysis we have determined that themost important research and development needs in the interface area concernthe visual channel, the haptic channel, locomotion displays, and positiontracking, and we make specific recommendations on these topics accordingly.computer hardware and softwareit is computer hardware and software that produce virtual environments.technology should be capable of generating such environments in a way thatmakes them appear convincingly real to human users and that allows them tointeract with the environments in real time. with available technology,however, there is a major tradeoff between realistic images and realisticinteractivity.hardware requirements for virtual environments include very largephysical memories, multiple highperformance scalar processors, highbandwidth mass storage devices, and highspeed interface ports for variousinput and output peripherals. in the committee's judgment, commercial marketforces, if they continue to grow at the current rate, will probably be sufficient tosupport the needed development. therefore, the committee recommends noaggressive federal involvement in computer hardware development in the searea at this time. rather we conclude that hardware development remain largelya privatesector activity. should serious lags in development occur, thegovernment might then consider strategies for leveraging privatesectordevelopment efforts.software requirements are such that a major unified research program,focusing on the generation, implementation, and application of virtualenvironments, should be undertaken. the basic topics that need to beconsidered in such a program include: (1) multimodal humancomputerinteractions, (2) rapid specification and rendering of visual, auditory, and hapticimages, (3) models and tools for representing and interacting with physicalobjects under multimodal conditions (including automated model acquisitionfrom real data), (4) simulation frameworks (5) a new timecritical, realtimeoperating system suitable for virtual environments with relatively simple input/output requirements, (6) registration of real and virtual images in augmentedreality applications, (7) navigational cues in virtual space, (8) the behavior ofautonomous actors, and (9) computer generation of auditory and haptic images.because the natural tendency of computer scientists to concentrate on graphicswillexecutive summary5virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.focus sufficient attention on the visual channel, special attention must be givento the modeling and generation of auditory and haptic images and to the needsassociated with integrating the different modalities in virtual environmentsystems.teleroboticsin many ways, research issues in teleoperation are similar to those invirtual environments. independent of the purpose for which a system is beingdesigned (e.g., to train an operator, to remove hazardous waste) andindependent of whether the relevant environment is real or virtual, both areconcerned with the design, construction, and application of multimodal,immersive systems that enable the operator to interact usefully with somestructured environment. because of these similarities and the relatively longhistory of research in the teleoperation area, results in teleoperation can beusefully exploited in the virtual environment area. concerns unique toteleoperator systems relate to the design and performance of the complexelectromechanical systems referred to as telerobots and the unavoidable timedelays that arise in communicating between the humanmachine interface andthe telerobot when these subsystems are separated by large distances. suchcommunication delays can result in degraded or unstable teleoperatorperformance.improved teleoperator systems require improved control algorithms andmethods for constructing and using predictive displays and for realizingeffective supervisory control techniques as strategies for combattingcommunication time delays. hardware requirements include: (1) multiaxis,highresolution tactile sensors to provide telerobots with an adequate sense oftouch; (2) robot proximity sensors for local guidance prior to grasping, (3)multiaxis force sensors to measure net force and torque exerted on endeffectors, (4) improved actuator and transmission designs for highperformancejoints, and (5) realtime computational architectures. also, since many newproblems arise when a human is interfaced to a microtelerobot, research isneeded to capitalize on the advances now being made in the field ofmicroelectromechanics. similarly, research is needed to explore possibilitiesand problems associated with the development and application of distributedtelerobots (macro and micro).networkscommunication networks have the potential to transform virtualenvironments into shared worlds in which individuals, objects, and processesinteract without regard to their location. in the future, such networksexecutive summary6virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.will allow us to use virtual environments for such purposes as distance learning,group entertainment, distributed training, and communication among telerobotsin diverse locations. although the technology is becoming able to support thedevelopment of distributed virtual environments, it is currently insufficient tosupport multiple users and multiple modes of input in real time. other problemsto be resolved are network hostinterface slowdowns caused by the multiplelayers of operating system software and the high cost of purchasing time onhighspeed, widearea networks.we anticipate that in the future most virtual environment applications willrely heavily on network hardware and software. because several forces in thefederal government and in the private sector are driving the major advances inhardware, we do not advise additional investment in network hardwaredevelopment at this time. we do propose, however, that the federal governmentprovide funding for a program (to be conducted with industry and academia incollaboration) aimed at developing network standards that support therequirements for implementing distributed virtual environments on a largescale. furthermore, we propose funding of an open ve network that can beused by researchers, at a reasonable cost, to experiment with various venetwork software developments and applications.evaluationin general, se technology and se systems are not being adequatelyevaluated. admittedly, the evaluation task is complex: it involvesconsiderations of many disciplines, both whole systems and individualcomponents, a wide variety of component technologies, and many differenttypes of evaluation goals. nevertheless, we believe that if the se field is toprogress beyond the stage of demonstrations, serious evaluations are crucial.they are needed not only for estimating overall costeffectiveness, but also foranalyzing performance in terms of the contributions made by differentcomponent features and thereby guiding the directions of future research anddevelopment.developers of se systems should conduct evaluations using a variety ofapproaches throughout the development process, and the federal governmentshould therefore encourage the developers whose work it supports to include acomprehensive evaluation plan in the design stages of their research anddevelopment projects. the federal government should also help coordinate thedevelopment of standardized testing procedures for use across studies, systems,and laboratories, particularly in areas in which the private sector has not beenactive.executive summary7virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.government policy and infrastructurebecause the field of synthetic environments is in its very early stages, thefederal government has a rare and important opportunity to foster carefulplanning for its research and development. in this section, we discuss somemechanisms that we think federal agencies should consider as part of theirstrategic planning for the research and development agenda in the area ofsynthetic environments.first, a national information system that provides comprehensive coverageof research activities and results on synthetic environments in a userfriendlyway to a wide variety of users could be a useful tool for promoting crossfertilization and integration of the research and development efforts. such asystem could serve as a repository of text, data, computational models, andsoftware and could include effective subsystems for both retrieval anddissemination.second, federal agencies might fruitfully consider establishing a smallnumber of national research and development teams, each focusing on aspecific application. these teams could involve government, industry, andacademia, as well as the various disciplines relevant to the given application.funding could be provided jointly by both the federal government and theprivate sector.third, it might be useful for some federal agencies and offices to explorethe use of synthetic environment technology to meet their own administrativeand program needs. in addition to the application of synthetic environments tothe defense and space programs already under way, other application domains,such as training, telecommunication and teletravel, and informationvisualization, are relevant to the activities of many agencies. one way for thegovernment to facilitate the development of the se field would be to select afew agencies to serve as test beds for synthetic environment technology in thesegeneral domains.fourth, although it is probably too early in the development of syntheticenvironment systems to establish standards and regulations, it is not too earlyfor the federal government to begin to evaluate the work already under way inconnection with the telecommunications and entertainment industries. problemsthat are already of concern and are likely to increase as the field develops relateto technological compatibility issues, enforcement and control issues, and socialand ethical issues.finally, in developing a funding strategy for specific research anddevelopment projects, it is essential that federal planners be aware of existingmarket forces, which are as likely to be shaped by the results of research anddevelopment as they are to shape the research and development that isperformed. with strategic planning, it would be possible for the federalgovernment to use its investment in research to leverage developmentsexecutive summary8virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.in the private sector. similarly, it is essential that federal planners take accountof the societal implications of the technology. as with most other technologies,the effects of the advances in synthetic environments are likely to be mixed:some effects will be positive and others negative. it cannot be assumed that alltechnological advances, even those that are likely to have substantial practicalapplications, will necessarily be beneficial.overall, the committee believes that synthetic environment systems havegreat potential for helping to satisfy various societal needs and stimulatingadvances in some important areas of science and technology. in pursuing thecommittee's recommendations for research and development, the federalgovernment has an opportunity to make important contributions to thedevelopment of this exciting new field.executive summary9virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.executive summary10virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.ioverview11virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.12overviewvirtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.at the request of a consortium of federal government agencies, thecommittee on virtual reality research and development was established toprovide guidance and direction on the allocation of resources for a coordinatedfederal program in the area of virtual reality. in responding to this charge, thecommittee has included both virtual environments and teleoperation in itsassessment of the field. such an extension is required not only for logical andscientific reasons, but also because many of the examples cited in the chargefeature the use of teleoperator systems.in a synthetic environment (se) system, the human operator is transportedinto a new interactive environment by means of devices that display signals tothe operator's sense organs and devices that sense various actions of theoperator. in teleoperator systems, the human operator is connected by means ofsuch displays and controls to a telerobot that can sense, travel through, andmanipulate the real world. in virtual reality (vr) or virtual environment (ve)systems, the human operator is connected to a computer that can simulate awide variety of worlds, both real and imaginary. simple remote manipulatorsare an example of the first type of system; video games of the second type.teleoperator systems effectively provide the operator with a transformedsensorimotor system that enables him or her to perform new types of actions inthe real world. virtual environment systems effectively provide the operatorwith controllable methods for generating new types of experiences. using bothteleoperator and virtual environment systems, one can (or will be able to)explore the ocean floor and outer space, visit samarkand while staying inelmira, try out products not yet manufactured, dig up a 10ton container ofhazardous waste, take a canoe trip through the human circulatory system, andhave one's hair trimmed by a barber in seville.scope of the synthetic environment fieldthe research and development required to realize the potential of sesystems is extremely challenging. the systems are complicated because theyinvolve both complex artificial devices and a complex biological system (thehuman operator). there is a crucial need for cooperation among manydisciplines, including computer science, electrical and mechanical engineering,sensorimotor psychophysics, cognitive psychology, and human factors. also,the range of possible applications is exceedingly broad. overall, the committeebelieves that the se field has great potential, that the research and developmentrequired to realize this potential is just beginning, and that work in this areashould be vigorously pursued by a wide variety of specialists in a wide varietyof institutions.13overviewvirtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.there is currently a great deal of excitement, a great deal of ''hype," and agreat deal of confusion associated with the se area. a major source of theconfusion is the combination of rapid acceleration of interest in the area and thecoming together of individuals from widely varying disciplines. in some cases,individuals are coming together because the problem to be solved requiresexpertise in diverse areas. in other cases, they are coming together because ithas suddenly become apparent that essentially the same problems are beingaddressed by individuals in different fields who have never had the benefit ofcommunicating with each other about them.associated with this interdisciplinary feature of the se field is confusionover terminology: each discipline brings to the field its own language and itsown biases. for example, whereas computer scientists naturally use the termsinput and output in reference to the computer, psychologists use these terms inreference to the human user. thus, in a virtual environment system, what isoutput to the psychologist is input to the computer scientist. similar confusionsoften arise with the term interface. whereas computer scientists frequently usethis term to designate a component internal to the computer's hardware orsoftware, many others use the term as a shorthand for humancomputerinterface devices external to the computer. also, of course, in addition to thecommunication difficulties associated with the interdisciplinary nature of thefield, there are communication difficulties associated with the tendency ofdifferent individuals, institutions, and countries to compete rather than tocooperate.another source of confusion results from political and public relationsconsiderations. virtual reality and virtual environment (two terms that weregard as equivalent) are such "hot" terms that many people tend to use themeven when their use is logically inappropriate. thus, for example, these termsare often used in a manner that implies that teleoperator systems are a specialcase of virtual reality systems. at the same time, however, when describing theorigins of virtual reality systems, the history of teleoperator systems (inparticular, the use of headmounted displays in these systems) is entirelyignored. similar distortions often occur in connection with simulator systems.although simulator systems, like teleoperator systems, are closely related tovirtual environment systems and have a long and distinguished history, pastaccomplishments in the simulation area are often inappropriately downplayed.further discussion of the basic concepts and terminology is presented in thenext section.generally speaking, virtual reality currently has an extremely high "talktowork" or "excitementtoaccomplishment" ratio. between 1992 and 1994,roughly 12 new books have been published, 4 new journals or magazines havebeen started, and 200 new articles have been published14overviewvirtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.on the topic of virtual reality. major professional meetings and trade shows areoccurring at a rate of roughly one per month. over 10 government agencieshave held conferences or written reports on ve during the same twoyearperiod. and practically everyone in the field is spending substantial timetraveling to other laboratories that are working on ve and providingdemonstrations of their own facilities in their own laboratories.despite this high talktowork or excitementtoaccomplishment ratio,substantial efforts are, in fact, under way in various research and developmentareas and in various application domains. significant research and developmentprograms, as well as applications of currently available technology, are beingpursued in government, in academia, and in industry. also, some attempts arebeing made to develop adequate course material for educational programs in these area; however, it is likely to be some time before most academicdepartments recognize se as a legitimate field of specialization (e.g., one inwhich faculty can achieve tenure).current research and development efforts directly relevant to the creationof useful se technology are concerned with (1) computer generation of virtualenvironments, (2) design of telerobots, (3) improvement of humanmachineinterfaces, (4) study of relevant aspects of human behavior, and (5)development of communication systems that are adequate to supportnetworking of se systems. items (3) and (4) are relevant to all the kinds ofsystems considered, item (1) to ve systems, item (2) to teleoperation systems,and item (5) to networked systems. an additional item of importance whenaugmentedreality systems are considered is (6) merging of computergeneratedimages with images derived directly from the real world.the "se challenge" is related to the high performance computing andcommunications (hpcc) grand challenge program initiated by the federalgovernment through both the computer generation of ves and networkedsystems. for many applications, adequate computer generation of the associatedvirtual worlds is going to require very highperformance computing. similarly,the networking of se systems is going to require very highperformancecommunications. in general, se systems will provide both a major applicationarea for hpcc and an important source of constraints for the design of hpccsystems.currently, the main commercial driving force for the development of vesystems is the entertainment application. there is no equivalent commercialdriving force for the development of teleoperators or augmentedreality systemsat this time.programs on se technologies and applications are under way in almostevery developed country (thompson, 1993). major players are the15overviewvirtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.united states, japan, and the european economic community; other playersinclude south korea, singapore, the netherlands, and sweden. although eachof these regions is engaged in a full range of research, development, andcommercial activities, the work in each region bears the marks of its distinctiveculture.today, more than 25 universities, at least 15 federal agencies, and morethan 100 large and small companies throughout the united states arecontributing to the growth of research and development in the se field. inindustry, research and development directed toward defense, space, scientificvisualization, and medicine are more prominent in the united states thanelsewhere. the european economic community and japan have regional ornational initiatives on se, but such initiatives are still being debated in thiscountry.although the recession of the early 1990s in europe has slowed downinvestment, a variety of se projects are under way in industry and, to a lesserextent, in universities. interests in the united kingdom are similar to those inthe united states but place more emphasis on education, training, andentertainment. the united kingdom may well be the world leader in seentertainment systems. on the continent, work on se applications is beingconducted at the european space research center in noordwijk, thenetherlands. research on computeraided architectural software and a virtualrailroad environment are also being supported in the netherlands. in france, theuniversity at metz is developing an autonomous motor vehicle for people withdisabilities that uses se technology. in lille, the university of technology isexploring the use of teleoperation in surgery. at the university of paderborn ingermany, a new method for walkthrough animation in threedimensionalscenes is under way. in italy, the university of genoa is developing aknowledgebased simulation for production engineers.japan entered the ve part of the se world later than the united states andeurope. recently, however, that country has realized that ve, as well asteleoperation, is a logical extension of its strong national interest andbackground in robotics, automation, and highdefinition television. concernwith haptic interfaces and forcefeedback sensor display systems is also intense.as a consequence, japan has established 10 national consortia for research anddevelopment in the se area that, taken together, provide more funds per yearthan all se investment in the united states (larinaji, 1994). in 1992, the japantechnology transfer association formed an artificial reality and teleexistence research committee of 90 participating companies from the seindustry. knowledge and technology sharing among companiesšgenerally aboon to japanese industryšare extensive. these indicators, together with itstypical longrange financial horizon, large targeted investments, and a nationaltechnology16overviewvirtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.agenda, could give japan a major competitive advantage in se. the extent towhich this advantage is actually realized will depend, at least in part, on theextent to which japan can become a leader in the relevant computer softwareareas.this overview begins by presenting some basic concepts and terminologythat are important in talking about virtual environments and teleoperatorsystems. we then present some visions of where we think the technology maybe leading. the visions section differs from the rest of this report in thespeculative nature of the material and in the incorporation of societal issues intothe scenarios. the overview then goes on to summarize the current state of thesynthetic environment (se) field, covering application domains, knowledgeabout human behavior and performance, technology issues, and evaluationissues. the committee's assessment of needs and priorities completes theoverview. in making these recommendations, we include consideration of theextent to which various research goals are likely to be realized without specialgovernment funding efforts or are likely to require such efforts. similarly, weconsider issues related to the infrastructure required to carry out variousresearch and development programs.basic concepts and terminologythere are currently no precise and generally accepted definitions of theterms being used in our area of interest. this is due in part, as already discussed,to the interdisciplinary nature of the field and to public relations matters. it isalso due to fundamental problems of the type usually encountered in efforts tocreate language that faithfully reflects the structures and processes to which thelanguage refers. for example, whereas language is fundamentally discrete, theevolutionary process by which virtual environment systems have developedfrom antecedent systems (such as desktop computing systems, simulators,teleoperator systems, etc.) is effectively continuous. thus, either the definitionof virtual environment systems must remain rather fuzzy, or one must setarbitrary thresholds on the complex, continuous evolutionary process.here, we outline some of the principal defining ideas and indicate how theterms virtual environment, teleoperator, and augmented reality are related toeach other and to other closely related terms such as simulator, telerobot, androbot. our purpose is to provide background on the meaning of the terms weuse in order to permit readers to understand later sections of the report. theprocess of creating and defining terms in this area will of course continue formany years.a teleoperator system consists of a human operator, a humanmachineinterface, and a telerobot (figure 1). environmental signals are sensed by17overviewvirtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.sensors (cameras, microphones, etc.) located in the telerobot, transmitted to thehumanmachine interface, and presented to the human by means of displaydevices (e.g., cathode ray tubes, earphones) in the interface. human responses,usually motor actions, are sensed by the interface and used to control the actionsof the telerobot. thus, a teleoperator system can be viewed as a system forextending the sensorimotor system of the human organism. the purpose of sucha system is to facilitate the human operator's ability to sense, maneuver in, andmanipulate the environment. teleoperator systems vary along manydimensions, including the structure of the humanmachine interface and thetelerobot and the nature of the control algorithms.teleoperator systems have been used to conduct work in outer space andunder the ocean; to perform a variety of tasks in connection with security,firefighting, nuclear plants, and hazardous waste removal; to assist in varioustypes of military operations; to perform microsurgery; and to aid in therehabilitation of individuals with severe physical disabilities. in someteleoperator systems, the human operator has direct and detailed control of allthe telerobots actions. in other systems, the human's control occurs only at asupervisory level and many of the telerobots detailed actions are controlledlocally and automatically. in the extreme, there is no human control, all actionsof the telerobot are automatic and autonomous, and the telerobot is calledsimply a robot.a virtual environment system (also illustrated in figure 1) consists of ahuman operator, a humanmachine interface, and a computer. the computer andthe displays and controls in the interface are configured to immerse the operatorin an environment containing three dimensional objects with threedimensionallocations and orientations in threedimensional space. each virtual object has alocation and orientation in the surrounding space that is independent of theoperator's viewpoint, and the operator can interact with these objects in realtime using a variety of motor output channels to manipulate them. the extent towhich a virtual environment is designed to simulate a real environment dependson the specific application in mind.as illustrated in figure 1, teleoperator and virtual environment systems aresimilar in that they both involve human operators and elaborate humanmachineinterfaces. they differ however, with respect to what takes place on thenonhuman side of the interface. whereas in a teleoperator system the interfaceis connected to a telerobot that operates in a masterslave or supervisory controlmode in a realworld environment, in a ve system the interface is connected toa computer.consistent with this difference in structure is the difference in purposebetween the two types of systems: whereas the purpose of a teleoperator systemis to sense, manipulate, and transform the state of the18overviewvirtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.realworld environment, the purpose of a ve system is to sense, manipulate,and transform the state of the human operator (as in training or in scientificvisualization) or to modify the state of the information stored in the computer(e.g., the virtual environment or some theoretical model represented in thecomputer software). virtual environment systems are being used in the areas oftelecommunication, information visualization, health care, education andtraining, product design, manufacturing, marketing, and entertainment. in thenear future, such systems are likely to find further applications in various areasof psychology, including basic psychophysical research, biofeedback, andpsychotherapy.figure 1 schematic outline comparing a teleoperator system, a virtualenvironment system, and an unmediated (normal) system.many systems are now being developed that are mixtures or blends ofteleoperator and virtual environment systems. thus, for example, ve systemsare now being introduced as subsystems of teleoperator systems in order toassist the human operator in controlling the telerobot. in particular, when thetelerobot is sufficiently far removed from the human operator to causesignificant time delays in the transmission of information between the telerobotand the human operator, virtual environments can be used to present computergenerated information derived from predictive models in the computer.people are also designing systems in which virtual and real environmentsare combined (figure 2). the use of such augmentedreality systems19overviewvirtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.is being explored in medical applications, manufacturing applications, anddriving applications (both airplanes and cars). in many such cases, informationfrom the real environment is sensed directly by means of a seethrough display,and the supplementary information from the virtual environment is overlaid onthis display. in other cases, the realenvironment information to be combinedwith the virtualenvironment information is derived by means of a teleoperatorsystem. although currently receiving less attention in the se community, it isalso possible of course to consider augmentedreality systems in which, insteadof combining input channels, output channels are combined. for example,speech sounds or commands uttered by the human operator might be combinedwith those uttered by an automatic speechsynthesis system, or physical objectsin the environment might be manipulated by systems that include both the handof the human operator and a telerobotic hand controlled by the operator. thereare certainly many tasks in which it would be extremely useful to have a thirdhand (with special features perhaps) that could work cooperatively with one'sown two hands and be controlled,figure 2 schematic outline of some augmented reality systems. one kind ofaugmentedreality system combines images obtained from direct sensing ofobjects in the environment with images generated by a computer using seethrough (or hearthrough or feelthrough) displays. a second kind combinesimages obtained by means of a telerobot with those generated by a computer.in principle, systems that combine all three channels of input informationcould be combined. also, of course, and as mentioned in the text, it is possibleto consider systems that merge output (control) information rather than, or aswell as, input (display) information.20overviewvirtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.perhaps, by simple speech commands. a further way of picturing somepossible relations between teleoperator and virtual environment systems withinan se system is illustrated in figure 3.in all of these systems, the human operator is projected into a newinteractive environment that is mediated by artificial electronic andelectromechanical devices, and in all of these systems, the operator'sperformance and subjective experience in the new environments dependstrongly on the humanmachine interface and the associated environmental (realor virtual) interactions. in general, we refer to all of these systems (teleoperatorsystems, virtual environment systems, augmentedreality systems, etc.) assynthetic environment (se) systems.in considering these different kinds of systems, it should be noted thatmany of the problems now facing designers of ve systems have been studiedpreviously in the field of telerobotics. this is the case, for example, in the areaof humanmachine interfaces. although the constraintsfigure 3 schematic outline of a further configuration of se systemcomponents. in this configuration, there are two computers, one on each sideof a communication link. if all the components on the left side of this link aredeleted and the computer on the right is used to control the telerobot, thesystem reduces to an autonomous robot. if the components to the right of thelink are deleted and the computer on the left is used to generate a virtualenvironment, the system reduces to a pure virtual environment system. if allthe components are included, but the computer on the left is not used togenerate a virtual environment, then the system reduces to a pure teleoperatorsystem (which would have supervisory control if local, lowlevel actions werecontrolled by one of the computers). if all the components are included, andthe computer on the left is used to generate a virtual environment, then thesystem becomes an augmentedreality system of the second type described inthe caption to figure 2.21overviewvirtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.on such interfaces for ve systems and for teleoperator systems are not identical,there is considerable overlap. similarly, many of the problems now facing vedesigners in the area of autonomous agents (i.e., computergenerated entitieswith programmedin behaviors that enable the entity to function without directcommands or supervision by the human operator) have been studied for manyyears by the designers of autonomous electromechanical robots.one aspect of the subjective experience in se systems that has receivedconsiderable attention is the extent to which the human operator loses his or herawareness of being present at the site of the interface and instead feels presentin the artificial environment. this feature, often referred to under the headingsof telepresence, virtual presence , or synthetic presence, is dependent on manyfactors, including the extent to which the interface is transparent and attenuatesstimulation from the immediate environment, as well as the amounts and kindsof interaction that take place in the artificial environment.the distinction between ve systems and simulator systems is more subtlethan the distinction between ve systems and teleoperator systems. also, thereis a more or less continuous transition from simulator systems to ve systems.generally speaking, the term ve system rather than simulator system isincreasingly used as the following conditions are more fully satisfied:(1) the system is easily reconfigurable by changes in software;(2) the system can be used to create highly unnatural environments as wellas a wide variety of natural ones;(3) the system is highly interactive and adaptive;(4) the system makes use of a wide variety of human sensing modalitiesand human sensorimotor systems; and(5) the user becomes highly immersed in the computersynthesizedenvironment and experiences a strong sense of presence in the artificialenvironment.it has also been suggested (breglia, personal communication, 1992) that,whereas a simulator is most intimately tied to the given physical system withwhich the user is expected to interact (i.e., it is designed to simulate thisphysical system), a ve system is most intimately tied to the human operator(i.e., it is designed to include a generalpurpose interface to match the humanorganism, as well as the capability for generating a large range of virtualworlds). accordingly, it is not surprising that a large fraction of ve equipmentconstitutes a kind of hightech clothing (headmounted displays, gloves, bodysuits, etc.). a further suggestion (allard, personal communication, 1994) is thatsimulators and ve systems differ in the extent to which the near field (i.e., theworld within the22overviewvirtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.user's reach) is real or simulated. a simulator ordinarily simulates only the farfield and uses real physical mockups for the near field, whereas ves cansimulate the near field as well as the far field.the distinction between ve systems and other types of highly flexiblecomputer systems (e.g., conventional desktop computing systems) is basedmainly on the extent to which the system is interactive, multimodal, andimmersive (items 3, 4, and 5 listed above). when focused on the visual channel,the characteristics of threedimensional rather than twodimensionalpresentations, plus a large field of view, are often cited as distinguishingcharacteristics.most current systems involve visual and auditory displays; very fewinvolve olfactory or gustatory displays (one exception is discussed inchapter 7). often the displays are presented by means of devices mounted onthe operator's head in a headmounted display system. control signals areusually derived from the human operator's motor behavioršactions of the head,hands, feet, or speech production mechanism. the use of control signals derivedfrom neural behavior (e.g., electroencephalogram signals) is still rare. in thecase of headmounted displays, the interface usually includes a system formonitoring head position, and the visual images displayed to the eyes and theauditory images displayed to the ears are modified in real time according to themeasured head position. by monitoring head position, the visual image seen bythe user can be continually modified so that, no matter how he or she moves,the objects in the virtual world remain in stable locations, just as they would inthe real world. the user is given the impression that she or he is moving aboutin a stable world even though the stable world is created artificially. (in ateleoperator system, the position of the human's head is used to control theposition of the telerobots optical and acoustic sensors and thereby the imagespresented on the displays; in a ve system, the human's head position is used tocontrol the characteristics of the synthesized visual and auditory images.)the term haptic interfaces refers to interfaces involving the human handand to manual sensing and manipulation. one common type of haptic interfacecurrently in use is a device consisting of a glove or manual exoskeleton thatmonitors hand position and posture (i.e., finger joint angles). these devices, likehead position trackers, provide no feedback and are used solely for control.other devices, such as forcereflecting joysticks, act as tool handles and servenot only a control function but also a display function because they are capableof providing force feedback. haptic interfaces in which hand position andposture are tracked and object properties such as texture and temperature aredisplayed to the hand (as well as simple force information) are not yetcommercially available.23overviewvirtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.the availability of force feedback is a powerful addition to a virtualenvironment. by sensing the position of the fingers relative to a virtual object,such as a simulated rubber ball, the system can introduce force cues as the usercloses his or her hand around the virtual object. with suitable sensors andactuators, the object can be made to feel stiff or spongy by systematicallymanipulating the characteristics of the force cues as a function of the positionand motion of the fingers relative to the position of the object. in this manner, itis possible to create haptic images of virtual objects (a further definingcharacteristic of ves).visionsin this section, we attempt to provide the reader with a glimpse of thefuture that we foresee if se systems continue to develop at the current rapidrate. to indicate the special nature of this discussion, presented in the form ofspeculative vignettes, we have used a different typography in the sections thatfollow.for specificity, we have chosen to convey this picture of the future interms of the activities of a family of two adults and two children in their home.although many of these activities clearly require advances in certaincomponents of the technology, we believe that most such advances will takeplace within the next 5 to 10 years. in cases in which there is substantialuncertainty about the achievability of a given hypothesized development, wehave indicated such uncertainty by referring to it as a research project. (perhapsthe most unrealistic aspect of the picture we paint is that of a traditional nuclearfamily at home together: current statistics indicate both a decrease in theincidence of traditional twoparent families and an increase in the incidence ofmultigenerational families.) for convenience, we have chosen to focus onactivities inside rather than outside the home, despite the fact that most of theactivities considered in our discussion of applications in chapter 12 take placeoutside the home.the envisioned family, the roberts family, includes a mother, jennifer, afather, henry, a 12yearold daughter, samantha, and a 16yearold son, peter.henry does not appear until the end of the sketch because he is not activelyinvolved in any se activity; he is suffering from ''se overdose."finally, it should be noted that in writing this section we have not hesitatedto interweave images based on assumed future technology with images basedon assumed future social and psychological phenomena. we have included thelatter not because we have any particular expertise in predicting suchphenomena, but because we believe that technology must be considered in thelight of such phenomena. it is our hope that those who follow up on this reportwill have the expertise appropriate to serious consideration of these issues.24overviewvirtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.surgical trainingjennifer roberts, the mother, is training to become a surgeon and is at herse station studying past heart operations.she previously spent many hours familiarizing herself with the structure and function of the heart by working with the virtualheart system she acquiredafter deciding to return to medical school and to specialize in heart surgery.this system includes a special virtualheart computer program obtained fromthe national medical library of physical/computational models of humanbody systems and a special haptic interface that enables her to interactmanually with the virtual heart. special scientific visualization subroutinesenable her to see, hear, and feel the heart (and its various componentsubsystems) from various vantage points and at various scales. also, the hapticinterface, which includes a special suite of surgical tool handles for use in surgical simulation (analogous to the forcefeedback controls used in advancedsimulations of flying or driving), enables her to practice various types ofsurgical operations on the heart. as part of this practice, she sometimesdeliberately deviates from the recommended surgical procedures in order toobserve the effects of such deviations. however, in order to prevent her medicalschool tutor (who has access to stored versions of these practice runs on hisown se station) from thinking that these deviations are unintentional (andtherefore that she is poor material for surgical training), she always indicates her intention to deviate at the beginning of the surgical run.her training also includes studying heart action in real humans by usingseethrough displays (augmented reality) that enables the viewer to combinenormal visual images of the subject with images of the beating heart derived (inreal time) from ultrasound scans. although there are still some minorimperfections in the performance of the subsystem used to align the two types ofvisual images, the overall system provides the user with what many years ago(in superman comics) was called xray vision. in this portion of her training,jennifer examines the effect of position, respiration, exercise, and medication on heart action using both the seethrough display and the traditional auditorydisplay of heart sounds.today, jennifer is studying recordings of a number of past real heart operations that had been recorded at the master surgical center in baltimore.in all of these operations, the surgery was performed by means of a surgicalteleoperator system. such systems not only enable remote surgery to beperformed, but also increase surgical precision (e.g., elimination of handtremor) and decrease need for immobilization of the heart during surgery (thesurgical telerobot is designed to track the motion of the heart and to move thescalpel along with the heart in25overviewvirtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.such a way that the relative position of the scalpel and the target can beprecisely controlled even when the heart is beating).the human operator of these surgical teleoperator systems generally hasaccess not only to realtime visual images of the heart via the teleroboticcameras employed in the system, but also to augmentedreality informationderived from other forms of sensing and overlaid on the real images. some ofthese other images, like the ultrasound image mentioned above, are derived inreal time; others summarize information obtained at previous times andcontribute to the surgeon's awareness of the patient's heart history.all the operations performed with such telerobotic surgery systems arerecorded and stored using visual, auditory, and mechanical recording andstorage systems. these operations can then be replayed at any time (and theoperation felt as well as seen and heard) by any individual, such as jennifer,who has the appropriate replay equipment available. recordings are generallylabeled "master," "ordinary," and "botched,'' according to the quality of theoperation performed. as one might expect, the american medical associationinitially objected to the recording of operations; however, they agreed to itwhen a system was developed that guaranteed anonymity of the surgeon and thesupreme court ruled that patients and insurance companies would not have access to the information. this particular evening, jennifer is examining twomaster doublebypass operations and one botched triplebypass operation.during her training time on the following day, she is going to monitor aheart operation in real time being performed by a surgeon at the mastersurgical center in baltimore on a patient in a rural area of maryland roughly200 miles away. although substantial advances have been made in combattingproblems of transport delay in remote surgery (by means of new supervisorycontrol techniques), very few heart operations are being conducted remotely atranges over 500 miles.in addition to spending time on her basic surgical training, jennifer isparticipating in a research project being conducted at the center that isconcerned with the use of microtelerobots for the diagnosis and treatment ofcirculatory disorders. microtelerobots with dimensions less than 1 nm arebeing designed to enter the circulatory system, make measurements of variouscirculatory parameters at various locations within the system, and then performlocal microsurgery under remote supervisory control.shoppingsamantha roberts, the 12yearold daughter, is spending the early hoursof the evening shopping for a dress via her se station.26overviewvirtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.a week ago she underwent the periodic body scan essential for individuals whose measurements are rapidly changing. this scan was performed using thevideo camera associated with the se station, a standard bodyscan programprovided by the shopping network to which the family had subscribed, and aspecial body stocking with grid lines to facilitate the automatic measurementprocess.dress material (design and fabric) is selected through the use of aninteractive program in which a sequence of samples is displayed to samantha(visually and tactually); she responds to each element in the sequence by ratingthe material on a scale of 1 to 10. the presentation sequence is adaptive in thesense that the choice of material to be presented on step n in the sequence isbased on the presentations and responses for steps 1 through n œ 1. all suchsequences are stored in the samantha file of the shopping system to provide background for future marketing efforts. the universe of patterns and fabricsconsidered is defined by the currently available manufacturing techniques. thenotion of inventory is no longer relevant because all clothing is manufacturedto design specifications of individual clients after the specifications aredetermined. the subjective ratings supplied by samantha to guide thepresentation sequence are based on feeling the fabric by means of a specialtactile display as well as seeing it by means of the standard visual display. thetactile display, an experimental component of the special se clothing package sold by the shopping network, consists of a rectangular array of microactuatorsthat allow one to feel different fabrics and textures by stroking the array withthe hand.after a tentative decision is made on the material to be used, a similarprocess is employed to pick out a style. in this case, the ratings are based solelyon visual displaysštactile images are irrelevant.given a tentative choice of fabric and style, the next step involves virtualmodeling of the dress by samantha herself. given the record of samantha'sphysical measurements and images in the shopping network's file, the systemnow synthesizes visual images of samantha wearing the dress she picked out byher sequence of rating responses. moreover, the actions of the synthesizedsamantha model are controllable by samantha herself by means of theshopping network's clothingmodel interface (again supplied by the shoppingnetwork as part of the special shopping package sold to the family). after amodest amount of practice with the interface, samantha is able to cause herimage to perform routines similar to those she has seen professional models perform in conventional fashion shows. initially, the shopping network's synthesis program was intentionally distorted to make the client's image appearmore like his or her ideal image (derived from a "gettoknowyou" programincluded in the initial package) than it actually does; however,27overviewvirtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.a special regulatory rule was introduced to control such distortions. in thefuture, samantha may also be able to "feel" the fit of each dress. the industryrealizes the importance of the sense of fit and has initiated an intensive, longrange research effort to develop the complex tactile displays required.the cost of each of the virtual dresses considered by samantha is presented to her as soon as the fabric and style are selected. occasionally, samantha scans through the fabric/style/cost matrix of the dresses she isconsidering in order to refresh her memory about these dresses.by paying a special fee, it is possible for the shopper to inspect the files ofother shoppers on the system. in particular, when considering the purchase of aspecific dress, it is possible to call up a file that provides information on all theother shoppers in the network who purchased a similar dress. (a substantialfraction of the fees collected in this manner are paid out to the shoppers on thenetwork in order to entice them to grant permission for such file inspection byother shoppers.)once a final decision is made as to the dress to be purchased, the shopper's decision is communicated to the manufacturing component of theshopping network; the selected "marketing design" is mapped into theappropriate "manufacturing design" and then manufactured in the shoppingnetwork's programmable factoryša demandactivated, computercontrolled,manufacturing system in which marketing and sales are fully integrated withdesign and production. simultaneously, the funds for purchasing the dress arededucted from the shopper's account in the shopping network bank. in mostcases, the dress is delivered within three days of its selection.no returns of merchandise are permitted until the subscriber has spent athreshold amount of funds via the network; thereafter, returns are permitted,but the cost of such returns to the shopping network is factored into the cost ofthe shopping service.high school educationpeter roberts, the 16yearold son, is doing what previously was calledschool homework. the distinction between doing school work at school anddoing homework for school at home has become very muddy; in both cases,much of the time is spent interacting with teachers, other students, and virtualworlds via networks of se stations. in addition, along with the deemphasis ofschool as a geographic location, the distinction between school and notschoolhas diminished. the defining characteristics of students' experiences are thenetwork to which they belong and the network courses or projects in which theybecome involved. among the major consequences of these fundamental changesare adaptive time schedules28overviewvirtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.to accommodate collaboration among individuals who live in different timezones across the world and the inclusion of children and teachers who arehomebound because of severe physical disabilities.currently, peter is participating in four network courses: mathematics, environmental science, empathy, and dancemusicdance.mathematicsin the mathematics course being taken by peter, the se facilities are usedto provide course participants with an intuitive understanding of various noneuclidean geometries. participants in the course enter into virtual worlds inwhich the properties of the space are determined by the axioms of the particulargeometry being studied. these properties are explored not only by virtuallytraveling through the space, but also by building virtual towers, bridges, andhouses within the space. the effects of changing the axioms of the geometry,which the students are encouraged to explore, are immediately realized in termsof the virtual world structure. when the changes lead to axiomatic systems thatare internally inconsistent, the space "blows up" and a tombstone appears withan inscription describing the inconsistency.in the special term project peter has selected (each student is required toteach some aspect of mathematics to younger children), he is designing amethod for showing younger children how speed is represented by the slope ofthe distanceversustime graph (all in euclidean space). peter's basic idea is toconstruct a virtual train that will move across a virtual horizontal track atvariable speed and to present associated graphs in virtual graph space. thefirst graph will plot the distance the train goes from its initial position in thetrain station as a function of time, and the second the instantaneous speed as afunction of time. in addition to augmenting the graphs with a conventionalclock face icon, peter plans to display the tangent to the first course at thecurrent time on the first graph continuously as the curve evolves over time,using the same color for plotting this tangent line on the first graph and forplotting the speed curve on the second graph.environmental sciencein this course, peter is participating in three projects, each of which is ledby a professional meteorologist in indiana who is donating four hours per weekto the course via the network. the first project focuses on the gathering ofinformation on atmospheric conditions around the world by means ofatmospheric measurement kits located in the homes of all the students takingthe network course around the world, entering this information into the networkcomputer assigned to the course, and then29overviewvirtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.studying the atmospheric condition displays generated by the system. inasmuchas each measurement kit records not only temperature, pressure, and humidity,but also certain molecular constituents of the sampled air, there are manyparameters that have to be represented in the display. some of the courseparticipants are comparing current conditions with those predicted by themodel developed by the meteorologist in indiana. others, including peter, areworking on improved methods for displaying and interacting with the empiricalinformation, the model's predictions, and the deviations between the two.in the second project, which the meteorologist introduced by explaining the concept of a microclimate, peter and his student collaborators are studyinga hypothetical environmental accident in birmingham, alabama. the specificquestion being addressed by the students on this day is the following: if ahypothetical accident released 8,000 kg of chlorine gas into the air at thevulcan tower in birmingham at 9:00 a.m. today, what portions of the cityshould be evacuated? a geographic profile of birmingham's topography wasmade available to the network course, and virtual sensors indicating currenttemperature, precipitation, barometric pressure, and wind velocity weredistributed throughout virtual birmingham. based on the readings from thesesensors and information provided by the meteorologist on how chlorine gas dissipates by bonding with various materials, estimates are being made of thechlorinecontent contours associated with the chlorine gas cloud as a functionof time. these estimates, combined with information on human tolerance tochlorine gas and on the capacity of various transportation facilities inbirmingham, are being used to construct evacuation plans.in the third project, the students are learning about the formation andbehavior of tornadoes. the meteorologist provided a computational model oftornadoes to the network school and the students are learning about tornadoesby virtually locating themselves at different positions in the tornado and alsoobserving how the velocity vector at different points of the tornado varies as afunction of the values assigned to the parameters of the computational model.production of the visual images seen by the students results from a blending ofvisual images generated by the computational model and images derived fromvideo recordings of real tornadoes. appreciation of the forces associated withthe tornado is facilitated by providing the students with an ability to placedifferent virtual objects (people, cars, houses, etc.) in the path of the tornadoand observing the effects of the tornado on these objects.empathythis course was developed by a multidisciplinary team of physical scientists, biologists, anthropologists, sociologists, and psychologists.30overviewvirtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.the goal of the course is to familiarize the network students not only withthe behavior of different kinds of people, animals, and physical system, but alsowith how it feels to participate in these other worlds. many of the techniquesused in this course are refinements of techniques previously developed inconnection with interactive virtual environment theater, and many of thesupporting personnel for the course are college students participating invarious types of internship programs.each student visiting a virtual world is assigned a virtual actor in thisworld and must learn to control this actor in a manner that satisfies theconstraints designed into the specific scenario considered. in general, theseconstraints are used to give the participants experience in living in differentphysical environments (e.g., in the desert, in the arctic, on the moon); indifferent social or anthropological settings (e.g., as a member of an ancientculture, a highly discriminatedagainst minority, a person with severe physicaldisabilities; or even as a member of a different animal species (e.g., as amember of an insect society or as a seadwelling creature low down on the foodchain). the role assignments are typically a month in duration and students areexpected to refine their understanding of the assigned role to the point at whichother observers cannot distinguish the behavior of the characters controlled bythe students from the "natives" (played by trained personnel or by highlydeveloped computercontrolled autonomous agents).in order to make the simulations employed in this course practical for realtime use by the students, they are greatly simplified. in most cases, such asthose concerned with other physical environments and other animal species,these simplifications are readily accepted. however, in some cases, particularlythose that focus on human social issues, such simplifications have occasionallybeen seen as offensive stereotypes and strongly resented. thus, these portions ofthe courses have become very controversial and have led to considerableturmoil.dancemusicdance!dancemusicdance is a new course introduced by a research associate inthe arts department of the network. she is a professional dancer who hasbecome highly skilled in the use of the human body as a musical instrument andin composing musical compositions by means of dance routines. her main workduring the past year had been concerned with evaluating different mappingsfrom the outputs of the body tracker she has been using (an optical system thatdoes not encumber her body or interfere with her dance movements) into thecontrol parameters used for generating sounds via the computermusic systemat her disposal. recently, she has become interested in the relations between the dance31overviewvirtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.routines used to generate the music and the dance routines the resultant musicinspires in other dancers listening to the music. similarly, she is studying therelations between the initial music generated by her own dance routines and thesecondary music generated by the other dancers. in the network course she hasconstructed, students select dancetomusic mappings to be used, choreographtheir own dance routines and thereby compose their own music, arrange for the other students to dance to this music, and then do an analytic study of the abovedescribed relationships. peter, who believes himself to be rather clumsy and israther inhibited about performing a full freebody dance, choreographed hisinitial dance piece using only the index finger of his right hand.network telemeetingslater in the evening, jennifer, samantha, and peter participate in theweekly network telemeeting. the main focus for this particular meeting is adiscussion with the network candidate for congress. except for a few minorfunctions, the members of congress now represent networks rather thangeographic regions such as states. whereas the last portion of the meeting isintended to accommodate freeranging discussion, the first portion is structuredto cover four specific sets of issues.the first set concerns the cost structure of network participation, the extentto which members of different networks are becoming isolated from each otherand incapable of communicating across network boundaries, and the increasingproblem of "ghetto networks."the second set of issues concerns the rapid rise of network gambling. theamounts of money involved in this activity now exceed that involved in medicalcare and education combined, and gambling taxes now exceed income taxes.the principal issue of concern is how the tax money collected should be splitbetween the federal government and the network.the third set of issues focuses on the creation of appropriate laws forgoverning behavior within virtual environments. the number of cases in whichve crimes and misdemeanors are occurring is increasing, and no significantbody of law is available to handle these cases. also, problems are arising thatinvolve crossing the ve boundary, i.e., unlawful acts being committed in thereal world in response to injuries suffered in ves. thus, for example, when avirtual pet salamander who was being guided by a man's son was deliberatelystepped on by a virtual actor being guided by a stranger on the network, theman located the home of the stranger and went over to his home and shot him.the fourth set of issues concerns the use of the "information highway" andse stations for purposes of lovemaking at a distance. although the use of thesefacilities for this purpose was clearly predictable32overviewvirtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.(e.g., based on the use of the mail for transmitting love letters, the use of thetelephone for vocal lovemaking, and the use of interactive video for includingvisual images), it nevertheless is the center of considerable controversy. ofparticular concern is the commercial introduction of special devices thatcontain sensors and actuators to facilitate tactual lovemaking at a distance("telesex kits"). the inclusion of the tactual channel, and the associatedincreased blurring between "direct sex" and "telesex," has caused a number ofattitudinal tolerance thresholds to be exceeded. for example, in addition to theusual strong feelings evidenced by different groups in society about who shouldhave what kinds of sexual relations with whom under what circumstances, andabout which kinds of commercial exploitation of sex should be allowed andwhich kinds disallowed, the legal community is now concerned about having tohandle cases in which sexual relations are conducted across the boundaries of states with different laws governing sexual behavior. similarly, on the basis of astudy in france that documented a decrease in sexually transmitted diseasesassociated with the increased use of telesex, the center for the control ofcommunicable diseases is now considering adding the use of telesex kits to theuse of condoms as an important means for controlling the stillexplodingincidence of the aids virus.two kinds of facilities are available for participating in these networktelemeetings. with the first kind, each individual in the family wears a headmounted display and works through his or her own se station; in the second,rather than using the traditional headmounted displays, the family sits togetherin a special room outfitted with a wallsized visual display, a set of acousticalloudspeakers, and a set of video cameras. each individual is assigned one ofthe video cameras, which then tracks that individual as he or she moves aroundthe room. the roberts family purchased both kinds of facilities because theybelieved that neither one alone is adequate for all purposes.se overdosehenry roberts, the father, is not participating in the telemeeting becausehe is suffering from se overdose. although problems associated withconventional simulator sickness were brought under control years ago, a set ofdeeper problems emerged as individuals began to spend substantial portions oftheir waking hours living in synthetic environments.one aspect of these problems is evidenced in the choices henry makes about how to spend his leisure time. initially, he spent much of his leisure timeplaying se games, taking se trips to foreign lands and planets (both real andimaginary), participating in interactive se theater, and exploring the worldfrom the viewpoint of different types of33overviewvirtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.creatures (again, real and imaginary). one of his favorite activities had been tointeract with real bees inside a real beehive using a telerobotic model beesystem. (a whole variety of such telerobotic model animal systems wasdeveloped in connection with scientific study of animal behavior at thetinbergenlorenz institute in munich.) now, however, he wants to get awayfrom all this "electromechanically mediated stuff" and interact with the worlddirectly. accordingly, he spent his last three vacations in the surroundingmountains camping out with some friends who are having similar problems. airquality has improved substantially with the introduction of se systems because of the associated reduction in auto travel. the fact that henry's desire to getback to nature is rather common is evidenced by the enormous growth takingplace in the camping equipment industry.henry has also increased the amount of time he spent exercising in thereal world. when he first acquired his se station, he made extensive use of sejogging (which involved the use of a sixdegreesoffreedom treadmill andsynthetic scenery) and se golf; however, as part of his reaction to too much se,he has switched back to the "real thing." also, he refuses to join the politicalmovement concerned with the large amounts of energy being wasted by theexercise mania that has swept the country and with finding a practical schemefor capturing, storing, and making use of this energy.henry is also undergoing therapy in connection with his se experiences. although many aspects of these experiences seem disturbing to henry and histherapist, one aspect of central importance concerns henry's body image, hissense of presence, and his underlying identity. apparently, the ease with whichhenry is able to transform himself into other creatures in other environments,and become realistically immersed in these other roles and other worlds, isbecoming a real psychological problem for him.ordinarily, the therapist administers treatment to his patients via an senetwork that incorporates a biofeedback mode. however, in cases such ashenry's (loss of presence and loss of identity due to se overdose), the use of anse system in the therapeutic procedure seems unwise.another factor of critical importance to henry's mental state concerns thearticles he has been reading about research on humanmachine interfaces thatare designed to tap directly into the human's neural system. although he fullyunderstands the advantages of such interfaces for individuals with severesensorimotor disabilities, the idea of them makes him uneasy.henry's job as an architect adds an additional important dimension to hismental state. initially, se played only a supporting role in his work; it was usedmerely as a tool for design or as a tool for marketing to34overviewvirtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.the client. recently, however, the company for which henry works receivedsome large contracts to design virtual spaces for use in virtual worlds.apparently, the large amounts of time now being spent in virtual spaces,combined with the limitations of computer scientists in their abilities to designvirtual spaces that are not only functional but also aesthetically pleasing, areleading to a new market for architectural firms. however, this new market is ofno interest to henry; in fact, it increases his desire to switch fields.unfortunately, when he scans the job opening information available to him onhis se network, he finds that the most common type of opening involving interaction with the real world concerns the installation and maintenance of sesystems.current state of the se fieldalthough some of the technologies assumed in our visions of the future arealready available and others are the subject of current research, these visions arewithout doubt visions of the future, not the present. in this section, we brieflydepict the current state of the se field. we begin by describing the applicationareas that are currently receiving the most attention. we then discuss a numberof topics in the field of psychology relevant to the design, use, and evaluation ofse systems and the human component of these systems. next we summarizethe status of the associated technologies that make ses possible: the interfacesused to link the machine and the human operator, the computer hardware andsoftware used to generate ves, the telerobots used in teleoperator systems, andthe communication networks used to integrate multiple se systems. the sectionends with a brief assessment of the se evaluation efforts to date. more detailedinformation on most of these topics can be found in the chapters of the report.application domains of se systemsthe range of potential applications for se systems is extremely large.application domains currently receiving considerable attention include: (1)entertainment, (2) national defense, (3) design, manufacturing, and marketing,(4) medicine and health care, (5) hazardous operations, (6) training, (7)education, (8) information visualization, and (9) telecommunication andteletravel.the entertainment domain is serving both as a massive informal test bedand as a major economic driving force for the development of new vetechnology. although some of this technology can be expensive (particularlythat associated with the entertainment of large groups), on the whole the vetechnology associated with the entertainment industry is "low end." forexample, the headmounted displays being used for entertainment35overviewvirtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.purposes arešas they would have to be to make the enterprise commerciallyviablešorders of magnitude less expensive (and correspondingly less capable)than those being developed for military purposes. even though applications inthe entertainment domain are still in their infancy, they are by far the mostwidely implemented of all ve applications. in essentially all of the otherdomains, the activities are in the stage of research and development rather thancommercial application or practical use. not only is there much to be learnedabout how best to utilize se technology, but also the costeffectiveness of mostcurrent se technology (i.e., the bang for the buck) is inadequate for anyapplication domain other than that of entertainment.the national defense domain, like the entertainment domain, constitutesboth a major test bed and a major driving force for ve technology. it differs inthat (with the exception of the use of traditional simulator systems) research anddevelopment activities still dominate, the associated technology tends to be highend rather than low end, the systems of interest include teleoperators as well asves, and the networking of large numbers of active participants is emphasized.this report discusses neither the entertainment domain nor the nationaldefense domain in detail. the former domain is omitted because it is alreadyreceiving extensive commercial support, many of the scientific and technicalresearch issues that arise in this domain also arise in other domains, andimproved entertainment technology does not appear to us as one of society'smost pressing needs. the latter domain is omitted because it is currentlyreceiving substantial attention within the government (e.g., thorpe, 1993),significant information may be classified for security reasons and thereforeinaccessible, and, again, many of the research issues that arise in this domainalso arise in others. this last reason is especially relevant to the national defensedomain because so many of the other domains considered, such as training,information visualization, and telecommunication and teletravel, are directlyrelevant to national defense.finally, although not included here as a formal application domain, vesystems are beginning to be envisioned as highly desirable facilities for researchgroups concerned with experimental psychology. clearly, not only isknowledge and understanding of psychological phenomena essential forefficient design and productive use of ve systems, but also a highquality vesystem that makes available a wide variety of precisely controlled stimuli,response measurements, and adaptive testing procedures constitutes an idealtool for conducting research in experimental psychology.in the following subsections, we discuss briefly the other applicationdomains listed above. as indicated in these subsections, significant research36overviewvirtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.and development is taking place in a wide variety of applications and, in a fewcases, the results of these efforts are beginning to be applied on at least anexperimental basis. it is not yet clear, however, how to choose the tasks thatwill eventually prove most appropriate for the application of se technology.not only are the results obtained in the various application domains still toomeager to allow one to specify the nature of such tasks from empirical data, butalso there is no evidence that much effort has been given to answering thequestion ''what is se technology good for?" theoretically.individuals with computer graphics backgrounds usually point to tasksinvolving threedimensional spatial information and to immersion in threedimensional space; those focused on multimodal interactive interfaces oftenpoint to tasks that depend strongly on sensorimotor involvement. in any case, inorder to fully specify what se is good for, one must estimate the costeffectiveness of the envisioned se system both compared with the way in whichthe same task is now being performed and compared with alternative newsystems that could be developed (e.g., that might achieve equivalent taskperformance at substantially reduced cost). eventually, of course, in addition tocomparative costeffective estimates to help select tasks for which se systemsare likely to be appropriate, one must evaluate such systems once they aredeveloped. the important and often neglected topic of se evaluation isconsidered further in chapter 11.design, manufacturing, and marketingdesign, manufacturing, and marketing are generally recognized as a majorapplication domain for se technology, and it is currently receiving substantialattention. although much of the activity in this domain is still in thedevelopment phase, it is clearly in the process of moving to actual usage. theprocedures and technologies used for design are progressing from thoseassociated with conventional computerassisted operations to those involvingve and augmentedreality systems. similarly, we are beginning to see at leastexperimental use of ve in the marketing of products and services. it appearsthat it will not be many years before design, manufacturing, and marketing willall take place within a unified system that makes substantial use of setechnology.independent of whether the item to be sold is a haircut, a kitchen, or anoffice building, the ability of the client to see and interact with realisticrepresentations of a variety of possible versions or realizations of the item canpositively influence both the evolution of the design and the attitude of theclient. furthermore, when very complex and expensive systems, such as anaircraft or submarine, are being designed, the potential for cost37overviewvirtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.savings by using virtual mockups and prototypes rather than real physical onesis enormous.medicine and health caremedicine and health care, like design, manufacturing, and marketing, areconsidered to be a major se application domain. although much of the work isstill at the experimental stage, applications of both ve technology andteleoperator technology are being pursued very actively.in addition to developing improved communication networks for providingthe right medical information to the right place at the right time, much of thecurrent research is directed toward improved methods for diagnosis; planning oftreatment; provision of information to the patient; provision of treatment; andtraining of medical personnel. ve systems are being developed and studiedexperimentally to extend conventional consultations and telediagnosisperformed over the telephone to include interactive visual displays of bothparticipants and medical information. such systems are also being studied foruse in planning surgical procedures and in helping to increase patients'awareness and understanding of these procedures and of the possible outcomes.augmentedreality systems are being studied to present visual displays in whichinformation previously obtained from special imaging techniques is overlaid onthe normal direct view of the patient; integrated ve and teleoperator systemsare being developed for use in telediagnosis and telesurgery and for the trainingof surgeons. in general, the potential benefits of telemedicine that are beingconsidered include not only the ability to obtain medical information andperform medical actions at a distance, but also the ability, as in any otherapplication of teleoperation, to effectively transform the sensorimotor system ofthe operator to better match the task at hand. the rapidly increasing use oflaparoscopic surgical procedures illustrates the importance of these otherbenefits.aside from the efforts required to realize technology that is adequate forthe various medical applications, substantial research is being initiated to realizeadequate physically based models of the human body (e.g., for ve training ofsurgeons). however, current success in creating virtual human skeletons,organs, and physiological subsystems constitutes only a tiny fraction of whatneeds to be achieved over the long term.additional healthrelated research and development activities in the searea are taking place in connection with physical rehabilitation. individuals withsensory or motor disabilities constitute a uniquely challenging domain forapplication of se systems with specially designed humanmachine interfaces(e.g., gestural tracking and recognition devices for individuals who have lostboth the ability to articulate speech and the38overviewvirtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.manual dexterity required to operate a keyboard). the application of setechnology to psychological rehabilitation (for example, to reduce phobicreactions) is also beginning to receive attention.hazardous operationsone of the driving forces for the creation and development of teleoperatorsystems has been the need to perform operations that are hazardous, and theapplication of se systems to this domain is certainly one of the olderapplications areas considered. thus, unlike the situation in some of the otherdomains, current activities in this domain include actual use as well as researchand development. among the specific applications in this domain that arereceiving attention are the handling of dangerous materials, operating heavymachinery, firefighting and policing, conducting military operations, andexploring the ocean floor and outer space.despite the potential benefits that can be obtained by using teleoperatorsystems in many of these areas, and despite the benefits that have already beendemonstrated in some of them (e.g., handling nuclear materials, underseaexploration) neither the government nor the public has evidenced greatenthusiasm about this domain. aside from the general lack of excitementengendered by visions of teleoperation compared with visions of virtual reality,perhaps interest in the use of teleoperator systems for hazardous operations islimited by the lack of personal experience most people have with hazardousoperations (e.g., defusing a bomb or locating and carrying to safety a child froma burning building). it is even conceivable that the use of teleoperation forhazardous operations may lack support from potential operators because it isinconsistent with a macho selfimage.although for many applications in this area further research anddevelopment is required to achieve teleoperator systems that are both reliableand costeffective, there is no evidence that such goals cannot be achieved.also, and quite apart from the use of teleoperator systems in conductinghazardous operations, substantial opportunity for the application of se systemsin this area arises in connection with the training of individuals to conducthazardous operations (with or without the use of teleoperation). as discussed inthe next subsection, the use of ve systems for training constitutes a majorapplication domain for such systems.trainingbecause most activities require at least some training, it is not surprisingthat the use of ve technology for training is a major application area39overviewvirtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.in almost all domains considered. thus, for example, it is of major interest fornational defense, medicine and health care, and hazardous operations, amongothers.on one hand, the use of simulators for training is quite extensive.simulations of various types have been used for a long time and, judging bytheir continued use, are relatively costeffective (although appropriate analyseshave rarely been performed). the apparently successful results obtained withsimulators in training various tasks (e.g., flying an airplane) constitute a majormotivation for interest in the exploitation of ve technology for training.on the other hand, the extent to which current ve technology is actuallybeing used in the training area is very limited: essentially all current work onve training is at the stage of research and development. given the existingbackground in the use of simulation for training, it is clear that one of thefactors responsible for this situation is the inadequacy of the currently availableve technology. however, that is not the only problem; others relate to ourinadequate understanding of basic psychological issues related to training andtraining transfer. the flexibility inherent in the use of ve systems for trainingand, in particular, the opportunity to create learning situations that are superiorto those that are realizable without such systems (e.g., by the use of specialmultisensory instructional cues, by purposefully distorting the real situation, byproviding multiple viewpoints and various levels of abstraction, and byadapting the system automatically to the individual and the individual's state oftraining) seriously challenge our basic understanding of the learning process. ofparticular concern is the issue of training transfer. much remains to be knownabout which of the possible differences between the real task and the task asrealized in the envisioned ve training systems are likely to be important, eitherpositively or negatively, and which are insignificant.educationalthough the term education can be used very broadly to cover almost anysituation in which learning takes place, in this report we use the term to refer tothe goals and activities normally associated with k12 education in schools.one major set of applications currently being explored in this domainfocuses on the communication component of se networks. examples includecommunication between students, between teachers, and between students andteachers at different sites; televisits to places of interest that would normallyinvolve costly travel (to explore another culture, to learn a foreign language, tovisit a site in outer space or under the ocean); and40overviewvirtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.even teleoperation of remote telerobots. other applications focus more on theuse of ves as immersive, interactive, experimental, and play facilities. at oneextreme, a ve can be used to present a welldefined situation in a highlystructured course. at another extreme, a ve system can be used to encouragefree play and various types of model building, or even the construction ofvirtual environment tools.as in the training domain, much of the current work in the educationdomain is being directed toward research to determine the ways in whichtechnology can be usefully applied.despite the potential of se technology to provide costeffectiveimprovements in k12 education, many people judge societal infrastructureproblems in education to be so overwhelming that attempts to exploit setechnology within the current education system would have only marginalbenefits. the history of attempts to introduce computers into the classroom iscited as an example of useful technology being available but not well used. ingeneral it is believed that, unless the infrastructure surrounding the educationsystem is radically changed, the best opportunity for using se technology tohelp educate children is likely to occur through the entertainment industry andthe entertainment facilities that will be available in many homes (leading to anew meaning for the phrase dual use that is now being so frequently used ingovernment circles). it is also conceivable, of course, that se technology,together with associated networking features, can play an important role inhelping to change the infrastructure.information visualizationthe dependence of our culture on information and the amount ofinformation that one needs to perceive, digest, understand, and act on aresteadily increasing. in attempts to prevent information overload or,alternatively, to prevent ignoring information that is vital to action, research isbeing conducted to determine methods of information visualization that aresuperior to those now used. (the term visualization is used here in its mostgeneral sense and only for historical purposes; we do not mean to imply that theinformation is necessarily presented only through the visual channel.) thisapplication domain, like the training domain, cuts across the other applicationdomains considered: effective visualization of information is important inessentially all domains.in general, the problem of information visualization is an extremely oldone. inventive pictorial representations of important events go back to cavepaintings. descartes' invention of analytic geometry and the associated use ofgraphs to represent tables of numbers constitutes a truly major advance in thisarea. less dramatic, but more technologically relevant41overviewvirtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.advances have taken place in the area of computer graphics. the extension oftwodimensional to threedimensional graphics and controlled manipulation ofscale and viewpoint are illustrative examples of these advances. unfortunately,there is relatively little guiding theory available, and relatively little systematicevaluation has been performed to determine the benefits of these new graphicstechniques (although they are clearly commercially successful). furthermore,the use of modalities other than vision and exploration of the benefits ofdifferent kinds of sensorimotor involvement in understanding information areonly now beginning to be seriously considered. perhaps the most advancedapplication area in this domain concerns the visualization of scientificinformation. scientific visualization is generally recognized as a major andgrowing application of ve and is starting to receive substantial attention.specialists in various fields of science are beginning to make use of advancedcomputer graphics techniques for improved visualization, and preliminaryresearch and development are being conducted on the use of the auditory andhaptic channels for this purpose.telecommunication and teletravelthe domain of telecommunication and teletravel, like training andinformation visualization, cuts across essentially all of the other domainsconsidered. telecommunication, which is intended to include teleconferencingas a special case, enables two or more people at disparate locations to interact inany manner permitted by the technology and chosen by the participants.although sophisticated telecommunication systems involving multipleparticipants and realtime video as well as audio have been envisioned for manyyears, their use is still primarily experimental. currently, only phone, facsimiletransmission, and electronic mail are being extensively used by large groups ofpeople. the incorporation of a realtime haptic channel into thetelecommunications network by means of which individuals can communicatetactually is only beginning to be considered. clearly, the ability to holdconferences with people at different locations in a manner that closelyapproximates real conferences (i.e., that permits one to see all the participantsinteracting together in a room, to focus one's attention arbitrarily on anymember of the group, to direct remarks to specific individuals in the group bydirecting one's gaze toward that individual, etc.) will require substantialadvances in the application of se ideas to the telecommunications domain.teletravel, which would allow individuals to effectively visit remotelocations for purposes of work or pleasure, also has not advanced very far; onlythe phone and noninteractive video are commonly available. the use ofteleoperation to facilitate active exploration and work at distant42overviewvirtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.locations still appears to be confined to the domain of hazardous operations andto experimental work in the domain of health care. little consideration is beinggiven to the use of teleoperation to enable individuals who are homebound (e.g.,because of physical disabilities or because they are incarcerated) to work at jobsthat are located elsewhere and that require substantial interactive sensing andmanipulation.finally, advances in the technology for telecommunications and teletravelare being accompanied by new ventures in the sexforprofit business. unlessappropriate societal strictures are imposed, it appears that interactive live audiovideo will be heavily exploited for this purpose within the next 2 to 3 years,with equivalent exploitation involving the tactual channel occurring within thenext 5 to 10 years.some psychological considerationsby definition, human operators constitute a major component of all sesystems. furthermore, the range of experiences to which the operator issubjected in these systems can be extremely broad. thus, there are very fewtopics concerning human behavior (sensorimotor performance, perception,cognition, etc.) that are not relevant to the design, use, and evaluation of sesystems. a number of the modalityspecific topics in this general area arediscussed in the section below in connection with humanmachine interfaces;some of the more general ones are considered here.one set of such topics focuses on human performance characteristics andincludes, for example, sensorimotor resolution, perceptual illusions, informationtransfer rates, and manual tracking. knowledge about all of these topics isessential to costeffective design of se systems. for example, the limits onhuman sensory resolution place an upper bound on the resolution required insensory displays. similarly, unintentional variability (noise) in motor responsesputs an upper bound on the resolution required in control devices. both sensoryinput and motor output characteristics are included in human operator modelsused to interpret performance in various types of manual tracking tasks. andinformation transfer rates help characterize the operator's ability to receiveinformation via displays, process information centrally, and transmitinformation via controls. perceptual illusions can be used to simplify (andthereby reduce the cost of) stimulus generation procedures. if not thought aboutin advance, however, they can also lead to unexpected failures in performance.a second set of such factors arises in connection with the alterations insensorimotor loops that occur when a human operator "drives" an se system,and the extent to which and the manner in which the human operator adapts tosuch alterations as a function of his or her experience43overviewvirtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.with the alteration. consider, for example, the case in which the operator isviewing a virtual image of his or her hand as his or her real hand moves throughspace. the alteration may be caused by defects in the technology and caninvolve a spatial discrepancy between the position of the seen hand and theposition of the kinesthetically sensed (felt) hand, a time delay between the feltmotion and the seen motion, or a statistical decorrelation between the seen andfelt hand positions (e.g., due to noise in the visual display channel). similarexamples are often encountered in the sensorimotor loop involving the visual(or auditory) scene presented by a headmounted display and the kinestheticallyfelt orientation of the viewer's (listener's) head being tracked by an appropriatesensor mounted on the head to control the displayed scene.although alterations involving time delays or noise generally havenegative effects and are therefore to be avoided in designing and constructingse systems, alterations involving fixed transformations or distortions may beintroduced intentionally to enhance performance. for example, such conditionsarise automatically in any teleoperator system that employs anonanthropomorphic telerobot (e.g., a telerobot that has four eyes, six arms, andmoves about on wheels). because the telerobot and human operator in suchcases are nonisomorphic, a special (unnatural) mapping must be employed torelate the sensing and motor actions performed by the telerobot to the sensingand motor actions performed by the human operator. similar issues arise whensensory substitution is used (e.g., auditory signals are used to represent forcefeedback) or when ve interfaces are designed to achieve supernormalresolution by magnification (e.g., by simulating increased distance between theeyes to achieve improved visual depth perception or increased distance betweenthe ears to achieve improved auditory localization).additional significant issues that arise in connection with such alterationsconcern the role these alterations play in eliciting the sopite syndrome (chronicfatigue, lethargy, drowsiness, nausea, etc.) and reducing the subjective sense oftelepresence, and the extent to which subjects can adapt to such alterations.unfortunately, there are as yet no adequate theoretical models for enabling sesystem designers to predict how subjects adapt to such alterations as a functionof the alteration and the kinds and magnitudes of exposure to the alteration.issues related to interactive effects among multiple alterations (e.g., a distortionplus a time delay plus some jitter in time or space) have hardly begun to beconsidered.a third set of topics of major importance in this general area concerns thedevelopment of appropriate cognitive models. in the design of systems, like vesystems, in which the goal is to alter the human operator, understanding thehuman mental processes involved in knowledge acquisition and knowledgeorganization, and the application of knowledge44overviewvirtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.to tasks, such as concept understanding, problem solving, decision making, andskill mastery, is critical. of particular importance is the organization, sequence,amount, and pace of information presented. if the information characteristics ofthe system correspond to the cognitive processing features of the individual,then the system will be more effective in facilitating training or education andenhancing task performance.cognitive scientists have been working for many years to describe theprocesses used by humans to acquire and build knowledge structures. this workhas led to a variety of hypotheses and some research results suggesting thatknowledge acquisition strategies and features of effective informationpresentation depend on a person's level of knowledge, the characteristics of thecontent area, and the type of task performance required.although significant progress has been made and strong research efforts bycognitive scientists will undoubtedly continue, a large number of questionsremain about the compatibility between types of tasks and preferred features ofinformation presentation. for example, should novices be presented withdifferent visualizations of scientific data than experts? if so, what featuresshould be different? what is the relationship between image fidelity, amount ofinformation presented, and knowledge or skill acquisition for different types oftasks? and how do we facilitate transfer from a training task to an operationaltask? these questions are not new, but efforts to date have only begun todevelop preliminary answers. in general, we do not yet understand therelationship between information presentation in an immersive se environmentand learning and performance by the se user.a fourth set of topics in this area concern what might be called cognitiveside effects. quite apart from how various features of the se influence thelearning and performance exhibited by the user with respect to the specific taskof interest, it is important to know how these features influence other aspects ofthe user's cognitive structure and behavior. for example, might not extensiveuse of certain kinds of se systems significantly decrease the user's sense ofpresence in his or her usual environment or alter the mental model held of his orher own body? at present, experience with truly immersive se systems is toolimited to provide reliable answers to such questions. another set of questionsinvolves how experiences in ses might affect an individual's attitudes towardsuch social behaviors as violence, sex, and fantasy role playing. there is at leastsome anecdotal evidence of a connection between aggressive behavior inchildren and playing violent video games. in addition, there have been severalcases of individuals who were reportedly so completely drawn into computerroleplaying games that they devoted all their time to them. it is possible thatexperience in immersive ses could have even45overviewvirtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.greater impact; however, the current experience with se systems is too limitedto allow one to draw any conclusions in this area.current state of se technologygenerally speaking, at the time of this writing, a substantial gap existsbetween the se technology that is commercially available and the setechnology that is needed to realize the potential envisioned in the variousapplication domains. even the demonstrations of what are considered advancedse research systems that can be seen at various universities, militaryinstallations, and industrial laboratories sometimes leave technicallysophisticated observers who have no vested interest in the technologyunimpressed.there are, of course, important exceptions to this relatively negativeassessment. current ve technology is certainly adequate to be used for someapplications in the entertainment domain. similarly, current teleoperatortechnology is adequate to be used for some applications in the hazardousoperations domain. also, newly developed se technology of various types(including that associated with augmented reality) is beginning to be applied inthe domains of design, manufacturing, and marketing; medicine and health care;and information visualization. nevertheless, it is clear that the development ofsignificantly improved technology is a major requirement for most seapplications to be truly successful.in the following sections, we summarize the current state of technology inthe areas of humanmachine interfaces, computer generation of ves,telerobotics, and networks. as in the body of the report, the material on humanmachine interfaces and networks has been separated from the material oncomputer generation of ves and telerobotics because it is generally applicableto both ve and teleoperator systems. furthermore, the section on humanmachine interfaces covers the visual, auditory, and haptic channels, whereas thesection on the computer generation of ves covers the visual channel only. thisasymmetry arises because the overwhelming majority of previous work oncomputer generation of ves has been restricted to the visual channel. as thefield matures and the computer science community becomes more involved inthe generation of auditory and haptic images as well as visual images and thecommunity concerned with the auditory and haptic channels becomes moreinvolved with computer synthesis of environmental signals and objects, thisimbalance will become less severe.humanmachine interfacesthe humanmachine interface in se systems consists of all devices used topresent information to the human operator and to sense the actions and46overviewvirtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.responses of the human operator that control the machine in question. althoughthe problems associated with humanmachine interfaces differ to some extentaccording to whether the system is a ve system or a teleoperator system (i.e.,whether the given machine is a computer or telerobot), there is clearlysubstantial overlap between these two sets of problems. in the followingsections, we briefly discuss interface issues for the visual channel, the auditorychannel, and the haptic channel. in addition, we consider position tracking andmapping, motion interfaces, speech communication, physiological responses,and a display system that presents information by means of odors and radiantheat.visual channel of all the devices associated with the humanmachineinterface component of ve systems, visual displays have received the greatestattention. in addition to continuing efforts directed toward the general use ofdisplays (home television, scientific research, etc.), substantial efforts have beendirected toward the development of visual displays specifically for se systems.the visual displays currently available for se use include both headmounted displays (hmds) and offhead displays (ohds). hmds, which alsoinclude devices for presenting auditory signals (earphones) and for measuringthe position and orientation of the head (head trackers), would be ideally suitedto the se field; however, they still suffer from information loss (poor resolution,limited field of view) as well as a variety of ergonomic problems, includingexcessive weight and poor fit (both mechanically and optically). they may alsocause the user to experience the sopite syndrome.in addition to hmds in which all of the visual images are computergenerated, hmds are being developed in which the computergenerated imagesare combined with directly sensed environmental images (seethrough displays)or with environmental images sensed and possibly transformed by a teleroboticoptical sensing system. to date such augmentedreality systems have not beenmuch in demand by the entertainment industry and therefore remain largely inthe research domain.lowend hmds, the development of which has been driven mainly by theentertainment industry, can be obtained for less than $10,000; highend hmds,the development of which has been supported mainly by the military, can costas much as $1 million. although the hmd area has been and continues to beextremely active, and although there is a wide range of hmds now availableand a substantial number of research projects exploring new technologies foruse in hmds, all of the hmds now available have major drawbacks. in fact,given the current drawbacks, we think it is extremely unlikely that anyindividual would choose to wear an hmd on a regular basis (e.g., 40 hr/week)without special incentives. at present, and for some years to come, ohds willprovide47overviewvirtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.significantly better performance than hmds for most tasks in most applicationdomains. one intermediate technology currently available is lightweightstereographic glasses and desktop stereo display screens. another includes adisplay mounted on a boom that can be moved about (its position andorientation appropriately tracked) manually. many other types of ohds, someof which involve tracking and some of which do not, are discussed in thechapter on visual displays.to a large extent, the design of the visual displays being used in the sefield takes little account of the dual structures found in the visual system (fovealversus peripheral vision, focal versus ambient systems, etc.). some research hasbeen done on the use of a highresolution inset and the tracking of gazedirection to locate the inset within the field so that it is always at the right placefor stimulating the fovea. however, the design and use of such systems, whichreduce the computational requirements associated with presenting continuouslyvarying complex scenes at the cost of the complexities associated withcontinuous eye tracking, are still mainly in the experimentation phase.an important set of issues concerning perceptual effects in the visualchannel that are only now beginning to be addressed concern the abovementioned augmentedreality displays. not only is relatively little known aboutthe detailed perceptual effects of misregistration (misalignment) of visualimages, but there are few guidelines available to help designers choose how tomerge real and synthesized information (a problem with a strong cognitivecomponent) even when there is no detailed registration problem.many of the perceptual issues that have important implications for thedesign of technology for the visual channel, and about which current knowledgeis inadequate, concern how humans respond to various types of sensorimotoralterations associated with the visual display. as mentioned earlier, suchalterations can result from the intentional introduction of distortions to achievesuperior performance (e.g., simulating greater interocular distance to achieveimproved depth perception, using a telerobot with a nonanthropomorphicoptical sensory system), from unintentional optical distortions in an hmd, orfrom time delays and noise generated somewhere within the visual channel.also, we still know relatively little about how various characteristics of thevisual display influence performance on various types of tasks. although somedeficiencies, like those that are likely to induce the sopite syndrome, may bemore or less task independent, other deficiencies are likely to depend stronglyon the task.auditory channel unlike the situation for the visual channel, currentlyavailable hardware for the auditory component of hmds is adequate for48overviewvirtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.essentially all se applications. earphones to present desired signals and ''eardefenders" (active or passive) to attenuate unwanted signals from the immediatereal environment are effective, inexpensive, and ergonomically reasonable.(the main ergonomic problem occurs when earphones and visual displays haveto be used together and the design of the helmet that includes the visual displaydoes not take proper account of the need to also stimulate the auditory channel.)although considered much less frequently in the context of se systems, roughlythe same conclusions apply to offhead displays and hearthrough displays.loudspeaker technology is sufficiently advanced to provide effective offheaddisplays in various types of spaces, and hearthrough displays can easily beachieved by the use of earphones with controlled acoustic leakage or by placingmicrophones in the environment and adding the synthetic signals and theenvironmental signals electronically. although earphones are preferred toloudspeakers for most applications, in some areas, such as those in which thesystem is intended to simulate battlefield sounds with sufficient energy to shakeportions of the body other than the eardrums, loudspeakers are clearly preferred.the main current inadequacies associated with the use of the auditorychannel in ve systems concern the synthesis of the signals to be presented viathe interface. one component of this problem concerns the spatialization ofsounds. despite extensive recent work on spatialization using earphones, theresults are far from perfect. particularly noteworthy is the inability of mostcurrent hmd spatialization systems to cause sound sources to be perceived aslocated in front of the listener (as opposed to behind the head, above the head,or inside the head). similarly, although the audio industry has devoted a greatdeal of attention to issues of spatialization for loudspeaker systems, the resultsare still overly sensitive to the precise location of the listener's head; relativelysmall movements away from the designated "sweet spot" can cause seriousdegradation of spatialization. the character of the spatialization responsedepends on both technology factors and perceptual factors.a second set of inadequacies involves the generation of acoustic signals.record and playback (sampling) methods suffer from the need to store anenormous number of sounds or be satisfied with crude approximations of thedesired sounds. reasonably satisfactory sound synthesis methods have beendeveloped only for speech and music; they do not yet exist for the generation ofenvironmental sounds. also, if the sounds are generated in real time rather thanahead of time and then stored, the process may consume substantial portions ofthe system's computing power.relevant perceptual issues that are being studied include those related tothe spatialization of sounds and issues similar to those already49overviewvirtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.discussed in connection with the visual channel and in the section on humanresponses to alterations in sensorimotor loops (e.g., concerning humanresponses to distortions, time delays, and noise within the channel). a furtherset of issues that are beginning to receive attention involves the use of theauditory system for sensory substitution. such substitution is being consideredboth when the visual channel is overloaded and when appropriate hapticfeedback is unavailable. another set of issues being studied, which is highlyrelevant to synthetic auditory displays (independent of whether they are beingused for sensory substitution), concerns the manner in which the auditorysystem organizes temporal sequences of acoustic signals into a coherentperception of the auditory scene (auditory scene analysis). understanding sceneanalysis in the auditory system is quite different from understanding that of thevisual system, because in the auditory system there is no peripheralrepresentation of source location (i.e., most information on source location isderived by comparing the signals received at the two ears, a process thatinvolves central processing).position tracking and mapping by position tracking is meant the realtime measurement of the pose (defined as the threedimensional position andthreedimensional orientation) of a moving object. position tracking is requiredin ves to control computergenerated stimuli and in teleoperators to control thebehavior of the telerobot. in many applications, position tracking of the head,the hand, or the fingers is crucial. in other applications, position tracking of theeyes, the torso, and the arms and legs may also be required. in suchapplications, partial pose measurement may be sufficientšfor example, threedimensional orientation just for head tracking or threedimensional position justfor hand tracking.by position mapping is meant the determination of a surface, such as thatof the body or of the environment, by measuring a dense set of threedimensional positions on that surface. position mapping is required fordetermining bodily dimensions, for recognizing facial expressions, and forenvironment mapping to create a geometrical model for simulation. inapplications such as environment mapping, realtime requirements may beabsent. when position mapping is used for body tracking, however, realtimeconstraints must be met. constraints on position mapping are likely to beexceptionally severe when augmentedreality applications are considered andregistration issues arise.knowledge of the values that can be assumed by the various motionparameters (e.g., velocity, acceleration, bandwidth) for different kinds of bodilyactions is reasonably adequate for purposes of tracker design. less adequate isour knowledge of how various deficiencies in tracker technology degradeperformance or contribute to the sopite syndrome in various types of tasks.50overviewvirtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.currently, there are four basic technologies for position tracking andmapping in se work: mechanical linkages, magnetic sensors, optical sensors,and acoustic sensors. se systems are likely to include a mix of such systems,because each type of system has particular strengths and weaknesses and therequirements depend on the particular application. although none of the inertialtrackers currently available is adequate for se applications, research is nowunder way to develop such trackers. as for many other kinds of devices,commercial specifications of position trackers and mappers are not reliable orconsistent.mechanical trackers are relatively inexpensive, have very small intrinsiclatencies, and can be reasonably accurate. yet bodybased linkage devices(called goniometers) may be cumbersome, whereas groundbased linkagedevices (e.g., hand controllers) suffer from workspace limitations. the use ofgoniometers involves problems of fit and measurement related to alignmentwith joints, rigidity of attachments, calibration of linkages mounted on humanlimbs, and variations among individuals. the use of groundbased linkagedevices involves the difficulty of tracking multiple limb segments and limbredundancies. hybrid systems, in which bodybased and groundbased devicesare combined, are also likely to be required for some applications (e.g., both totrack finger motion and to provide force feedback to the hand without causingforces to be applied to other portions of the body).magnetic trackers are commonly used because of their convenience, lowcost, reasonable accuracy, and lack of obscuration problems. significant currentdisadvantages that limit their usefulness include modest accuracy, short range,high latency (2030 ms), and susceptibility to magnetic interference.optical sensing is one of the most convenient methods to use for certainkinds of tracking and is capable of providing accuracies and sampling rates thatmeet many ve requirements. the main drawbacks include visibility constraintsand especially high costs.acoustic trackers are very attractive for ve because the costs are relativelymodest and the accuracies and sampling rates are often sufficient. efforts arebeing made to improve accuracies by taking into account atmospheric effectsand by using echo rejection.inertial trackers, despite having played a distinguished role in the field oflongrange navigation, have received little attention in the se area. their uniqueadvantage is that they are unconstrained by range limitations, interference, andobscuration; also, latencies are low. further reductions in sensor size and costare needed to make inertial trackers a convenient and economical alternative toother trackers.an ideal eye tracker would satisfy three requirements: linear response overa large range (roughly 50 deg), high bandwidth (1 khz), and51overviewvirtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.tolerance to relative motion of the head. although many eyetracking devicesare available, none of them satisfies all three requirements.haptic channel the haptic channel differs from the visual and auditorychannels in two major ways. first, it involves manipulation as well as sensing.second, it has received less attention than the other two channels with respect toboth basic science and device development. one reason for the relativelybackward state of the haptics field appears to be the intrinsic difficulties instudying haptics associated with the complexity of combining sensory functionswith manipulative functions and with the use of electromechanical systems (forexample, the control and measurement of the effective stimulus in haptics hasalways been difficult). another reason is the lack of a recognized societal needto develop the haptics field. whereas research and development related to thevisual and auditory channels has been strongly driven by both medical needsand entertainment considerations, hapticsrelated research and development hashad no such support. until very recently, the main support for research anddevelopment on haptic interfaces has come from the field of telerobotics and, asindicated previously, this field has had limited support. it should be noted,however, that the introduction of the relatively simple haptic interfaces knownas mice is having a major effect on the interest in such interfaces by computerscientists as well as those concerned primarily with humanmachine interfaceissues.the most useful currently available haptic interfaces fall into one of twocategories: (1) bodybased gloves or exoskeletons that track the position andposture of the hand (as discussed in position tracking) and (2) groundbaseddevices, such as "joysticks," that both sense certain actions of the hand andprovide force feedback. whereas many of the latter devices have beendeveloped in connection with humanmachine interfaces for teleoperators andhave a relatively long history, many of the former devices have been developedrecently with ve systems in mind. although exoskeletons that provide forcefeedback have been developed for research and development purposes, theytend to be both cumbersome and expensive and are not in widespread use evenexperimentally. at present, one of the main experimental thrusts is directedtoward the development of toolhandle systems, in which the human usermanipulates a real tool handle, the actions of the tool handle are used to controlsome feature of a telerobot or a ve, and force feedback is displayed through thetool handle according to interactions of the telerobot with the real environmentor of the virtual tool with virtual objects in the ve.relatively little work, even at the research and development level, hasbeen directed toward haptic interface devices that provide feedback (related toperception of texture or temperature) of kinds other than52overviewvirtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.simple force feedback, and no such device is yet available commercially. toconvey more detailed information through the skin, tactile displays of the typeused to convey visual or auditory information to individuals who cannot see orhear need to be considered. although a variety of such displays has beendeveloped (e.g., involving vibratory or electrocutaneous arrays), none has beensuccessfully incorporated into se systems.current research associated with the development of haptic interfacesinvolves investigation of human haptics, development of technology, andoptimizing the interactions between the two. basic research on human hapticsincludes biomechanical studies of the hand and psychophysical studies of thesensorimotor and cognitive systems associated with the hand. illustrative issuesof particular concern in this research include determination of the mechanicalproperties of the soft tissues in contact with haptic interfaces; quantification oflimits on human sensing and control of contact forces and hand displacements;identification of stimulus cues in the perception of contact conditions and objectproperties; and characterization of human sensorimotor performance in thepresence of time delays, distortions, and noise. basic research on technologyincludes development of novel technologies for sensor and actuator hardware;design of computer architectures for fast computation of physical models; anddevelopment of algorithms for realtime control of devices that render tactualimages.among the more applied topics that are beginning to be studied are thosethat are related to the inclusion of tactual images in multimodal ves. theseinclude the design of highperformance haptic interfaces with appropriatesensors, actuators, linkages, and control as well as their evaluation in uni ormultimodal ves. the interfaces may be ground or bodybased and may or maynot include force reflection or tactile displays. avoidance of mechanicalinstabilities and false cues in contact tasks requires capabilities that are at thelimits of current technology in the areas of range, resolution, and bandwidth offorces and displacements. evaluation of the effectiveness of the interfaces iscritical to the design of improved versions. systematic studies to evaluate thehuman user's comfort in operating the devices and to investigate multimodaldisplay methods for achieving optimal task performance and telepresence(immersion) are barely at the planning stages at this point.motion interfaces in the real world, many kinds of motion occur, includingwholebody passive motion (passive transport), wholebody active motion(locomotion), and partbody active and passive motion (e.g., when an arm ismoved passively or actively). also, in many cases such motion is accompaniedby a wide variety of stimuli in a wide variety of sensory channels: motion cuesmay be contained in signals from the vestibular53overviewvirtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.system, the motor system, the visual and auditory systems, and theproprioceptive/kinesthetic and tactile systems. it is no great surprise, therefore,that, in addition to the existence of many types of motion and many types ofmotion cues, there are many ways in which motion can be simulated and manytypes of motion interfaces.currently, motion interfaces for passive transport are being used primarilyin flight simulation for flight training, in the entertainment industry for "thrillrides," and in research projects directed toward improved understanding ofhuman perception and performance (including motion sickness) in a widevariety of contexts involving real or simulated passive transport.motion interfaces for passive transport can be divided into two categories:inertial displays, in which body mass is actually moved, and noninertialdisplays, in which motion is simulated without moving body mass but bystimulating various sensory channels in appropriate ways. often, inertial andnoninertial techniques are used in combination.with inertial displays, patterns of force vectors are applied to the body thatapproximate to varying degrees of completeness and accuracy the patterns thatwould be present in the real situation being simulated. such displays aregenerated by the use of centrifuges, rotating rooms, motion bases, tiltingplatforms, spinning chairs, etc. in some sense, gseats, in which the seat andback can be inflated or deflated as well as vibrated, also fall into this category.with noninertial displays, the body remains stationary, but patterns ofstimuli are presented that are usually associated with movement of the bodythrough the environment. the most obvious example in this category, one thathas been studied in some detail and frequently applied, involves altering thevisual scene in a manner that corresponds to the changes that would occur if thebody moved through the given environment (e.g., in a car or plane). similarresults can be achieved using the auditory channel; however, auditoryinducedselfmovement has been studied less and applied less and, as one would expectbecause of the poor resolution, the results appear to be less dramatic. othertechniques for inducing the perception of selfmotion include stimulating thevestibular system by changing temperature (caloric irrigation) or applyingelectrical currents (galvanic stimulation); stimulating the cutaneous system bysliding surfaces over the skin (e.g., beneath the soles of the feet) or bystimulating muscle spindles by vibrating muscle tendons. however, none ofthese techniques, with the possible exception of the one involving cutaneousstimulation, appears to be practical for use in se applications.motion interfaces for active transport (i.e., locomotion) permit the user toexperience active sensations of walking, running, climbing, etc., whileremaining within a constrained volume of space. the best known54overviewvirtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.and most widely used interfaces of this type are the common linear, onedimensional treadmill and the stairclimbing exercise machine. the idealsystem would incorporate a sixdegreesoffreedom platform (shoe) for eachfoot with position and force sensors and force feedback. although a number ofsystems that are considered more advanced than the common treadmills andexercise machines are beginning to be developed and evaluated by researchgroups, no such systems are yet available commercially.current research in this general area, apart from work associated withattempts to develop improved technology, is focused on evaluating the differentmethods of movement simulation, with respect to both achieving the desiredsensation of movement and minimizing the extent to which the simulationresults in motion sickness (or, more generally, the sopite syndrome).other types of interfaces the above discussion by no means covers all theinterface communication channels of interest. for example, nothing has beensaid about the olfactory (smell) or gustatory (taste) channels, or about interfacesrelated to the sensing of heat, wind, and humidity. apart from influencing thegeneral sense of presence and immersion in various types of environments, suchsensations might be of specific use for conveying core information in somemajor application areasšfor example, olfactory information in trainingfirefighters or medical personnel concerned with lowtech diagnosticprocedures. furthermore, the technological problems in creating an olfactoryinterface do not appear overwhelming. not only have odorreleasing systemsfor use in theaters and odor (scratch) records for novelty home use beenavailable for many years, but also significant current research is now beingconducted in this area.perhaps the two most important methods for interfacing that have not yetbeen discussed concern speech communication (automatic speech recognitionand speech synthesis) and direct physiological sensing and control.although the discrete nature of speech makes it less appropriate forconveying information that is represented by continuous variables than bydiscrete variables, it clearly is one of the most natural methods for humans touse in communicating with another entity, human or machine. previous researchon automatic speech recognition and speech synthesis, which has been drivenby needs outside the se area, has produced a variety of systems that are nowavailable commercially and that can be usefully applied in the se field. currentspeech recognition systems differ from the ideal in that they have limitedcapacity to handle large vocabularies, different voices, continuous speech (asopposed to isolated words or phrases), interference produced by backgroundnoise, and degraded55overviewvirtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.speech production. nevertheless, reasonably high accuracies (e.g., 95 percentcorrect word identification) can be obtained for taskspecific applications inwhich modest demands are made along the justmentioned dimensions ofdifficulty. current speech synthesis systems appear relatively adequate in theirability to produce speech that is highly intelligible (and in comparison tosynthesis systems for environmental sounds); however, they are only nowbeginning to produce speech that sounds reasonably natural or that mimics theidiosyncratic speech patterns of individual talkers. in general, however, there isno question that speech communication interfaces are now becoming availablethat can be usefully integrated into se systems for a variety of practicalapplications, and that the overall quality of such interfaces will continue toimprove over the next few years (with or without help from the se field).physiological interfaces (direct stimulation of neural systems or sensing ofphysiological responses or states of the human organism) have received verylittle attention in the se field. direct stimulation of neural systems seemsrelatively inappropriate except in cases in which the subject is disabled by lossof sensory function. even in these cases, however, the appropriate transducersare likely to become part of the subject, (i.e., to be implanted on a more or lesspermanent basis, as in the case of cochlear implants to mitigate loss of hearing)so that no special requirements are imposed on the se interface. the use ofinternally generated physiological signals associated with activities of the brain,muscles, circulatory system, respiratory system, etc. can be used, at least inprinciple, to indicate general emotional and cognitive human states and tocontrol specific variables in the se system. although significant research isbeing conducted in this area in an effort to determine and improve the reliabilityof such signals for purposes of control (by the military as well as by thoseconcerned with aiding individuals who have severe motor disabilities),extensive practical use of such signals for se control purposes appears severalyears away.computer generation of vesto many people, and certainly to most computer scientists, computergeneration of ves is the core of the se field; to them, humanmachineinterfaces, telerobots, and even the human operators, are of secondaryimportance. furthermore, most past and current work in this area has focusedon the generation of visual images. apart from individuals who are themselvesinvolved in the development or use of teleoperators, when people think of sesystems they tend to think of interactive, computergenerated visual images.except for speech synthesis, which has been developed primarily by speechscientists rather than computer scientists,56overviewvirtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.the generation of auditory and haptic images has been ignored. accordingly,our discussion in this section is focused primarily on the visual channel.information related to the computer generation of the auditory or hapticcomponents of ves is found primarily in discussions of humanmachineinterfaces.it is possible to imagine a ve system that can create photorealistic images,that can be fully interactive in real time, and that has graphics, computation, andcommunication capabilities to handle all possible environments of interest withequal easešthat is, a generalpurpose system that can generate environmentsrelevant to manufacturing, health care, military training, etc. such a system isbeyond the current technology, and it is anticipated that for a long time to come,tradeoffs between realism and interactive capacity will be required.furthermore, due to these limitations, effective ve implementation will dependon targeted application domains. some applications, such as architecturalvisualization, may require photorealistic rendering, whereas others, such astraining, may not. many manufacturing and medical applications may require amuch higher level of realtime interaction than an architectural walkthrough.although there are many applications in which a realistic visual environment isunnecessary (and maybe even undesirable), the ability to generate such anenvironment is clearly an important target for the development of vetechnology.one requirement for creating a realistic visual environment concerns theframe rate, that is, the number of still images that must be presented per secondto provide the illusion of continuous motion. it has been demonstrated thatframe rates must be greater than 8 to 10 per s to maintain this illusion. a secondrequirement concerns the response time a ve system must exhibit to preservean illusion of instantaneous interactive control. research shows that such delaysmust be less than 0.1 s. a third requirement concerns the picture resolutionneeded for realism. according to some ve technologists, a scene can berendered in all of the detail resolvable to the human eye with 80 millionpolygons (catmull et al., 1984). however, using today's hardware, a system thatused 80 million polygons per picture would be far too slow to be trulyinteractivešthus the current major tradeoff between realistic images andrealistic interactivity. these requirements, of course, can be highly applicationdependent. applications with rapidly moving objects may require significantlyhigher frame rates and shorter delays, whereas highly abstract or stylizedapplications may require fewer polygons or lower resolution.hardware maintaining an adequate graphic frame rate is socomputationally demanding that specialpurpose hardware is often necessary.the main purpose of this graphics hardware is to provide rapid geometric57overviewvirtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.transformations, clipping, hidden surface elimination, polygon fill, and surfacetexturing.several of today's leading graphic workstations are realityengine2(produced by silicon graphics), pixel planes 5 and pixelflow (developed at theuniversity of north carolina), and the evans & sutherland freedom systems.all of these systems run on parallel architectures (in which graphic renderingoperations occur on parallel paths); however, they differ on a variety ofcharacteristics, including frame rate, processing speed, and antialiasingcapabilities. both realityengine2 and pixel planes 5 can process approximately2 million texturemapped polygons per s. pixelflow promises significantlyhigher polygon processing rates than are available in current designs. none ofthese machines is able to render photorealistic timevarying visual scenes athigh frame rates.vectorized or massively parallel super computers are also in use in veapplications to improve computational throughput. however, the use of parallelsuper computers may not significantly reduce the run times of ve applicationsthat cannot be performed in parallel or that require large amounts of datamovement. one approach suggested for maximizing versatility is to basecomputations in ve systems on a few parallel highpowered scalar processorswith large shared memories. in such systems, different processors would handledifferent parts of the ve. alternatively, different processors might be dedicatedto different types of data (e.g., one might handle all computations related to thedensity field of a fluid, whereas another might handle all those related to itsvelocity field).other limiting factors relate to speed of data access. these include the timerequired to find the data in a mass storage device (seek time) and the timerequired to read the data (bandwidth). in circumstances in which very largedatasets are needed for a single computation or picture, the bandwidth is criticaland semiconductor memory may be the only viable storage medium in the nearfuture.software in order to provide a fully interactive, realtime, naturalappearing environment, software development is required in a wide variety ofareas. the realtime generation of ves requires consideration of interaction,navigation, modeling, the creation of augmented reality, hypermediaintegration, and operating system software.interaction interaction software makes use of the outputs of humanmachine control devices to modify the ve. the control devices now in useinclude position trackers, mice, keyboards, joysticks, and speech recognitionsystems. as a rule, tasks in ves are performed using a number of controltechniques in combination because none is adequate by itself. the interactionsoftware takes all such control signals, scans them58overviewvirtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.for obvious errors resulting from equipment or user malfunction, and thentransfers the resulting information to those portions of the system involved ingeneration of the appropriate ve.navigation visual navigation software controls what the user sees as he orshe moves through the ve and turns his or her head. more specifically, what auser sees is determined by two parameters: the user's location in the ve and thegaze direction. typically, a head tracker is used to sense the position andorientation of the user's head. changes in the user's location within the ve canbe effected by a virtual vehicle (e.g., a humanoperated treadmill, bicycle, orjoystick that moves the viewpoint of the human user), by the specification of anew vantage point within the environment and the execution of a logicalcommand to fly to that point, or by simulated teleportals in the distance atwhich the user can suddenly appear without moving through the interveningspace.two important sets of software issues that arise when such usermovements are taking place concern the mapping of the user's control actionsinto specifications of how the visual scene should be changed (an interactionissue) and the navigator aids provided to the user to prevent the user fromgetting lost in the ve.a further very important software issue concerns the need to minimize theload on the graphics processors. even if the contents of a ve remain static (asin the case of a virtual building depicting an architectural design), the display tothe user changes as his or her point of view changes. a number of techniqueshave been developed to reduce the polygon flow to the graphics processor, butno general solution is yet available. current solutions are generally applicationspecific, and some work well only if the underlying environment does notchange dynamically. two general techniques for minimizing polygon flow arethe partitioning of the polygondefined world into volumes that can be readilychecked for visibility by the viewer and the lowresolution rendering of objectsthat are small in the user's visual field (e.g., as the result of being very far away).modeling models that define the form, appearance, and behavior of virtualobjects are the core of any ve. today, geometric models constructed for vesare developed, for the most part, using commercially available computeraideddesign (cad) systems. the tools provided by these systems aid designers inspecifying object shapes and sizes; however, these objects are often difficult touse in situations that were not considered by the original cad designer. as aresult, a substantial amount of manual manipulation may be required to use anobject specified with a cad system for a ve. when a ve application requires areplica of a real environment, it is generally considered preferable to map59overviewvirtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.the real environment rather than build a model of it. active mapping techniques,such as scanning laser range finders and light stripes, are used to make threedimensional measurements directly. the drawback of these methods is that theycapture only information visible from a particular viewpoint. to achieve acomplete map can require taking multiple views and combining them into acoherent picture. some passive techniques, such as stereoscopic methods, arealso in use. however, none of the stereo algorithms is robust enough to competewith active methods. for many purposes, far more is required of anenvironmental model than just a map of object surface geometry. if, inapplications that attempt to model realworld behaviors, the objects are to bemanipulated, the physics of the objects is neededšhow they behave, theircomposition, etc. even when the relevant physics is well understood,simulations based on this understanding can be tedious and timeconsuming(both to construct and to run). in a ve, these simulations must run reliably andautomatically; any situation that might arise must be adequately anticipated andhandled correctly in real time.the need for autonomous agents may arise in many ve applications, suchas entertainment, training, manufacturing, and education. although the abilityto create fully credible simulated humans is well beyond our grasp in theforeseeable future, we do have the capability to develop simple agents. theagent's body in a ve is a physical object to be controlled to achieve coordinatedmotion. a computer model of a human figure that can move and function in ave is called a virtual actor. a guided virtual actor is one whose movement isdirected and controlled by the motions of a real human being. an autonomousvirtual actor operates under program control and is capable of independentbehavior that is responsive to the ve, including both human participants andsimulated objects and events. an autonomous actor may touch and manipulateobjects, make contact with various surfaces, or make contact with other humansdirectly (e.g., shaking hands) or indirectly (e.g., two people lifting a heavyobject). autonomous agents need not be literal representations of human beingsbut may be represented by various abstractions.augmented reality in an augmentedreality system, virtual and real objectsappear in the user's view simultaneously; the artificial or virtual image isoverlaid on the realworld image. creating adequate software for augmentedreality applications is a difficult task that requires a complete model of the realenvironment as well as of the synthesized environment. automatic generationof effective augmented reality is still at the research stage. a major issue is theability to create and maintain accurate registration between the real andsynthetic environments, particularly when they are both rapidly varying.60overviewvirtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.hypermedia integration hypermedia is multimedia data composed ofaudio, compressed video, and text that is linked together in a nonlinear manner.a hypermedia system provides an individual with the opportunity to explore atopic by moving through a series of logically linked information nodes. when anode is reached, the individual may obtain all of the information available atthat node in a variety of forms. one example is a virtual museum containinghypermedia nodes that provide significantly expanded information aboutparticular artifacts. once at a node, an individual can pursue a particular objectin depth or explore its relationship to other objects in the exhibit. hypermediaintegration software (which involves the blending of computer graphics, video,sound, and, in the future, haptic images) is used to combine hypermediasoftware with ve. embedding hypermedia nodes into a ve system allows aparticipant in a ve to go to a node and gain additional information about aparticular experience. work in this area is currently at the test bed stage.operating system current operating systems (unix, windows nt) are notgeared to supporting the realtime multimodal requirements of ves, andsignificant modifications are required if the development of ves is to proceedefficiently. there is a major need for systems that ensure that highpriorityprocesses (such as user tracking) receive service at short, regular intervals andto provide timecritical computing and rendering with negotiated gracefuldegradation algorithms that meet frame rate and lagtime guaranteesšthis is anew computing paradigm.teleroboticsas indicated previously, a number of important se applications involvesensing, navigating through, and manipulating objects in realworldenvironments. such applications frequently arise in the domains of hazardousoperations and medicine. in all such applications, telerobotics plays an essentialrole; humanmachine interfaces and computergenerated ves are not sufficient.in many ways, current research activities concerned with teleoperation aresimilar to those concerned with ves. independent of the purpose for which thesystem is being designed (e.g., to train someone to fly an aircraft or to actuallyremove some hazardous waste), and independent of whether the relevantenvironment is real or virtual, in both cases these activities are concerned withthe design, construction, and application of multimodal immersive systems thatenable the operator to interact usefully with some structured environment.furthermore, although concern with complex electromechanical systems waspreviously confined to61overviewvirtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.individuals working on telerobotics, now, as haptic interfaces are becomingrecognized as important components of ve systems as well as of teleoperatorsystems, the ve community is also beginning to show interest in this area.independent of whether the haptic master in the interface controls the behaviorof a telerobot or of some computergenerated virtual entity, design andconstruction of the master requires consideration of electromechanicalphenomena and devices.the principal differences that currently exist relate to (1) the design andperformance of the telerobots and (2) communication time delays, and (3) thedemands of realtime input/output operations. unavoidable time delays(transport delays) arise in communicating between the humanmachine interfaceand the telerobot when these subsystems of teleoperator systems are separatedby a large distance (e.g., a delay of 30 ms between washington, d.c., and losangeles, a delay of a little more than 1 s between the earth and the moon). suchtransport delays decrease in importance as the distance decreases, as otherdelays in the overall systems (e.g., resulting from inadequate computationalspeed) increase, and as the importance of haptic interfaces with force feedbackdecreases. it should also be noted, however, that transport delays will increasein importance in ve systems not only as such systems increase their use ofhaptic interfaces with force feedback, but also as ve networks with tightlycoupled players at distant locations come into being. the approaches being usedto alleviate the time delay problem, which can arise in connection withproblems of both physical stability and perception, include supervisory controland predictive modeling. both of these approaches are being actively pursuedby the teleoperator research community and, more recently, by the ve researchcommunity as well.in typical ve systems, only simple haptic interfaces or trackers may beused, thus placing modest realtime input/output demands on the computer. insuch circumstances, the realtime performance of current operating systemsmay be adequate or could become adequate with some modifications. fortelerobotic systems and for ve systems with complicated haptic and otherhuman interfaces, the input/output requirements are too massive to be handledby ordinary workstationbased architectures. the approach is to use a separatemicroprocessor system for realtime operations, connected to a workstation orpc that acts as a front end.apart from the research related to time delay problems and control issues,much of the current research activity in the telerobotics area is focused ontelerobotic hardware. although substantial advances have been made in thisarea and a number of impressive telerobots have been developed and applied topractical problems, the limitations imposed by inadequate hardware are stillsubstantial. for example, sensor technology (to sense object proximity, objectsurface properties, and applied62overviewvirtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.force) remains inadequate. similar remarks apply to actuator and transmissiontechnology.other current activities in the telerobotic area concern the development anduse of new materials and the exploitation of advances inmicroelectromechanical systems. the availability of extremely small structures(including sensors and actuators) is stimulating exciting new work onmicrotelerobotic hardware and the interface and control problems associatedwith scaling down movements and forces from human scale to micro scales.also, and partly as a consequence of the advances inmicroelectromechanics, engineers are beginning to think about the possiblebenefits and feasibility of creating teleoperator systems that make use ofdistributed telerobotics, that is, a large number of relatively simple, relativelysmall telerobots with relatively narrow bandwidth communication among thesetelerobots. the use of multiple patrol telerobots for security purposes is oneexample of such an application. a major challenge in this area, aside from thedevelopment of the telerobots themselves, concerns the nature of the humanmachine interface and the design of a display and control system that treats theset of telerobots as an integrated system rather than as a collection ofindependent entities that require a separate interface for each telerobot (andperhaps a whole set of human operators rather than a single operator).the general problem of networking telerobotic systems, either in the senseof networking telerobots or networking the human operators, has receivedrelatively little attention.networkscommunication networks can transform ves into a shared environment inwhich individuals, objects, and processes interact without regard to theirlocation. these networks will allow us to use ve for such purposes as distancelearning, group entertainment, distributed training, and distributed design.currently, the two application domains in which the most networkingactivity is occurring are entertainment and national defense. in theentertainment industry, vr companies are in the process of forming cooperativearrangements with cable television companies to develop multiuser games andinteractive shopping.applications for the military have focused on largescale simulatednetwork training exercises, such as those offered by simnet. in simnet, asmany as 300 soldiers in tank and aircraft simulators located at different militarybases can engage in a realistic battle against an intelligent enemy on a commonbattlefield. currently, the defense department is using63overviewvirtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.new communication software that upgrades the simnet protocol. thisupgrade, named distributed interactive simulation, is expected to be used bythe military in building future distributed training scenarios as well as insimulating the acquisition process for various pieces of planned equipment.another example of network use can be found in the experimental programcurrently being pursued in telemedicine by the state of georgia. networkapplications such as these, all of which are discussed in chapter 12, constituteonly a first step.to move the communication software development forward to a point atwhich it can truly support ve applications will require the development of vespecific, applicationslevel network protocols. these applications protocols arerequired for communicating worldstatechange information to the variousnetworked participants in the operating ve.as mentioned in the previous section, the networking of teleroboticsystems in diverse locations has also begun to receive some attention. forexample, the universities associated with the space automation/roboticsconsortium (texas a&m university, the university of texas at austin, theuniversity of texas at arlington, and rice university), in conjunction with thenational aeronautics and space administration (nasa), have developedsoftware and protocols to use the internet to control telerobots in their differentlocations. this type of effort is under consideration elsewhere and is likely togrow in scope. advances in networking will strongly impact such developments.widearea network (wan) hardware is being developed on a variety offronts. the national telecommunications infrastructure is being radically alteredby the installation of fiber optic cabling that is capable of operating at gigabitspeeds across the country. as a result, major commercial carriers are installingspecial switches that handle both synchronous and asynchronous signals at veryhigh speeds. governmentsupported networks are also in the process ofupgrading. for example, nsfnet, the backbone of the internet, is currentlyoperating at t3 speeds (45 mbit/s) and plans to move to oc3 (155 mbit/s) in1994 and to oc12 (622 mbit/s) by 1996. the national research and educationnetwork is one of four components in the u.s. high performance computingand communication program, which is supporting the installation of oc12networks at five regional test beds for research purposes.workstations with threedimensional graphics will be connected to thewans discussed above through localarea networks (lans). most of theselans, which currently use ethernet technologies (10 mbit/s), do not have thecapabilities to support the highperformance demands of ve and multimedia.however, work is proceeding on larger and faster local networks, such as thefiber distributed data interface. this system currently operates at 100 mbit/s,but the followon, expected by the middle64overviewvirtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.to late 1990s, will operate at speeds up to 1.25 gbit/s. moreover, the institute ofelectrical and electronics engineers (ieee) has issued a draft standard,integrated services local area network interface, which defines a lan thatcarries voice, data, and video traffic.although networks are becoming fast enough to support the developmentof distributed ve, we need greater bandwidth to support the very largescale,multimodal, and multipleuser applications that we foresee in the near future.another problem is network hostinterface slowdowns caused by the multiplelayers of the operating system software. a further issue is the high costassociated with buying time on a highspeed wan. the current estimated costfor one year exceeds the entire budget of most research groups. finally, itshould also be noted that, despite the beginning efforts mentioned above tonetwork teleoperator systems, it is important to focus on teleoperator systems aswell as ve systems as work in this area progresses.evaluation of se systemsse both draws on and provides research and development challenges toseveral wellestablished disciplines, including computer science, electrical andmechanical engineering, sensory physiology, cognitive psychology, and humanfactors. in each discipline, the requirements associated with creating setechnology raise new questions that call for research and evaluation. someexamples include: (1) identifying the capabilities and limitations of humanbeings as criteria for system design, (2) developing the hardware and softwarethat can deliver ses in a costeffective manner, and (3) determining areas inwhich se can make an important difference in human experience orperformance.as with the introduction of many new technologies, se technology has notbeen adequately evaluated. moreover, evaluation with regard to ultimateeffectiveness is difficult because the technology is at an early stage in itsevolution and, as a result, does not provide the highfidelity environments andthe natural interfaces that are planned for the future. in addition, se offers someparticularly complex evaluation issues because of its interdisciplinary natureand the requirement to integrate several technologies to create a full se system.human studies are needed to generate requirements for both component and fullsystem design. moreover, it is desirable to have costeffectiveness evaluationsfor each component as well as for prototype systems in each application area.some evaluation questions concern the engineering reliability and efficiency ofcomponents or full systems. other questions focus on how well the designaccounts for human perceptual and cognitive features or for human responses toalterations in sensorimotor loops. in the cognitive area, research indicates that itis difficult to make generalizations about the65overviewvirtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.relationship between types of tasks, task presentation features, and humanperformance. this indicates a need to conduct studies to explore theserelationships as the technology improves and ideas for new se applications areproposed.another set of questions for evaluation includes the possible medical andpsychological effects of se technology on human beings. for example, studieswill be needed to ensure that the technology will not have any adverse effectsover time on human visual, auditory, or haptic systems. furthermore, there arethe important concerns about induced motion sickness in ses and the potentialaftereffects of adaptation to ses when an individual moves back into the realenvironment.there are many types and levels of evaluation that can be used to providedirection, understanding, and a general picture of performance effectiveness.standard evaluation methodology offers a range of options, including: (1)empirical studies using observation techniques or experimental designs tocollect data in laboratory or field settings and (2) analytic studies involvingtheoretical modeling, heuristic evaluations, and simulations of varying systemfunctions. each of these methods can be used at various points in the system'sevolution.at present, relatively little evaluation of se systems is taking place. theextent to which the novel aspects of se will require new evaluation toolsremains to be seen.recommendationsthe committee's overarching conclusion is that se systems have greatpotential both for helping to satisfy various societal needs and for stimulatingadvances in some important areas of science and technology. in ourdeliberations, when reservations about the value of investment in someparticular se area were expressed, they often reflected the judgment that theimportance of advances in the particular area were dwarfed by the need tomodify related social and political factors rather than that the area wasunimportant or inappropriate from a scientific or technical point of view.in recommending topics and areas for concentrated research anddevelopment efforts, the committee rejected the approach of developing a smallnumber of highpriority (''star") recommendations. the possible applicationscover such a broad range of societal activities, and the advances required torealize truly costeffective systems for these applications cover such a broadrange of research and development activities, that the star approach seemedtotally inappropriate.in constructing this research agenda, the committee used three kinds ofcriteria. the first is concerned with advancing the state of the art.66overviewvirtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.specifically, it is concerned with the extent to which a project underconsideration can be expected to lead to improved understanding of importantphenomena and/or to improved technology. we refer to criteria of this type asscience and technology criteria.the second is concerned with the likelihood that the project in questionwill have important practicalšand positivešconsequences within the nottoodistant future (i.e., the next five years). we refer to criteria of this type aspractical applications criteria.the third is concerned with factors such as leverage, costeffectiveness,and ratio of payoff to effort. in addition to technical matters, evaluation of thesefactors must involve consideration of current conditions and forces in society,which go beyond those the committee was appointed to examine. in general, werefer to criteria in this category as leverage criteria.finally, it should be noted that the recommendations have not beenprioritized in any detailed manner. this is due primarily to our judgment thatsuccessful development and application of se systems depends on an entirematrix of interrelated factors, not on one or two isolated factors. wenevertheless feel that it is important to stress the crucial need for improvedhardware technologies to enable development of improved interface devicesand improved computer generation of multimodal images. unlike the situationin the area of teleoperation, in the area of ves there are relatively fewindividuals who have primary interests or backgrounds in hardware; mostindividuals in the ve area are involved primarily in the software end ofcomputer science, in communication or entertainment media, and in humanperception and performance. thus, the importance of adequate hardware,without which the ve field will never come close to realizing its potential,tends to be underplayed by the ve community. a somewhat similar commentconcerns the issue of user comfort. to date, a very large fraction of ve usagehas occurred in the context of short demonstrations, a context in which thedegree of comfort is relatively unimportant. however, if the comfort of vesystems (particularly headmounted displays) cannot be radically improved, thepractical usage of these systems will be limited to emergency situations or tovery short time periods. in other words, adequate comfort, as well as technicallyadequate hardware, are essential to realizing the potential of the se field.the research agenda we propose covers four main categories:(1) application domains,(2) some psychological considerations,(3) development of improved se technology, and(4) evaluation of se systems.67overviewvirtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.in this section we present recommendations for research and developmentin the near future in terms of these categories. we also indicate to the extentpossible the role played by the various types of criteria in making theserecommendations. further details are provided in the chapters of this report. inaddition, we make comments and suggestions for government policy andinfrastructure based on the experience and judgment of committee members.they are suggestive of the kinds of tools and mechanisms that federal agenciesmight use to encourage coherence, integration, and overall development of thefield.application domains of se systemsrecommendation: the committee concludes that four applicationdomains show the most promise for se: (1) design, manufacturing, andmarketing; (2) medicine and health care; (3) hazardous operations; and(4) training. we recommend that the research needs in these domains beused as one of the principal means to focus se technology developmentand testing.our review shows that each of these domains includes tasks that areparticularly compatible with the projected capabilities of se. each of thesedomains received high scores with respect to the science and technology andpractical applications criteria. the domain of hazardous operations alsoreceived a high score with respect to the leverage criterion because of therelative lack of attention and funding given to this domain.the committee has not assigned priority to the application domains ofeducation; information visualization; and telecommunication and teletravel.although committee members agreed that the education domain is exceedinglyimportantšperhaps the most important of all the domains consideredšit wasnot assigned priority because of our judgment that the development of improvededucation technology will have only a minor effect on the quality of educationactually received. in other words, the main current obstacles to achievingsubstantial improvements in education are social, political, and economic, nottechnological. thus, even though the education domain can be viewed as a highleverage domain with respect to funding considerations, it is regarded as a lowleverage domain overall. also, the committee did not rate this domain highlywith respect to the science and technology criterion. although further scientificresearch is required to determine how se technology can best be utilized ink12 education, it is believed that other application domains are likely to play amore important role in driving se technology. if the relevant infrastructureundergoes changes that greatly facilitate widespread and indepth use oftechnology within this area, then priority for the education domain would beindicated.68overviewvirtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.for the present, however, it is believed that efforts with the highest payoffrelevant to the education domain will be those that are directed toward thegeneral development of improved technology, toward alteration of the relevantsocial, political, and economic infrastructure, and toward influencing theentertainment industry to include programs, activities, systems, and facilities inthe soontobe upgraded interactive home entertainment centers that haveincreased educational value. to the extent that the introduction of improvedcommunication technology that links students and teachers (and parents) atdisparate locations can help improve the current educational infrastructure aswell as directly improve the quality of education, the committee would stronglysupport such an effort.the information visualization and the telecommunication and teletraveldomains are not assigned priority for a number of reasons. paramount amongthem, and consistent with their crosscutting characteristics, is that work in thespecified priority domains will necessarily include work in these domains aswell. thus, for example, work in design, manufacturing, and marketing as wellas medicine and health care will necessarily include work on informationvisualization. similarly, work on hazardous operations and training willnecessarily involve work on telecommunication and teletravel. an additionalreason for denying the information visualization domain priority is the fact thatwork in the area of scientific visualization using ve is already quite active.thus we did not feel that this domain deserved a high score on the leveragecriteria.although equivalent remarks could be made about the crosscutting natureof the training domainšthat is, applications of se technology to training willoccur in connection with each of the other highpriority domainsštraining wasjudged to be so important and the potential for achieving substantially improvedcosteffectiveness in training so great, that it retained a priority rating.special projectsrecommendation: the committee recommends two projects forspecial attention: (1) modeling the human body for purposes of medicaleducation, surgical planning, and providing explanations of proceduresand outcomes to patients and (2) studying the transfer of knowledge andskill gained in training in a ve to performance in a realworld taskenvironment.modeling the human body is required for many of the ve applicationsconsidered within the general domain of medicine and health care. for example,within the subdomain of surgery, physical models of various69overviewvirtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.bodily organs, skeletal structures, physiological subsystems, etc., are needed forthe planning of surgery, the training of surgeons, and explaining possibleprocedures and outcomes to patients. these models must be sufficientlyaccurate to serve the purpose at hand yet sufficiently simple to satisfy thecomputational constraints imposed by the limited power of the available vesystem. in many cases, the fidelity of the virtual body parts, processes, andsystems will be limited not only by inadequate ve facilities, but also byinadequate scientific knowledge (i.e., inadequate empirical data about thephenomena to be modeled or inadequate theoretical techniques for thequantitative modeling of these phenomena). nevertheless, we recommend thisproject because it satisfies all three criteria and because even very crude modelsare likely to provide results that are substantially superior to those nowachieved. thus, for example, it should not be overwhelmingly difficult todevelop physical models with enough fidelity to create ve educationalexperiences (for medical personnel and patients) that usefully supplement thosenow being realized by means of static twodimensional illustrations,conventional videos, and spoken or written words. not only should it bepossible to develop models in which the visual components are superior to thosenow achieved with conventional techniques, but the addition of the auditory andhaptic channels in such efforts should also greatly facilitate the educationprocess.studies of training transfer, the second special project area cited, areessential to the successful development of ve training procedures for a widevariety of training tasks in a wide range of application domains. although it isgenerally recognized that ve systems have great potential for costeffectivetraining, in order to realize this potential it is necessary to determine andunderstand how various differences between the situation faced by the trainee inthe ve training system and the situation faced by the trainee in the realworldsituation for which the individual is being trained influence the effectiveness ofthe training as measured by task performance in the realworld situation.research on this transferoftraining issue has, of course, been conducted formany years in connection with other types of training. however, the class ofdifferences between the training and realworld performance situations that willarise when ve training is used are likely to include numerous elements thathave not been studied before (e.g., the differences associated with unique pointsof view or instructional cues that can be generated by ve systems but not byconventional training systems). furthermore, even when no such new elementsexist, understanding of training transfer issues remains rather limited. as withthe humanbody modeling project, the training transfer project has high ratingson all three criteria. it is very challenging scientifically, advances will haveimportant and70overviewvirtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.immediate applications, and modest amounts of additional funding are likely tosignificantly increase the probability of success in ve training applications.psychological considerationsrecommendation: the committee recommends that support forpsychological studies be organized around the following objectives:(1) development of a comprehensive, coherently organized review of theory and data on human performance characteristics (includingconsideration of basic sensorimotor resolution, perceptualillusions, information transfer rate, and manual tracking) from theviewpoint of se systems.(2) development of a theory that facilitates quantitative predictions ofhuman responses to alterations in sensorimotor loops for allchannels, with special emphasis on: (a) degradations inperformance resulting from deficiencies in se technology (e.g., inthe form of distortions, time delays, and system noise), (b)supernormal performance achievable through introduction ofpurposeful enhancing distortions, (c) radical sensorimotortransformations that arise, for example, in connection with the useof sensory substitution or strongly nonanthropomorphic telerobots, (d) methods of accelerating both adaptation to various types of alterations and readaptation to normal conditions, (e)channelinteraction effects that occur with multimodal interfaces,(f) factors governing the occurrence, kind, and magnitude of sopitesickness from se exposure, and (g) factors governing the strengthof subjective telepresence and its relationship to objectiveperformance.(3) development of cognitive models that will facilitate effective designof ve systems for purposes of education, training, and information visualization.(4) development of improved understanding of the possibledeleterious effects of spending substantial portions of time in sesystems.although a wide variety of research on human performance characteristicshas already been performed, the results have not yet been coherently organizedto reflect the viewpoint of se systems. such a review would not be too difficultor expensive to prepare, and would be extremely useful to a large segment ofthe se research community. in addition to providing important relevantinformation to this community, it would help delineate the further research thatis required in this area to guide design of improved se systems.71overviewvirtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.the task of characterizing and modeling human responses to alterations insensorimotor loops constitutes a major challenge. knowledge in this area,which is fundamental to the design of essentially all se systems for all types ofapplications, is seriously inadequate. currently, it is difficult to predict howsuch alterations will influence either objective performance or subjective state,and in particular, how performance and state will change over time as the usergains experience with the alteration.some of the subareas in which we urge special emphasis here may bestrongly applicationdependent. we also recognize that past efforts in some ofthese subareas have not always been as fruitful as one would like. nevertheless,because of the importance of these subareas to the progress of the se field, thecommittee's recommendation to pursue this research received high ratingsaccording to all three classes of criteria.the development of improved cognitive models for characterizing themanner in which experience gets organized, problems get solved, world viewsare formed, and learning takes place is essential to a wide variety of ve uses;however, it is particularly important in the application domains of education,training, and information visualization. without such models, and without theproper integration of such models into a general framework that includes theeffects of multimodal sensorimotor experience on perception and cognition, thedesign of ve systems for these application areas will be seriously handicapped.a final set of topics for research concerns the study of the possibledeleterious effects of spending substantial portions of time in se systems. tosome extent, problems of this type will be addressed automatically as aconsequence of the governmentmandated humanuse monitoring ofexperimental research involving human subjects; however, the precautionsassociated with this monitoring are not likely to adequately address possiblenegative effects of continued usage in everyday life. in any case, it is obviouslyessential to cover issues related to the effects of se on relatively deeppsychological factors such as selfimage, cognitive style, affective state, andmotivation.development of improved se technologyhumanmachine interfaceshumanmachine interfaces include all the devices used in an se system topresent information to the human or to sense the actions or responses by thehuman that control the machine in question.recommendation: the committee recommends support of researchon visual displays, haptic interfaces, and locomotion interfaces, withemphases as outlined below.72overviewvirtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.these recommendations were highly rated on all three criteria: science andtechnology, applications, and leverage.visual displays the development of adequate headmounted displays isvery important to the se field. the main deficiencies in current hmds involvethe quality of the visual display and the ergonomics of the helmet used to mountthe visual display on the user's head. (both the quality and the ergonomics ofthe auditory component of hmds are currently adequate.) despite thesubstantial effort that is already being directed toward reducing these problemsby the entertainment industry (whose focus is on lowend systems) and by themilitary, because of the large payoff that would result if substantialimprovements were made, the committee recommends that strong support beprovided to research in this area. of particular importance for the visual displayis substantially improved combinations of resolution and field of view. also tobe considered in this area is the inclusion of seethrough options for use inaugmentedreality applications.work on the ergonomics associated with the hmd should include not onlyconsideration of mass and center of gravity (as well as fit), but alsoconsideration of wireless, broadband communication to eliminate the customarytether used. integration of visual, auditory, and position tracking in one device(possibly helmet, but much preferably, sunglasssized) should take account ofdetailed data and understanding of norms and variations in human heads. ofparticular importance is exploring alternative materials and configurations andevaluating them in a ve context.because there is no guarantee that hmds that are fully adequate for allimportant tasks will be available even within the next 10 years, attention mustalso be given to improving offhead displays. in particular, the committeerecommends that ohd research be carefully monitored and support be providedto those display projects that appear promising from the performance point ofview (e.g., some of those concerned with autostereoscopic displays) but are toorisky from the business point of view to be supported by industry. the projectssupported by the military in this area have not led to displays that areaffordable, and the projects conducted by private hobbyists and inventors in thisarea are likely to die out because of inadequate funding.special attention should also be given to the study of perceptual effectsassociated with the merging of displays from different display sources (as inaugmentedreality applications). independent of whether the images that arecombined with computergenerated ve images are derived directly from thereal environment by means of seethrough displays or indirectly through theartificial eyes of a telerobot, the psychophysics of such merged displays must becarefully studied. not only is73overviewvirtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.there relatively little past work in this area, but also the use of such mergeddisplays is likely to become increasingly common. equivalent psychophysicalstudies in the auditory and haptic channels are regarded as worthwhile but lessurgent.haptic interfaces many important potential se applications cannot berealized adequately without the development of substantially improved hapticinterfaces. although the haptic devices that can be developed over the next 5 to10 years will undoubtedly fall short of the ideal device, they can vastly improvethe range and quality of haptic interactions that are now available in sesystems. particular attention should be given to the development of toolhandleinterfaces in which the possible haptic interactions are constrained by the natureof the tool.special support also should be given to basic haptic science relevant to thedevelopment of improved haptic interfaces. empirical data and theoreticalmodels describing phenomena for this channel are much less adequate than forthe visual and the auditory channels, and considerable scientific work is neededto support technology development for this channel. this work should includestudies confined entirely to the haptic channel (e.g., exploration of hapticillusions), as well as studies concerned with the interaction effects of this andother channels (especially vision).locomotion interfaces another technology area for which the committeerecommends special support is that of locomotion interfaces. the range ofpossible applications would be substantially increased by the availability ofimproved locomotion interfaces, and there seems to be no fundamental obstacleto the creation of such interfaces.position tracking and mapping the development of improved devices forposition tracking and mapping is extremely important.recommendation: in this area the committee recommends amultiphased approach: (1) conduct research and development onmechanical trackers and inertial trackers, (2) explore the possibility ofobtaining improved costeffectiveness in tracking by using hybrid systems,and (3) carefully monitor commercial developments in magnetic,acoustical, and optical trackers, in eye trackers, and in trackers directedtoward registration problems in augmented reality. if market forces donot drive the development of these trackers, federal research support isurged.the committee's recommendations in this area are complex because offactors relating to the leverage criteria.market forces in the development of mechanical trackers, inertial trackers,and hybrid systems currently appear minimal, so research in74overviewvirtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.these areas requires support. in the committee's judgment, the other devicesmentioned in the recommendation will continue to improve without specialsupport. for example, it is believed that commercial interest in themanufacturing and health care domains is sufficient to drive development ofimproved optical systems for registration of images from different sources inaugmentedreality applications. if this should turn out to be incorrect, however,the committee would assign high ratings to research in this area.testing and evaluation a further important recommendation concerninginterface technology concerns the physical testing and evaluation of interfacedevices.recommendation: the committee recommends the establishment ofa set of standards or an independent laboratory to evaluate se interface devices.because of the lack of reliability in manufacturer's specifications, andbecause the techniques and standards employed by different laboratories in suchefforts vary widely (even if the laboratory performing the work has no vestedinterest in the device), it is recommended that an independent laboratory beestablished to evaluate se interface devices or, alternatively, strict standards beset for such evaluations.other interface issues we have not included recommendations concerningsupport for auditory displays, speech communication interfaces, physiologicalinterfaces, or other types of displays previously mentioned (related to olfaction,temperature, wind, etc.). for auditory, speech, and physiological interfaces, thisdetermination results from our judgment that any required advances in theseareas will be driven by forces outside the se field and thus no special support isrequired. although work on the other types of interfaces may not be driven bysuch outside forces, we do not regard the need for these other types of interfacesto be as important for most application domains. it should be noted, however,that their inclusion is likely to provide a major increase in the sense of presenceor immersion in ves and, to the extent that such subjective effects are likely tohave a strong positive influence on performance, they may be important.computer generation of veshardware advances in computational and communication hardware areessential to the full realization of ves. the hardware capabilities availabletoday have given researchers, entrepreneurs, and consumers just a taste ofvirtual worlds and a promise of possible applications. because of75overviewvirtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.the potentially wide appeal of ves and the large variety of applications withdiffering performance requirements, it is important to continue hardwaredevelopment at several levels, from highend supercomputer workstations tolowend personal workstations with modest capabilities.extrapolating current trends, we expect that ve applications will continueto saturate available computing power and data management capabilities forsome time to come (dataset size will be the dominant problem for suchapplications as physical modeling and scientific visualization). in the future,highend ve platforms will require the following features: very large physicalmemories (> 15 gbyte), multiple highperformance scalar processors, highbandwidth (> 500 mbyte/s), low latency (< 1 millionth s) mass storage devices,and highspeed interface ports for various input and output peripherals.the research and development required to achieve the hardwarecapabilities stated above gets high ratings in terms of both the science andtechnology criteria and the practical applications criteria. however, we aremuch less clear about the leverage criteria. on one hand, current market forcesdo not appear to be driving adequate development of coherent and integratedarchitectures for multimodal modeling, representation, and rendering. evenwithin the confines of the visual channel, it appears that relatively littleattention is being given to timedeterministic generation of images (i.e., systemsthat guarantee compliance with appropriate bounds on graphics update rate andlag, possibly at the expense of resolution). on the other hand, if the overallcommercial market for computer hardware continues to grow at the same rate asit has in the past, or if the se field looks very promising to the industry, thensuch advances will probably require no special support. therefore:recommendation: the committee recommends no aggressivefederal involvement in computer hardware development in the se area atthis time. rather we conclude that hardware development remain largelya privatesector activity. should serious lags in development occur, thegovernment might then consider strategies for leveraging privatesectordevelopment efforts.software in the past, research on and development of software for ve hasbeen conducted through small, independent research programs.recommendation: the committee recommends that a major unifiedresearch program be created that focuses on those areas of developmentdirectly related to the generation, implementation, and application of ves. the basic topics that need to be considered in such a program include: (1)multimodal humancomputer interactions, (2) rapid specification andrendering of visual, auditory,76overviewvirtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.and haptic images, (3) models and tools for representing and interacting with physical objects under multimodal conditions (including automated model acquisition from real data), (4) simulation frameworks, (5) a newtimecritical, realtime operating system suitable for ves, (6) registrationof real and virtual images in augmentedreality applications, (7)navigational cues in virtual space, (8) behavior of autonomous agents, and(9) computer generation of auditory and haptic images.there is a need to develop methods and software to interpret and respondto multimodal inputs from a wide variety of devices, including those associatedwith position tracking, haptic manipulation, and speech commands, oftenoccurring in concert. improved software must be developed to determine andfilter out errors in these control signals and to provide the appropriate resultinginformation to the software generating the ve.fast and realistic specification and rendering of visual, auditory, and hapticimages is a fundamental topic for ve research. the combined requirements ofrealism and interactive performance are extraordinarily severe. for example, inthe visual modality, it has been estimated that the creation of a totally realisticinteractive environment would require 80 million polygons per picture and aminimum of 10 pictures/sša total of 800 million polygons/s, far beyond thecapacity of current graphics workstations. across modalities, there is a need todevelop representations and rendering algorithms that dramatically increaseeffective throughput without sacrificing realism. a key issue will be togeneralize static rendering methods to effectively handle dynamic scenes, forexample by exploiting temporal coherence and by automatically adjusting thelevel of detail to match what the user will be able to perceive. research anddevelopment in parallel rendering will also become important.realtime rendering for interactive systems poses additional problemswhen the auditory and haptic modalities are included. for each modality, thesoftware for rendering images receives the output of the physical model andgenerates the commands needed to drive and control the interface devices. themajor issues are: (1) the accuracy of rendering in relation to computed output ofthe model, the capabilities of the display device, and the sensory resolution ofthe human user in each of the modalities; (2) minimization of time delays in therendering for each modality; and (3) synchronization of displays amongmultiple modalities. efficient rendering requires that the capabilities of therendering software should be commensurate with those of the physical model,the display device, and the human user. the higher the display resolution, themore timeconsuming is the rendering, leading to time delays. such delays, if77overviewvirtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.excessive, generally lead to perceivable lags and distortions. in the case of forcereflecting haptic interfaces, they may also cause mechanical instabilities.therefore, rapid rendering software that minimizes time delay while retainingoptimal display resolution is critical for each of the modalities. undermultimodal conditions, the additional condition of synchronization of themodalityspecific displays needs to be satisfied. we recommend support fordevelopment of software tools for rapidly driving visual and auditory displaydevices, together with fast, realtime control of haptic interfaces. such softwarecan have both devicedependent and deviceindependent components.there is a major need to develop more powerful methods for acquiring andrepresenting realistic models of physical objects and for realistic simulation ofthe physical behavior of these objects. to construct realistic models of trulycomplex environments is all but impossible with current computerassisteddesign tools. creation of such complex models will require a combination ofautomated model acquisition from real data and automated model synthesisbased on concise descriptions. such models will ultimately need to capture notonly the object characteristics relevant to the visual channel, but also all of thephysical properties that must be specified to realistically simulate objects'appearance and behavior in the broad multimodal sense. simulating themechanics of the everyday world will be of central importance in giving virtualenvironments a sense of solidity and allowing users to effectively manipulatevirtual objects through haptic interfaces. the problems that arise in generalizingstandard batchsimulation methods to handle interactive ves are analogous tothose that arise in the extension of static rendering techniques.research into the development of environments in which object behavioras well as object appearance can rapidly be specified is an area that needsfurther work. we call this area simulation frameworks. such a frameworkmakes no assumptions about the actual behavior (just as graphics systemscurrently make no assumptions about the appearance of graphical objects). agood term for what a simulation framework is trying to accomplish is metamodeling. such frameworks would facilitate the sharing of objects betweenenvironments and allow the establishment of object libraries. issues to beresearched include the representation of object behavior and how differentbehaviors are to be integrated into a single system.because most current operating systems are built on commercial versionsof unix, which is not designed to meet realtime performance requirements,the committee recommends that approaches to a new operating system suitablefor ves be studied. in principle this could be achieved by creating a newoperating system architecture or providing upgrades or enhancements toexisting operating systems (e.g., unix, windows nt).78overviewvirtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.the operating system capabilities required for ve include support of verylarge numbers of lightweight processors communicating by means of sharedmemory, support of automatic or transparent distribution of tasks to multiplecomputing resources, support of timecritical computation and rendering, andvery high resolution timeslicing and guaranteed execution for highpriorityprocesses (to within 0.001 s resolution). although not specifically addressing allof these concerns, the efforts of the ieee posix standards committee arestarting to bring realtime capabilities to the opensystem workstationenvironment.1 supporting these capabilities in the operating system willsignificantly facilitate the development of many ve applications, especiallylarger, more ambitious efforts). the commercial sector cannot be expected toperform the necessary research and development in this area without incentivesfrom the federal government. specifically, we recommend that the governmentparticipate with industry in funding the upgrades and enhancements needed toprovide an operating system that will meet the performance requirements forimplementing ves. moreover, these joint funding efforts should beaccompanied by a plan to move the new or upgraded systems to commercialadoption. to ensure that ve systems are written using an appropriate operatingsystem, a financially sound transition plan must be formulated, funded, andexecuted.another important area for development is registration of real andsynthetic images for augmentedreality applications. to create the illusion thatsynthetic and real objects exist in the same world requires highly accurateregistration. for example, to make a synthetic object appear to rest on a realtable, the object must move with the table as the observer moves, and accurateregistration requires both a good geometric model of the scene and goodmeasurements of observer motion. in addition to the purely geometric aspectsof registration, illumination effects (casting synthetic shadows onto real objects)must be handled. note also that significant misregistration may be disastrous incertain applications (e.g., surgery).in addition to the general areas discussed above, the committeerecommends that research and development on the following topics besupported: navigational cues in virtual space, the behavior of autonomousagents, and the computer generation of both auditory and haptic images for ve.navigational cues are important because there is a great tendency in current vesystems for users to lose their way during virtual travel (or even simply duringrotations of the head). work on autonomous agents1 for ves with relatively simple input/output, the realtime requirements are differentfrom those of the telerobotics community. the fundamental difference is that themassive input/output requirements for complicated haptic and other human interfacesassociated with telerobotics cannot be handled by an ordinary workstationbasedarchitecture.79overviewvirtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.is important because many future applications are likely to require such agents,and the task of designing appropriate psychological and physical models for''driving" these agents is an extremely difficult one. with regard to computergeneration of auditory images, spatialization, synthesis of environmentalsounds, and auditory scene analysis are judged to be the most critical; in thehaptic channel, because so few results are currently available, a wide array ofresearch projects should be supported. although certain components of some ofthese problems relate primarily to design of humanmachine interface devices,others relate primarily to software.teleroboticsrecommendations in the area of telerobotics that are not already includedelsewhere concern: (1) the effects of communication time delays on teleoperatorperformance, (2) telerobotics hardware (structures, actuators, and sensors), (3)microtelerobotics, (4) distributed telerobotics, and (5) realtime computationalarchitectures.recommendation: the committee recommends that support begiven to improving control algorithms, improving methods forconstructing and using predictive displays, and improving methods forrealizing effective supervisory control strategies.unless communication delays are properly handled, teleoperatorperformance will be severely degraded and may, under certain circumstances,become unstable. in order to combat the effects of such delays, continuedefforts should be directed toward the development of improved controlalgorithms that ensure stability and yet, to the extent possible, providereasonable gains. at the same time, continued effort should be directed towardthe development of improved methods for constructing and using predictivedisplays and for realizing effective supervisory control strategies. advances incombatting the delay problem are required not only in connection withhazardous operations, but also in connection with certain components oftelemedicine (particularly telesurgery).recommendation: the committee recommends work in four areasof hardware development: (1) multiaxis, highresolution tactile sensors, (2) robot proximity sensors for local guidance prior to grasping, (3) multiaxisforce sensors, and (4) improved actuator and transmission designs.multiaxis highresolution tactile sensors are needed to provide thetelerobot with an effective sense of touch. robot proximity sensors are requiredto provide local guidance prior to grasping. such guidance80overviewvirtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.would greatly facilitate the development of adequate supervisory control.multiaxis force sensors are needed to measure the net force and torque exertedon end effectors. for example, miniature force sensors of this type could bemounted on finger segments to accurately control fingertip force. improvedactuator and transmission designs are required to provide highperformancejoints and improved performance of telerobotics limbs.recommendation: the committee recommends that research beconducted on issues that arise when microtelerobots are used inteleoperation.as the field of microelectromechanics evolves, and smaller and smallertelerobots can be constructed, the need for both basic and applied research inthis area will steadily increase. for example, it will be necessary to addressproblems associated with the scaling of movements and forces. because themechanical behavior of objects in the micro domain are radically different thanin the macro domain, such scaling will require the development of new types oftelerobotic controllers.recommendation: the committee recommends that considerationbe given to the development and application of distributed teleroboticsystems.relatively little attention has been given to teleoperator systems in whichthe human operator is interfaced to a distributed set of telerobots. because manyfunctions require sensing or acting over a region that is large relative to the sizeof an individual telerobot (e.g., patrolling land or structures for securityreasons), such systems, if appropriately designed and developed, would havemany important applications. issues that need to be addressed in this areainclude the careful selection of specific applications, the design of thecommunication system for transmitting information among the telerobots andbetween the set of telerobots and the human operator, and the design of humanmachine interfaces that are well matched to human sensory and controlcapabilities in situations involving multiple telerobots.recommendation: the committee recommends the establishment ofintercommunication standards for pointtopoint connections in coarsegrained parallel computational architectures. however, for applicationswith demanding input/output operations, the committee does notrecommend new realtime development systems or operating systems.the most demanding ve system will require powerful realtime input/output capabilities to handle haptic interfaces, trackers, visual displays,81overviewvirtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.and auditory displays. in robotics and telerobotics, in which the requirementsare similar, there is a general movement toward coarsegrained systems basedon pointtopoint communications, such as transputers and c40 systems.commercial development environments and realtime operating systems areadequate for such systems. however, there has not yet emerged a highspeedintercommunication standard for pointtopoint computational architectures,which would offer users great flexibility in mixing and matching componentsacross vendors and different processor types.networksthe committee anticipates that in the future most ve applications will relyheavily on network hardware and software. although networks are nowbecoming fast enough for distributed ve applications, development is needed toprovide the enormous bandwidth required to support multiple users, video,audio, haptics, and possibly the exchange of threedimensional interactionprimitives and models in real time. moreover, handling the mix of data overnetwork links will require new applications level protocols and techniques.because of the central nature of network technology to the implementation ofves, the committee sees network hardware and software development ascritical to advancing the science of ve and its applications. however, webelieve that the hardware necessary to support ve applications will bedeveloped without intervention from the ve research community. in otherwords, there are forces in both the federal government and the private sectorthat are driving major advances in hardware. as a result, we do not recommendadditional investment in network hardware development at this time.nevertheless, it is important to acknowledge the existence of significantinfrastructure problems that could impede the use of networks for veapplications. for these problems, specific effort should be provided in supportof ve requirements. one infrastructure issue is the high cost of research onlargescale networked ves. a very limited number of universities can afford tohave dedicated t1 lines (with installation expenses of $40,000 and operatingcosts of $140,000 per year, as for the defense simulation internet currently)needed to support these activities. various approaches, such as an open venetwork and the necessary ve applications protocol, should be considered forproviding research universities with access to the needed facilities. unless costsare significantly reduced, it will not be possible to initiate a concerted effort todevelop software solutions for networked ve.perhaps our greatest infrastructure concern is the need for the developmentof network standards that will be compatible with the longrange82overviewvirtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.needs of distributed ves. one danger is that the entertainment industry, with itsinterest in interactive games for the home, will set the networking protocolstandards at the low end, and the military community will set the standards atthe high end. therefore:recommendation: the committee recommends that the federalgovernment provide funding for a program (to be conducted withindustry and academia in collaboration) aimed at developing networkstandards that support the requirements for implementing distributedves on a large scale. furthermore, we recommend funding of an open venetwork that can be used by researchers, at a reasonable cost, toexperiment with various ve network software developments andapplications.evaluation of se systemsrecommendation: the committee recommends that the federalgovernment encourage the se system developers it supports to include acomprehensive evaluation plan in the early design stages of their researchprojects. it also recommends that the federal government help coordinatethe development of standardized testing procedures for use across studies, systems, and laboratories, particularly in those areas in which the privatesector has not acted.se technology is in the early stages of development, is growing rapidly,and is the subject of highly optimistic projections about its usefulness. incontrast, the extent to which its usefulness has actually been seriously evaluatedis vanishingly small.in general, evaluations are required not only to compare overall costeffectiveness of se approaches with other approaches addressed to the samegoals, but also to provide insights to guide modifications and new designdirections. to be optimally effective, such evaluations must take place both atthe overall system level, at the component level, and at all stages of thedevelopment process. although many of the specific questions to be addressedin an evaluation effort are likely to depend to some degree on the structure andpurpose of the system or component in question, it should be possible todetermine a common framework for a substantial portion of the evaluation needs.in order to help ensure adequate se evaluation, the federal governmentshould encourage individuals involved in federal supported research anddevelopment to include serious evaluation plans in the design of their projects.such plans should address questions about engineering performance, user needsand acceptance, dependence of human performance and safety on varioussystem or component features, and costs of83overviewvirtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.development, implementation, and marketing. furthermore, in order to facilitateconsistency across se projects, the federal government should help coordinatethe development of standardized testing procedures for use across studies,systems, and laboratories, particularly in those areas in which the private sectorhas not acted. these procedures should include methods for identifying keysystem dimensions that affect task performance, developing special metricsuniquely suited to evaluating ses, and comparing se system performance toperformance of other systems intended to meet the same or similar goals.suggestions for government policy and infrastructurethe magnitude, quality, and effect of the seoriented research anddevelopment that is accomplished will clearly depend on the role played by thefederal government. the current status of the se field is sufficiently embryonic,compared with what is likely to develop over the next 10 years, that the federalgovernment now has a rare opportunity to foster coherent planning in this area.furthermore, the recently established national science and technology councilat the white house would appear to be an appropriate organization to provideoversight for such a planning effort. also, in conducting such a planning effort,substantial benefits would be gained by attending carefully to the developmentsthat are already taking place in the other areas of the administration's planningeffortšfor example, the advanced technology program of the nationalinstitute of standards and technology, the high performance computing andcommunications program, and the programs associated with defense conversion.in this section, we discuss a number of mechanisms that illustrate the kindof leadership role that the government could play. we see that role as bothinforming and complementing the federal agencies' strategic planning for theirsupport of research and development programs.establish an effective information infrastructurea national information system that provides comprehensive coverage ofresearch activities and results in the se field in a userfriendly way to a widevariety of users could be a useful tool for promoting crossfertilization andintegration of the research and development efforts. the free flow of ideas andinformation among researchers, users, and individuals in government,academia, and industry who require information for se planning and decisionmaking is crucial to the development of this new field. also, in order todiminish the increasing threat of a major societal division between thetechnologically advantaged and the technologically deprived (as well as tocounter the current hype about virtual84overviewvirtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.reality), the public should have information of the appropriate type in an easilyavailable form. although information by itself cannot prevent such a division, itis a necessary ingredient of any program that could.we suggest that the federal government consider establishing a nationalinformation system in order to promote these vital communication goals. toreduce costs and to realize potential benefits as soon as possible, considerationcould be given to integrating the se information system with other publicinformation systems currently being developed. for example, such a systemmight be an ideal component of the national digital library based on highspeednetworking envisioned in the national information infrastructure (nii) initiativeof the clinton administration. issues of ownership and control, as well astechnological issues, will be important to consider in the design of an seinformation system.to some extent, the technology, procedures, and ideas being developedwithin the se field itself could be usefully exploited in the design of the seinformation system. such a system might eventually have uses well beyondthose initially envisioned; for example, it might include a library ofcomputational models. although for many years there has been a tendency forscientists to express their understanding of various systems and phenomena interms of computational models, this tendency is clearly being accelerated by therole such models play in the generation of ves. indeed, it seems possible that,in the near future, computational models will constitute one of the society'sprimary forms of knowledge representation. thus, for example, reading a bookabout newtonian mechanics is likely to be augmented by interacting with avirtual world based on a computational model that includes newtonianmechanics and then, perhaps, "reading" the computational model. the samekind of evolution is, of course, occurring with fiction and imaginary worlds;independent of whether a structure or a series of events is real or imaginary,much of the relevant information can be stored in the form of a computationalmodel. in order to make such computational models available to society, thefederal government might consider establishing a national system forstandardizing, collecting, storing, and disseminating such models. in view ofsociety's current concerns with health care, initial efforts in this area might befocused on computational models related to the structure and function of thehuman body and modifications of the human body associated with injury,trauma, disease, aging, and medical and surgical treatments.encourage appropriate organizational structures and behaviorstwo major factors that could inhibit advances in se involve the ability ofresearchers to communicate and cooperate across disciplines and85overviewvirtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.across organizations. because the creation of effective se systems requirescontributions from many different disciplines (with many different associatedcultures), special efforts are required to ensure adequate communication andcooperation across disciplines. similarly, because of the high value placed oncompetition within our society, special efforts are needed to ensure adequatecommunication and cooperation across government agencies, military branches,industrial firms, and academic institutions. at present, the organizational barrierappears to be more debilitating than the disciplinary (or cultural) barrier. in fact,the lack of cooperation among competing organizational entities (for example,competing companies) probably constitutes the main obstacle to achieving atruly satisfactory solution to the information infrastructure problem discussedabove. consideration of explicit incentives for cooperative behavior might bevery useful.in order to reduce these problems, the committee suggests that the federalgovernment consider establishing a small number of national research anddevelopment teams, each of which would focus on a specific application area.these teams could involve government, industry, and academia, as well as thevarious disciplines relevant to the given application area. funding could beprovided jointly by the federal government and the private sector.the work to be performed by each national research and developmentteam would include basic research, technology development, functionalprototypes, technology evaluation, and technology transfer to industry. despitethe emphasis on applications that is implied by how the teams are defined, thework could be directed toward longterm as well as shortterm goals, and thebasic research needed to achieve these goals would then be a priority forsupport. also, to the extent feasible, it might make sense to connect thesecollaborative teams or applications consortia not only to already existing federalactivities (as has been the case, for example, with the textile partnershipamtex that is being managed by the department of energy), but also toalready existing professional societies. in setting up these teams, the choice ofleadership for the activity will be crucial. in some cases, federal leadership maybe appropriate; in others, industrial; in still others, academic. finally, each ofthe envisioned teams might well find it appropriate to develop a powerfulnetworked communication system among its members to ensure truecollaboration at the working level.use se systems within the governmentit might be useful for some federal agencies and offices to explore the useof se to meet their own administrative and program needs. in addition86overviewvirtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.to the application of ses to the defense and space programs already under way,other application domains, such as training, telecommunication and teletravel,and information visualization, are relevant to the activities of many agencies.one way for the government to facilitate development of the se field would beto select a few agencies to serve as test beds for synthetic environmenttechnology in these general domains.there are a number of reasons for suggesting that the government makeuse of se systems in conducting its own activities. government agencies (localas well as federal) are natural early users of new technology: they could helpspearhead development efforts and provide feedback to the developers. also,such use could increase the costeffectiveness of government activities. inaddition, such use could create a market for se systems and thus stimulateprivate industry to become involved in the design and production of se systems.at present, uses within the government that are receiving the mostattention are those associated with the department of defense; however, otherentities, such as nasa and the department of energy, are also involved.although military applications trail behind those associated with theentertainment industry as an economic driving force, they neverthelessconstitute a force that is significant. this significance is derived not only fromthe overall magnitude of the associated economic activity, but also from thespecial role played by defense agencies in stimulating the development ofrelatively highquality systems for military applications. also of interest in thisconnection are the current efforts to explore the use of se systems in thedepartment of defense for education and training. if the results of these studiesare positive, they could play a significant role in stimulating the use of sesystems for education and training throughout the nation as a whole.the use of se systems in nasa appears to hold great potential not onlywith respect to training people for operations in hostile environments, but alsowith respect to performing the operations themselves. the enormous expenseassociated with manned space flights and space stations may well serve as astrong stimulus to the use of teleoperation in space activities.developing national standards and regulationsalthough it is probably too early in the development of se systems toestablish national standards and regulations, it is not too early to begin toevaluate the work already under way in connection with the formulation ofstandards and regulations for the telecommunications and entertainmentindustries. problems that are already of concern but are likely to become ofeven greater concern as the se field develops relate to technological87overviewvirtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.compatibility and interoperability issues, enforcement and control issues, andsocial and ethical issues. for example, in the technological area, problemsrelated to the timing of information flow in se networks merit specialconsideration. similarly, in the social and ethical area, the potential of se forproviding participants with powerful emotional experiences (including thoserelated to sex and violence) needs to be addressed.in general, it appears that se, because of its mass entertainment potential,is likely to become one of the largest uses of highspeed communicationnetworks, and its use should have an early and continuing part in thedevelopment of standards, regulatory principles, and tariffsetting models forsuch networks. the recent congressional attention that has been given to thekinds of material that are appropriate for the media to present is but a mildprecursor to the public debate that is likely to arise when advanced vetechnology becomes widely available.it will be critical for the federal government to consider ves in theformulation of national standards and regulations. studies could be undertakento illuminate issues related to technological compatibility and interoperability,enforcement and control, and social and ethical problems raised by the use ofves in society.analyze and evaluate market forces and societal impactthe extent to which government funds will be directed toward specific seresearch depends, at least in part, on the likelihood that such projects will befunded independently, i.e., by industry. estimating this likelihood requires notonly an analysis of current market forces, but also predictions of how marketforces will evolve in the future. although such predictions are notoriouslydifficult to make with accuracy, and market forces are as likely to be shaped bythe results of the research and development as they are to shape the research anddevelopment that is performed, failure to consider market forces in makingfunding decisions is likely to seriously reduce the extent to which the funding iseffective in advancing the field. for these reasons, it would be prudent for thefederal government to monitor market forces as part of developing its strategicplan for the allocation of scarce resources.as with most other technologies, the effects of the advances in se arelikely to be mixed; some effects will be positive and others negative. and aswith the predictions of market forces, although accurate predictions of societalimpact are difficult to derive, serious attempts to consider such factors would bedecidedly worthwhile. it cannot be assumed that all technological advances,even those that are likely to have substantial practical applications, willnecessarily be beneficial.88overviewvirtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.iiresearch and technology 89virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved. 90virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.the research and technology that is relevant to the se field covers anenormous range because of the many disciplines involved, the multimodalaspects of most se systems, and the wide variety of potential se applications.thus, the topics covered in this part are not allinclusive; they have beenselected because they were judged to be relatively crucial by the committee.the chapters in this part, the heart of the report, mirror the topics in theoverview. specifically, we present detailed information on psychologicalconsiderations (chapter 1) and on the available research and technologyinvolved in the creation of synthetic environments (se) including humanmachine interfaces (chapters 27), computer generation of virtual environments(ves) (chapter 8), telerobotics (chapter 9), networks (chapter 10), andevaluation (chapter 11). much of the material in chapters 27 applies to allkinds of systems (including augmentedreality systems). chapter 8 is directedspecifically toward ve systems, and chapter 9 covers teleoperator systems.it should also be noted that the visual channel is treated differently fromthe other channels and appears in more than one place. the material on thevisual channel in chapter 2 is restricted rather rigorously to humanmachineinterface issues. however, because most previous work by computer scientistson the computer generation of ves has focused on the visual channel (i.e.,graphics), chapter 8, which deals with these hardware and software issues, isnecessarily focused mainly on the visual channel. in order to obtain acomprehensive overview of the visual channel, it is necessary to read bothchapters 2 and 8.in contrast, for the auditory, haptic, and other channels, for which themajority of past work has been performed by individuals from other disciplinesand has been directly concerned with interface issues (or issues traditionallylumped together with such issues by these individuals), essentially all of therelevant material is contained within each chapter.it should also be noted that our descriptions of the various channels differin the extent to which previous knowledge on the part of the reader is assumed.for example, because it is expected that most readers are less familiar withissues related to haptic interfaces than those related to auditory interfaces, moregeneral information is provided.in principle, the chapter on networks (chapter 10) is relevant to both vesand teleoperators; however, most current activities in this area are directedtoward the networking of ve systems rather than teleoperator systems.furthermore, even within the domain of ves, relatively little attention is beinggiven to the communication of signals required for haptic interactions. thesefactors too, like those mentioned above, are reflected in the way in which thematerial is presented. 91virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved. 92virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.1some psychological considerationsbecause human beings are an essential component of all syntheticenvironment (se) systems, there are very few areas of psychology that are notrelevant to the design, use, and evaluation of se systems. for example, if thesystem under consideration is a virtual environment (ve) system that isintended to provide realistic simulations, then all the issues relevant to theidentification of the effective stimulus in real environments, as well as theissues that focus on how equivalent perceptions or responses can be achievedwith more simply synthesized artificial stimuli, must be examined. if it is a vesystem that is intended to maximize information transfer to the user andincorporates special distortions for this purpose, or if it is a teleoperator systemthat incorporates a nonanthropomorphic telerobot, then all the issues relevant tothe perception of, adaptation to, and learning about altered perceptual cuesystems must be considered. in addition, to the extent that the system can bethought of as an extension of traditional manual control systems, many of theconcepts and findings relevant to such systems are likely to be applicable.further issues arise in connection with higherlevel processes related to learningand the formation of problemsolving strategies and cognitive models, as wellas with the effects of se experience on affect, motivation, personality, etc. asimilarly broad range of issues is generated when one scans across the variousapplication areas of se systems.the topics covered in this chapteršwhich represent only a minute sampleof all relevant topicsšwere chosen to illustrate some of the types of issues thatneed to be considered. although the topic of discomfortsome psychological considerations93virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.obviously contains elements outside the domain of psychology, it is includedhere for convenience.resolution, illusions, and information transferperhaps the most obvious kinds of knowledge about human perception andperformance that are needed to design costeffective se systems concern theresolution of the human's input and output systems and the way in whicheffective resolving power is changed as these systems are integrated with seinterface systems having various kinds of displays and controls. (the termresolution refers here to the ability to separate out and independently sensedifferent signals as well as to detect small changes in isolated signals.) givensuch knowledge, one can then examine implications for task performance forvarious types of tasks, and the costperformance tradeoffs for these tasks.knowledge of normal human resolving power on the input side, i.e., thesensory side, allows one to predict the display resolution beyond which finerresolution could not be perceived and would therefore be wasted. a similarstatement holds for the output, i.e., control, side. although knowledge of humanresolving power in vision and audition is incomplete, it is sufficiently advancedto provide designers of se systems with solid background for design choices.areas in which current knowledge is considerably less adequate include boththe input (sensory) side and the output (motor) side of the haptic system, as wellas the ways in which performance is degraded when displays and controls (inany of the modalities) with lessthanhuman resolution are used. information onresolution for specific modalities (e.g., vision) is provided in the chaptersconcerned with these modalities.a further and related set of issues that is important to consider in thedesign of se systems concerns perceptual illusions. generally speaking, a givenperception is thought of as illusory to the extent that it appears to be generatedby a stimulus configuration that is different from the actual one. vesthemselves can be regarded as integrated sets of illusions. detailed study ofboth intrasensory and intersensory illusions is important because, in manycases, the existence of illusions enables se system design to be simplified andtherefore to increase its costeffectiveness. at the opposite end of the spectrum,the occurrence of unexpected illusions can seriously interfere with the expectedperformance of the system. elicitation of motion sickness often involves theoccurrence of illusions concerning the position, orientation, and movements ofvarious portions of the body.it is possible to regard certain types of illusions, such as the illusion ofcontinuous motion that can be generated by sequences of static images atsome psychological considerations94virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.rates of 30 hz, merely as reflections of imperfect sensory resolution andtherefore to assume that studies of resolution will automatically include studiesof illusions. however, other types of illusions, such as the mullerlyer illusion,are more appropriately characterized in terms of response bias and thereforecannot be regarded in this manner. thus, it is necessary to consider illusions asa separate issue from resolution.much of the past work on illusions has focused on the visual channel andon the implications of these illusions for theories of visual perception andcognition. however, some results, such as the continuous motion illusion justcited, clearly have direct implications for se design. other illustrative resultsrelevant to se design include those on the dominance of vision over auditionand haptics in cases of intermodality conflict (e.g., as evidenced in theventriloquist effect) and on the use of auditory stimuli to improve the perceptionof events that are represented primarily in the visual or haptic domains (as in theuse of sound effects). material on illusory effects for vision can be found inhoward and templeton (1966); for audition in bregman (1990); and for hapticsin loomis and lederman (1986), hogan et al. (1990), and fasse et al. (1990).it should also be noted that relatively little work has been done onsensorimotor illusions associated with wholebody movements. the factorsinvolved in these illusions, which usually involve the perception of bodymovement, support surface stability, and visual field stability, are likely to be ofconsiderable importance in se designs that include voluntary locomotionthrough virtual space. further material on these kinds of illusions can be foundin chapter 6.finally, it should be noted that the merging of data from different sourcesin augmentedreality systems is likely to lead to a whole new set of illusoryeffects that will require study. relatively little is known about the effects ofdifferent merging techniques (even if one restricts one's attention solely to themerging of visual images).issues related to information transfer rates tend to be very complexbecause such rates depend not only on basic resolving power, but also onfactors related to learning, memory, and perceptual organization. with respectto information transfer rate, an se system can be thought of as consisting of ahuman operator, an artificial machine (a computer or telerobot), and a twowaycommunication link consisting of displays that send information from themachine to the human operator and controls that send information from thehuman operator to the machine. one of the main goals in such systems is tooptimize the efficiency of communication in both directions. for manypurposes, it is useful to characterize the imperfections in the communicationchannels in terms of statistical variability (noise), to include in this noise bothchannel noise and noise internal to the human and/or the machine, and tomeasure the efficiencysome psychological considerations95virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.of the communication by the information transfer rate. crudely speaking, theinformation transfer is defined as the information gain resulting from thecommunication, which in turn can be defined as ''how much more the receivingsystem knows about the state of the transmitting system after thecommunication signal is received than before it is received." the informationtransfer rate is then defined simply as the rate at which information istransferred. within this context, a good humanmachine interaction technique isone in which the information transfer rate is high and the amount of trainingrequired to achieve this high rate is low. extensive background on the use ofinformation theory concepts in characterizing human performance is availablein quastler (1955), garner (1962, 1974), sheridan and ferrell (1974), stelmach(1978), and rasmussen (1986).in general, in order to get high information rates into the human operatorvia displays or out of the human operator via controls, the human operator mustbe very familiar with the information coding scheme employed. perhaps someof the coding schemes with which individuals are most familiar are thoserelated to language. estimates of maximum information transfer rates involvinglanguage reception and transmission in various modalities have been presentedby reed and durlach (1994). the results indicate that maximum rates forreading english (vision), listening to spoken english (hearing), and observingthe signs in american sign language are all roughly the same and lie in therange 6070 bits/s. reception of language in the haptic domain (by means ofgrade 2 braille, the feeling of signs in sign language, or by the tadomamethod, in which certain deaf and blind individuals receive speech tactually byplacing one's hand on the face of the talker and monitoring the mechanicalactions of the speech articulation system) shows maximum rates of roughly onehalf those obtained via the visual and auditory channels.the maximum output rate for the motor actions of the speech articulatorsin speech production is estimated to be roughly the same as the maximum ratefor listening to speech (60 bits/s) and for the motor actions of the hands intyping to be roughly 20 bits/s. to the extent that (1) the assumptions thatunderlie these results are valid (in particular, that the estimate of 1.4 bits ofinformation per letter for sentence length segments of speech is reasonable) and(2) these results provide an upper bound on the information transfer rates thatcan be achieved in communicating between human operator and machine in sesystems, the amount by which the results achieved with a given se system fallbelow these rates provides a measure of the room for improvement. it shouldalso be noted that the figure of 20 bits/s appears to provide an upper bound onthe rate that can be achieved in simple discrete spatial tracking tasks (e.g., onein which the transmitted signal consists of lighting up a randomly selectedsquare in a checkerboard array presented on a visual display and thesome psychological considerations96virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.correct response consists of touching the lit square with a finger or directingone's gaze at the lit square).unfortunately, although there are a number of general statements that canbe made about the properties of a good coding system (e.g., it should be wellmatched to the properties of the sensory or motor system involved, it shouldmake use of perceptually highdimensional stimuli or responses, it should havecognitive properties that make it easy to learn, etc.), there is no theory availablethat enables one to make reliable predictions of performance as a function of thedetailed coding scheme and detailed training procedures employed.these problems become even more challenging when one replaces theindividual human operator by a collaborative team. in such cases, the systemdesigner must also consider how to best break up the input information andoutput control among a number of individuals.manual control, tracking, and human operatormodelsin se systems, motor outputs of the operator are used to control thebehavior of a simulation or a telerobot. furthermore, these outputs usually takeaccount of feedback and occur in real time. in an important sense, therefore,such systems can be regarded as descendants of traditional manual controlsystems, with their emphasis on tracking paradigms and human operatormodels. the term manual control is taken to mean the receipt of sensoryinformation about the desired state of a system and its current state by a humanoperator and the use of that information by the operator to command inputs tothe system through mechanical devices (hand controllers, pedals, etc.) so as tominimize some function of the error between those two states (sheridan andferrell, 1974). the principal differences between current se systems andtraditional manual control systems consist of the increased variety andcomplexity of the displays and controls, as well as of the constraints imposed onthe relevant transfer functions.research in manual control was most active in the 1950s, 1960s, and1970s due to the interest in operator control of aircraft, automobiles, and othervehicles (a comprehensive overview is found in sheridan and ferrell, 1974).additional useful references include jex (1971), kleinman et al. (1974), andmcruer et al. (1965). in manual control systems, there is a closed loop thatincludes the behavior of the human operator (human operator dynamics), thesystem or plant being controlled, and feedback to the operator regarding theperformance goals and plant state. classification of manual control systemsaccording to the type of input to the human operator is as follows.some psychological considerations97virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved. compensator the operator controls the system to reduce the errorbetween the state of the system and a fixed reference state. pursuit the operator controls the system to reduce the error between thestate of the system and a changing reference state. preview the operator, having knowledge of future values of the referencestate, controls the system to reduce the error between the state of thesystem and a changing reference state. precognitive the operator, having foreknowledge of input in terms otherthan a direct view (for example, statistics on the input), controls thesystem to reduce the error between the state of the system and a changingreference state.human operator dynamics influence the closedloop performance inmanual control systems as much as do the plant dynamics. for example, thecontrol of aircraft cannot be predicted unless a model of the pilot is factored inas well (sheridan, 1992c). initially it was thought that a human operator modelindependent of the manual control task could be formulated; however, researchshows that, depending on the controlled process, the behavior of the humanoperator is modified to achieve satisfactory performance. a number of modelsof the human operator of varying complexity have been proposed; the morewellknown of these are listed below. linear, quasilinear models the simplest is a linear model of the operatorwith adjustable gain and a remnant (noise). these models have beenfurther extended to handle reactiontime delay (sheridan and ferrell,1974). crossover model experiments by elkind (1956) and mcruer et al. (1965)show that operator behavior was dependent on the error signal; theoperator was found to change his dynamics so that the combined systemhad good servo behavior at crossover frequency. these results were validfor compensatory tracking tasks but were not valid for highbandwidthtasks or control of highorder linear and highly nonlinear systems. optimal control model these are another class of models of the humanoperator in manual control tasks based on results obtained fromcompensatory tracking experiments in which the operator is modeled asan optimal controller within limits of internal constraints and knowledgeof task objectives (bryson and ho, 1975).as human manual control behavior became better characterized and therole of the human operator in teleoperation changed to performing complexmanual and decisionmaking tasks in unstructured environments, researchshifted to modeling the higherlevel functions performed by the operator (baronet al., 1982; kok and van wijk, 1977; rasmussen,some psychological considerations98virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.1983; sheridan, 1976, 1992c). current related work has focused on theincorporation of manual control models of the operator in the characterizationof the human operator as a supervisor and decision maker.subsystem decomposition of the human operatorthe human operator models mentioned above are black box models basedon control theory formulations. the original ideal of developing humanoperator models independent of the task is still a good one; however, thisrequires a detailed understanding of cognitive, perceptual, and motor functionsthat is still far from complete. a subsystem decomposition of the humanoperator for pursuit tracking based on physiology would include the followingelements (jones and hunter, 1990): sensory processing, in which the central nervous system (cns) measurestracking error (actual minus desired target position) either visually orkinaesthetically; cognitive processing, in which the cns generates appropriate centralcommands to the motor neuron pool in the spinal cord; excitation/contraction coupling (muscle), which reflects the propagationof action potentials through muscle as a result of efferent nerve input andthe initiation of contractile mechanisms (crossbridges), which producemuscle force; limb mechanics, in which muscle force generates limb displacement; and reflexes and nerve delays in which displacements of the limb excitemuscle spindles that feedback to the motor neuron pool in the spinal cordover afferent nerves and sum with the cnsgenerated motor neuron inputto produce muscle activation over efferent nerves.each subsystem contributes its own dynamics to the overall humanoperator dynamics. delays associated with each subsystem are different.whereas the transformations are linear and timeinvariant for some subsystems,they are nonlinear and timevarying for others. for the neuromuscularsubsystems, recent advances in stochastic system identification techniques nowmake it possible to determine the dynamics of each subsystem for everyindividual operator (kearney and hunter, 1987).methodologically, the dynamics of the sensory and cognitive subsystemsare what remain when the dynamics of all the other subsystems have beensubtracted from overall human operator performance. of course, modeling thecognitive aspect is the most difficult and incomplete. certain aspects of themodeling of the sensory attributes are discussed in chapter 1 for vision,chapter 2 for audition, and chapter 3 for mechanical interface variables. inparticular, a knowledge of the sensorysome psychological considerations99virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.resolution limits for length, force, stiffness, viscosity, and mass is important forunderstanding human operator performance and for design of haptic interfaces(jones and hunter, 1992; see also durlach et al., 1989; pang et al., 1991; tan etal., 1992, 1993). increasing the stiffness of a manipulandum decreases theresponse time but also the accuracy (jones and hunter, 1990) while increasingthe viscosity decreases the delay and the natural frequency of the humanoperator (jones and hunter, 1993). this enhancement of performance with theaddition of stiffness and viscosity to the manipulandum may be due to anincrease in proprioceptive feedback from the periphery.sensory substitutionan ongoing area of research is the substitution of feedback to alternativesensory channels of the operator, that is, presentation of sensor information tothe operator in a sensory channel other than that in which it was sensed by theteleoperation system. there are a number of reasons for choosing sensorysubstitution: shielding the operator from hazards but still conveying information on theconditions of the environment, for example, in chemical spill, hightemperature, and radioactive environments. presenting sensor information in visual form, for example, dials, gauges,etc., for lack of other suitable choices. using the higher sensitivity available in the alternative operator sensorychannel, for example, the representation of temperature may be to tenthsof degrees on a visual display; this is far above the ability of the operatorto discriminate when sensing actual temperature. reducing the cognitive load on the operator. overcoming the drawbacks of force reflection due to instabilities whenoperating with time delay. additional reasons are to reduce the size ofhand controllers and eliminate reaction forces on the operator.reports of research in this field have concentrated on sensory substitutionof force reflection. some early work in this area was that of bertsche et al.(1977) and bejczy (1982) on the use of visual displays of force feedback. morerecently, massimino and sheridan (1993) documented the use of tactile andauditory displays for force feedback. there are many more applications yet tobe tested, particularly in situations in which data from different types of sensorsare to be fused into a single measure of interest. further results on sensorysubstitution are available from studies concerned with aiding individuals whoare deaf or blind or deaf and blind (e.g., bachyrita, 1972, 1992; reed et al.,1982; warren and strelow, 1985; reed et al., 1989).some psychological considerations100virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.in addition to sensory substitution, the provision of visual aids and hapticaids may enhance operator performance (see chapters 8 and 11 for furtherdiscussion).the subjective sense of telepresenceone major feature of a user's experience when operating an se systemconcerns the extent to which the user is immersed in, and actually feels presentin, the remote or synthesized environment, i.e., the extent to which subjectivetelepresence occurs. whereas objective telepresence refers to the use ofteleoperator technology for sensing and manipulating remote entities, subjectivetelepresence refers to the sensations and perceptions experienced by the user.for simplicity, in the following remarks on subjective telepresence, the modifiersubjective is dropped and we use simply the term telepresence. also, becauseessentially all the issues with which we are concerned here are independent ofthe distinction between ve and teleoperation, in the following remarks weignore this distinction; the term telepresence is used in connection with all typesof se systems. (those wishing to make such a distinction might use the termvirtual presence for ves, as suggested by sheridan, 1992a.)at present, there appear to be three main questions of interest in relation tothe concept of telepresence: how should telepresence be defined operationally?how can one create telepresence? what is telepresence good for?although there have been a number of discussions of telepresence in theliterature (e.g., akin et al., 1983; fontaine, 1992; heeter, 1992; held anddurlach, 1991, 1992; loomis, 1992; pepper and hightower, 1984; schloerb,1994; sheridan, 1992b; steuer, 1992; zeltzer, 1992), there is still no generallyagreedon operational definition of telepresence. it should be noted, however,that a serious effort in this direction has recently been initiated by schloerb(1994) and that a number of individuals are now attempting to conductempirical research on telepresence.almost all of the articles just cited attempt not only to define telepresence,but also to identify the factors that play a role in the creation of telepresence.although the particular set of factors considered or emphasized varies with theauthor's viewpoint, there are a number of factors that are relatively obvious.one set of such factors concerns the exclusion of stimuli originating in theimmediate environment. in other words, the sense of telepresence is likely to bereduced if the operator is constantly reminded of his or her presence in the realenvironment by stimulation originating in this environment. such stimulationcan arise from sources outside the system (e.g., the auditory component of thehumanmachine interface provides inadequate attenuation to prevent theoperator of thesome psychological considerations101virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.se system from hearing a door slam in the room that houses the se system) orfrom the system itself (e.g., the helmet used for the visual display is toointrusive to be ignored).a second set of such factors concerns the existence of userpredictableinteractivity. telepresence is likely to increase when the user's actions, and theconsequences of these actions as represented by the subsequent stimuli sensedby the user (i.e., the feedback), constitute a rich and easily perceived andinfluenced interaction pattern. when the synthetic world is highly realistic, suchconditions will be satisfied automatically. when it is unrealistic, the extent towhich they are satisfied will depend on the extent to which the user can adapt tothe new world. as the user adapts, the degree of telepresence (and thetransparency of the interface) will generally tend to increase. the extent towhich the user can adapt to the new world, however, will depend strongly onboth the nature of this world and the nature of the user's exposure to it. if theworld is incomprehensible (either because the relations between the user'sactions and the effects of these actions are random or simply because they areso complicated that they appear random), adaptation will not occur. furtherdiscussion of adaptation is presented in the next subsection.a third set of such factors relates to higherlevel, more cognitive featuresof telepresence and to similar experiences that occur outside the domain of setechnology. in general, the ability of humans to be transported into unrealworlds is the basis of most art, literature, theater, and entertainment, not tomention hypnosis and the use of hallucinogenic drugs. the extent to whichconsideration of these other forms of transportation will prove useful in thestudy of se telepresence is not yet clear. it does appear likely, however, that thevariation among individuals in their susceptibility to transportation by theseother methods is likely to also occur with se telepresence.the question "what is telepresence good for?" has not yet been adequatelyanswered. the interest in the concept of telepresence is due in part to intrinsicphilosophical and scientific interest in issues concerned with reality andillusion. it is also due in part, however, to an implicit assumption that a highdegree of telepresence is positively correlated with good performance. that thisis not generally the case, however, can be easily demonstrated merely by notingthat one of the primary motivations for the use of teleoperator systems inhazardous environments is to prevent the operator from experiencing noxiousstimuli present in the real environment (i.e., reducing the sense of presence inthe real environment). in general, the relationship between telepresence andperformance has not yet been determined. furthermore, even if telepresencewere adequately defined in an operational sense, and even if it were determinedusing this operational definition that in most situations telepresence andsome psychological considerations102virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.performance were highly correlated, it still would not be clear that the conceptof telepresence has practical significance. in particular, it is not clear that itwould enable one to design better se systems. in order for this to be the case, itwould be necessary to show that models and measurements of telepresence canbe usefully substituted for models and measurements of performance, or at thevery least, that models and measurements of telepresence provide significantadded value to the results that can be achieved solely through the use of modelsand measurements of performance.alterations of sensorimotor loopsin practically all se systems, the human operator's normal sensorimotorloops will be altered by the presence of distortions, time delays, and noise(statistical variability) in the system. in many cases, such alterations will beintroduced unintentionally and will degrade performance. for example, timedelays may result from the need to communicate over long distances, and timedelays, noise, and unwanted distortions may result from the inclusion ofimperfect system components. in some cases, however, these alterations(specifically distortions) may be introduced intentionally in an attempt toachieve performance that is better than normalšfor example, in a teleoperatorsystem that incorporates a telerobot that is intentionally nonanthropomorphic.because of the lack of isomorphism (i.e., structural and functional similarity)between the operator and the telerobot in such systems, the mapping betweenthe human operator and the telerobot will necessarily result in alteredsensorimotor loops for the operator. similar conditions will exist in all virtualenvironment systems in which special features of the environment areartificially emphasized and unrealistic methods for interacting with thisenvironment are employed in an effort to achieve superior task performance.attempts to achieve improved resolution by magnification of perceptual cuesrepresent only one line of investigation in this area.independent of the nature and origin of the alteration, in order for a systemdesigner to predict the performance of a candidate system, theoretical modelsmust be available for characterizing human responses to the alterationsassociated with the use of the system. such models should be able to predict theeffect of the alterations on such variables as simulator sickness (and also,perhaps, telepresence), as well as on objective task performance, and to describehow the various response components change over time due to sensorimotoradaptation and learning.although considerable work has been performed in this area (e.g., see theextensive review by welch, 1978), there are as yet no adequate modelsavailable for predicting performance. for example, no adequate modelssome psychological considerations103virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.are available for specifying the amount of sensorimotor adaptation that isachievable with different kinds of distortions using different types of trainingprocedures. similarly, no attention has been given in the research on adaptationto changes in resolution; attention has been focused almost exclusively onchanges in response bias (i.e., the deviation between the mean response and thecorrect response). furthermore, with only minor exceptions, interactions amongdifferent kinds of alterations (distortions, time delays, and noise), many ofwhich are likely to be present simultaneously in se systems, have been ignored.a few modalityspecific comments on sensorimotor adaptation are available inchapters 2, 3, and 4. extensive further discussion of sensorimotor adaptation inthe context of wholebody motion is available in chapter 6.discomfortan important prerequisite for widespread use of se systems is that they becomfortable for people to use. independent of whether the discomfort caused bythe system is most appropriately considered under the heading of "motionsickness," "poor ergonomics," or the ''sopite syndrome" (see chapter 6 for adefinition of this syndrome), such discomfort must be reduced sufficiently topermit individuals to make effective use of the system over extended periods oftime. despite significant previous research on some components of thisproblem, substantial further research in this area is warranted for a number ofreasons.first, as the situation now stands, discomfort is a real threat to the effectiveuse of ses. for example, quite apart from the deficiencies in currently availablehelmetmounted displays with respect to the visual information provided, theytend to cause such a high degree of discomfort that daily longterm use seemsalmost out of the question. in fact, the combination of relatively limited visualinformation and relatively high discomfort is leading some individuals toseriously consider using offhead displays in their se systems (discussion ofboth helmetmounted displays and offhead displays is presented in chapter 2).second, past work on the sources and effects of discomfort has not yetresulted in adequate understanding of the phenomena involved. morespecifically, we do not yet know how the magnitude of each discomfortcomponent depends on the characteristics of the system (the properties of thevisual and auditory displays, the weight of the devices mounted on the head, themethod by which movement through space is simulated, etc.) or on thecharacteristics of the individual user (including the user's prior experience withthe system). although some progress has been made in related areas (e.g.,studies of motion sickness conducted in connection with flight simulators,sophisticated use of anthropometric measurementssome psychological considerations104virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.for cockpit design), this progress has not yet led to adequate comfort for seusers.third, many of the situations created by evolving se technology are new;the stimulusresponse configurations to which individuals are exposed in sesystems are often not covered by those previously studied. in other words, theincreased flexibility associated with the new technology provides us withopportunities not only to perform certain tasks with increased costeffectiveness, but also to expose ourselves to situations that exhibit new kindsor magnitudes of discomfort.fourthšand illustrative of the point made elsewhere in this report aboutthe use of se systems as basic laboratory facilities for psychological researchševolving se technology provides us with new tools to study some of theseissues. not only is work in this area essential to the realization of practicalapplications, but it should also advance our understanding of the humanorganism.further material related to the discomfort issue is available in laterchapters. in particular, extensive discussion of motion sickness and the sopitesyndrome appears in chapter 6.learning and problem solvingunderstanding how humans learn and solve problems is critical to thedevelopment of educational and training systems regardless of the nature of theinstructional tools employed. ve is one such tool, and the more we know abouthow the mind works, the better able we will be to create experiences thatfacilitate learning by its use. thus, in general, an appreciation of humancognition is an important element in using synthetic environment technology toalter human behavior.work in the development of cognitive models has a long history. as earlyas 1932, bartlett developed the notion of a schemata as a large knowledgestructurešthe basic unit of memory and thought. schemata are conceived toexist at all levels of abstraction and to be hierarchically organized andinterrelated; they are used to comprehend new and complex situations and are,in turn, modified by experience. this concept was revived in the late 1960s bycognitive psychologists and computer scientists who used it as a paradigm totest hypothesis about human mental processes.a similar concept was brought into use by computer scientists who werealso concerned with modeling cognitive processes. they used the term framesto describe the organization, structure, and developmental process of humanmemory (minsky, 1975; kuipers, 1975). much of their work involved creatingcomputer programs that acquired knowledge, followed procedures as describedin scripts, and solved problems in wayssome psychological considerations105virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.that were hypothesized to mimic human problemsolving strategies (newell andsimon, 1972). the results of these studies led to new hypotheses and more fullyelaborated theories.more recently, researchers have used cognitive models to developintelligent tutors. one of the more longstanding research efforts in this area isthe work of john anderson and his colleagues at carnegie mellon university inwhich the act (adoptive control of thought) theory of learning and problemsolving was used to build intelligent tutors in algebra, geometry, and lispprogramming language. the act theory makes a distinction between factual ordeclarative knowledge and procedural knowledge. declarative knowledgeinvolves the acquisition of facts (the content of a theorem); proceduralknowledge in the form of production rules relates to the development ofcognitive skill (the ability to apply a theorem). the early stages of learning aredominated by declarative knowledge; the later stages by procedural knowledge.according to a recent review by anderson et al. (1993), the 10year effort hasled to further understanding of human cognition as well as to an appreciationfor how to implement the system in the classroom. one important finding wasthat the original conception of tutoring as a process of human emulationchanged to the notion of a tutor as a learning environment in which helpfulinformation can be provided and useful problems can be selected.another important line of research in cognitive science is modeling theknowledge structures and judgments of experts and novices and comparing thetwo as a basis for understanding the nature of expertise and for training novicesto become experts. for example, chi et al. (1981) have examined thedifferences in the knowledge structures and problem approaches of expert andnovice physicists as a way to better understand how the acquisition ofknowledge and rules changes problemsolving strategies. the schema,algorithms, and heuristics used by experts were explicated by using suchmethods as cognitive task analysis or thinkaloud protocols (newell and simon,1972). according to glaser et al. (1991), several knowledge modelsrepresenting various stages in moving from a novice to an expert would beuseful in guiding the learning process.using cognitive task analysis, lesgold and his colleagues have describedvarious stages of expertise in electronic troubleshooting as a basis fordeveloping dynamic assessments of learner competence. the resultingcomputer system, known as sherlock, uses information on the stages ofexpertise to track a learner's performance, to diagnose strengths and weaknessesin both knowledge and process, and to provide corrective feedback (lesgold etal., 1990; lajoie and lesgold, 1992).although cognitive researchers have made considerable progress insome psychological considerations106virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.understanding how the mind works, there are still many issues to be examined.specifically, we need to know much more about what aspects of syntheticenvironments will facilitate learning. research is needed to further understandthe relationship between content, types of tasks, the individual's knowledgestate, and preferred information presentation features. moreover, it will beimportant to examine the special contributions of immersive environments thatare feasible through the use of ve compared with other formats for enhancingeducation and training. of particular interest here is the opportunity forextensive sensorimotor involvement provided by ve systems.motivationa question that cuts across the objectivesubjective boundary is whetherimmersive environments contain intrinsic advantages with respect to motivationor incentives to participate in entertainment and educational experiences. again,it is too early for definitive answers. however, anecdotal reports suggest thatyoung people can become so immersed in computerbased group games thatthey neglect such basic activities as eating and sleeping. in education, neuman(1989) has shown that young people with learning difficulties have taken tocomputerbased instruction with enthusiasm that lasted longer than could beattributed to a novelty effect. observations suggested that the children weretransforming the otherwise ordinary lessons into competitive games. for youngpeople, computers and game playing seem to be conceptually linked. perhapsthis link can be constructively exploited by giving ve instructional experiencesa suitable game like quality to strengthen student engagement.in more recent studies, another explanation for student enthusiasm hassurfaced (coombs, 1993; morrison et al., 1993; coldevin et al., 1993; dubriel,1993). according to this explanation, the computer appears to empowerstudents who are otherwise educationally disadvantaged. the promise of ve isthat its effects should be stronger than those of other computerbasedinstructional facilities. for example, students with physical handicaps could begiven experiences via ve that they otherwise could not have. likewise,students in remote settings could participate in experiences otherwiseimpossible to obtainšsuch as a stroll through a big city. finally, ve might bean ideal means for simulating certain types of experiences for which there areemotional, political, or economic barriers. for example, animal rights advocateshave made a political issue of the dissection of animals for biology instruction.ve could provide a means for learning anatomical details that would be free ofnegative emotional overtones and might be more economical.some psychological considerations107virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.although some skeptics have argued that most of the observed advantagesattributed to all forms of computerbased instruction derive from noveltyeffects, there is some additional evidence to the contrary. introspective reportsfrom adult learners confronting difficult material (vasu and vasu, 1993) andfrom teachers for whom the novelty had certainly worn off after repeated use ofthe same courseware (novelli, 1993) indicate a wellsustained sense ofcontinued interest in using computerbased resources.attitude, opinion formation, and personalityeffectsfrom both scientific and social perspectives, problems exist with respect tothe possibility that ve might change individuals' attitudes or selfperceptions.the most credible hypotheses are that such powers are limited by both technicaland psychological factors. with regard to the psychological aspect, changes inattitudes and opinions are seen to be most effectively brought about by personto person interactions. for example, it has long been known that personalcontact plays a major role in expediting changes in the attitudes that controlacceptance or rejection of innovations. likewise, of the development of a senseof self appears to depend strongly on social transactions (e.g., mead, 1934). inother words, one's idea of selfšincluding selfregard or selfesteemšdependson what messages are sent day after day by a set of significant others. we donot yet know the extent to which and manner in which such social transactionschange as the world in which the person is operating becomes more imaginary.this emphasis, in both attitude formation and selfconcept development,on social processes highlights specific technical properties of the ve situation.that is, how cost effectively can ve generate images of humans who caninteract realistically with the subject? in particular, can ve generate images ofspecific significant others, such as family members? and how rigidly can ormust such images be programmed in advance? although social interactionmight not be absolutely necessary to engender attitude change, such changemay be greatly facilitated by social persuasion.finally, in the domain of personality development, there are possiblechanges that might be situationally but not socially induced. a good example isrisk tolerance. a subject could be exposed via ve to situations in which normalphysical laws do not operate, as in the mode of wiley coyote in a roadrunnercartoon. after experiencing in a ve situation any number of events whereby theaction of gravity was significantly delayed and the ultimate consequence ofbeing hammered down by two tons ofsome psychological considerations108virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.rock was only a slight dizziness, would one tend to act less cautiously in the realworld? the answer, based again on arguments by analogy, appears to be thatsome such shifts in perception are possible, but only if the conditions are tightlycontrolled. one crucial parameter to be controlled is task ambiguity. researchon effects of social influence (kidd, 1958) shows that some effects can beinduced very quickly if the situation is ambiguous and there are no seriousconsequences for making the wrong choice.another potentially crucial parameter is the initial attitudes of participants.if they are young, inexperienced, and uncertain about underlying eventprobabilities or vulnerable to certain forms of peer pressures, change can bereadily induced. such subjects could possibly be more influenced by elaboratelycontrived experiences in a ve situationšagain, particularly if the ve systemfabricated specific images of other people, such as important peers (sjoberg andtorell, 1993; benthin et al., 1993).finally, there is some anecdotal evidence that computer roleplayinggames and video games that portray violence may have some influence onindividual attitudes and behavior (much of this preliminary research grows outof the efforts to demonstrate the influences of television on attitudes andbehavior). for example, there are cases of individuals becoming so involved inplaying multiple user dungeons (a computer fantasy game) that they leavelittle or no time for other activities. a recent article in the washington post(schwartz, 1994) described a college student who flunked out of college andstayed up almost all night every night to play a fantasy character and interactwith other fantasy characters in the multiple user dungeons game. basically,this young man lost his real identity to a character in a game.with regard to violent video games, there are some recent studies thatsuggest that children who play these games are more aggressive as a result.some preliminary studies in this area include fling et al. (1992), funk (1992),and cesarone (1994). these studies are based on questionnaire and surveyresults rather than empirical evidence of changes in performance. nevertheless,they suggest that further attention should be given to the potential effects ofsuch games, particularly as these games become more realistic and moreinteractive.in summary, ve could probably be used to engender substantial changes inthe psychic structure of participants. the magnitude of such changes willdepend on the quality and types of images that the ve system can generate, thecongruence of the inclusion of some form of controlled message content in theve programming, the initial susceptibility of participants, and the ongoingwillingness of such participants to accept messages or situations that appear tocontradict or deviate from their other, nonve experiences.some psychological considerations109virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.research needsmany of the needs in the area of psychological research, implicitlyoutlined in the above discussion, will be automatically satisfied as the field ofexperimental psychology follows its normal evolutionary course ofdevelopment. without special effort, however, much of the information andunderstanding required to guide evolution of the se field will be available toolate to be useful. in order to significantly increase the costeffectiveness of these research and development work, as well as to determine the likelypsychological effects of heavy se usage before these effects are prevalentthroughout the society, substantial work must be done within the next few years.as indicated previously, the goals of this work should be to develop (1) acomprehensive, coherently organized review of human performancecharacteristics from the viewpoint of se systems; (2) a theory that facilitatesquantitative predictions of human responses to alterations in sensorimotor loopsthat are likely to occur in se systems; (3) cognitive models that will facilitateeffective design of ve systems for purposes of education, training, andinformation visualization; and (4) increased understanding of the possibledeleterious effects of spending substantial portions of time in se systems. theimportant issue of user comfort is partially addressed by item 2 in that feelingsof discomfort such as those associated with simulation sickness constitute aparticular type of response to alterations of sensorimotor loops. many otheraspects of discomfort, such as those related to poorly fitting helmets for visualdisplays, are best thought of purely in terms of physical effects.some psychological considerations110virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.2the visual channelthe visual interface will, in many cases, provide the human user with themost salient and detailed information regarding the synthetic environment (se).given current technical limitations on the realtime display of detailed,continuousmotion imagery, making optimal use of normal visual sensorycapabilities constitutes a formidable challenge for system designers. of themany display options that currently exist, none is completely adequate across allapplications.status of the relevant human researchit is clear that the limits imposed by technological constraints onstimulation interact with the informationprocessing capabilities of the humanobserver. consequently, the effectiveness of current technology must beassessed against knowledge of the full range of human sensory capabilities. aproper understanding of both the technological and sensory aspects of visualdisplays defines the challenge for subsequent research and development (boff etal., 1986; haber and hershenson, 1980; sekuler and blake, 1990).an effective visual interface provides an appropriate match of theparameters of stimulation to the characteristics of the sense organ. theproduction of visual stimuli in such a system depends in part on technologicalcapabilities for monitoring movements of the observer (to generate bothmovementcontingent and noncontingent sensory information), graphicsprocessing power, and display characteristics. for a visual interfacethe visual channel111virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.to be effective in an se context, the sensory information need not exceed, andmay often be less than, the sensoryprocessing capabilities of the averagehuman observer. the latter observation is borne out by the experience of peoplewith deficient vision (or hearing): people with many kinds of sensorydisabilities can still experience both real and virtual environments (ves).indeed, perturbations of normal visual stimulation may be disturbing but notnecessarily destructive of perception in either real or synthetic environments.when sensory conditions are maintained with minimal variation, various formsof adaptation can occur and can be used to advantage in overcoming technicallimitations (held and durlach, 1991).serelevant aspects of visual system organizationthe superficial similarity between the human visual system and createdimaging systems (such as a television camera) has the potential to misguideefforts to advance the state of the art of the visual channel of se systems. oneexample of a fundamental difference between the design goals of the twosystems lies in the manner in which images are collected. created systemsgenerally strive to collect a uniformly resolved image of a scene. forapplications in which any area of the scene might be attended to (by theultimate viewer), this is an appropriate goal. however, the ideal proximalstimulus in an se system is one in which the information is presented in amanner that is complementary to the normal operation of the sense organ. in thecase of the eye, an optimal system would not ignore the role of eye motions, theuneven distribution of photoreceptors in the retina, and the limitations of theeye's optics (westheimer, 1986). because the eyes are used to actively probe ascene (gregory, 1991) in a way somewhat similar to an insect probing the worldwith antennae, the concentration of image quality at the center of the fixationpoint may be a highly appropriate use of resources. in a sense, the fovea is likethe sensitive tip of a blind person's cane; the rest is just context.although these concepts have begun to be addressed by designers of eyeslaved displays, the design of the visual channel of se systems is likely tobenefit from more widespread conceptualization of the eye as an output deviceas well as an input device. there are a number of potential benefits of utilizingsuch fixation information. it can guide the allocation of resources by matchingthe information transfer rate to the receiver (e.g., high resolution can be limitedto the fovea). it can reduce the computational burden of image generation in vesystems. the fixation point information collected can be used to adapt theinformation presented (in the case of teleoperator systems) or generated (in thecase of ve systems) to more seamlessly match the interests of the viewer.the visual channel112virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.the effects of spatiotemporal blur can be managed using fixation pointinformation. in this case, the analogous phenomena of display persistence (dueto the time constants of the imaging device) and retinal persistence or smear(due to the time constants of the receptors and lightadaptation mechanisms)have different perceived outcomes based on the mode in which they are used.direct view of a panoramic display requires that moving images present in thedisplay be carefully managed so that attention is not directed to motion blur.this usually involves creative application of cinematographic techniques(spottiswoode, 1967). the human observer, however, remains largely unawareof motion blur because of the links between eye motion, retinal and higherlevel"suppression," and the perceptual processing involved in active viewing of ascene (matin, 1975; matin et al., 1972). thus, rapid target motions that elicitsaccadic (ballistic) eye movements are not perceived as blurred because visualsensitivity to the target is reduced during the period beginning about 50 msbefore the saccade is initiated to 50 ms after the new fixation point is reached(latour, 1962). slower target motions in the range of 5 to 40 deg of visual angleper second resulting in pursuit eye movements are much less subject tosuppression effects, thus presenting a challenge for the display designer.in the visual system there are many such opportunities to achieve the dualgoals of more closely accommodating the sense organ while allocatingtechnological resources economically. the development of color headmounteddisplays (hmds), for example, has presented difficulties due to the need topresent three channels of chromatic information. examination of thephotoreceptor population of the retina reveals first that the colorsensingreceptors (cones) are not evenly distributed throughout the retina. the greatestconcentration of cones is found where the visual axis intercepts the retina, butbeyond 10 deg of visual angle the cone density is uniform at about 5 percent ofthe central value. moreover, the noncolorsensing rod population at this interceptpoint is zero but increases with eccentricity to a maximum at 18 deg. thus rodsare present in far greater density throughout the periphery than are cones, withthe result that the resolution of color information is limited in the periphery(hood and finkelstein, 1986).cones come in three general types as defined by the photopigment present.by measuring the differential response of the three cone types, the visualsystem is able to determine (within a metameric equivalent) the wavelength ofstimulation. but unlike the case of created imaging systems, the quantity anddistribution of the different cone types is not uniform. overall, the ratio of short(s), medium (m), and long (l) wavelength peak cones is about 3:7:14. shortcone representation in the retina is relatively sparse, drastically limiting theresolving power of the eye onthe visual channel113virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.this color dimension. (it also follows that the foveal vision system isparticularly dependent on the medium and long cones and relativelyindependent of the short cones and rods.)these nonhomogeneities provide another opportunity to match the needsof the visual system while achieving technological savings. since the bandwidthfor color information is not uniform throughout the retina, the imagegeneratingcomponent of the display system is relieved of a technological constraint.there are about 120 × 106 rod and 6.5 × 106 cone photoreceptors per eye.this represents an extraordinary amount of information to be processed by thevisual system. in order to accommodate this flow of information, initial imageprocessing occurs at the retinal level. the mosaic of receptors is not connectedonetoone with the optic nerve fibers. in fact, there are only about 1 × 106fibers leaving the eye that contain the codified information from the over 126 ×106 photoreceptors. intermediate cells (horizontal, bipolar, and amacrine) makeexcitatory and inhibitory connections to groups of receptors in various spatialconfigurations. these connections result in receptive fields that preferentiallyrespond to retinal excitation of specific spatial configurations and temporalqualities. (the duplex nature of the retina alluded to earlier also becomesevident in the connectivity pattern: rods are part of one network and the variousclasses of cones are part of another, with minimal synaptic communicationbetween the two.) once again we find that the visual system allocates thesmaller, higher spatial resolution channels to the fovea.the rather long axons of the retinal ganglion cells constituting the opticnerve regroup into left and right optic tracts containing left field or right fieldinformation from both eyes. about 20 percent of these fibers split off from hereto make synaptic connections at the superior colliculus (sc), while theremaining 80 percent terminate in the lateral geniculate nucleus (lgn). (of theganglion cells destined for the lgn, there are two classes: p ganglion cellsmake connections to parvocellular areas and m ganglion cells make connectionsto the magnocellular areas of the lgn.) these connectivity patterns yield twoimportant insights to the functioning of the visual system that have relevancefor the display designer.the primary flow of information through the lateral geniculate nucleus isfrom the retina to the visual cortex. but, in addition to the initial processing ofmotion, color, etc., described above, the lateral geniculate nucleus also serves asa modulator of visual stimulation. the thalamus, of which the lateral geniculatenucleus is a part, generally serves as a sort of volume control for sensorystimulation. (there are corresponding nuclei of the thalamus that provideanalogous modulatory control for most other senses.) in this capacity, feedbacksignals from the reticular activatingthe visual channel114virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.system provide control that is based on the general level of arousal, whereassignals from the visual cortex help direct visual attention.the first insight based on the postretinal connectivity patterns concerns theemergence of separate temporally acute and spatially acute processing channels.these channels roughly correspond to the periphery and fovea, respectively.there are six layers observable in the structure of the lgn: four outer layersmade up of small cells (parvocellular layers) and two inner layers containinglarge cells (magnocellular layers). the parvo cells, although slower, processfiner details in the image and support color perception (opponent colorconnection patterns, etc.). the colorblind magno cells respond quickly and areinvolved with motion perception.the disproportionately large representation of the fovea, along with the''receptive field" neural coding of higher visual features (including moreabstract visual characteristics such as orientation and contour) repeatsthroughout the cortex and is a basic mechanism of information extraction in thevisual system (graham, 1989). these representations may have implications forcreating appropriate image compression and generation algorithms in sesystems.a second insight (based on connectivity patterns) that should be morecarefully studied and exploited involves the ambient visual system. asmentioned above, 20 percent of fibers from the optic tracts converge on thesuperior colliculus. this midbrain structure has close links to lower autonomiccenters responsible for emotion, arousal, and motor coordination. the control ofeye movement depends in large part on these signals (hallett, 1986). oneimportant neighboring area, the pretectal region, controls the pupillary reflex.also, vestibular and tactile inputs, as well as signals coming from muscletension and joint position sensors, converge in various ways with these signalsfrom the superior colliculus (and other brainstem nuclei) yielding a concretefeeling of bodily position and configuration that ultimately guides motion.we have seen many dualities present in the visual system: rod versus coneand their retinal distribution with respect to the fovea; p ganglion versus m,with polysynaptic and the analogous parvo connections supporting theperception of detail and color; superior colliculus versus cortical processing,with cortical signals underlying conscious perception of visual images. thislatter distinction supports the notion that there are two visual systems: the focalsystem and the ambient system. one example of the separateness of the focalsystem is the ability (often called blindsight) of cortically blind people to pointto objects they cannot "see."this focalambient duality lends support to bridgeman's (1991)demonstrations of separate cognitive and motororiented maps. theimplications for display designers include customizing the display based on itsthe visual channel115virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.intended function: spatial displays, such as representations of instruments anddials, are subject to motioninduced illusions and cognitive biases; displaysdesigned to serve as a means of facilitating visually guided behavior (such astracking or piloting tasks) are not subject to these illusions, but are subject toconstraints based on limited memory for this information and need to becontinuously displayed.the integration of vestibular and visual information leads to interactions ofform and orientation that can be exploited by the display designer (mittelstaedt,1991). given the technical difficulties involved in artificially communicatinginertial and gravitational cues in an se, and considering the potential forenhancing the effectiveness of environments emphasizing orientation and motorcontrol (as almost any application of ses does), the area of ambient visualsystem effects should be explored. in addition, further knowledge of theautonomic effects of the ambient visual system are likely to be instrumental inmitigating the sopite syndrome.the implications of the technical factors presented above for immersionand performance are not well known. given the present limitations ingenerating veridical displays, much of what is known about vision will not beaccommodated by the technology in the near term. however, if we are to realizethe full potential of the visual interface, we must not let display development bedriven solely by current trends in electronic design.sensory constraints for displaysspatial distortion and image degradation may be caused by eitherinadequate channel capacity in the display unit or by inadequate opticalarrangements between the display and the user's eyes. channel capacity sets afundamental limit on all attributes of the displayed imagery, including spatialresolution, temporal update rates and lags, contrast, luminance, and colorresolution. distortion of optical factors can arise in connection with (amongother things) the choice of focal distance, the stereoscopic arrangement, thealignment of the optic axis, and the relation between convergence andaccommodation.special optical arrangements are required to transfer the image on thedisplay screen to the image on the retina of the human observer. undesirabledistortions can result from this transfer, especially when the display screens arepositioned just a few centimeters from the eyes (as in most virtual environmentdisplays). to project the image on a tiny display screen into a realworldsizeimage on the retina, a high degree of optical magnification is required.furthermore, at a fixed close viewing distance, the larger the magnification, thegreater the geometric distortion (particularlythe visual channel116virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.at the greater eccentricities that can arise in displays with large fields of view).in general, the optics must allow for clear focusing, minimization ofgeometric distortion, and offaxis eye movements without significant imagevignetting. it must also be lightweight and easily adjustable for a wide varietyof human users. in the case of stereoscopic displays, great care must be taken toensure adequate alignment of the images provided to the two eyes andminimization of the inherent conflict between ocular convergence andaccommodation (e.g., see miyashita and uchida, 1990). for example, in thecase of nearfield imagery, the image disparity between the two eyes willnecessarily be large to elicit the proper stereo depth effect. in natural viewingthis disparity in the near field is accompanied by an appropriate level of ocularconvergence and lens accommodation. in a collimatedoptics hmd, theseadditional depth cues are not present, so the percept is degraded.temporal resolution limitations center on three separable factors: framerate, update rate, and delays (piantanida et al., 1993). frame rate is related tothe perception of flicker and is hardwaredetermined. because the humantemporal contrast sensitivity function (csf) shifts toward higher frequencies inresponse to increases in the brightness level of the display, higher light levelsresult in greater flicker sensitivity. although models of the csf can predict thehuman response to these stimuli (wiegand et al., 1994), preciserecommendations of frame rates must take into account the emissioncharacteristics of the display as well as the time course of the stimuli. thetypical luminances produced in current nonseethrough displays are sufficientto cause flicker for rates of 30 hz and below. the higher light levels requiredfor seethrough displays, coupled with the limited control of realworld lightingin many augmentedreality situations, may result in the need for higher framerates for such applications. the csf is also dependent on retinal eccentricity:peripheral sensitivity to flicker is greater than foveal (pirenne and mariott,1962). as fieldofview limitations are removed, peripheral flicker will becomeobjectionable.update rate is determined primarily by computational speed. when theview in an hmd is updated at a rate above 12 hz, motion is perceived assmooth, and motion parallax cues support depth perception. below this rateillusory motion or even simulator sickness may be induced (piantanida et al.,1993).at the current time, orientation and position tracker system delays are themajor contributing factor to viewpoint update delays in hmd images. these"phase lag" delays can be as long as 250 ms for commonly availableelectromagnetic tracking systems (meyer et al., 1992). because all of the abovedelays are additive, motion artifacts and desynchronizationthe visual channel117virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.of the visual channel with respect to the other channels of a multimodal se arelikely to dwarf the more subtle characteristics of human temporal responsebased on the csf.the third dimensiondistance in the third dimension (from the eyes) and its corollary, depthperception, are ubiquitous properties of realistic vision. a variety of monocularstimulus conditions are capable of eliciting the appearance of relative depth.these include relative motion, perspective, occlusion, shadows, texturecoarseness, aerial perspective (changes of color and luminance with distance),and others. beyond approximately 1 m, these monocular depth cuespredominate in creating the percept of depth, whereas in the near field (arm'slength), the combined use of motion cues with stereopsis becomes crucial foreliciting threedimensional perception that is convincing.binocular vision provides all the stimulus conditions of monocular visionplus the advantage of stereopsis with its high resolution (a few seconds of arc)as well as the disadvantage of requiring alignment of the two eyes to precludethe appearance of double images and binocular rivalry.our ability to simulate full binocular vision requires better technology thanis now available for the following reasons. in stereoptic vision not only isresolution of each image good to about 1 minute of arc, but also relativedisplacements between corresponding image features on the two retinas(disparities) can be discriminated to at least an order of magnitude better. nocurrently available display can meet such specifications. as a consequence,resolution of detail in all three dimensions is considerably less than optimalšitcorresponds to the vision of a visually disabled observer. however, inasmuch asdepth perception is also elicited by the relative motion of retinal images and theuse of triangulation (parallax), some degree of distance discrimination can beachieved without stereopsis.augmentedreality systemsmany of the factors discussed above become especially critical whenaugmentedreality systems are considered. the difficulties stem largely fromthe immediate comparison between the realworld stimulus and the syntheticelements displayed as an overlay. when the subtle shifts in light qualities, zerodelay viewpoint changes, and threedimensional qualities of objects present inthe real world are combined with the lessresponsive synthetic stimuli,separation of the real and the synthetic is obvious. similarly, the perception ofthe surface qualities of reflectivethe visual channel118virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.objects, the distinction between selfluminant and reflective objects, theperceptual invariance of color and brightness of objects based on the perceptualintegration of light source and other information present in the entire scene, areall likely to constrain the type of information that can be effectively andunambiguously displayed (roufs and goossens, 1988; westerink and roufs,1989).the visual system has evolved to resolve ambiguities in the proximalstimulus through knowledge about how visual scenes behave. in using seethrough hmds, it will be hard not to violate this knowledge. further research isneeded to evaluate these effects and to develop robust presentation techniquesthat are effective within the technological limitations.sensory adaptation to visual distortionmany simple alterations involving the visual channel are effortlesslyadapted to. for example, most users have little difficulty learning to position acursor on a vertical computer screen through the use of a mouse constrained to ahorizontal tabletop. stable homogeneous alterations can be adapted to, even ifthey are extreme (kohler, 1964 [1951]). nonhomogeneous alterations that arestable with respect to the visual field (such as those introduced by prismgoggles) can also be adapted to (held and freedman, 1963). because hmdoptics (or any lens system placed in front of the eyes) exhibits these prismaticdistortions, the design of such systems can be informed by what is known aboutthe process of adaptation to prismatic distortions.often, distortions are purposely introduced to enhance perception (e.g.,magnification to increase visual acuity). by incorporating appropriatedistortions, displays can be better matched to specific environments or tasks,thus enhancing the visual capabilities of the observer. these enhancementswould typically involve local changes in magnification (similar to the use ofbifocal lenses), but they can also be based on higherorder properties of thevisual stimulus such as alterations of optical flow fields in motionoriented tasks.another example of an enhancing visual distortion involves changes in theeffective interocular distance. depth through stereopsis is limited to a maximumretinal disparity that can be accommodated by ocular geometry and the size ofthe retinal fusional areas. the result is that stereopsis starts to substantiallydegrade at approximately 10 m and is not effective beyond about 135 m fromthe viewer (haber and hershenson, 1980). increased interocular distance hasbeen used in binoculars and other optical instruments to overcome thislimitation. enhancement of the near field is not so straightforward, however.the ability to fuse the disparate retinal images into a perception of depth islimited by fairlythe visual channel119virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.unchangeable neural connectivity patterns. the limit of maximum disparity(parnum's limit) suggests an upper bound to the degree of enhancement ofstereopsis that is possible through eye position manipulation. adaptation toextreme interocular distance changes, if they occur at all, are likely to haveextremely long time courses and may never become complete.it is clear that the purposeful introduction of alterations has the capacity toboth overcome equipment limitations and enhance the sensory capabilities ofthe observer. development of visual displays that capitalize on these effectsmust combine existing knowledge of perception with findings from our growingexperience with ses.status of the technologyan ideal visual display would be capable of providing imagery that isindistinguishable from that encountered in everyday experience in terms ofquality, update rate, and extent. clearly, however, current technology will notsupport such highly veridical visual displays, nor is it clear that such a highlevel of verisimilitude is required for most applications. the relative importanceof various display features including visual properties (e.g., field of view,resolution, lumination, contrast, and color), ergonomics, safety, reliability, andcost must therefore be carefully evaluated for any given application. mckennaand zeltzer (1992) provide a thorough comparison among several of thesefeatures for five major threedimensional display types.a complete visual interface consists of four primary components: (1) avisual display surface and an attendant optical system that paints a detailed,timevarying pattern of light onto the retina of the user; (2) a system forpositioning the visual display surface relative to the eye of the operator; (3) asystem for generating the optical stimuli (either from camera sources of realworld scenes, stored video imagery, or computer synthesis or some combinationof the three); and (4) a system for sensing the positions and motions of the headand/or eyeballs. items (3) and (4) are treated in chapters 5 and 8; they are nottreated here except insofar as they have direct implications for the first twotopics.two major classes of visual display systems are available for seapplications: headmounted and offhead displays. the imagery displayed byeither form of device may be coupled to the motions of the user's head eitherdirectly (using sensors that measure head position and orientation) or indirectly(typically using manual control devices such as joysticks or speech input).at present, the majority of visual displays for synthetic environments arephysically coupled to the head of the operator by mounting displaythe visual channel120virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.hardware on a helmet or headband worn by the user. a significant advantage ofheadmounted displays is that the display positioning servomechanism isprovided by the human torso and neck. this allows the generation of acompletely encompassing viewing volume with no additional hardware andeliminates lag introduced by display surface positioning systems required bysome offhead strategies. in many hmds, all the imagery is synthetic andgenerated by computer. for certain augmentedreality displays, however, asemitransparent display surface is used and the synthetic imagery is overlaid onimagery resulting directly from objects in the environment. in other augmentedreality displays, the synthetic imagery is combined with imagery derived fromoptical sensors on a telerobot.among the disadvantages of headmounted displays are the weight andinertial burden that interferes with the natural, unencumbered motions of theuser, the fatigue associated with these factors, and the increased likelihood ofsymptomatic motion sickness with increased head inertia (dizio and lackner,1992). in general, it is difficult to build headmounted display devices thatexhibit good spatial resolution, field of view, and color yet are lightweight,comfortable, and costeffective.although often not associated with synthetic environment applications, avariety of offhead displays are available for se tasks. these displays rangefrom relatively inexpensive monoscopic and stereoscopic panel and projecteddisplays to experimental autostereoscopic displays. highresolution color paneland projected display systems are available at relatively low cost due tocommon use in the computer graphics and entertainment fields. these systems,at most, require a lightweight pair of active or passive glasses to generate highquality stereo and therefore impart a minimal inertial burden on the user and arecomfortable to use. within the limits of comfortable viewing range, the staticfield of view and spatial resolution of panel and projected displays isdetermined by user distance from the display surface. relatively large field ofview (e.g., > 100 deg horizontal field of view) can be readily achieved withlarger display surfaces without requiring correcting optics. panel and projectiondisplays are typically larger and heavier than hmds, a disadvantage in volumeand weightlimited applications. in addition, servocontrolled or multiple staticdisplay surfaces must be used to provide a completely encompassing visualenvironment. another approach to increasing viewing volume is to use headposition and orientation sensors of the same type and form used on headmounted displays. although the relationship between the user's head positionand orientation and the remote or virtual viewpoint is not so straightforward asthat encountered when using headmounted displays, this form of viewpointcontrol can nonetheless be used to advantage to both control the viewpoint intothe syntheticthe visual channel121virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.environment (spain, 1992) and to correct for offaxis viewing perspective(mckenna and zeltzer, 1992).hybrid panel display systems exist that use smaller display surfaces servocontrolled or manually steered to the user's head position and orientation(oyama et al., 1993; macdowall et al., 1990). these systems remove theinertial burden of headmounted displays yet allow the use of more readilyavailable, nonminiature, highresolution color panel display devices to generatethe imagery. in addition, user head position and orientation information isreadily available through instrumentation of the display support structure, andconsequently a large effective viewing volume is achieved without additionalcomplexity. in servocontrolled variants, delay and distortions introduced by theelectromechanical control mechanisms must be dealt with, in addition to thesafety issues associated with coupling a powered device to the human head.manually steered devices do not have the same safety concerns but do limit theapplications to which these devices may be applied, since one or both handsmust be used to move the visual display and therefore are not available formanual interaction tasks.autostereoscopic offhead display techniques, such as slicestacking andholographic video, do not require special viewing aids (e.g., fieldsequential orpolarized glasses). displays of this type, currently in the research anddevelopment phase, are discussed further in the next section.components and technologiesdisplay surfaces and opticsthe most frequently used and technically mature display types forsynthetic environment applications are cathode ray tubes (crts) andbacklighted liquid crystal displays (lcds). although these technologies haveproven to be very useful for nearterm hmd applications, several shortcomingscompromise their longterm promise. crt technology has been able to deliversmall, highresolution, highluminance monochromatic displays. these crts,however, are relatively heavy and bulky and place very high voltages on thehuman head. in addition, the development of miniature, highresolution, highluminance, color crts has been difficult. in contrast, current lcd technologycan produce color images at low operating voltages, but only at marginalpictureelement densities. both technological approaches require bulky optics toform highquality images with sufficiently large optical exit pupils. thefollowing paragraphs discuss emerging technological approaches that have thepotential to produce highresolution color (tektronix or custom built devices)the visual channel122virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.imagery while reducing some of the weight, bulk, and cost of current displays.a nearterm approach that is bringing highquality, color, crtbasedhmds to the commercial marketplace is the use of mechanical or electroniccolor filtering techniques applied to monochrome crts. in this approach, thecrt is scanned at three times the normal rate, and red, green, and blue filtersare sequentially applied. some of the integrated systems discussed insubsequent sections of this report use this technique.commercially available se displays have almost exclusively relied on tvquality liquid crystal display technology. this technology was developed andchosen because of the demand for largearea displays for computer andtelevision screens; it is limited in its maximum pixel density and size by thethinfilm manufacturing techniques used for its fabrication. the quality oftransistors, lithographic resolution, and parasitic resistance of wires in the thinfilm technologies is considerably inferior to that capable of being realized insilicon very large scale integrated (vlsi) technologies. in the ve andteleoperator fields, largearea displays are not desirable. in these applications,very high resolution in compact, lightweight form is needed.thomas knight of the mit artificial intelligence laboratory is pursuingsuch display characteristics using silicon vlsi chip technology. the potentialfor density and performance can be seen by comparing the available resolutionof liquid crystal (lc) displays with the density of commercially available imagesensors based on silicon technology. the lcds have maximum resolutions ofabout 640 × 400, with a pixel pitch of about 330 µ. silicon sensors have beenfabricated with resolutions of 4,000 × 4,000 pixels, with dot pitches of 11 µ.smaller pixel dimensions are lithographically possible, but light sensitivitydemands the use of larger pixel areas. knight believes that a feasible prototypein today's technology is a 2,000 × 2,000 image display with pixels on an 8 µpitch, producing a display with an active area of 16 mm × 16 mm.there are several approaches to coating such a highresolution substratewith optically active materials. one simple approach is to use conventionalfieldsensitive liquid crystal material as a reflective display. an advantage of seapplications is the ability to control the illumination environment, making manyproblems in lc display technology, such as angular field of view, less critical.some indication of the possible resolution available with lc techniques onsilicon is the application of liquid crystals to diagnosis of failing integratedcircuit parts, in which lines of under 3 µ are routinely imaged (picart et al.,1989).kopin, inc., in taunton, massachusetts, is also investigating highresolution lcds on silicon, with the use of very thin (1,000 a) siliconsubstrates, which are optically transmissive. the kopin approach reliesthe visual channel123virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.on fabricating very thin layers of recrystallized silicon on a silicon dioxidesubstrate, forming circuitry on the substrate, and finally releasing therecrystallized silicon by etching. the released thin silicon sheet is then floatedonto a glass substrate and used as a transparent display. devices with aresolution of 1,100 lines per square inch over a 0.5 × 0.5 inch area have beenmanufactured.another optical interface approach is the use of electrochromic polymers,such as polyaniline (kobayashi et al., 1984). this material changes colorreversibly from a pale yellow to a dark green when reduced by current flow insolution. by coating an area array of noble metal electrodes fabricated on anactive silicon structure, reduction in pixelsized regions is achievable. witheither of these display technologies, the major advantages of costeffective,mass producible, lithographically defined devices remain, leading to potentiallydramatic systemlevel economic benefits.the optical portion of a se visual display is potentially amenable tosignificant cost savings with the use of holographic optical elements (hoes) asa replacement for bulky and heavy lens assemblies. the combination of asilicon display and hoes could lead to a light, costeffective se interfacesuitable for mass production. work in this area is ongoing at the university ofalabama, where the focus is primarily on computergenerated holograms usinglc materials on silicon substrates as a highresolution binary display.a unique alternative to the classic hmd based on crt or lcd technologyis being pursued at the university of washington's human interfacetechnology (hit) laboratory (lalonde, 1990). the hit laboratory isdeveloping a display based on laser microscanner technology; it will use tinysolid state lasers to scan color images directly onto the retina. the advantage ofthis approach is that it does not require the use of the heavy, bulky opticstypically used in crt/lcdbased collimated aerial image systems and has thepotential for developing highresolution, lightweight, and lowcost displaysystems. the laser microscanner display, however, still faces substantialtechnical obstacles. a thoughtful analysis of this form of display may be foundin holmgren and robinett (1994).autostereoscopic displaysautostereoscopic displays are interesting in that no viewing aids (e.g.,fieldsequential or polarized glasses) are required and no inertial burden isadded to the human user. depending on the size of their viewing zone orviewing volume, autostereoscopic displays can be seen by multiple viewers.some displays, such as lenticular or parallax barrier displays, can present asmall number of precomputed perspective views, such thatthe visual channel124virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.limited motion parallax and lookaroundšusually restricted to the horizontalplanešcan be generated without tracking viewer head position. holographicand slicestacking displays can present continuously varying perspective views,although in a restricted viewing volume. since autostereoscopic displays entaileither restricted viewing volumes or large, resolutionlimited display surfaces,however, they may not be appropriate for se applications requiring acompletely encompassing viewing volume (mckenna and zeltzer, 1992).lenticular displays a lenticular sheet is an array of cylindrical lenses thatcan be used to generate an autostereoscopic threedimensional image bydirecting different twodimensional images into viewing subzones. thesubzones are imaged out at different angles in front of the lenticular sheet.when an observer's head is positioned correctly, each eye will be in a differentviewing zone and will see a different image, thus allowing for binoculardisparity.lenticular imaging requires very high resolution to image a large numberof views. with crt technology, the pixel size limits the upper resolution andthus the number of views. the bandwidth requirements can also become verylarge, since n views are displayed. furthermore, n views must be rendered inreal time, with the imagery ''sliced" and placed into the vertical strips behind thelenticules. the number of displayable views is limited by the imperfect focusingability of the cylindrical lenses. lens aberrations and diffraction of the lightreduce the directivity of the lenses, so that the focused imagery from the backscreen does not emerge with parallel rays, but rather spreads with some angle.this spread limits the number of subzones that can be differentiated from eachother. another key issue with lenticular sheet displays is that the back screenimagery must be closely aligned with the slits or lenticules, otherwise thesubzone imagery will not be directed into the appropriate subzone.parallax barrier displays a parallax barrier is a vertical slit plate that isplaced in front of a display, simply to block part of the screen from each eye. aparallax barrier acts much like a lenticular screen, except that it uses barriers toobstruct part of the display rather than lenticules to direct the screen imagery.the screen displays two images, each of which is divided into vertical strips.the strips displayed on the screen alternate between the left and right eyeimages. each eye can see only the strips intended for it, because of the slit plate.more than two images can be displayed on the screen to create multiple viewsfrom side to side. when a crt monitor is used with a parallax barrier, thehorizontal resolution is divided by the number of twodimensional viewsprovided. multiple projecting monitors can be used to maintain a higherhorizontal resolution with a large number of views. each projector images adifferentthe visual channel125virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.viewpoint, and the barrier and diffusion screen direct the light back to theviewing zones. parallax barriers are not commonly used because they sufferfrom several drawbacks. first, the displayed imagery is often dim, because thebarrier blocks most of the light to each eye. also, with small slit widths, thediffraction of light from the slit gap can become problematic, as the light raysspread. as discussed above, the crt imagery must be segmented into strips, aswith a lenticular sheet display.slicestacking displays slicestacking displays, also referred to asmultiplanar displays, build up a threedimensional volume by layering twodimensional images (slices). just as a spinning line of light emitting diodes(leds) can perceptually create a planar image, a rotating plane of leds cancreate a volumetric image. a similar volume can be scanned using crtdisplays and moving mirrors. rather than using a planar mirror, which wouldhave to move over a large displacement at a high frequency, a variablefocus, orvarifocal, mirror can be used. a 30 hz acoustic signal is commonly used tovibrate a reflective membrane. as the mirror vibrates, its focal length changes,and a reflected monitor is imaged, over time, in a truncatedpyramid viewingvolume. the mirror continuously changes its magnification, so that imageryscanned over time (as a crt operates) will be continually changing in depth(not in discrete slices). a variant on this approach is under development bytexas instruments. in this technique, micromechanical mirrors, 17 µ square, aresupported by silicon beams on diagonal corners. the two unsupported cornersare metalized and used as electrodes in an electrostatic actuator, which allowsthe mirrors to be pulled to one side or the other. actuation rates of about 10microseconds and angular deflections of about 10 deg allow these microscopicmirrors to deflect incoming light to form highresolution displays. a versioncontaining approximately 700 × 500 pixels using frame sequential color derivedfrom a color filter wheel and six bit pulse width control of the intensity of eachpixel has been demonstrated.another method for generating a volumetric image is to illuminate arotating surface with a random access light source. some experimental systemshave employed a spinning double helix, illuminated by lasers and controlled byacoustooptic scanners (soltan et al., 1992). in order to illuminate a specificlocation in the volume, the laser is timed to strike the helical surface as it passesthrough that location.slicestacking methods trace out a luminous volume, such that objects aretransparent, and normally obscured objects, further in depth, cannot be hidden.this can be ideal for volumetric datasets and solid modeling problems, but it ispoorly suited to photographic or realistic images with hidden surfaces. theaddition of headtracking would allowthe visual channel126virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.hidden surfaces to be approximately removed in the rendering step for oneviewer. not all surfaces can be correctly rendered, however, because the twoeyes view from differing locations; each eye should see some surfaces that areobscured to the other.holographic displays computergenerated holograms fall under two maincategories, computergenerated stereograms and computergenerated diffractionpatterns. computergenerated stereograms are recorded optically, from a set oftwodimensional views of a threedimensional scene. the final hologramprojects each twodimensional image into a viewing zone, and stereo views canbe seen with horizontal parallax (benton, 1982). fullcolor, highresolutionimages have been generated, as well as large, wide fieldofview holograms.this is a nonrealtime imaging technique, however; it requires offlinerecording. a large amount of information is needed to generate the hologram aswell, since every view (typically 100 to 300) must be generated.rather than record a set of twodimensional views holographically, a truediffraction pattern can be generated by computer. when illuminated, thehologram will create a threedimensional wavefront, imaging threedimensionalobjects and light sources in space (dallas, 1980; tricoles, 1987). the methodsused to compute the diffraction patterns are complex and computationallyexpensive. until recently, computergenerated holograms had to be recordedusing plotter or printing techniques, as an offline process. a new method,however, allows a holographic image to be displayed in real time, from a fastframe buffer storage (st. hilaire et al., 1990; benton, 1991). because theholographic signal can be scanned in real time and potentially broadcast, thissystem is referred to as holographic video by its creators at the massachusettsinstitute of technology.the information contained in a hologram with dimensions of 100 × 100mm with a viewing angle of 30 deg corresponds to approximately 25 gbyte (25billion samples), well beyond the range of current technology to update at framerates of 30 hz (st. hilaire et al., 1990). the mit system addresses this problemby reducing the information rate in three waysšby eliminating vertical parallax(saving several orders of magnitude), by limiting the viewing zone toapproximately 14 deg (wider angles require higher spatial frequency diffractionpatterns), and by limiting the image size. the diffraction patterns for a frameare computed on a supercomputer (connection machine ii) in under 5 s forfairly simple objects composed of luminous points. the hologram is stored in ahighresolution frame buffer (approximately 6 mbyte per frame) and istransmitted to a highbandwidth acoustooptical modulator (aom). the aommodulates a coherent light source to create the threedimensional image. boththe visual channel127virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.monochrome and tricolor displays have been demonstrated. recently, a largerversion of the system with a modified scanning architecture has been developedand can display a holographic image at 140 mm × 70 mm and a 30 deg angle ofview, each frame of which requires 36 mbyte of computed diffraction pattern.integrated systemsin the united states, following the pioneering work of ivan sutherland andhis students in the late 1960s, research and development of display systems forsynthetic environments have been carried out at mit's media lab, the amesand langley research centers of the national aeronautics and spaceadministration (nasa), wrightpatterson air force base, the naval oceansystems center, the university of north carolina, leep optics, the universityof washington, the japanese government's mechanical engineering laboratoryat tsukuba, cae electronics, vpl research, virtual research, technologyinnovation group, kaiser electronics electrooptics division, hughes electrooptical & data systems group, stereographics corporation, and fake spacelabs, to name a few of the more prominent research and development centers inacademia, government, and private industry. applications of the variousresearch systems include scientific visualization, vehicle guidance, and remotemanipulation. a variety of relevant references are available in aukstakalnis andblatner (1993), earnshaw et al. (1993), and kalawsky (1993).headmounted displaysover the years, advanced, highperformance hmds have been developedfor various military research applications (merritt, 1991). these systems,although quite capable, often cost in excess of $.5 million. the highestresolution hmd technology used to date has been developed by caeelectronics of canada for a variety of military applications. highresolutioncolor images produced by a pair of general electric liquid crystal light valve(lclv) projectors are condensed onto fiber optic bundles that convey imageinformation to an hmd incorporating wideangle eyepiece optics. the phase vversion of this fiber optic hmd provides a 160 deg horizontal (h) by 80 degvertical (v) field of view with a 38 deg (h) stereoscopic overlap centered in theoperator's field of view. in contrast, commercial hmds until recently consistedmainly of lowresolution, widefieldofview lcdbased systems. historically,the most widely used commercial headmounted systems were the virtualresearch flight helmet, the vpl eyephone, and leep systems cyberfacethe visual channel128virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.ii. all three provide a large total horizontal field of view of roughly 100 to 110deg using lcd screens with effective spatial resolutions of around 300 × 200pixels in the standard ntsc (national television standard code) aspect (width/height) ratio. all of these devices are or were sold for under $10,000. vplresearch introduced but then withdrew a higherresolution hmd titled thehrx, due to maintainability problems. w industries (in the united kingdom)markets a rugged hmd titled the visette, with properties similar to that of thevr flight helmet. neither the vr flight helmet nor the vpl eyephone iscurrently being sold, and few cyberface devices remain in use.the past few years have witnessed the introduction of commerciallyavailable systems that can serve as relatively simple, robust, and inexpensivedevelopmental tools for those interested in developing ses. these systems areoften commercial variants of military systems and are introducing color crtbased and advanced solidstatebased display devices to the commercialmarketplace.virtual research, upon discontinuing its successful flight helmet,introduced a lightweight helmetmounted display using miniature monochromecrts with color wheels known as the eyegen3. this 28ounce system takestwo ntsc inputs, has a resolution of 250 (h) × 493 (v), and a field of viewthat ranges from 32 deg (h) at 100 percent binocular overlap to 48 deg (h) at 50percent binocular overlap. it has outstanding brightness and contrast and costsunder $8,000. in a similar price range, liquid image corporation's miragehmd uses a 5.7inch diagonal, thinfilm transistor, activematrix lcd displayto provide a 240 (h) × 720 (v) bioccular (i.e., nonstereoscopic) color displayover a 110 deg (h) field of view. although the system accepts an ntsc input,its response time is 80 ms (typical for lcd displays). at the higher end of thecommercial performance spectrum (approximately $150,000), kaiser electrooptics has introduced the color sim eye. a direct result of kaiser's militarywork, the color sim eye provides 1,280 (h) × 1024 (v) interlaced lines over a40 deg diameter field of view. variants with 6080 deg field of view are underdevelopment; a monochrome version is also available. crts are mounted lowalongside the head to minimize their inertial burden during turns of the head.simple relay optics are used to convey images into the operator's eyes. theyalso provide a 44 percent seethrough capability. surfacestabilized ferroelectricliquid crystal shutters are used to sequentially display the three primary colorsat a refresh rate of 60 hz (i.e., 180 hz scan rate); it weighs 3.5 lb (1 lb less thanthe monochrome version). technology innovation group's recently developedhmd systems provide (at somewhat higher cost) greater adjustability, moresophisticated relay optics that yield a wider field of view, and improved helmetfeatures. using an overall configuration similar to that of the color sim eyethe visual channel129virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.(i.e., crts mounted close to the turning axes of the head), the system is capableof providing an expanded field of view while minimizing geometric distortions.offhead displaysclassic crtbased panel and projection displays with sufficient scan ratesand bandwidths can generate highresolution (i.e., 1,000+ horizontal line)stereoscopic displays using field sequential techniques. highresolution, color,highbandwidth displays of this form start at approximately $2,000 for panelversions and $13,000 for widescreen projection versions. this form ofstereographic display uses a temporal sequencing approach to providealternating stereo display pairs to the right and left eyes. the premier providerof field sequential devices in the computer graphics market place isstereographics corporation. stereographics manufactures a series of lcdshuttered glasses that use infrared technology for field synchronization. theseglasses and the infrared transmitter can be added to appropriate computerconsoles and projection systems for well under $2,000. a variety of even lowercost, wiresynchronized, field sequential systems are available from 3d tv.total system costs are approximately one order of magnitude less than that ofcomparable headmounted displays.several interesting hybrid systems are available. fake space labs hasdeveloped a class of displays known as the boom. the user holds the displayhead by two handles, pointing and moving the view direction by turning thedisplay head as if it were a pair of binoculars. thumb buttons can be used tocontrol forward and backward motion within the se. several configurations ofthis rugged, floormounted device are available. the lowend system uses twincrt displays to provide 640 (h) × 480 (v) resolution in monochrome. thehighend threecolor version of the boom provides 1280 (h) × 960 (v)interlaced resolution using color filtering techniques. boom viewers haveintegral tracking sensors in the sixdegreesoffreedom support structure. thistracking system is very fast, returning orientation and position information tothe computer in 5 ms or less after a change of viewing direction. leep opticshas introduced cyberface iii, a higherresolution monochrome system with asingle image source and a countersprung support arm to neutralize the weighton the wearer's head. cyberface iii is aimed at the pcbased computeraideddesign (cad) market and is a lightweight, lowpriced alternative to the fakespace boom. it can be used with a facefitting head mount or in a handguidedmode. costs for these hybrid systems fall between those of offhead paneldisplays and headmounted displays.the visual channel130virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.advantages and disadvantages of headmounted and offhead displaysin summary, headmounted displays provide a straightforward approach tothe generation of a seamless, allencompassing viewing volume. highperformance hmds, however, are costly, and current technological challengesresult in less than ideal performance. in particular: (1) highresolution,miniature, lightweight, lowcost display surfaces are yet to be realized; (2)weight and inertial burdens imposed by most hmds affect the incidence ofsymptomatic motion sickness, the ability of users to make proper judgmentsconcerning orientation, and their longterm habitability; and (3) due to size,performance, and cost constraints, fixation/focus compensation is utilized inmost hmds and conflicting visual depth cues are provided to the user.furthermore, the proper operation of hmds is intimately tied to theperformance of headtracking systems (i.e., update rate and lag), which iscurrently less than ideal.offhead display approaches can help alleviate some of theseshortcomings. since the display is not worn on the head: (1) relativelyinexpensive, widely available display surfaces (i.e., panel and projection) canprovide highresolution color imagery; (2) the effects of placing additionalweight and inertial burdens on the user's head are minimized; (3) some of thevolume and weight constraints on fixation/focus compensation systems arerelaxed; and (4) some offhead configurations (i.e., those that supply aninstantaneous encompassing viewing volume) do not necessarily require headorientation tracking, and therefore, lags and distortions due to the trackingsystem are minimized. offhead displays, however, are by no means perfect. ifan instantaneous encompassing viewing volume is not provided, some form ofhead tracking is required. not only must the computergenerated imagery beslaved to the user (as in hmds), but also the display surfaces must be servocontrolled. finally, the overall volume and weight of the display system istypically much greater for offhead systems than for headmounted systems.research needswithin the past few years, the design and development of visual displaysystems appropriate for use in synthetic environments have engagedunprecedented commercial interest and involvement. highend hmdtechnology, originally developed for military applications, has been transferredto the commercial sector and new commercial markets have resulted ininnovative lowerend designs. lightweight, intermediateresolution(approximately 1,000 horizontal lines), color, seethrough capable hmds arenow available for under $200,000; lowerresolution (i.e.,the visual channel131virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.ntsc level) systems are available for under $10,000. intermediateresolution,offhead, fieldsequential, panel and projection systems are available for wellunder $20,000. equivalent boomtype systems cost under $100,000. nospecial governmentdirected research effort seems to be required as an impetusto drive the efforts to refine system design and reduce system costs in theintermediate resolution display technologies under discussion. that is not to saythat there remains no significant research and development to be done, butrather that commercial pressures seem adequate to motivate and fund theseefforts.current commercial and technological trends, however, are still likely toresult in displays for synthetic environments that have several shortcomings.the best of the current commercial systems, with resolutions roughly equivalentto highdefinition tv, cannot provide eyelimited resolution except across arelatively small field of view (i.e., approximately 38 deg [h] field of view). itmay be some time before commercial market pressures by themselves will drivethe development of the higherresolution display devices required for widerfieldofview, eyeresolutionlimited display systems. eyetracking approaches(i.e., in which a highresolution display area is kept within the foveal region ofthe eye and a lowerresolution image is presented elsewhere), do not alleviatethe required absolute display surface resolution but rather mitigate thecomputational requirements for generating visual images in virtualenvironments or the camera resolution requirements for teleoperators. somegovernment involvement therefore may be appropriate for encouraging highresolution display design that will enable widefieldofview (i.e., perceptuallyseamless), eyeresolutionlimited display systems to be built. in order to providea 180 deg (h) by 120 deg (v) field of view (i.e., the instantaneous field of viewof the human visual system), display devices providing approximately 4,800(h) × 3,800 (v) lines of resolution are required (mckenna and zeltzer, 1992). itseems prudent for both crtbased and solidstate devicebased approaches fordisplay devices to be pursued. a particular area for emphasis should be thedevelopment of miniature, highresolution crt and solidstate display surfaces,since commercial pressures tend toward larger versions of these devices andhmds require smaller versions. autostereoscopic offhead displays (e.g., videoholographic displays), which are especially attractive for situation assessmentapplications, should also be considered for continued governmentfundedresearch and development.in designing a visual display system for synthetic environments, it isimportant to consider the specific tasks to be undertaken and, in particular, thevisual requirements inherent in these tasks. none of the available technologiesis capable of providing the operator with imagery that is in all importantrespects indistinguishable from a direct view of a complex,the visual channel132virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.realworld scene. in other words, significant compromises must be made. thecompromises made by designers should be based on the best available evidenceregarding the relationships between vision system features and objectiveperformance metricsšnot on wholly subjective criteria. although manyimportant lessons can be learned from the work conducted over many years onlargescale simulators concerning these issues, further research is clearlyrequired.unanswered questions include: for a defined task domain, how should onecompromise between spatial resolution and field of view? is color worth theadded expense and loss of spatial resolution? is a stereoscopic display worth thetrouble? if so, what are the appropriate parameters (baselines, convergenceangles, convergence/accommodation mismatches) for various types of tasks?can supernormal cues (e.g., depth, contrast, etc.) be used to advantage? can thesimulatorinduced queasiness that accompanies the use of widefieldofviewhmds for some users be minimized or eliminated entirely? which forms ofdelays and distortions induced by the visual display system can be adapted to?how are the required visual display system parameters affected withinmultimodal systems? can visual display system requirements be relaxed inmultimodal display environments? what are the perceptual effects associatedwith the merging of displays from different display sources? how do we designa comfortable hmd that integrates visual, auditory, and position trackingcapabilities? these are but a few of the many research issues that impact thepractical design of any visual display system for synthetic environments.the visual channel133virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.3the auditory channelas indicated previously, the accomplishments and needs associated withthe auditory channel differ radically from those associated with the visualchannel. specifically, in the auditory channel, the interface devices (earphonesand loudspeakers) are essentially adequate right now. in other words, from theviewpoint of synthetic environment (se) systems, there is no need for researchand development on these devices and no need to consider the characteristics ofthe peripheral auditory system to which such devices must be matched. what isneeded, however, is better understanding of what sounds should be presentedusing these devices and how these sounds should be generated. accordingly,most of the material presented in this section is concerned not so much withauditory interfaces as with other aspects of signal presentation in the auditorychannel.status of the relevant human researchthere is no topic in the area of auditory perception that is not relevant tothe use of the auditory channel in some kind of se system. the illustrativetopics included here have been chosen for discussion because we believe theyhave exceptional relevance or are receiving considerable attention byinvestigators in the field of audition who are concerned with the use of theauditory channel in se systems. general background on audition can be foundin carterette and friedman (1978); pickles (1988); moore (1989); yost (1994);and fay and popper (1994).the auditory channel134virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.resolution and information transfer ratesgeneral comments on resolution and information transfer rates, most ofwhich are not modality specific, are presented in the overview, in thediscussion of the current state of the se field. here we supplement thoseprevious, more general remarks with information that is specific to the auditorychannel.most work on auditory resolving power has focused on artificially simplestimuli (in particular, tone pulses) or speech sounds. except for a small amountof work directed toward aiding individuals with hearing impairments, relativelylittle attention has been given to the resolution of environmental sounds.nevertheless, knowledge of the normal auditory system's ability to resolvearbitrary sounds is quite advanced. thus, for example, there is an extensiveliterature, both experimental and theoretical, on the ability to discriminatebetween two similar sounds, or to perceive a target sound in the presence of abackground masking sound (presented simultaneously or temporally separated).on the whole, knowledge in this area appears to be adequate for most se designpurposes. useful information on auditory resolution can be found in the generaltexts on audition cited above, in the handbook of human perception andhuman performance (boff et al., 1986), in the engineering compendium (boffand lincoln, 1988), and, most importantly, in the many articles published eachyear by the journal of the acoustical society of america.issues related to information transfer rates are much more complexbecause such rates depend not only on basic resolving power, but also onfactors related to learning, memory, and perceptual and cognitive organization.it appears that the upper limits on information transfer rates for spoken speechand morse code, two methods of encoding information acoustically for whichthere exist subjects who have had extensive training in deciphering the code, areroughly 60 bits/s and 20 bits/s, respectively. unfortunately, we are unaware ofany estimates of the information transfer rate for the perception of music. wewould guess, however, that the rate lies between the above two, with a valuecloser to that of speech than of morse code (because of the much higherdimensionality of the stimulus set in music than in morse code). although wecannot prove it, we suspect that the rate achieved with spoken speech is close tothe maximum that can be achieved through the auditory channel. we say thisnot because we believe that speech is special, in the sense argued by variousspeech experts (e.g., liberman et al., 1968), but rather because of the highperceptual dimensionality of speech sounds and because of the enormousamount of learning associated with the development of speech communication.furthermore, except possibly for the case of music, we believe that it would beextremely difficult to achieve a comparativelythe auditory channel135virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.high rate with any other coding system. certainly, none of the individuals weknow would be willing to spend an equivalent amount of time attempting tolearn an arbitrary nonspeech code developed for purposes of general research orfor use in se systems. unfortunately, there is no theory yet available thatenables one to reliably predict the dependence of information transfer rates onthe coding scheme or training procedures employed.auditory displaysmuch of the current work concerned with communicating information toindividuals through the auditory channel falls under the heading of ''auditorydisplays." a comprehensive view of work in this area, together with hundredsof references, can be found in the book edited by kramer (1994).the focus of work in this area has been the creation of different kinds ofdisplays. relatively little attention has been given to questions of evaluation, toan analysis of the information transfer achieved, or to training issues and howperformance changes with practice. the kind of displays include audification,in which the acoustic stimulus involves direct playback of data samples, usingfrequency shifting, if necessary, to bring the signals into auditory frequencyrange; and sonification, in which the data are used to control various parametersof a sound generator in a manner designed to provide the listener withinformation about the controlling data. in general, such displays are being usedboth for monitoring tasks (e.g., to monitor the condition of a hospital patient orthe state of a computer) and for data exploration (e.g., in physics, economics).in the attempt to create effective displays, investigators are usingperceptually highdimensional stimulus sets and display sounds and codes thatcapitalize on the special sensitivities and strengths of the auditory system.generally accepted positive characteristics of the auditory system include theability to detect, localize, and identify signals arriving from any direction(despite the presence of objects that would cause visual occlusion), the tendencyfor sounds to cause an alerting and orienting response, and the exceptionalsensitivity to temporal factors and to changes in the signal over time. thesecharacteristics make auditory displays particularly useful for warning andmonitoring functions.a further set of issues in the design of acoustic displays concerns theextent to which the display makes use of everyday sounds and natural codes sothat learning time is minimized. although such codes are always preferred,some applications, such as the sonification of financial data, require codes thatare highly abstract. furthermore, when an abstract code is required, attentionmust be given to the way in which thethe auditory channel136virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.resulting complex of auditory signals will be analyzed into distinct perceptualstreams (see the detailed discussion on auditory scene analysis below).spatial perceptionthe topic of auditory spatial perception is important because theperception of the spatial properties of a sound field is an important componentof the overall perception of real sound fields, because the location of a soundsource is a variable that can be used to increase the information transfer rateachieved with an auditory display, and because it has been a central researchfocus in the simulation of acoustic environments for virtual environment (ve)systems.auditory localization of isolated sound sources in anechoic (i.e., echofree)environments is relatively well understood. the direction of a source isdetermined primarily by comparing the signals received at the two ears todetermine the interaural amplitude ratio and the interaural time delay or phasedifference as a function of frequency. whereas interaural phase provides thedominant cue for narrowband lowfrequency (< 1,500 hz) signals, interauralamplitude ratio provides the dominant cue for narrowband highfrequencysignals. for signals of appreciable bandwidth, interaural time delay also plays asignificant role at high frequencies. these empirical psychophysical results areconsistent with what one would expect based on the relevant physical acoustics:at the low frequencies, the effects of head shadow are relatively small becauseof diffraction; at the higher frequencies, measurement of time delay suffersfrom phase ambiguities unless there is sufficient bandwidth to eliminate theseambiguities. the results are also consistent with the known physiology: theneural firings in the auditory nerve are able to follow the individual cycles of anarrowband signal only at the lower frequencies; they can, however, followmodulations in the envelope of highfrequency signals with significantbandwidth at the output of the auditory "critical band" filters.ambiguities in the determination of direction achieved using interauralamplitude ratio and interaural time delay are well predicted by assuming thatthe head is a sphere with sensors located at the end of a diameter, and thendetermining the threedimensional surfaces over which both the interauralamplitude ratio and the interaural time delay remain constant. all such surfacesare surfaces of revolution about the interaural axis, which can be approximatedby cones for sources far removed from the head, and the median plane (inwhich the signals to the two ears are identical if one assumes the head issymmetrical) constitutes a limiting case of this family of surfaces.the methods used by the auditory system to resolve these ambiguitiesthe auditory channel137virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.involve (1) head movements and (2) monaural processing. thus, for example,frontback ambiguities are easily eliminated by rotating the head in azimuth.directional information from monaural processing is achieved by estimatingproperties of the directiondependent filtering of the transmitted signal thatoccurs when the transmitted signal propagates from the signal source to theeardrum. the directional information achieved in this manner, however,depends strongly on (1) the listener having adequate a priori information on thespectrum of the transmitted signal (so the spectrum of the received signal can befactored into the spectrum of the transmitted signal and the transfer function ofthe filter) and (2) the existence of highfrequency (> 5,000 hz) energy in thetransmitted signal (so that the listener's pinnae can have a strong directionaleffect on the spectrum of the received signal).for isolated sound sources in an anechoic environment, the justnoticeabledifference (jnd) in azimuth for sources straight ahead of the listener is roughly1 deg. however, the jnd in azimuth for sources off to the side, the jnd inelevation within the median plane, and the jnd in distance are relatively large.in all these cases, the jnd constitutes a substantial fraction (e.g., onefifth) ofthe total meaningful range of the variable in question.when reflections (echoes and reverberations) are present, direction findingtends to be degraded below that which occurs in anechoic environments.however, this degradation is limited by the tendency of the auditory system toenhance perception of the direct acoustic wave and suppress the latearrivingechoes (the precedence effect), at least with respect to direction finding.perception of sound source distance is generally very poor. whateverability there is seems to be based on the following three changes in the receivedsignal, as source distance is increased: a decrease in intensity, an increase inhighfrequency attenuation, and an increase in the ratio of reflected to directenergy. distance perception is poor because none of these cues is reliable: theyall can be influenced by factors other than distance. the influence of reflectionson both direction perception and distance perception, together with aninadequate understanding of how listeners separate out the characteristics of thetransmitted signal from the characteristics of the acoustic environment thatmodify the characteristics of the transmitted signal as it approaches the ear,make the study of reflection effects in audition a highpriority item.even though distance perception is poor, under normal circumstances,sound sources are perceived to be located outside the head, that is, they areperceptually externalized. however, under special circumstances, the sourcescan be made to appear inside the head. although inhead localization seldomoccurs under normal conditions, it is a majorthe auditory channel138virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.problem when sounds are presented through earphones as they are in headmounted displays for ve systems.further important issues in the area of auditory spatial perception concernthe identification of source position (as opposed to discrimination) and thephenomena that occur when multiple sources are simultaneously active atdifferent locations. identification of source position, like identification of manyother variables, is limited by inadequate shortterm memory and evidencesconstraints on information transfer that are much stronger than those that wouldbe implied solely by the jnd data. in the area of multiple sources, a great dealof research has been conducted on the ability to detect a given source in abackground of interference emanating from sources located at other locations;however, very little is known about the ability to localize a given source in sucha background of interference. furthermore, most of the work in this area islimited to the case in which the interference arises from a single location inspace and the space is anechoic.detailed information on the topics discussed above is available in thefollowing articles, chapters, and books, and in the many references cited inthese publications: blauert (1983); colburn and durlach (1978); durlach andcolburn (1978); durlach et al. (1992c); durlach et al. (1993); gilkey andanderson (1994); mills (1972); searle et al. (1976); wenzel (1992); and yostand gourevitch (1987).the parameters of the human auditory system pertinent to realtime systemdynamics include spatial resolution, as estimated by the jnd in angle or, as it issometimes called, the minimum audible angle or maa (mills, 1958); thevelocitydependent minimum audible movement angle or mama (perrott,1982); and the corresponding minimum perceivable latencies. as noted above,the auditory system is most sensitive to changes in the azimuthal position ofsources located in front of the listener, so that the necessary angular resolutionand accuracy of a head tracker are determined by the jnd in azimuth forsources in front of the listener. this localization blur (blauert, 1983) isdependent to some extent on the nature of the signal, but is never less than 1deg. the angular accuracy of the commonly used magnetic trackers nowavailable is on the order of 0.5 deg, and thus meets this requirement. thenecessary translational accuracy of trackers depends on the distances of thesources to be simulated. however, since perception of source distance itself isvery poor, permissible tracker translational error is not bound by error insimulated source distance but rather by error in simulated source angle. againconsidering sources in front of the listener, source distance must be largeenough that the maximum angular positional error (equal to the angular trackererror plus the error in angular position due to translational tracker error) issmaller than the jnd in angular position. if the angular error forthe auditory channel139virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.the tracker is assumed to be 0.5 deg, then, as long as the translational errorcauses no greater than about 0.5 deg of angular error, it will be perceptuallyinsignificant. for a given translational accuracy, sources above some minimumdistance (dmin) from the listener can be simulated without perceptual error. thevalue of dmin is given by dmin = e / 2tan (a/2), where a is the allowable angularerror due to translational error (0.5 deg) and e is the translational accuracy ofthe tracker. for an accuracy of e = 1 mm, dmin is about 12 cm, a value thatseems sufficient for most practical cases. it should also be noted that the limitedaccuracy of the tracking device will, in most cases, cause no audible effectseven if the sources are somewhat closer to the subject, because the localizationblur (even in azimuth) is larger than 1 deg for many kinds of signals and isconsiderably larger for directions other than straight ahead.the questions of the latency constraints and update rates required to createnatural virtual auditory environments are still largely unanswered. somerelevant perceptual studies include the work of perrott et al. (1989) andgrantham and his colleagues (e.g., grantham, 1986, 1989, 1992; chandler andgrantham, 1992). these studies have begun to examine the perception ofmoving auditory sources. for example, for source speeds ranging from 8 to 360deg/s, minimum audible movement angles were found to range from about 4 to21 deg, respectively, for a 500 hz tone burst (perrott, 1982; perrott and tucker,1988). the recent work on the perception of auditory motion by perrott andothers using real sound sources (moving loudspeakers) suggests that thecomputational latencies currently found in fairly simple auditory ves areacceptable for moderate velocities. for example, latencies of 50 ms (associatedwith positional update rates of 20 hz as found in the convolvotron system fromcrystal river engineering) correspond to an angular change of 18 deg when therelative sourcelistener speed is 360 deg/s, 9 deg when the speed is 180 deg/s,etc.as yet, no relevant studies have been performed that investigate the delaysthat may be tolerable in rendering dynamic cues when subjects actively movetheir heads. such psychophysical questions are currently being investigated inthe united states by the national aeronautics and space administration(nasa) and in europe by the scatis (spatially coordinated auditory/tactileinteraction scenario) project. in any event, the overall update rate in a complexauditory ve will depend not only on tracker delays (nominally 10 ms for thepolhemus fastrak), but also on communication delays and delays in eventprocessing or handling as well. the tracker delays in such systems willprobably contribute only a small fraction of the overall delay, particularly whenmultiple sources or larger filters (e.g., required to simulate reverberant rooms)are used.the auditory channel140virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.auditory scene analysisunlike the visual system, in which there is a direct mapping of sourcelocation to image on the retina, in the auditory system, there is no peripheralrepresentation of source location. there are only two sensors (the ears), eachsensor receives energy from all sources in the environment, and the spatialanalysis takes place centrally after the total signal in each ear is analyzed by thecochlea into frequency bands. thus, the auditory picture of the acoustic scenemust be constructed by first analyzing the filter outputs into components, thencombining these components both across frequency bands and across the twoears. this signalprocessing architecture differs not only from that found in thevisual system, but also from that which would be used in the design of anartificial acoustic sensing system. ideally, in an artificial system, the spatialprocessing would be performed prior to the frequency processing: one woulduse an array of microphones and parallel processing of the set of microphonesoutputs to achieve separate channels corresponding to different regions ofspace, and then separately analyze the frequency of the signal arising from eachspatial cell (colburn et al., 1987; durlach et al., 1993).independent of the evolutionary developments underlying the way inwhich the auditory system is designed, the existing design presents a uniqueproblem for creating a coherent auditory scene. the lack of a peripheralrepresentation of source location complicates not only the task of determiningsource location, but also that of determining the number and character of thesources. somehow, the higher centers in the auditory system must decomposethe output of each auditory filter in each ear into elements that, after thedecomposition, can be recombined into representations of individual sources.in general, understanding auditory scene analysis is important to the designof se systems because properties of this analysis play an important role indetermining the effectiveness of auditory displays. background information onthis topic can be found in hartman (1988), handel (1989), bregman (1990), andyost (1991). briefly, auditory scene analysis refers to the fact that, analogous tothe visual domain, one can conceive of the audible world as a collection ofacoustic objects. the auditory system's job is to organize this world byidentifying and parsing the frequency components belonging to individualobjects from the mixture of components reaching the two ears that could haveresulted from any number of "real" acoustic sources.historically, research in this area has been concerned with the phenomenaof auditory stream segregation, in which an auditory stream corresponds to asingle perceptual unit or object. studies have shown that, in addition to spatiallocation, various acoustic features such as synchronythe auditory channel141virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.of temporal onsets and offsets, timbre, pitch, intensity, frequency and amplitudemodulation, and rhythm, can serve to specify the identities of objects andsegregate them from one another. a single stream or object, such as a malehuman voice, will tend to be composed of frequencies that start and stop at thesame time, have similar lowfrequency pitches (formants), similar rhythmicchanges or prosody, and so on. a female voice is parsed as a separate objectbecause it has higher frequency formants, a different prosody, etc. nonspeechsounds can possess the same sorts of distinguishing characteristics.furthermore, gestalt principles of perceptual organization normally applied tovisual stimuli, such as principles of closure, occlusion, perceptual continuity,and figureground phenomena, have their counterparts in audition as well. forexample, these principles operate in a drawing when one object, say the letter bis occluded by another, say an irregular blob of ink. the visual system has notrouble seeing the discontinuous visible fragments of the occluded b as aunitary and continuous object. however, if the blob is removed, it is much moredifficult to recognize the same fragments as belonging to a b. a similar effectcan be heard when a series of rising and falling tonal glides are interspersedwith bursts of white noise (dannenbring, 1976). when the noise is not present,one hears bursts of rising and falling tones that seem like isolated events.however, when the noise is present the auditory system now puts the tonalglides together into one continuous stream that seems to pass through the noise,even though the glides are not physically present during the noise.while such illusions may seem merely like perceptual curiosities, thesekinds of effects can have a profound impact on whether information isperceived and organized correctly in a display in which acoustic signals areused to convey meaning about discrete events or ongoing actions in the worldand their relationships to one another. for example, if two sounds are assignedto the same perceptual stream, simultaneous acoustic masking may be greaterbut temporal comparisons might be easier if relative temporal changes cause achange in the gestalt or overall percept or cause the single stream toperceptually split into two streams. suppose two harmonically related tonalpulses with similar rhythms are being used to represent two different friendlyship signatures in a sonar display. the information content of this nicelyharmonic, unitary percept might be that all is well. however, if the signalrecognition system of the sonar system now detects a change in one ship'sacoustic signature, this might be represented as an inharmonicity of onecomponent relative to the other. a further change in their relative temporalpatterns might break the sound into two separate, inharmonic objects signaling"potential enemy in the vicinity." conversely, if the inharmonic and temporalrelationships are not made sufficiently distinct, the warning signal mightthe auditory channel142virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.not be recognized. whereas the above discussion is rather speculative, itillustrates how acoustic signals could be used, or misused, for monitoringimportant events in situations like a sonar display in which the operator's visualchannel is already overloaded.in the context of display design, the notion of auditory scene analysis hasbeen most influential in the recent interest in using abstract sounds,environmental sounds, and sonification for information display (kramer, 1994).the idea is that one can systematically manipulate various features of auditorystreams, effectively creating an auditory symbology that operates on acontinuum from literal everyday sounds, such as the rattling of bottles beingprocessed in a bottling plant (gaver et al., 1991), to a completely abstractmapping of statistical data into sound parameters (smith et al., 1990). principlesfor design and synthesis can be gleaned from the fields of music (blattner et al.,1989), psychoacoustics (patterson, 1982), and higherlevel cognitive studies ofthe acoustical determinants of perceptual organization (bregman, 1990; buxtonet al., 1989). recently, a few studies have also been concerned with methods fordirectly characterizing and modeling such environmental sounds as propellercavitation, breaking or bouncing objects, and walking sounds (howard, 1983;warren and verbrugge, 1984; li et al., 1991). other relevant research includesphysically based acoustic models of sound source characteristics, such asradiation patterns (morse and ingard, 1968). further discussion of some ofthese issues is presented in the section below on computer generation ofnonspeech audio.adaptation to unnatural perceptual cuesthere are many se situations in which a sensory display, or the manner inwhich such a display depends on the behavior of the operator of the se system,deviates strongly from normal. such situations can arise because ofinadequacies in the design or construction of an se system or becauseintentional deviations are introduced to improve performance. whenever asituation of this type exists, it is important to be able to characterize theoperator's ability to adapt.there have been a number of instances in which acoustic signals have beenused as substitutes for signals that would naturally be presented via othermodalities (e.g., massimino and sheridan, 1993). unfortunately, however,much of this work has not included careful experimentation on how subjectsadapt to such sensory substitutions over time. a major notable exception to thiscan be found in the work on sonar systems for persons who are blind (kay,1974; warren and strelow, 1984, 1985; strelow and warren, 1985).most of the work on adaptation to unnatural perceptual cues hasthe auditory channel143virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.focused on spatial perception and on transformations of cues within the auditorysystem, i.e., no sensory substitution has been involved. extensive reviews ofthis work, with long lists of references, can be found in welch (1978) andshinncunningham (1994).current research in this area directly relevant to se systems continues tobe concerned with spatial perception and is of two main types. first, studies arebeing conducted to determine the extent to which the signalprocessing andmeasurement procedures required to achieve realistic simulations of realacoustic environments using earphone displays can be simplified and mademore costeffective. an important part of this effort concerns the extent towhich listeners can adapt to the alterations in cues associated with theseprocedures. thus, for example, a number of investigators have begun to explorethe extent to which listeners can learn to make use of the directiondependentfiltering associated with someone else's ears (wenzel et al., 1993a). second,research is under way to determine the extent to which listeners can adapt todistortions that are purposefully introduced in order to improve spatialresolution. in particular, a number of transformations are being studied thatpresent the listener with magnified perceptual cues that approximate in variousways the cues that would be present if the listener had a much larger head(durlach and pang, 1986; van veen and jenison, 1991; durlach et al., 1993).also being studied now are transformations that enable the listener to perceivethe distance of sound sources much more accurately (see the review by durlachet al., 1993). although it is fairly obvious that effective spatial resolution can beimproved by such transformations, the degree to which the response biasintroduced by these transformations can be eliminated by adaptation is notobvious.in general, although adaptation is a topic that has been of interest topsychologists for a long time, there is still no theory available that will enableone to predict the rate and asymptote of adaptation as a function of thetransformation and training procedure employed.status of the technologya recent comprehensive review of technology for the auditory channel inve systems is available in durlach et al. (1992b), and much of the materialpresented in this section is taken from that report.an auditory interface for virtual environments should be capable ofproviding any specified pair of acoustic waveforms to the two ears. morespecifically, it should (1) have high fidelity, (2) be capable of altering thosewaveforms in a prescribed manner as a function of various behaviors or outputsof the listener (including changes in the position and orientation of the listener'shead), and (3) exclude all sounds not specificallythe auditory channel144virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.generated by the ve system (i.e., real environmental background sounds).requirement (3) must be relaxed, of course, for an augmentedreality system inwhich the intention is to combine synthetically generated sounds and realenvironmental sounds (see the section on hearthrough displays below).generally speaking, such results can be most easily achieved with the useof earphones; when loudspeakers located away from the head are employed,each ear receives sound from each loudspeaker and the control problembecomes substantial. although commercial highfidelity firms often claimsubstantial imaging ability with loudspeakers, the user is restricted to a singlelistening position within the room, only azimuthal imaging is achieved (with nocompensation for head rotation), and the acoustic characteristics of the listeningroom cannot be easily manipulated. in addition, since the ears are completelyopen, extraneous (undesired) sounds within the environment cannot beexcluded. finally, although the tactual cues associated with the use ofearphones may initially limit the degree of auditory telepresence, since the userwill be required to transit back and forth between the virtual and the realenvironments, such tactual cueing may actually prove useful. in any case, suchcuing is likely to be present because of the visual interface. one set of situationsfor which loudspeakers might be needed are those in which highenergy, lowfrequency acoustic bursts (e.g., associated with explosions) occur. in suchcases, loudspeakers, but not earphones, can be used to vibrate portions of thebody other than the eardrums.headphone displaysearphones vary in their electroacoustic characteristics, size and weight,and mode of coupling to the ear (see, for example, shaw, 1966; killion, 1981;killion and tillman, 1982). at one extreme are transducers that are relativelylarge and massive and are coupled to the ear with circumaural cushions (i.e.,that completely enclose the pinnae). at the other extreme are insert earphonesthrough which the sound is delivered to some point within the ear canal. theearphone may be very small and enclosed within a compressible plug (orcustomfitted ear mold) that fits into the ear canal, or the earphone may beremote from the ear and its output coupled via plastic tubing (typically 2 mminner diameter) that terminates within a similar plug. intermediate devicesinclude those with cushions that rest on the pinnae (e.g., walkmantypesupraaural earphones) and those with smaller cushions (about 1.5 cm diameter)that rest within the concha (socalled earbuds).all earphone types can deliver sounds of broad bandwidth (up to 15 khz)with adequate linearity and output levels (up to about 110 db soundthe auditory channel145virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.pressure level). precise control of the sound pressure at the eardrum requiresknowledge of the transfer function from the earphone drive voltage to theeardrum sound pressure. probemicrophone soundlevel measurementsproximal to the eardrum can be used to obtain this information. thesemeasurements, particularly at frequencies above a few khz, are nontrivial toperform. in general, this transfer function is expected to be more complex(versus frequency) and more different across individuals as the size of the spaceenclosed between the eardrum and the earphone increases. thus, circumauralearphones that enclose the pinnae are expected to have more resonances thaninsert earphones. one might similarly expect greater variability with repeatedplacement of the earphones on a given individual, but recent measurements ofhearing thresholds at high frequencies indicate that, even with relatively largesupraaural earphones, testretest variability measures are small (a few db up to14 khzšstelmachowicz et al., 1988). thus, repeated measures of this transferfunction on a given listener may be unnecessary. for some applications, eveninterindividual differences may be unimportant and calibration on an average(mannequin) ear may be sufficient. insert earphones (or earplugs), by virtue ofhaving a high ratio of contact area (with the ear canal wall) to exposed soundtransmission area (the earplug crosssection), afford relatively good attenuationof external sounds (about 35 db above 4 khz, decreasing to about 25 db below250 hz). circumaural earphones can achieve similar highfrequency attenuationbut less lowfrequency attenuation. recently developed activenoisecancelingcircumaural headsets (e.g., bose corp.) provide up to 15 db additional lowfrequency attenuation, thereby making their overall attenuation characteristicssimilar to that of insert earphones. supraaural earphones, which rest lightly on(or in) the external ear, provide almost no attenuation (consistent with theircommonly being referred to as openair design). the greatest attenuation can beachieved by combining an insert earphone with an activenoisecancelingcircumaural hearing protector. even including such a protector, it is unlikelythat the cost of such a sound delivery system will exceed $1,000.most of the past work on auditory interfaces for virtual environments hasbeen directed toward the provision of spatial attributes for the sounds.moreover, within this domain, work has focused primarily on the simulation ofnormal spatial attributes (e.g., blauert, 1983; wenzel et al., 1988; mckinleyand ericson, 1988; wightman and kistler, 1989a,b; wenzel, 1992). relativelylittle attention has been given to the provision of supernormal attributes(durlach and pang, 1986; durlach, 1991; van veen and jenison, 1991; durlachet al., 1992a).normal spatial attributes are provided for an arbitrary sound bymultiplying the complex spectrum of the transmitted sound by the transferthe auditory channel146virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.function of the space filter associated with the transformation that occurs as theacoustic waveform travels from the source to the eardrum. (in the time domain,the same transformation is achieved by convolving the transmitted time signalwith the impulse response of the filter.) for binaural presentation, one suchfilter is applied for each of the two ears. inasmuch as most of the work onvirtual environments has focused on anechoic space, aside from the time delaycorresponding to the distance between source and ear, the filter is determinedsolely by the reflection, refraction, and absorption associated with the body,head, and ears of the listener. thus, the transfer functions have been referred toas headrelated transfer functions (hrtfs). of course, when realisticreverberant environments are considered, the transfer functions are influencedby the acoustic structure of the environment as well as that of the human body.estimates of hrtfs for different source locations are obtained by directmeasurements using probe microphones in the listener's ear canals, by roughlythe same procedure using mannequins, or by the use of theoretical models(wightman and kistler, 1989a,b; wenzel, 1992; gierlich and genuit, 1989).once hrtfs are obtained, the simulation is achieved by monitoring headposition and orientation and providing, on a more or less continuous basis, theappropriate hrtfs for the given source location and head position/orientation.the process of measuring hrtfs for a set of listeners is nontrivial interms of time, skill, and equipment. although restriction of hrtfmeasurements to the lower frequencies is adequate for localization in azimuth,it is not adequate for vertical localization, for externalization, or for eliminationof frontback ambiguities (particularly if no head movement is involved). thus,ideally, a sampling frequency of roughly 40 khz is required (corresponding toan upper limit for hearing of 20 khz). similarly, if the hrtfs are measured inan acoustic environment with reverberation, the associated impulse responsescan be very long (e.g., more than a second). these two facts, combined with thedesire to measure hrtfs at many source locations, in many environments, andfor many different listeners, can result in a monstrously timeconsumingmeasurement program.preliminary investigations have demonstrated that, without training,nonindividualized hrtfs cause greater localization error, particularly inelevation and in frontback discrimination, than do hrtfs measured from eachsubject (wenzel et al., 1993a). these results have been interpreted as showingthat hrtfs must be measured for each individual subject in order to achievemaximum localization performance in an auditory se. however, essentially allof the work done to date has been done without regard for the effects that couldbe achieved by means of sensorimotor adaptation (e.g., see welch, 1978). wesuspect that if subjects were giventhe auditory channel147virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.adequate opportunity to adapt in a closedloop se, much of the current concernwith detailed characteristics of the hrtfs and with the importance ofintersubject differences would disappear. in addition, significant advances arenow being made in the modeling of hrtfs and the development of parametricexpressions for hrtfs based on abstractions of the head, torso, and pinnae. infact, in the nottoodistant future, it may be possible to obtain reasonably goodestimates of an individual's hrtfs merely by making a few geometricmeasurements of the outer ear structures. these two factors, combined with theuse of models for describing the effects of reverberation, should greatlysimplify the hrtf estimation problem.given an adequate store of estimated hrtfs, it is then necessary to selectthe appropriate ones (as a function of source and head position/orientation) andfilter the source signal in real time. although some ability to perform suchprocessing has been achieved with relatively simple analog electronics (e.g.,loomis et al., 1990), the devices available for achieving the most accuratesimulations employ digital signal processing. the earliest commerciallyavailable systems (e.g. the focalpoint system from gehring research and theconvolvotron) employed simple timedomain processing schemes to''spatialize" input sound sources. the acoustetron (successor to theconvolvotron) quadrupled the computational capabilities of these earliersystems. it stores 72 pairs of spatial filters sampled at 50 khz for spatialpositions sampled at 30 degrees in azimuth and 18 deg in elevation. the spatialfilters that most closely correspond to the instantaneous position of the sourcerelative to the listener's head are retrieved in real time, interpolated to simulatepositions between the spatial sampling points, and then convolved with theinput sound source to generate appropriate binaural signals. the system canspatialize up to 32 sources in parallel, enabling simulation of simple acousticroom models (with first and secondorder reflections) in real time.another timedomain spatialization system was developed by mckinleyand his associates in the bioacoustics and biocommunications branch of thearmstrong laboratory at wrightpatterson air force base in order to presentthreedimensional audio cues to pilots (r.l. mckinley, personalcommunication, 1992). the hrtfs incorporated into this system, derived frommeasurements using the mannequin kemar, are sufficiently dense in azimuth(hrtfs are measured at every degree in azimuth) to eliminate the need forinterpolation in azimuth. in elevation, the measurements are much less denseand linear interpolation is employed. the researchers at wrightpatterson, inconjunction with tucker davis technologies of gainesville, florida, haverecently developed a new timedomain processor based on their earlier efforts.this machine, which isthe auditory channel148virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.now commercially available, contains more memory than has been available inprevious spatialization systems. in addition, the product has been designed tomaximize the flexibility of the system, allowing researchers to allocate theavailable processing power as necessary for the individual application.frequencydomain filtering is being employed in the convolvotron ii, anextgeneration spatialization device under development at crystal riverengineering. by performing frequencydomain filtering, the convolvotron iiwill be capable of greater throughput for relatively low cost. in addition,because the convolvotron ii is being built from generalpurpose, massproduced digital signal processing (dsp) boards, the system will be completelymodular and easily upgraded.roomacoustics modelingthe state of the art in spatial auditory displays is not yet quite adequate forhighquality ve applications. for example, there are serious questions about theadequacy of the current techniques used for constructing interpolated hrtfs(wightman et al., 1992; wenzel and foster, 1993). similarly, with the currentgeneration processors, limitations in memory and filter size, as well as inprocessing speed and algorithm architecture, have limited the ability to simulatenonisotropic sources or reverberant environments (measured or synthesized).to date, the only realtime auditory spatialization system that included evensimplified realtime room modeling was that developed by foster and hisassociates (foster et al., 1991) for the acoustetron. even though the acousticmodel used was rather simple and provided only a small number of first andsecondorder reflections, this system provides increased realism (e.g., itimproves externalization). the role played by reverberation (as well as otherfactors) in the externalization of sound images produced by earphonestimulation is discussed in durlach et al. (1992c).more sophisticated and realistic soundfield models have been developedfor architectural applications (e.g., lehnert and blauert, 1991, 1992; vian andmartin, 1992), but they cannot be simulated in real time by any of thespatialization systems currently available. an overview of the current state ofthe art for soundfield modeling and a representative collection of contemporarypapers may be found in special issues in applied acoustics (1992, 1993). as thecomputational power of realtime systems increases, the use of these detailedmodels will become feasible for the simulation of realistic environments.the most common approach to modeling the sound field is to generate aspatial map of secondary sound sources (lehnert and blauert, 1989). in thismethod, the sound field due to a source in echoic space is modeledthe auditory channel149virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.as a single primary source and a cloud of discrete secondary sound sources(which correspond to reflections) in an anechoic environment. the secondarysources can be described by three major properties (lehnert, 1993a): (1)distance (delay), (2) spectral modification with respect to the primary source(air absorption, surface reflectivity, source directivity, propagation attenuation),and (3) direction of incidence (azimuth and elevation).in contrast to the digital generation of reverberation (which has a longhistoryše.g., schroeder, 1962), very few people have experience with realtimesoundfield modeling. in order to achieve a realtime realization ofsophisticated sound models, it has been suggested that early and late sources(lehnert and blauert, 1991; kendall and martens, 1984) be treated differently.early reflections would be computed in real time, whereas late reflectionswould be modeled as reverberation.two methods are commonly used to find secondary sound sources: themirrorimage method (allen and berkley, 1979; borish, 1984) and variants ofray tracing (krokstadt et al., 1968; lehnert, 1993a). lehnert and colleagues(shinncunningham et al., 1994) have compared the computational efficiencyof the two methods with respect to their achievable frame rate and their realtime performance for a room of moderate complexity with 24 reflectivesurfaces. a total of 8 firstorder and 19 secondorder reflections were calculatedfor a specific senderreceiver configuration.for this test scenario, the mirrorimage method was more efficient than theraytracing method. in addition, the mirrorimage method is guaranteed to findall the geometrically correct sound paths. for the raytracing method, it isdifficult to predict the required number of rays to find all desired reflections.the raytracing method does have the advantage that it produces reasonableresults even when very little processing time is available, and it can easily beadapted to work at a given frame rate by adjusting the number of rays used,whereas the mirrorimage method cannot be scaled back easily since thealgorithm is recursive. ray tracing will yield better results in more complexenvironments since the dependence of the processing time on the number ofsurfaces is linear not exponential, as is the case for the mirrorimage method.thus, although for the given test case the mirrorimage method was moreefficient, there will probably be some scenarios in which the mirrorimagemethod is superior and others in which the raytracing method offers betterperformance.since calculation of the soundfield model will be the most timeconsuming part of the auditory pipeline, optimization of these calculations isworthwhile (e.g., see the discussion in shinncunningham et al., 1994).computational resources can be assigned as necessary to achieve the necessaryaccuracy of the simulation. if, for example, a primary source is tothe auditory channel150virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.be presented, no reflection filters are required and more resources (i.e., morefilter coefficients) can be assigned to the hrtfs to obtain more precise spatialimaging. for a secondorder reflection, spectral filtering must take place, butsince the directivity of the delayed reflection is less salient psychoacoustically,less accuracy may need to be used for the hrtf filtering. the structure of theauralization unit should allow for task adequate assignment of resources.efficient algorithms and signalprocessing techniques for realtime synthesis ofcomplex sound fields are currently being investigated in a collaborative projectby crystal river engineering and nasa.offhead, hearthrough, and augmentedreality displaysapart from work being conducted with entertainment applications in mind,most of the research and development concerned with auditory displays in these area has been focused on stimulation by means of earphones. however, asindicated previously, such stimulation has two drawbacks: (1) it encumbers thelistener by requiring that equipment be mounted on the user's head and (2) itstimulates only the listener's eardrums.in considering item (1), it should be noted that for many of the headtracking techniques now in use (see chapter 5), headtracking devices as well asearphones must be mounted on the head. if the visual display in the system isalso head mounted, then the ergonomic difficulties associated with adding theearphones are minor: not only would a headtracker presumably be required forthe visual display even if none were required for the auditory display, but alsothe incremental burden of adding earphones to the headmounted visual displaywould be relatively trivial (provided, of course, that the use of earphones wasenvisioned when the mounting system was designed and not added later as anafterthought).in considering item (2), it should be noted that, even though earphones cangenerate sufficient power to deafen the user, stimulation via earphones is totallyinadequate for delivering acoustic power to the user in a manner that affectsbody parts other than the ear. although for most applications in the se field,stimulation of the auditory system via the normal acoustic channel (outer ear,eardrum, middle ear, cochlea, etc.) is precisely what is desired, if one wants toprovide realistic simulations of highenergy acoustic events in the environment,such as explosions or lowaltitude flyovers by fastmoving aircraft, thenacoustic stimulation of the rest of the body (e.g., shaking the user's belly) mayalso be important.the design of offhead auditory displays, i.e., loudspeakers, has been acentral focus of the audio industry for many years. many of the loudspeakersystems now available are, like earphones, more than adequatethe auditory channel151virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.for all se applications with respect to such characteristics as dynamic range,frequency response, and distortion. they are also adequate with respect to cost,although they tend to be more expensive than earphones, particularly if theapplication requires the production of very high intensity levels throughout avery large volume (e.g., loud rock music in a big theater). for se applications,the main problem with loudspeaker systems, as it is with earphones, is that ofachieving the desired spatialization of the sounds (including both the perceivedlocalization of the sound sources and the perceived acoustic properties of thespace in which the sources are located).the major problem that arises in spatialization for loudspeaker systemsthat does not arise when earphones are used concerns the difficulty ofcontrolling the signals received at the two eardrums (and the differencesbetween these two signals). unlike the situation with earphones, in which thesignal received at a given eardrum is determined simply by the signaltransmitted by the earphone at that ear (and the fixed filtering associated withacoustic transmission from the earphone transducer to the eardrum), when aloudspeaker system is used, the signal received at the given eardrum isinfluenced by all the signals transmitted by all the loudspeakers in the room,together with all the transformations that each of the signals undergoes as itpropagates through the room from the loudspeaker to the eardrum. among thenames used to designate various approaches to one or more components of thisproblem are binaural stereo, transaural stereo, spectral stereo, multichannelstereo, quadrophony, ambisonics, and surround sound. even when a givensystem is tuned to provide an adequate perception for a given position of thelistener's head, this perception is likely to rapidly degrade as the listener's headis moved out of the "sweet spot." to date, no loudspeaker system has beendeveloped that incorporates headtracking information and uses this informationto appropriately adjust the inputs to the loudspeakers as the position ororientation of the listener's head is altered.background material on the spatialization problem for signals presented bymeans of loudspeakers is available in eargle, 1986; trahiotis and bernstein,1987; cooper and bauck, 1989; griesinger, 1989; and maccabe and furlong,1994.within the ve area, one of the bestknown systems that makes use of offhead displays is the cave, a ve system developed at the electronicvisualization laboratory, university of illinois at chicago (cruzneira et al.,1993). the current cave system employs four identical speakers located at thecorners of the ceiling and amplitude variation (fading) to simulate direction anddistance effects. in the system now under development, speakers will be locatedat all eight corners of the cube and reverberation and highfrequency attenuationwill be added to the parametersthe auditory channel152virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.that can be used for spatialization purposes. in a new offhead system beingdeveloped by crystal river engineering, attempts are being made to utilizeprotocols similar to those used in the earphone spatialization systems (e.g., theconvolvotron), so that the user will be able to change from one type of systemto the other with minimal waste of time and effort.finally, it should be noted that relatively little attention has been given toaugmented reality in the auditory channel. as in the visual channel, there arelikely to be many applications in which it is necessary to combine computersynthesized or sampled audio signals with signals obtained (1) directly from theimmediate environment or (2) indirectly from a remote environment by meansof a teleoperator system. in principle, the signals from the immediateenvironment can be captured from controlled acoustic leakage around theearphones (hearthrough displays) or by positioning microphones in theimmediate environment (perhaps on the headmounted display, hmd) andadding signals in the electronic domain rather than the acoustic domain.however, because one may want to manipulate the environmental signalsbefore adding it, or because one may want to use the same system for the casein which the sources of the environmental signals are remote and sensed by atelerobot, the latter approach seems preferable. ideally, an acoustic augmentedreality system should be capable of receiving signals sensed by microphones inany environment (immediate or remote), transforming these signals in a mannerthat is appropriate for the given situation, and then adding them to the signalspresented by the ve system. currently, the most obvious use of acousticaugmentedreality systems is to enable an individual who is deeply immersed insome ve task to simultaneously monitor important events in the real world(e.g., the occurrence of warning signals in the real world).computer generation of nonspeech audiowhile much work remains to be done in the areas of sound spatializationand modeling of environmental effects, even less has been accomplished in thearea of physical modeling and the computer generation of nonspeech sounds.how can we build a realtime system that is general enough to produce thequasimusical sounds usually used for auditory icons, as well as purelyenvironmental sounds, like doors opening or glass breaking? the idealsynthesis device would be able to flexibly generate the entire continuum ofnonspeech sounds described above as well as be able to continuously modulatevarious acoustic parameters associated with these sounds in real time. such adevice or devices would act as the generator of acoustic source characteristicsthat would then serve asthe auditory channel153virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.the inputs to a sound spatialization system. thus, initially at least, sourcegeneration and spatial synthesis would remain as functionally separatecomponents of an integrated acoustic display system. while there wouldnecessarily be some overhead cost in controlling separate devices, theadvantage is that each component can be developed, upgraded, and utilized asstandalone components so that systems are not locked into an outmodedtechnology.synthesis techniquesmany of the algorithms likely to be useful for generating nonspeechsounds will be based on techniques originally developed for the synthesis ofmusic as well as speech. the main goal in speech synthesis is the production ofintelligible (and natural sounding) speech waveforms. to do this, one mustaccurately synthesize the output of a specific type of instrument, the humanvoice. also, in speech synthesis, the final acoustic output can be rated accordingto the measurable amount of information conveyed and the naturalness of thespeech. in synthesizing music, typically the goals are not as specific orrestricted: they are defined in terms of some subjective criteria of the composer.usually, the goal is to produce an acoustic waveform with specific perceptualqualities: either to simulate some traditional, physical acoustic source or toproduce some new, unique sound with appropriate attributes.because the aims of synthesized music are more diverse than those ofspeech synthesis, there are a number of different, acceptable methods for itssynthesis. choice of the method depends on the specific goals, knowledge, andresources of the composer. in any synthesis method, the composer controlssome set of timevarying parameters to produce an acoustic waveform whoseperceptual qualities vary accordingly. these computercontrolled parametersmay be related to a physical parameter in a modeled instrument, to the shape orspectrum of the acoustic waveform, or to the perceptual quality desired for thesound. often, these varying techniques are combined to get a specific effect.some of the most common techniques are described below.one method used for computercontrolled synthesis is known as additive synthesis. in this method, a synthesized voice (or instrumental line) is generatedby the addition of simple sine waves using shorttime fourier methods. one ofthe problems with this method is that the number of parameters needed togenerate an acoustic signal with specific qualities is large, reflecting the factthat many important music percepts are far removed from the shorttime fourierparameters. thus, synthesizing a particular sound quality can be cumbersome.often, additive synthesis is used to simulate known sounds by first analyzingthe desired sound andthe auditory channel154virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.measuring these parameters directly (grey and moorer, 1977; moorer, 1978;portnoff, 1980; dolson, 1986). small alterations in these known parameters canthen finetune the acoustic waveform.another common technique, which cook et al. (1991) have described as anexample of an abstract algorithm, is frequency modulation or fm synthesis, inwhich the composer specifies the carrier frequency, modulation index, andmodulation frequency of a signal (chowning, 1973; chowning and bristow,1986; schottstaedt, 1977). by varying the relation of the carrier to modulationfrequency, the resultant sound can be harmonic or inharmonic. if harmonic, therelation defines what overtones exist in the sound (important in timbreperception). changing the value of the modulation index controls the spread ofthe spectrum of the resultant sound, and therefore its perceptual brightness.since the relationship between the values of the frequency modulationparameter and the corresponding perceptual aspects of the synthesized output isreasonably straightforward, the frequency modulation technique has provenboth powerful and versatile. this technique is employed by many commerciallyavailable synthesizers, such as the yamaha dx7.subtractive synthesis is a term in music synthesis that refers to the shapingof a desired acoustic spectrum by one or more filtering operations and is aprecursor to the more modern approaches that have come to be known asphysical modeling. subtractive synthesis is often used when a simple physicalmodel of an instrument can be described by an excitatory source that drivessome filter(s). both the source waveform and the filters, which may or may notbear any relation to a real instrument, are specified by the composer.most traditional instruments can also be mathematically modeled in thisway: they are excited by some vibratory noise source, and their physicalproperties acoustically filter that source. for brasslike instruments, vibrationsof the player's lips are the acoustic source; in a stringed instrument, it is thestring vibration; for a singer, it is the actions of the vocal chords. the physicalproperties that affect the acoustic output include the shape of the instrument, itseffective length, the reflectivity of the material out of which the instrument isconstructed, etc. by modeling the vibratory mode of the excitatory source andthe acoustic effects of the physical properties of the instrument, subtractivesynthesis has been used to synthesize known instruments (jaffe and smith,1983; risset and mathews, 1969). as an instrument is played, the effectivedimensions of the instrument are altered by opening or closing keys or valves.these changes cause the modeled filters to change in time. because suchmodeling is closely tied to physical parameters, it may be less cumbersome (andintuitively easier) to adjust parameters to achieve a desired effect.another alternative is to develop methods of generating sounds bythe auditory channel155virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.modeling the motions of the physical sound events, i.e., by numericalintegration of the wave equation. generating sound by solving the equations ofmotion of a musical instrument captures a natural parameterization of theinstrument and includes many of the important physical characteristics of thesound. the conventional and perhaps most general way of representing anacoustics system is to use a set of partial differential equations (pdes) in thetemporal and threedimensional spatial domain. however, it is not practicalconsidering the intensive numerical computation and the constraint of realtimecomputing. to reduce the computational complexity of the sound generationtasks without giving up the physical essence of the representation, one approachis to use the aggregate properties of the physical model instead of solving theproblem at the microscopic level, i.e., solving the pdes.as noted above, subtractive synthesis was the first attempt at this type ofmodeling of aggregate properties. several more recent physical modelingtechniques based on aggregate properties have been developed, including thedigital waveguide technique, transfer function techniques, modal synthesis, andmass spring models to synthesize sounds ranging from musical instruments tothe singing voice of the human vocal tract (borin et al., 1992; cadoz et al.,1993; cook, 1993; djoharian, 1993; keefe, 1992; morrison and adrien, 1993;smith, 1992; wawrzynek, 1991; woodhouse, 1992).collision sounds are an example of a simple auditory event in a virtualenvironment. we may predict the potential collision of two objects byobserving their paths with our eyes but, short of actual breaking or deformationof the objects, it is the sound of the collision that best reveals how the structureof the objects has been affected by the collision.to illustrate how to generate collision sounds by using physically basedmodels, we choose a uniform beam as an example structure. the vibrationalmechanics theory of beam structures has been examined carefully and canprovide a solid groundwork for collision sound synthesis. the collision can bedecomposed into two parts, the excitation component, which initiates the impactevent and the resonator component, in which the interesting vibrationphenomena take place.the excitation module is affected by the force, position, and duration ofthe impact; the resonator module is determined by the structure boundaryconditions, material density, modulus of elasticity, and object geometry (e.g.,length, width, and height). because the uniform beam has a simple structure,one can derive the equations to depict its major vibrational modes and calculatethe natural resonant frequency associated with each mode based on theaggregate physical properties. the natural resonant frequency reveals the stronglinkage between material types and object shapes and can show objects'attributes. for complex freeformthe auditory channel156virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.objects, finite element analysis can be used to calculate the associated resonantfrequency for the major vibrational modes.additional topics that are often grouped into the broad field of musicsynthesis and may be relevant to the use of sound in ses include the use ofspecial computational structures for the composition of music, specifichardware and software developed for music synthesis, and notational orcomputational conventions that are specialized for music synthesis. ofparticular interest is the research by cadoz and associates that makes use ofhumanmachine interfaces with tactual feedback and focuses on the productionof music by gestural control of simulated instruments (cadoz et al., 1984, 1990;cadoz and ramstein, 1991). further information on music synthesis is availablein moore (1990), mathews and pierce (1989), roads and strawn (1988), andrichards (1988).synthesis technologycurrent devices available for generating nonspeech sounds tend to fall intotwo general categories: samplers, which digitally store sounds for later realtime playback, and synthesizers, which rely on analytical or algorithmicallybased sound generation techniques originally developed for imitating musicalinstruments (see cook et al., 1991; scaletti, 1994; and the discussion above).with samplers, many different sounds can be reproduced (nearly) exactly, butsubstantial effort and storage media are required for accurately prerecordingsounds, and there is usually limited realtime control of acoustic parameters.synthesizers, in contrast, afford a fair degree of realtime, computerdrivencontrol.most widely available synthesizers and samplers are based on midi(musical instrument digital interface) technology. the baudrate of such devices(31.25 kbar), especially when connected to standard serial computer lines (19.2kbar), is still low enough that continuous realtime control of multiple sources/voices will frequently "choke" the system. in general, synthesisbased mididevices, such as those that use frequency modulation, are more flexible thansamplers in the type of realtime control available but less general in terms ofthe variety of sound qualities that can be reproduced. for example, it is difficultto generate environmental sounds, such as breaking or bouncing objects, froman fm synthesizer (see gaver, 1993).largescale systems designed for sound production and control in theentertainment industry or in music composition incorporate both sampling anddigital synthesis techniques and are much more powerful. however, they arealso expensive, require specialized knowledge for their use, and are primarilydesigned for offline sound design and postproduction. a potentialdisadvantage of both types of devices is that they arethe auditory channel157virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.primarily designed with musical performance and/or sound effects in mind.this design emphasis is not necessarily well suited to the generation and controlof sounds for the display of information and, again, tends to require that thedesigners and users have specialized knowledge of musical or productiontechniques.the most general systems would be based on synthesis via physical oracoustical models of sound source characteristics. a simpler but less versatileapproach would use playback of sampled sounds or conventional midi devices,as in most current systems. since very general physical models are bothdifficult (perhaps impossible) to develop and computationally intensive, a morepractical and immediately achievable system might be a hybrid approach thatuses techniques like realtime manipulation of simple parameters, such as thepitch, filter bandwidth, or intensity, of sampled sounds, and realtimeinterpolation between sampled sounds, analogous to ''morphing" in computergraphics; the emu morpheus synthesizer is an example of this kind of approach.recently, several commercial synthesizer companies have announced newproducts based on physical modeling techniques. a sound card being developedby mediavision is based on digital waveguides (smith, 1992); the yamaha vl1keyboard synthesizer is based on an unspecified physical modeling approach;and the macintoshbased korg synthkit allows construction of sounds viainterconnection of a visual programming language composed of modular unitsrepresenting hammerstrikes, bows, reeds, etc.a few nonspeechsoundgeneration systems have been integratedspecifically for virtual environment applications (e.g., wenzel et al., 1990; vplresearch's audiosphere; see also wenzel, 1994); some designers are beginningto develop systems intended for data "sonification" (e.g., smith et al., 1990;scaletti and craig, 1991; wenzel et al., 1993b). related developments inauditory interfaces include the work on audio "windowing" systems forapplications like teleconferencing (ludwig et al., 1990; cohen and ludwig,1991). however, far more effort needs to be devoted to the development ofsound generation technology specifically aimed at information display (seekramer, 1994). even more critical, perhaps, is the need for further research onlowerlevel sensory and higherlevel cognitive determinants of acousticperceptual organization, since these results should serve to guide technologydevelopment. furthermore, relatively little research has been concerned withhow various acoustic parameters interact to determine the identification,segregation, and localization of multiple, simultaneous sound streams.understanding such interaction effects is likely to be critical in any acousticdisplay developed for se systems.with the advances in algorithms and hardware to produce such simulations,the auditory channel158virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.there is also a need to develop an extensible protocol for virtual audio. such aprotocol will need to encompass all the acoustic models in use today and thoseexpected to be developed in the near future. this protocol should allowdevelopers and designers of sonification systems to utilize se technology, evenas that technology makes dramatic improvements in its capabilities.research needsmost of the needs associated with the auditory channel lie in the domain ofperceptual studies. with one major exception, discussed below, the technologyfor the auditory channel is either adequate now or will be adequate in the nearfuture.perceptual issuesmany of the perceptual issues in the auditory channel that require attentionfall under the general heading of adaptation to alterations in sensorimotor loops.some specific examples in this category, all of which relate to the spatializationproblem, concern the extent to which (and rate at which) listeners adapt tovarious types of distortions, including those associated with (1) the use ofsimplified hrtfs or transformations designed to achieve supernormallocalization performance in ve systems and (2) various mappings betweentelerobotic sensor arrays and the human auditory system in teleoperator systemsthat make use of nonanthropomorphic telerobots. knowledge of how well andhow fast individuals can adapt to distortions or transformations of these typesunder various training conditions is essential to the design of effective systems.other closely related examples focus on the use of the auditory channel forsensory substitution purposes, e.g., the presentation of auditory signals tosubstitute for haptic feedback that is difficult to provide because of currentequipment limitations.another major area in the perceptual domain that requires substantial workfalls under the heading of auditory information displays. current knowledge ofhow to encode information for the auditory channel in ways that producerelatively high information transfer rates with relatively small amounts oftraining is still rather meager. it is obvious that encoding all information intospeech signals is inappropriate and the statement that the encoding should benatural is simply not adequate to guide specific design choices. this generalencoding and display problem is judged to be both important and difficult tosolve. also, it is believed that progress in this area will depend, at least in part,on an improved understanding of auditory scene analysis.the auditory channel159virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.technologythe main technology area that requires attention concerns the computergeneration of sounds: software and hardware are required to enable ve systemusers to specify and generate the desired acoustic behavior of the objects andagents in the ve under consideration. physical modeling of environmentalsounds, together with the development of appropriate mathematicalapproximations and software and hardware that enables such sounds to becomputed and rendered in real time, constitutes a major task in this area.subsequently, it will be necessary to develop a userfriendly system forproviding speech, music, and environmental sounds all in one integratedpackage. and eventually, of course, the whole sound generation system willhave to be appropriately integrated with the software and hardware system forgenerating visual and haptic images. compared with these sound generationproblems, most of the remaining technological problems in the spatializationarea for both hmds and offhead displays appear relatively minor. it should benoted, however, that current technology is still unable to provide interactivesystems with realtime rendering of acoustic environments with complex,realistic room reflections.the auditory channel160virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.4haptic interfaceshaptic interfaces are devices that enable manual interaction with virtualenvironments (ves) or teleoperated remote systems. they are employed fortasks that are usually performed using hands in the real world, such as manualexploration and manipulation of objects. in general, they receive motor actioncommands from the human user and display appropriate tactual images to thehuman. such haptic interactions may or may not be accompanied by thestimulation of other sensory modalities, such as vision and audition. computerkeyboards, mice, and trackballs constitute relatively simple haptic interfaces.other examples of haptic interfaces available in the market are gloves andexoskeletons that track hand postures and joysticks that can reflect forces backto the user. even more sophisticated devices have been built and implementedsuccessfully in research laboratories. to realize the full promise of ves andteleoperation, further development of haptic interfaces is critical. in pursuingthis goal, many of the issues and technologies described in the sections onposition tracking (chapter 5) and telerobotics (chapter 9) are relevant. toachieve success, a comprehensive research program is needed in human haptics,technology development, and interactions between the two.in contrast to the purely sensory nature of vision and audition, only thehaptic system can both sense and act on the environment. the human hand is aversatile organ that is able to press, grasp, squeeze, and stroke objects; it canexplore object properties such as surface texture, shape, and softness; it canmanipulate tools for repairing a watch and breaking concrete. in the words ofpaul valéry (1938), the hand is "a device which can,haptic interfaces161virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.in turn, strike, receive and give, feed, take an oath, beat a musical rhythm, readfor the blind, speak for the mute, reach to a friend, stop a foe." being able totouch, feel, and manipulate objects in an environment, in addition to seeing (andhearing) them, provides a sense of immersion in the environment that isotherwise not possible. it is quite likely that much greater immersion in a vecan be achieved by the synchronous operation of even a simple haptic interfacewith a visual and auditory display, than by large improvements in, say, thefidelity of the visual display alone. real environments or ves in which one isdeprived of the touch and feel of objects seem impoverished, seriously handicaphuman interaction capabilities, and, at worst, can be disorienting.although haptic interfaces are typically designed to be operated by theuser's hands, alternative designs suitable for the tactual and motor systems ofother body segments are conceivable. however, not all interfaces that interactwith the human mechanosensorimotor systems are haptic interfaces. thedistinction is based on the nature of the tasks for which the interface is used. forexample, whole body motion interfaces (chapter 6) concerned with conveying asense of mobility to the user are not haptic interfaces in a strict sense.status of the relevant human researchthe human haptic systemin order to develop costeffective haptic interfaces, it is necessary tounderstand the roles played by the mechanical, sensory, motor, and cognitivesubsystems of the human haptic system. the mechanical structure of the humanhand consists of an intricate arrangement of 19 bones, connected by almost asmany frictionless joints and covered by soft tissue and skin. altogether, thebones are attached to about 20 intrinsic and extrinsic muscles through numeroustendons, which serve to activate 22 degrees of freedom (dof) of the hand. thesensory system includes large numbers of various classes of receptors and nerveendings in the skin, joints, tendons, and muscles. appropriate mechanical,thermal, and chemical stimuli activate these receptors, causing them to transmitelectrical impulses via the afferent neural network to the central nervous system(of which the brain forms a part), which in turn sends commands through theefferent neurons to the muscles for desired motor action.haptic exploration and manipulation of solid objects covers a wide rangeof haptic functions yet provides a task framework within which the roles of thebiomechanical, sensory, motor, and cognitive subsystems can be understood.exploration is concerned mainly with the extraction of object properties, and itis therefore a sensory dominant task, althoughhaptic interfaces162virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.wellcontrolled motor actions are necessary to obtain reliable information aboutthe object. it consists primarily of discrimination or identification of surfaceproperties (for example, shape and surface texture) and volumetric properties(for example, mass and compliance) of objects. manipulation is concernedmainly with modification of the environment and thus it is a motor dominanttask, although sensory feedback is essential for successful performance.manipulation tasks can be grossly subdivided into precision tasks (for example,watch repair) and power tasks (for example, using a hammer).in any task involving physical contact with an object, be it for explorationor manipulation, the surface and volumetric physical properties of the skin andsubcutaneous tissues play important roles in its successful performance. forexample, the finger pad, which is used by primates in almost all precision tasks,consists of hairless ridged skin (about 1 mm thick) that encloses soft tissuescomposed of mostly fat in a semiliquid state. as a block of material, the fingerpad exhibits complex mechanical behavioršinhomogeneity, anisotropy, andrate and time dependence. the compliance and frictional properties of the skin,together with the sensory and motor capabilities of the hand, enable one to bothglide over a surface without losing contact, to explore the shape of the surface,and to stably grasp a smooth object to manipulate it. the mechanical loading onthe skin, the transmission of the mechanical signals through the skin, and theirtransduction by the cutaneous mechanoreceptors are all strongly dependent onthe mechanical properties of the skin and subcutaneous tissues (phillips andjohnson, 1981b; srinivasan, 1989; srinivasan and dandekar, 1992).tactual sensory information from the hand in contact with an object can bedivided into two classes: (1) tactile information, referring to the sense of contactwith the object, mediated by the responses of lowthreshold mechanoreceptorsinnervating the skin (say, the finger pad) within and around the contact regionand (2) kinesthetic information, referring to the sense of position and motion oflimbs along with the associated forces, conveyed by the sensory receptors in theskin around the joints, joint capsules, tendons, and muscles, together withneural signals derived from motor commands. (the term proprioceptive is usedalmost equivalently to kinesthetic by many authors.) for discussion ofterminology see dariansmith (1984); loomis and lederman (1986). onlytactile information is conveyed when objects contact a passive, stationary hand,except for the everpresent kinesthetic information about the limb posture. onlykinesthetic information is conveyed during active, free (i.e., no contact with anyobject or other regions of skin) motion of the hand, although the absence oftactile information by itself conveys that the motion is free. even when the twoextreme cases just mentioned are included, it is clearhaptic interfaces163virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.that all sensory and manipulatory tasks performed actively with the normalhand involve both classes of information. in addition, free nerve endings andspecialized receptors that signal skin temperature, mechanical and thermal pain,and chemogenic pain and itch are also present (sherrick and cholewiak, 1986).the control of contact conditions is often as important as sensing thoseconditions for successful task performance. in humans, such control action canrange from a fast spinal reflex to a relatively slow conscious deliberate action.in experiments involving lifting of objects held in a pinch grasp, it has beenshown that motor actions such as increasing grip force are initiated as rapidly aswithin 70 ms after an object begins to slip relative to the finger pad, and that thesensory signals from the cutaneous afferents are critical for task performance(johansson and westling, 1984; johansson and cole, 1992). clearly, themechanical properties of the skin and subcutaneous tissues, the rich sensoryinformation provided by a wide variety of sensors that monitor the taskscontinuously, and the coupling of this information with the actions of the motorsystem are responsible for the human abilities of grasping and manipulation. inthe following three subsections we employ a systems viewpoint to brieflyreview the results on haptics in the psychophysics and neurophysiologyliterature.inputoutput variables of haptic interactionshaptic interfaces in teleoperation or ve systems receive the intendedmotor action commands from the human and display tactual images to thehuman. the primary inputoutput variables of the interfaces are displacementsand forces, including their spatial and temporal distributions. haptic interfacescan therefore be viewed as generators of mechanical impedances that representa relationship between forces and displacements (and their derivatives) overdifferent locations and orientations on the skin surface at each instant of time. incontact tasks involving finite impedances, either displacement or force can beviewed as the control variable, and the other is a display variable, depending onthe control algorithms employed. however, consistency among free handmotions and contact tasks is best achieved by viewing the timevaryinggeometrical configuration of the hand (for example, the vector of all jointangles and their derivatives with respect to time) as the control variable, and theresulting net force vector and its distribution within the contact regions as thedisplay variables.because the human is sensing and controlling the position and forcevariables of the haptic interface, the performance specifications of the interfaceare directly dependent on human abilities. in a substantial numberhaptic interfaces164virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.of simple tasks involving active touch, one of the tactile and kinestheticinformation classes is fundamental for discrimination or identification, whereasthe other is supplementary. for example, in the discrimination of length of rigidobjects held in a pinch grasp between the thumb and the forefinger (durlach etal., 1989), kinesthetic information is fundamental, whereas tactile information issupplementary. in such tasks, sensing and control of variables such as fingertipdisplacements are crucial. in contrast, for the detection of surface texture orslip, tactile information is fundamental, whereas kinesthetic information issupplementary (srinivasan et al., 1990). here, the sensing of spatiotemporalforce distribution within the contact region provides the basis for inferencesconcerning the contact conditions and object properties. both classes ofinformation are clearly necessary and equally important in more complex haptictasks.we now summarize briefly the psychophysical and neurophysiologicalresults available on human haptic abilities in real environments at two levels:(1) sensing and control of interface variables and (2) perception of contactconditions and object properties. although humans can feel heat, itch, pain, etc.through sensory nerve endings in the skin, we refrain from discussing thesesensations here because the availability of practical interface devices employingthem is unlikely in the near future.sensing and control of interface variableslimb position and motionour awareness of the relative positions and motions of our limbs arisesfrom the kinesthetic sensory system, which consists of sensory receptors in thejoint capsules, tendons, muscles, and skin around the joints, as well as thesignals derived from motor commands during voluntary motion (see reviews byclark and horch, 1986; matthews, 1982). the joint capsules are innervated bythree different types of mechanoreceptive nerve terminals, namely, free nerveendings, ruffini corpuscles, and paciniform corpuscles (dariansmith, 1984),each of which have distinct response characteristics. in addition, the tendonscontain golgi organs, which seem to respond to tension, and the musclespindles measure the muscle stretch and its rate of change. the skin around thejoints contains four types of sensory endings (discussed below) in the hairlessskin, in addition to receptors in the hair follicles in the hairy skin, with eachreceptor type coding different aspects of the mechanical loading imposed on theskin. furthermore, the efferent copy (also referred to as corollary discharge) ofthe command signals generated to drive the muscles during voluntarymovements gives information about the intended motor actionhaptic interfaces165virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.to the perceptual portions of the brain. because of the presence of multiple,simultaneously active subchannels that are not individually accessible toexperimentation, even basic questions about the functioning of the kinestheticsensory system have not been answered unequivocally.the source of kinesthetic information that enables us to know the relativepositions of limb segments or joint angles is still controversial (clark andhorch, 1986). initially, it was proposed that the receptors in the joints were thesource (skoglund, 1956; mountcastle and powell, 1959). later it was foundfrom neurophysiological experiments that these receptors were activated only inthe extremities of the range of joint rotations (burgess and clark, 1969; griggand greenspan, 1977). also, patients with artificial joints did not seem to losetheir joint angle sense significantly (grigg et al., 1973). it also should be notedthat ferrel (1980) has argued that joint afferent discharge is sufficient to helpsignal the joint angle over its full range, but does not claim that the afferents areexclusively responsible for position sense (ferrel et al., 1987). muscle spindles,which are believed to be muscle length detectors, have also been proposed ascandidates that provide position sense (matthews, 1982). support for thishypothesis comes from the wellknown haptic illusion that, when vibration isimposed on muscles and tendons, the corresponding limbs are perceived to bemoving (goodwin et al., 1972). however, because of cocontractions of agonistand antagonist muscles, the lengths of muscles may change without any changein the joint angle. thus, computations involving all the muscular forcesimposed on the joint are needed to extract the joint angle information frommuscle spindles. nevertheless, matthews (1988) has proposed that it might bepossible to recover angular velocity independent of position by combining thespindle signals with corollary discharges from motor centers. the third possiblesource of joint angle information is the stress and strain field in the skinsurrounding the joint, which is directly related to the angle of rotation of thejoint. although this possibility has been mentioned in the literature, we are notaware of any systematic investigation of this hypothesis. recently, edin (1993)has shown that the strains produced in the skin can be large enough to signal thejoint angle.a large variety of psychophysical experiments have been conductedconcerning the perception of limb position and motion (clark and horch, 1986;jones and hunter, 1992). it has been found that humans can detect jointrotations of a fraction of a degree performed over a time interval of the order ofa second. the bandwidth of the kinesthetic sensing system has been estimatedto be 2030 hz (brooks, 1990). it is generally accepted that our sensitivity torotations of proximal joints is higher than that of more distal joints. the justnoticeable difference (jnd) is about 2.5 deg for the finger joints, 2 deg for thewrist and elbow, and about 0.8 deg for thehaptic interfaces166virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.shoulder (tan et al., 1994). in locating a target position by pointing a finger, thespeed, direction, and magnitude of movement, as well as the locus of the target,can all affect accuracy. in the discrimination of length of objects by the fingerspan method (durlach et al., 1989; tan et al., 1992), the jnd is about 1 mm fora reference length of 10 mm, and increases to 24 mm for a reference length of80 mm, thus violating weber's law (i.e., jnd is not proportional to thereference length). in the kinesthetic space, psychophysical phenomena such asanisotropies in the perception of distance and orientation, apparent curvature ofstraight lines, noneuclidean distance measures between two points, etc., havebeen reported (for a review, see loomis and lederman, 1986; hogan et al.,1990; fasse et al., 1990). investigations of the human ability in controlling limbmotions have typically measured human tracking performance withmanipulanda having various mass, spring, and damping characteristics (brooks,1990; poulton, 1974; sheridan, 1992; jones and hunter, 1992). the differentialthresholds for position and movement have been measured to be about 8 percent(jones and hunter, 1992). human bandwidth for limb motions is found to be afunction of the mode of operation: 12 hz for unexpected signals, 25 hz forperiodic signals, up to 5 hz for internally generated or learned trajectories, andabout 10 hz for reflex actions (reviews by brooks, 1990). in summary, thesensing and control of limb position and motion are complex at all levels,ranging from psychophysical measures to the inner neurophysiologicalmechanisms.net forces of contactwhen we contact or press objects through active motion of the hand, thecontact forces are sensed by both the tactile and kinesthetic sensory systems.overall contact force is probably the single most important variable thatdetermines both the neural signals in the sensory system as well as the controlof contact conditions through motor action. it appears that the jnd for contactforce is 515 percent of the reference force value over a wide range ofconditions involving substantial variation in force magnitude, muscle system,and experimental method, provided that the kinesthetic sense is involved in thediscrimination task (jones, 1989; pang et al., 1991; tan et al., 1992). in closelyrelated experiments exploring the human's ability to distinguish among objectsof different weights, a slightly higher jnd of about 10 percent has beenobserved (see reviews by clark and horch, 1986; jones, 1986). an interestingillusion first observed in the late nineteenth century by weber is that coldobjects feel heavier than warm ones of equal weight (see review by sherrickand cholewiak, 1986). in experiments involving grasping and lifting of objectsusing a twofinger pinch grasp, johansson and westling (1984) have shown thathaptic interfaces167virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.subjects have exquisite control over maintaining the proper ratio betweengrasping and lifting forces (i.e., the orientation of the contact force vector), sothat the objects do not slip. however, when tactile information was blockedusing local anesthesia, this ability deteriorated significantly because the subjectscould not sense contact conditions such as the occurrence of slip and hence didnot apply appropriate compensating grasp forces. thus, good performance intasks involving contact requires the sensing of appropriate forces as well asusing them to control contact conditions. the maximum controllable force thatcan be exerted by a finger pad is about 100 n and the resolution in visuallytracking constant forces is about 0.04 n or 1 percent, whichever is higher(srinivasan and chen, 1993; tan et al., 1994).perception of contact conditions and object propertiesalthough humans experience a large variety of tactile sensations, thesesensations are really combinations of a few building blocks or primitives. forsimplicity, normal indentation, lateral skin stretch, relative tangential motion,and vibration are the primitives for conditions of contact with the object.surface microtexture, shape (mm size), and compliance can be thought of as theprimitives for the majority of object properties perceived by touch. the humanperception of many of these primitives is through tactile information conveyedby cutaneous mechanoreceptors. the associated neural codes can be classifiedas intensive, temporal, spatial, and spatiotemporal (for a review, see loomisand lederman, 1986). we first describe neurophysiological findings of receptorresponse characteristics from experiments involving indentations with roundedprobes, and then discuss results from experiments involving pressing andstroking of objects configured to emphasize specific geometric or materialproperties.monkeys are used as experimental models for physiological mechanismsin humans since the types of mechanoreceptors, their spacing in the skin, andthe sensory capacities to detect and discriminate vibratory stimuli are similar forthe two species. in monkey skin, on the finger pads and palm,mechanoreceptive afferents have been classified on the basis of their responseproperties to ramp and steady indentations of a probe with or without vibration(knibestol and vallbo, 1970; mountcastle et al., 1972; pubols, 1980; pubolsand pubols, 1976, 1983; talbot et al., 1968). they fall into three distinctclasses. (1) slowly adapting afferents (sas), believed to originate from merkelcells, respond both during ramp onset and steady indentation by a probe. whenthe probe is vibrated sinusoidally at the most sensitive spot on the skin, the sasare tuned (i.e., one nerve impulse per stimulus cycle) at the lowest amplitudes(about 20 µ) when the frequencieshaptic interfaces168virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.are low (less than 20 hz). (2) rapidly adapting afferents (ras), emanating frommeissner corpuscles, respond to ramp onset but are quiet during steadyindentation. their tuning threshold amplitudes (about 5 µ) are the lowest in themiddle frequency range (2050 hz). (3) pacinian corpuscle fibers (pcs), whilebehaving in a similar manner to ras for ramp and steady indentations, havevery low tuning threshold amplitudes (about 1 µ) at high frequency ranges(100300 hz). microneurographic techniques of recording singlenerve fiberresponses from awake humans (vallbo and hagbarth, 1968; knibestol andvallbo, 1970; johansson and vallbo, 1979) have revealed another class ofslowly adapting afferents that are primarily sensitive to skin stretch and areassociated with ruffini endings. the response properties, such as thresholds andbandwidths of each of the receptor types obtained through neurophysiologicalexperiments, give some of the design specifications for tactile display part ofhaptic interfaces.considerable research effort has been invested on psychophysics ofvibration perception and electrocutaneous stimulation using single or multipleprobes (for a review, see sherrick and cholewiak, 1986). these studies aremostly directed at issues concerned with tactile communication aids forindividuals who are blind, deaf, or deaf and blind, areas that are beyond thescope of this chapter. a comprehensive list of references describing such tactiledisplays can be found in kaczmarek and bachyrita (1993) and reed et al.(1982). in designing these devices, human perceptual abilities in both temporaland spatial domains are of interest. the human threshold for the detection ofvibration of a single probe is about 28 db (relative to 1 µ peak) for 0.4 to 3 hz.it decreases at the rate of 5 db/octave for 3 to 30 hz, and decreases further atthe rate of 12 db/octave for 30 to about 250 hz, after which the thresholdincreases for higher frequencies (rabinowitz et al., 1987; bolanowski et al.,1988). spatial resolution on the finger pad, as measured by the localizationthreshold of a point stimulus, is about 0.15 mm (loomis, 1979), whereas thetwopoint limen is about 1 mm (johnson and phillips, 1981).to answer questions concerning perception and neural coding ofroughness or spatial resolution, precisely shaped rigid surfaces consisting ofmmsized bar gratings (lederman and taylor, 1972; morley et al., 1983;phillips and johnson, 1981a,b; sathian et al., 1989), embossed letters (phillipset al., 1983, 1988), or braille dots (lamb, 1983a,b; dariansmith et al., 1980)have been used in psychophysical and neurophysiological experiments (seereview by johnson and hsiao, 1992). the perception of surface roughness ofgratings is found to be solely due to the tactile sense and is dependent on thegroove width, contact force, and temperature but not the scanning velocity(loomis and lederman, 1986). spatial resolution on the finger pad, asmeasured by the localization threshold of ahaptic interfaces169virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.point stimulus is about 0.15 mm (loomis, 1979), whereas the twopoint limenis about 1 mm (johnson and phillips, 1981).some of the salient results on the perception of slip, microtexture, shape,compliance, and viscosity are given below. humans can detect the presence of a2 µ high single dot on a smooth glass plate stroked on the skin, based on theresponses of meissnertype rapidly adapting fibers (ras) (lamotte andwhitehouse, 1986; srinivasan et al., 1990). moreover, humans can detect 0.06µ high grating on the plate, owing to the response of pacinian corpuscle fibers(lamotte and srinivasan, 1991). among all the possible representations of theshapes of objects, the surface curvature distribution seems to be the mostrelevant for tactile sensing (srinivasan and lamotte, 1991; lamotte andsrinivasan, 1993). slowly adapting fibers respond to both the change and rate ofchange of curvature of the skin surface at the most sensitive spot in theirreceptive fields, whereas ras respond only to the rate of change of curvature.human discriminability of compliance of objects depends on whether the objecthas a deformable or rigid surface (srinivasan and lamotte, 1994). when thesurface is deformable, the spatial pressure distribution within the contact regionis dependent on object compliance, and hence information from cutaneousmechanoreceptors is sufficient for discrimination of subtle differences incompliance. when the surface is rigid, kinesthetic information is necessary fordiscrimination, and the discriminability is much poorer than that for objectswith deformable surfaces. for deformable objects with rigid surfaces held in apinch grasp, the jnd for compliance is about 515 percent when thedisplacement range is fixed, increases to 22 percent when it is roved (variedrandomly), and can be as high as 99 percent when cues arising out ofmechanical work done are eliminated (tan et al., 1992, 1993). using acontralaterallimb matching procedure involving the forearm, jones and hunter(1992) have found that the differential thresholds for stiffness and viscosity are23 and 34 percent, respectively. it has been found that a stiffness of at least 25n/mm is needed for an object to be perceived as rigid by human observers (tanet al., 1994).summaryin this section, we summarize the available quantitative research results onhuman haptics separately for tactile, kinesthetic, and motor systems, as well asresults when all the three systems are involved under active touch conditions.tactile sensory systemhumans can distinguish vibration sequences of up to 1 khz through thetactile sense. the human threshold for the detection of vibration of ahaptic interfaces170virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.single probe is about 28 db (relative to 1 µ peak) for 0.4 to 3 hz; it decreases atthe rate of 5 db/octave for 3 to 30 hz, and decreases further at the rate of 12db/octave for 30 to about 250 hz, after which the threshold increases for higherfrequencies. spatial resolution on the finger pad, as measured by thelocalization threshold of a point stimulus is about 0.15 mm, whereas the twopoint limen is about 1 mm. human detection thresholds for features on asmooth glass plate are a 2 µ high single dot and a 0.06 µ high grating.kinesthetic sensory systemhumans can detect joint rotations of a fraction of a degree performed overabout a second. the bandwidth of the kinesthetic system is estimated to be2030 hz. the jnd is about 2.5 deg for the finger joints, 2 deg for the wrist andelbow, and about 0.8 deg for the shoulder.motor systemhuman bandwidth for limb motions is found to be a function of the modeof operation: 12 hz for unexpected signals, 25 hz for periodic signals, up to 5hz for internally generated or learned trajectories, and about 10 hz for reflexactions. the differential thresholds for position and movement have beenmeasured to be about 8 percent.active touch involving all three systemsthe jnd for length is about 1 mm for a reference length of 10 mm, andincreases monotonically to 2.4 mm for a reference length of 80 mm. the jndfor contact force is 515 percent of the reference force value. the maximumcontrollable force that can be exerted by a finger pad is about 100 n and theresolution in visually tracking constant forces is about 0.04 n or 1 percent,whichever is higher. the jnd for compliance of deformable objects with rigidsurfaces can range from 5 to 99 percent depending on the cues available to thehuman subject. a stiffness of at least 25 n/mm is needed for an object to beperceived as rigid by human observers. the differential threshold for viscositysensed by activating the forearm is about 34 percent.status of the technologyterminology and variables of haptic interfacessince haptic interfaces are devices composed of mechanical components inphysical contact with the human body for exchange of informationhaptic interfaces171virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.with the human nervous system, it is natural to borrow the terms used inmechanics, human physiology, and robotics to describe the subsystems of theinterfaces. in performing tasks with a haptic interface, the human user conveysdesired motor actions by physically manipulating the interface, which, in turn,displays tactual sensory information to the user by appropriately stimulating hisor her tactile and kinesthetic sensory systems. thus, in general, haptic interfacescan be viewed as having two basic functions: (1) to measure the positions andcontact forces (and time derivatives) of the user's hand (or other body parts) and(2) to display contact forces and positions (or their spatial and temporaldistributions) to the user. among these position (or kinematic) and contact forcevariables, the choice of which ones are the motor action variables (i.e., inputs tothe computer) and which are the sensory display variables (i.e., inputs to thehuman) depends on the hardware and software design, as well as the tasks forwhich the interface is employed.although a forcereflecting haptic interface needs only to display forces,the sensing of forces by the interface (in addition to position sensing) is likely tobe needed for several reasons. first, the presence of noise in the system, as wellas the need to compensate for friction and inertia, requires closedloop forcecontrol and hence force sensing. second, the limitations on available vetechnology make it necessary to achieve reconfigurability through changes inhardware as well as software (see below). in other words, a generalpurpose vesystem might need to augment the exoskeleton with a variety of hardwaremanipulanda, some of which would include force sensing. third, in certainapplications, it may be desirable to create nonnatural environments. forexample, in certain cases it might be appropriate to use a fixedposition, forcesensing joystick together with a visual display of tactile information.alternatively, one might find it helpful to employ a positiondisplaying joystick,with or without force sensing, to present certain kinds of spatial information(e.g., for guiding a passive hand through a maze).classification of haptic interfacesa primary classification of haptic interactions with real environments orves that affects interface design can be summarized as follows: (1) free motion,in which no physical contact is made with objects in the environment; (2)contact involving unbalanced resultant forces, such as pressing an object with afinger pad; (3) contact involving selfequilibrating forces, such as squeezing anobject in a pinch grasp. depending on the tasks for which a haptic interface isdesigned, some or all of these elements will have to be adequately simulated bythe interface. for example, grasping and moving an object from one location toanother involves all threehaptic interfaces172virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.elements. the design constraints of a haptic interface are strongly dependent onwhich of these elements it needs to simulate. consequently, the interfaces canbe classified according to whether they are forcereflecting or not, as well as bywhat types of motions (e.g., how many degrees of freedom) and contact forcesthey are capable of simulating.an alternative but important distinction in our haptic interactions with realenvironments or ves is whether we touch, feel, and manipulate the objectsdirectly or with a tool. the complexity in the design of a haptic interface isseriously affected by which of these two types of interactions it is supposed tosimulate. note that an ideal interface, designed to provide realistic simulation ofdirect haptic exploration and manipulation of objects, would be able to simulatehandling with a tool as well. such an interface would measure the position andposture of the user's hand, display forces to the hand, and make use of a singlehardware configuration (e.g., an exoskeleton with force and tactile feedback)that could be adapted to different tasks by changes in software alone. forexample, the act of grasping a hammer would be simulated by monitoring theposition and posture of the hand and exerting the appropriate forces on thefingers and palm when the fingers and palm were in the appropriate position.however, the large number of degrees of freedom of the hand, extremesensitivities of cutaneous receptors, together with the presence of mass, friction,and limitations of sensors and actuators in the interface, make such an idealimpossible to achieve with current technology. in contrast, an interface in theform of a tool handle, for which reconfigurability within a limited task domainis achieved through both hardware and software changes, is quite feasible.thus, one of the basic distinctions among haptic interfaces is whether theyattempt to approximate the ideal exoskeleton or employ the toolhandleapproach.another set of important distinctions concerning haptic interfaces resultsfrom a consideration of the force display subsystems in an interface. broadlyspeaking, force display systems can be classified as either groundbased, suchas joysticks and other hand controllers, or bodybased, such as gloves andexoskeletons. frequently, the distinction between grounding sites is overlookedin the literature. for example, exploration or manipulation of a virtual objectrequires that force vectors be imposed on the user at multiple regions of contactwith the object. consequently, equal and opposite reaction forces are imposedon the interface. if these forces are selfequilibrating, as in simulating thecontact forces that occur when we squeeze an object, then the interface need notbe mechanically grounded. however, if the forces are unbalanced, as inpressing a virtual object with a single finger pad, the equilibrium of theinterface requires that it be attached somewhere. a forcereflecting joystickattached to the floor would be a groundbased display, whereas a forcereflecting exoskeletalhaptic interfaces173virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.device attached to the user's forearm would be a bodybased display. thegrounding choice affects whether the user experiences throughout his or herentire body the stresses induced by contact with a virtual object. theconsequences of using a bodybased display to simulate contact forces thatreally stem from groundbased sources are not known and warrantinvestigation. a further example of improperly grounded displays occurs withmost tactile stimulators. if a tactile stimulator array is attached to the finger padvia a strap surrounding the finger, then the net applied force by the stimulator isbalanced by a reaction force on the back of the finger. whether this reactionforce can be distributed with a low enough pressure distribution to beimperceptible, and whether the absence of stresses throughout the rest of themusculoskeletal system is inconsequential, are not known. although mostdevices built to date are either groundbased or bodybased, hybrid interfacesthat are a combination of the two (such as the dextrous teleoperation systemmaster built by sarcos, inc.) are also possible.current technologyhardwarehaptic interface hardware for synthetic environments (ses) is in the veryearly stages of development. many of the devices available today have beenmotivated by needs predating those of ve technology. simple position/motionmeasuring systems have long been employed to provide control inputs to thecomputer. these have taken many forms, such as those that involve contactwith the user without controlled force display (e.g., keyboards, computer mice,trackballs, joysticks, passive exoskeletal devices) and those that measureposition/motion without contact (e.g., optical and electromagnetic trackingdevices). applications motivating development of these devices have rangedfrom the control of equipment (e.g., instruments, vehicles) to biomechanicalstudy of human motion (e.g., gait analysis, time and motion studies). therequirements for position trackers and a variety of design approaches anddevices are described in chapter 6 on position tracking and mapping.the early developments in forcedisplaying haptic interfaces were drivenby the needs of the nuclear energy industry and others for remote manipulationof materials (sheridan, 1992). the forcereflecting teleoperator master arms inthese applications were designed to communicate to the operator informationabout physically real tasks. the recognition of the need for goodquality forcedisplays by early researchers (goertz, 1964; hill, 1979) continues to be relevantto today's ve applications. although sutherland's (1965) pioneering descriptionof ves includedhaptic interfaces174virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.forcereflecting interfaces, development of practical devices has proven to bedifficult. the current state of kinematics, actuators, sensors, and control ofmaster manipulators described in chapter 9 on telerobotics is directly relevantto haptic interfaces.a rough breakdown of major types of haptic interfaces that are currentlyavailable or being developed in laboratories and companies around the world isas follows:(1) groundbased devices joysticks/hand controllers(2) bodybased devices exoskeletal devices flexible (gloves and suits worn by user) rigid links (jointed linkages affixed to user)(3) tactile displays shape changers shape memory actuators pneumatic actuators microelectromechanical actuators vibrotactile electrotactilejoysticks are probably the oldest of these technologies and were originallyconceived to control aircraft. even the earliest of control sticks, connected bymechanical wires to the flight surfaces of the aircraft, unwittingly presentedforce information about loads on the flight surfaces to the pilot. in general, theymay be passive (not force reflecting), as in the joysticks used for cursorpositioning, or active (force reflecting), as in many of today's modern flightcontrol sticks. for example, measurement systems inc. has marketed several 2and 3dof positionsensing joysticks, some of which can sense but not displayforce. examples of forcereflecting 2dof joysticks designed for relativelyhigh bandwidth are the at&t minijoystick (schmult and jebens, 1993) andone built in the mit newman laboratory (adelstein and rosen, 1992).many of the forcereflecting hand controllers available today have beendeveloped for the control of remote manipulators (jacobus et al., 1992; meyeret al., 1992). generally, these devices employ at most 6 dof (plus grip control)and have a wide range of performance qualities. particularly good reviews ofperformance characteristics are found in brooks (1990) and mcaffee andfiorini (1991), and a broad overview of the devices is available in honeywell(1989). a great deal of work concerning ergonometrics (shape, switchplacement, motion and force characteristics, etc.) has gone into the design of thehand grip of these devices (brookshaptic interfaces175virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.and bejczy, 1985). one of the first applications of forcereflecting handcontrollers to ves was in project grope at the university of north carolina(brooks et al., 1990). the argonne mechanical arm (arm) was usedsuccessfully for force reflection during interactions with either simulations ofmolecule docking or with data from a scanning tunneling microscope. recently,highperformance devices have been specifically designed for interaction withves. the mit sandpaper is a 3dof joystick that is capable of displayingvirtual textures (minsky et al., 1990). in japan, desktop master manipulatorshave been built in tsukuba (iwata, 1990; noma and iwata, 1993). at theuniversity of british columbia, highperformance hand controllers have beendeveloped by taking advantage of magnetic levitation technology (salcudean etal., 1992). perforce is a 6dof hand controller that delivers highperformance (cybernet systems, 1992). the phantom, built in the mitartificial intelligence laboratory, is a multilink, lowinertia device that canconvey the feel of virtual objects (massie and salisbury, 1994).sophisticated teleoperation masters have been built that can be used to feeland manipulate virtual objects as well. at harvard, howe (1992) has developeda teleoperation system with a twofinger master that can be used to executeprecision tasks with a pinch grasp between the thumb and the index finger. oneof the most complex forcereflecting devices built to date is the dextrousteleoperation system master designed by sarcos, inc., in conjunction with theuniversity of utah's center for engineering design and the naval oceansystems center (nosc). although it is primarily groundbased, by havingattachment points at the forearm and upper arm of the user it has the advantagesof an exoskeleton, such as a large workspace comparable to that of the humanarm. this device utilizes highperformance hydraulic actuators to provide awide dynamic range of force exertion at relatively high bandwidth on a jointbyjoint basis for 7 dof. another highperformance forcereflecting master is agroundbased system built by hunter et al. (1990) to enable twohandedteleoperation of a microrobot that can meet the dual requirements of widebandwidth (exceeding 1 khz) and high accuracy (as low as a few nanometers).improved versions of these devices have been built for teleoperated eye surgeryand represent the stateoftheart performance that can be achieved usingcurrently available technology (hunter et al., 1994).exoskeletal devices are characterized by the fact that they are designed tofit over and move with the limbs or fingers of the user. because they arekinematically similar to the arm and hands that they monitor and stimulate, theyhave the advantage of the widest range of unrestricted user motion. as positionmeasuring systems, exoskeletal devices (gloves, suits, etc.) are relativelyinexpensive and comfortable tohaptic interfaces176virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.use. the wellknown vpl dataglove and datasuit use fiberoptic sensors toachieve a joint angle resolution of about a degree. the virtex cybergloveachieves a higher resolution of about half a degree by using strain gauges.exos and the utah/mit dextrous hand master consist of rigid linkexoskeletons and use hall effect sensors to obtain a resolution of about 0.2 to0.5 deg. rigid link exoskeletons that provide force reflection in addition to jointangle sensing have also been designed and built. shimoga (1992) provides anexcellent review of these devices and design issues, including both humanfactors and technology. the utah handwrist master (jacobsen et al., 1989), therutgers portable dextrous master (burdea et al., 1992), the jpl glovecontroller (jau, 1992), the tsukuba fingertip force display (iwata et al., 1992),and the exos safire fall into this category of device. however, providinghighquality force feedback with such devices that is commensurate with humanresolution is difficult and places great demands on actuator size minimizationand control bandwidth.while the display of net forces is appropriate for coarse object interaction,investigators have also recognized the need for more detailed displays withinthe regions of contact. in particular, the display of tactile information (e.g.,force distributions for conveying information on texture and slip), thoughtechnically difficult, has long been considered desirable for remotemanipulation (bliss and hill, 1971). tactile display systems in the last twodecades have been mostly used in conveying visual and auditory information todeaf and blind individuals (bachyrita, 1982; reed et al., 1982). displaysystems that attempt to convey information about contact use a variety oftechniques. shapechanging displays convey the local shape of contact bycontrolling the deformation or forces distributed on the skin. this has beenaccomplished by an array of stimulators actuated by dc solenoids (friskengibson et al., 1987), shape memory alloys (tini, 1990), and compressed air.the use of a continuous surface actuated by a electrorheological fluid has beenproposed by monkman (1992). vibrotactile displays deliver mechanical energythrough an array of vibrating pins placed against the skin. the opticon,marketed by telesensory systems, and the bagej corporation tactile stimulatorbelong to this class. the exos touch master consists of a single voice coilvibrator. a particularly promising desktop tactile array capable of highperformance as both a shape changer and a vibrator over 0 to several hundredhz is being developed at johns hopkins university (schneider, 1988).electrotactile displays stimulate the skin through surface electrodes. a reviewof principles and technical issues in vibrotactile and electrotactile displays canbe found in kaczmarek and bachyrita (1993). various types of tactile displaydevices mentioned above are reviewed by shimoga (1992).haptic interfaces177virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.softwarein general, haptic interfaces receive motor action commands from thehuman user and display appropriate tactual ''images" to the user. tactual imagesconsist of force and displacement fields to be imposed on the observer in orderto simulate the observer's desired mechanical interactions with objects in theve. in general, these images stimulate both tactile and kinesthetic informationchannels in the observer and are driven by the actions of the observer. majorcomponents of the information conveyed are the mode of contact with theobjects (e.g., indentation, slip), mechanical properties of the objects (e.g.,texture, shape, compliance), as well as the motions and forces involved inexploration and manipulation in a ve.since haptic interfaces for interacting with ves are in the early stages ofdevelopment, there is very little software that has been specifically designed forgenerating tactual images. commercially developed codes necessary for usingposition trackers of various manufacturers are available. however, for forcereflecting devices, as in the case of hardware, most of the software has beendeveloped in the context of teleoperation or controlling autonomous robots.several research laboratories have developed ve systems with visual and hapticdisplays achieved through appropriate integration of mechanistic models ofvirtual objects and control of haptic interfaces for rendering tactual images withsoftware used to drive the visual images. for example, the phantominterface developed in the mit artificial intelligence laboratory has been usedto tactually display the forces of contact of a stylus held in the user's hand witha variety of static and dynamic virtual objects in synchrony with visual imagesof the objects and their motion.similar to the software needed to generate visual images (chapter 8), thesoftware necessary to generate tactual images can be classified into three majorgroups: haptic interaction software, physical models of virtual objects andenvironments, and software for rendering tactual images. haptic interactionsoftware mainly consists of reading the state of the haptic interface device. forexample, the signal conditioning and noise reduction software necessary forreading position or force sensors would fall within this category. in the case ofexoskeletal devices used for tracking hand posture, a higherlevel softwarebased on the human kinematic model of the hand is needed as well forinterpreting the sensor signals as corresponding to a hand posture.physical models of virtual objects and environments receive user'scommands through the sensors in the haptic interface and generate force ordisplacement outputs corresponding to the physical behavior of a simulatedobject in the ve. as mentioned in the section on world modeling inhaptic interfaces178virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.chapter 8, this can either be accomplished by a unified model for all themodalities (visual, haptic, acoustic) or through separate models for eachmodality together with correlation algorithms for consistency among thedisplays corresponding to each of the modalities. however, the computationsneeded for the former approach (e.g., involving finite element methods) tend tobe extremely intensive and are difficult to complete in real time, even when oneuses supercomputers. simplifications in generating multimodal images arenecessary, not only because of the computational difficulties, but also becausethe display devices at present have limited capabilities. therefore, even thoughthe physics governing the visual, haptic, and acoustic behavior of an object isthe same, different approximations might be needed for each of the modalities.for example, visual images are scalar, twodimensional projections of theobjects, whereas tactual images are, in general, threedimensional vector fields.for realistic visual images, all the objects within the visual field need to bedisplayed and, typically, each object needs to appear as a continuous twodimensional projection. in the case of tactual images, often only the display offorces within isolated contact regions is sufficient. also, lumpedparametermodels that approximate a continuum through discrete elements may be goodenough to generate inputs to the haptic rendering devices. however, these forcefields are tightly coupled to the user's actions as well as the mechanicalproperties of the soft human tissues in contact with the interface device. themechanics of interaction between the observer and the environment plays afundamental role in the generation of tactual images. models of the humanoperator's behavior and performance developed in teleoperation literature areapplicable to ves as well (sheridan, 1992).the software for rendering the tactual images receives the output of thephysical model and generates the commands needed to drive the interfacedevice. in the case of the sandpaper, a 2dof joystick capable of force display(minsky et al., 1990), the authors report success in conveying the feel ofexploring rough surfaces by using a simple rule that contact forces to bedisplayed are proportional to the local gradient of the textured surface. evenwhen such simple algorithms generate the tactile images, if the user has inaddition visual or auditory inputs that are consistent, it is possible that theinteractions with ves will seem sufficiently realistic to him or her. therefore,the algorithms for the generation of tactual images depend strongly on theparticular application as well as the capabilities of the display device, includingthe available computational speed. because force displays are prone tomechanical instabilities and human users are sensitive to even low disturbancesunrelated to the task, realtime control of the interface devices needs to be ofhigh quality. in the robotics and teleoperation literature (chapter 9),considerable efforthaptic interfaces179virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.has been directed at implementing conventional proportionalintegralderivative(pid) controllers for contact tasks. impedance control techniques (reviewed bybrooks, 1990) and the use of the passivity principle have been reported to besuccessful in combatting instabilities. substantial theoretical research iscurrently being pursued in the areas of multivariable control and advancednonlinear techniques, such as adaptive and robust control.summary of current technology and future possibilitiescomputer keyboards, mice, and trackballs are the simplest hapticinterfaces and are being widely used to interact with computers. positionsensing gloves and exoskeletons without force reflection are also available onthe market but are used mainly for research purposes. among the forcereflecting devices, groundbased devices such as joysticks are being used, andmodified versions of such devices for different tool handles are feasible in thenear future. forcereflecting exoskeletons are harder to design for adequateperformance, and only a few such have been built for research purposes. tactiledisplays offer particularly difficult design challenges because of the highdensity of receptors in the skin to which they must apply the stimulus. thereexist a number of examples of tactile stimulators for the finger, includingpneumatic shape changers, electrocutaneous stimulators, and vibrating arrays,but none provides convincing tactile images and all are awkward to use(durlach et al., 1992).the emerging field of microelectromechanical systems (mems) holdspromise for providing very fine arrays of tactile stimulators. arrays of surfacenormal, electrostatic actuators currently being developed for sensors could beadapted for use in highresolution tactile displays (trimmer et al., 1987).although capable of relatively small forces and deflections, arrays of suchactuators integrated with addressing electronics would be inexpensive,lightweight, and compact enough to be worn without significantly impedinghand movement or function. in addition, the current technology makes feasiblea 20 × 20 array of individually controlled stimulators on a 1 cm × 1 cm chip.finally, recent work on thinfilm, shapememory alloys would enhance theattractiveness of shapechanging displays by increasing stimulator densities andactuation bandwidths. it should be noted that with synchronized multimodalstimulation, such as for simulating the contact between a tool and a rigid object,more realism can probably be achieved by providing an audible "ping" togetherwith low bandwidth force feedback, than by improving the force bandwidth tothe maximum value that is possible with current technology. because of thedifficulties in developing good cutaneous stimulator devices, initialhaptic interfaces180virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.efforts on haptic displays should probably focus on devices that apply net forceson the hand or fingertips (the toolhandle approach discussed above). even withthis simplification, large improvements on existing devices can be achievedonly by a proper match between the performance of the device and humanhaptic abilities.due to inherent hardware limitations, haptic interfaces can deliver onlystimuli that approximate our interactions with the real environment. it does not,however, follow that synthesized haptic experiences created through the hapticinterfaces necessarily feel unreal to the user. consider an analogy with thesynthesized visual experiences obtained while watching television or playing avideo game. whereas visual stimuli in the real world are continuous in spaceand time, these visual interfaces project images at the rate of about 30 frames/s.yet we experience a sense of realism and even a sense of telepresence becausewe are able to exploit the limitations of the human visual apparatus. the hopethat the necessary approximations in generating synthesized haptic experienceswill be adequate for a particular task is based on the fact that the human hapticsystem has limitations that can be similarly exploited. to determine the natureof these approximations or, in other words, to find out what we can get awaywith in creating synthetic haptic experiences, quantitative human studies areessential. basic understanding of the biomechanical, sensorimotor, andcognitive abilities of the human haptic system is critical for proper designspecification of the hardware and software of haptic interfaces. in addition, allmechanical devices will have their own intrinsic properties (such as friction,mass, compliance, viscosity, time delay) that will necessarily be interposedbetween the user and the desired stimulation. this lack of perfect transparencywill always be present to some degree and will thus make all stimulators lessthan ideal. given the approximate nature of synthetic haptic stimulation, it isclear that there is a need to assess which types of stimulation provide the mostuseful and profound haptic cues for the task at hand.research needscompared with the visual and auditory domains, the capabilities of hapticdevices and our understanding of human haptics are quite limited. acomprehensive program to develop a variety of haptic interfaces for ves andteleoperation needs to include research in three major areas: (1) human haptics,(2) technology development, and (3) matching the performance of humans andhaptic devices. it does not mean, however, that such research has to precede anyusage of haptic devices. for applications that are simple from a hapticstandpoint, such as those requiring relatively lowresolution hand positioninformation, joysticks and gloveshaptic interfaces181virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.currently available off the shelf can be sufficient. more complex applicationsinvolving force and tactile displays might need research in some or all of theareas mentioned above. since progress in the three areas is interdependent, thedesirable course of development for a challenging application is to continuallybuild improved versions of haptic devices based on experimental data obtainedfrom the previous versions on the performance of humans, and devices and theinteraction between the two. due to the availability of powerful computers andhighprecision mechanical sensors and actuators, it is now possible to exertcontrol over experimental variables as never before.human hapticsas mentioned above, the biomechanical, sensorimotor, and cognitiveabilities of humans set the design specifications for devices. therefore,multidisciplinary studies involving biomechanical and psychophysicalexperiments together with computational models for both are needed in order tohave a solid scientific basis for device design. perhaps to a lesser extent,neurophysiological studies concerning peripheral and central neuralrepresentations and the processing of information in the human haptic systemwill also aid in design decisions concerning the kinds of information that needto be generated and how these should be displayed. a major barrier to progressfrom the perspectives of biomechanics, psychophysics, and neuroscience hasbeen the lack of robotic stimulators capable of delivering a large variety ofstimuli under sufficiently precise motion and force control.biomechanical investigationsthe tight mechanical coupling between the human skin and hapticinterfaces strongly influences the effectiveness of the interface. therefore, thespecifications for the design of sensors and actuators in the interface, as well asthe control algorithms that drive the interface, require the determination ofsurface and bulk properties of, say, the finger pad. the measurement of forcedistributions within the contact regions with real objects is needed to determinehow a display should be driven to simulate such contacts in ves. in addition,computational models of the mechanical behavior of soft tissues will aid insimulating the dynamics of task performance for testing control algorithms, aswell as in determining the required taskspecific force distributions for thedisplays. this requires measurement of the in vivo skin and subcutaneous softtissue response to timevarying normal and tangential loads. information onsuch human factors as the size, shape, degrees of freedom, and ranges of motionof the fingers, hand, and arm are generally available in handbooks.haptic interfaces182virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.psychophysical investigationsdetermination of the basic sensorimotor and cognitive abilities of thehuman haptic system needed for developing haptic interfaces can be subdividedas follows:(1) sensing and control of contact forces and joint angles or endpoint displacements: even simple questions concerning our abilities (such aswhat is the resolution, range, and bandwidth in the sensing and controlof interface variables) or mechanisms (such as how we perceive jointangles or contact forces) do not yet have unequivocal answers.(2) perception of contact conditions and object properties: the importantconnection between the loads imposed on the skin surface within theregions of contact with objects and the corresponding perception hasonly begun to be addressed. psychophysical experiments directed atdetermining the primary cues that signal various object properties needto be undertaken.(3) integration of local contact information with nonlocal perception ofthe environment: tactual perception typically provides localinformation about an object. to be effective in training tasks, such ascockpit familiarization, that information must be integrated intononlocal perception of the space within which the hand and arm move.however, haptic perception of mechanical quantities has been found tobe significantly distorted (fasse, 1992; hogan et al., 1990). therelationship between these haptic distortions and human internalperceptual models of space and the objects in it is unknown. theinfluence of these distorted perceptions on production of motorbehavior has barely been addressed. the theoretical framework togenerate testable hypotheses must be built on a fundamentalunderstanding of the relations between haptic perception of geometricand mechanical quantities, such as magnitudes and orientations oflengths, forces, and stiffnesses. experimentally verified models of therelationship between haptic perceptions and motor actions are criticalfor the design of effective synthetic haptic environments. similarstudies need to be performed under multimodal conditions as well.(4) performance in the presence of inherent time delays, distortions, andnoise: these experiments are needed for all modalities individuallyand in combination. studies directed at sensorimotor and cognitiveadaptation and training effects are needed.(5) theoretical developments concerning information flow: theoreticaldevelopments concerning the taskspecific flow of sensory informationand control of motor action are needed to generate testable hypotheseson our haptic interactions with both real environments and ves.development of improved models of human operator behavior andperformance (available in the teleoperation literature) through tests inrealistic tasks would be beneficial in both the design and operation ofse systems.haptic interfaces183virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.technology developmenthardwarefour areas of hardware development are of interest: (1) finger, hand andarm position/joint angle measurement (trackers); (2) displays of forces andtorques; (3) tactile displays; and (4) other stimulus distributions applied as twodimensional fields to the skin, such as thermal stimuli.the major problems with the position/anglemeasuring devices are theintrusion the user feels while wearing, say, an exoskeleton, and the everpresentneed for improvements in ranges, resolutions, and bandwidths. in order todisplay forces, designs with good actuation and control need to be developedsuch that they have sufficient force range, resolution, smoothness, andbandwidth. attention needs to be paid to the friction, backlash, mechanicalstiffness, apparent mass, inertia, and natural frequencies of the devices. highposition resolution is needed to minimize the effect of quantization errors onstability of contact interaction. force feedback systems need to have vibrationrigorously controlled to prevent false cues to the human user. in order toachieve such high performance without mechanical instabilities, robust andadaptive closedloop control of the devices is necessary. the mechanics of thedevices must be intrinsically correct so that the difficult problems ofcompensating for the mass and inertia of the control arm are avoided orminimized.although many of the design specifications for haptic interfaces are taskdependent, we can estimate some of the interface performance requirementsbased on human haptic abilities. for example, since the human finger jointangle jnd is of the order of a deg, the fingertip position resolution is about 1mm. for the haptic interface to perform well, its fingertip position displayresolution should probably be about 0.1 mm, and the bandwidth should be about30 hz to match the estimated human kinesthetic bandwidth. the maximumstiffness of the actuators should be in excess of 25 n/mm to have realisticsimulation of contact with rigid stationary objects. to fully match human hapticsensory capabilities, the tactile or force displays should have a bandwidth ofabout 1 khz, whereas the signals representing the human motor action need tohave a bandwidth of only 10 hz. in order to prevent false cues to the user,vibrations that are not part of the intended display should have amplitudes lessthan human detection threshold, which is about 25 m at 0.4 to 3 hz, 3 m at 30hz, 0.3 m at 250 hz, and is higher for higher frequencies. for tactile displays,the spatial density of actuating elements should be at least 1 mm/taxel to matchthe human tactile resolution. to realistically simulate continuous surfaces ofvirtual objects, the actuating arrays need to behaptic interfaces184virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.even more densely packed, or should have a continuous surface over them,because of the high sensitivity of the tactile sensory system to point loads andsharp edges. it should be noted that when visual and/or auditory senses are alsostimulated, haptic interfaces with lower performance capabilities than the aboveestimates may be adequate.exploration of novel technologies is needed for quantum improvements inrotary and linear actuators. use of shape memory alloys (smas) andmicroelectromechanical systems (mems) for tactile displays also needs to beinvestigated further. it has been estimated that realtime mechanical interactionswith typical finite element models need computational speeds on the order ofgflops (hunter et al., 1990). similar to graphics engines used commonly withvisual displays, special computational hardware specifically designed toaccelerate the computations needed for haptic displays will become necessary inthe near future.softwaremodeling of the haptic environment and control of realtime interactionstogether with synchronous operation of other sensory modalities is a major needin software development that requires substantial research. what needs to bemodeled and how to interact and display is task dependent. tradeoffs inprecision and computational speed are critical. standard methods for easilyimplementing physical models that range from high fidelity to coarseapproximations need to be developed. in addition, models of the humanoperator, the environment, and interaction dynamics available in teleoperationliterature need to be adapted and improved for ve applications.simulation of multibody environments will be possible only if we addresscomputational efficiency and appropriate architectures for modeling andmaintaining a mechanical world. it is likely that this problem is much harderthan simple graphic simulations. some parallels exist, like texture, collisiondetection, and simulation of object dynamics, but to feel right a world model forhaptic display must possibly run substantially faster, at least at the points ofcontact between the user and the synthetic environment. realtime controlalgorithms are available to render the calculated outputs of the models to thehuman user through tactual displays. however, in order for the displays to berobust and feel right, the control bandwidths need to achieve frequencies of theorder of several khz. efficient methods of implementing the control softwareneed to be developed, including the use of special hardware, such as transputersconnected in parallel. also, theoretical advances in multivariable control andadvanced nonlinear techniques, such as adaptive and robust control, are needed.haptic interfaces185virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.matching performance of humans and haptic devicescomfort (ergonomics)making the human user comfortable when wearing or interacting withhaptic interfaces is of paramount importance, since pain, or even discomfort,supersedes all other sensations. appropriate attachment methods for groundbased and bodybased haptic interfaces need to be developed. design principlesof achieving kinematics and the dynamics of devices that impose minimalconstraints or bias on operator's hand/arm operation need to be explored.methods of stimulationthe right balance of complexity and performance in system capabilities isgenerally task dependent. in particular, the fidelity with which the tactualimages have to be displayed and the motor actions have to be sensed by theinterface depends on the task, stimulation of other sensory modalities, andinteraction between the modalities. experimenting with the available hapticinterfaces, in conjunction with visual and auditory interfaces, is necessary toidentify the needed design improvements. design compromises and tricks forachieving the required task performance capabilities or telepresence(immersion) need to be investigated. one of the tricks might be the use ofillusions (such as visual dominance) to fool the human user into believing a lessthan perfect multimodal display. techniques such as filtering the user's normaltremor or the use of sensory substitution within a modality (e.g., the use oftactile display to convey kinesthetic information) or among different modalities(e.g., visual display of a force) need to be developed to overcome the limitationsof the devices and the limitations of the human user, perhaps to achievesupernormal performance. to tackle the everpresent time delays, efficient andreliable techniques for running modelbased and realtime controls concurrentlyare needed.evaluation of haptic interfacesevaluation of haptic interfaces is crucial to judge their effectiveness and toisolate aspects that need improvement. however, such evaluations performed inthe context of teleoperation have been so taskspecific that it has beenimpossible to derive useful generalizations and to form effective theoreticalmodels based on these generalizations. there is a strong need to specify a set ofelementary manual tasks (basis tasks) that can be used to evaluate and comparethe manual capabilities of a givenhaptic interfaces186virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.system (human, robotic, ve) efficiently. ideally, this set of basis tasks shouldbe such that (1) knowledge of performance on these tasks enables one to predictperformance on all tasks of interest and (2) it is the minimal set of tasks (interms of time consumed to measure performance on all tasks in the set) that hasthis predictive power.two basic psychophysical questions in evaluation are: (1) with a given setup, how good is the task performance or realism of the subjective experience?(2) how does a change in the setup improve the performance of a given task,realism of the experience, or both? an example of the former is theinvestigation of the consequences of using an ungrounded display to simulatecontact forces that really stem from grounded sources. in the latter question, theword change is to be interpreted in a broad sense and includes modifications ofthe interface hardware, object models, interaction software, and addition/subtraction of visual or auditory modalities. theoretical and experimentalapproaches to quantify information transfer rates to and from the user undervarious single and multimodal conditions need to be developed.haptic interfaces187virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.5position tracking and mappingposition tracking and mapping of the human body and environments is abasic requirement that permeates virtual environment (ve) systems: (1) headand eye tracking for visual displays; (2) hand and arm tracking for hapticinterfaces; (3) body tracking for locomotion and visual displays; (4) bodysurface mapping for facial expression recognizers, virtual clothiers, and medicaltelerobots; and (5) environment mapping to build a digitized geometrical modelfor simulation.this review is approached from a standpoint of basic sensory systems forposition tracking and mapping: mechanical linkages, magnetic sensors, opticalsensors, acoustic sensors, and inertial sensors. these systems vary in suchaspects as accuracy, resolution, sampling rate, latency, range, workspace, cost,encumbrance, convenience, susceptibility to obscuration, ease of calibration, thenumber of simultaneous measurements, and orientation versus positiontracking. ve systems are likely to include a mix of these basic sensory systems,because each system has particular strengths and weaknesses and therequirements on position tracking depend on the particular application. we nowattempt to state some requirements on position tracking for particularapplications.hand trackingfor normal arm movements during reaching, a fast motion is accomplishedin about 0.5 ms, and wrist tangential velocities are about 3 m/s (atkeson andhollerbach, 1985) and accelerations are about 56 g. for theposition tracking and mapping188virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.fastest arm motions such as throwing a baseball, good pitchers release the ballat 37 m/s and accelerate their hands at more than 25 g. motion bandwidths ofnormal arm movements are around 2 hz (neilson, 1972); the fastest handmotions including handwriting are at around 56 hz (brooks, 1990a, 1990b). inteleoperation, it is commonly presumed that 10 hz is the maximum frequencyof position commands from the human operator (fischer et al., 1990).if hand motion is being used to drive a telerobot or a dynamic simulationof an arm, then the general rule of thumb is that the sampling rate should bearound 20 times the bandwidth (franklin et al., 1990) in consideration of suchfactors as sensor noise. taking 5 hz as defining the frequency content ofnormal arm motion, then a sampling rate of roughly 100 hz is called for.andersson (1993) has proposed a virtual batting cage, in which a batter swingsat virtual pitches shown through a headmounted display (hmd). the bat mustbe tracked to simulate the hit (or miss). andersson proposes that sampling ratesof about 1 khz are required to track such motions.latency requirements are determined by the psychophysical requirementsof the application and are harder to define. for force feedback applications, thehandtracking latencies must be very low, because the human arm is part of thecontrol loop. for nonforcefeedback applications, the handmotiontovisualfeedback lag can probably be much longer.eye trackingeye movements can be as fast as 600 deg/s. the smallest time constant forsaccades is around 50 ms; the smallest saccades can be finished in 60 ms. thepower spectral densities can have significant power up to 50 hz for position and74 hz for velocity (bahill et al., 1981). given again the engineering rule ofthumb that the sampling rate should be 20 times the bandwidth for noisymeasurements, it has been recommended that eye movements be sampled at 1khz (inchingolo and spanio, 1985). this should allow sufficiently precisetracking of the eye trajectory to characterize the movement time and endpoint.as mentioned in durlach et al. (1992), the eye sees continuous images whendisplay temporal frequency is 60 hz and above. with 1 khz sampling rates foreye movement, display targets can be well chosen every 1/60th of a second.head trackinghead movements can be as fast as 1000 deg/s in yaw, although more usualpeak velocities are about 600 deg/s for yaw and 300 deg/s for pitch and roll(foxlin, 1993). the frequency content of volitional head motionposition tracking and mapping189virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.falls off approximately as 1/f2, with most of the energy contained below 8 hzand nothing detectable above about 15 hz. trackertohost reporting rates musttherefore be at least 30 hz. delays of 60 ms or more between head motion andvisual feedback are known to impair adaptation and the illusion of presence(held and durlach, 1987), and much smaller delays may cause simulatorsickness. headtrackers should therefore contribute as little as possible tosystem latency, no more than 10 ms for highperformance systems. accuracyrequirements are very application dependent. to maintain apparently perfectregistration between the real and virtual worlds with infiniteresolution seethrough hmds, the absolute accuracy must be 0.01 deg for yaw and pitch androll and about 0.03 mm for translation. for purely virtual opaque hmdapplications, large offset errors are tolerable; tilt error must not exceed about 15deg, because that would cause vestibular conflict. however, the resolutionneeds to be 0.03 deg for orientation and 0.1 mm for translation to achieveperfectly smooth, jitterfree motion.body mapping and trackingfor the bodypart tracking applications above, it is sufficient to track a fewpoints or landmarks on limb segments. if the position tracker does not directlyyield orientation, then multipleposition measurements on a limb segment canbe employed to derive orientation. body motion can be considered to havesimilar measurement requirements to hand motion tracking.by contrast, for body surface mapping or realenvironment sensing, theposition tracker must be able to scan a volume or surface to yield a dense arrayof points. the realtime requirements are typically absent when environmentalreconstruction is the goal, although the image must be captured sufficientlyrapidly if the environment (e.g., a body surface) can move. accuracyrequirements depend on the application; for example, in medical imagingsystems, accuracies of 1 mm or better are desirable; for environmental mapping,accuracies of a few mm might be acceptable.additional information may be found in durlach et al. (1992) and meyer etal. (1992). various implementations of the basic sensory system types in thecontext of head tracking are well covered in meyer et al. (1992), and additionalinformation on eye trackers appears in durlach et al. (1992).mechanical trackersmechanical trackers can be an inexpensive, relatively accurate means oftracking head or bodysegment positions. mechanical trackers canposition tracking and mapping190virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.measure up to full body motion and do not have intrinsic latencies. forcereflection is readily incorporated by mounting actuators at the linkage joints.we distinguish two types, depending on whether the mechanical linkages areentirely worn (bodybased) or are partly attached to the ground (groundbased).goniometersbodybased linkagesšexoskeletons or goniometersšhave beenfrequently used in biomechanics for joint angle measurement. for hapticinterfaces, they form the basis for master gloves and forcereflectingexoskeletons. because they are worn, goniometers are portable and facilitatemobility; however, if there is body motion, some other tracking method is alsorequired. they typically have the same workspace as the natural motion of theattached limbs and hence permit the full range of normal motion to be measured.we may distinguish two ways to use the goniometric data: to infer jointangles and to infer endpoint positions. the latter could be employed, forexample, to track hand position relative to the body or fingertip position relativeto the hand. then the goniometer may be viewed like a hand controller that ismanipulated by the operator; the output is based on calculations from thegoniometers angles, and there is no concern as to how this maps to limb jointangles.for inferring joint angles, there are significant difficulties. attachment isproblematic, as it is for other tracking technologies, because of the soft tissueand potential for relative motion between the goniometer and limb. how best toclamp perturbation devices to limbs has been a concern for the biomechanicscommunity; for the arm, tight clamping to the wrist is possible because of itsbony features (jones et al., 1991; xu et al., 1991).it is difficult to align goniometers with joints, especially for multipledegreeoffreedom (dof) joints such as the shoulder. since goniometers areexoskeletons outside the limb, the centers of rotation of the goniometer differfrom the joint rotation centers. due to this kinematic mismatch, there has to beslippage between the goniometer attachments and the limb during motion. onesolution in the context of hand masters is to employ fourbar linkages to projectthe measured centers of rotation to the finger joints (rohling et al., 1993);because distances between joint centers are lost with this method, a fingercalibration scheme for each individual is required (rohling and hollerbach,1993).accentuating this problem is that human joints are not perfect hinge jointsor spherical joints: the axes of rotation move with the joint angles. this problemis shared by any tracking method that seeks to determine joint angles.according to rohling and hollerbach (1993), a master gloveposition tracking and mapping191virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.in conjunction with an optotrak 3d motion tracking system (northern digital,waterloo, ontario) was employed to calibrate the human hand geometry forfingertip control during teleoperation. it was found that the resulting fingertipaccuracy of a few millimeters was primarily limited by assumptions of idealjoint structures (e.g., hinge joints) and by the accuracy of joint anglemeasurements by the master glove due to relative movement.more information on goniometers can be found in chapter 4 on hapticinterfaces.groundbased linkagesgroundbased linkages are primarily used for 6dof endpoint trackingsuch as the position and orientation of the head or of the hand. hence the issuessurrounding joint angle measurement do not typically arise. it is assumed thatthe human operator can grasp the manipulandum rigidly, or that the headmounted system is rigidly attached to the head; both of these assumptions maybe problematic. although the head is bony, it is nevertheless difficult to designhelmets for visual displays that are fitted for each individual and that do notexperience relative movement during fast head motion. groundbased linkagesare easier to actuate for force reflection than are bodybased linkages, becauseactuators do not have to be placed and carried on the body.one drawback of groundbased linkages is that the operator is tied to theground and hence the workspace is limited. even if one is willing to increaseinertia and lower the mechanical resonance frequency in order to have longerarms, the range of a twosegment arm is ultimately limited to about 2 m.nevertheless, they are a good option when operators are seated or not movingvery much. another drawback is the restricted numbers of degrees of freedomthat can be measured, usually six. this is a problem for simultaneouslymeasuring multiple limbs or for measuring redundant linkages. for example,multiplefinger motion is not conveniently measured with a groundbasedlinkage; a goniometer is better. moreover, the human arm is a redundant 7dofmechanism, and a groundbased linkage cannot by itself resolve the redundancy.for hand tracking, there are many examples of hand controllers, joysticks,and other haptic interfaces mentioned. for head tracking, a commercial exampleis the adl1 six degree tracking system by shooting star technology. arelated example is the boom viewer from fake space labs, in which thevisual display is not worn but supported on a pedestal through the boomlinkage. based on experiences in robotics, accuracies of 0.1 mm and highsampling rates should be achievable with such systems, when properly androutinely calibrated (mooring et al., 1991).position tracking and mapping192virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.for linkages attached to a helmet, one possibility is to usecounterbalancing to reduce the gravity load of a headmounted display on thewearer (maeda and tachi, 1992). one potential drawback is the inertia of thecounterbalancing arms, which might impede head movement. one possiblesolution (not yet implemented as far as we know) is to use an active linkageservocontrolled to remove all dynamic loads on the head; this solution wouldobviously be complicated and expensive.an inexpensive method for position measurement is to use three wirepotentiometers. this method has been employed in robot calibration (fohanno,1982; payannet et al., 1985), in which submillimeter accuracy has beenreported. a cable connected between a torque motor and the human wrist hasbeen employed for movement perturbations (soechting and lacquaniti, 1988).forcereflecting hand controllers have been built using multiple cables attachedto motors with pulleys at the edge of a frame (agronin, 1987; atkinson et al.,1977; kawamura and ito, 1993; sato, 1991); however, the handle in the middleof this frame has a restricted workspace. similarly, actuated strings have beenemployed by iwata (1991, 1992) to apply force to the feet for walkthroughsimulation.magnetic trackersthe most popular position trackers are magnetic because of low cost,modest but reasonable accuracy, and convenience of use. magnetic trackers donot suffer obscuration problems, although they are sensitive to environmentalmagnetic fields and ferromagnetic materials in the workspace. multiple trackerscan be employed to map wholebody motion and to increase the range oftracked motion to a small room (badler et al., 1993).the commercial trackers from polhemus and ascension technology arecurrently the most frequently used. the polhemus fastrak is a recentintroduction and improvement over their original sensor: the commercialbrochure states static accuracies of 0.03 in and 0.15 deg and an update rate of120 hz. the update rate decreases with the number of sensors, which need to bemultiplexed. although latencies are stated as 4 ms, an independent source hasdetermined a 2030 ms latency range. the useful range is 1 m. the ascensionbird sensor has a stated accuracy of 0.1 in and 0.5 deg, and an update rate of120 hz; the latencies are 30 ms.in general, the advertised performance specifications of commercialmagnetic sensors should be treated cautiously, as they do not meet their specs inrealistic situations. the accuracies for both sensors depend on how close thetransmitter and receiver are to each other. in the case of the fastrak, whichemploys ac magnetic fields, neighboring metal surfaces will degrade theaccuracy because of induced eddy currents; there is no a priori way to gauge theeffect other than individual environmental testing.position tracking and mapping193virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.the ascension bird sensor employs dc magnetic fields and hence is lesssensitive to surrounding metal, although a reluctance effect might alter themagnetic path. in hollerbach et al. (1993), the bird sensor was compared withthe optotrak in the context of robot calibration; the relative accuracy was foundto be roughly proportional to price.optical sensingthere are a variety of approaches to optical sensing for position trackingand mapping. distance may be measured by triangulation (e.g., stereo vision),by time of flight (laser radar or ladar), or by interferometry. the passive light ofthe environment may be employed (stereo vision systems), structured light maybe projected (laser scanning), light may be pulsed (ladar), or active markers(infrared light emitting diodes or ireds) or passive markers may be placed on amoving body. cameras or detectors may employ linear or planar chargecoupled device (ccd) arrays, positionsensing detectors (psds), orphotodiodes. below we discuss some major approaches to position tracking andmapping according to the following categories: passive stereo vision systems,marker systems, structured light systems, laser radar systems, and laserinterferometric systems. a more complete review of active range imagingsensors may be found in besl (1988).passive stereo vision systemssubstantial effort is being expended by the computer vision community ondeveloping artificial humanlike vision capabilities. passive stereo visionsystems employ ambient light and squarearray ccd cameras, which have atypical resolution of roughly 600 × 400 pixels. a key issue is to solve thecorrespondence problem: relating the same points in two different images.although the vision community is far from solving the general stereo visionproblem, substantial progress is being made. advances in algorithms andhardware are resulting in realtime (30 frames/s) threedimensional imaging atmoderate resolutions (inoue, 1993; kanade, 1993). passive stereo visionsystems are unlikely to be useful in ve in the near term, as robustness andaccuracy are not yet comparable to active ranging systems. in the long term, asthe computer vision community continues to advance, the use of passive visionfor mapping and tracking is likely to become quite prevalent in ve.marker systemsthe stereo correspondence problem is solved in marker systems because afew, easily identifiable fiducial points are tracked on a movingposition tracking and mapping194virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.body. the simplest and most accurate approach is to use a number of ireds,which create very bright spots in the image. the ireds may be pulsed insequence with camera detection to uniquely identify each marker. for detection,one frequently employed approach is psds (also called lateral effectphotodiodes), which are available as roughly 1 cm2 squares. incident lightinduces a current that is measured at each edge of the square to yield the xylocation of the centroid of the incident light.commercial examples include the selspot ii system developed by selcomand the watsmart system developed by northern digital. ireds aremultiplexed at high rates (3,000 hz) and sensed by two cameras, whichtriangulate the markers to an accuracy of about 5 mm at a distance of 2.5 m.the cameras are mounted on tripods; a calibration cube mounted with preciselyknown ireds is employed to calibrate camera poses for triangulationcalculation. multiple markers can be tracked to yield orientation and to followmultiple bodies simultaneously. workspace is distancedependent: at 2.5 m, it isabout 1 m3. researchers at the university of north carolina have demonstrateda highperformance tracker using four ccd cameras mounted on a helmetpointing up toward a custom ceiling grid with ired markers mounted at knownpositions in the tiles (ward et al., 1992). the insideout arrangement allows formuch better accuracy in orientation and a large, scalable work area.a fundamental problem with the use of psds is reflections of ired lightfrom environmental surfaces that move the apparent centroid of the sensedlight; the amount of reflected light is high, about 25 percent of the total. theresult is that it is very difficult to get camera resolutions beyond 1 part in 4,000;for this reason, northern digital has abandoned the watsmart in favor of theoptotrak, which employs multiple cameras with 2,048element linear ccdarrays. an ired beam is transformed by a cylindrical lens into a line of lightprojected onto a linear ccd array. because of the gaussian spread of lightintensity, an area of the ccd array is illuminated, which allows subpixellocalization by area fitting. the result is a camera resolution of about 1 part in200,000. reflections are removed by image processing and thresholding: ifthere is another peak, it is detected and simply removed. this is the substantialadvantage over the use of psds. at a viewing distance of about 2.5 m, theaccuracy is about 0.1 mm and the resolution is about 0.01 mm. according tomarc rioux (national research council of canada, ottawa) the limiting factoris air turbulence, whose effect is about 0.01 mm at the 2.5 m distance. theoptotrak comes in two forms. the series 2000 employs two housings, each withtwo cameras, that can be mounted on tripods; larger viewing distances arepossible, at the cost of the use of a calibration cube. the series 3000 is a singlehousing, with three cameras embedded in an aluminum block; the cameraensemble is calibrated at the factory andposition tracking and mapping195virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.hence there is no need for field calibration as for the series 2000 (at the cost of asmaller workspace). the optotrak is capable of tracking full poses of twomoving bodies at 100 hz through multiple marker placement.there are several commercial stereo vision systems that employ passiverather than active markers. accuracies and sampling rates are not as good asthose for the commercial active marker systems, although the absence of wireson the body is an attraction. position and orientation tracking is also possiblewith a single squarearray camera instead of two; meyer et al. (1992) refer tothis method as pattern recognition. when several points on a plane withprecisely known relative locations can be recognized, then reasonably accurateestimates can be obtained. this alternative is especially attractive for headtracking because only one camera is used.structured light systemsanother approach to solving the correspondence problem in stereo visionis to employ structured light, usually a precisely known ray or plane of light. inone common configuration, a plane of light, created by passing a laser beamthrough a cylindrical lens, is swept across a scene using a galvanometerdrivenmirror. at each position of the plane, a light stripe is created, which is sensed bya twodimensional camera. the intersection of the known plane and the line ofsight from the camera determines the threedimensional coordinates. to reducethe cost of such systems and improve the frame rate, kanade (1993) hasdeveloped a cellparallel light stripe range sensor, based on custom very largescale integrated (vlsi) design. the camera employs ''smart" photosensitivecells, which can detect the time at which peak incident light falls. resolution iscurrently 32 × 32 pixels, acquisition time is 1 ms, and accuracies are about 0.1percent of the range.another common configuration is laser spot scanning, using a variety ofdifferent movable mirror arrangements. a common misconception is that thebaseline in a laser scanning system (distance between light source and detector)must be large for accurate ranging. instead, rioux (1984) developed asynchronized scanner in which the horizontal position detector and beamprojector are located nearly collinearly and oppositely. a first scanned mirrorbetween the two directs light on one of its surfaces from the source to the scenevia a fixed mirror to a second scanned mirror. reflected light is redirected byanother fixed mirror to the opposite surface of the first scanned mirror to thedetector. this reduces the shadow effect over other laser spot scanners andyields a more compact system.one version of the synchronized scanner is a random access laser scanner,in which the first scanned mirror is a simple twosided mirror,position tracking and mapping196virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.and the laser spot can be arbitrarily directed to any point in a scene (beraldin etal., 1993). the advantage of random access is the ability to scan just a portionof the scene, for example to track an object using a lissajou scan pattern, or toscan a full scene coarsely and then scan a selected scene portion more finely.the working range is from 0.5 to 100 m, the field of view is 40 × 50 deg. in adigitization mode, the sampling rate is 20 khz (sample points per second) withccds and 10 mhz with psds. in a tracking mode, single objects can betracked at 130 hz; this rate is divided by the number of objects to be tracked.when the range is less than 10 m, the accuracy is typically 0.1 to 0.2 mm. atthe far range of 100 m, retroreflectors must be used. reflectance can also beinferred from light intensity. a ve application of this scanner involveddigitizing for simulation the cargo bay and experimental setups of the spaceshuttle orbiter (maclean et al., 1990). this simulation was used by the nationalaeronautics and space administration (nasa) during a mission. becausereflectance is measured, the graphical depiction was reported as being veryrealistic; during blackout one could hardly tell that a simulation was being used.a second version is a raster scanner that employs as the first scannedmirror a multifacet pyramidal mirror, which is rotated continuously at a highrate. linear ccd arrays or psds may be used for detection. frames of 480 linesby 512 pixels are collected at video rates (33 hz). in an early prototype, theworking volume was a 50 mm cube (beraldin et al., 1992); hymarc isdeveloping a version with a 1 m cube field of view. resolution in the field ofview is 9 bits (1 part in 512).laser radarlaser radar, or ladar for short, involves calculating the time required for alight beam to travel from the source, reflect off an object, and travel back to adetector; this principle is similar to ultrasonic ranging. the time of flight (in thepicosecond range) can be directly measured for laser pulses emitted in rapidsuccession and scanned. ladar is more appropriate for long distances thantriangulation systems; for example, maclean et al. (1990) employed atriangulation laser scanner for short range (0.5 to 10 m) and a ladar scanner forlong range (up to 2 km).in one commercial example, ibeo lasertechnik obtained accuracies of 2cm at 4,600 point measurements per second. two other methods for calculatingthis time are: (1) the phase shift between outgoing and incoming amplitudemodulated (am) light beams and (2) the beat frequency of a frequencymodulated (fm) beam (besl, 1988; blais et al., 1991).for the am method, the diffused reflected beam is typically six orders ofmagnitude less than the outgoing beam and is detected by a telescopeposition tracking and mapping197virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.near the outgoing beam (chen et al., 1993). mirrors, as in rioux (1984), areemployed to scan the beam in a pointtopoint fashion. because phase is beingdetected, there is an ambiguity interval of 1/2 wavelength, typically around 1 m.two notable examples of such ladar systems are the environmental researchinstitute of michigan (erim) system (hebert and kanade, 1989) and theodetics 3d laser imaging system. the odetics system has a resolution of 9bits, a frame of 128 × 128 pixels, a frame period of 835 ms/frame, and a field ofview of 60 × 60 deg. by use of advanced calibration procedures, accuracies of0.15 in have been reported (chen et al., 1993).further information on ladar sensors, as well as on millimeterwave radar,may be found in the review of remote vehicles in chapter 9.laser interferometerslaser tracking systems employing interferometry have been used in robotcalibration and tracking in the past decade; good surveys are presented in jianget al. (1988), kyle (1993), and mooring et al. (1991). the precision andaccuracy of interferometry is very high, although the cost is currently too greatfor routine use.a laser beam is steered to a retroreflector on a robot end effector ormoving body by a servocontrolled mirror on a twoaxis, galvanometerdrivenscanner. either a mirror retroreflector (an open corner) or a solid glassretroreflector (referred to as a cat's eye) is used (kyle, 1993). the mirrorretroreflector has a smaller working range than the cat's eye (±20 deg versus±60 deg), but can be made smaller. the retroreflector reflects the beam back tothe scanner, where a beam splitter directs the beam to a photodetector forinterference fringe counting and to a psd. based on the psd output, theoutgoing beam is deflected to the center of the retroreflector for tracking. thecalculation of the threedimensional position of the retroreflector is done in oneof two ways:(1) the two mirror angles on the scanner are measured (lau et al., 1985).then one scanner is sufficient, as the spherical coordinates areprovided. a commercial version is the smart 310 system by leica.(2) the two mirror angles are not employed, but three laser scanners arerequired for three distances. chesapeake laser systems, inc., hasdeveloped a commercial system.a problem with laser interferometers is that only incremental displacementis provided. to obtain absolute distance, some calibration procedure must befollowed. for the chesapeake laser systems device, it has been shown that byadding a fourth scanner the system can selfcalibrate (zhuang et al., 1992).position tracking and mapping198virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.another problem is to provide orientation as well as position. one solutionis to use a steerable mirror instead of a retroreflector (lau et al., 1985), whichprovides two orientation measurements. more recently, prenninger et al.(1993a, 1993b) have determined all orientation components by imaging of thediffraction patterns of the edges of a modified mirror retroreflector. orientationresolutions of 1 arcsec are stated, and motions can be tracked that accelerate at100 m/s2.acoustic trackersacoustic trackers employ at least three microphones to triangulate anemitter on the moving body. they have been employed in robotics forcalibration (stone, 1987) and in biomechanics for motion tracking (soechtingand flanders, 1989). commercial implementations for the ve market includethe gp83d developed by science accessories, the logitech 3d/6d mouse,and the mattel power glove. for point tracking at modest accuracies andspeeds, ultrasonic trackers are a reasonable and very inexpensive alternative tomagnetic sensors: the ranges are larger, and magnetic interference is not aproblem. however, a clear line of sight must be maintained and the latency isproportional to the largest distance being measured. because at least threepoints are required to infer body orientation, it is difficult to measure full poseat adequate rates.most such systems measure the time of flight of ultrasonic pulses. thereare a number of technical problems that make it difficult to achieve goodaccuracy, speed, and range with this technique. the first factor is the frequencyof the ultrasonic carrier wave. a shorter wavelength makes it possible to resolvesmaller distances, but atmospheric attenuation increases rapidly with frequencystarting at about 5060 khz. most current systems use 40 khz tone pulses witha wavelength of about 7 mm. metallic sources such as jingling keys produceenormous quantities of energy in this frequency band, making it extremelydifficult to achieve immunity to acoustic interference. the use of higherfrequencies could avoid some of the interference and increase the resolution,but atmospheric attenuation would limit the range. furthermore, at highultrasonic frequencies it is difficult to find an omnidirectional radiator, andmicrophones are expensive (over $1,000 each) and require high voltage.another major problem is echos from hard surfaces, which can have asmuch as 90 percent reflectivity to ultrasonic waves. at the siggraph '93convention, bauer of acoustic positioning research, inc., demonstrated anultrasonic tracker that uses patented algorithms to achieve robust noise and echorejection while tracking over a 25 ft range with 1 in resolution. this system,called gams, also employs an unusual invertedposition tracking and mapping199virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.strategy, with the sound sources mounted on the ceiling and the microphones onthe users, so that any number of users may be tracked simultaneously withouthaving to reduce the 30 hz update rate. by contrast, the gp83d has an updaterate of 150 hz divided by the number of emitters being tracked. an advantageof the traditional oneemitteratatime approach is that echo cancellation iseasier: the first arrival is always the direct path unless the line of sight is blocked.celesco transducer products, inc., is advertising a new wireless 40 khztimeofflight system called the vscope, with 0.1 mm resolution over a 16 ftrange.instead of time of flight, phase coherence (meyer et al., 1992) is anincremental motion technique (like interferometry) for which absolute distancemust initially be calibrated by some other means. phasecoherent tracking alsohas problems with reflections and some environmental noises. in order toovercome the drift problem, applewhite (1994) has developed a variation calledmodulated phasecoherence, which can achieve submillimeter accuracy.inertial trackingthe use of accelerometers or of angular rate sensors for motion tracking isbecoming increasingly attractive because of advances in sensor design. forexample, silicon micromachining has begun to produce very small inertialsensors. ic sensors markets solidstate piezoresistive accelerometers, whichemploy a micromachined silicon mass suspended by multiple beams to a siliconframe. the gyrochip developed by systron donner employs a pair ofmicromachined tuning forks, which sense angular velocity through the coriolisforce.1 the gyroengine developed by gyration is a miniaturized spinningwheel gyroscope that is even smaller than the gyrochip.to derive position or orientation, the output of these sensors must beintegrated. the result is sensitive to the drift and bias of the sensors. anotherserious problem is that any inertial sensor based on beam bending will sufferinaccuracies due to nonlinear effects. forcebalance accelerometers, such asthose from sundstrand, avoid nonlinear beambending problems; they areremarkably stable and have been employed in kinematic calibration after doubleintegration (canepa et al., 1994).nevertheless, some drift is inevitable. either an inertial package mustperiodically be returned to some home position for offset correction, or it mustbe used in conjunction with some other (possibly coarse) position1 a deflecting force acting on the body in motion due to the earth's rotation.position tracking and mapping200virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.sensor and an appropriate method of data fusion. the latter option could bequite attractive. an inertial orientation tracker has been built at mit usingtriaxial angular rate sensors with gravimetric tilt sensors and a fluxgate compassfor drift compensation (foxlin and durlach, 1994). it achieves 1 ms latency,unlimited tracking volume, 0.008 deg resolution, and 0.5 deg absolute accuracy(no drift). the system is now being extended to track translation as well asorientation.inertial sensors are also useful in conjunction with other position trackingsystems for lead prediction. in the highend hmd system from caeelectronics, the outputs of a fast optical headtracker are combined with angularvelocity measurements to predict future head orientation. in this manner, the100 ms graphics rendering latency is effectively shortened to under 60 ms (ronkruk, cae electronics, personal communication, 1994). lead prediction hasalso been implemented using kalman filters (friedmann et al., 1992; liang etal., 1991). additional information on the use of inertial sensors is presented inchapter 9.eye trackingeye movement trackers have been generally surveyed by durlach et al.(1992). the general types are electroocular (measurement of the corneoretinalpotential with skin electrodes), electromagnetic (measurement of magneticallyinduced voltage on a coil attached to a lens on the eye), and optical (reflectionsfrom the eye's surface). only the opticaltracking methods seem particularlysuited for general use, because they are less invasive and can be reasonablyaccurate.the rk416 pupil tracking system developed by iscan inc. employs avideo camera to track the pupil of the eye. the camera coordinates areconverted to eye rotation angle through a calibration procedure involvingfixation at known targets. accuracy is stated as 1 deg and bandwidth as 60 hz.compensation for minor head movements is accomplished by relative trackingof the first purkinje image or by head spot tracking.the series 1000 infrared eye movement spectacles developed bymicroguide, inc. employ differential reflections of infrared light from two sidesof the iris, detected by photodiodes. sensitivity is stated as 0.1 deg, bandwidthas 100 hz, but linearity of only 10 percent. sources of artifact for such systemsdue to inaccurate positioning of sensors were considered by truong and feldon(1987), due to changes in reflectivity among sclera, iris, and pupil. the sensorsare mounted on a post projecting down from a band wrapped around theforehead. the ober2 developed by permobil meditech, which operates by asimilar principle, employs goggles instead.position tracking and mapping201virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.research needsin general, position trackers are required that have adequate performanceat reasonable costs. for limb segment tracking, some forms of optical sensingare already accurate and fast enough but are too expensive for routine use (inthe $50,000100,000 range), especially when multiple placements to overcomeobscuration or to increase workspace are considered. whatever sensory systemis employed for limb tracking, there will be difficulties in identifying reliablefiducial points because of the softness of tissue and clothes. to infer jointangles, some calibration procedure must be applied to set up coordinate systemsin limb segments; subsequent joint angle inferences will only approximate thetrue biomechanical angles because of less than ideal joints and measurements.for wholebody tracking, the size of the workspace is an important issue.ideally, one should be able to track a moving person in a sufficiently largespace without loss of resolution or worries about obscuration. people shouldeven be able to move from room to room in a building without loss of tracking.if all of the major body segments are to be tracked, then sensory systems thatrequire mounting something on the body (reflectors, markers, goniometers) areless attractive than systems that scan the body as is. for body andenvironmental surface mapping, some forms of laser trackers are alreadyaccurate and fast enough to be generally useful but are still expensive.the longterm goal for ve purposes is for a highaccuracy, realtimeoptical mapping system that tracks the human body as required. this could be astereo vision system with natural light or active laser scanners. one step in thisdirection has been taken by mulligan et al. (1989), who employ a modelbasedvision system to track a moving excavator arm that is controlled in cartesiancoordinates by a hand controller. the excavator joints are not sensed but arecalculated from the locations of the boom, stick, and bucket of the excavator. inanother project, a human hand is observed in a grasping task, then the grasp isduplicated by a robot hand (kang and ikeuchi, 1993). for hmd applications,the user has to wear something anyway, so headtracking, which has much moredemanding requirements than bodytracking, can probably be more accuratelyaccomplished by sensors on the hmd.below we comment on particular needs for each basic type of sensorysystem. mechanical trackers for goniometers, research is needed on difficultissues of fit and measurement, such as adjustments to differentindividuals, alignment with joints, sufficiently rigid attachments, andcalibration of the linkage plus human limb. for bodybased linkages, theability to track multiple limb segments and limb redundancies needs to beaddressed.position tracking and mapping202virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.hybrid groundbody based systems are likely to be required; forexample, finger motion should be tracked as well as hand motion. magnetic trackers significant current disadvantages that limit usefulnessinclude modest accuracy and high latency (2030 ms). the high latenciesare particularly troubling, as this limits their usefulness in realtimeinteraction. the accuracies are not competitive with most other trackingtechnologies. furthermore, the influence of extraneous magnetic fields inthe case of ac sensors makes it difficult to know the accuracy one isgetting; there is no simple way of determining and compensating forinterfering magnetic fields. it is an open question to what extent theaccuracies and latencies can be improved. optical sensing optical sensing is one of the most convenient methods,but has drawbacks due to visibility constraints. these drawbacks can bepartially ameliorated by using multiple camera placement or targetplacement. the ability of passive stereo vision systems to processarbitrary environments is a longrange goal of the computer visioncommunity; when eventually developed, stereo vision would represent anextremely attractive method for position tracking and mapping. in themeantime, developments in laser scanning and laser radar are promising,as sampling rates, fields of view, and accuracies are becoming quitereasonable.laser interferometers are capable of the highest accuracy, which doesnot change with viewing distance. if the cost could be brought down, theywould represent an attractive method of endpoint or headtracking.because of the need for retroreflectors, it will be relatively difficult totrack multiple limbs. visibility constraints are a problem; if beams areever interrupted, the absolute reference is lost. relatively robust methodsfor establishing absolute references are required, perhaps throughredundant sensing.in general, costs will have to be brought down for optical trackers to bemore widely used. acoustic trackers these trackers have a definite role to play in ves,because the costs are relatively modest and the accuracies are oftensufficient. if in situ calibration of the speed of sound in air could beperformed, or if ambient measurements could be taken that feed into amodel, then the accuracy of acoustic trackers could be improved.improvements in detection methods could probably reduce the effect ofechos. by using multiple frequencies, it should be possible to trackmultiple markers simultaneously. drift problems with phase coherentsystems might be resolved by dead reckoning with timeofflightmeasurements. inertial trackers further reductions in sensor size and cost are needed tomake inertial trackers a convenient and economical alternative tomagnetic trackers. hybrid systems combining inertial sensors withposition tracking and mapping203virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.other technologies need to be developed for highperformance hmdtracking applications requiring both accurate registration and fastdynamics. a particularly promising combination is an allinertialorientation tracker combined with a hybrid inertialacoustic positiontracker. eyemovement trackers an ideal eye tracker would satisfy threerequirements: linear response over a large range (roughly 50 deg), highbandwidth (1 khz), and tolerance to relative motion of the head. to date,no devices satisfy all three requirements. the ccd imaging systems suchas iscan have a reasonably linear response, but the sampling rates aretoo low and range is limited to about 20 deg. the infrared reflectiondevices have the bandwidth, but are linear only in a small range;calibration to overcome their nonlinear responses is difficult. characterization methods one of the most confusing issues whenconsidering which positiontracking solution to adopt is the lack ofagreement on the meaning of the performance specifications and howthey should be measured. standards need to be set defining how tomeasure accuracy, resolution, latency, bandwidth, sensitivity tointerference, and jitter. equipment for making inhouse measurementsshould be made commercially available, and the establishment of anindependent testing laboratory would also be beneficial so that consumerswould not be forced to rely on manufacturer's specifications, whichusually have little relation to actual performance.position tracking and mapping204virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.6wholebody motion, motion sickness, andlocomotion interfacesmany virtual environments (ves) will give the user a sense of motioneither in the form of passive transport in a "vehicle" or through active locomotormovements on a supporting surface. the latter experience couples the voluntarylocomotor activity with visual and auditory sensations. in both situations, thesupport of the user's body by the physical contact surfaces constitutes a hapticinterface. the term haptic interface is being used here in a more general sensethan in chapter 4. here we use this term to refer to any place at whichmechanical energy interchanges take place between the body and theenvironment. in the case of passive transport in a vehicle, haptic interfaces withthe vehicle are the places at which the vehicle applies accelerative forces to theindividual's body. the haptic stimulation associated with these forcescontributes to the perception of body motion. similarly, during voluntarylocomotion, such as walking, the energy interchange between the feet and thesupport surface produces haptic cues that provide information about surfacecharacteristics (e.g., compliance, slope, texture) and body displacement.both passive and active displacements of the entire body (and of parts suchas the torso during rotary movements of the torso on the hips) reorient sensoryreceptors visàvis the environment. the resulting changes in receptor activitynormally are taken into account, so that perceptual stability of the surroundingsis maintained during active or passive selfmovement. in ves, headtrackersand other forms of position monitors are used to update visual and auditorydisplays to compensate for changes in body position. to the extent that there aretime delays beyond 20 or 30wholebody motion, motion sickness, and locomotion interfaces205virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.ms or gain changes between the body movement and the stimulus update,performance problems can be anticipated. similarly, if the optical verticals in ave do not correspond with the gravitational vertical of the real environment,orientation and movement difficulties may be experienced by the user.wholebody movements and locomotion raise a large set of issuesconcerning the forms of compensation that take place during selfmovement,the perception of forces on the surface of the body during movement, theperception of selfdisplacement through space, and of one's voluntary actions.the way in which adaptive compensations for unusual environments and formaintenance of accurate sensorimotor calibration are achieved is also crucial.all of these issues are critical to understanding how people will adapt to vesinvolving locomotory movements, passive transport, and head, arm, and torsomovements. however, it is important to recognize that our knowledge of theseareas is incomplete. motion sickness is a factor that is certainly going to affectperformance in ves involving locomotion and experienced selfmotion. it isnecessary to be aware of the wide range of factors that contribute to motionsickness, the variety of the symptoms, and its sometimes subtle characteristics.status of the relevant human researchsensorimotor stability during selfmotionunder normal circumstances, an individual accurately perceives his or hervoluntary movements and perceives the surroundings to be stable when they areactually stable. this stability that we take for granted is the result of complexsensorimotor adaptations to the 1g force field of earth. the existence of theseadaptations becomes apparent during exposure to non1g force levels orconditions of sensory rearrangement (lackner and graybiel, 1981). one aspectof these adaptations includes not perceiving veridically the forces acting on thebody surface when they are due to voluntary activity or passive support of thebody against the force of gravity. for example, the forces on the bottom of thefeet feel roughly comparable when one is standing on one or both feet eventhough in the former case the force on the stance foot is twice as large (lacknerand graybiel, 1984c). similarly, in running, the forces on the feet can vary from0 to 3g, yet these huge changes are not perceptually registered. when thesesame force levels are passively applied, the sensation is very strong.body movements are accompanied by various sensorimotor consequences.in the case of object manipulation, the pattern of sensory stimulationwholebody motion, motion sickness, and locomotion interfaces206virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.of the hands in relation to the patterns of motor activity controlling the handsand arms and other parts of the body allows identification of properties of theobject such as its shape, texture, and weight. in the case of locomotion, similartypes of information along with vestibular cues allowševen in the absence ofvisionšquite accurate judgments of the distance and direction moved andproperties of the support surface, such as its inclination and surface properties.manipulation of virtual objects and locomotion in ves will disrupt manyof the sensorimotor regularities that are present in the normal environment. thisis a significant issue because we have limited experience with violation of theseconstraints, and most probably only a relatively small set of the relevant oneshave yet been identified. consequently, movements made in ves that violateterrestrial sensorimotor constraints may initially cause performance decrementsand elicit symptoms of motion sickness (lackner et al., 1991). in fact, reports ofmotion sickness in simulated environments are becoming commonplace. aswith other unusual sensorimotor environments, continued exposure is likely tolead to adaptive compensations that restore accurate performance and alleviatesymptoms of motion sickness. however, on return to the normal environment,there may be persistence of adaptation to the virtual situation leading toperformance decrements and motion sickness in the normal environment. itmay be possible, however, to create states of dual simultaneous adaptation suchthat accurate performance is possible in the normal as well as in one or moreves. the possibility of such dual adaptations has been shown for a number ofunusual force situations, such as rotating and nonrotating environments(lackner, 1990), as well as for other types of situations (e.g., welch et al., 1993).head movementshead movements made during exposure to increased or decreasedgravitoinertial force background levels tend to bring on symptoms of motionsickness because the normal patterning of sensorimotor control of the head andpatterns of sensory feedback are disrupted (lackner and graybiel, 1984a, 1985,1987; lackner et al., 1991). virtually any alteration in the normal patterning ofcontrol can be provocative (lackner and dizio, 1989). for example, wearing aneck brace requires head movements to be achieved by motion of the torso; thisalteration in motor control can be quite provocative for many individuals.passive exposure to various types of motion is provocative because of thelabyrinthine stimulation involved (e.g., on shipboard), but such exposure is evenmore provocative if the head is not passively supported but rather activelycontrolled (lackner et al., 1991).wholebody motion, motion sickness, and locomotion interfaces207virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.head movements made during everyday activities are not provocative. bycontrast, the same head movements made during passive rotation of the bodyare extremely stressful and rapidly evoke disorientation and symptoms ofmotion sickness. movements of the head during body rotation create unusualpatterns of activation of the semicircular canals of the inner ear and generatecoriolis forces on the head (lackner and graybiel, 1984b, 1986). thesecoriolis forces are proportional to the velocity of rotation and the velocity ofthe radial motion of the head (or other body part such as the hand) and act in thedirection opposite that of the rotation. they are absent prior to and at the end ofa movement. coriolis forces are present when head movements are made duringpassive and voluntary body rotations. the fact that they are disorienting andproduce motion sickness during passive and not active turning movementsmeans that we automatically take them into account in controlling our voluntarymovements.a sense of presence may be a key factor in determining the human'sresponses to coriolis stimulation and to expected coriolis stimulation in ves.visual stimulation, especially wholefield visual motion, can elicit sensations ofselfmotion. if exclusive selfmotion is being experienced in an apparentlystationary visual field, then head movements can rapidly elicit symptoms ofmotion sickness and disorientation (dichgans and brandt, 1978). by contrast, ifvisual motion is being experienced and little selfmotion, then head movementstend to suppress the sensation of selfmotion and less motion sickness will beexperienced than from simply looking passively without moving the head(lackner and teixeira, 1977; teixeira and lackner, 1979). accordingly, vedisplays that induce apparent wholebody motion and that require headmovements are likely to elicit greater levels of sickness the greater the fidelityof the experienced selfmotion and sense of presence.altering the sensorimotor control of the head with a neck brace can bemildly provocative. however, altering the weight of the head and its effectivemoment of inertia can be extremely provocative, even when the pattern ofvestibular input is normal for the actual motion of the head. for example,wearing a helmet that increases the effective weight of the head by 50 percentgreatly increases susceptibility to motion sickness during exposure to constantpatterns of angular acceleration and also causes natural voluntary headmovements to be provocative (lackner and dizio, 1989). headmounteddisplays (hmds) are an integral component of many ve systems and affect thesensorimotor control of the head. this altered inertia of the head may beprovocative if the displays are worn for more than several minutes and headmovements are made. that is, simply wearing an hmd can be provocative initself, regardless of the scenes displayed. to the extent that the display inducesapparent selfmotion,wholebody motion, motion sickness, and locomotion interfaces208virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.head movements may be extremely provocative. in addition, the time delays inhmds associated with updating the visual scene to compensate for headmovements disrupt the normal patterning of the vestibuloocular reflex. this isespecially nauseogenic in widefield hmds in which largegaze shifts occur asindividuals turn their head and eyes to acquire targets in the peripheral visualfield (lackner and dizio, unpublished observations). the point to be taken isthat there is a spectrum of factors that can be expected to evoke sickness.arm movementsarm movements made to objects or to control devices are accompanied bypatterns of sensory feedback related to the control of the arm and the objectmanipulated. object characteristics include inertia, mass, weight, texture,compliance, etc. expectancies about the properties of objects are exceedinglyimportant. for example, when visually similar objects of different physical sizesbut identical masses are hefted in succession, the larger sized one will beperceived as being considerably lighter. the motor plans for hefting the objectsprobably include compensation for a greater expected mass of the larger object.this effect, known as the sizeweight (or charpentier's) illusion, persists,however, even when the objects are known to be of comparable weight. insofaras object manipulation in ves violates cognitive expectancies about objectbehavior, performance decrements will occur, adaptation will be required, andaftereffects can be anticipated on return to the normal environment.during passive transport of the body in vehicles as well as duringvoluntary locomotion and turning of the body, arm movements are usually quiteaccurate. in a familiar vehicle, even one not being selfcontrolled, compensationcan be made for the ongoing and expected motion of the vehicle (e.g., onshipboard, the rolling and scending motions are periodic and can beanticipated). during exposure to linear and angular acceleration, adjustments ofthe entire body as well as arm movement control may be required. for example,the driver of a vehicle will physically lean into a turn (fukuda, 1975).arm movements made during exposure to passive rotation of the bodygenerate coriolis forces that deviate the arm from its intended trajectory andtarget. with repeated reaches, even without visual feedback about movementaccuracy, accuracy will be rapidly regained if the hand makes physical contactat the end of the reaching movement. however, in the absence of vision, ifterminal contact of the hand is not present at the end of the movement,adaptation does not take place or is greatly slowed (lackner and dizio, 1994).on cessation of body rotation, pointing movements made to targets show errorpatterns that are mirror imageswholebody motion, motion sickness, and locomotion interfaces209virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.of the initial perrotary reaches. in other words, adaptation to the rotatingenvironment persists and readaptation to the stationary environment is necessary.these findings are relevant to movement control in ves for severalreasons. first, in everyday behavior, reaching movements made duringvoluntary body movements of the trunk or whole body generate coriolis forcesfar in excess of those used in experimental studies, yet movements are madeaccurately. this means that motor compensation is made for expected coriolisforces associated with selfmovement. second, during illusory (e.g., visuallyinduced) selfrotation, compensation may be made during reaching movementsfor coriolis forces that would be present during actual body rotation;consequently, reaching errors result. thus, the perception of selfmotion per semay be adequate to induce motor compensations for coriolis forces expectedwith actual rotation. ves that induce apparent motion of the body may lead tocompensatory motor adjustments during arm and body movements for expectedforces that are actually absent. third, with prolonged exposure, individualsadapt their movements. such adaptation can persist on return to the normalenvironment so that compensation is not initially made during body motion forcoriolis forces that are actually present. posture and movement control take intoaccount not only coriolis forces but also a variety of other forces associatedwith experienced linear and angular accelerations and angular velocity, such ascentrifugal forces (lackner, 1993). to the extent that ves cause experiencedbody motion, compensation for expected forces may be generated.the extent to which an individual feels present in a ve may determine theextent to which compensation for expected inertial forces occurs. moreover, thesignificance of these compensations may differ for different ves. for example,if simulation of a highperformance aircraft is being used for training purposesand a high degree of presence is achieved for dynamic flight conditions, thenthere potentially could be negative transfer to actual flight situations. forexample, compensation might not be made initially for actual inertial forcesgenerated in flight because this compensation has been trained out in thesimulator. to deal with such issues, it will be necessary to exploresystematically the possibility of adapting to multiple environmentssimultaneously. it is notable that motion sickness in flight simulators is morecommon in experienced pilots than in flight trainees (kennedy et al., 1990). thepilots expect patterns of forces that are absent in the simulator; for example, in aflight simulator, backward tilt is often used to simulate forward acceleration; bycontrast, during actual forward acceleration the gravitoinertial resultant force onthe individual rotates and changes in magnitude.wholebody motion, motion sickness, and locomotion interfaces210virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.locomotionduring normal walking, the pattern of visual flow is determined by stepfrequency and stride length. muscles of the neck and torso are activatedcyclically to keep the head and torso from being rotated backward by thereaction forces generated. the walking individual appropriately perceives thevisual surroundings as stationary, the walking surface as stationary, andcorrectly appreciates his or her actual stride frequency and length. theserelationships are dependent on dynamic sensorimotor calibrations that are tunedover time to accommodate for changes in body dimensions, for example, in leglength and body weight.when the relationship between visual flow and locomotory movements isdisrupted (e.g., by having a subject walk in a circular treadmill within a largeoptokinetic drum) so that the visual flow is inappropriate in speed or indirection for the stepping movements being made, a variety of perceptualremappings occur (bles, 1981; lackner and dizio, 1988). these remappingsare such as to normalize the experienced body displacement through space andthe apparent frequency, direction, and length of stepping movements (lacknerand dizio, 1988, 1992, 1993). if visual flow is double normal for the steppingmovements being made, the individual will perceive either an increase instepping rate or that the stepping leg lengthens to give the extra displacement tothe body. by contrast, if the visual flow is inappropriate in direction, anindividual will feel he or she is voluntarily stepping backward when actuallymaking forwardstepping movements, or feel that forward steps paradoxicallypropel them backward. if the person is holding a stationary bar, the bar willseem to take on a life of its own. with double normal visual flow, the bar willseem to pull the person forward, causing him or her to go faster; with reversevisual flow, the bar seems to move backward, forcing the individual to gobackward. this means that the perception of causality and of the forces on thebody are being perceptually remapped along with the individual's perception ofvolitional activity and body configuration. when individuals are exposed tovisual velocity increases, such as would be associated with body acceleration,they have difficulty controlling their stepping movements and may have to holdon for support.interestingly, when individuals walk at a constant speed on the treadmillduring conditions of constant visual flow (regardless of whether the flow isappropriate in direction or magnitude for the actual stepping movements beingmade), they experience relatively few motion sickness symptoms. by contrast,individuals who are seated and exposed to the same visual flow patterns willreport symptoms within a few minutes. it is likely that individuals walking on atreadmill who are exposed to variations in visual flow velocity will experiencemany more symptoms.wholebody motion, motion sickness, and locomotion interfaces211virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.ve systems that embody mismatches between patterns of visual flow andactivity associated with locomotion can be expected to distort the perception ofbody displacement and of voluntary activity and even of body dimensions.however, for ve displays involving locomotion, the degree to which motionsickness is evoked may be relatively minor when visual motion is coupled tovoluntary activity.body orientationunder terrestrial conditions, the force of gravity provides a constantreference for orientation, determining unequivocally the direction of up anddown (howard and templeton, 1966; howard, 1982). our environment, bothnatural and artificial, also embodies an orientational polarization. the trunks oftrees are near the ground (down) and their tops are up. rooms have floors andceilings. within this environment, only certain body orientations andconfigurations are possible. locomotion can only take place on the floor of aroom, not the walls or ceiling. this means that only certain perspectives of theroom are naturally possible. for example, one cannot view the floor of the roomor the walls from the perspective of being physically located at the ceiling.similarly, one cannot (without being artificially suspended) see one's feetspatially separated from the floor when no other part of one's body is in contactwith the floor of the room. these considerations raise the possibility that if oneexplores a ve by means of a locomotory walking interface until one hasmastered the geographical layout of the environment, then this sameenvironment may seem unfamiliar if one traverses it in a different way, such asusing a control stick allowing one to ''fly" through it.experience in weightless environments in which terrestrial constraints onorientation can be violated shows that our prior cognitive experience with 1genvironments limits the perceptual patterns that are experienced (lackner,1992a, 1992b, 1992c, 1993). in particular, patterns of orientation cues thatwould not be possible on earth are interpreted in such a fashion as to create aperceived terrestrial orientation. for example, an individual floating free in anaircraft that is flying parabolic maneuvers so as to create weightless conditionsmay not correctly perceive his or her true orientation. the person mayexperience an orientation that would be possible on earth. in other words, acognitive map of terrestrial possibilities influences the perceived orientation.vertigo may also be a problem with ve systems depending on theperspectives generated. height vertigošsensations of fright and instabilityusually accompanied by increased body swayšoccur when the viewingperspective is elevated and there are no nearby objects visible (bles et al.,1980). ve systems will have the clear potential to create such circumstances.wholebody motion, motion sickness, and locomotion interfaces212virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.on earth, sensations of falling are experienced during actual falling.however, there may be a strong cognitive component to the elicitation of thesesensations. during actual free fall (e.g., in parabolic flight or in orbital flight),individuals do not generally experience sensations of falling; they feelstationary (lackner, 1992a, 1992b). consequently, visual motion and quick lossof support and knowledge that one is falling, for example, from a ladder, maytrigger the sensations under terrestrial conditions. however, ve systems havethe possibility of creating situations, for example, of a person going out awindow and moving downward, that could well evoke feelings of falling anddefensive reactions. one key in designing environments will be to evaluate howdifferent motions throughšand positions withinšthe environment can beexpected to affect orientation and movement control.illusions of selfmotionthe perception of ongoing body orientation is influenced by multisensoryand motor factors. there are a variety of techniques that can be used in vesystems to provide compelling sensations of motion. however, as we notebelow, often when individuals experiencing such apparent motion makevoluntary head movements, or their heads are passively moved, they begin toexperience symptoms of motion sickness or lose their sense of selfmotion.visual influences on apparent selfmotion are often subsumed under theterms circular vection and linear vection (dichgans and brandt, 1978).individuals exposed to constantvelocity visual motion in a large rotating drumsoon feel themselves rotating at constant velocity (in the direction opposite thephysical rotation of the drum) and see the drum as stationary. when the illusionof selfmotion (vection) is highly compelling, tilting head movements can elicitdisorientation (pseudocoriolis effects partly analogous to those that occurwhen head movements are made during actual body rotation). this illusion canalso cause motion sickness (dichgans and brandt, 1978). by contrast, if vectionis not compelling or it is just starting to be experienced, head movements cansuppress it (lackner and teixeira, 1977). peripheral visual field stimulation isespecially effective in eliciting vection, but even small central fields can havean effect (dichgans and brandt, 1978).linear vection can be induced by exposure to constantvelocity linearvisual motion (berthoz et al., 1975; howard, 1982). its time course and othercharacteristics are similar to those of circular vection. depending on thedirection of visual flow, horizontal or vertical linear vection can be induced.rotation of the visual field about the optical axis can elicit sensations of bodytilt and rotation, but the exact pattern experienced alsowholebody motion, motion sickness, and locomotion interfaces213virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.depends on head orientation visàvis the body and the gravitational forcevector (held et al., 1975). wholefield visual motion is commonly used in flightsimulators to induce apparent body motion.rotating sound fields that have strong spatial volume distribution can elicitapparent motion as well as eye movements compensatory for the apparentmotion in subjects in the dark (lackner, 1977). head movements stronglysuppress the induction of apparent selfmotion by sound fields. however, it islikely that moving sound fields in conjunction with visual motion would makethe vection illusions more compelling.illusions of selfmotion can also be induced in seated individuals byhaving them pedal a platform moving under their feet or asking them to movetheir hands to turn a circular railing (lackner and dizio, 1984). such illusionscan be extraordinarily compelling and, if tilting head movements are made,disorientation and motion sickness can be evoked. the pedaling illusions workfor angular motion and undoubtedly will for linear as well. tactile stimulationof the soles of a seated subject's feet by a moving surface can also induceapparent selfmotion, as can stimulation of the palms of the hands. in fact, insituations involving physical motion of the body, somatosensory stimulationcan be as important as, and sometimes more important than, vestibularstimulation in determining apparent body orientation (lackner and dizio, 1988,1993).the spindle receptors within skeletal muscles are important elements in thedetermination of the overall apparent configuration (i.e., body schema orposition sense) of the body. the signals from these receptors are interpreted inconjunction with the motor signals controlling the physical length of themuscles. mechanically vibrating a muscle excites its spindle receptors, causingit to contract, an effect known as tonic vibration reflex. resisting the motion ofthe limb controlled by the muscle will cause illusory motion of the stationarylimb. for example, vibration of the biceps muscle of the upper arm will causeillusory extension of the restrained forearm (goodwin et al., 1972). suchillusory motion is in the direction that would be associated with physicallengthening of the vibrated muscle. it is possible by vibrating the appropriatemuscles to elicit apparent rotation about a vertical axis in standing subjects, tiltof the body, or tilt of the head (lackner, 1988; lackner and levine, 1979). suchapparent motion is generally accompanied by eye movements compensatory forthe experienced motion.individuals who are standing and walking in place on a treadmill indarkness or with synergistic patterns of visual flow come to feel that they arelocomoting over a stable stationary surface. such apparent motion throughspace is totally compelling (bles and kapteyn, 1977; lackner and dizio, 1988).the precise patterns experienced depend crucially on the relationship betweenthe visual flow patterns and stepping movementswholebody motion, motion sickness, and locomotion interfaces214virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.(see the discussion of locomotion above). presenting visual flow that isinappropriate in direction for the stepping movements being made can, forexample, make forward stepping individuals feel that they are walkingbackward over a stable surface.intersensory effects provide ways of creating various patterns ofcompelling apparent selfmotion and changes in apparent body orientation instationary individuals. such experienced motion is usually accompanied bycompensatory eye movements and other compensatory postural changes. itshould be recognized that physical motion of the body that elicits eyemovements also influences visual and auditory localization. for example, aperson in darkness exposed to leftward angular acceleration will experienceleftward turning motion, and his eyes will exhibit a compensatory nystagmuswith slow phase right. if the person is given a target light to fixate that isstationary in relation to his or her body midline, he or she will perceive it todisplace leftward off the midline. this phenomenon is known as the oculogyralillusion (graybiel and hupp, 1946): there is also an analogous audiogyralillusion. similarly, if the gravitoinertial force vector is increased in magnitudeand rotated by exposing an individual to centrifugal rotation, various types ofoculogravic (graybiel, 1952) and audiogravic illusions are elicited. analogouskinds of mislocalizations of auditory and visual signals occur duringexperienced as opposed to actual body motion.motion sicknessmotion sickness is likely to be a highly significant problem in ves thatinvolve hmds and that involve voluntary locomotion or passive displacementof the body. hmds that significantly influence the effective inertia of the headdisrupt the normal sensorimotor control of the head. this in itself can elicitmotion sickness and disorientation (lackner and dizio, 1989). likewiselocomotion and passive displacement through simulated environments will beunaccompanied by the normal patterns of forces and accelerations associatedwith such motion through the real environment. the absence of the normallyoccurring patterns of forces will render many ves highly provocative ineliciting motion sickness.motion sickness is often equated with the nausea and vomiting that canoccur during exposure to air and sea travel and, more recently, space travel. infact, it is a much more complex syndrome and at times can be difficult toidentify as such (graybiel et al., 1968; lackner, 1989). motion sickness israrely experienced during everyday activities except under conditions of passivetransport. individuals without functioning vestibular systems seem to beimmune to motion sickness; by contrast, all normal individuals are susceptibleto varying extents, although there are tremendouswholebody motion, motion sickness, and locomotion interfaces215virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.table 61 diagnostic categorization of different levels of severity of acute motion sicknesscategorypathognomonic 16 pointsmajor 8 pointsminor 4 pointsminimal 2 pointsaqsa 1 pointnausea syndromevomiting or retchingnauseab ii, iiinausea iepigastric discomfortepigastric awarenessskinpallor iiipallor iipallor iflushing/subjectivewarmth > iicold sweatingiiiiiiincreased salivationiiiiiidrowsinessiiiiiipainheadache > iicentral nervous systemsyndromedizziness, eyesclosed > ii, eyes openiiilevels of severity identified by total points scoredfrank sicknesssevere malaisemoderate malaise amoderate malaise bslight malaise> 16 points815 points57 points34 points12 pointsa aqs = additional qualifying symptoms.b iii = severe or marked, ii = moderate, i = slight.wholebody motion, motion sickness, and locomotion interfaces216virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.individual differences in susceptibility (kennedy et al., 1968; reason andbrand, 1975).the primary signs and symptoms of motion sickness are summarized intable 61. the pattern of expression of symptoms depends in critical part onindividual susceptibility and the type and duration of provocative stimulation(graybiel et al., 1968; lackner, 1989). highly provocative stimulation tends tobe more associated with elements of the nausea syndrome, such as stomachdiscomfort or vomiting. by contrast, less provocative stimulation can evokemore "head" signs and symptoms, such as drowsiness and fatigue. motionsickness is comparatively easy to identify under laboratory conditions because(1) relatively provocative stimulation is generally used to keep testing periodsbrief, (2) highly trained observers are present, and (3) subjective and objectivemeasurements of sickness are being taken so that onset and severity can bereadily determined. by contrast, under operational and other nonexperimentalconditions, motion sickness may be very difficult to recognize. fatigue,headache, and drowsiness experienced by a worker using a visual motiondisplay could, for example, be due to lowgrade motion sickness or fromstraining too long in front of the display without a rest break. most lowgrademotion sickness probably fails to be recognized as such (graybiel and knepton,1976).over the years there have been many attempts to correlate the onset andseverity of motion sickness with various physiological parameters, such as heartrate, blood pressure, peripheral blood flow, electrogastrogram activity, etc.(graybiel and lackner, 1980; reason and brand, 1975; lawson et al., 1991;cowings et al., 1990; stern et al., 1987). despite many claims of strongcorrelations, none has so far stood up to systematic experimental scrutiny, norhas any combination of measures been adequate to predict individual sicknessand severity (lawson et al., 1991). at present, training individuals to be awareof the symptoms presented in table 61 is the best that can be done in terms ofidentifying sickness onset. in fact, this may be adequate under most reallifecircumstances because subjective wellbeing and the ability to perform tasksappropriately is usually all that is necessary.progress in studying motion sickness has been hampered by the lack of anadequate theory (crampton, 1990). motion sickness is often attributed tosensory conflict because many situations that evoke sickness are associated withvarious types of conflicts between different receptor system activities, forexample, visual versus semicircular canal signals. conflict theories generallyinvolve neural models of the environment or of the physiological controlsystems of the body; so, for example, conflict occurs when expectation based onprevious experience does not match current inputs during voluntary bodymovements (reason and brand, 1975).wholebody motion, motion sickness, and locomotion interfaces217virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.unfortunately, conflict theories fail to provide a way of understanding whichconflicts will be provocative and which will not. also, such theories do notprovide for quantifying the severity of sickness. they only embody the obvious:situations that evoke sickness tend to be ones involving head or bodymovements under conditions of passive transport.these limitations make it difficult to develop techniques for predictinghow susceptible individuals will be to different forms of stimulation. therehave been a number of attempts to relate motion sickness susceptibility tovestibular sensitivity, i.e., thresholds for angular or linear acceleration.however, these attempts have failed. for example, susceptibility to motionsickness during constant velocity, offvertical rotation of the body (whichinvolves continuous stimulation of the otolith receptors of the inner ear) is notcorrelated with ocular counterrolling, a measure of otolith sensitivity and gain.similar lack of correlation has been found between provocative tests involvingsemicircular canal function and thresholds for perception of angularacceleration, a measure of canal sensitivity (miller and graybiel, 1972).susceptibility to seasickness has been related to the slope of sensationcupolograms (plots of sensation duration versus the log of impulse angularacceleration), but this relationship has proven weak at best. recently,susceptibility to motion sickness in 0g and 2g force environments has beencorrelated with the extent to which individuals exhibit velocity storage ofvestibular and visual activity (dizio and lackner, 1991). this is currently thebest predictor for space motion sickness. it is not yet known whether velocitystorage1 correlates with susceptibility to other types of motion sickness,although individuals without functioning labyrinths apparently cannot be mademotion sick and they also do not exhibit velocity storage.a confounding factor in predicting susceptibility to motion sickness is thatan individual's susceptibility to one form of provocative motion may notcorrelate well with susceptibility to another form (miller and graybiel, 1972;calkins et al., 1987). for example, susceptibility in situations primarilyinvolving canal stimulation may not correlate well with those primarilyinvolving otolith stimulation, such as bithermal caloric irrigation (to activatehorizontal semicircular canals) and offvertical rotation. susceptibilities tosimilar forms of vestibular stimulation also do not correlate that well fordifferent test situations, such as caloric irrigation1 velocity storage was hypothesized to account for spatiotemporal differencesbetween signals about rotational velocity of the body in space and responses mediatedthrough the vestibular nuclei. for example, the responses of vestibular nucleus cells,slow phase nystagmus velocity, and the sense of selfmotion persist temporally and arethreedimensionally reorganized relative to the stimulus velocity input. such responsesare modeled as a sum of the velocity input and a velocity storage signal (cohen et al.,1977; robinson, 1977).wholebody motion, motion sickness, and locomotion interfaces218virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.versus impulsive angular accelerations in a rotating chair. this means that, todetermine reliably an individual's susceptibility in a given situation, it may wellbe necessary to test him or her in that situation. the one exception to thisgeneralization is that a small percentage of the population, perhaps 10 percent,seems highly susceptible in all motion situations. such individuals havehistories of persistent sickness in cars, boats, and other vehicles and show littlechange with repeated exposure.the issue of adaptability, as well as retention and transfer of adaptation,can be as important as basic susceptibility in a given exposure situation. aperson with high susceptibility who has rapid adaptation and abatement ofsymptoms, shows retention of adaptation between widely spaced exposureperiods, and shows generalization of adaptation of motion sickness responses toother situations may be better suited for certain situations than an individual ofmoderate susceptibility who adapts slowly, has poor retention, and shows littletransfer (graybiel and lackner, 1983). adaptability and retention of adaptationhave not been explored adequately, and only a few studies have even attemptedto assess them (graybiel and lackner, 1983).adaptation to provocative motion can generally be enhanced if exposure isgradual and incremental in intensity. for example, adaptation to rotatingenvironments can be achieved by initially exposing individuals to a very lowrate of rotation, one that neither disrupts motor control nor elicits motionsickness, and having them make many head and body movements (graybiel etal., 1968; graybiel and knepton, 1976). by repeating the movements afteradditional 1 rpm increases in velocity, it is possible to adapt individuals to quitehigh velocities of rotation without significant performance decrements andwithout eliciting motion sickness. if the final velocity, say, 10 rpm, wereintroduced in a single step, most individuals would be incapacitated by motionsickness and unable to adapt regardless of exposure duration. the principle ofincremental exposure facilitating adaptation seems to be a general oneapplicable to all situations so far evaluated (lackner, 1985; lackner andlebovits, 1978) and is likely to apply to ves.in thinking about the relevance of motion sickness in ves, it is critical toremember that it is a complex syndrome with multiple etiological factors, therelative importance of which varies for different individuals and for differentintensities of exposure (kennedy et al., 1992). the presence of more than oneeliciting factor (e.g., making head movements as well as looking at a movingvisual display) is almost always synergistic in bringing on symptoms. althoughnausea and vomiting are often viewed as the most severe manifestations ofmotion sickness, they can generally be dealt with using antimotion sicknessdrugs, and in cases of extreme sickness, drug injections (graybiel and lackner,1987). from the standpoint ofwholebody motion, motion sickness, and locomotion interfaces219virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.ves, a more severe problem after elements of the nausea syndrome have abatedis almost certain to be elements of the sopite syndrome (graybiel and knepton,1976). this refers to the chronic fatigue, lack of initiative, drowsiness, lethargy,apathy, and irritability that may persist even with very prolonged exposureperiods. its elicitation would limit the ability to use virtual reality systems on aregular basis. a further disadvantage of the sopite syndrome is that it can persistfor prolonged periods even after exposure to the unusual environment is over.wholebody motion and locomotion interfacesthe control and perception of real wholebody movement and locomotioninvolves interaction with the world through nearly every possible sensory andmotor channel. in the following discussion we sketch out the range of possiblelevels of technology involvement in wholebody motion and locomotioninterfaces relevant to teleoperator and ve systems.such interfaces have the potential for extending the capabilities of humansat the expense of producing undesired side effects. exact specification of thecostbenefit ratio for a given system is not possible because we lack a theory ofthe sensorimotor regularities to which humans are normally attuned or to whichthey can become attuned. in the following discussion, we consider a variety ofinterface systems or system components relevant to wholebody motion forteleoperator and ve systems. the discussion is subdivided into threesubsections: inertial displays, locomotion displays, and noninertial displays.inertial displaysthese systems induce a sense of passive wholebody movement byexposing subjects to a different, highly constrained, body movement throughthe use of movingbase simulators. in these systems, a constrained motion basegenerates, in whole or in part, the pattern of force vectors that would be presentin the situation being simulated. these systems are extremely expensive and areusually built either for research or for very specific applications.full inertial displaysthe goal of these systems is to simulate continuously the pattern of forcesthat would be present in the real situation. a highfidelity example is thedynamic flight simulator at the naval air development center in warminster,pennsylvania. this is a large 16,000 hp gimballed centrifuge that can simulatethe angular accelerations and gloading encountered inwholebody motion, motion sickness, and locomotion interfaces220virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.f/4a, f/a18, and other aircraft. this device can convey the feel of actuallyflying with no external visual display. these types of devices represent thetechnology to be replaced by ves because they are very expensive and areinflexible in terms of simulating multiple aircraft.one new item in this class that deserves mention is the veda corporationmodel 2400 mark iv vertifuge. it has two components: one is a short arm (2.5m) centrifuge with a double gimballed capsule capable of accepting multiplecockpit simulators or a virtual cockpit; the other component is software thatmaps the pattern of force vectors that would be associated with themanipulation of cockpit controls in an actual aircraft onto the distribution oftorques in the degrees of freedom of the simulator that would produce the samepatterns of forces. the software attempts to minimize spurious coriolis affectsthat are usually associated with shortradius centrifuges by combining thedevices' centrifugal accelerations with the primary, secondary, and tertiarycoriolis forces. it also takes into account aspects of vestibular control ofperceived orientation in trying to enhance the simulation.all the systems in this class can reproduce the specified patterns of forcevectors imposed on an individual or other object that remains stationary withinthe simulator. however, all of them fail to simulate both the mechanicaldynamics of any voluntary movements made within the simulator and thevestibular stimulation that would be generated during head movements.partial inertial displaysthe goal of these systems is to provide a subset of the forces that wouldoccur in the situation being simulated. there are devices, such as tiltingplatforms, that can provide the initial cues compatible with inertial bodyacceleration but cannot simulate sustained acceleration. complementary devicesoften provide some sort of cue that sustains the experience of selfmotion (e.g.,see the discussion of gseats below).a sixlegged synchronized motion base for a commercial flight simulatoris an example of a system in which a subject is seated in position and exposedto a lowfidelity simulation of the forces that would be present in a highperformance aircraft. the base can pitch back and translate forward slightly topartially simulate a forward inertial acceleration. the motion is usually used toenhance the sense of selfmotion driven mainly by a visual display. suchsystems are widely available commercially, so no specific example is mentioned.gseats are used to complement the partial inertial simulations provided bymotion bases. a gseat is itself stationary, but the seat pan and back can bedeflated to allow the user to sink into the seat as would be thewholebody motion, motion sickness, and locomotion interfaces221virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.case during forward and upward accelerations. the cushions can be inflated tosimulate the reverse. some gseats have an active harness system that controlsthe pressure applied to the front of the body. increasing the tension in theharness and simultaneously thrusting the user forward with the seat cushionssimulates deceleration. some gseats also have the capacity for providingvibration cues. most vehicles vibrate in a way that is correlated with theiracceleration through space. for experienced users, providing vibration cuesenhances perceived selfmotion, even when the proper accelerative forces arelacking. gseats may be considered haptic interfaces for the whole body but arelisted here because of their close functional association with moving basesimulators and the intimate theoretical link between haptic and inertial cues. anexample of a gseat that has all the capabilities just listed is the alcogsdevice at wrightpatterson air force base.variable gravity displaysthe graybiel laboratory slow rotation room at brandeis university wasdesigned in part to study artificial gravity. this rotating room is an extremelyflexible research tool that enables a subject to be exposed to a noninertial,non1g force background while moving about freely and being monitored bycomplex onboard equipment.examples of the room's flexibility as a research tool is the ability it gives toinvestigate all of the fundamental problems described in the first section of thischapter, including the nature of automatic load compensation during arm andhead movements, the control of eye movements, and sensory localization duringbody movement. it can also be used to study the control of locomotion innormal and moving environments. such studies are virtually impossible toconduct in small chambers in which locomotion is limited and there is no roomto set up complex equipment for threedimensional movement analysis.another way of simulating non1g environments is by placing the body ina weighted or counterweighted sling and adjusting the angle of the body inorder to increase the effective contact forces between the support surface andthe soles of the feet along the body's long axis. the national aeronautics andspace administration (nasa) funded a facility at langley field in the 1960sthat used suspension but still allowed locomotion around a track that wasbanked such that the component of gravitational force acting along the body'slong axis was equivalent to the moon's full gravitational force. results ofstudies with this device helped to ascertain what patterns of gait would be mostenergy efficient and be easiest to control during exploration of the moon'ssurface.wholebody motion, motion sickness, and locomotion interfaces222virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.locomotion displaysideally, a ground surface interface (which might be called a haptic interface for the feet) is a device that permits the user to experience activesensations of walking, running, climbing, etc., while remaining within aconstrained volume of space. such a device must allow the user to move his orher limbs in a natural fashion and provide feedback to the user that is matchedto the spacetime characteristics of the simulated surface (e.g., an inclined planeor a heaving ship deck) and must sense the behavior (positions, movements,forces) of the user in order to both control the actions of the device and toprovide appropriate information to the other display systems in the syntheticenvironment (se).locomotion interfaces can provide the experience of moving about in alarge space while actually being confined to a small space. to the extent thatsuch devices reduce the workspace volume of the synthetic environment (se)system, they reduce the requirements on the other system components. forexample, both the subsystem for general monitoring of position and thesubsystem for grounding of the haptic interface (for the hands) need not coverlarge spatial regions. more significant, however, it extends the range ofapplicability of the se system to situations in which inclusion of locomotionand/or controlled surface conditions are important, i.e., situations in whichconditions underfoot are abnormal, in which visual information is severelyreduced, or in which several individuals must coordinate their movements.treadmilltype displaysthe term treadmill originated to describe a form of prison punishment:walking on an endless belt driven by rollers has risen socially to a form ofvoluntary exercise, and the technology has advanced mainly to serve thecommercial demands of health clubs. a typical highperformance treadmill (forexample, one made by the quinton corp.) has some features that make itsuitable for conversion to a research or se tool. the motor and belt have acombined stiffness that allows the horizontal ground contact forces usuallyapplied in walking to be manipulated. it is microprocessorcontrolled so thatusers can preprogram their preferred exercise regimens, but realtime controlmust be achieved by a custom interface. the entire base can be inclined underthe control of a motorized drive system, which could also be controlled in realtime by an external custom interface.a major limitation of commercial treadmills is their onedimensionalnature. a first step in expanding their capabilities would be the development ofsystems with a belt for each foot. a splitbelt system would bewholebody motion, motion sickness, and locomotion interfaces223virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.useful for simulating turning. the laboratory of esther thelen at indianauniversity has a custombuilt device suitable for such research with infants.a useful addition to treadmill systems is being developed by the kistlercorporation. this system has a forceplate under the belt that can sense thevertical ground reaction forces produced during locomotion. this couldpotentially provide information that could be fed back to the belt through a realtime control algorithm to simulate slippery ground surfaces, for example.haptic interfaces for individual feettreadmills will be limited to use in ses that involve locomotion onuniform (but not necessarily horizontal) ground surfaces. stair climbingexercise machines are just vertical treadmills, in this sense. it may be desirableto generate ses with arbitrary footing conditions, such as sandy, muddy, andicy conditions, discrete obstacles, stairs, wobbly platforms. a general approachto realizing these conditions would be to develop individual sixdegreesoffreedom platforms or shoes for each foot. such a system would be analogous toforcefeedback haptic devices for the hands. challenges to development includedeciding whether anthropomorphic linkages are necessary, identifying drivesystems with sufficient power and bandwidth, and learning more about the roleof impedance matching in human locomotion. programs that have led to thedevelopment of improved bicycles and humanpowered aircraft have facedsome of these problems (especially impedance matching) and may provide aninitial strategy and some data for the present application.noninertial displaysnoninertial displays induce a sense of wholebody movement in stationary individuals although they can also be used in conjunction with moving bases.many simulators currently work by presenting stationary individuals withstimuli that are normally associated with body motion so as to enhance a senseof selfmotion. this section concentrates on new approaches in this area.visual displaysa variety of devices are being used at present for stimulating individualswith representations of scenes that change in a way that is consistent with bodymotion through the environment in a vehicle. the major characteristic of visualdisplays to be used in ses involving wholebody motion and locomotion is alarge stereoscopic field of view. hmds requirewholebody motion, motion sickness, and locomotion interfaces224virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.hardware and programming to adjust the display to compensate for changes inviewpoint brought about by head movements. a highresolution display is notcritical for visual induction of illusory selfmotion. however, if a lowresolutiondisplay is used, the visual scene must contain low spatial frequencies and strongpictorial depth cues. some real visual scenes contain only relatively high spatialfrequency textures. if a highfidelity facsimile of such a scene is moved, anobserver experiences selfmotion; however, presentation through a lowresolution display unit does not result in perceived selfmotion. obviously, theneed for wide (and high) fields of view, the preference for high resolution, andchanges in the visual scene to accommodate head movements will generate ahigh demand for computational and rendering speeds. although there is no clearpsychophysical data on required frame rates, occasional skips in a display thatis being generated at an average rate of 30 frames per second can reduce thesense of virtual body motion (assuming there is no moving base or any othersupplementary stimulus).research that may help guide the integration of visual displays with otherse subsystems for simulating human locomotion is being carried out in severaluniversity laboratories (lackner and dizio, 1984, 1988; warren, 1993). onedevice used for such research includes an independently controllable circulartreadmill and visual surround developed by lackner and dizio at graybiellaboratory, brandeis university. the circular treadmill is advantageousbecause combinations of overground and treadmill walking can be combined inthe same apparatus. the subject can move through space without walking awayfrom the treadmill and visual display. in this device it is possible to have asubject walk in place at a set pace on the treadmill while the visual displaypresents a scene whose movement varies in speed and direction. in thissituation, the subject perceives wholebody movement with a range of speedsand directions and varying patterns of stepping consistent with it (lackner anddizio, 1988). warren and his colleagues at brown university have shown thataltering the normal ebb and flow of visual feedback (expansioncontraction,vertical oscillation) associated with the walking cycle can alter the pattern oflocomotion and the perception of selfmotion. this points out the need fordeveloping physical models of the visual feedback from voluntary movementand incorporating it into visual simulations of wholebody movement.auditory displaystwo techniques have been used in this area: one involves presenting abinaural auditory stimulus that exhibits the same spatial and temporal pattern asit would during movement through a natural environment; thewholebody motion, motion sickness, and locomotion interfaces225virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.other involves cognitive cuing. induction of illusory selfrotation by simulatinga sound field rotating around the head of a stationary subject is an example ofthe former (lackner, 1977), and augmentation of visually induced selfmotionby virtual wind or engine noise in an aircraft simulator is an example of thelatter (previc et al., 1991). technology for realizing the first approach isdiscussed in chapter 3.there are no adequate psychophysical data to specify the exactcharacteristics of auditory displays that are critical for inducing a sense of selfmotion. however, neither the number of possible sound sources nor the fidelitywith which these sources are simulated is critical for inducing a sense of selfmotion. of more importance is creating the impression of a virtual terrainthrough which selfmotion can occur and, toward this end, the ability tosimulate a rich set of sound reflecting surfaces. technology for simulation ofechos and reverberation is also discussed in chapter 3.vestibular displaysa sense of body motion in a stationary subject can also be elicitedartificially by stimulating the semicircular canals of the vestibular system. onemethod consists of directing streams of cool air or water into one auditory canaland warm into the other. a few seconds of such caloric irrigation can lead toseveral minutes of perceived selfmotion and nystagmus. there are at least twomechanisms governing this response. altering the temperature of the temporalbone around the external auditory canal (1) locally alters the temperature of themembranous labyrinth of the lateral semicircular canal encased within the bone,and thereby causes a convection current that mechanically stimulates the haircells of the crista (barany, 1906) and (2) alters the temperature of the hair cellsand primary afferent fibers by direct conduction, leading to modulation of theiractivity levels (coats and smith, 1967). the convection component accounts forabout 75 percent of the response (minor and goldberg, 1990). this is a standardclinical technique for testing vestibular function that could, in principle, beadapted to se technology. its drawbacks for application to ses are that thelatency to onset is about 15 s, the effect is limited to the yaw plane of the head,and it can be nauseogenic. galvanic stimulation achieved by applying a currentbetween the mastoid bones can also be used to elicit apparent selfmotion bydirectly exciting the vestibular end organs.proprioceptive/kinesthetic displayseven in darkness, a sense of moving through the environment, as well ascompensatory postural and oculomotor reactions, arises whenwholebody motion, motion sickness, and locomotion interfaces226virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.someone walks in place on a treadmill (bles and kapteyn, 1977; lackner anddizio, 1988) or manually pushes a revolving railing around (brandt et al.,1977; lackner and dizio, 1984). these results demonstrate that if individualsproduce the locomotory movements that normally propel them through space(but without actually displacing), the associated muscular, joint, and tactilefeedback, as well as efferent signals, lead to an experience of selfmotion.even if no locomotor movements are made, patterns of cutaneous ormuscular afference that are normally associated with movement through theenvironment can induce apparent selfmotion. one method for achieving this ispassing a moving surface against the soles of the feet or the palms of the hands;individuals then report a sense of body motion in the opposite direction.presenting differential surface speeds to the hands and feet can lead to thefeeling that the torso is twisting (lackner and dizio, 1984).another method of eliciting wholebody motion utilizes muscle vibration.if a standard physiotherapy vibrator oscillating at 120 hz is applied to the bodysurface overlying a muscle tendon, such as the achilles tendon, the spindlereceptors of the associated muscle are stretched relative to the extrafusal forcegenerating fibers because of differential viscoelastic properties. this increasesthe output of muscle spindle ia and probably ib fibers relative to the levelappropriate for maintenance of the desired posture. the muscle contractsreflexively to relax the spindle and restore its output to the original level, andthe limb controlled by the muscle movesšfor example, the ankle extends. thisis called a tonic vibration reflex (tvr). if a limb is prevented from movingunder the influence of a tvr, the spindle activity remains high, and a limbmovement will be experienced that is consistent with the muscle's beingstretched, for example, flexion of the ankle (goodwin et al., 1972). whenstanding on the ground, ankle flexion would ordinarily mean that either theground is tilted up or the body is tilted forward. vibrating the achilles tendonsof a subject who is restrained in a standing posture thus elicits an experience offalling forward (lackner and levine, 1979). lackner (1988) has shown thatvirtually any apparent movement of the body can be elicited by vibration of theproper postural muscles. the movements experienced can be supranormal in thesense that anatomically impossible apparent body configurations are generatedšfor example, hyperextension of limbs.research needsthe research and development efforts on wholebody motion displays thatare needed for development of the se field, beyond those directedwholebody motion, motion sickness, and locomotion interfaces227virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.toward achieving more computational power, are summarized briefly in thefollowing paragraphs. inertial displays relatively simple, partial inertial displays need to bedeveloped to replace complex, fullinertial displays because of the costfactor. currently, however, there is little technology available forreproducing critical subparts of the accelerations that are present in realsituations involving active or passive transport through a large volume.development of such systems should take advantage of the intimaterelationship that normally exists between wholebody movement ininertial space and the contact forces that must be applied to the body inorder to accelerate it. for example, when a subject is accelerated, thevestibular system senses the inertial motion and cutaneous receptorsrespond to the contact force propelling the body. research needs to befocused on this area of hapticvestibular interactions. an area oftechnology development that could assist in exploring the roles of thesetwo factors is new padding materials that can allow experimenters tosystematically manipulate the distribution of contact forces on the bodysurface during accelerations in an inertial display. electrorheologicalmaterials hold some promise for research applications and eventually,perhaps, for se displays.another area that could benefit from attention is how activemovements affect the perception of wholebody motion induced bynoninertial wholebody motion displays. achieving the same bodyreferenced limb or head motions requires different muscle forces instationary and accelerative environments and also leads to differentsensory feedback because of noncontact inertial forces on the limb. thelack of expected feedback for the state of body motion being experiencedcan inhibit the perception of selfmotion and lead to a perceptual mappingthat better fits all the current sensors. making head movements duringvisually induced illusory selfmotion can suppress or enhance the sense ofselfmotion, depending on whether those movements are in the plane ofmotion and begin when visual stimulation begins or are out of the motionplane and begin after its onset.basic research may help determine methods for preventing theinhibition of perceived wholebody motion when the head or arms moveor enhancing a weak sense of selfmotion. a crucial issue here concernsthe extent to which methods can be found for inducing people to perceivecontact cues provided by means of haptic ve displays as noncontactinertial perturbations of their limbs. locomotion displays current locomotion displays consist of constantspeed linear treadmills that can provide an individual confined to a smallvolume with the pattern of visual, auditory, and tactile cues thatwholebody motion, motion sickness, and locomotion interfaces228virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.would be present if they were locomoting through a larger space.however, the only situation that can physically be mimicked by such adevice is constantvelocity, linear locomotion.there are several routes to expanding on these capabilities. first,visual, auditory, and other displays may be used to enhance simpletreadmills. to accomplish this, psychophysical work is needed todetermine the degree to which perceived acceleration and deceleration(including rotation) can be elicited by such displays, even when it isabsent in the mechanical stimulus. another direction is to improve thetreadmills. linear acceleration can be simulated if hardware and softwareinterfaces are developed that allow control of treadmill acceleration anddeceleration when propulsive step forces are generated by the subject.another advance would be treadmills with a belt for each foot. thiswould be the simplest version of individual haptic interfaces for each footand would better allow simulation of changes in direction, i.e., turning.finally, research should be performed with a view toward providing asystem that has separate multidegreeoffreedom platforms for each footwith appropriate sensors and feedback subsystems that can mimic theconditions of walking on level or inclined ground, climbing stairs, andnavigating around and over obstacles. the padding materials mentionedabove for inertial displays designed for passive wholebody motion mightalso be useful here for simulating different ground conditions. visual displays requirements on visual displays imposed by considerationof passive wholebody motion or active locomotion are similar to thosepreviously mentioned in other chapters: the best displays would be anhmd that is inexpensive, lightweight, and comfortable; has highresolution and a wide field of view (both horizontally and vertically); andincludes both fullcolor images and refined stereopsis. of all thesecharacteristics, color is probably the least important. auditory displays the most important needs in this area concern thosefeatures of the synthesized acoustic field relevant to the illusion ofmoving through the field. aside from simulating changes in the directionof sound sources, changes in the apparent distance of the sources andchanges in the apparent location of the individual within the reflectingenvironment are important. thus, one of the main special needsassociated with passive wholebody motion and active locomotion in thisarea concerns the inclusion of a rich array of reflecting surfaces in theacoustic simulation. motion sickness over the years, motion sickness has arisen as asignificant problem with all new modes of passive transport of the body(guignard and mccauley, 1990). clearly it will be a problem in ses aswell, especially those involving virtual acceleration and motion of thebody (biocca, 1992). reports of sickness in ses are already commonwholebody motion, motion sickness, and locomotion interfaces229virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.(mccauley, 1984; mccauley and sharkey, 1992). research should bedirected toward identifying the factors that determine which ses areespecially provocative and how to minimize this while preserving theefficacy of the system. mechanical factors, such as altered inertial loadingof the head by hmds, as well as sensory factors, need to be considered.also, attention must be given to elements of the sopite syndrome that aremore subtle then those usually associated with motion sickness. sensorimotor loops many se systems introduce distortions, time delays,gain changes, and statistical variability (noise) between voluntarymovements and associated patterns of sensory feedback. systematicresearch is necessary to determine the extent to which these factorsdegrade performance and the subjective state of the user. acceptabletolerances should be determined for these factors, as well as for the extentto which sensory feedback across different modalities must be in temporalsynchrony. multisensory and motor influences on orientation this is a criticalresearch area for designing effective ves that involve locomotion andhaptic exploration. very little is known at present about these influences,except that they are highly complex and pervasive. they are difficult toidentify as such because so much of what we take for granted in oureveryday activities, such as the perceptual stability of our environmentand our bodies during movement, is due to their action.wholebody motion, motion sickness, and locomotion interfaces230virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.7speech, physiology, and other interfacecomponentsspeech recognition and synthesissince speech is the most natural form of human intraspeciescommunication in the real world, it is important to examine the progress andproblems associated with research and technology for speech recognition andsynthesis by computers for use in communicating with humans in a syntheticenvironment (se). machine recognition and understanding of oral speech hasbeen and continues to be a particularly difficult problem because of theenormous flexibility and variability in speech at both the intersubject andintrasubject levels. moreover, basic grammar rules are often violated in oralcommunication. although humans clearly are able to overcome the problemsinvolved (e.g., by using contextual cues to assist in the interpretation ofmeaning), it is extremely difficult to duplicate these capabilities with computersoftware. general background on speech communication, both human andautomatic, is available in o'shaughnessy (1987); information on commerciallyavailable automatic speech recognition systems can be found in currentnewsletters and magazines such as asr news, voice news, voice processing,and speech technology.computer generation of speech also suffers from problems that remain tobe solved. in particular, currently available speech synthesis technology doesnot provide speech that sounds natural or that can be easily matched to thecharacteristics of an individual speaker. nevertheless, there are several types ofspeech synthesis systems available commerciallyspeech, physiology, and other interface components231virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.that produce speech with a range of quality and intelligibility. thesesynthesizers are in use in reading machines for the blind and in certaincommercial applications that require information to be provided automaticallyin response to telephone requests. a comprehensive review of speech synthesisappears in klatt (1987); information on commercial speech synthesis systems isavailable in the same newsletters and magazines cited above for speechrecognition.speech recognitionthere are at least three critical factors contributing to the complexity ofspeech recognition by machine. the first relates to variation among speakers.for a system to be speaker independent, it must be able to functionindependently of all the idiosyncratic features associated with a particulartalker's speech. in speakerdependent systems, the computer system functionsproperly only for the voice or voices it has been trained to recognize. thesecond critical problem relates to the requirement that the system be able tohandle continuous speech input. at the present time, most systems are capableof recognizing only isolated words or commands separated by pauses of100250 ms; however, some systems are now becoming available that recognizelimited, clearly stressed, continuous speech. the third important factor is thenumber of words the system is capable of reliably recognizing. vocabularysizes in existing systems range from 2 to 50,000 words. further importantfactors contributing to the difficulty of speech recognition include intrasubjectvariability in the production of speech and the presence of interferingbackground noise and unclear pronunciation. in general, the more predictablethe input speech, the better the performance. thus, for example, a systemdesigned to recognize discretely presented digits spoken by a single person in asoundisolated room can be made to perform essentially perfectly.most of the current successful speech recognition systems rely primarilyon an informationtheoretic approach in which speech is viewed as a signal withproperties that can best be discerned through statistical or stochastic analysis.recognition systems based on this approach use a simple model to relate text toits acoustic realization. the parameters of this model are then learned by thesystem during a training phase. widely accepted practice represents speech as aset of 10 to 30 parameters extracted from the input at a fixed rate (typicallyevery 5 to 20 ms). in this fashion, the input speech is reduced to a stream ofrepresentative vectors or numerical indices for each parameter (davis andmermelstein, 1980).two classes of systems that fall into the informationtheoretic category aredynamic time warping (dtw) and hidden markov modelingspeech, physiology, and other interface components232virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.(hmm). in its simplest form, dtw compares the input speech to sets ofprestored templates or exemplars corresponding to the prerecorded utterance ofeach vocabulary item. to add robustness, several templates per word may berecorded by several speakers representing a variety of dialects, speaking styles,and sentence positions. dynamic programming algorithms are used to evaluatethe best match between the input and the templates. dtw refers to the need totime compress (or expand) the input word in order to make it comparable induration to the stored exemplar. further discussion of this topic can be found indixon and martin (1979) and rabiner (1983).in dtw, both storage and processing times increase, at least linearly, withvocabulary expansion. the system must be trained on every new word in thevocabulary. furthermore, for speakerindependent recognition, an inordinatenumber of templates describing the possible variations need to be recorded andstored. as a result, systems with more than a few hundred words are notpractical. because of these limitations, accuracy considerations, and the need totrain the system on every word, dtw has been replaced by the hmm approachfor the development of highperformance systems.hmms represent speech units (words, syllables, phones, etc.) as stochasticmachines consisting of several (3 to 20) states. with each state is associated aprobability distribution describing the probability of that state's emitting a givenobservation vector. in addition, there is a set of probabilities of moving fromone state to another. control is transferred between states according to theseprobabilities each time a new vector is generated (i.e., at the frame rate of thesystem).for recognition, a solution to the reverse problem is desired. given a set ofpretrained models and a sequence of observation vectors, the model most likelyto have generated the observation must be determined. the utterance associatedwith this model is then the recognizer's output. the parameters of the model, theprobabilities, are determined from labeled training speech data, presumablycontaining realizations of all the modeled utterances. efficient algorithms existfor both the training and recognition tasks. the structure of the model (numberof states, speech unit, etc.) is chosen judiciously and depends on the language aswell as other speech knowledge.when largevocabulary recognition is involved, the usual approach is tomodel speech as a sequence of very basic units such as phones. coarticulatoryphonetic effects are taken into account by having for each phone a differentmodel for every context in which that phone exists (the context is specified bythe neighboring phones). as a result, the number of models that must be trained,stored, and evaluated grows as the number of phonetic contexts increases. bytaking context into account, thespeech, physiology, and other interface components233virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.recognition error rate can be cut in half (chow et al., 1986). in phoneticallybased hmm systems, it is possible to recognize new words by simply includingtheir phonetic pronunciation in the dictionary; there is no need to train thesystem specifically on the new words (although the accuracy on those wordswould be higher if the system is trained on them as well).stochastic systems make use of grammar rules that constrain the sequenceof speech units that can occur. grammars are usually either finitestate orstatistical. finitestate grammars describe explicitly the allowable wordsequences (a word sequence is either allowed or it is not); statistical grammarsallow all word sequences but with different probabilities. a rough measure of agrammar's restrictiveness is its socalled perplexity (bahl et al., 1983). thelower this number, the more predictable the word sequence. it is easy to seehow grammar restrictions can diminish the search that must be performed todetermine the identity and order of words in an utterance. not surprisingly,speech recognition for tasks with low perplexity can be performed much fasterand more reliably than tasks with high perplexity.for some applications, in addition to recognizing the sequence of wordsspoken, it is important to understand what has been said and give an appropriateresponse. for this purpose, the output of the recognizer is sent to a languageunderstanding component, which analyzes and interprets the recognized wordsequence. to allow for possible errors in recognition, the recognitioncomponent sends to the language understanding component not only the topscoring word sequence, but also the n topscoring word sequences (where n istypically 1020). the language understanding component then chooses the wordsequence that makes most sense in that context (schwartz and austin, 1991).state of the art in speech recognitionmore and more, speech recognition technology is making its way from thelaboratory to realworld applications. recently, a qualitative change in the stateof the art has emerged that promises to bring speech recognition capabilitieswithin the reach of anyone with access to a workstation. highaccuracy, realtime, speakerindependent, continuous speech recognition, for mediumsizedvocabularies (few thousand words), is now possible in software on offtheshelfworkstations. users will be able to tailor recognition capabilities to their ownapplications. such softwarebased, realtime solutions usher in a whole new erain the development and utility of speech recognition technology.as is often the case in technology, a paradigm shift occurs when severaldevelopments converge to make a new capability possible. in thespeech, physiology, and other interface components234virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.case of continuous speech recognition, the following advances have convergedto make the new technology possible: higheraccuracy, continuous speech recognition, based on hidden markovmodeling techniques, better recognition search strategies that reduce the time needed for highaccuracy recognition, and increased power of offtheshelf workstations.the paradigm shift is taking place in the way we view and use speechrecognition. rather than being mostly a laboratory endeavor, speech recognitionis fast becoming a technology that is pervasive and will have a profoundinfluence on the way humans communicate with machines and with each other.for a recent survey of the state of the art in continuous speech recognition, seemakhoul and schwartz (1994).using hmms, the word error rate for continuous speech recognition hasbeen dropping steadily over the last decade, with a factor of two drop in errorrate about every two years. research systems are now able to tackle problemswith large vocabularies. for example, in a test using the arpa wall streetjournal continuous speech recognition corpus, word error rates of 11 percenthave been achieved for speakerindependent performance on read speech(pallett et al., 1994). although this performance level may not be sufficient fora practical system today, continuing improvements in performance are likely tomake such systems of practical use in a few years.because of the availability of large amounts of training speech data fromlarge numbers of speakers (hundreds of hours of speech), speakerindependentperformance has reached such levels that it rarely makes sense to train systemson the speech of specific speakers. however, there will always be outlierspeakers for whom, for one reason or another, the system does not performwell. for such speakers, it is possible to collect a relatively small amount ofspeech (on the order of minutes of speech) and then adapt the system's modelsto the outlier speaker to improve performance significantly for that speaker.for information retrieval applications, it is important to understand theuser's query and give an appropriate response. speech understanding systemshave reached the stage at which it is possible to develop a practical system forspecialized applications. the understanding component must be tuned to thespecific application; the work requires significant amounts of data collectionfrom potential users and months of laborintensive work to develop thelanguage understanding component for that application. in the arpa airlinetravel information service (atis) domain, users access flight informationusing verbal queries. speech understanding systems in the atis domain haveachieved understandingspeech, physiology, and other interface components235virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.error rates of less than 10 percent in speakerindependent mode (pallett et al.,1994). in these tests, users speak in a normal spontaneous fashion.until recently, it was thought that to perform highaccuracy, realtimecontinuous speech recognition for large vocabularies would require eitherspecialpurpose very large system integrated (vlsi) hardware or amultiprocessor. however, new developments in search algorithms have sped upthe recognition computation at least two orders of magnitude, with little or noloss in recognition accuracy (schwartz and austin, 1991). in addition,computing advances have achieved two orders of magnitude increase inworkstation speeds in the last decade. these two advances have made softwarebased, realtime, continuous speech recognition a reality. the only requirementis that the workstation must have an analogtodigital converter to digitize thespeech. all the signal processing, feature extraction, and recognition search isthen performed in software in real time on a singleprocessor workstation.for example, it is now possible to perform a 3000word atis recognitiontask in real time on such workstations as the silicon graphics indigo (sgi)r3000 or the sun sparcstation 2. most recently, a 40,000word continuousdictation task was demonstrated in real time on a hewlettpackard 735workstation, which has about three times the power of an sgi r3000. thus, thecomputation grows much slower than linear with the size of the vocabulary(nguyen et al., 1993).the realtime feats just described have been achieved at a relatively smallcost in word accuracy. typically, the word error rates are less than twice thoseof the best research systems.the most advanced of these realtime demonstrations have not as yet madetheir way to the marketplace. however, it is possible today to purchase productsthat perform speakerindependent, continuous speech recognition forvocabularies of a few thousand words. these systems are being used incommand and control applications, the routing of telephone calls by speakingthe full name of the party being called, the training of air traffic controllers, andthe control of workstation applications. of particular significance to the publicwill be the development of transactional applications over the telephone, suchas home shopping and airline reservations. practical largevocabulary,continuousspeech, speakerindependent dictation systems are a few years away.hmms have proven to be very good for modeling variability in time andfeature space and have resulted in tremendous advances in continuous speechrecognition. however, some of the assumptions made by hmms are known notto be strictly true for speechšfor example, the conditional independenceassumptions in which the probability of being in a state is dependent only on theprevious state, and the output probability at a state is dependent only on thatstate and not on previousspeech, physiology, and other interface components236virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.states. there have been attempts at ameliorating the effects of theseassumptions by developing alternative speech models, such as the use ofstochastic segmental models and neural networks. in all these attempts,however, significant computational limitations have hindered the fullexploitation of these methods; in fact, the performance of such alternativemodels have barely approached that of hmm systems. however, when suchmodels are used in conjunction with hmm systems, the resulting hybrids haveachieved word error rate reductions of 1020 percent (makhoul and schwartz,1994).despite all these advances, much remains to be done. speech recognitionperformance for very large vocabularies and larger perplexities is not yetadequate for useful applications, even under benign acoustic conditions. anydegradation in the environment or changes between training and test conditionscauses a degradation in performance. therefore, work must continue to improverobustness to varying conditions: new speakers, new dialects, different channels(microphones, telephone), noisy environments, and new domains andvocabularies. what will be especially needed are improved acoustic models andmethods for fast adaptation to new conditions. many of these research areaswill require more powerful computing sources. fortunately, workstation speedand memory will continue to grow in the years to come. the resulting morepowerful computing environment will facilitate the exploration of moreambitious modeling techniques and will, no doubt, result in additionalsignificant advances in the state of the art.in comparison with speech recognition, the field of languageunderstanding, which is a much harder problem, is still in its infancy. onemajor obstacle for advancement is the lack of a representation of semantics thatis general and powerful enough to cover major applications of interest. andeven if such a representation were available, there is still a strong need todevelop automatic methods for interpreting word sequences, without having torely on the currently dominant methods of laborintensive crafting of detailedlinguistic rules.speech synthesisa speech synthesizer is a device that accepts at its input the text of anutterance in orthographic or computerreadable form and transforms that textinto spoken form. the synthesizer performs much the same function as a humanwho reads a printed text aloud. the synthesizer usually contains a componentthat performs an initial transformation from a written text into a sequence ofphonetic units (e.g., transforming caught to /kot/), and these phonetic units thencontrol the production of sound in the synthesizer.speech, physiology, and other interface components237virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.the synthesizer can generate the simulated speech in one of two ways.one method, called concatenative synthesis, is to create utterances by stringingtogether segments of speech that have been excerpted from the utterances of aspeaker. these segments consist of pieces of syllables, usually a sequence ofconsonant and vowel or a vowel and a consonant. for example, a word like matwould be synthesized by concatenating two pieces ma + at. special proceduresare used to ensure that the joining of the pieces is done smoothly with minimalartifacts. rules are used to provide suitable adjustment of timing and otherprosodic characteristics. an inventory of several hundred of these prerecordedsegments of speech is needed to synthesize most utterances in english. in orderto synthesize speech for different speakers, it is necessary to make newrecordings for each speaker and to edit these recordings to obtain the necessaryinventory of segments.a second method for generating the sound from a phonetic sequence is touse a synthesis device that models some aspects of the human speechgenerating process. this process regards the human speech system as consistingof a set of sound sources that simulate vocal cord vibration or noise due toturbulent airflow, together with a set of resonators that simulate the filtering ofthe sources by the airways that constitute the vocal tract. the control parametersfor this model specify attributes such as the frequency and amplitude of vocalcord vibration and the frequencies of the resonators. depending on thecomplexity of the synthesizer, there can be as many as 40 such controlparameters or as few as 10. this type of synthesizer has been called a formantor terminalanalog synthesizer.in this second type of synthesizer, the rules for converting phonetic unitsinto control parameters specify an array of parameter values for each speechsound and describe how these parameters should move smoothly from onespeech sound to the next. by proper adjustment of the ranges of the parameters,different male and female voices can be synthesized. the best synthesis basedon these formant or articulatory models is somewhat more intelligible thanconcatenative synthesis.for both concatenative and formant synthesis, the generation of naturalsounding utterances requires that rules be developed for controlling thetemporal aspects of the speech and changes in fundamental frequency thatindicate prominent syllables and that delineate groupings of words. the mostsuccessful devices for synthesis of speech from text produce speech withreasonably high intelligibility, although not quite as intelligible as humanproduction of speech, and with some lack of naturalness. continuing research isleading to improvements in naturalness through adjustments of rules for therhythmic and other prosodic aspects of the speech.speech, physiology, and other interface components238virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.physiological responsesin all the previous discussion of humanmachine interfaces, the inputs tothe human (the displays) and the outputs from the human (the controls) haveoccurred at the limits of the human periphery, i.e., they have made use of thenatural input and output mechanisms. in principle, it is possible to constructinterfaces that bypass the periphery and display information by stimulatingneural structures directly or sense physiological variables for purposes ofcontrol. independent of the extent to which such interfaces provide a usefuladjunct to conventional interfaces for normal subjects, they undoubtedly willprove exceedingly important for subjects with certain kinds of sensorimotordisabilities (warner et al., 1994). in this section, we discuss briefly differentkinds of physiological responses that might serve as useful control signals. wehave omitted consideration of neural stimulation because, with only minorexceptions, we believe that most devices for providing such stimulation will beemployed only for individuals with severe sensory disabilities; for suchindividuals, these devices will be permanently implanted and become part of thesubject, thus eliminating the need to include such devices in humanmachineinterfaces for se systems. we have also omitted discussion of the use ofphysiological response measurements (e.g., associated with muscle actions) tohelp individuals with severe motor disabilities control computers and telerobots,because this topic is included in our discussion of the medicine and health careapplication domain in chapter 12.most practical work on physiological responses has been conducted in thelaboratory for purposes of establishing the effects of selected experimentalconditions on an individual's emotional state or for designing systems andsystem tasks that take human capabilities and limitations into consideration. inthe near term, those measures that have been found useful as indicators ofmental, emotional, or physical states in the real world should be equally usefulas indicators of the states in an se. however, apart from use with individualshaving severe motor disabilities, it is likely to be several years beforephysiologicalresponse sensing will be included as a control element in most sesystems. (a possible exception to this statement is suggested by the currentresearch on brainactivated control at armstrong medical research laboratoryby junker et al., 1988.) the following discussion briefly reviews the currentstatus of work in physiological response measurement.measures of physiological responses have been used by researchers todescribe the physical, emotional, and mental condition of human subjects,usually in relation to performance of a specific task, involvement in a particularsocial transaction, or exposure to a particular set of environmental variables.over the years, ergonomic researchers have conductedspeech, physiology, and other interface components239virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.an enormous number of studies employing physiological responses as indicatorsor correlates of fatigue, stress, onset or decline of attention, and level or changein level of mental workload (wierwille, 1979). the physiological responsestraditionally used in such studies focus on involuntary responses controlled bythe autonomic nervous system, such as heart rate, blood pressure, stomachactivity, muscle tension, palmar sweating, and pupil dilation. the techniques formeasuring these responses vary in the ease and intrusiveness of data collection,the statistical properties of the data flow, and the certainty with which analyzeddata can be interpreted.another area of physiological research activity involves studies of brainorganization and cognitive function (druckman and lacey, 1989; kramer,1993; zeffiro, 1993). neurophysiologists have been heavily involved indeveloping and using physiological measures to map sensory and motorfunctions in the brain and to identify patterns of brain activity associated withattentional states and cognitive workload.autonomic nervous system responsesthe autonomic nervous system is composed of two subsystems, thesympathetic and the parasympathetic. the sympathetic nervous system isresponsible for mobilizing the body to meet emergencies; the parasympatheticnervous system is responsible for maintaining the body's resources. these twosystems can either work together or in opposition to one another. thus, anincrease in a physiological response such as heart rate or muscle tension may beinterpreted as an increase in sympathetic activity or as a decrease inparasympathetic activity.physiological responses that are interpreted as sympathetic nervous systemactivities are often cited as indicators of emotional response. these includeheart rate, systolic and diastolic blood pressure, muscle tension, and skinconductance (hassett, 1978). in evaluating these measures, it is important torecognize that the sympathetic nervous system does not respond in a unitaryway; humans often exhibit an increase in one of these responses such aselevated heart rate, without showing an increase in others. thus, it is misleadingto speak of arousal as if it were a unitary response. moreover, there is no simpleonetoone correspondence between any single physiological response and aparticular emotion or cognitive process (cacioppo and tassinary, 1990).although it may be possible in the future to identify patterns of responses thatrelate to specific psychological states, the results to date are mixed. forexample, it appears to be possible to describe the intensity of an emotion but notnecessarily whether it is positive or negative.perhaps the most thorough study of the measurement and use of autonomicnervous system responses has been accomplished by researchersspeech, physiology, and other interface components240virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.interested in identifying physiological indicators of mental workload. wierwille(1979) provided a complete review of 14 physiological measures and theirusefulness. the results of this analysis point to three measures that have somecorrelation with mental workload: catecholamine excretion as determined bybodily fluid analysis, pupil diameter, and evoked cortical potential. althougheach of these was judged to have promise, none is easily measured. in addition,wierwille suggests that they should be used in combination with behavioralmeasures when drawing conclusions about levels or types of mental workload.all physiological measures suffer from some drawback, either in terms ofease of measurement or accuracy of interpretation. one reason it is so difficultto make inferences about possible psychological concomitants of physiologicalprocesses is the complexity of the physiological systems and the sensoryenvironments involved. each physiological process may be internally regulatedby multiple control systems and also may be complicated by individualdifferences (e.g., one person may respond to threat primarily with elevated heartrate, whereas another will respond with elevated blood pressure).heart rate can be assessed using electrocardiogram electrodes attached tothe chest or by attaching a plethysmograph to the finger or earlobe to detectchanges in blood flow. although heart rate is relatively easy to measure, theresults are conflicting. according to a review by hartman (1980), some studiesshow increases in heart rate with mental workload; others show a decrease or nochange in heart rate with increases in workload. wierwille's (1979) resultssuggest that one fairly stable and useful measure is spectrum analysis ofheartbeat intervals.systolic and diastolic blood pressure are usually monitored by using a selfinflating arm cuff system (weber and drayer, 1984). a newer technology fromohmeda uses a finger cuff to track pulse pressure continuously; it provides ameasure of blood pressure and heart rate every two seconds. systolic pressure isthe peak pressure when the blood is being ejected from the left ventricle;diastolic pressure is the pressure between contractions while the ventricle isrelaxing (smith and kampine, 1984). heart rate and systolic blood pressuretend to show significant increases when subjects are shown emotionallyarousing films, asked to carry out demanding tasks (such as mental arithmetic),or are placed in embarrassing or threatening situations (krantz and manuck,1984). there are many other cardiovascular variables that can be assessed (e.g.,pulse transit time, forearm blood flowšsmith and kampine, 1984). dependingon the precise nature of the research question, additional measures may beneeded in order to identify the underlying mechanisms that have produced anyobserved changes in blood pressure or heart rate. for example, an increase inblood pressure could be due to an increase in cardiacspeech, physiology, and other interface components241virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.output (either heart rate or stroke volume), or it could be due to an increase inperipheral resistance (constriction of blood vessels or capillaries). knowingwhich of these has occurred may in turn point to the neurotransmitters likely tohave been active.monitoring of muscle tension (electromyographyšemg) involvesattaching two or more electrodes over the surface of the target muscle to detectnaturally occurring bioelectric potentials. large skeletal muscles such as thetrapezius (located in the back of neck) or frontalis (forehead) may be monitoredas a way of assessing muscle tension, or facial expression. even changes toofleeting or slight to be observed by a human judge can be detected by placingelectrodes near key facial muscles (hassett, 1978; cacioppo and petty, 1983).the bioelectric potentials being detected are very small in magnitude and therange of frequencies includes 60 hz. to ensure clean recording of emg,isolation from electrical fields is important. this recording procedure isrelatively straightforward. muscle tension may be an indicator of alertness;however, it also changes as a function of fatigue, level, the individual's physicalcondition, and the physical demands of a task.emg signals have been employed for the control of powered prosthesessuch as the utah artificial arm (jacobsen et al., 1982). the basis for thiscontrol is detailed models of the muscle biomechanics that are used to predictthe proper joint torques from the emgs (meek et al., 1990). given the successin prosthetics, it is therefore natural to consider that emgs could be used tocontrol robotic devices. hiraiwa et al. (1992) used emgs to control fingerposition, torque, and motion of an artificial robot hand. control was achievedwith a neural net, trained by using a vpl dataglove; accuracies were quitemodest. farry and walker (1993) are working on emg control of the utah/mitdextrous hand master; various emg processing methods are being tested usingan exos dextrous hand master to predict basic grasp types. they also presenta concise uptodate review of emg signal processing and the use of emgs inthe control of prostheses. the use of emgs to control telerobots or computers isclearly less appropriate for normal humans than for humans with severe motordisabilities. they are used for persons with disabilities (e.g., to controlprosthetics) only because better choices do not exist.eda (electrodermal activity, also known as gsr, galvanic skin reflex) isassessed by attaching a pair of gold cup electrodes to the skin (usually on thehand), passing a very small electrical current between them, and measuring theresistance. as palmar sweating increases, the conductance increases (hassett,1978). in response to stimuli, people typically show a transient change in eda;the number and latency of peaks in eda are often measured as a way ofassessing stimulus response. according to wierwille (1979), stress will causedecreases in skin conductance; however,speech, physiology, and other interface components242virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.extensive time averaging is required to demonstrate a change. moreover, skinresponse changes as a function of several variables including ambienttemperature, humidity, physical exertion, and individual differences in bodymetabolism.research on pupil dilation has shown a change in pupil size as mentalworkload increases: during high levels of workload the pupil dilates, but whenthe operator becomes overloaded, pupil size is reduced. pupil dilation can berecorded by a motion picture or video camera; analysis is accomplished bymeasuring each frame through manual or automated techniques. the evaluationof pupillary response is complicated by the fact that pupil size changes with thelevel of ambient illumination, fatigue, and acceleration.brain activityin recent years, many researchers have made significant contributions tothe measurement of brain activity (kramer, 1993; zeffiro, 1993; druckman andlacey, 1989). four technologies are of particular interest: event relatedpotentials (erp), positron emission tomography (pet), magnetic resonanceimaging (mri), and magnetic stimulation mapping.cognitive functioneventrelated potentials are found on the electroencephalographic record.they reflect brain activities that occur in response to discrete events or tasks.druckman and lacey (1989) report several important research effortsinvolving erps. for example, a negative voltage with a stimulusresponselatency of 100 ms (n100) has been shown to be related to the individual's focusof attention. this finding suggests that n100 might be used to determinewhether an individual is following task instructions or has shifted attention tosome other aspect of the environment. a negative voltage with a latency of 200ms (n200) has been found to be related to a mismatch between an individual'sexpectations and events occurring in the environment. a positive voltagehaving a 300 ms latency (p300) has been associated with mental workload(donchin et al., 1986; kramer, 1993) and analysis of memory mechanisms(neville et al., 1986; karis et al., 1984). kramer (1993) reports the extensiveuse of p300 in measuring both primary and secondary mental task load. aprincipal finding across studies is that the p300 elicited by discrete secondarytasks decreases as the difficulty of the primary task increases. although noformal reliability studies have been conducted, there is a substantial body ofevidence that suggests that erps are reliable measures of mental workload inthe laboratory.speech, physiology, and other interface components243virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.the next step is to begin experimentation in field environments (druckman andlacey, 1989; kramer, 1993).another important aspect of the erp is the readiness potential that ismanifested as a slow negative wave preceding a voluntary response. this wavehas been labeled the contingent negative wave (cnv) by walter et al. (1964). itis one of a class of events preceding negative waves that have been shown torelate to a subject's mental preparatory processes. there is some evidence thatthe size of the negative potential and its location by hemisphere can provideevidence regarding the response a subject is thinking about independent of theresponse that is actually made (coles et al., 1985).it is important to note that the time lag associated with obtaining reliableestimates of erps for cognitive functions may take several seconds. this timedelay limits the potential of erps for ve applications requiring realtime,closedloop control.functional and structural informationfunctional and structural information about the brain can be obtained by acombination of methods, including magnetic stimulation mapping, positronemission tomography, and magnetic resonance imaging. magnetic stimulationmapping is accomplished by exciting a magnetic coil over the surface of thehead. this induces a magnetic field that stimulates the underlying brain. whenthe coil is placed over one of the motor areas in the brain and a brief burst ofpulses is initiated, some of the nerve cells in that area will discharge and thepart of the body controlled by those cells will move. magnetic stimulationmapping is also used to stimulate peripheral nerve cells, such as those in theback or limbs for both clinical and research purposes. the number of cellsactivated by this technique is unclear.pet is an imaging technique that portrays the flow of radioactivesubstances throughout the body. it is based on a combination of principles fromcomputer tomography and radio isotope imaging (martin et al., 1991).researchers at the national institutes of health use pet to map the motorcortex. they examine blood flow data in the brain from a pet scan as subjectsmove different parts of their bodies. areas of high blood flow are matched withthe body part being moved; the increase in blood flow has been shown to beproportional to the rate of movement or degree of contraction. although petprovides accurate information about the function of a particular area of thebrain, it does not present particularly useful anatomical information. as a result,mri, which does provide anatomical information, has been used in conjunctionwith petšthe images from both techniques can be taken simultaneously forthe same subjectspeech, physiology, and other interface components244virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.and then spatially registered with one another. this provides a pattern that linksstructure and function.recently a procedure has been developed at massachusetts generalhospital to use mri to collect functional data by imaging oxidation status inresponse to stimulation and motor responses. this is a completely noninvasiveapproach to mapping the cortex, and it is much faster than pet. functionalmri data have been collected that show changes in oxidation status in thepremotor cortex and somatosensory cortex when a subject thinks about makinga movement. during actual movement, both of these plus the primary motorcortex are activated. another promising process, magnetic resonanceangiography (mra), images blood vessels and can be used to measure bloodflow through the brain (martin et al., 1991).brainactuated controlfinally, there has been some interesting work conducted at armstrongmedical research laboratory (junker et al., 1988, 1989) on brainactuatedcontrol of a roll axis tracking simulator. the focus of the work has been onproviding closedloop feedback to subjects that allows them to learn to controlthe resonance portion of the eeg and then to use these resonances to sendcontrol signals to a tracking simulator. essentially, the studies have shown thatsubjects have been able to accurately roll to the left or right in the simulator bycontrolling brain resonance signals. although this research is in early stages, itdoes appear to hold some promise for training individuals to use physiologicalresponses as simple control signals (e.g., onoff, leftright, etc).recently this work on brainactuated control has extended to its use forrehabilitation (calhoun et al., 1993). these authors believe that, by employingappropriate evoking stimuli and feedback, people can learn to use eegresponses to control wheelchairs, computers, and prosthetics. furthermore, theyconclude that brainactuated control may be better than most existing interfacesfor people with disabilities.other interface componentsother interface components that could be of value concern stimulation ofthe olfactory (smell) or gustatory (taste) channels, and diffuse sensations relatedto the sensing of heat, air currents, humidity, and chemical pollutants in the airthat affect the eyes or skin surface. in some cases, stimulation can have directfunctional significance for the specified task; in others, it may serve merely toincrease the general sense of presence or immersion in the environment.speech, physiology, and other interface components245virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.in one research and development project aimed at the training offirefighters, the traditional helmetmounted display of visual and auditorystimuli is being extended to include the display of odor and radiant heat stimuli(cater et al., 1994). the current version of this experimental system, whichmakes use of a 44 lb backpack for housing the special additional equipmentrequired, includes both a computercontrolled odor delivery system and acomputercontrolled radiant heat delivery system. the odor system deliversodors by blowing air across microcapsules crushed by pinch rollers; the radiantheat system provides directional as well as overall intensity characteristics bycontrolling the individual intensity of infrared lamps arranged in a circular array.the visual display component of the system is driven by video outputs oftwo silicon graphics indigos (one for each eye); polhemus fastrak positioninformation is processed by a pc and communicated to one of the indigos viaan rs232 serial bus; another indigo serial bus controls the odor deliverysubsystem; a third serial connection on the second indigo drives a midi bus tocontrol the radiant heat subsystem; and an ethernet connection between the twoindigos is used for synchronization purposes and for sharing peripheralinformation.the demonstration program pyro makes use of two virtual human armsand hands, polhemus trackers attached to the user's wrists, a virtual torch, andsome virtual flammable spheres. the user's task in this demonstration is to lighteach of the spheres with the torch.among the problems encountered in this work are the visualization of theflame, gumming up of the pinch rollers used to crush the odor microcapsules,and the weight of the backpack.speech, physiology, and other interface components246virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.8computer hardware and software for thegeneration of virtual environmentsthe computer technology that allows us to develop threedimensionalvirtual environments (ves) consists of both hardware and software. the currentpopular, technical, and scientific interest in ves is inspired, in large part, by theadvent and availability of increasingly powerful and affordable visuallyoriented, interactive, graphical display systems and techniques. graphical imagegeneration and display capabilities that were not previously widely available arenow found on the desktops of many professionals and are finding their way intothe home. the greater affordability and availability of these systems, coupledwith more capable, singlepersonoriented viewing and control devices (e.g.,headmounted displays and handcontrollers) and an increased orientationtoward realtime interaction, have made these systems both more capable ofbeing individualized and more appealing to individuals.limiting ve technology to primarily visual interactions, however, simplydefines the technology as a more personal and affordable variant of classicalmilitary and commercial graphical simulation technology. a much moreinteresting, and potentially useful, way to view ves is as a significant subset ofmultimodal user interfaces. multimodal user interfaces are simply humanmachine interfaces that actively or purposefully use interaction and displaytechniques in multiple sensory modalities (e.g., visual, haptic, and auditory). inthis sense, ves can be viewed as multimodal user interfaces that are interactiveand spatially oriented. the humanmachine interface hardware that includesvisual and auditory displays as well as tracking and haptic interface devices iscovered in chapterscomputer hardware and software for the generation of virtualenvironments247virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.3, 4, and 5. in this chapter, we focus on the computer technology for thegeneration of ves.one possible organization of the computer technology for ves is todecompose it into functional blocks. in figure 81, three distinct classes ofblocks are shown: (1) rendering hardware and software for driving modalityspecific display devices; (2) hardware and software for modalityspecificaspects of models and the generation of corresponding display representations;(3) the core hardware and software in which modalityindependent aspects ofmodels as well as consistency and registration among multimodal models aretaken into consideration. beginning from left to right, human sensorimotorsystems, such as eyes, ears, touch, and speech, are connected to the computerthrough humanmachine interface devices. these devices generate output to, orreceive input from, the human as a function of sensory modal drivers orrenderers. the auditory display driver, for example, generates an appropriatewaveform based on an acoustic simulation of the ve. to generate the sensoryoutput, a computer must simulate the ve for that particular sensory mode. forexample, a haptic display may require a physical simulation that includesfigure 81 organization of the computer technology for virtual reality.computer hardware and software for the generation of virtualenvironments248virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.compliance and texture. an acoustic display may require sound models basedon impact, vibration, friction, fluid flow, etc. each sensory modality requires asimulation tailored to its particular output. next, a unified representation isnecessary to coordinate individual sensory models and to synchronize output foreach sensory driver. this representation must account for all human participantsin the ve, as well as all autonomous internal entities. finally, gathered andcomputed information must be summarized and broadcast over the network inorder to maintain a consistent distributed simulated environment.to date much of the design emphasis in ve systems has been dictated bythe constraints imposed by generating the visual scene. the nonvisualmodalities have been relegated to specialpurpose peripheral devices. similarly,this chapter is primarily concerned with the visual domain, and material onother modalities can be found in chapters 37. however, many of the issuesinvolved in the modeling and generation of acoustic and haptic images aresimilar to the visual domain; the implementation requirements for interacting,navigating, and communicating in a virtual world are common to all modalities.such multimodal issues will no doubt tend to be merged into a more unitarycomputational system as the technology advances over time.in this section, we focus on the computer technology for the generation ofves. the computer hardware used to develop threedimensional ves includeshighperformance workstations with special components for multisensorydisplays, parallel processors for the rapid computation of world models, andhighspeed computer networks for transferring information among participantsin the ve. the implementation of the virtual world is accomplished withsoftware for interaction, navigation, modeling (geometric, physical, andbehavioral), communication, and hypermedia integration. control devices andheadmounted displays are covered elsewhere in this report.ve requires high frame rates and fast response because of its inherentlyinteractive nature. the concept of frame rate comes from motion picturetechnology. in a motion picture presentation, each frame is really a stillphotograph. if a new photograph replaces the older images in quick succession,the illusion of motion in engendered. the update rate is defined to be the rate atwhich display changes are made and shown on the screen. in keeping with theoriginal motion picture technology, the ideal update rate is 20 frames (newpictures) per second or higher. the minimum acceptable rate for ve is lower,reflecting the tradeoffs between cost and such tolerances.with regard to computer hardware, there are several senses of frame rate:they are roughly classified as graphical, computational, and data access.graphical frame rates are critical in order to sustain the illusion of presencecomputer hardware and software for the generation of virtualenvironments249virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.or immersion in a ve. note that these frame rates may be independent: thegraphical scene may change without a new computation and data access due tothe motion of the user's point of view. experience has shown that, whereas thegraphical frame rate should be as high as possible, frame rates of lower than 10frames per second severely degrade the illusion of presence. if the graphicsbeing displayed relies on computation or data access, then computation and dataaccess frame rates of 8 to 10 frames per second are necessary to sustain thevisual illusion that the user is watching the time evolution of the ve.fast response times are required if the application allows interactivecontrol. it is well known (sheridan and ferrell, 1974) that long response times(also called lag or pure delay) severely degrade user performance. these delaysarise in the computer system from such factors as data access time, computationtime, and rendering time, as well as from delays in processing data from theinput devices. as in the case of frame rates, the sources of delay are classifiedinto data access, computation, and graphical categories. although delays areclearly related to frame rates, they are not the same: a system may have a highframe rate, but the image being displayed or the computational result beingpresented may be several frames old. research has shown that delays of longerthan a few milliseconds can measurably impact user performance, whereasdelays of longer than a tenth of a second can have a severe impact. the framerate and delay required to create a measurable impact will in general depend onthe nature of the environment. relatively static environments with slowlymoving objects are usable with frame rates as low as 8 to 10 per s and delays ofup to 0.1 s. environments with objects exhibiting high frequencies of motion(such as a virtual handball game) will require very high frame rates (> 60 hz)and very short delays. in all cases, however, if the frame rate falls below 8frames per s, the sense of an animated threedimensional environment begins tofail, and if delays become greater than 0.1 s, manipulation of the environmentbecomes very difficult. we summarize these results to the following constraintson the performance of a ve system:frame rates must be greater than 8 to 10 frames/s.total delay must be less than 0.1 s.both the graphics animation and the reaction of the environment to useractions require extensive data management, computation, graphics, and networkresources. all operations that take place to support the environment mustoperate within the above time constraints. although one can imagine a systemthat would have the graphics, computation, and communications capability tohandle all environments, such a system is beyond current technology. for a longtime to come, the technology necessarycomputer hardware and software for the generation of virtualenvironments250virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.will generally be dependent on the application domain for which the ve isbeing built. realworld simulation applications will be highly bound by thegraphics and network protocols and by consistency issues; informationvisualization and scientific visualization applications will be bound by thecomputational performance and will involve issues of massive datamanagement (bryson and levit, 1992; ellis et al., 1991). some applications,such as architectural visualization, will require photorealistic rendering; others,such as information display, will not. thus the particular hardware and softwarerequired for ve implementation will depend on the application domaintargeted. there are some commonalities of hardware and softwarerequirements, and it is those commonalities on which we focus in ourexamination of the state of the art of computer hardware and software for theconstruction of realtime, threedimensional virtual environments.hardware for computer graphicsthe ubiquity of computer graphics workstations capable of realtime, threedimensional display at high frame rates is probably the key development behindthe current push for ves today. we have had flight simulators with significantgraphics capability for years, but they have been expensive and not widelyavailable. even worse, they have not been readily programmable. flightsimulators are generally constructed with a specific purpose in mind, such asproviding training for a particular military plane. such simulators aremicrocoded and programmed in assembly language to reduce the total numberof graphics and central processing unit cycles required. systems programmed inthis manner are difficult to change and maintain. hardware upgrades for suchsystems are usually major undertakings with a small customer base. an evenlarger problem is that the software and hardware developed for such systems aregenerally proprietary, thus limiting the availability of the technology. thegraphics workstation in the last 5 years has begun to supplant the specialpurpose hardware of the flight simulator, and it has provided an entry pathwayto the large numbers of people interested in developing threedimensional ves.the following section is a survey of computer graphics workstations andgraphics hardware that are part of the ve development effort.notable graphics workstations and graphics hardwaregraphics performance is difficult to measure because of the widely varyingcomplexity of visual scenes and the different hardware and software approachesto computing and displaying visual imagery. the mostcomputer hardware and software for the generation of virtualenvironments251virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.straightforward measure is given in terms of polygons/second, but this onlygives a crude indication of the scene complexity that can be displayed at usefulinteractive update rates. polygons are the most common building blocks forcreating a graphic image. it has been said that visual reality is 80 millionpolygons per picture (catmull et al., 1984). if we wish photorealistic ves at 10frames/s, this translates into 800 million polygons/s. there is no currentgraphics hardware that provides this, so we must make approximations at themoment. this means living with less detailed virtual worlds, perhaps viajudicious use of hierarchical data structures (see the software section below) oroffloading some of the graphics requirements by utilizing available cpuresources instead.for the foreseeable future, multiple processor workstations will be playinga role in offloading graphics processing. moreover, the world modelingcomponents, the communications components, and the other softwarecomponents for creating virtual worlds also require significant cpu capacity.while we focus on graphics initially, it is important to note that it is the wayworld modeling effects picture change that is of ultimate importance.graphics architectures for ve renderingthis section describes the highlevel computer architecture issues thatdetermine the applicability of a graphics system to ve rendering. twoassumptions are made about the systems included in our discussion. first, theyuse a zbuffer (or depth buffer), for hidden surface elimination. a zbuffer storesthe depthšor distance from the eye pointšof the closest surface ''seen" at thatpixel. when a new surface is scan converted, the depth at each pixel iscomputed. if the new depth at a given pixel is closer to the eye point than thedepth currently stored in the zbuffer at that pixel, then the new depth andintensity information are written into both the zbuffer and the frame buffer.otherwise, the new information is discarded and the next pixel is examined. inthis way, nearer objects always overwrite more distant objects, and when everyobject has been scan converted, all surfaces have been correctly ordered indepth. the second assumption for these graphic systems is that they use anapplicationprogrammable, generalpurpose processor to cull the database. theresult is to provide the rendering hardware with only the graphics primitivesthat are within the viewing volume (a perspective pyramid or parallel piped forperspective and parallel projections respectively). both of these assumptions arevalid for commercial graphics workstations and for the systems that have beendesigned by researchers at the university of north carolina at chapel hill.the rendering operation is composed of three stages: perprimitive,computer hardware and software for the generation of virtualenvironments252virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.rasterization, and perfragment (as shown in figure 82). perprimitiveoperations are those that are performed on the points, lines, and triangles thatare presented to the rendering system. these include transformation of verticesfrom object coordinates to world, eye, view volume, and eventually to windowcoordinates, lighting calculations at each vertex, and clipping to the visibleviewing volume. rasterization is the process of converting the windowcoordinate primitives to fragments corresponding to the pixels held in the framebuffer. the frame buffer is a dedicated block of memory that holds intensity andother information for every pixel on the display surface. the frame buffer isscanned repeatedly by the display hardware to generate visual imagery. each ofthe fragments includes x and y window coordinates, a color, and a depth for usewith the zbuffer for hidden surface elimination. finally, perfragmentoperations include comparing the fragment's depth value to the value stored inthe zbuffer and, if the comparison is successful, replacing the color and depthvalues in the frame buffer with the fragment's values.figure 82 the graphics pipeline.the performance demanded of such a system can be substantial: 1 milliontriangles per second or hundreds of millions of fragments per second. thecalculations involved in performing this work easily require billions ofoperations per second. since none of today's fastest general purpose processorscan satisfy these demands, all modern highperformance graphics systems arerun on parallel architectures. figure 83 is a general representation of a parallelarchitecture, in which the rendering operation of figure 82 is simply replicated.whereas such an architecture is attractively simple to implement, it fails tosolve the rendering problem, because primitives in object coordinates cannot beeasily separated into groups corresponding to different subregions of the framebuffer. there is in general a manytomany mapping between the primitives inobject coordinates and the partitions of the frame buffer.to allow for this manytomany mapping, disjoint parallel rendering pipesmust be combined at a minimum of one point along their paths, and this pointmust come after the perprimitive operations are completed. the point orcrossbar can be located prior to the rasterization (the primitive crossbar),between rasterization and perfragment (the fragmentcomputer hardware and software for the generation of virtualenvironments253virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.crossbar), and following pixel merge (the pixel merge crossbar). a detaileddiscussion of these architectures is provided in the technical appendix to thischapter. there are four major graphics systems that represent differentarchitectures based on crossbar location. silicon graphics realityengine is aflowthrough architecture with a primitive crossbar; the freedom series fromevans & sutherland is a flowthrough architecture with a fragment crossbar;pixel planes 5 uses a tiled primitive crossbar; and pixelflow is a tiled, pixelmerge machine.figure 83 parallel graphics pipelines.ordered rendering has been presented to help clarify a significantdistinction in graphics architectures; however, it is not the only significantfactor for ve rendering. other primary issues for ve rendering are imagequality, performance, and latency. measured by these metrics, realityengineand pixelflow are very effective ve machines architecturally. freedom andpixel planes 5 are less suitable, though still useful.computation and data management issues in visual scenegenerationmany important applications of ve require extensive computational anddata management capabilities. the computations and data in the applicationprimarily support the tasks taking place in the application. for example, insimulation, the computations may support the physical behavior of objects inthe ve, while in a visualization application the computations may support theextraction of interesting features from a complex precomputed dataset. suchcomputations may require on the order of millions of floating point operations.simulations currently demandcomputer hardware and software for the generation of virtualenvironments254virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.only modest data management capabilities but, as the complexity of simulationsincreases, the data supporting them may increase. visualization applications, incontrast, often demand a priori unpredictable access to gigabytes of data(bryson and geraldyamasaki, 1992). other types of applications can havesimilar demands. as computer power increases, more ambitious computationaldemands will be made. for example, an application may someday compute afluid flow solution in real time to high accuracy. such computations can requiretrillions of floating point operations.an example: the virtual wind tunnelin this section, we consider the implications of the ve performanceconstraints on the computation and data management requirements of a vesystem. an example of an application that is both computationally intensive andworks with large numbers of data is the virtual wind tunnel (bryson and geraldyamasaki, 1992). a modest modern problem in the virtual wind tunnel is thevisualization of a precomputed dataset that gives five values (one for energy,one for density, and three for the velocity vector) at 3 million points at a time,for 106 times. this dataset is a total of 5.3 gbytes in size, with each time stepbeing about 50 mbytes. if the virtual wind tunnel is to allow the user tointeractively control the timevarying visualization of this dataset, each timestep must be loaded, and the visualizations must be computed. assuming that10 time steps must be loaded per second, a data bandwidth of 500 mbytes persecond is required. the computations involved depend on the visualizationtechnique. for example, the velocity vector field can be visualized by releasingsimulated particles into the flow, which implies a computation requiring about200 floating point operations per particle per time step. a typical visualizationrequires thousands of such particles and hundreds of thousands of floating pointoperations. the computation problem expands further as such visualizations arecombined with other computationally intensive visualization techniques, suchas the display of isosurfaces. it is important to stress that this example is only ofmodest size, with the size and complexity of datasets doubling every year or so.it is quite difficult to simultaneously meet the ve performance constraintsand the data management requirements in the above example. there are twoaspects to the data management problem: (1) the time required to find the datain a mass storage device (seek time), which results in delays, and (2) the timerequired to read the data (bandwidth). the seek time can range from minutes inthe case of data stored on tape through a few hundred thousandths of a secondin the case of data stored on disk, to essentially nothing for data stored inprimary memory. bandwidthscomputer hardware and software for the generation of virtualenvironments255virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.range from a few megabytes per second in the case of tapes and disk to on theorder of a hundred megabytes per second for raid disks and physical memory.disk bandwidth is not expected to improve significantly over the next few years.support is needed to meet the requirements of ve applications for realtime random access to as much as several gigabytes (bryson and geraldyamasaki, 1992). whereas for some visualization techniques, only a smallnumber of data will be addressed at a time, a very large number of suchaccesses may be required for data that are scattered over the file on disk. thusthe seek time of the disk head becomes an important issue. for othervisualization techniques (such as isosurfaces or volume rendering), many tensof megabytes of data may be needed for a single computation. this implies diskbandwidths of 300 to 500 mbytes/s in order to maintain a 10 hz update rate, anorder of magnitude beyond current commercial systems. for these types ofapplications, physical memory is the only viable storage medium for data usedin the environment. workstations are currently being released with as much as16 gbytes of memory, but the costs of such large amounts of memory arecurrently prohibitive. furthermore, as computational science grows through theincrease in supercomputer power, datasets will dramatically increase in size.another source of large datasets will be the earth observing satellite, whichwill produce datasets in the terabyte range. this large number of data mandatesvery fast massive storage devices as a necessary technology for the applicationof ves to these problems.strategies for meeting requirementsone strategy of meeting the data management requirements is to observethat, typically, only a small fraction of the data is actually used in anapplication. in the above particle injection example, only 16 accesses arerequired (with each access loading a few tens of bytes) per particle per timestep. these accesses are scattered across the dataset in unpredictable ways. thebandwidth requirements of this example are trivial if only the data actually usedare loaded, but the seek time requirements are a problem: 20,000 particleswould require 320,000 seeks per time step or 3.2 million seeks per second. thisis two orders of magnitude beyond the seek time capabilities of current disksystems.another way to address the data size problem is to develop datacompression algorithms. the data will be decompressed as they are used,trading off reduced data size for greater computational demands. differentapplication domains will make different demands of compression algorithms:image data allow "lossy" compression, in which the decompressed data will beof a slightly lower fidelity than the original; scientificcomputer hardware and software for the generation of virtualenvironments256virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.data cannot allow lossy compression (as this would introduce incorrect artifactsinto the data) but will perhaps allow multiresolution compression algorithms,such as wavelet techniques. the development of appropriate data compressiontechniques for many application domains is an open area of research.another strategy is to put as much of the dataset as possible in physicalmemory. this minimizes the seek time but restricts the number of data that maybe investigated. this restriction will be relieved as workstation memoriesincrease (see figure 84). datasets, however, are expected to grow radically asthe available computational power increases.computational requirements can be similarly difficult to meet. the aboveexample of injecting 20,000 particles into a flow requires 4 million floatingpoint operations, implying a computational performance of 40 million floatingpoint operations per second (or 40 mflops) just to compute the particlevisualization. such an application will often use several such visualizationssimultaneously. as more computational power becomes available, we may wishto include partial differential equation solvers, increasing the computationalrequirements by several orders of magnitude.there are many ways in which supercomputer systems have attained veryhigh computational speeds, but these methods typically work only for specialcomputations. for example, cray supercomputers rely on a vectorizedarchitecture, which is very fast for arraytype operations but isfigure 84 history of workstation computation and memory.computer hardware and software for the generation of virtualenvironments257virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.not so fast as for the particle example discussed above. another example is themassively parallel system, which distributes memory and computation amongmany processors. massively parallel systems are very fast for someapplications, but are slow for computations that are not parallelizable or requirelarge amounts of data movement. in a ve system, many kinds of computationsmay be required, implying that a unique computational architecture typicallywill be unsuitable. to maximize versatility, computations in ve systems shouldbe based on a few parallel highpower scalar processors with large sharedmemory.as figure 84 shows, workstation computational power is increasingdramatically. it is expected that in 1994 workstations will be available that willmatch the computational power of the supercomputers of 1992.the runtime software architecture of the ve is an area of serious concern.there are two runtime models that are currently common in computergraphics: the simulation loop model, in which all operations in the visualizationenvironment (including interaction, computation, data management, andgraphics) are performed in a repeated single loop; and the eventdriven model,in which operations occur in response to various events (usually generated bythe user). neither model is attractive for large ves.the time required for a single loop in the simulation loop model may, dueto the combination of data management, computation, and graphics, exceed theve performance constraints. this is a particularly severe problem if thesevarious operations are performed in sequence, drawing each frame only afterthe entire computation has been completed. this can lead to very low framerates both with respect to display and interaction, which is unacceptable in a vesystem. for multiprocessing systems, one answer is to put the computation anddata management in one process while the graphics is in another,asynchronously running process. then the graphics can be performed as fast aspossible even though the computations may take much longer times. formultiprocessor systems, the computation can be parallelized as well, in whichall computation takes place on as many processors as possible to reduce theoverall time required for a computation. this parallel implementation of thecomputation is still a single loop. the time needed for execution will bedetermined by the slowest computation in that loop.the eventdriven model is unsuited for ve, as there are many events thatmay be generated at any one time (including repeated "compute theenvironment" events that amount to an effective simulation loop), and the timeordering and priority of these events are critical. for example, several userinteraction events may occur simultaneously and the priority and meaning ofthese events will depend on their relationship to one another and theirenvironment. put more succinctly, the meaning of thecomputer hardware and software for the generation of virtualenvironments258virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.events will be contextsensitive and will require the system to interpret the stateof the user. this operation will be difficult to do on the basis of an event queue.an alternative runtime model that is gaining popularity is the concurrentmodel, in which different operations in the environment are runningsimultaneously with one another, preferably on several processors. the exampleof the simulation loop broken into the two asynchronously running graphics andcomputation processes discussed above is a simple example of concurrency. infull concurrency, one may assign a process to each element of the ve. theseprocesses should be implemented as threads or lightweight processes, which areregularly preempted to prevent a single process from taking too much time.each process would be a small simulation loop, which repeatedly computes anddraws its object. the concurrent model has the advantage that slow processeswill not block down faster processes. it has the disadvantage that processesrequiring very different time scales (fast streamlines versus slow isosurfaces ina visualization application, for example) will not always be in sync. this is aserious problem for timedependent environments, in which a concurrentimplementation may lead to the simultaneous display of, for example, thestreamline from one time and the isosurface from another. one can constrainthe various processes to stay in sync, but the result would be an environment inwhich all processes are executed in a time determined by the slowest process (ineffect, a parallelized simulation loop).the choice of runtime architecture will be closely tied to and constrainedby the operating system of the computer platform running the ve. in order toallow the parallelization of graphics and computation described above, theoperating system should support many lightweight, sharedmemory processes,thus minimizing the time required for context switching and interprocesscommunication. the operating system should be capable of ensuring that highpriority processes (such as the processes handling user tracking) can be servicedat very short and regular intervals. in addition, a synchronous process capabilitycould be provided for various types of simulation computations. a furthercapability of operating systems that would significantly facilitate thedevelopment of ve applications is facilities for timecritical computing andrendering. while it is probably unreasonable to ask the operating system toschedule the timecritical tasks by itself, these facilities should provide theability for the developer to determine scheduling through tunable parameters.looking farther into the future, we expect that distributed ve applications willbecome common. developing operating systems that make such distributiontransparent and easy to implement then becomes high priority.computer hardware and software for the generation of virtualenvironments259virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.another strategy to meet the computation and data managementrequirements is to distribute the computation and data management to severalmachines. there are several possible models for such a system. one is to keepall data and perform all computations on a remote supercomputer (bryson andgeraldyamasaki, 1992). this approach is motivated when the localworkstation does not have a large amount of computing power or largememory. another approach is to distribute the data and computations amongseveral computers. in the virtual wind tunnel example, there would be a densitymachine, which would contain the density data and handle all visualizations ofthe density field, a velocity machine, which would contain the velocity vectordata and handle all visualizations of the velocity vector field, and so on. theresulting visualizations would be collected by the workstation that is handlingthe graphics and driving the ve interface. these distributed architectures wouldrequire fast lowlatency networks of the type discussed elsewhere in thisdocument.there are many occasions on which the computations required to supportthe ve cannot be done to full accuracy within the ve speed performanceconstraints. the tradeoff between accuracy and speed is a common theme inthe design of ve systems. there are occasions in which faster, less accuratecomputational algorithms are desirable over slower, more accurate algorithms.it is not known at this time how to design these tradeoffs into a system in away that can anticipate all possibilities. research into how these tradeoffs aremade is therefore needed. a current strategy is to give users full control overthese tradeoffs. a related issue is that of timecritical computing, in which acomputation returns within a guaranteed time. designing timecriticalcomputational architectures is an active area of research and is critical to thesuccessful design of ve applications.extrapolating current trends, we expect that ve applications will saturateavailable computing power and data management capabilities for the indefinitefuture. dataset size will be the dominant problem for an important class ofapplications in ve. in the near term, an effective ve platform would includethe following: multiple fast processors in an integrated unit; several graphicspipelines integrated with the processors; very large shared physical memory;very fast access to mass storage; operating systems that support sharedmemory, multiprocessor architectures; and very highspeed, lowlatencynetworks.graphics capabilities in pcbased ve systemssmall ve systems have been successfully built around highend personalcomputers (pcs) with specialpurpose graphics boards. notablecomputer hardware and software for the generation of virtualenvironments260virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.examples are the w industries system from england, which uses an amigacomputer controlling auxiliary graphics processors. this system is capable ofrendering several hundred polygons at about 15 hz, and is used extensively inthe virtuality video arcade ve games. the virtuality systems are networkedand allow a few participants to play together in the same environment. anothercommon example is the use of an ibmcompatible personal computer with theintel dvi graphics board, which is capable of rendering a few hundred texturedpolygons at 1520 hz.pcbased ve systems are a natural consequence of the widespreadavailability of pcs. pcbased systems will provide the public with a taste ofvirtual reality that will eventually lead to demand for more capablecomputational and graphics platforms. it is anticipated that, by 1996, systemssimilar to the entrylevel indy machines from silicon graphics should replacethe pcbased platforms as the total price of the pc system becomes comparableto that of the indy. already there are signs that computer graphics workstationcompanies are developing risc cpus with ibm pc compatibility nodes tosimplify this transition.software for the generation of threedimensional virtual environmentsthere are many components to the software required for the realtimegeneration of ves. these include interaction software, navigation software,polygon flow minimization to the graphics pipeline software, world modelingsoftware (geometric, physical, and behavioral), and hypermedia integrationsoftware. each of these components is large in its own right, and all of themmust act in consort and in real time to create ves. the goal of theinterconnectedness of these components is a fully detailed, fully interactive,seamless ve. seamless means that we can drive a vehicle across a terrain, stopin front of a building, get out of the vehicle, enter the building on foot, go up thestairs, enter a room and interact with items on a desktop, all without delay orhesitation in the system. to build seamless systems, substantial progress insoftware development is required. the following sections describe the softwarebeing constructed in support of virtual worlds.interaction softwareinteraction software provides the mechanism to construct a dialogue fromvarious control devices (e.g., trackers, haptic interfaces) and to apply thatdialogue to a system or application, so that the multimodal display changesappropriately. the first part of this software involves taking raw inputs from acontrol device and interpreting them. several librariescomputer hardware and software for the generation of virtualenvironments261virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.are available both as commercial products and as ''shareware" that read the mostcommon interface devices, such as the dataglove and various trackers.examples of commercial libraries include world toolkit by sense8. sharewarelibraries are available from the university of alberta and other universities.these libraries range in sophistication from serial drivers for obtaining the rawoutput from the interface devices to routines that include predictive tracking andgesture recognition.the second part of building interaction software involves turning theinformation about a system's state from a control device into a dialogue that ismeaningful to the system or application, at the same time filtering out erroneousor unlikely portions of dialogue that might be generated by faulty data from theinput device. the delivery of this dialogue to the virtual world system is thenperformed to execute some applicationmeaningful operation.interaction is a critical component of ve systems that involves bothhardware and software. interface hardware in ves provides the positions orstates of various parts of the body. this information is typically used to: (1)map user actions to changes in the environment (e.g., moving objects by hand,etc.), (2) pass commands to the environment (e.g., a hand gesture or buttonpush), or (3) provide information input (e.g., speech recognition for spokencommands, text, or numerical input). the user's intent must be inferred from theoutput of the hardware as read by the computer system. this inference may becomplicated by inaccuracies in the hardware providing the signal.existing technologiesalthough there are several paradigms for interaction in ves, includingdirect manipulation, indirect manipulation, logical commands, and data input,the problem of realistic, realtime interaction is still comparatively unexplored.generally, tasks in ves are performed by a combination of these paradigms.other paradigms will certainly need to be developed to realize the potential of anatural interface. below we provide an overview of some existing technologies.with direct manipulation, the position and orientation of a part of the user'sbody, usually the hand, is mapped continuously to some aspect of theenvironment. typically, the position and orientation of an object in the ve iscontrolled via direct manipulation. pointing in order to move is another exampleof direct manipulation in which orientation information is used to determine adirection in the ve. analogs of manual tasks such as picking and placingrequire display of forces as well and therefore are well suited to directmanipulation, though more abstract aspects of the environment, such asbackground lighting, can also be controlled in this way.computer hardware and software for the generation of virtualenvironments262virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.when indirect manipulation is employed, the user performs directmanipulation on an object in the ve, which in turn controls some other aspectof the environment. this is an extension to ve of the concept of a widget, thatis, a twodimensional interface control used in graphics interface design. thusone may directly manipulate a slider that controls the background color, whiledirect manipulation of another slider may control the volume of sound output.several groups, including the university of north carolina and the nationalaeronautics and space administration (nasa), have developed this conceptwith generalizations of menus and sliders to ves (holloway et al., 1992;jacoby, 1992; conner et al., 1992). the term employed by these groups is threedimensional widget. creators of threedimensional widgets go beyond thetypical slider and checkboxes of traditional twodimensional interfaces andattempt to provide taskspecific widgets, such as the computational fluiddynamics (cfd) widgets used in the virtual wind tunnel and surface modelingwidgets (bryson, 1992a). indirect manipulation provides the opportunity tocarry out many actions by using relatively few direct manipulation capabilities.logical commands detect the state of the user, which is then mapped toinitiate some action by the environment. logical commands are discrete events.the user's state that triggers the command may be detected via buttons, gesturesas measured by haptic devices, voice commands, etc. the particular commandtriggered by a user state may depend on the state of the environment or on thelocation of parts of the user's body. for example, a point gesture may dodifferent things depending on which virtual object happens to be coincidentwith the position of the user's hand. logical commands can also be triggered viaindirect manipulation using menus or speech recognizers.data or text input can be provided by conventional keyboard methodsexternal to the ve. within the environment, speech recognition may be used forboth text and numerical input, and indirect manipulation of widgets provideslimited numerical input.there are highlevel interfaces that should be explored. research must beperformed to explore how to use data measuring the positions of the user's bodyto interact with a ve in a way that truly provides the richness of realworldinteraction. as an example, obvious methods of manipulating a virtual surfacevia a dataglove have proven to be difficult to implement (bryson, 1992b;snibbe et al., 1992). this example demonstrates that research is needed todetermine how user tracking data are to be applied as well as how the objects inthe ve are to be defined to provide natural interaction.in addition, research is needed on the problem of mapping continuousinput (body movement) to discrete commands. there are significantcomputer hardware and software for the generation of virtualenvironments263virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.segmentation and disambiguation problems, which may require semanticdecoding. since such decoding is applicationdependent, the ve user interfacecannot easily be separated from the application in the way that it can be withcurrent twodimensional wimp (windows, icons, mouse, pointer) interfaces.design approaches and issues to be addresseda crucial decision in designing the interaction is the choice of conceptualapproach. specifically, should researchers focus on ways in which the existingtwodimensional technology might be enriched, or should the starting point bethe special attributes and challenges of threedimensional immersiveenvironments? some researchers are recreating the twodimensional graphicuser interface (gui) desktop metaphor in three dimensions by placing buttonsand scroll bars in the environment along with the user. while we believe thatthere is great promise in examining the very successful twodimensionaldesktop metaphor as a source for ideas, we also believe that there are risksbecause of the different sets of problems in the two environments. relyingsolely on extensions of our experience with two dimensions would not provideadequate solution approaches to threedimensional interaction needs, such asflying and navigation or to issues related to bodycentered coordinates systemsand lines of sight.two of the more important issues associated with interacting in a threedimensional environment are line of sight and acting at a distance. with regardto line of sight, ve applications have to contend with the fact that some usefulinformation might be obscured or distorted due to an unfortunate choice of userviewpoint or object placement. in some cases, the result can lead tomisinformation, confusion, and misunderstanding. common pitfalls includeobscuration and unfortunate coincidences. obscuration at times, a user must interact with an object that is currentlyout of sight, hidden behind other objects. how does dealing with thisspecial case change the general form of any user interface techniques wemight devise? unfortunate coincidences the archetypical example of this phenomenonis the famous optical illusion in which a person stands on a distant hillwhile a friend stands near the camera, aligning his hand so that it appearsas if the distant friend is a small person standing in the palm of his hand.such devices, while amusing in some contexts, could under othercircumstances, such as air traffic control, prove quite dangerous. perhapswe should consider alternative methods for warning the user when suchcoincidences are occurring or for ensuring that the user has enough depthinformation via parallax to perceive this.computer hardware and software for the generation of virtualenvironments264virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.when the user is immersed in a threedimensional environment, he or sheis interacting with objects at a distance. some are directly within arm's reach,others are not. in each case, there is a question of how to specify the argumentsto a particular commandšthat is, how does a user select and manipulate objectsout of the reach envelope and at different distances from the user (that is, in thesame room, the same city, across the country)? will the procedure for distantobjects be different from those used in selecting and manipulating nearbyobjects? some solutions to the selection problem involve ray casting or voiceinput, but this leaves open the question of specifying actions and parameters bymeans of direct manipulation.some solutions emphasize a bodycentric approach, which relies solely onthe user's proprioceptive abilities to specify actions in space. under this scheme,there is no action at a distance, only operations on objects in close proximity tothe user. this approach requires one of three solutions: translate the user'sviewpoint to within arm's reach of the object(s) in question, scale the user sothat everything of interest is within arm's reach, or scale the entire environmentso that everything is within arm's reach.the first solution has several drawbacks. first, by moving the user oversignificant distances, problems in orientation could result. next, moving objectsquickly over great distances can be difficult (moving an object from losangeles to new york would require that the user fly this distance or that theuser have a pointandclick, putmethere interface with a global map). finally,moving close to an object can destroy the spatial context in which that moveoperation is taking place. the second and third solutions are completelyequivalent except when other participants or spectators are also in theenvironment.perhaps the most basic interaction technique in any application is objectselection. object selection can be implicit, as happens with many directmanipulation techniques on the desktop (e.g., dragging a file to the mac trashcan), or it can be explicit, as in clicking on a rectangle in any common guidrawing package to activate selection handles for resizing. it is interesting tonote that most twodimensional user interface designers use the phrase"highlight the selected object," to mean "draw a marker, such as selectionhandles" on the selected object. with ve systems, we have the ability toliterally highlight the selected object. most examples thus far have used threedimensional extensions of twodimensional highlighting techniques, rather thansimply doing what the term implies; applying special lighting to the selectedobject.the following list offers some potentially useful selection techniques foruse in threedimensional computergenerated environments:computer hardware and software for the generation of virtualenvironments265virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved. pointing and ray casting. this allows selection of objects in clear view,but not those inside or behind other objects. dragging. this is analogous to "swipe select" in traditional guis.selections can be made on the picture plane with a rectangle or in anarbitrary space with a volume by "lassoing." lassoing, which allows theuser to select a space of any shape, is an extremely powerful technique inthe twodimensional paradigm. carrying this idea over to threedimensions requires a threedimensional input device and perhaps avolume selector instead of a twodimensional lasso. naming. voice input for selection techniques is particularly important inthreedimensional environments. "delete my chair" is a powerfulcommand archetype that we should not ignore. the question of how tomanage naming is extremely important and difficult. it forms a subset ofthe more general problem of naming objects by generalized attributes. naming attributes. specifying a selection set by a common attribute or setof attributes ("all red chairs with arms") is a technique that should beexploited. since some attributes are spatial in nature, it is easy to see howthese might be specified with a gesture as well as with voice, offering afluid and powerful multimodal selection technique: all red chairs, shorterthan this [user gestures with two hands] in that room [user looks overshoulder into adjoining room].for more complex attribute specification, one can imagine attribute editorsand sophisticated threedimensional widgets for specifying attribute values andranges for the selection set. selection by example is another possibility: "selectall of these [grabbing a chair]." all of the selection techniques described abovesuffer from being too inclusive. it is important to provide the user with anopportunity to express "but not that one" as a qualification in any selection task.of course, excluding objects is itself a selection task.an important aspect of the selection process is the provision of feedback tothe user confirming the action that has been taken. this is a more difficultproblem in three dimensions, where we are faced with the graphic arts questionof how to depict a selected object so that it appears unambiguously selectedfrom an arbitrary viewing angle, under any lighting circumstances, regardless ofthe rendering of the object.another issue is that of extending the software to deal with twohandedinput. although manipulations with two hands are most natural for many tasks,adding a second pointing device into the programming loop significantlycomplicates the programmer's model of interaction and object behavior and sohas been rarely seen in twodimensional systems other than research prototypes.in threedimensional immersive environments, however, twohanded inputbecomes even more important, ascomputer hardware and software for the generation of virtualenvironments266virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.individuals use both gestures and postures to indicate complex relationshipsbetween objects and operations in space.if an interface is poorly designed, it can lull the user into thinking thatoptions are available when in fact they are not. for example, current immersivethreedimensional systems often depict models of human hands in the scenewhen the user's hands are being tracked. given the many kinds of actions thathuman hands are capable of, depicting human hands at all times might suggestto users that they are free to perform any action they wishšyet many of theseactions may exceed the capabilities of the current system. one solution to thisproblem is to limit the operations that are possible with bare hands, specifyingfor more sophisticated operations the use of tools. a thoughtful design woulddepict tools that suggest their purpose, so that, like a carpenter with a toolbox,the user has an array of virtual tools with physical attributes that suggest certainuses. cutting tools might look like saws or knives, while attachment tools mightlook like staplers. this paradigm melds together issues of modality with voice,context, and command.interaction techniques and dialogue design have been extremely importantresearch foci in the development of effective twodimensional interfaces. untilrecently, the ve community has been occupied with getting any input to work,but it is now maturing to the point that finding common techniques acrossapplications is appropriate. these common techniques are points of leverage: byencapsulating them in reusable software components, we can hope to build vetools similar to the widget, icon, mouse, pointer (wimp) application buildersthat are now widely in use for twodimensional interfaces. it should also benoted that the progress made in threedimensional systems should feedback intotwodimensional systems.visual scene navigation softwarevisual scene navigation software provides the means for moving the userthrough the threedimensional virtual world. there are many component partsto this software, including control device gesture interpretation (gesturemessage from the input subsystem to movement processing), virtual cameraviewpoint and view volume control, and hierarchical data structures for polygonflow minimization to the graphics pipeline. in navigation, all act together in realtime to produce the next frame in a continuous series of frames of coherentmotion through the virtual world. the sections below provide a survey ofcurrently developed navigation software and a discussion of special hierarchicaldata structures for polygon flow.computer hardware and software for the generation of virtualenvironments267virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.survey of currently developed navigation softwarenavigation is the problem of controlling the point and direction of view inthe ve (robinett and holoway, 1992). using conventional computer graphicstechniques, navigation can be reduced to the problem of determining a positionand orientation transformation matrix (in homogeneous graphics coordinates)for the rendering of an object. this transformation matrix can be usefullydecomposed into the transformation due to the user's head motion and thetransformation due to motions over long distance (travel in a virtual vehicle).there may also be several virtual vehicles concatenated together.the first layer of virtual world navigation is the most specific: theindividual's viewpoint. one locally controls one's position and direction of viewvia a head tracking device, which provides the computer with the position andorientation of the user's head.the next layer of navigation uses the metaphor of a virtual vehicle, whichallows movement over distances in the ve greater than those distances allowedby the headtracker alone. the position and orientation of the virtual vehicle canbe controlled in a variety of ways. in simulation applications, the vehicle iscontrolled in the same way that an actual simulated vehicle would be controlled.examples that have been implemented are treadmills and bicycles and joysticksfor flight or vehicle simulators. for more abstract applications, there have beenseveral experimental approaches to controlling the vehicle. the most commonis the point and fly technique, wherein the vehicle is controlled via a directmanipulation interface. the user points a threedimensional position andorientation tracker in the desired direction of flight and commands theenvironment to fly the user vehicle in that direction. other methods ofcontrolling the vehicle are based on the observation that in ve one need not getfrom here to there through the intervening space. teleoperation is one obviousexample, which often has the user specify a desired destination and then"teleports" the user there. solutions have included portals that have fixed entryand exit locations, explicit specification of destination through numerical orlabel input, and the use of small threedimensional maps of the environment topoint at the desired destination. another method of controlling the vehicle isdynamic scaling, wherein the entire environment is scaled down so that the usercan reach the desired destination, and then scaled up again around thedestination indicated by the user. all of these methods have disadvantages,including difficulty of control and orientation problems.there is a hierarchy of objects in the ve that may behave differentlyduring navigation. some objects are fixed in the environment and are acted onby both the user and the vehicle. other objects, usually virtualcomputer hardware and software for the generation of virtualenvironments268virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.tools that the user will always wish to have within reach, will be acted on by thehead transformation only. still other objects, such as data displays, are alwaysdesired within the user's field of view and are not acted on by either the user orthe vehicle. these objects have been called variously world stable, vehiclestable, and head stable (fisher et al., 1986). although most of the fundamentalmathematics of navigation software are known, experimentation remains to bedone.survey of hierarchical data structure techniques for polygon flowminimizationhierarchical data structures for the minimization of polygon flow to thegraphics pipeline are the back end of visual scene navigation. when we havegenerated a matrix representing the chosen view, we then need to send the scenedescription transformed by that matrix to the visual display. one key method toget the visual scene updated in real time at interactive update rates is tominimize the total number of polygons sent to the graphics pipeline.hierarchical data structures for polygon flow minimization are probablythe least well understood aspect of graphics development. many people buyworkstations, such as the silicon graphics, that promise 2 million polygons/sand expect to be able to create realistic visual scenes in virtual worlds. this is avery common misconception. visual reality has been said to consist of 80million polygons per picture (catmull et al., 1984). extending this to the veneed for 10 frames/s minimum, 800 million polygons/s are needed.today, as noted above, workstations are advertised to have the capabilityto process approximately 2 to 3 million polygons/s (flat shaded, nontextured). iftextured scenes are desired, the system will run slower at approximately900,000 textured polygons/s. we expect to see 10 to 25 percent of thisadvertised performance, or 225,000 textured polygons/s. at 10 frames/s, this is22,500 polygons per frame or 7,500 textured polygons at 30 frames/s (7,500polygons is not a very detailed world).the alternatives are to live with worlds of reduced complexity or to offload some of the graphics work done in the pipeline onto the multiple cpus ofworkstations. all polygon reduction must be accomplished in less time than ittakes just to send the polygons through the pipeline. the difficulty of polygonflow minimization depends on the composition of the virtual world. thisproblem has historically been approached on an application specific basis, andthere is as yet no general solution. current solutions usually involve partitioningthe polygondefined world into volumes that can readily be checked forvisibility by the virtual worldcomputer hardware and software for the generation of virtualenvironments269virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.viewer. there are many partitioning schemesšsome of which work only if theworld description does not change dynamically (airey et al., 1990).a second component of the polygon flow minimization effort is the pixelcoverage of the object modeled. once an object has been determined to be inview, the secondary question is how many pixels that object will cover. if thenumber of pixels covered by an object is small, then a reduced polygon count(lowresolution) version of that object can be rendered. this results inadditional software complexity, again software that must run in real time.because the levelofdetail models are precomputed, the issue is greater datasetsize rather than level selection (which is nearly trivial).the current speed of zbuffers alone means we must carefully limit thepolygons sent through the graphics pipeline. other techniques that use the cpusto minimize polygon flow to the pipeline are known for specific applications,but those techniques do not solve the problem in general.in a classic paper, clark (1976) presents a general approach for solving thepolygon flow minimization problem by stressing the construction of ahierarchical data structure for the virtual world (figure 85). the approach is toenvision a world database for which a bounding volume is known for eachdrawn object. the bounding volumes are organized hierarchically, in a tree thatis used to rapidly discard large numbers of polygons. this is accomplished bytesting the bounding volumes to determine whether they are contained orpartially contained in the current orientation of the view volume. the processcontinues recursively until a node is reached for which nothing underneath it isin the view volume.figure 85 hierarchical data structure for polygon flow minimization.computer hardware and software for the generation of virtualenvironments270virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.this part of the clark paper provides a good start for anyone building a threedimensional ve for which the total number of polygons is significantly largerthan the hardware is capable of drawing.the second part of clark's paper deals with the actual display of thepolygons in the leaf nodes of the tree. the idea is to send only minimaldescriptions of objects through the graphics pipeline (minimal based on theexpected final pixel coverage of the object). in this approach, there will bemultipleresolution versions of each threedimensional object and software forrapidly determining which resolution to draw. the assumption of multipleresolution versions of each threedimensional object being available is a largeone, with automatic methods for their generation remaining an open issue.other discussions of this issue are found in dehaemer and zyda (1991),schroeder et al. (1992), and turk (1992).applicationspecific solutionspolygon flow minimization to the graphics pipeline is best understood bylooking at specific solutions. some of the more interesting work has been doneby brooks at the university of north carolina at chapel hill with respect toarchitectural walkthrough (brooks, 1986; airey et al., 1990). the goal in thosesystems was to provide an interactive walkthrough capability for a planned newcomputer science building at the university that would offer visualization of theinternal spaces of that building for the consideration of changes beforeconstruction.the walkthrough system had some basic tenets. the first was that thearchitectural model would be constructed by an architect and passed on to thewalkthrough phase in a fixed form. a display compiler would then be run onthat database and a set of hierarchical data structures would be output to a file.the idea behind the display compiler was that the building model was fixed andthat it was acceptable to spend some 45 minutes in computing a set ofhierarchical data structures. once the data structures were computed, a displayloop could then be entered, in which the viewpoint could be rapidly changed.the walkthrough system was rather successful, but it has the limitation that theworld cannot be changed without rerunning the display compiler. otherwalkthrough systems have similar limitations (teller and sequin, 1991;funkhouser et al., 1992).realtime display of threedimensional terrain is a wellresearched areathat originated in flight simulation. terrain displays are an interesting specialcase of the polygon flow minimization problem in that they are relatively wellworked out and documented in the open literature (zyda et al., 1993a). thebasic idea is to take the terrain grid and generate a quadtree structure containingthe terrain at various display resolutions. the notion of the grid cell is used forreducing polygon flow by drawingcomputer hardware and software for the generation of virtualenvironments271virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.only those objects whose cell is also to be drawn (assuming the grid cell onwhich the object lies is known). this strategy works well for groundbasedvisual displays; a more comprehensive algorithm is required for air views ofsuch displays.world modelingmodels that define the form, behavior, and appearance of objects are thecore of any ve. a host of modeling problems are therefore central to thedevelopment of ve technology. an important technological challenge ofmultimodal ves is to design and develop object representation, simulation, andrendering (rsr) techniques that support visual, haptic, and auditoryinteractions with the ve in real time. there are two major approaches to thersr process. first, a unified central representation may be employed thatcaptures all the geometric, surface, and physical properties needed for physicalsimulation and rendering purposes. in principle, methods such as finite elementmodeling could be used as the basis for representing these properties and forphysical simulation and rendering purposes. at the other extreme, separate,spatially and temporally coupled representations could be maintained thatrepresent only those properties of an object relevant for simulating andrendering interactions in a single modality (e.g., auditory events). the formerapproach is architecturally the most elegant and avoids issues of maintainingproper spatial and temporal correlation between the rsr processes for eachmodality. practically, however, the latter approach may allow better matchingbetween modalityspecific representation, simulation, and rendering streams.the abilities and limitations of the human user and the ve system for each ofthe modalities impose unique spatial (e.g., scale and resolution) and temporal(e.g., device update rate and duration) constraints. for example, it is likely thatthe level of detail and consequently the nature of approximations in the rsrprocess will be different for each of the modalities. it is unclear, therefore,whether these modalityspecific constraints can be met by systems based on asingle essential or core representation and still operate in real time.the overwhelming majority of verelevant rsr research anddevelopment to date has been on systems that are visually rendered (e.g.,witkin and welch, 1990). the theoretical and practical issues associated witheither of the two rsr approaches or variants in a multimodal context, however,have received minimal attention but are likely to become a major concern forve system researchers and developers. for example, geometric modeling isrelevant to the generation of acoustic environments (i.e., room modeling) aswell as visual environments, and the development of physical models is criticalto the ability to generate andcomputer hardware and software for the generation of virtualenvironments272virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.modulate auditory, visual, and haptic displays. novel applications such as theuse of auditory displays for the understanding of scientific data (e.g., kramer,1992; blattner et al., 1989) require models that may not be physically based. inthis section, we concentrate on the visual domain and examine the problems ofconstructing geometric models, the prospects for visionbased acquisition ofrealworld models, dynamic model matching for augmented reality, thesimulation of basic physical behavior, and simulation of autonomous agents.parallel issues are involved in the other modalities and are discussed inchapters 3 and 4.geometric modeling: construction and acquisitionthe need to build detailed threedimensional geometric models arises incomputeraided design (cad), in mainstream computer graphics, and invarious other fields. geometric modeling is an active area of academic andindustrial research in its own right, and a wide range of commercial modelingsystems is available. despite the wealth of available tools, modeling isgenerally regarded as an onerous task. among the many factors contributing tothis perception are sluggish performance, awkward user interfaces, inflexibility,and the low level at which models must typically be specified. it is symptomaticof these difficulties that most leading academic labs, and many commercialanimation houses (such as pixar and pdi), prefer to use inhouse tools, or insome cases a patchwork of homegrown and commercial products.from the standpoint of ve construction, geometric modeling is a vitalenabling technology whose limitations may impede progress. as a practicalmatter, the ve research community will benefit from a shared open modelingenvironment, a modeling environment that includes physics. in order tounderstand this, we need to look at how threedimensional geometric modelsare currently acquired. we do this by looking at how several ve efforts havereported their model acquisition process.geometric models for ves are typically acquired through the use of a pcbased, macintoshbased, or workstationbased cad tool. if one reads the workdone for the walkthrough project at the university of north carolina (airey etal., 1990), one finds that autocad was used to generate the 12,000+ polygonsthat comprised the orange united methodist church. in the presentation of thatpaper, one of the problems discussed was that of ''getting the required data outof a cad program written for other purposes." getting the threedimensionalgeometry out of the files generated by autocad was not difficult, but therewas a problem in that not all of the data required were present in the formneeded for the ve walkthrough. in particular, data related to the actual physicsof the building were not present, and partitioning information useful to the realtimecomputer hardware and software for the generation of virtualenvironments273virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.walkthrough algorithms had to be added later "by hand" or "back fed in" byspecially written programs.the vpl reality built for two (rb2) system (blanchard et al., 1990) useda macintosh ii as its design station for solid modeling and an iris workstationas its rendering/display station. rb2 is a software development platform fordesigning and implementing realtime ves. development under rb2 is rapidand interactive, with behavior constraints and interactions that can be edited inreal time. the geometric modeling function of rb2 was provided by a softwaremodule called rb2 swivel and a data flow/realtime animation control packagecalled body electric. rb2 has a considerable following in organizations that donot have sufficient resources to develop their own inhouse ve expertise. rb2is a turnkey system, whose geometric and physics file formats are proprietary.in the npsnet project (zyda et al., 1992), the original set of threedimensional icons used was acquired from the simnet databases. thesemodels were little more than threedimensional skins of the weapons systemsknown to simnet. as a result, project researchers have developed an openformat for storing these threedimensional models (zyda et al., 1993a), addedphysics to the format (zyda et al., 1992), and have rewritten the system toinclude objectoriented animation capabilities (wilson et al., 1992). forexample, at the naval postgraduate school, the npsnet research group iscurrently using software systems' expensive and proprietary multigen cadtool for the development of physicsfree models for its sgi performerbasednpsnet4 system. computeraided design systems with retrofitted physics arebeginning to be developed (e.g., deneb robotics and parametric technologies),but these systems are expensive and proprietary.many applications call for ves that are replicas of real ones. rather thanbuilding such models by hand, it is advantageous to use visual or other sensorsto acquire them automatically. automatic acquisition of complex environmentmodels (such as factory environments) is currently not practical but is a timelyresearch issue. meanwhile, automatic or nearly automatic acquisition ofgeometric models is practical now in some cases, and partially automatedinteractive acquisition should be feasible in the near term (ohya et al., 1993;fuchs et al., 1994).the most promising shortterm approaches involve active sensingtechniques. scanning laser finders and lightstripe methods are both capable ofproducing range images that encode position and shape of surfaces that arevisible from the point of measurement. these active techniques offer the strongadvantage that threedimensional measurements may be made directly, withoutthe indirect inferences that passively acquired images require. active techniquesdo, however, suffer from somecomputer hardware and software for the generation of virtualenvironments274virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.limitations: because sensortosurface distances must be relatively small, theyare not applicable to largescale environments. surfaces that are nonreflectiveor obliquely viewed shiny surfaces may not return enough light to allow rangemeasurements to be made. noise is enough of a problem that data mustgenerally be cleaned up by hand. a more basic problem is that a single rangeimage contains information only about surfaces that were visible from aparticular viewpoint. to build a complete map of an environment, many suchviews may be required, and the problem of combining them into a coherentwhole is still unsolved.among passive techniques, stereoscopic and motionbased methods,relying on images taken from varying viewpoints, are currently most practical.however, unlike active sensing methods, these rely on pointtopoint matchingof images in order to recover distance by triangulation. many stereo algorithmshave been developed, but none is yet robust enough to compete with activemethods. methods that rely on information gleaned from static monocular viewsšedges, shading, texture, etc.šare less effective.for many purposes, far more is required of an environment model than justa map of objects' surface geometry. if the user is to interact with theenvironment by picking things up and manipulating them, information aboutobjects' structure, composition, attachment to other objects, and behavior is alsoneeded. unfortunately, current vision techniques do not even begin to addressthese deeper issues.dynamic model matching and augmented realitythe term augmented reality has come to refer to the use of transparentheadmounted displays that superimpose synthetic elements on a view of thereal surroundings. unlike conventional headsup displays in which the addedelements bear no direct relation to the background, the synthetic objects inaugmented reality are supposed to appear as part of the real environment. thatis, as nearly as possible, they should interact with the observer and with realobjects, as if they too were real.at one extreme, creating a full augmentedreality illusion requires acomplete model of the real environment as well as the synthetic elements. forinstance, to place a synthetic object on a real table and make it appear to stay onthe table as the observer moves through the environment, we would need toknow just where the table sits in space and how the observer is moving. for fullrealism, enough information about scene illumination and surface properties tocast synthetic shadows onto real objects would be needed. furthermore, wewould need enough information about threedimensional scene structure toallow real objects to hide or be hidden by synthetic ones, as appropriate.naturally, all of this wouldcomputer hardware and software for the generation of virtualenvironments275virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.happen in real time, in response to uncontrolled and unpredictable observermotions.this sort of mix of the real and synthetic has already been achieved inmotion picture special effects, most notably, industrial light and magic'seffects in films such as the abyss and terminator 2. some of these effects wereproduced by rendering threedimensional models and creating a composite ofthe resulting images with liveaction frames, as would be required in augmentedreality. however, the process was extremely slow and laborious, requiringmanual intervention at every step. after scenes were shot, models of cameraand object motions were extracted manually, using framebyframe manualmeasurement along with considerable trial and error. even small geometricerrors were prone to destroy the illusion, making the synthetic objects appear tofloat outside the live scene.automatic generation of augmentedreality effects is still a researchproblem in all but the least demanding cases. the two major issues are: (1)accurate measurement of observer motions and (2) acquisition and maintenanceof scene models. the prospects for automatic solutions to the latter werediscussed above. if the environment is to remain static, it would be feasible tobuild scene models offline using interactive techniques. although ve displaysprovide direct motion measurements of observer movement, these are unlikelyto be accurate enough to support highquality augmented reality, at least whenreal and synthetic objects are in close proximity, because even very small errorscould induce perceptible relative motions, disrupting the illusion. perhaps themost promising course would use direct motion measurements for grosspositioning, using local imagebased matching methods to lock real andsynthetic elements together.physical simulation for visual displaysin order to give solidity to ves and situate the user firmly in them, virtualobjects, including the user's image, need to behave like real ones. at aminimum, solid objects should not pass through each other, and things shouldmove as expected when pushed, pulled, or grasped.analysis of objects' behavior at the scale of everyday observation lies inthe domain of classic mechanics, which is a mature discipline. however,mechanics texts and courses are generally geared toward providing insight intoobjects' behavior, whereas to support ve the behavior itself is of paramountimportancešinsight strictly optional. thus classic treatments may provide therequired mathematical underpinnings but do not directly address the problem athand.simulations of classic mechanics are extensively used as aids inengineering design and analysis. although these traditional simulations docomputer hardware and software for the generation of virtualenvironments276virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.yield numerical descriptions of behavior, they still do not come close to meetingthe needs of ves. in engineering practice, simulation is a long, drawnout, andhighly intellectualized activity. the engineer typically spends much time withpencil and paper developing mathematical models for the system under study.these are then transferred to the simulation software, often with muchtweaking, and parameter selection. only then can the simulation actually be run.as a design evolves, the initial equations must be modified and reentered andthe simulation rerun.in strong contrast, a mechanical simulation for ves must run reliably,seamlessly, automatically, and in real time. within the scope of the world beingmodeled, any situation that could possibly arise must be handled correctly,without missing a beat. in the last few years, researchers in computer graphicshave begun to address the unique challenges posed by this kind of simulation,under the heading of physically based modeling. below we summarize the mainexisting technology and outstanding issues in this area.solid object modeling solid objects' inability to pass through each other isan aspect of the physical world that we depend on constantly in everyday life:when we place a cup on a table, we expect it to rest stably on the table, not floatabove or pass through it.in reaching and grasping, we rely on solid handobject contact as an aid (asdo roboticists, who make extensive use of force control and compliant motion).of course, we also rely on contact with the ground to stand and locomote.the problem of preventing interpenetration has three main parts. first,collisions must be detected. second, objects' velocities must be adjusted inresponse to collisions. finally, if the collision response does not cause theobjects to separate immediately, contact forces must be calculated and applieduntil separation finally occurs.collision detection is most frequently handled by checking for objectoverlaps each time position is updated. if overlap is found, a collision issignaled, the state of the system is backed up to the moment of collision, and acollision response is computed and applied. the bulk of the work lies in thegeometric problem of determining whether any pair of objects overlap. thisproblem has received attention in robotics, in mechanical cad, and incomputer graphics. brute force overlap detection for convex polyhedra is astraightforward matter of testing each vertex of every object against each faceof every other object. more efficient schemes use bounding volumes or spatialsubdivision to avoid as many tests as possible. good general methods forobjects with curved surfaces do not yet exist.in fact, checking for object overlaps at each update is not sufficient tocomputer hardware and software for the generation of virtualenvironments277virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.guarantee noninterpenetration, because objects may have collided and passedthrough each other between the previous configuration and the new one. this isnot merely an esoteric concern, because it means that rapidly moving objects,e.g., projectiles, may pass entirely through thin objects, such as walls, with nocollisions ever being detected. needless to say, large errors can result.guaranteed methods have been described by lin and canny (1992) for the caseof convex polyhedra with constant linear and angular velocity.collision response involves the application of an impulse and producing aninstantaneous change in velocity that prevents interpenetration. the basics ofcollision response are well treated in classic mechanics and do not pose anygreat difficulties for implementation. problems do arise in developing accuratecollision models for particular materials, but many ve applications will notrequire this degree of realism.to handle continuous multibody contact, it is necessary to calculate theconstraint forces that are exchanged at the points of contact and to identify theinstants at which contacts are broken. determining which contacts are breakingis a particularly difficult problem, turning out, as shown by baraff, to requirecombinatorial search (baraff and witkin, 1992; baraff, 1989). fortunately,baraff also developed reasonably efficient methods that work well in practice.many virtual world systems exhibit rigid body motion with collisiondetection and response (hahn, 1988; moore and wilhelms, 1988; baraff, 1989;baraff and witkin, 1992; zyda et al., 1993b). baraff's system also handlesmultibody continuous contact and frictional forces for curved surfaces. thesesystems provide many of the essential elements required to support ves.constraints and articulated objects in addition to simple objects such asrigid bodies, we should be able to handle objects with moving partsšdoors thatopen and close, knobs and switches that turn, etc. in principle, the ability tosimulate simple objects such as rigid bodies, together with the ability to preventinterpenetration, could suffice to model most such compound objects. forinstance, a working desk drawer could be constructed by modeling thegeometry of a tongue sliding in a groove, or a door by modeling in detail therigid parts of the hinge. in practice, it is far more efficient to employ directgeometric constraints to summarize the effects of this kind of detailedinteraction. for instance, a sliding tongue and groove would be idealized as apair of coincident lines, one on each object, and a hinge would be represented asan ideal revolute joint.the simulation and analysis of articulated bodiesšjointed assemblies ofrigid partsšhave been treated extensively, particularly in robotics. incomputer hardware and software for the generation of virtualenvironments278virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.addition to classic techniques such as lagrangian dynamics, streamlinedrecursive formulations have been developed, making it possible to simulateforward dynamics of a kinematic chain in linear time, rather than the n&and;3time that lagrangian dynamics requires. these methods only pay off forrelatively long chains (n > 9, according to featherstone) and in their originalform do not readily handle closed loops in the graph of part connectivity.building on the work of lathrop, schroeder demonstrated that it is neverthelessfeasible to build a "virtual erector set" based on recursive formulations(schroeder and zeltzer, 1990).another approach to simulating constrained systems of objects builds onthe classic method of lagrangian multipliers, in which a linear system is solvedat each time step to yield a set of constraint forces. this approach offers severaladvantages: first, it is general, allowing essentially arbitrary holonomicconstraints to be applied to essentially arbitrary (not necessarily rigid) bodies.second, it lends itself to onthefly construction and modification, an importantconsideration for ves. finally, the constraint matrices that form the linearsystem are typically sparse, reflecting the fact that everything is not usuallyconnected directly to everything else. using numerical methods that exploit thissparsity can yield performance that competes with recursive methods. methodsof this kind were used for animation by platt and by barzel and barr (platt andbarr, 1988; barzel and barr, 1988). witkin et al. (1990) demonstrated a fullyinteractive snaptogether construction system and showed how the constraintequations could be built on the fly and solved in a way that exploits sparsity.nonrigid objects a vast body of work treats the use of finite elementmethods to simulate continuum dynamics. most of this work is probably oflimited relevance to the construction of conventional ves, simply because suchenvironments will not require finegrained nonrigid modeling, with the possibleexception of virtual surgery. however, interactive continuum analysis forscience and engineering may become an important specialized application ofves once the computational horsepower is available to support it.highly simplified models for flexiblebody dynamics are presented bywitkin and welch (1990), by pentland and williams (1989), and by baraff andwitkin (1992). the general idea of these models is to use only a few globalparameters to represent the shape of the whole object, formulating the dynamicequations in terms of these variables. these simplified models capture only thegross deformations of the object but in return provide very high performance.they are probably the most appropriate choice for ves that require simplenonrigid behavior.a special form of nonrigid modeling, constituting a potential ve applicationcomputer hardware and software for the generation of virtualenvironments279virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.in itself, is interactive sculpting of freeform surfaces. the general idea is to usesimulated flexible materials as a sculpting medium. flexible thin sheets areemployed by celniker and gossard (1992) and by welch and witkin (1992).szeliski and tonnesen (1992) uses clouds of oriented particles to form smoothsurfaces.motivated by the obvious need in both computer graphics and engineeringfor realism and physically based environments that support various levels ofobject detail and interaction (depending on the application), metaxas (1992,1993; metaxas and terzopoulos, 1992a, 1992b, 1993; terzopoulos andmetaxas, 1991) developed a general framework for shape and nonrigid motionsynthesis, which can also handle rigid bodies as a special case. the frameworkfeatures a new class of dynamic deformable part models. these models haveboth global deformation parameters that represent the gross shape of an objectin terms of a few parameters and local deformation parameters that represent anobject's details through the use of sophisticated finite element techniques.global deformations are defined by fully nonlinear parametric equations. hencethe models are more general than the linearly deformable ones included inwitkin and welch (1990) and quadratically deformable ones included inpentland and williams (1989). by augmenting the underlying lagrangianequations' motion with very fast dynamic constraint techniques based onbaumgarte (1972), he adds the capability to compose articulated models(metaxas, 1992, 1993; metaxas and terzopoulos, 1992b) from deformableparts, whose special case for rigid objects is the technique used by barzel andbarr (1988). moreover, metaxas (1992, 1993) also develops fast algorithms forthe computation of impact forces that occur during collisions of complexflexible multibody objects with the simulated physical environment.issues to be addressed most of the essential pieces that are required toimbue ves with physical behavior have already been demonstrated. somešnotably snaptogether constraints and interactive surface modelingšhave beendemonstrated in fully interactive systems, and othersšnotably the handling ofcollision and contactšare only now beginning to appear in interactive systems(recent work by david baraff at carnegie mellon university involves aninteractive 2.5dimensional simulation of noninterpenetrating objects). themost immediate challenge at hand is one of integrating the existing technologyinto a working system, along with other elements of ve construction software.many performancerelated issues are still to be addressed, for example,doing efficient collision detection in a largescale environment (systems withfrom 500 to 300,000 players or parts) and further accelerating constraineddynamics solutions. in addition, many of the standardcomputer hardware and software for the generation of virtualenvironments280virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.numerical techniques are not tuned to realtime systems. for example, the ratioof compute time to real time can vary by orders of magnitude in the simulationof noninterpenetrating bodies, slowing even further when complex contactsituations arise. maintaining a constant frame rate will require the developmentof new methods that degrade gracefully in such situations.autonomous agentsthe need for simulated autonomous agents arises in many ve applicationareas, such as training, education, and entertainment, in which such agentscould play the role of adversaries, trainers, or partners or simplysupernumeraries to add richness and believability. although fully crediblesimulated humans are the stuff of science fiction, simple agents will oftensuffice. the construction of simulated autonomous agents draws on a number oftechnologies, including robotics, computer animation, artificial intelligence, andoptimization.motion control placing an autonomous agent in a virtual physicalenvironment is essentially like placing a robot in a real environment: the agent'sbody is a physical object that must be controlled to achieve coordinated motion.fortunately, controlling a virtual agent is much easier than controlling a realone, since many simplifications and idealizations can be made. for example, theagent can be given access to full and perfect information about the state of theworld, and many troubling mechanical effects need not arise.closedloop controllers were used to animate virtual agents by mckennaand zeltzer (1990) and by miller (1988). more recently, raibert and hodgkins(1992) adapted their controller for a real legged robot to the creation ofanimation. rather than handcrafting controllers, witkin and kass (1988) solvenumerically for optimal goaldirected motion, in an approach that has sincebeen elaborated by van de panne et al. (1990) and by cohen (1992).human figure simulation in many applications, a ve system must be ableto display accurate models of human figures, possibly including a model of theuser. consider training systems, for example. outthewindow views generatedby highend flight simulators hardly ever need to include images of humanfigures. but there are many situations in which personnel must cooperate andinteract with other crew members. carrier flight deck operations, small squadtraining or antiterrorist tactics, for example, require precise coordination of theactions of many individuals for safe and successful execution. ve systems tosupport training,computer hardware and software for the generation of virtualenvironments281virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.planning, and rehearsal of such activities must therefore provide computationalmodels of human figures.we call a computer model of a human figure that can move and function ina ve a virtual actor. if the movement of a virtual actor is slaved to the motionsof a human using cameras, instrumented clothing, or some other means of bodytracking, we call that a guided virtual actor, or simply, a guided actor.autonomous actors operate under program control and are capable ofindependent and adaptive behavior, such that they are capable of interactingwith human participants in the ve, as well as with simulated objects and events.in addition to responding to the typed or spoken utterances of humanparticipants, a virtual actor should be capable of interpreting simple taskprotocols that describe, for example, maintenance and repair operations. givena set of one or more motor goalsše.g., pick up the wrench and loosen theretaining bolts, or put the book on the desk in my officeša virtual actor shouldbe capable of generating the appropriate motor acts, including necessary andimplicit tasks and motor subgoals.beyond the added realism that the presence of virtual actors can provide inthose situations in which the participants would normally expect to see otherhuman figures, autonomous actors can perform two important functions in veapplications. first, autonomous actors can augment or replace humanparticipants. this will allow individuals to work or train in group settingswithout requiring additional personnel. second, autonomous actors can serve assurrogate instructors. ve systems for training, education, and operationsrehearsal will incorporate various instructional features, including knowledgebased systems for intelligent computeraided instruction (icai) (ford, 1985).as icai systems mature, virtual actors can provide personae to interact withparticipants in a ve system.the required degree of autonomy and realism of simulated human figureswill vary, of course, from application to application. however, at the presenttime, rigorous techniques do not exist for determining these requirements. itshould also be noted that autonomous agents need not be literal representationsof human beings but may represent various abstractions. for example, thesimnet system provides for semiautonomous forces that may representgroups of dismounted infantry or single or multiple vehicles that are capable ofreacting to simulated events in accordance with some chosen military doctrine.in the remainder of this section, we confine our discussion to simulated humanfigures, i.e., virtual actors.in the course of everyday activity, we touch and manipulate objects, makecontact with various surfaces, and make contact with other humans eitherdirectly, e.g., shaking hands, or indirectly, e.g., two people lifting acomputer hardware and software for the generation of virtualenvironments282virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.heavy object. there are other ways, of course, in which two or more humansmay coordinate their motions that do not involve direct contact, for example,crew members on a carrier flight deck who communicate by voice and handsignals. in the computer graphics community, there is a long history of humanfigure modeling, but this work has considered, for the most part, kinematicmodeling of uncoupled motion exclusively.with today's graphics workstations, kinematic models of reasonablycomplex figures (say, 30 to 40 degrees of freedom) can be animated in real ornearreal time; dynamic simulations cannot. we need to understand in whichapplications kinematic models are sufficient, and in which applications therealism of dynamic simulation is required.action selection in order to implement autonomous actors that canfunction independently in a virtual world without the need for interactivecontrol by a human operator, we require some mechanism for selecting andsequencing motor skills appropriate to the actor's behavioral goals and the statesof objectsšincluding other actorsšin the ve. that is, it is not sufficient toconstruct a set of behaviors, such as walking, reaching, grasping, and so on. inorder to move and function with other actors in a virtual world that is changingover time, an autonomous actor must link perception of objects and events withaction. we call this process motor planning.brooks (1989) has developed and implemented a motor planningmechanism he calls the subsumption architecture. this work is in large part areaction against conventional notions of planning in artificial intelligence.brooks argues for a representationless paradigm in which the behavior of arobot is modulated entirely by interaction between perception of the physicalenvironment and the robot's taskachieving behavior modules.esakov and badler (1991) report on the architecture of a simulationanimation system that can handle temporal constraints for task sequencing, rulesets, and resource allocation. no online planning was implemented. taskdescriptions were initially in the form of predefined animation task keywords.this keywordbased input constraint was subsequently relaxed to allow simpledothis/dothat commands, e.g., ''the man should flip (switch) tglj1 with hisleft hand and the woman should move (switch) twf1 to position 1." mostrecently, reactive planning based on sensory perception has been used inlocomotion control (beckett and badler, 1993), as well as realtime collisionavoidance. a highlevel task expansion planner (geib, 1993) creates taskactions that are interpreted by an objectspecific reasoner to execute animationbehaviors. recent work by badler et al. (1991) also involves the exploration ofnatural language as a means of communicating task descriptions. badler'scomputer hardware and software for the generation of virtualenvironments283virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.new animnl project is deeply committed to highlevel motion planning(badler et al., 1993, 1991; webber et al., 1993); various other motor planningissues have also been studied and published (badler et al., 1993; ching andbadler, 1992).magnenatthalmann and thalmann (1990, 1991), and rijpkema andgirard (1991) have reported some work with automated grasping, but theirsystems seem to be focused on key framelike animation systems for makinganimated movies, rather than for realtime interaction with virtual actors.maiocchi and pernici (1990) describe the pinocchio system, which iscapable of animating realistic character movement derived from recordedhuman movements. their system uses limited natural language for describingbody configurations, e.g., dance motions. however, this has only limited use indescribing interactions with objects in the environment.ridsdale (1990) describes the director's apprentice, which is intended tointerpret film scripts by using a rulebase of facts and relations about cinematicdirecting. this work was primarily concerned with positioning characters inrelation to each other and the synthetic camera, but it did not address therepresentation and control of autonomous agents. in later work, ridsdaledescribes a method of teaching skills to an actor using connectionist learningmodels (ridsdale, 1990).maes (1990) has developed and implemented an action selection algorithmfor goaloriented, situated robotic agents. her work is an independentformalization of ideas discussed in earlier work by zeltzer (1983), with animportant extension that accounts for the continuous flow of activation energyamong a network of motor skills. routine, stereotypical behavior is a functionof an agent's currently active drives, goals, and motor skills. as a virtual actormoves through and operates in an environment, motor skills are triggered bypresented stimuli, and the agent's propensities for executing some behaviors andnot others are continually adjusted. the collection of skills and the patterns ofexcitation and inhibition determine an agent's repertoire of behaviors andflexibility in adapting to changing circumstances.populating the world: npsnet as an exampleone of the key aspects of a virtual world is the population of that world.we define population as the number of active entities within the world. anactive entity is anything in the world that is capable of exhibiting a behavior. bythis definition, a humancontrolled player is an active entity, a tree that is blownup is midway between an active and static entity, and an inert object like a rockis a static entity. in the npsnet system, all of the active entities have beendivided into four general categoriescomputer hardware and software for the generation of virtualenvironments284virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.based on the control mechanism: expert system, scripting system, networkentity, and driven entity. recently, the term computer generated forces (cgf)has been developed to group all entities that are under computer control into asingle category. in npsnet, the entities controlled by both the expert systemand scripting system are part of this category. the controlling mechanisms ofthe expert systems and autonomous players are briefly discussed below.the expert system is capable of executing a basic behavior when astimulus is applied to an entity. within npsnet it controls those entities thatpopulate the world when there are an insufficient number of human ornetworked entities to make a scenario interesting. these added entities arecalled noise entities. the noise entity expert system has four basic behaviors:zigzag paths, environment limitation, edge of the world response, and fight orflight. these behaviors have been grouped by the stimuli that causes thebehavior to be triggered. the zigzag behavior uses an internal timer to initiatethe behavior. environment limitation and edge of the world response are bothdependent on the location of the entity in the database as the source of stimuli.the fight or flight behavior is triggered by external stimuli.the purpose of an autonomous force is to present an unattended, capable,and intelligent opponent to the human player at the simulator. in npsnet, theautonomous force is broken down into two components: an observer modulethat models the observation capabilities of combat forces and a decision modulethat models decision making, planning, and command and control in a combatforce. the autonomous force system employs battlefield information, tacticalprinciples, and knowledge about enemy forces to make tactical decisionsdirected toward the satisfaction of its overall mission objectives. it then usesthese decisions in a reactive planning approach to develop an executable planfor its movements and actions on the battlefield. its decisions includedistribution of multiple goals among multiple assets, route planning, and targetengagement. the autonomous force represented in this system consists of acompany of tanks. the system allows for cooperation between like elements aswell as collaboration between individuals working on different aspects of a task.the observer module, described by bhargava and branley (1993), acts asthe eyes and ears of the autonomous force. in the absence of real sensors, theobservation module uses probabilistic models and inference rules to generatethe belief system of the autonomous force. it accounts for battlefield conditions,as well as the capabilities and knowledge of individual autonomous forces, todetermine whether and with how much accuracy various events on thesimulated battlefield can be observed. the system converts factual knowledgeabout the simulated environment intocomputer hardware and software for the generation of virtualenvironments285virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.a set of beliefs that might correspond to the beliefs that a real combat forcemight form under the given conditions. it does so by combining the agent'sobservations with evidence derived from its knowledge base and inferenceprocedures.hypermedia integrationif one considers threedimensional ves as the ideal interface to a spatiallyorganized database, then hypermedia integration is a key technologicalcomponent. hypermedia consists of nonsequential media grouped into nodesthat are linked to other nodes. if we embed such nodes into a structure in avirtual world, the node can be accessed, and audio or compressed videocontaining vital information on the layout, design, and purpose of the buildingcan be displayed, along with historical information. such nodes will also allowus to make a search of all other nodes and find related objects elsewhere in thevirtual world.we also envision hypernavigation, which involves the use of nodes asmarkers that can be traveled between, either over the virtual terrain ataccelerated speeds or over the hypermedia links that connect the nodes. thinkof rabbit holes or portals to information populating the virtual world.hypermedia authoring is another growing area of interest. in authoring mode,the computer places nodes in the ve as a game is played. after the game, theplayer can travel along these nodes (which exist not only in space but also intime, appearing and disappearing as time passes) and watch a given player'sperformance in the game. authoring is especially useful in training and analysisbecause of this ability to play back the engagement from a specific point ofview. some examples of the uses of hypermedia in virtual worlds are presentedin the following paragraphs.one application is the extension of hypermedia to npsnet (zyda et al.,1993a). hypernpsnet combines virtual world technology with hypermediatechnology by embedding hypermedia nodes in the terrain of the virtual world.currently, hypertext is implemented as nonsequential text grouped into nodesthat are linked to other text nodes. the npsnet group also has embeddedcompressed video (quicktime and moviemaker) into its worlds. this videocontains captured video of the world being represented geometrically. thus itprovides information not easily represented or communicated by geometry.in another application, the university of geneva has a project under wayentitled "a multimedia testbed" (de mey and gibbs, 1993), in which an objectoriented test bed for multimedia is presented. this is a test bed for prototypingdistributed multimedia applications. the test application of that software is avirtual museum. the museum is a threedimensionalcomputer hardware and software for the generation of virtualenvironments286virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.geometric structure, the barcelona pavilion, in which is embedded variousmultimedia objects, compressed video, audio, and still imagery.hardware and software issues to be addressedin all likelihood, the main shortterm research and development effort andcommercial payoff in the ve field will involve the refinement of hardware andsoftware related to the representation, simulation, and rendering of visuallyoriented synthetic environments. this is a natural and logical extension ofproven technology and benefits seen in such areas as general simulation,computeraided design and manufacturing, and scientific visualization.nevertheless, the development of multimodal synthetic environments is anextremely important and challenging endeavor. independent of the fundamentalpsychophysical issues and device design and development issues, multimodalinteractions place severe and often unique burdens on the computationalelements of synthetic environments. these burdens may, in time, be handled byextensions of current techniques used to handle graphical information. theymay, however, require completely new approaches in the design of hardwareand software to support the representation, simulation, and rendering of worldsin which visual, auditory, and haptic events are modeled. in either case, thegeneration of multimodal synthetic environments requires that we carefullyexamine our current assumptions concerning ve architectural requirements anddesign constraints.in general, multimodal ves require that object representation andsimulation techniques now represent and support the generation of informationrequired to support auditory signal generation and haptic feedback (i.e.,rendering). both of these modalities require materials and geometric (i.e.,volume) information that is not typically incorporated into today's surfaceoriented geometric models. consequently, volumetric approaches may becomemore attractive at all three levels of information handling (i.e., representation,simulation, and rendering). not only may volumetric approaches facilitate therepresentation of the information needed for objects in multimodal ves but theymay also lend themselves to local interaction models of physics that are elegantand straightforward to implement (toffoli, 1983). in addition, hardware tosupport this form of physical simulation is starting to become available on suchmachines as the cam8 and the fx1 from exa corporation. these approacheshave been successfully employed in the modeling of fluid flow (frisch, 1987)and may point the way for future ve representation, simulation, and renderingapproaches.in addition, the concept of frame rate, both in terms of update ratecomputer hardware and software for the generation of virtualenvironments287virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.and acceptable lags previously discussed in this chapter, must be refined oraltered. display update rates, which may be perfectly adequate for visualonlysynthetic environments, are wholly inadequate when we consider the auditoryand haptic modalities. auditory events not only require rendering at ratesexceeding 40 khz but also have a temporal extent that may be measured inseconds. furthermore, in even moderately complex synthetic environments,several auditory events may need to be generated and spatialized at any giveninstant. update rate requirements for haptic events are also problematic andmay be viewed on two levels. the first level is that associated with renderingkinesthetic or gross position/force information. these events require an updaterate approaching 20 hz. at the second level, tactile information (e.g., texture)requires rendering rates measured in the hundreds of hz.from the perspective of temporal lag, the generation of ves that haveauditory and haptic displays also places unique burdens on computationalelements. visual, auditory, and haptic events must be displayed withoutunacceptable intermodal lags. although currently an area of research for thethree modalities, it is known that delays on the order of a few tens ofmilliseconds can cause undesirable perceptual effects (e.g., the decorrelation ofvisual and auditory stimuli such that they are perceived as belonging to separateevents). it is likely that the temporal alignment of visual, auditory, and hapticstimuli will need to be on the order of 10 ms. this number can potentially placeburdens on the computation system one order of magnitude greater thancurrently acceptable update rates of visualonly ve.how these issues and the numerous others that are likely to be encounteredwith further exploration are to be handled by computational systems is still anopen and important area for further research. clearly, however, more attentionmust be given to these issues from a computer hardware and softwareperspective. currently, auditory and haptic interactions are predominantlyhandled by devices outside the major computational (usually graphical)workstation. this approach makes the essential temporal correlation of trimodalstimuli even more problematic and costly. current generations of computerworkstations benefit tremendously from specialpurpose hardware that supportsthe rendering of graphical information. related hardware may also be needed tosupport the rendering of auditory and haptic events. in a like manner, specialpurpose hardware to support the representation and simulation of objects withina multimodal ve may be beneficial. multimodal ves put an even higherpremium on several issues and shortcomings associated with currentcomputational systems, such as: (1) veridical, realtime, physically inspired,simulation technology; (2) highbandwidth, lowlatency, inputoutputcapabilities; (3) multimodal representation and simulation informationcomputer hardware and software for the generation of virtualenvironments288virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.exchange formats and methodologies; and (4) short and longhaul informationnetworking technologies. finally, an area that presents both a challenge and thepromise of multimodal ves is the proper use of the three modalities in thecontrol of ves and other computational environments. certain metaphors (e.g.,pulldown menus), albeit flawed, have served us well in twodimensional,graphically oriented humanmachine interfaces. metaphors for threedimensional, multimodal systems will require further research.research needsthe state of the art in computer technology for the generation of ves isconstantly shifting. we have tried to define the edge of the currently accessibleand available technology and some of the difficult problems yet to be solved.we now turn to a discussion of the hardware and software needed to addressthese problems and to move the field forward.hardwareas noted in the opening section of this chapter, advances in graphics andcomputer hardware are key to the full realization of ves. the hardwarecapabilities available today have given researchers, entrepreneurs, andconsumers just a taste of virtual worlds and a promise of possible applications.because of the potentially wide appeal and the large variety of applications withdiffering performance requirements, it is important to continue hardwaredevelopment at several levels from the highend multimodal workstations to thelowend personal workstation with modest visualonly threedimensionalcapabilities. the following paragraphs detail some of the key technical needsgenerated by ves.there are several computer hardware requirements needed to support highend ve systems in the future. computer architectures that provide forapplications with high computational demands are devices for which we alreadyhave a requirement. these machines must have very large physical memories (>15 gbytes), multiple highperformance scalar processors, highbandwidth (>500 mbytes/s), lowlatency (< 0.03 s) mass storage devices, and highspeedinterface ports for various input and output peripherals. disk bandwidth is notexpected to improve significantly over the next few years. (disk bandwidth andsize thus arise as limiting factors in video on demand and hypermediaintegration in virtual worlds.) current projections suggest that workstationscapable of supporting 15 gbytes of physical memory might be availablesometime in 1994 but that the cost will be prohibitive for all but the most wellfunded research groups.computer hardware and software for the generation of virtualenvironments289virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.extensive computational and data management capabilities will also berequired. physical modeling and visualization computations will be the drivingforce behind this computational requirement. machines capable of 40 mflops orgreater are needed now for some problems. if we wish to add more data pointsor develop finer resolution models, we could easily use all of this level ofavailable computing power. it is important to encourage the actual productionof required machines.there is not just one computer architectural requirement. different vesystems require different computer architectures. some systems require a fewparallel, highpower scalar processors with large shared memories; othersrequire large numbers of cpus operating in parallel (cpus perhaps withoutscalar processing capabilities). this flexible requirement for cpu configurationmay not be possible if the majority of the hardware fabricators move into thepc clone business.extrapolating current trends, we expect that ve applications will saturateavailable computing power and data management capabilities for some time tocome. dataset size will be the dominant problem for an important class ofapplications in ve. in the near term, an effective ve platform would includemultiple fast processors in an integrated unit; several graphics pipelinesintegrated with the processors; very large shared physical memory; very fastaccess to mass storage; operating systems that support shared memory withinmultiprocessed architectures; and very highspeed, lowlatency networks.to ensure continued development, it is necessary to encourage bothprivate and publicsector participation. a key concern is that the number ofserious research and development efforts associated with ve design andimplementation are decreasing. for example, the number of commercialcompanies in the business of producing special architectures for highperformance graphics rendering systems has decreased since 1988 to only a fewtoday. furthermore, the university of north carolina is perhaps the onlysignificant universitybased computer graphics hardware research group in theunited states that is still working. these developments have three importantimplications. first, with a small number of participants it is possible that fewerideas will be generated. second, the pace of development may be slowed andthe cost to consumers may remain high due to lack of competition. third, thosefew companies currently producing hardware for ve research and applicationmay turn their interests to other technology areas that for the moment mightappear more lucrative, such as video games or the television of the future. suchcould be the consequence if corporate america continues its trend toward highyield, shortterm investment rather than the lower yields over the long term.computer hardware and software for the generation of virtualenvironments290virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.softwaremost current se systems are built using commercial workstations runningsome variant of the unix operating system (which was not originally designedto meet realtime performance requirements). other approaches are based onusing collections of more specialized embedded computational elements(possibly with a generalpurpose workstation acting as the "front end") runningoperating systems that have been designed to support realtime, distributedcomputation. the latter approach has been extensively used in fields havinghard realtime requirements, such as process control and telerobotics, and isdiscussed in some detail in chapter 9 of this report.from the perspective of using commercial workstations and their powerfulgraphical capabilities, however, the committee feels that what is needed iseither a new operating system (os) architecture specifically designed forsynthetic environment (se) applications or upgrades and enhancements ofexisting operating systems. the operating system capabilities required forvirtual environments include: support of very large numbers of lightweightprocesses communicating via shared memory, support of automatic andtransparent distribution of tasks to multiple computing resources, support oftimecritical computation and rendering, and very high resolution time slicingand guaranteed execution for highpriority processes (to within 0.001 sresolution). although not specifically addressing all of these concerns, theefforts of the institute for electrical and electronics engineers (ieee) posixstandards committee are starting to bring realtime capabilities to the opensystem workstation environment. in particular ieee standard 1003.4 (on realtime extensions to unix), standard 1003.4a (on threads extensions to unix),and standard 1003.13 (on application environment profiles for realtimeapplications support) are important to se developers requiring some level ofrealtime control. substantial subsets of the capabilities specified in thesestandards are now available on some graphics workstations (e.g., sgiworkstations running the irix 5 version of the operating system). supportingthese capabilities in the operating system will significantly facilitate thedevelopment of many se applications, especially larger, more ambitious efforts.government funding for the development of such a ve operating systemor upgrade should be accompanied by a plan to shift this new system tocommercial sponsorship. in the past, the federal government has funded aconsiderable amount of operating system research, much of which has nevermade the transition from university research project to commercial viability. tomake sure that ve systems are written using an appropriate operating system, areal, financially sound transition plan forcomputer hardware and software for the generation of virtualenvironments291virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.specially designed or upgraded operating systems must be formulated, funded,and executed.a second critical need is for large, multiyear, basic science programscreated for developing largescale ves. current trends in computer scienceresearch funding are for small university research grants with a typical size ofapproximately $400,000 over three years. the great majority of ve researchersreceive significantly fewer grant funds. in addition, the trend in researchfunding for most agencies is toward the funding of projects with firmdeliverables and schedules. with these constraints we believe that the level ofexperimentation researchers are willing or able to engage in will decrease, and,as a result, we cannot hope to see major advances in the technology. anexample of the problem is the tendency of some government agencies to divertfunds from large software development projects to impressive technologydemonstrations. although such demonstrations are appropriate for theculmination of significant basic research projects, they have most recently beenused for rushed presentations in which the software was pieced together over afew months rather than carefully planned and designed over three to four years.there is an important need for the funding of large software developmentprojects in which the goal is the development of large, openended, networkedves. it is critical to concentrate funding on the basics. an additional trend ofconcern in the research funding arena is that exemplified by the technologyreinvestment program (trp). much of the nation's research dollars are movinginto the trp, which requires that universities take on a corporate partner. suchmoves lock up research results in proprietary agreements and diminish thelikelihood of shared research results.interaction softwarethe general problem of inferring user intentions so as to provide a naturalinterface for all tasks in a threedimensional ve is an area requiring a great dealof further research. because language, which provides much of the intentionalinformation in the real world, is not currently available for use in virtual worlds,other options must be thoroughly explored and developed. unfortunately, thesize of current government research efforts for work on providing naturalinterfaces is small. one government agency recently indicated that their entireve human computer interface budget was approximately $150,000; anotherprogram in ve interfaces was funded at $2.5 million, with the goal ofsupporting six universities over one year. the rationale for these limited effortsis the belief that much of the interface research will be funded and carried outcomputer hardware and software for the generation of virtualenvironments292virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.by industry. however, it seems likely that, once the novelty wears off, industryinterest will wane. thus it is unlikely that the private sector will take on longterm development efforts in the absence of standards.nevertheless, highlevel interface issues should be explored. specifically,research should be performed to examine how to use data measuring thepositions of the user's body for interaction with the ve in a way that trulyprovides the richness of realworld interaction. critical concerns are how toapply user tracking data and how to define objects in ve to ensure naturalinteraction.one of the major research challenges that has both hardware and softwareimplications is the continued use of the rs232c interface for control devices.current workstation technology typically provides one or two such ports.control devices are usually attached to these ports, with commands sent via theunix write system call. there is a speed limitation on the use of these ports, alimitation often seen as latency in input response. it is not uncommon to hear 70ms touted as the fastest response from the time of input device movement to thereporting of the change back to the application running on the workstation. that70 ms is too long a delay for realtime interaction, for which a maximum of 10ms is more appropriate. and there is the additional problem with unix systemsoftware layers that must be traversed for events to be reported back to theconcerned ve application.current workstation manufacturers do not focus on the design of such highspeed ports. even within one manufacturer there is no guarantee that such portswill behave consistently across differing models of workstations. real standardsand highly engineered ports are needed for control devices. in fact, arevolutionary redesign and restandardization of the input port is required ifcontrol devices are to take off. in addition, we need to rethink the layers of vesystem architecture.visual scene navigation softwaregiven the current workstation graphics polygon filling capabilities and theextrapolation of those speeds into the future, software solutions will be neededto reduce the total number of graphics primitives sent through the graphicspipeline for some time to come. the difficulty of polygon flow minimizationdepends on the composition of the virtual world. this problem has historicallybeen approached on an applicationspecific basis, and there is as yet no generalsolution. current solutions usually involve partitioning the polygondefinedworld into volumes that can readily be checked for visibility by the virtualworld viewer. there are many partitioning schemesšsome of which work onlyif the world description does not change dynamically. we need to encourageresearchcomputer hardware and software for the generation of virtualenvironments293virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.into generalizing this software so that dynamically changing worlds can beconstructed.furthermore, there is a need to encourage the funding of research to reacha common, open solution for polygon flow minimization. current researcherswho have tackled polygon flow minimization have closely guarded theirdeveloped code. in fact, most software source code developed under universityresearch contract today in the united states is held as proprietary by theuniversities, even if that code was developed under government contract. thisfact, coupled with the stated goal of federal agencies of recouping investments,is counterproductive and disturbing. the unavailability of such softwareincreases the overall development time and cost of progress in technology, asresearchers duplicate software development. these redevelopment efforts alsoslow the progress of new development.there are additional technical issues in polygon flow minimization that areimportant. one of these issues, the generation of multiple resolution threedimensional icons, is a closely related technological challenge. in much of thework of polygon flow minimization, it is assumed that multiple resolutions,lower polygon count, and threedimensional icons are available. thisassumption is a large one, with automatic methods for the generation ofmultipleresolution threedimensional icons an open issue. there is some workin this area, and it is recommended that a small research program be developedto encourage more (dehaemer and zyda, 1991; schroeder et al., 1992; turk,1992). in fact, the development of such public software and a public domain setof threedimensional clip models with geometry and associated behavior couldgo a long way toward encouraging the creation of threedimensional ves.modelingsimulation frameworks research into the development of environments inwhich object behavior as well as object appearance can rapidly be specified isan area that needs further work. we call this area simulation frameworks. sucha framework makes no assumptions about the actual behavior (just as graphicssystems currently make no assumptions about the appearance of graphicalobjects). a good term for what a simulation framework is trying to accomplishis metamodeling. such frameworks would facilitate the sharing of objectsbetween environments and allow the establishment of object libraries. issues tobe researched include the representation of object behavior and how differentbehaviors are to be integrated into a single system.geometric modeling because geometric modeling is integral to theconstruction of ves, its current limitations serve as limits to development. ascomputer hardware and software for the generation of virtualenvironments294virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.a practical matter, the ve research community needs a shared open modelingenvironment that includes physical and behavioral modeling. the current stateof the art in ve technology is to use available cad tools, tools more suited totwodimensional displays. the main problem with cad tools is not in gettingthe threedimensional geometry out of the cad files but rather the fact that datarelated to the actual physics of the threedimensional objects modeled by thecad systems are not usually present in such files. in addition, the partitioninginformation useful for realtime walkthrough of these data usually has to beadded later by hand or back fed in by specially written programs. cad filesalso have the problem that file formats are proprietary. an open ve cad toolshould be developed for use by the ve research community. this tool shouldincorporate many of the threedimensional geometric capabilities in currentcad systems as well as physics and other verelevant parameters (i.e., threedimensional spatial partitioning embedded into the output databases). it shouldalso capture parameters relevant to haptic and auditory channels.visionbased model acquisition although cad systems are useful forgenerating threedimensional models for new objects, using them can betedious. currently, modelers sit for hours detailing each door, window, and pipeof a threedimensional building. ves could be much more widely used if thispainful step could be automated, perhaps via laser range finders and the right''surface generation to cad primitive" software. unfortunately, there is the veryhard multiple view, laser range image correlation problem. automatic modelacquisition would be a good first step toward providing the threedimensionalobjects for virtual worlds. however, the physics of the objects scanned wouldstill need to be added. this technology has many uses beyond developing ves.an additional application area of high interest is providing cad plans for olderbuildings, structures designed and constructed before the advent of cadsystems.augmented reality realtime augmented reality is one of the tougherproblems in ves research. the two major issues are (1) accurate measurementof observer motions and (2) acquisition and maintenance of scene models. theprospects for automatic solutions to the scene model acquisition andmaintenance were discussed above. the problems with measuring observermotion are more difficult and represent a major research area. although vedisplays provide direct motion measurements of observer movement, these areunlikely to be accurate enough to support highquality augmented reality, insituations in which real and synthetic objects are in close proximity. even verysmall errors could induce perceptible relative motions that could disrupt anillusion. perhaps the mostcomputer hardware and software for the generation of virtualenvironments295virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.promising course would be to use direct motion measurements for grosspositioning and to use local imagebased matching methods to lock real andsynthetic elements together.technical appendixgraphics architectures for ve renderingthe rendering operation has three stages: preprimitive, rasterization, andprefragment. because of the performance demand, all modern highperformance graphics systems are run on parallel architectures. to allow themanytomany mapping, the parallel rendering pipes must be combined at onepoint along their paths. the three possible locations for the crossbar areillustrated in figures 86 through 88. the primitive crossbar (figure 86)broadcasts windowcoordinate primitives from the engines that transformed andlighted them to the one or more rasterization engines that correspond to framebuffer regions that each primitive intersects. depending on the windowcoordinate size of a primitive, it might be processed by just one rasterizationengine, or by all of the rasterization engines. thus this crossbar is really a onetomany bus.the fragment crossbar (figure 87) is a true, onetoone crossbarconnection. each fragment that is generated by a rasterization engine is directedto the one fragment processor that manages the corresponding pixel in the framebuffer. thus the fragment crossbar is itself more easily parallelized than theprimitive crossbar, allowing for the necessarily greater bandwidth of rasterizedfragments over windowcoordinate primitives. the primary disadvantage of thefragment crossbar comparedfigure 86 primitive broadcast.computer hardware and software for the generation of virtualenvironments296virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.with the primitive crossbar is that fragment crossbar systems have difficultyrendering primitives in the order that they were presented to the graphicssystem, whereas primitive broadcast systems easily render primitives in theorder presented.figure 87 fragment crossbar.whereas the frame buffers in the primitive broadcast and fragmentcrossbar systems were disjoint, collectively forming a single, screensize buffer,the frame buffers of a pixel crossbar system (figure 88) are each complete,screensize buffers. the contents of these buffers are merged only after all ofthe primitives have been rendered into one of the buffers. the primaryadvantage of such a system over primitive and fragment crossbars is that pixelmerge, using the zbuffer algorithm to choose the final pixel value, is infinitelyextensible with no performance loss. again,figure 88 pixel merge.computer hardware and software for the generation of virtualenvironments297virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.the term crossbar is misleading, since the pixel merge can be accomplishedwith onetoone paths between adjacent buffer pairs.the primary disadvantage of pixel merge systems is the requirement forlarge, duplicate frame buffers. a secondary disadvantage exists only withrespect to primitive broadcast systems: the pixel crossbar, like the fragmentcrossbar, has difficulty rendering primitives in the order presented. (each pathrenders the primitives presented to it in the order that they are presented, but thepostrendering pixel merge cannot be done in order.)the primary disadvantage of frame buffer size can be mitigated byreducing the size of each frame buffer to a subregion of the final, display buffer.if this is done, the complete scene must be rendered with multiple rasterizationpasses, with the subbuffers being merged into the final display buffer (which isfull size) after each pass is completed. application of such a multipasstechnique introduces the second differentiator of parallel graphics systems:whether the rendering is flowthrough or tiled. flowthrough systems completethe processing of each primitive soon after that primitive is presented to therendering system, in which "soon" is a function of the number of processingsteps. tiled systems accumulate all the primitives of a scene after the perprimitive processing is complete, then begin the rasterization and perfragmentprocessing. they must do this because frame buffer tiles are allocatedtemporally rather than spatially, and so are not available in the randomsequence that the primitives arrive in. the primary disadvantage of tiledsystems over flowthrough systems is therefore one of increased latency, due tothe serialization of the processing steps.the third major differentiator is image quality: does the architecturesupport mapping images onto geometry (texture mapping), and is the samplingquality of both these images and the geometry itself of high quality (antialiasing)? this differentiator is less one of architecture than of implementationšprimitive, fragment, and pixel crossbar systems, both flowthrough and tiled,can be implemented with or without texture mapping and antialiasing.the final differentiator is performance: the number of primitives andfragments that can be processed per second. again this differentiator is less oneof architecture than of implementation, although at the limit the pixel mergearchitecture will exceed the capabilities of primitive broadcast and fragmentcrossbar architectures.now we consider the architectures of four modern graphics systems, usingthe previously discussed differentiators. the silicon graphics realityengine isa flowthrough architecture with a primitive crossbar. it therefore is able toefficiently render primitives in the order that they are presented and has lowrendering latency. realityengine supports texture mapping and antialiasing ofpoints, lines, and triangles and therefore iscomputer hardware and software for the generation of virtualenvironments298virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.considered to have high rendering image quality. realityengine processes up to1 million texture mapped, antialiased triangles/s, and up to 250 million texturemapped, antialiased fragments/s. it is able to generate 1,280 × 1,024 scenes ofhigh quality at up to 30 frames/s.freedom series graphics from evans & sutherland use a flowthrougharchitecture with a fragment crossbar. thus freedom machines also have lowrendering latency, but are less able than the realityengine to efficiently renderprimitives in the order that they are presented. freedom machines supporttexture mapping and can antialias points and lines, but they are unable toefficiently antialias surface primitives such as triangles. hence the renderingquality of freedom machines for fullframe solid images is relatively low.although exact numbers for freedom fragment generation/processing rates arenot published, the literature suggests that this rate for texturemapped fragmentsis in the tens of millions per second, rather than in the hundreds of millions. ifthat is the case, then the performance of freedom graphics is not sufficient togenerate 1,280 × 1,024 images at even 10 frames/s, the absolute minimum forinteractive performance.pixel planes 5, the currently operational product of the university of northcarolina's research efforts, uses a tiled, primitive crossbar architecture. becausethe architecture is tiled, the advantage of ordered rendering typical of primitivecrossbar systems is lost. also, the tiling contributes to a latency of up to 3frames, which is substantially greater than the singleframe latencies of thefreedom and the realityengine systems. the rendering performance, especiallythe effective fragment generation/processing rate, is substantially greater thaneither the freedom or realityengine systems, resulting in easily maintained1,280 × 1,024 30 frame/s image generation. however, pixel planes 5 cannotantialias geometry at these high rates, so the image quality is lower than that ofrealityengine.finally pixelflow, the proposed successor to pixel planes 5, is a tiled,pixel merge machine. thus it is unable to efficiently render primitives in theorder in which they are received, and the rendering latency of pixelflow isperhaps twice that of freedom and realityengine, though less than that of pixelplanes 5. pixelflow is designed to support both texture mapping and antialiasing at interactive, though reduced rates, resulting in a machine that canproduce highquality, 1,280 × 1,024 frames at 30 or even 60 frames/s.silicon graphics from the iris1400 to the realityengine 2silicon graphics, inc., a computer manufacturer, creates visualizationsystems with some of the more flexible and powerful digital media capabilitiescomputer hardware and software for the generation of virtualenvironments299virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.in the computer industry, combining advanced threedimensional graphics,digital multichannel audio, and video in a single package. silicon graphicssystems serve as the core of many ve systems, performing simulation,visualization, and communication tasks. in such a role, it is critical that thesystems support powerful computation, stereoscopic, multichannel videooutput, and fast input/output (i/o) for connectivity to sensors, control devices,and networks (for multiparticipant ves). textured polygon fill capability is alsoone of the company's strengths with respect to virtual worlds in that texturingenhances realism.in support of this role, silicon graphics has engaged in the development ofmultiple processing, graphics workstations at the leading edge of technologysince late 1983. a brief look at the graphics performance numbers of their highend systems since that time is warranted (table 81). those systems comprisethree generations, as described in the realityengine graphics paper (akeley,1993). the 1000, 2000, and g are first generation, the gtx, vgx, and vgxtare second generation, and the realityengine and realityengine2 are thirdgeneration. performance is listed for first, second, and thirdgenerationoperations for all these machines. notice that the curve for firstgenerationperformance falls off with second and thirdgeneration machines, because theyare not optimized for firstgeneration rendering.onyx realityengine2in january 1993, silicon graphics announced the onyx line of graphicssupercomputers, which incorporate a new multiprocessing architecture,powerpath2, to combine up to 24 parallel processors based on the mips r4400risc cpu, which operates at 150 mhz. i/o bandwidth is rated at 1.2 gbytes/sto and from memory, with support for the vme64 64bit bus, operating at 50mbytes/s.onyx systems can utilize up to three separate graphics pipelines based onthe new realityengine2 graphics subsystem. this new graphics system offers50 percent higher polygon performance than the original realityengineintroduced in july 1992. realityengine2 is rated at 2 million flat triangles/second and 900,000 textured, gouraud shaded, antialiased, fogged, zbufferedtriangles/s.the optional multichannel board enables users to take the frame bufferand send different regions out to different display devices. thus, a single 1.3million pixel frame buffer could be used either as a 1,280 × 1,024 display or asfour 640 × 512 displays. the multichannel option provides up to six separateoutputs.computer hardware and software for the generation of virtualenvironments300virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.table 81 performance history for sgi graphicsdepth buffered, lighted, gouraud shadeddepth buffered, lighted, gouraudshaded, antialiased, texture mappedsystemdatepointstrispixelstrispixelstrispixels100019830.060.00140n/an/an/an/a200019840.070.01460.00080.1n/an/ag19860.140.011300.0032.0n/an/agtx19880.450.135800.13540n/an/avgx19901.51.12000.8100(0.08)(10)vgxt19911.51.12000.8100(0.08)(50)re19922.01.43801.43800.6250re219933.02.03802.03801.0250note: the first three columns are for generic rendering, no depth buffer, no lighting or shading, and no texture mapping or antialiasing. all values are in millions per second(points, triangles, or rendered pixels). values in parenthesis are texture mapped but not antialiased. all these systems were introduced in the $50,000 to $100,000 range. tris =triangle meshes; pixels = pixel fill rate.computer hardware and software for the generation of virtualenvironments301virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.evans & sutherland freedom 3000evans & sutherland (e&s), and old line flight simulator company, hasrecently announced the freedom series of graphics accelerators targeted for thesun microsystems sparc 10 line of workstations. the freedom series offers awide range of performance levels: from 500,000 polygons per second for thefreedom 1000 to 3 million polygons per second for the freedom 3000. thefreedom series uses standard hardware and software interfaces to joinseamlessly with the sun microsystems environment. the freedom acceleratorsare programmable with sun's standard interfaces and are softwarecompatiblewith workstations currently available from e&s and sun.the freedom 3000 has 1,280 × 1,024, 1,536 × 1,280, and highdefinitiontv display formats. it also supports hardware texture mapping, including mipmapping, and resolutions up to 2,000 × 2,000. additional features supportedare: antialiased lines, dots, and polygons, alpha buffering, accumulationbuffering, 128 bits per pixel, and dynamic pixel allocation.the freedom 3000 contains the following technology: five proprietaryvlsi asic chips types using 0.8 µ cmos, a parallel array of programmablehighspeed microprocessors (dsps), a very fast, proprietary graphics bus (gbus) capable of speeds well beyond 3 million polygons/s, highspeed pixelrouting interconnection, highspeed access to frame buffer for image processing(up to 100 million pixels/s), and a pixel fill rate of 95 million pixels/s.graphics hardware from the university of north carolina,chapel hill: pixelplanes 4, 5, and pixelflowthe university of north carolina at chapel hill is one of the last schoolsstill developing graphics hardware. their efforts differ widely from what hasbeen attempted in the commercial world, since their work is more basicresearch than machine production. despite this research focus, the machinesdeveloped by fuchs, poulton, eyles, and their colleagues have been close to theleading edge of graphics hardware at each of their prototypical stages (fuchs etal., 1985, 1989). pixel planes 4 had a 27,000 polygons/s capability in 1988,with a followon machine pixel planes 5 shown first in 1991 with a 1 millionpolygons/s capability. the latest machine, pixelflow, is still under developmentbut shows great promise (molnar et al., 1992). it is expected to be working in1994.pixelflow and its graphics performance scalability are an important part ofthe future of highperformance threedimensional ves. pixelflow, anarchitecture for highspeed image generation, overcomes the transformationcomputer hardware and software for the generation of virtualenvironments302virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.and framebuffer access bottlenecks of conventional hardware renderingarchitectures (molnar et al., 1992). it uses the technique of image composition,through which it distributes the rendering task over an array of identicalrenderers, each of which computes a fullscreen image of a fraction of theprimitives. a highperformance imagecomposition network combines theseimages in real time to produce an image of the entire scene.image composition architectures offer performance that scales linearlywith the number of renderers. a single pixelflow renderer rasterizes up to 1.4million triangles/s, and an nrenderer system can rasterize at up to n times thisbasic rate. it is expected that a 128 renderer pixelflow system will be capableof a polygon rate approaching 100 million triangles/s.pixelflow performs antialiasing by supersampling. it supports deferredshading with separate hardware shaders that operate on composite imagescontaining intermediate pixel data. pixelflow shaders compute complexshading algorithms and procedural and imagebased textures in real time, withthe shading rate independent of image complexity. a pixelflow system can becoupled to a parallel supercomputer to serve as an intermediatemode graphicsserver, or it can maintain a display list for retainedmode rendering.computer hardware and software for the generation of virtualenvironments303virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.9teleroboticsthis chapter reviews issues and needs in telerobotics. a telerobot isdefined for our purposes as a robot controlled at a distance by a humanoperator, regardless of the degree of robot autonomy. sheridan (1992c) makes afiner distinction, which depends on whether all robot movements arecontinuously controlled by the operator (manually controlled teleoperator), orwhether the robot has partial autonomy (telerobot and supervisory control). bythis definition, the human interface to a telerobot is distinct and not part of thetelerobot. haptic interfaces that mechanically link a human to a telerobotnevertheless share similar issues in mechanical design and control, and thetechnology survey presented here includes haptic interface development.introductiontelerobotic devices are typically developed for situations or environmentsthat are too dangerous, uncomfortable, limiting, repetitive, or costly for humansto perform. some applications are listed below:underwater: inspection, maintenance, construction, mining, exploration,search and recovery, science, surveying.space: assembly, maintenance, exploration, manufacturing, science.resource industry: forestry, farming, mining, power line maintenance.process control plants: nuclear, chemical, etc., involving operation,maintenance, decommissioning, emergency.telerobotics304virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.military: operations in the air, undersea, and on land.medical: patient transport, disability aids, surgery, monitoring, remotetreatment.construction: earth moving, building construction, building and structureinspection, cleaning and maintenance.civil security: protection and security, firefighting, police work, bombdisposal.this chapter is divided into five sections, which represent one way ofcategorizing past developments in telerobotics:(1) remote manipulators(2) remote vehicles(3) lowlevel control(4) supervisory control(5) realtime computinga recent survey including these and other topics is provided by sheridan(1992a).relation to roboticstelerobots may be remotely controlled manipulators or vehicles. thedistinction between robots and telerobots is fuzzy and a matter of degree.although the hardware is the same or is similar, robots require less humaninvolvement for instruction and guidance than do telerobots. there is acontinuum of human involvement, from direct control of every aspect ofmotion, to shared or traded control, to nearly complete robot autonomy.any robot manipulator can be hooked up to a haptic interface and hencebecome a telerobot. similarly, any vehicle can be turned into a teleoperatedmobile robot. there are many examples in the literature of different industrialrobots that have been used as telerobots, even though that was not the originalintended use. for example, a common laboratory robot, the puma 560, hasfrequently been teleoperated (funda et al., 1992; hayati et al., 1990; kan et al.,1990; lee et al., 1990; salcudean et al., 1992). there have also been a numberof telerobots specifically designed as such, often with a preferred hapticinterface. the design issues for robots, telerobots, and haptic interfaces areessentially the same (although pennington, 1986, seeks to identify differences).often telerobots have to be designed for hazardous environments, which requirespecial characteristics in the design. industrial robots have most often beendesigned for benign indoor environments.why don't we do everything with robots, rather than involve humans intelerobotic control? we can't, because robots are not that capable. often there isno substitute for human cognitive capabilities for planningtelerobotics305virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.and human sensorimotor capabilities for control, especially for unstructuredenvironments. in telerobotics, these human capabilities are imposed on therobot device. the field of robotics is not that old (35 years), and the task ofduplicating (let alone improving upon) human abilities has proven to be anextremely difficult endeavor; it would be disturbing if it were not so. there is atendency to overextrapolate from the few superior robot abilities, such asprecise positioning and repetitive operation. yet robots fare poorly whenadaptation and intelligence are required. they do not match the human sensoryabilities of vision, audition, and touch, human motor abilities in manipulationand locomotion, or even the human physical body in terms of compact andpowerful musculature that adapts and selfrepairs, and especially in terms of acompact and portable energy source. hence in recent years many roboticsresearchers have turned to telerobotics, partly out of frustration.nevertheless, the longterm goal of robotics is to produce highlyautonomous systems that overcome hard problems in design, control, andplanning. as advances are made in robotics, they will feed through to better andmore independent telerobots. for example, much of the recent work in lowlevel teleoperator control is influenced by developments in robot control. often,the control ideas developed for autonomous robots have been used as thestarting points for slave, and to a lesser extent, master controllers. advances inhighlevel robot control will help in raising the level of supervisory control.yet the flow of advances can go both ways. by observing what is requiredfor successful human control of a telerobot, we may infer some of what isneeded for autonomous control. there are also unique problems in teleroboticcontrol, having to do with the combination of master, slave, and humanoperator. even if each individual component is stable in isolation, when hookedtogether they may be unstable. furthermore, the human represents a complexmechanical and dynamic system that must be considered.relation to virtual environmentstelerobotics encompasses a highly diversified set of fundamental issuesand supporting technologies (vertut and coiffet, 1985a, 1985b; todd, 1986;engelberger, 1989; sheridan, 1992b). more generally, telerobots arerepresentative of humanmachine systems that must have sufficient sensory andreactive capability to successfully translate and interact within theirenvironment. the fundamental design issues encountered in the field oftelerobotics, therefore, have significant overlap with those that are and will beencountered in the development of veridical virtual environments (ves). withinthe virtual environment, the humanmachine systemtelerobotics306virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.must allow translation of viewpoint, interaction with the environment, andinteraction with autonomous agents. all this must occur through mediatingtechnologies that provide sensory feedback and control. the humanmachineinterface aspects of telerobotic systems are, therefore, highly relevant to veresearch and development from a device, configuration, and humanperformance perspective.yet the realenvironment aspect of telerobotics distinguishes it from virtualenvironments to some extent. telerobots must: interact in complex, unstructured, physicsconstrained environments, deal with incomplete, distorted, and noisy sensor data, including limitedviews, and expend energy which may limit action.the corresponding features of virtual environments are more benign: form, complexity, and physics of environments are completelycontrollable. interactions based on physical models must be computed. virtual sensors can have an omniscient view and need not deal with noiseand distortions. the ability to move within an environment and perform tasks is notenergylimited.despite such simplifications, virtual environments play an important rolein telerobotic supervisory control. a large part of the supervisor's task isplanning, and the use of computerbased models has a potentially critical role.the virtual environment is deemed an obvious and effective way to simulateand render hypothetical environments to pose ''what would happen if"questions, run the experiment, and observe the consequences. simulations arealso an important component of predictive displays, which represent animportant method of handling large time delays. ve research and developmentpromises to revolutionize the field of multimodal, spatially oriented, interactivehumanmachine interface technology and theory to an extent that has not beenachievable in the robotics field. the two fields should therefore not be viewedas disparate but rather as complementary endeavors whose goals include theexploration of remote environments and the creation of adaptable humancreated entities.remote manipulatorsthis section reviews remote manipulators from standpoints of kinematics,actuation, end effectors, and sensors. specific examples of robotstelerobotics307virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.and telerobots in this review will tend to be drawn from more recent devices;some of the older telerobots are reviewed in vertut and coiffet (1985a). areview with similar categories is provided by sheridan (1992a).kinematicsin this section we describe the number of joints and their geometricallayout. some of the issues within kinematics are discussed below.general positioning capabilitiesa manipulator requires at least 6 degrees of freedom (dofs) to achievearbitrary positions and orientations. when a manipulator has exactly 6 dofs, itis said to be general purpose. examples include many industrial robots, such asthe puma 560, as well as a number of commercial telerobots (kraft, shilling,western electric, ise). the space shuttle's remote manipulator system (rms),designed by spar aerospace, is another example.if there are less than 6 dofs, the device is said to be overconstrained.often a task will require fewer dofs, such as positioning (x  y) and orientating(a rotation about the normal z axis) restricted to a plane (a 3dof task).another popular example is the scara robot geometry with 4 dofs; themotions are planar with an extra translation in the direction normal to the plane.a modified scara robot, to which one joint was added, is being used for hipreplacement surgery (paul et al., 1992). teleoperated heavy machinery usuallyis overconstrained; excavators have 4 dofs (khoshzaban et al., 1992).an important subclass of mechanisms is a spherical joint, for which 3rotations and no translations are required; this joint is useful in headneck andheadeye systems. an example is the headneck system described by tachi etal. (1989). a 2dof pantilt system is presented in hirose et al. (1992) andhirose and yokoyama (1992). other pantilt systems are reviewed by bedersonet al. (1992), who also proposed a novel headeye pantilt system employing aspherical motor. a 3dof paralleldrive headneck system (gosselin andlavoie, 1993) has the potential for very fast motion, with some limitations inrotations.redundancieswhen there are 7 or more dofs, the mechanism is underconstrained. theextra dofs may be used to fulfill secondary criteria (to general positioning),such as obstacle avoidance. there has been a lot of research in roboticsaddressing redundancy resolution. the human arm is a redundanttelerobotics308virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.7dof mechanism (not counting shoulder shrug). commercial examplesinclude the sarcos dextrous arm (jacobsen et al., 1990a, 1990b, 1991), therobotics research arm, and the omnidirectional arm (rosheim, 1990).laboratory examples include the langley laboratory telerobotic manipulator,the cesarm (jansen and kress, 1991), and the anthropomorphic teleexistence slave robot (tachi et al., 1989, 1990a, 1990b). the special purposedextrous + manipulator (spdm) designed by spar aerospace will have 8 dofs.for direct control by the human arm, an exoskeleton master with 7 dofscan be used to guide a slave 7dof manipulator. handcontrollers (groundbased systems) are usually 6dof devices. to control a redundant arm, eitherthe resolution is left to the discretion of the computer or an auxiliary control(such as a knob) must be manipulated.workspacethe term workspace describes the extent of the volume within which themanipulator may position the endpoint, relative to the size of the manipulator.certain manipulator geometries are known to offer superior workspaces(hollerbach, 1985; paden and sastry, 1988; vijaykumar et al., 1985);interestingly, these geometries are similar to the human arm geometry.serial versus parallel mechanismin a serial mechanism, joints are cascaded. the workspace is the union ofmotions of the joints, and hence is large. because a proximal link must carry theweight of distal links, these arms may be slower, heavier, and less strong. mostmanipulators (whether in robotics or telerobotics) are serial mechanisms,because a large workspace is often important.in a parallel mechanism, several independent linkages meet at a commonterminal link (the end effector). the workspace is the intersection of theindependent linkages, and hence is small. one independent linkage does notcarry the weight of the others, so these devices can be lightweight, strong, andfast. a prominent example of a parallel mechanism is the stewart platform usedin flight simulators. the human hand can also be viewed as a parallelmechanism; there are 5 independent 4dof linkages that can contact an object.a lot of recent research in robotics has focused on parallel mechanisms, toexploit their intrinsic advantages for specific tasks suited to their restrictedworkspaces. examples include a 6dof parallel manipulator to be teleoperatedfor excavation (arai et al., 1992), a 3dof microrobot based on beam bending(hunter et al., 1991), and 6dof parallel hand controllers (hayward et al.,1993; long andtelerobotics309virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.collins, 1992). landsberger and sheridan (1985) have designed a cabledrivenparallel mechanism. the paralleldrive hydraulic shoulder joint in hayward etal. (1993) uses actuator redundancy to increase the workspace.kinematic solvabilityfor serial mechanisms, the forward kinematics (find the endpoint positiongiven the joint angles) is easy, but the inverse kinematics (find the joint anglesgiven the endpoint position) is hard. the inverse kinematics is complicatedunless the mechanism has a special structure: either a spherical joint or a planarpair (tsai and morgan, 1985). almost all industrial robots have these specialstructures, but some for design convenience do not, such as the roboticsresearch arm, which has been used in teleoperation. because of fastcomputers, iterative techniques to solve the nonlinear kinematics can work inreal time.for parallel mechanisms, the reverse is true: inverse kinematics is easy, butforward kinematics is hard (waldron and hunt, 1991).actuationactuation comprises the force or torque source (henceforth called theactuator) and any transmission element to connect to a joint or link. theactuation is the primary determinant of performance (speed, accuracy, strength).a survey of actuators for robotics is presented by hollerbach et al. (1992).macrorobotsfor macro motion control, standard actuators are electric, hydraulic, orpneumatic. for smaller robots (human size and less), electric actuatorsdominate. for larger robots (e.g., cranes), hydraulic actuators dominate.electric motors and drives electric actuators are the most convenient,because the power source is an electric plug. for pneumatic or hydraulicsystems, air supplies and hydraulic supplies often make them much less easy toinstall and maintain. electric actuators, however, are weak relative to theirmass; hence payloads are not great.to amplify motor torque and couple highrev motors to lowrev joints,nearly all electrical motor drives employ some form of transmission element,primarily gears. thus nearly all commercial electric robots employ gears ofsome form. for space applications, the space shuttle rms employs special highratio hollow planetary gears (wu et al., 1993). these same gears will beemployed in the twoarmed spdm (mack et al., 1991).telerobotics310virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.the flight telerobotic servicer (fts) system produced by martin mariettaemployed harmonic drives (andary and spidaliere, 1993; ollendorf, 1990),which are also commonly employed in industrial robots (e.g., the roboticsresearch arm, asea robots). advanced spherical joint designs employinggears have been produced by rosheim (1990).yet gears bring serious drawbacks: substantial friction, backlash, andflexibility. the performance consequences are poor joint torque control, poorendpoint force control, reduced accuracy, and slower response. to overcomethese drawbacks, some attempts are made to model the gear dynamics so thatthey may be compensated for in a controller (armstronghelouvry, 1991).other attempts include better gear designs that reduce friction and losses;examples include the artisan arm (vischer and khatib, 1990) and the rotexmanipulator (hirzinger, 1987; hirzinger et al., 1993), which is meant for spacelaboratory teleoperation.cable or tendon drives (including belts) are another common way toreduce arm weight, by remote location of the actuators. a number of masterslave systems have been designed using cable drives. more recent examplesinclude the frhc from jpl (hayati et al., 1990; kan et al., 1990) and thewhole arm manipulator (salisbury et al., 1990) (both salisbury's design).nearly all multifingered robot hands employ tendon or cable routing; spaceconstraints preclude direct mounting of actuators at joints (jacobsen et al., 1986).another recent development is the integration of gears and actuators. forexample, rosheim (1990) has used miniature integrated lead screw mechanismsfor finger jointmounted actuators. similar systems, developed originally forrotex (hirzinger et al., 1993), are now commercially available (wittensteinmotion control gmbh). a related concept is the harmonic motor, in which therotor rolls along the inside of the stator (jacobsen et al., 1989). analogous toharmonic drives, with harmonic motors, high effective gear ratios can beobtained.to avoid problems with transmission elements, direct drive actuators androbots have become popular in the past decade to provide smooth andcontrollable motion (an et al., 1988; asada and youceftoumi, 1987).examples of direct drive telerobots include the cmu ddarm ii(papanikolopoulos and khosla, 1992) and meister (sato and hirai, 1988);meister is also an example of a direct drive master. advances in electricmotor technology continue, and particularization to robotics is a key toenhanced performance. one example of a new electric motor specificallydesigned for directdrive robotics is the mcgill/mit direct drive motor(hollerbach et al., 1993).an important new area of development in mechanism design is magneticbearings and levitation, which seek to avoid problems of transmission elements,including bearings as structural members. in principle,telerobotics311virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.devices with magnetic bearings should produce the smoothest motions. hollis(hollis et al., 1990) has designed a 6axis magnetically levitated wrist, whichcan be used either as a hand controller or as a robot end effector. salcudean(salcudean et al., 1992) has developed a teleoperated robot, in which the masteris a magnetically levitated wrist and the slave is an industrial 6axis robot(coarse positioner) and the end effector is a magnetically levitated wrist.because the wrist's range of motion is small, the hand controller is used in ratemode for large excursions and in proportional mode for fine motions. althoughnot magnetically suspended, a 2axis forcereflecting mouse was developed bysalcudean using the same actuator elements (kelley and salcudean, 1993).another area under development, related to microelectromechanicalsystems (mems), is electrostatic actuators. all the electric motors mentionedabove work by magnetic attraction. at small scales, the electrostatic effect ismore favorable than the magnetic effect (trimmer, 1989). by using small airgaps and many poles, powerful musclelike actuators are conceivable. in termsof what has been realized on the macro scale, higuchi has demonstratedlightweight but very strong linear electrostatic actuators (niino et al., 1993).hydraulic actuators hydraulic actuators offer the most strength for thesize. there are a number of commercial telerobot systems that are hydraulic,such as the kraft arm, the shilling arm, and the international submarineengineering (ise) arm. teleoperation of excavators (which are hydraulic) withhand controllers has also been pursued (e.g., khoshzaban et al., 1992). to someextent, hydraulics have received a bad reputation due to concerns about leakageand controllability. advances such as the sarcos dextrous arm are acounterpoint to these concerns.pneumatic actuators pneumatic actuators are intermediate betweenelectric and hydraulic drives, in terms of force produced for a given size andmass. there are very few highperformance robots powered pneumatically,because of control problems associated with the compressibility of air. perhapsthe most advanced example is the utah/mit dextrous hand master (jacobsenet al., 1986).micromotion actuatorsone of the more exciting new areas under development is micromotionrobotics, in which (macro) robots are able to position precisely down to 1nanometer (dario, 1992). as a counterpoint to simulated molecular docking(ouhyoung et al., 1988), these robots would actually be able to manipulatemolecules. the development of true microsize robots is still somewhere off inthe future, but the new area of microelectromechanicaltelerobotics312virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.systems (mems) holds promise for developing the proper components:structures, actuators, and sensors.for micromotion control, piezoelectric actuators dominate. they are usedin scanning tunneling microscopes (stms) and atomic force microscopes(afms). hollis has used a magnetically levitated hand controller to control anstm (hollis et al., 1990). a stacked actuator consisting of a linear voice coilmotor in series with a piezoelectric element was the basis for hunter'stelemicrorobot (hunter et al., 1991). hatamura (hatamura and morishita, 1990)has teleoperated a 6axis forcereflecting nanorobot whose individual axes areflexure elements activated by piezoelectrics; the masters are two bilateraljoystick mechanisms, and vision is provided by a stereo scanning electronmiscroscope (sem).shape memory alloy (sma) actuators hold considerable promise as acompact but powerful actuator source. various robotic mechanisms have beenproposed that incorporate sma actuators, including active endoscopes (dario etal., 1991; ikuta et al., 1988). a tactile stimulator employing cantilever arraysactivated by sma wires has been developed commercially (tini alloycompany). at the moment, two major drawbacks of sma are highly nonlineardynamics and slow response speed. recent developments by hunter (hunter etal., 1991) have sped up the response by 100 times and hold promise for thefuture.end effectorsmost end effectors on robots or telerobots are unremarkable, usually twojaw grippers or special purpose tooling. multifingered robot hands have beendeveloped to provide robots with the same dexterity as the human hand. themajor commercial examples are the stanford/jpl hand (salisbury, 1985) andthe utah/mit dextrous hand (jacobsen et al., 1986). master gloves have beenused to teleoperate particularly the utah/mit dextrous hand (hong and tan,1989; pao and speeter, 1989; rohling and hollerbach, 1993; speeter, 1992).forcereflecting multifingered masterslave systems have been developed byjacobsen et al. (1990a) and jau (1992). a 3dof hand partly inspired byprosthetics is the end effector for the sarcos dextrous arm.sensorssensor technologies for telemanipulators include sensors required tomonitor the internal mechanical state of the arm (joint angle sensors and jointtorque sensors), the external contact state (wrist force/torque sensors and tactilesensors), and proximity sensors. we do not cover visualtelerobotics313virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.sensors (cameras) and image processing. position trackers and inertial sensorsare reviewed in chapter 5.joint motion sensorsa review of traditional joint motion sensors is provided by desilva (1989).for rotary motion, common sensors are potentiometers, optical encoders,resolvers, tachometers, halleffect sensors, and rotary variable differentialtransformers (rvdts). for linear motion, common sensors are linear variabledifferential transformers (lvdts), linear velocity transducers (lvts), andposition sensitive detectors (psds).most of the rotary sensors listed above are analog sensors. potentiometersare not that favored because of noise and sensitivity problems, and they are hardto make small. for use in robot fingers and hand masters, compact halleffectsensors are used in the utah/mit dextrous hand and in the exos dextroushand master. the resolution is not high (0.2 deg), but is adequate for thesedevices. the vpl dataglove employs fiber optic sensing, but this effect is toocoarse to be really useful and there have been many complaints about theaccuracy of this system. resolvers and tachometers are suitable for largeractuators and joints, such as robot shoulders and elbows. rvdts are moderatein size but also have a moderate resolution.the trend is increasingly toward digital sensors. optical encoders offer thehighest resolution; for example, canon produces an incremental laser rotaryencoder with 24 bits of resolution. the sarcos dextrous arm has 18 1/2 bitincremental encoders. the trend is for rotary encoders to become less expensivewhile maintaining high resolution, to become more compact in size, and toprovide highresolution absolute joint angle readings (steve jacobsen, personalcommunication).for linear transduction, lvdts and lvts are common. resolutions in therange of 10100 nm are possible for lvdts. linear psds have been reported tohave resolutions in the range of 15 nm. digital linear sensors are beingdeveloped with a resolution of 2.5 nm (steve jacobsen, personalcommunication). the ultimate in highresolution linear measurement is ofcourse interferometry, for which resolutions of 0.1 nm are possible. anadditional consideration is the sampling rate while maintaining high resolution;charette et al. (1993) reported a 1 mhz rate. future developments should resultin reduced size of such sensors and increased use in micromanipulation.with increased resolution such as that provided by optical encoders, thecalculation of joint velocity and acceleration from positional data will becomemore accurate. this calculation is required for precise controltelerobotics314virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.and calibration. recent research has focused on how these derivatives are to becalculated (belanger, 1992; brown et al., 1992).joint torque sensorsstrain gauges are most commonly used for force and torque sensing; thereview by desilva (1989) is again relevant. typically some flexible structure isattached in series with a joint axis; an example is the sarcos dextrous arm,with torque sensors having a dynamic range of 1:2,000. autonomous calibrationof joint torque sensors was considered by ma et al. (1994). joint torque sensorshave been retrofitted to puma robots by pfeffer et al. (1989), to the stanfordarm by luh et al. (1983), and to a direct drive arm by asada and lim (1985).an instrumented harmonic drive for joint torque sensing was presented byhashimoto et al. (1991).displacement sensors may also be employed to measure joint torque; forexample, inductive sensors were employed by vischer and khatib (1990) and inrotex. a variable reluctance joint torque sensor is also discussed by desilva(1989). halleffect sensors on a cantilever system are employed to sense tendontension on the utah/mit dextrous hand. optical joint torque sensors usinglight emitting diodes have been developed by hirose and yoneda (1989).displacement sensors can have advantages over strain gauge sensors, such aslower cost and higher robustness, although the sensitivity is typically lower.the future will probably continue to see alternatives to, and a movement awayfrom, strain gauge sensing as micro positional sensors improve.another trend is the use of improved electric motor models to predicttorque accurately open loop. this may involve the design of new motors(hollerbach et al., 1993; wallace and taylor, 1991) or the reverse engineeringof existing motors (newman and patel, 1991; starr and wilson, 1992). when atransmission element is employed, one alternative is a careful characterizationof joint friction to compensate for its effect (armstronghelouvry, 1991).accurate knowledge of joint torque is very important for precise controland, in the context of teleoperation and haptic interfaces, for force reflection.despite this importance, very few manipulators actually have this capability.the development of torquecontrolled joints will be essential for higherperformance devices in the future.wrist force/torque sensorsan alternative, or a complement to, joint torque sensing is to employmultiaxis force/torque sensors, usually mounted at the wrist. such sensors havealso been used as haptic interfaces, such as the trackball ortelerobotics315virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.spacemouse. sensor technology is the same as that discussed under joint torquesensors, but the multiaxis characteristic offers substantial complications.most frequently, a 4beam arrangement in a maltese cross configurationhas been employed with strain gauges; commercial examples include the jr3sensor and the assurance technology sensor. a significant problem is crossaxis interference, due to nonlinear beam bending (flatau, 1973; hirose andyoneda, 1990); this effect may produce errors up to 3 percent. although morecomplex calibration may alleviate this effect, another approach is to usealternative structures. nakamura et al. (1988) proposed a boxlike sensor.yoshikawa and miyazaki (1989) proposed a threedimensional crossshapedstructure. another possibility is a membrane suspension (gerd hirzinger,personal communication).as an alternative to strain gauges, the use of optical sensing has beenproposed (hirose and yoneda, 1990; hirzinger and dietrich, 1986; kvasnica,1992). the most precise multiaxis force/torque sensor built to date employs amagnetically levitated wrist and optical sensors (tim salcudean, personalcommunication). the wrist is servoed to a null position, and a motor model isemployed to infer the forces and torques; hence there is no crosscoupling. thisidea is similar to that of the sundstrand accelerometers, the most accurate on themarket.there is considerable room for improvement in the market for commercial6axis force/torque sensors. sometime in the future we can expect accuracies ofaround 0.1 percent, including crosscoupling effects; this would represent aboutan order of magnitude improvement over those currently on the market.tactile sensorsthere have been a number of reviews of tactile sensing technology (dario,1989; dario and de rossi, 1985; hollerbach, 1987; howe and cutkosky, 1992;nicholls and lee, 1989; pugh, 1986). many different effects have beenemployed; piezocapacitance, piezoresistance, and piezoelectrics are some of themore common ones. tactile sensors have also been produced using opticalsensing (maekawa et al., 1992). very large scale integrated (vlsi) fabricationmethods have also been employed to produce tactile sensors.commercially, piezoresistive tactile sensors were produced by the formerlord corporation and by barry wright controls division. piezoresistive inkshave been employed in the interlink electronics tactile sensors. very few otherexamples of commercially available tactile sensors may be found.hysteresis, sensitivity, and repeatability are often problems withtelerobotics316virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.piezoresistive sensors. piezocapacitance sensors circumvent some of theseproblems. piezoelectric sensors are temperature sensitive, often function only inan ac mode, and are hard to make very small because of crosstalk. othersensor technologies are often too complicated or fragile to make useful devices.the vast majority of tactile sensors sense only normal force. multiaxisstress sensors have been proposed by de rossi et al. (1993) and mccammonand jacobsen (1990). in the context of teleoperation and tactile stimulation,tactile sensors for normal force are probably adequate because tactilestimulators are likely only to be able to produce normal force.the bottom line is that, despite all the published work on tactile sensors,almost none is used on robots. the problems have to do with packaging, costand complexity, response properties, robustness, and suitability for curvedsurfaces such as fingertips. this is a technology area that needs considerabledevelopment, although economic drivers may not be in place for it.proximity sensorsproximity sensors, intermediate between contact sensors and visualsensors, are used for docking maneuvers of a manipulator end effector with anobject or target. they are particularly useful in teleoperation to account formodel discrepancies and to compensate for obstructed vision. for example,proximity sensors in the end effector of the rotex manipulator play animportant role.main technologies include electromagnetic, optical, and ultrasonicproximity sensors. electromagnetic sensors (induction, capacitance) are limitedin range and detectable materials (novak and feddema, 1992). ultrasonicsensors are not useful for shortrange measurements. hence most proximitysensing has hinged on optical reflectance sensors. a review of such sensors isprovided by espiau (1987). a challenge for these sensors is to separate theeffects of distance, surface orientation, and reflectance properties. multipledetectors are one way of inferring surface orientation (lee et al., 1990; okada,1982; okada and rembold, 1991; partaatmdja et al., 1992). an opticalproximity sensor based on the confocal principle has been reported by brenan etal. (1993).remote vehiclesremote vehicles, or mobile robots, encompass any basic transport vehiclethat can be operated at a distance: indoor motorized carts, road vehicles, offterrain vehicles, airborne or space vehicles, boats, and submersibles.telerobotics317virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.many mobile robots also will carry one or more remote manipulators. thissection highlights mobile robots exemplifying the current state of the art, majorissues involved in the development of mobile robotic systems, and remainingresearch and development challenges.systemsthe arguably perfect mobile robotic system would: (1) be easy to controlor program, (2) automatically transit an unstructured, highly complex, dynamicenvironment, (3) automatically perform general sensory and manipulative tasks,and (4) if required or desired, transmit detailed, easily interpreted, sensoryinformation describing its environment and task state in real time. it would becapable of performing these tasks for long periods of time, over long distances,and would not require a physical tether for either power or data transmission.unfortunately, such a system is not currently technically feasible or evenphysically achievable for some scenarios.environmental and physical factors attenuate data transmission bandwidthwith distance; platform design places constraints on the environments that canbe traversed; available energy systems limit endurance; sensors and sensorprocessing technology limit the type, form, and reliability of information aboutthe environment available to the robot; the state of the art in highlevel controllimits the robot's autonomous capabilities; and available computational deviceslimit how much sensor processing and highlevel control can be embedded inthe remote system. in addition, reliability, volume, and cost issues exert a stronginfluence on the designs of current mobile robotic systems. in spite of theseconstraints, however, highly successful mobile robotic systems have beendeveloped. these systems can be classified into four major physicalconfiguration classes based on different weighting of endurance,maneuverability, automation, and cost attributes.class 1 systems: power and data tetheredthe power and data tether that characterizes class 1 vehicles allows thesesystems to be optimized for endurance and cost. due to the use of a power anddata tether, mission duration is essentially unlimited and telemetry can be veryhighbandwidth and immune to noise, jamming, and occlusion. range,however, is limited by tether length and the tether is subject to entanglement. inaddition, combined power and data tethers tend to be bulky and can imparttremendous drag to the remote vehicle, thereby limiting maximum achievablespeed. as a rule, class 1 systems are teleoperated and have minimal onboardautomation (usually limitedtelerobotics318virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.to closedloop servocontrol of actuators). all major navigation and strategydecisions are made by a human operator using vehicle navigation, collisionavoidance, and scene understanding sensors. humanmachine interfaces forthese types of systems range from relatively simple collections of analog andsymbolic interface devices to more sophisticated systems with stereoscopicvideo feedback and forcereflecting manipulator controllers. simple shortrangeland vehicles and a majority of undersea vehicles are been exemplars of class 1vehicles.typical of class 1 vehicles are the dozens of lowcost, commerciallyavailable, remotely operated vehicles developed for undersea inspection or lightwork tasks to depths of a few thousand feet. an example is the hydrovisionltd. hyball undersea inspection system (busby associates, 1990). this small(.46 m × .51 m × .47 m), lightweight (39 kg) system has an onboard videocamera on a pan and tilt and work lights. the hyball can operate to 300 m andis operated using a simple video display and joystick. it can be outfitted with ascanning sonar, a lowlight level camera and auto altitude system.representing the high end of class 1 vehicles, the advanced tetheredvehicle (atv) (morinaga and hoffman, 1991; busby associates, 1990) is alarge (6 m × 3 m × 2.5 m, 5,000 kg) undersea work system developed by thenavy for general undersea repair and recovery tasks at full ocean depth. itcurrently holds the world depth record for a tethered vehicle (20,600 ft) and iscapable of speeds to 2 kn. its overall sensor and actuator complement includes:(1) four video camerasša stereo pair, a single camera with zoom lens on pan/tilt devices, and a fixed camera for position reference; (2) three manipulatorsštwo forcereflecting arms (6dof arm and 1dof gripper) and a simplegrasping device; (3) a scanning forward looking sonar; and (4) depth andheading sensors. navigation is augmented by a longbaseline acousticpositioning system. a sophisticated vanbased control system includes a vehicledriver and a manipulator or work system operator console. the manipulatoroperator console contains a stereoscopic panelmounted display and a pair ofhumansized, replicate, forcereflective master controllers. the vehicle driverconsole has access to video, sonar, and navigation information. the atvmus,which has a 1.2 inch diameter, 23,000 ft power and data tether, represents oneof the project's major contributions to class 1 underwater vehicles (freund,1986).class 2 systems: data tetheredclass 2 systems are highly maneuverable yet cost effective. remotevehicle power requirements and overall system costs are minimized by relyingheavily on the human operator for sensory and control decisionstelerobotics319virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.rather than onboard automation systems. access to remote vehicle actuationand sensory capabilities is provided by a tethered telemetry system. datatethers, typically fiber optic cables, are much less bulky than power tethers.these cables can support very highbandwidth, secure, nonjammable, nonlineofsight telemetry to ranges of over 100 km without repeaters, and they do sowithout imparting significant drag to the remote vehicle, since they can beactively or passively payed out from it. fiber optic cables cost approximately$12 a meter and, depending on the application, may not be reusable. due to thepossibility of cable entanglement or breakage, a nontethered, lowbandwidth,nonlineofsight telemetry system is frequently employed as a minimalcapability backup. mission duration of class 2 systems is limited due to therequirement to carry onboard energy sources. this class of mobile robots hashistorically received the most interest in the humanmachine interface area,since they are aimed at being highly maneuverable and capable yet stillpossesses continuous highbandwidth telemetry. some air vehicles, moreadvanced undersea vehicles, and most land vehicles capable of executingrealistic missions are exemplars of class 2 systems.an example of the latter is the department of defense unmanned groundvehicle program teleoperated vehicle (tov) system, an exterior, offroadcapable, surveillance robot (aviles et al., 1990). the remote vehicle platform isbased on the military's fourwheel drive utility vehicle, the high mobility multipurpose wheeled vehicle (hmmwv). it is configured as a modular, remotelyoperated mobile platform with a fiber optic data link to provide highbandwidthtelemetry out to 30 km. in addition to making all basic vehicle functionsremote, a stereoscopic camera pair and two artificial pinnae are mounted on apan and tilt platform to provide feedback for remote driving. navigationinformation is provided by a satellitebased navigation system that performsdead reckoning between satellite updates. up to three addon missionspecificsubsystems (mission modules) can be added to the base vehicle. the usualmission module is a reconnaissance, surveillance, and target acquisition systemthat includes a lowlight level video system, a forwardlooking infrared sensor,and a laser range finder and designator. all these sensors are mounted on a panand tilt platform on top of an extendable 15 ft scissors mast.the tov control station is mounted in a mobile shelter and is designed toprovide the human operator with a control and sensory experience as similar aspossible to normal, nonremote driving. the operator is provided with replicas ofthe hmmwv steering wheel, accelerator, brake, shifter, and ignition controls.feedback for driving is provided primarily by a stereoscopic headmounteddisplay (hmd) with a binaural headset. the remote vehicle camera pan and tiltis slaved to the operator's headtelerobotics320virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.motions while wearing the hmd, and basic navigation information is overlaidonto the operator's visual scene. the tov has been extensively tested in bothonroad and offroad conditions to 55 km/hr and can be remotely driven to thelimits of the basic platform.sometimes class 2 systems, such as the xp21, a modular underseavehicle developed by applied remote technology (busby associates, 1990),are used as test beds for autonomous control. the xp21mus data tetherallows use of powerful offboard computational systems and quickreconfigurability. the xp21 is approximately 5 m in length and .5 m indiameter, has a maximum speed of 6 kn, and a 40 mi cruise range. it can alsorun on preprogrammed missions without the tether.class 3 systems: nontethered telemetryclass 3 systems fall into two major categories. the first contains mobilerobots that have continuous, highbandwidth, lineofsight, nontetheredtelemetry systems. these mobile robots are equivalent to class 2 systems butare limiting their range of operation in order to remove the disadvantages of aphysical data tether. sometimes a class 2 system will be put in a configurationof this type for training purposes or shortrange missions and use cable only forextendedrange missions.the second category of class 3 systems represents a uniquely differentapproach to the development of mobile robots. onboard automation isemphasized in order to remove the physical data tether yet still be capable ofperforming longrange missions. class 3 systems have a telemetry connectionto their control station, but it is a lowbandwidth, nonlineofsight, connectionincapable of supporting direct manual control by the human operator. thesesystems typically exhibit at least supervisorylevel control and can oftenperform reasonably complex behaviors autonomously. the human's role is oneof a supervisor, giving highlevel commands to the remote vehicle andmonitoring its progress. this level of control is not only a goal for human/operator load reduction purposes but also is a requirement for stable control ofthe remote platform under telemetryinduced delays (ferrell, 1965; sheridan,1970). highresolution imagery is often selectively telemetered to the operatorat very low frame rates (on the order of seconds per frame). mission duration isstill limited by onboard energy systems but through intelligent powermanagement approaches this can be significantly extended. this class ofsystems has historically received reasonable interest in the humancomputerinteraction arena as relating to control partitioning and sharing (chu and rouse,1979), but relatively little attention has been paid to remote presenceapproaches. most air vehicles, more advanced undersea vehicles, some landvehicles, and planetary rovers have been exemplars of this type.telerobotics321virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.rocky iii (wilcox, 1992; desai et al., 1992) is a 15 kg, 6wheel, planetaryrover test bed developed at the california institute of technology's jetpropulsion laboratory. it has a 9,600 baud radio telemetry system, onboardcomputation, and a 3dof arm outfitted with a soft soil scoop. rocky iii's onboard batteries provide a 10hour mission duration. very simple navigation andcollision avoidance sensors are used. navigation is accomplished using agimballed fluxgate compass and wheel encoders for dead reckoning. collisionavoidance information is provided by sensors connected to the front wheels andto a skid plate for objects that go between the wheels. using the telemetrysystem, an operator designates a site to be sampled with the soft soil scoop andoptional intermediate waypoints. the rover then accomplishes its mission,including obstacle avoidance maneuvers, with no further communication.rocky iii's larger cousin, robby (desai et al., 1992), a 6wheeled 3body, 1,000kg, articulated vehicle, has demonstrated semiautonomous navigation through arough natural terrain at a rate of 80 meters per hour using both deliberative andreactive control paradigms on stereovisionprovided data.the mobile detection assessment and response system (mdars)program, a joint effort of the u.s. army's armament research developmentand engineering center and the navy's naval command, control and oceansurveillance center, has developed an interior, supervisory controlled, physicalsecurity robot as an adjunct to fixed security sensors (everett et al., 1990, 1993;laird et al., 1993). the 3wheel drive, 3wheel steered, remote platform is 6feet tall and weighs 570 pounds. it is outfitted with a 9,600 bit per secondbidirectional radio telemetry system, navigation sensors, and intruder detectionsensors. collision avoidance sensors include a 9element ultrasonic array andbumpermounted collision detectors. navigation is accomplished using a hybridnavigation scheme that combines compass/encoderbased dead reckoning and awallfollowing/reindexing system. the wallfollowing system updates andrefines the robot's computed position using an a priori map of static features inthe environment and readings from acoustic ranging sensors. intruder detectionsensors include a 360 deg, 24element ultrasonic array, microwave motiondetectors, passive infrared motion detectors, a video motion detection systemwith a nearinfrared light source, and nearinfrared proximity detectors. themobile platform can automatically follow preprogrammed or random paths andperforms automatic obstacle detection and avoidance maneuvers. it periodicallystops to look for intruders and alert a human supervisor when an onboardsecurity assessment system determines that an intruder is likely (smurlo andeverett, 1992). the operator then has the option of (1) ignoring the alert andordering the robot to continue its patrol, (2) asking the robot to get closer to thedetected object for evaluation using the onboard video camera, or (3) takingtelerobotics322virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.over control in a reflexive teleoperated mode (automatic collision avoidance).initial tests of the system in military warehouse environments havedemonstrated probabilities of detection well in excess of 0.90 with a very lowfalse alarm rate. the system is being extended to allow the supervision ofmultiple mobile platforms by one operator.in the underwater environment, the advanced unmanned search system(auss) (walton et al., 1993; uhrich and walton, 1993), developed by thenavy, is a supervisory controlled, broadarea, undersea search system. theremote vehicle is 17 ft long and 31 inches in diameter, weighs approximately2,800 lb, has an endurance of 10 hr and a maximum velocity of 5 kn, and canoperate to depths of 20,000 ft. an acoustic link transmits compressed searchdata from the vehicle at 4,800 bits/s and sends highlevel commands to thevehicle at 1,200 bits/s. the primary search sensor is a sidelooking sonar.electronic still and 35 mm film cameras provide imagery for identification.depending on the amount of compression desired, sonar and video images takefrom 20 s to 2 min to transmit. onboard navigation sensors include a forwardlooking sonar, a doppler sonar, gyrocompass, depth sensor, attitude sensors,and rate sensors. in addition, bottomdeployed longbaseline acoustictransponders and shipbased shortbaseline acoustic, loranc, and globalposition system (gps) navigation systems can be used to update the remotevehicle navigation system and to allow the surface support craft to maneuver tomaintain the acoustic telemetry link. in a typical scenario, the auss systemoperator commands the remote vehicle to execute a search path and supervisesthe system by monitoring vehicle position, status, and transmitted imagery. if anobject of interest is detected by the operator, the vehicle can be ordered toautomatically home in on the object and get higher resolution video imagery forevaluation.class 4 systems: nontethered, no telemetrythe final class of systems represents the perceived high ground of mobilerobotics research and development. the premium on onboard automation isextremely high, and the remote vehicle carries out its mission without requiringhuman monitoring or intervention. the human is involved only in programmingor specifying the desired highlevel behavior of the system and possibly inretrieving mission or sensory data after the mobile robot has returned from anexcursion. this means that all sensor regard control, interpretation, and thereasoning required to transit within the environment without collisions mustoccur on board. class 4 systems do not need a telemetry connection to theircontrol station and therefore can be highly maneuverable and operate to longdistances. mission duration, like class 2 and class 3 vehicles, is still limited byonboardtelerobotics323virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.energy sources but again can be extended tremendously by intelligent powermanagement. in addition to being an intellectual focus of the mobile roboticscommunity, class 4 systems can perform tasks for which maintaining telemetrywould be problematic or impossible, such as excursions deep inside ofstructures. this class of systems has historically received minimal attention inthe humanmachine interface area, since the overall effort is to limit humaninvolvement to goal specification at most. as yet, systems capable of rapidtransit in general, unconstrained environments without a priori knowledge ofthat environment do not exist. some very interesting systems that operate inmore constrained environments have been constructed, however.the carnegie mellon university (cmu) navlab and navlab ii (kanade,1992; mettala, 1992; thorpe, 1990) mobile ground robot test beds havedemonstrated impressive performance at road following and crosscountrytraversal. these vehicles navigate using sonar, gigahertz radar, and an erimlaser rangefinder. the best runs to date over moderate offroad terrain haveoccurred at 6 mph. alvinn, a neural network roadfollowing system, has beenused by the cmu researchers to drive the navlab ii up to 62 mph on highwaysand for a continuous distance of over 21 miles. the neural network is trained byobserving a human driver. image understanding for mobile robotic applications,as exemplified by the cmu work, is currently the focus of intense researchsponsored by the advanced research projects agency (1992).technologies and directionsalthough all the major technology areas depicted in figure 91 continue tobe the focus of intense research and development, the most significant andrelevant developments have been in the sensor, platform, actuator, highlevelrobotic control, and humanmachine interface fields.sensor systemsone of the classic problem areas constraining the development of mobilerobotic systems has been the development of sensors supporting navigation andcollisionfree transit through the environment. the platform must be able tonavigate from a starting position to a desired new location and orientation,avoiding any contact with fixed or moving objects en route. the difficulty canbe directly related to the requirement for the platform to move at reasonablespeeds and the unstructured nature of the operating environment.navigation sensors and systems major techniques for determining vehicleposition and orientation are dead reckoning, inertial navigation, beacons,telerobotics324virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.satellite navigation, and map matching. recent developments in small, lowcostinertial linear accelerometers and angular rate sensors and the maturation ofglobal positioning system (gps) technology, however, are of particular importfor mobile robot navigation and sensing. inertial and gps technologies arehighly complementary. inertial systems are very precise for short distances andshort times but are subject to longterm drift. gps systems, in contrast, providesomewhat less accurate but realtime updates over long periods of time and longdistances without drift.figure 91 mobile robot system and technology areas.standard gps technology provides a location solution with an accuracy ofabout ± 30 m. the use of differential gps techniques, through which correctionfactors from a fixed gps receiver station are transmitted to the mobile platformand its gps receiver, allow a positional accuracy of approximately ± 2 m. gpsreceivers are now available in both handheld and chipset versions, such as therockwell navcore v positioning system receiver engine. in addition,multiantenna systems such as those developed by trimble navigation use signalphase timeofarrival information to provide both orientation and more accurateposition information.siliconbased miniature accelerometers are challenging the dominance ofmore traditional rate and acceleration sensors, such as quartz beam or fiberoptic gyroscopes. in general, siliconbased microaccelerometerstelerobotics325virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.measure the displacement of a proof mass attached to a silicon chip in responseto inertial force by piezoresistive, capacitive, or resonant means (yun andhowe, 1992). for example, triton technologies inc. has developed anaccelerometer using capacitive sensing with 0.1 mg resolution, 120 db dynamicrange, and a crossaxis sensitivity of less than 0.001 percent (henrion et al.,1990). researchers at the berkeley sensor and actuator center have developeda capacitive accelerometer that integrates the sensor and readout electronics in a2.5 mm × 5.0 mm area (yun et al., 1992). pointing to potential futureperformance enhancements, researchers at jpl's center for space are exploringthe use of electron tunnel sensors for measuring acceleration. these systemshold the promise of surpassing the performance of capacitive sensors by fourorders of magnitude (vanzandt et al., 1992). magnetohydrodynamic angularrate sensors (laughlin et al., 1992) also show promise with high precision (>0.1 µrad) and dynamic range (> 100 db) at lowpower consumption (< 0.3 w).to date, however, sensors of this type capable of measuring constant input ratesare still under development.these advances in gps and inertial technology are allowing thedevelopment of lightweight, lowcost, integrated gps/inertial navigationsystems specifically targeted for remote vehicle applications. a prototypesystem, built by rockwell international, weighs 5.5 lb, draws 18 watts, and hasa volume of 115 cubic inches (griffin and murphy, 1992). a navcore v gpschip set and tactical grade inertial sensors using piezoelectric bender crystalsmounted on a motor shaft are utilized. initial positional error specifications are76 m sep (spherical error probable) with future systems slated to havepositional accuracies on the order of 15 m sep by using the military version ofgps and solidstate accelerometers.collision avoidance and scene understanding sensors and systemsacoustical, optical, and electromagnetic sensors using proximity, triangulation,time of flight, phase modulation, frequency modulation, interferometry, sweptfocus, and return signal intensity ranging techniques have been employed onmobile robots for collision detection and scene understanding purposes (everettet al., 1992). recent developments in millimeter wave radars and laser radarsare especially relevant to the mobile robotics community.millimeter wave (mmw) radar utilizes that portion of the electromagneticspectrum from wavelengths of approximately 500 µ to 1 cm. in theory, mmwsensing systems can have much higher resolution and fit into smaller packagesthan conventional radar systems in the microwave portion of theelectromagnetic spectrum (approximately 3 to 100 ghz) with some penalty inshorter operating distances and more attenuationtelerobotics326virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.by environmental factors. although, mmw radar systems are currently notwidely commercially available, several promising prototype systems have beendeveloped. examples of current mmw radar sensors are systems developed bymillitech, battelle, and kruthmicrowave. millitech has developed twoprototype sensors for robotic collision avoidance applications (millitech, 1989).the first sensor has a 30 × 30 degree field of view, a maximum range of 10 m, arange resolution of 1 cm, and an update rate of 100 hz. it is targeted atproviding the range to the nearest obstacle. the second sensor is designed toscan 360 degrees in azimuth, track multiple targets, and determine range andbearing for each. it has a maximum range of 100 m, a 2 degree azimuthresolution, a range resolution of 10 cm, and an update rate of 40 hz.researchers at battelle memorial institute have developed an mmw radarsystem for use as an automobile collisionavoidance sensor (wittenburg, 1987).it allows range, velocity, amplitude of returned signal, and angle to bedetermined for multiple targets. in an envisioned configuration for automotiveuse, it would scan a 30 deg sector at a 5 hz update rate with a 5 m rangeresolution out to 60 m and a 10 m range resolution out to 100 m. finally, kruthmicrowave electronics company has developed a lowcost, lowpowerprototype mmw radar system for unmanned system use (kruthmicrowaveelectronics, 1989). it has a theoretical range of 1 km and a range resolution ofbetter than 1 m, can detect targets of approximately 0.1 m crosssection, and canbe packaged in approximately onethird of a cubic ft volume.laser radar (ladar) or laser scanner technology for terrestrialapplications is now relatively mature and can be used for both obstacledetection and landmark identification (besl, 1988; hebert et al., 1988). systemsdeveloped by the environmental research institute of michigan (erim) andodetics exemplify the state of the art. the erim laser scanner, used bycarnegie mellon university on its navlab series of mobile robots, providesresolutions of 0.5 deg/pixel horizontal (80 deg field of view) by 0.3 deg/pixelvertical (30 deg field of view). it has a maximum unambiguous range of 20 m, arange resolution of 8 cm, and a 2 hz update rate. the odetics system provides0.5 deg/pixel over a 60 deg vertical and horizontal field of view. it has aresolution of 1.8 cm out to a range of 9.4 m.along a similar front, laserbased imaging systems for the underwaterenvironment are being developed (fletcher and fuqua, 1991). these systemstypically use lasers capable of producing energy in the blue/green spectrum(wavelengths in the range of 460 to 560 nm), which corresponds to a window ofminimum absorption in seawater. sparta incorporated has developed a rangegated imaging system that has dimensions of approximately 12 × 27 in, a 12deg field of view, a 10 hz updatetelerobotics327virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.rate; it weighs approximately 120 lb (neutral in water), and consumes 450 wattsof power (swartz, 1993). westinghouse has continued development and run seatrials on a blue/green laser line scanner with fields of view from 15 to 70 deg(gordon, 1993).platformsin the area of platform design, research and development on legged robotsis particularly important. in his overview of the field, raibert (1990) suggeststhat legged locomotion research is well advised, since only half of the earth'sland mass is navigable using current wheeled and tracked vehicles. in addition,not only is legged platform research attempting to develop platforms that cantraverse difficult terrain but also it has served as a focus for understandinghuman and animal locomotion. this work has led to a reasonable understandingof gait under a variety of regimes (song and waldron, 1989) and to a variety ofcommercial and research platforms. odetics inc. has built a series ofsupervisory controlled hexapods. its first legged platform, the odex i (russell,1983), weighed 370 lb, could lift 900 lb, and could walk onto a small truck bed.the adaptive suspension vehicle, a 6000 lb, 6legged, terrainadaptivesuspension vehicle developed at the ohio state university (waldron andmcgee, 1986; song and waldron, 1989) has a single human operator and a 5mph top speed and is capable of crossing 6 ft ditches, stepping over or ontoobstructions over 4.5 ft high, and navigating 60 percent grades. raibert at cmuand mit has developed a variety of monoped, biped, and quadruped test bedsincluding a planar biped capable of jumping through hoops and running atspeeds of over 13 mph (raibert, 1990). legged platform research anddevelopment is particularly relevant to the design and control of figures andautonomous agents in virtual environments (see badler et al., 1991).highlevel robotic controla relatively new and controversial approach to highlevel control formobile robotics is reactive control. this form of control and its subset, behaviorbased control, rely heavily on the immediacy of sensory information,channeling it directly to motor behaviors without the use of an interveningsymbolic representation of the world. more traditional strategies constructrepresentations or models of the world and then reason based on these modelsprior to acting (brooks, 1986, 1991; jones and flynn, 1993). approachesrelying on this traditional approach have typically been hampered by sensorproduced inaccuracies in the world model and the significant amount ofcomputation required to reason about the abstracttelerobotics328virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.model. reactive systems do not possess these two weaknesses because theybypass the creation of a world model altogether and directly couple sensoryperceptual activities with action (arkin, 1992). the ultimate limits on usefuladaptive behavior achievable using reactive control approaches are unclear. avariety of systems have been developed that perform in dynamic, unstructuredenvironments (brooks, 1990; connell, 1990; anderson and donath, 1991). forexample, brooks and his colleagues at mit have built a series of mobile robots,including a small 6legged robot (brooks, 1990) that uses simple pitch and rollsensors, passive infrared sensors, and whiskers to exhibit robust autonomouswalking, prowling, and directed prowling behaviors that avoid collisions. thislevel of competency is achieved without the generation of a symbolicrepresentation of the world.humanmachine interfacehumanmachine interfaces for mobile robots range from simple analog orsymbolic controls and displays maintaining minimal isomorphisms with thesystem to be controlled to highly immersive, spatially oriented, isomorphic,veridical humanmachine interfaces engaging the visual, auditory, and hapticsenses. interfaces of the latter type have typically been designed in order toprovide the human operator with some sense of remote presence or telepresenceproviding sensor feedback of sufficient richness and fidelity and controls ofsufficient transparency that human operators feel as if they are present at theremote site. this approach is typically taken in order to engage the human'snaturally evolved sensory, cognitive, and motor skills in the ways they are usedin everyday tasks so as to minimize task completion times and the trainingrequired to operate the remote system (pepper and hightower, 1984). thedefinition and efficacy of remote presence, however, are currently a majorresearch topic within both the robotics and virtual environment communities(held and durlach, 1992; sheridan, 1992b). the type and form of the humanmachine interface, although often not clearly separated, is orthogonal to thelevel of control (manual through autonomous) of the mobile robotic system.the type and form of the elements and level of information to be controlled,programmed, or monitored vary whether the mobile robot (or virtual entity in ave) is manually or autonomously controlled, but similar humanmachineinteraction approaches, symbolic to immersive, can be applied.although a complete overview is beyond the scope of this paper, mobilerobotics research and development on the humanmachine system is highlyrelevant to the ve community. not only have successful systems beendeveloped that attempt to create a sense of remote presencetelerobotics329virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.(aviles et al., 1990; morinaga and hoffman, 1991) but a wide body of researchoutlining the time, speed, accuracy, and configuration tradeoffs of differenthumanmachine configurations exists (sheridan, 1989, 1993; mcgovern, 1990;spain, 1992; heath, 1992).lowlevel control of teleoperatorsteleoperators are complex systems composed of the human operator,master manipulator (joystick), communication channel, slave manipulator, andthe environment (remote task). since each of these systems is complex in itsown right, in combination they create formidable analytical and designchallenges. in particular, when slave contact force information is fed back to theoperator through the master manipulator, the system becomes closed loop andthus stability is often a problem, even if each of the individual subsystems isstable in isolation. a related technology not considered here consists of ''manamplifiers" or "extenders" through which master and slave are effectivelycombined into one mechanism in direct contact with (or worn by) the human(kazerooni and guo, 1993).the most common controller for robot manipulators in practice remainsthe classic proportionalintegraldirective control (pid) compensator used onthe individual joint positions. experimental systems have become moresophisticated. for example, to accurately follow trajectories, robot controllersare greatly assisted by incorporating a dynamic model of the manipulator (luhet al., 1980; khosla, 1988).most teleoperators are used in tasks involving heavy contact with rigid ormassive environments. position and contact force cannot be simultaneouslycontrolled because their relation is constrained by the environment. thus, thetask space can be segmented according to the contact constraints into subspacesin which position and force are individually controlled (mason, 1981; raibertand craig, 1981). alternatively, position error and force error can be related toeach other through a stiffness constant (salisbury, 1980) to generate actuatorcontrol signals in what is called stiffness control; for a review of these methods,see whitney (1985). more generally, position and force can be related toactuator torque through a secondorder dynamic model representing the desiredmechanical impedance of the end effector (hogan, 1985a, 1985b, 1985c).although force reflective teleoperators have been in existence for morethan four decades (spooner and weaver, 1955; sheridan, 1960; goertz, 1964),there are still very few successful implementations of complex, highdofsystems that satisfy what we envision as the ideal system. although much of theperformance limitation is due to physical hardware insufficiencies, greatincreases in performance can be achievedtelerobotics330virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.through proper design and implementation of the embedded control strategiesused.the ideal performance of a teleoperator has been described as a massless,infinitely stiff mechanical connection between the input device (master), andthe effector (slave) (biggers et al., 1989; handlykken and turner, 1980).augmentation of the operator's sensory and motor skills, such as forcemagnification or displacement scaling, as well as compensation forenvironmental effects, such as gravity, are often additional design challenges(dario, 1992; flatau, 1973; hatamura and morishita, 1990; hollis et al., 1990;hunter et al., 1990; vertut and coiffet, 1985a). desired characteristics of thecoupled masterslave system include: low operator input impedance in free space inertia viscous drag friction high intersystem stiffness highbandwidth force reflection stability for a wide range of contact impedancesthese characteristics attempt to maximize the "transparency" of the overallsystem. an optimal system would be indistinguishable from direct operation onthe environment itself, without the interposed machinery. however, givennonideal machinery, and given that there is still no clear consensus on an idealremote manipulator, sheridan et al. (1989) state that "the ideal manipulator is anadjustable one." these goals, however, have shown themselves to be formidableproblems given the realities of limitedbandwidth actuation, limited sensorycapability, and time delays in the communications and computation pathways.positionbased teleoperationthe simplest form of teleoperator consists of a remote manipulator that isservocontrolled to follow the operator's position commands. the operator'sintended motion is measured through a joystick or similar device. trajectoriesfor autonomous robot arms are mathematically defined for smoothness so thatvelocity and acceleration profiles are available as slowly changing inputs to thecontrol system. in contrast, in teleoperation, only the position is measurable at agiven time so that velocity and acceleration commands must be estimated andwill therefore be noisier. fortunately, most human movements are relativelysmooth (flash and hogan, 1985).the volume of space in which it is comfortable for the human operator tomaintain hand position for extended periods is small comparedtelerobotics331virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.with the total work volume of the human arm. also, it is difficult to design handcontrollers that can safely cover the entire work volume. as a result, manydesigns employ a much smaller work volume for the master hand controllerthan for the slave robot. to effectively use the slave robot then requires amethod for selectively changing the offset between master and slave positions,usually referred to as indexing. this is accomplished in most systems with afinger button that momentarily breaks the connection between master and slave,during which time the operator repositions his or her hand. when the button isreleased, teleoperation resumes with motion increments referenced to the newmaster position.the other significant form of control for remote manipulators is resolvedrate control (whitney, 1969). in this mode, human hand displacements areinterpreted as a velocity command in an assigned cartesian frame. this mode isused for example in the space shuttle remote manipulator system (rms). one3dof joystick is used for orientation commands and another for translation. a6axis hand controller has also been used as a rate controller for teleoperation(bejczy et al., 1988). the command frame can be set arbitrarily so thatcommands can be referenced to the center of gravity of the held object, forexample. one important requirement for rate control joysticks is a spring return.the spring return can be implemented with hardware springs or by computergenerated "software spring" force commands to an active joystick (bejczy et al.,1988). this passive force feedback is essential for easily stopping thecommanded motion.a significant issue concerns which mode is better for teleoperators withoutforce feedback. kim et al. (1987) found that position control gave bettercompletion times in simulated teleoperation, except for very slow simulatedmanipulators, for which rate control was slightly better. it is widely thoughtthat, for tasks requiring large displacements, rate control can be superiorbecause it eliminates the need for repeatedly indexing to perform motions largerthan the master work volume.performance degradation occurs when there are significant rotationsbetween the rate controller's frame and the frame defined by the user's body(kim et al., 1993). in other words, if leftright hand motion causes end effectormotion that is in a visibly different direction, confusion can result. this problemis particularly severe for the control of orientation when rate commands arereferenced to rotation axes fixed to the robot hand. feedback of contact forcesin rate control has been tried by many laboratories without success. parker et al.(1993) developed a novel control law that solved some of the problems by usinga deadband. however, much work remains to show true performance benefits.novel modes of rate control, transitions between rate control and positioncontrol, and relative performance between rate control and position controltelerobotics332virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.in teleoperation are unresolved issues that could have major impacts onapplication design.coupled control of manipulators for forcefeedbackteleoperationwhen mechanical masterslave manipulators were first made electricallyremote, it was realized that force feedback to the master side was essential forgood manipulation performance. the earliest systems used identical master andslave devices with decoupled controls of the individual joints in which jointtorque for both master and slave was a function of position difference betweenthem (goertz and thompson, 1954; goertz, 1964). as improved computingpower became available in the 1970s, it became possible to use kinematicallydifferent master and slave devices in which the master could be optimized forinterfacing with the human operator, and the slave for its particular task (bejczyand salisbury, 1980, 1983). the computer performed the necessary coordinatetransformations of the force and motion information and calculated the controllaws in real time (bejczy and handlykken, 1981).the details of these coordinate transforms depends on details of the masterand slave manipulators. human operator position is usually sensed indirectlythrough joint sensors on the master. these joint angle readings must betransformed through the forward kinematics of the master arm to derive thehand's position and orientation. alternatively, the increments of joint motioncan be transformed to cartesian displacements through the manipulatorjacobian matrix. similarly, in one popular force reflection architecture, slavemotion, controlled and sensed in terms of joint torques, must be transformedinto cartesian coordinates, through the jacobian transpose inverse of the slaveand then into the joint coordinates of the master through the jacobian transposeof the master. if a wristmounted force/torque sensor is used, the first of thesetransforms in not necessary. as with autonomous robot control, theperformance of these operations is very sensitive to the numerical conditioningof the jacobian matrix of the master and slave manipulators, since matrixinverses appear frequently in the relevant equations. control methods have beendeveloped for manipulators operating near singular configurations (nakamuraand hanafusa, 1986; wampler, 1986) but have not so far been applied toteleoperated robots.many teleoperator controllers have since been developed. most of themhave relied on existing position, stiffness, or impedance controllers on the slaveand master manipulators (jansen and herndon, 1990; yokokohji andyoshikawa, 1992; tachi, 1991; goldenberg and bastas, 1990; strassberg et al.,1992). to link the master and slave and to providetelerobotics333virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.kinesthetic feedback, these approaches choose one of the interaction variables(force or velocity) to send from master to slave (forward) and send itscomplement (velocity or force) from slave to master (feedback). a usefulgeneral representation of these models was developed by fukuda et al. (1987),who formulated the controller in matrices that relate all measured variables toall actuated variables.for practical reasons, many of these studies have been carried out usinghardware designed for other purposes, with little capability for delicate forcecontrol. often, for example, an industrial robot manipulator is used for the slaverobot. master manipulators (joysticks) are often highly geared mechanisms withjoint limits or kinematic singularities significantly affecting the operator'smotion. mechanical limits are of key importance in determining teleoperatorfidelity. control technology can never fully overcome limitations imposed byfriction, bandwidth, and actuator properties.for example, stiction (static friction) imposes a lower limit on themagnitude of forces that can be displayed; actuator saturation provides an upperlimit. mechanisms capable of handling higher forces often have lowerbandwidth and higher friction. thus dynamic range of forces becomesimportant. similarly, torque ripple in actuators will distort force commands tothe operator or environment and costly force transducers in a closedloop modecan only partially compensate. many current studies are limited by anarrowness of approach in that they study only the control system. majorimprovements in teleoperator performance will require attention to theunderlying mechanisms as well as their control. this tightly integrated approachbetween control, actuation, and mechanization has been termed mechatronics.the mechatronics of teleoperation needs further interdisciplinary study.however, research progress is currently limited by a lack of tools with which toquantify teleoperator performance. although some work has been done(discussed below), few of the implementation studies have quantified theperformance of their design. in the absence of standardized quantitativemeasures, an event at which researchers could bring their implementationstogether to allow comparative subjective testing could have a substantial impacton the field.modelingthe performance of a masterslave teleoperator has aspects that have beendescribed qualitatively as "crispness," "viscosity," "stiffness,'' and "bandwidth."in the 1980s it was recognized that a useful analogy could be constructedbetween teleoperators with force feedback and 2port electrical networks (raju,1988; hannaford, 1989). this was an extension oftelerobotics334virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.the earlier treatment of robots in contact with their environments as 1portimpedances (hogan, 1985a, 1985b, 1985c). the 2port model describes andunifies the above qualitative descriptors into a multidimensional measure oftransparency.the 2port model can be expressed in several forms by different matrices(impedance matrix, z, admittance matrix, y, hybrid matrix, h, and scatteringmatrix, s). each of these matrices is a useful way to quantify the performanceof forcereflecting manipulators. the elements of the various 2port matricesare dynamic functions of frequency that quantify the stiffnesses, damping, anddynamic properties of the telemanipulators. each 2port matrix is a completedescription of the teleoperator in a specific configuration, but each matrix isuseful for different types of analysis.delaya challenging issue arising in many applications is time delay betweenmaster and slave sites. this delay ranges from a few milliseconds in the case ofcomputation delays to 10100 ms delays induced by computer networks, todelays of seconds or more induced by multiple satellite communication links.these delays can induce loss of teleoperator fidelity and task performance aswell as dangerous instability of operation (hannaford and kim, 1989; kim etal., 1992). a breakthrough in this area occurred when anderson and spong(1988, 1989) developed an approach to making the twoway informationchannel between master and slave satisfy a passivity constraint regardless oftime delay. passivity quantifies the total energy output of a system and has beenused before as a test for stability (wen, 1988). with anderson and spong'sapproach used for the communication channel, if the master and slavecontrollers could be shown to be passive, then the system as a whole can beguaranteed passive and thus stable. this analysis assumes as well that thehuman operator and environment are passive.the assumptions of passivity for the operator and environment are wellaccepted and appear to be consistent with the everyday experience ofmechanical manipulators that are guaranteed passive and always stable underhuman control and actuation. niemeyer and slotine (1991) reformulated thepassivity approach and explicitly normalized the relation between force andvelocity by introducing the characteristic impedance. this work corrected adetail omitted by anderson and spong, who implicitly used a value of 1 for thecharacteristic impedance. although passivity is an elegant method to provestability, it allows no way to assess performance. recent results (lawn andhannaford, 1993) have shown that the characteristic impedance of thesimulated passive transmissiontelerobotics335virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.line induces a tradeoff between the series compliance from master to slave andthe free motion damping. one or the other must be increased in proportion totime delay, which induces a signficant performance penalty. therefore, theshortcoming of the passivitytransmission line theory is that performance of thesystem is not addressed.robustnessrecently, robust control concepts have been applied to teleoperatorcontrol, starting with earlier work in impedance control (colgate and hogan,1988) and continuing by applying hinfinity optimization of control over aspecified bandwidth (andriot and fournier, 1991; kazerooni et al., 1993). thiswork allows the designer to optimize a controller to minimize distancemeasures between the 2port model of the teleoperator and ideal transparencyand passivity. this work seems especially promising since it is aimed atsimultaneously achieving performance and stability for all loads over aspecified bandwidth.scalingan important extension of forcereflecting teleoperation is the idea of thescaling of mechanical energy between master and slave (feynman, 1960;flatau, 1973; hannaford, 1990, 1991; colgate, 1991). recent implementationsof this idea have encompassed an astonishing 1010 range of scale variationsfrom 109 reduction of the operator's motion between powered joysticks andscanning tunneling microscopes (hollis et al., 1990; hatamura and morishita,1990), to approximately 101 increasing of the human motion in the control oflarge robots, such as construction or demolition manipulators. in between areapplications such as muscle fiber manipulation (hunter et al., 1990) andmicrosurgery (colgate, 1991).an important issue arises with scaling because of the scaling properties ofdynamical systems. if an object is uniformly scaled in linear dimension by l,and some assumptions are made, then the inertia (which depends on volume)can be expected to scale with the cube of l, the friction (which depends onsurface area) can be expected to scale with l2, and the stiffness with l. thisnonlinear scaling of the dynamic parameters results in qualitative changes indynamic behaviors, such as natural frequency and damping ratio as size ischanged. for example, if size is scaled down, natural frequencies will rise.approaches to correcting this change in dynamics include time domainmanipulations and frequency domain approaches (colgate, 1991). muchresearch needs to be done in this area since the objects we will manipulate inthe micro domain clearly will not be scaled versions of the objects in our ownworld.telerobotics336virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.performance evaluationfinally, the performance of forcereflecting teleoperators is more difficultto assess than in unilateral systems with statically defined inputs and outputs.true system performance depends on visual display quality and manipulatorand hand controller capabilities, in addition to the lowlevel controller'sperformance. the presence of the human operator introduces a source ofvariability that must be treated statistically at great cost in experiment time.there have been a wide variety of measures used to characterize operatorperformance. many of these measures, such as time of task completion,accuracy, and error, are similar to those used by productionline industrialengineers and motorskill psychologists (sheridan, 1992a). other measuresinclude fitts information processing rate (hill, 1979), peak force, variance inforce, and sum of squared forces (hannaford et al., 1989, 1991). classes oftasks to which these measures have been applied include: calibration tasks, such as tracking a specified path; elementary tasks, such as stacking blocks, putting pegs in holes, andthreading nuts on bolts; and actual tasks, which depend on an application such as assembly ofcomponents of a machine.in many cases, the measures are experimentdependent and, due to thelarge variety of experiments humans have been subjected to, there is a largevariety of measures. even in the performance measurement of similar tasks withsimilar variables sensed, there is a lack of a standard metric (sheridan, 1992a).for example, a variety of metrics that are a function of the force/torque ofinteraction have been used to characterize operator performance (das et al.,1992; hannaford et al., 1991; mclean et al., 1994). in addition, the tasksthemselves need to be standardized so that comparisons can be made betweenalternative studies.related to the measure of human performance from experimental data isthe analysis of the data. statistical evaluation of data in teleoperationexperiments has lacked standardization. often, visual inspection of the meansand standard deviations of datasets is used to make conclusions. there are anumber of statistical methods available for the processing of data, and otherfields of experimental research have adopted standards. it is encouraging to notethat this trend is occurring in human performance studies in teleoperation.in terms of published results, kim et al. (1987) compared position controlversus rate control, taking into account the joystick type (isotonic or isometric),display mode (pursuit or compensatory), threedimensionaltelerobotics337virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.task (pickandplace or tracking), and manipulator workspace. they found that,regardless of joystick type, display mode, or task, when the workspace is small,position control is superior to rate control, by measures of completion time andaccuracy. for larger workspaces or slow manipulators, rate control becomessuperior to position control. das et al. (1992) also found that position control issuperior to rate control. in contrast, zhai and milgram (1993a) examined a sixdimensional task and concluded that isometric rate control can be as good asisotonic position control.isometric (pure force) versus isotonic (pure position) control are theextreme ends of a continuum of variablestiffness hand controllers: infinitelystiff versus infinitely pliant. for intermediate stiffnesses, zhai and milgram(1993b) found that elastic rate controllers are better than isometric ratecontrollers, especially for less well rehearsed tasks. mention should be madeagain of a comparable study by jones and hunter (1990), first discussed inchapter 1, who found that increasing the stiffness of a manipulandum decreasesthe response time. a contrary effect is that high stiffnesses decrease accuracy,and there is an optimal value of low stiffness for best accuracy. more recently,they found that increasing the viscosity of the manipulandum decreases thedelay and the natural frequency of the human operator (jones and hunter, 1993).the effect of other mechanical properties of manipulanda, i.e., bandwidth,coulomb friction, and backlash, were investigated by book and hannema(1980) using a fitts' law paradigm. in this paradigm, a motion with accuracyconstraints is segregated into a gross motion (approach to a target) and a finemotion (accurately attaining the target). both coulomb friction and backlashincreased the fine motion time but kept the gross motion time unchanged;backlash was the hardest to handle. decreasing bandwidth increased the grossmotion and fine motion times about the same.another issue is force reflection versus position control for teleoperators.in general, it is found that force reflection is significantly better than positioncontrol (das et al., 1992; hannaford et al., 1991).supervisory controlthe term supervisory control derives from the analogy between asupervisor's interaction with subordinate human staff members in an allhumanorganization and a person's interaction with intelligent automated subsystems. asupervisor of humans gives directives that are understood and translated intodetailed actions by staff subordinates. in turn, subordinates collect detailedinformation about results and present it in summary form to the supervisor, whomust then infer the state of the systemtelerobotics338virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.and make decisions for further action. the intelligence of the subordinatesdetermines how involved their supervisor becomes in the process. automationand semiintelligent subsystems permit the same sort of interaction to occurbetween a human supervisor and the computermediated process (sheridan andhennessy, 1984; sheridan, 1992a).in the strictest sense, supervisory control means that one or more humanoperators are intermittently programming and communicating to the computerinformation about goals, constraints, plans, contingencies, assumptions,suggestions, and orders relative to a remote task, getting back integratedinformation about accomplishments, difficulties, and concerns and (asrequested) raw sensory data. in addition, the operator continually receivesfeedback from a computer that itself closes an autonomous control loop throughartificial effectors and sensors to the controlled process or task environment.in a less strict sense, supervisory control means that one or more humanoperators are continually programming and receiving information from acomputer that interconnects through artificial effectors and sensors to thecontrolled process or task environment, even though that computer does notitself close an automatic control loop. the strict and notstrict forms ofsupervisory control may appear the same to the supervisor, since he or shealways sees and acts through the computer (analogous to a staff) and thereforemay not know whether the computer is acting in an openloop or a closedloopmanner in its fine behavior.once the supervisor turns control over to the computer, the computerexecutes its stored program and acts on new information from its sensorsindependently of the human, at least for short periods of time. the human mayremain as a supervisor or may from time to time assume direct control (this iscalled traded control), or may act as supervisor with respect to control of somevariables and direct controller with respect to other variables (called sharedcontrol).supervisory control has been applied wherever some automation is useful,but the task is too unpredictable to trust it to 100 percent automation. thisincludes essentially all of what are called "robot" applications currently. amongthe reasons to employ supervisory control, are:(1) improved task performance (both speed and accuracy) and, in thespecial case of loop time delay, and avoidance of instability.(2) human safety, when the work environment is hazardous.(3) convenience, when the human must attend to other tasks while theautomation is working and assuming the task does not requirecontinuous monitoring.(4) a means to construct, control, and continuously modify intelligentsystems, so as to better appreciate the relation of people to machines.telerobotics339virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.generic paradigm for supervisory controlfigure 92 illustrates the generic supervisory control paradigm. at thebottom the tasks represent various material processing machines and transferdevices, each controlled by its own computer. all the computers are controlledfrom a central control station in which the human supervisor cooperates with acomputer to coordinate the control of the multiple automatic subsystems. thesupervisor's functions are five, which in turn can be subdivided as shown in theupper part of the diagram by the upper case labels. for each of the mainsupervisory functions, the computer provides decisionaiding andimplementation capabilities, as represented by the separate blocks. these fivefunctions are:(1) plan, consisting of (a) modeling the physical system or process to becontrolled, (b) deciding on the objective function or trading relationamong the various goal states that might be sought, and (c) formulatinga strategy, which consists of scheduling and devising a nominalmission profile.(2) teach, which means (a) selecting the control action to best achieve thedesired goal and (b) selecting and executing the commands to thelowerlevel computers to make this happen.(3) monitor, which means (a) allocating attention appropriately among thevarious subsystems to measure salient state variables, (b) estimatingthe current state (arriving at the current belief state), considering inaddition to measurements the predicted response based on the previousbelief state and the previous control actions, and (c) detecting anddiagnosing an abnormality.(4) intervene, which means (a) to make minor parameter adjustments tothe automatic control as it continues in effect, (b) to reprogram if thesystem has come to a normal stopping point and is awaiting furtherinstruction, (c) to take over manual control if there has been a failureof the automation, and (d) to abort the process in the case of a majorfailure.(5) learn from experience so as to do better next time.status of research in supervisory controlcomputerbased planning of telerobot actionsmany new computerbased aids for planning (for collision avoidance,energy minimization, and other criteria of satisfaction) operate "whatwouldhappenifš" trials on system models with hypothetical inputs. typically, theseprovide the capability for graphical entry of test commands or trajectories andindicate the projected quality of the results. the recent system of park (1991) isan example. park's system allows thetelerobotics340virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.figure 92 generic supervisory control framework. at the bottom aresubtasks, all of which may be automated for short periods through a taskinteractive microcomputer. at the top are the supervisory functions (plan,teach, monitor, intervene) subdivided into elements, all of which must beattended to by the human supervisor, but any of which may be aided by acomputerbased expert system or online decision aid. in addition, offline andnot shown is the supervisory function learn. the supervisor must allocate hisor her attention among all the boxes (sheridan, 1992).telerobotics341virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.human operator to indicate, on a computer graphic model of theenvironment (as best it is known), a series of subgoal positions to serve asintermediate points along a multiplestraightlinesegment path for motion ofthe endpoint of the teleoperator. the computer graphic model provides depthcues in the form of lines from the underside of the object to the floor asdescribed above. another algorithm checks for collisions with the environmentby any part of the teleoperator or objects carried by it (again, as best known).as necessary, the algorithm makes small modifications to avoid such collisions.areas that cannot be seen, or are not known from other modeling information,are considered virtual objects. if an initial trajectory turns out to beunsatisfactory, the operator can try another. the operator can thus evaluate atrajectory before ever committing to an actual motion. if the video camera ismoved for a better view, the computer can reduce the virtual objects to moreclosely conform to actual objects. if, upon actual motion, collisions occur, themodel can be updated with the operator's assistance.teaching the telerobot what to domachida et al. (1988) demonstrated a technique by which commands froma masterslave manipulator can be edited much as one edits material on a videotape recorder or a word processor. once a continuous sequence of movementshad been recorded, it can be played back either forward or in reverse at any timerate. it could be interrupted for overwrite or insert operations. theirexperimental system also incorporated computerbased checks for mechanicalinterference between the robot arm and the environment.funda et al. (1992) extended the machida et al. work and the earliersupervisory programming ideas in what they call teleprogramming. again, theoperator programs by kinesthetic demonstration as well as visual interactionswith a (virtual) computer simulation. that is, commands to the telerobot aregenerated by moving the teleoperator master while getting both force and visualfeedback from a computerbased model slave. however, a key feature of theirwork is that instructions to be communicated to the telerobot are automaticallydetermined and coded in a more compact form than record and playback ofanalog signals. several freespace motions and several contact, sliding, andpivoting motions, which constitute the terms of the language, are generated byautomatic parsing and interpreting of kinesthetic command strings relative tothe model. these are then sent on as instruction packets to the remote slave. thefunda et al. technique also provides for error handling. when errors inexecution are detected at the slave site (e.g., because of operator error,discrepancies between the model and real situation and/or the coarsenesstelerobotics342virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.of command reticulation), information is sent back to help update thesimulation. this is to represent the error condition to the operator and allow himor her to more easily see and feel what to do to correct the situation.computer assistance in monitoringthere are various ways the computer can assist the human supervisor inmonitoring the effectiveness of the automation, getting a better viewpoint anddetecting and diagnosing any abnormality that may occur.das (1989) devised an algorithm for providing a "best" view forteleoperator control within a virtual model. his technique computes theintersection of the perpendicular bisector planes to lines from the end effector tothe immediate goal and to the two nearest obstacles. such a point provides anorthogonal projection of the distances to the obstacles and to the goal. inevaluation experiments it proved useful to have the viewpoint thus setautomatically, unless the operator needed to change it often, in which case theoperator preferred to set it.hayati et al. (1992) and kim et al. (1993) report a computeraided schemefor performing telerobotic inspection of remote surfaces. the scheme involvesboth automation in scanning and capture of video images that appear to revealflaws as well as graphic overlays and image maneuvering under human control.tsach et al. (1983) report a technique for automatically scanning anddetecting a discrepancy in dynamic inputoutput relations relative to amultisubsystem normative model. actual process measurements of upstreamvariables are fed to the appropriate model subsystems and correspondingdownstream measurements are compared with outputs of the model. theadvantages of their technique is that failures can be determined even duringprocess transients, and failure locations can be isolated.intervening and learningif the automation fails, if the programmed actions end, or for other reasons,occasionally the human supervisor must intervene to reprogram or take overmanual control. criteria for doing this and which takeover mode is best tend tobe context dependent.supervisory learning can be accomplished by keeping track of conditionalfrequencies (probabilities) and weighting by outcomes, by steepest ascentmethods, by building up fuzzy rules and "growing an expert system" in terms ofgiven fuzzy linguistic variables, calling for more evidence in state space regionsin which membership is poorest, or by moretelerobotics343virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.rigorous neural net techniques. there is a dearth of research in both theintervening and learning roles of the supervisor; the reasons are elusive.predictor displays to cope with time delaycontrol engineers are familiar with the destabilizing effect of loop timedelays that occur in space teleoperation due to the limited speed ofelectromagnetic radiation, or similarly in underwater teleoperation when soniccommunication is used (sheridan, 1993). predictor displays have beendemonstrated (noyes, 1984; hashimoto et al., 1986; cheng, 1991) to reducetask execution time by as much as 50 percent under ideal conditions, namelywhen: (a) the kinematics and dynamics of the object being controlled can bereasonably modeled; (b) there are minimal interactions with externaluncontrollable objects or disturbances that cannot be modeled; (c) movementsof the real object can be superimposed on the model movements and both canbe seen in the same image plane; and (d) actions occur at a pace slower thanhuman movement and perception. condition (b) does not obtain in assemblytasks in which external objects are small enough to be moved. however, muchas an athlete apparently does in running over rough ground, catching a ball, etc.,it is possible to build and calibrate a model of the salient external objects byvision or optical proximity sensing and to use it to predict interaction with theend effector or object under direct control.although the dynamics of the computer model used for prediction andplanning must be synchronized to the dynamics of the actual task, the use ofthat model by the human operator need not be synchronized. in other words, aneasy maneuver in free space may require no attention by the operator, and asthat easy maneuver is occurring he or she may wish to focus attention on a morecomplex maneuver coming up later, even running the prediction of the complexmaneuver in slow time or repeating it. conway et al. (1987) called this''disengaging time control synchrony using a timeclutch" and "disengagingspace control synchrony using a positionclutch." they built and demonstratedsuch a system.visual and haptic aidspredictor displays may be considered to provide visual and haptic aids tothe operator. whether or not there are delays, other forms of visual and hapticaids may enhance operator performance. brooks and ince (1992) suggest avariety of display enhancements: in zone displays, areas of the visual field are highlighted that placerestrictions on the operator, such as slave reachability (due to its owntelerobotics344virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.kinematics and to limits in the hand controller range) and imminentcollisions. the reachability limits of the master can be changed byreindexing. depth cues can be provided by superimposing a perspective grid withmarkers dropped onto the grid from the end effector and objectworkpoint, by virtual placement of the camera in a more useful locationor by superimposing graphs representing end effector position andorientation relative to the task. view enhancements include delineating edges and surfaces that may bepoorly visible due to lighting or similar textures, image distortions tohighlight positioning errors, and status indication, such as coloring agripper depending on the grasp stability and hot spots in the environmentsensed with a temperature probe on the end effector.milgram et al. (1993) also propose virtual pointers to mark points indisplays, virtual tape measures to show the distance between points, and virtualtethers between slave endpoint and target point to help guide a motion.an issue is registration of the graphical image with the real image (oyamaet al., 1993). in addition, the graphical simulation has to be periodicallycorrected to correspond to the real display because of modeling errors; thisprocess has been called stimulusresponse reconciliation (brooks and ince,1992). rasmussen (1992) emphasized the importance of aligning the visual andkinesthetic reference frames for best performance; concern was expressed forlaparascopic procedures in which vision via an overhead monitor is badlyaligned with manipulation (tendick et al., 1993).haptic aids are not as well developed as visual aids. sayers and paul(1993) propose the concept of synthetic fixtures, whereby parts matingoperations are facilitated by force attractors. similarly, forcereflecting mice forgraphical interfaces have been proposed to mechanically enforce windowboundaries, simulate push buttons, and act as attractors (hannaford andszakaly, 1992; kelley and salcudean, 1994; ramstein and hayward, 1994).a hypothetical scenario for the use of predictor displays and sensory aidsis presented in chapter 12 for hazardous operations.performance measurement, learning, and modelingany task performed under supervisory control requires time for the humanto program (teach) the operation and then time to monitor while the operation isbeing executed by the computer. each of these components takes more time asthe complexity of the task increases. for very simple tasks, one might expectdirect control to be quicker because instruction of a machine, as with that ofanother person, requires sometelerobotics345virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.minimum time. for very complex tasks, instructing the computer is likely to beextremely difficult and therefore supervisory control is essentially impossible.in between, as has been shown, the sum of the two times can be significantlyless than that for direct human control.there have been models of supervisory control, but none has captured allof the elements, including those of planning and setting objectives. the baronet al. (1980) procru model includes continuous sensorimotor skill, attentionsharing among displays, decision making, and procedure following and hasbeen applied successfully, for example, to the final approach of aircraft.realtime computingin a virtual environment or teleoperation system, the user senses asynthetic environment through visual, auditory, and haptic displays and thencontrols the synthetic environment or telerobot through a haptic or auditoryinterface. these input/output (i/o) operations must be sufficiently fast to beeffective, that is to say, for control of a device or for presentation of humanstimuli. how to organize a computer system to handle the computationaldemands of the many components of a ve or teleoperation system, especiallythose of the i/o operations, is a challenge for realtime computing.at the most basic level, computers simply have to be faster and cheaper tobe used to compute the necessary algorithms in real time. such requirementshave already been discussed for graphic displays, auditory displays, andinformation visualization. a more particular requirement is for realtime i/o:often powerful computer systems can compute fast, but they are unable toperform realtime i/o for control and sensing of external devices. for example,the elaborate operating systems of workstations typically do not permit fast andreliable interaction with the external world. given the diverse components of ave or teleoperation system, a decentralized computing architecture with anindividual computer attending to just one component, such as a haptic interface,seems more appropriate than a monolithic supercomputer. software andoperating systems have to facilitate the programming and interaction of suchnetworks of computers, walking a delicate line between efficiency and features.since world war ii, realtime computing has been particularly associatedwith the development of control systems, simulators, and inputdisplay systems,which are the three primary ingredients of ve and teleoperation systems. manyapproaches and issues have been addressed in robotics and telerobotics. in fact,telerobotics puts greater demands on realtime computing than does robotics,because, in addition to the controltelerobotics346virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.of a robot, sampling and control of the human interface and the realtimecomputation of a predictive display are required. that ve and teleoperationsystems must comply with the requirements of the human beings with whomthey interact puts lower bounds on any measure of performance.requirementsve and teleoperation systems might include a headmounted visualdisplay, eye trackers, various limb trackers, haptic interfaces, sound generators,and remotely controlled robots. each of these components may itself be acomplicated system; for example, a telerobot can have many sensors andactuators and complicated end effectors. not counting the visual data, it can beestimated that such system should be able to sustain, at the bottom layer of arealtime control hierarchy, several hundreds of thousands of transactions persecond with latencies measured in a few microseconds. these numbers may notseem so high by today's standards in terms of communication systems, but,unfortunately, because of the short latency requirements (ideally a few tens ofmicroseconds for a rate of a thousand of samples per second) currentlycommercially available equipment either barely can keep up or simply isinadequate, because often this equipment is designed for quite different purposes.levels of controla telerobot requires a hierarchical control system, with servos and higherlevel control operating at several levels. a forcereflecting haptic display or alocomotion display is also essentially a robot with similar control systemrequirements. the realtime requirements vary considerably, depending on thelevel of the hierarchy in question. at the lower levels, speed, precision,responsiveness, and predictability are the features that are sought from the realtime control; at the higher levels, flexibility will be the key, along with facilitiesnormally provided by highlevel operating systems.lowlevel controlat the lowest level is joint control. in the simplest case, the control designmay be approached by considering each joint of a complex robot (or hapticinterface) as a singleinput/singleoutput system, with joint torque as input andjoint position as output. manipulators and haptic interfaces are made of acollection of actuated and passive joints organized into a kinematic structure.position, force, and relationships between these quantities and their derivativesfor the entire manipulator ortelerobotics347virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.haptic interface will then become the variables of interest. from a systemsperspective, the manipulator or the haptic interface will then be viewed as amultipleinput/multipleoutput system, as seen from a higher level in a controlhierarchy.for a mediumsize manipulator, servo rates will be on the order of afraction of to several thousand samples per seconds, which translates into 50kflops to 5 mflops, depending on the control algorithm (ahmad, 1988). feedforward dynamic compensation, for mediumsize manipulators, can be usedeffectively with rates as low as 20 updates per second; nevertheless, thecomputational burden is significant (hollerbach, 1982; izaguirre et al., 1992).for micro manipulators, these numbers are much higher: on the order of gflopsin extreme cases (yoshichida and michitaka, 1993).intermediate level controlhigher still in the hierarchy, not only will the state of the system at oneinstant in time be considered, but also entire state trajectories. the calculationof these trajectories will in turn require an additional computational demand,which depends on the number and the nature of the constraints that need to beenforced. as a general rule, each control system at a level of a control hierarchymust handle subsystems, which grow more complex as we move up the controlhierarchy, but if the design is correct, their complexities will be hidden by thelower levels.highlevel controlat the higher levels, the control of the system is described in terms ofgeneral directives that cover a complete task or subtask. here the time scale ismeasured in seconds, minutes, or hours. at such a level, the realtime controlrequirements are described only in general terms. the higher up in thehierarchy, the more the features offered by conventional operating systemsfound on engineering workstations will supply the required services for ve andteleoperation systems. the requirements are expressed in terms of powerfulprogramming environments, languages, and ordinary system services, such asdata storage and retrieval (hayward and paul, 1984).latency versus update periodthe latency is affected not only by calculation time, but also by input/output transactions between computing units and peripheral devices. theseoperations may include analog to digital conversion, data transfer fromperipheral devices, data formatting and conversion, safety checks, and so forth.telerobotics348virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.the capital rule of digital control systems is the minimization of latency,defined as the time elapsed between the time a measurement is taken, therelevant calculations are performed, and the signal is fed back to the system tobe controlled (in general, to the actuators). in fact, it can be readily realized thata digital control system with a latency larger than the basic response time of thesystem being controlled will fail at its task: at each update, the new controlsignals will be based on measurements made in the past, thus reflecting a stateof the system that no longer is relevant.the update period is simply the time between two successive outputs ofcontrol signals. notice that parallel processing can always be used to achieve avery small update period, but the effect on latency is limited by the minimumnumber of sequential operations that have to be performed on the inputs tocalculate the outputs (lathrop, 1985). an experimental study (rizzi et al., 1992)demonstrates the effect of a 40fold increase in either latency or update periodon the performance of a tracking controller. the increase in update period from1 ms to 40 ms has a very minor degrading effect on tracking performance andstability. the 40fold increase in latency completely destabilizes the system.one way to counteract a large latency is for the control system to predictfuture states of the system based on previous measurements. of course, preciseprediction of the state is computationally demanding, and this will result in evenlarger control latency, possibly leading to worse performance than a simplercontroller well matched to the response of the controlled system. theconclusion is: if latency can be avoided, it must beševen if it means simplercontrol algorithms.for lowlevel control, latency requirements for manipulators and hapticinterfaces are identical to the sampling period, which is of the order of a fewtens to hundreds of microseconds. for intermediatelevel control, latencyrequirements range from a few milliseconds to a few seconds. for highlevelcontrol, latency requirement can vary from a fraction of a second to a fewseconds.system architectureby system architecture for realtime computing is meant the type andnumber of processors, the connectivity and communications betweenprocessors, the input/output interfaces, the operating systems, and thedevelopment tools.processors and computing platformsmajor processor architectures can be classified as cisc (complexinstruction set computers), risc (reduced instruction set computers), vliwtelerobotics349virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.(very long instruction word), superscalar, and dsp (digital signal processors).table 91 lists some of the main commercial examples. at the moment, certainof these processors are more prevalent and dominant than others; juliussen(1994) discusses their relative strengths and future commercial viability. someof the cisc and risc processors have been designed specifically for personalcomputers (pcs), such as the 486s and the pentium; others are incorporated intoboth pcs and workstations, such as the alpha, the powerpc, and mips.processors such as transputers and the texas instruments tms320c40 (c40)dsp chips are not meant for generalpurpose computing platforms, but for fastnumerical computation and i/o. the scenario of some generalpurposecomputing platform as a front end to a network of transputers or c40s that dothe i/o and realtime computing has become popular in robotics and may wellserve as an example for realtime ve computing as well.there is a tendency to discuss only workstations for ve and teleoperationsystems, primarily because of graphics and networking, but pcs areapproaching workstations in power and are already heavily used in realtimecomputing. the outcome of the muchdiscussed battle, or convergence, betweenpcs and workstations will have a major impact on the nature of realtimesystems in general and on ve and teleoperation systems in particular. becauseof the generally lower costs of pcs and associated software, their developmentwill facilitate the spread of ve and teleoperation systems.in turn, the development of pcs is threatened from below by the advancesof more powerful video game players and settop interactive tv players.planned game players from sega and nintendo will soon have comparablecomputational power to the fastest desktop computers. the addition of typicalpc applications software such as spreadsheets, word processing, and electronicmail are likely in the near term and will convert such tv players into moregeneralpurpose computers. with the combination of video, sound, inputdevices, and fast computation, these tv players could represent a formidablecomputational force in ve. giventable 91 categorization of some advanced processorsciscriscvlwidspalphasparcamd bit slicetms320c4068040rs/6000m960002486powerpcat&tdsp32cpentiummipsparisctransputertelerobotics350virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.the huge market forces driving these developments, costs of such systemswould run at several hundred dollars and severely threaten pc or workstationbased ve systems.parallel processing and communicationsto run processes faster and meet realtime constraints, parallel processingcan be advantageous up to a point. that point is defined by losses in throughputand increases in latency due to communication, and by limitations in the abilityto parallelize a computation. when there are 10 or more processors tocoordinate, it can become a formidable task to avoid conflict and to ensuretimely performance. in such cases, it may be best to upgrade to a fasterprocessor to reduce the number required.the main approaches toward connecting processors include a commonbus, pointtopoint links, and crossbars. a common bus is the most traditionalapproach; industry standards with a roughly equivalent performance of 40 to 80mbytes/s include vme bus, multibus ii, futurebus, eisa, nubus, sbus, andpci bus. each computing unit is equipped with fast local memory, and a globalmemory bank is made available to all computing units (bejczy, 1987; chen etal., 1986; clark, 1989; narasimham et al., 1988). the advantages of such anarchitecture include modularity, easy expansion for more performance, and thepossibility to mix hardware from different sources (backes et al., 1989). thedisadvantages obviously lie in the bottleneck created by the two sharedresources: the global memory and the system bus.pointtopoint links circumvent the bottleneck problem of busbasedarchitectures to some extent: communication between connected points will befast, but that between distant nodes in a graph will be slow. such builtin linksare used in most engineering workstations for graphic processors, soundprocessors, memory access, disk controllers, etc. the advantages include designfor minimum cost, but then the system, once configured, offers few possibilitiesfor expansion.some developers have proposed highperformance central processing units(cpus) equipped with builtin highspeed pointtopoint links (120 mbytes/s).following this philosophy, inmos from europe (now a division of sgsthomson) introduced the transputers series (t800, t9000), which can looselybe classified as risc computers. these computers are each fitted with fourcommunication channels, which permit the user to design a variety of coarselyparallel architectures (buehler et al., 1989; zai et al., 1992). recently texasinstruments combined the transputer concept with its experience in dsp designleading to the c40 with six highspeed links, thus competing headon with thelatest inmos transputer t9000. the principal disadvantage of pointtopointcommunicationtelerobotics351virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.systems, such as those made available with transputers or c40s, is the lack ofindustry standards in terms of signal conversion and data acquisition hardware.whereas such hardware is abundantly available for most bus standards, it isonly beginning to become so for pointtopoint communication systems,requiring for system implementers much custom electronic design.the recent availability of these powerful processors, which lendthemselves to parallel processing, together with commercially availabledevelopment and runtime software, has virtually eliminated the need to buildcustom computing platforms and develop custom multiprocessingenvironments. in the past, this has been necessary and has consumed substantialfinancial investment and personnel. examples are the chimera ii (stewart etal., 1989) and condor (narasimham et al., 1988) systems. even though most ofthese custom systems relied increasingly on standard commercial boards, theefforts in upgrading, to stay compatible with the host operating system as wellas the latest runtime boards, constituted a major ongoing investment that fewerand fewer labs are willing to undertake. the same situation of the availability ofpowerful processors applies to ve and teleoperation systems. this is veryfortunate, since it means that all the custom development efforts can be focusedon the critical i/o interface, a current bottleneck that may be better served bycommercial suppliers in the future.the most generic architecture consists of a full n × m crossbar switchingnetwork connecting a set of cpus and register files totalling n inputs and moutputs. this architecture has the advantage of being capable of reaching thetheoretical optimum performance, which minimizes latency and maximizesthroughput. an example is a system for control of a complete microrobotsystem (hunter et al., 1990; nielson et al., 1989). the disadvantage is themassive complexity and high cost of the system.although considerably slower, it is also possible for processors tocommunicate across networks. researchers have become interested in runningrobotic experiments or virtual environment setups across a computer networkwith nodes located in different cities or even in different continents. in oneexample, robotic sessions are run among a network of sites across the unitedstates (graves et al., 1994); in another, a telerobotic experiment has beenperformed between japan and the united states (mitsuishi et al., 1992). thistype of work is bound to be increasingly investigated given the high potentialfor applications. in effect, with such systems, the services of specialists in anyarea could be requested and put to use without asking the specialists to travelwhere they are needed. clearly, the demand on communication networks willfollow the same route that is being taken by lowlevel realtime control systems:high volumes of data transactions, low latency and time precision. it must betelerobotics352virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.expected that the performance levels required at the scale of a laboratory site oran application site will need to become available across entire networks.operating systems and development environmentsa large number of realtime operating systems are currently in competitionto provide the services needed to implement highperformance realtimeapplications (gopinath and thakkar, 1990). these operating systems (os) maybe classified as embedded, fullfeatured realtime, or hybrid.an embedded operating system is targeted at original equipmentmanufacturer (oem) applications, and as such provides only essential facilities,like a realtime executive (realtime multitasking, memory management, anddevice i/o). it is usually a simplified system offering no support for a filesystem and must be used as a crossdevelopment tool. yet most offer hard realtime features essential to ve and teleoperation systems. in this category, spox(spectron microsystems) has emerged as one of the industry standards.a fullfeatured realtime os resembles the more standard unix but withadditional realtime features. an example of this category is helios bydistributed software ltd., which offers an eightlevel priority realtimescheduler, virtual message routing, x and microsoft windows graphic support,as well as sun and pc host interfaces. clearly, processing nodes withmicrosecond interrupt latency requirements canšand mustšbe stripped of theinherently slow highlevel functionality.in between embedded and fullfeatured realtime operating systems are thehybrids, the most popular of which is probably vxworks by wind riversystems. these typically are realtime operating systems running on dedicatedsharedmemory vme processor boards and provide a transparent interface tothe host unix operating system. despite its high cost ($10,00020,000)vxworks has been attractive for its convenient interface to existing unixhosts, debugging and development facilities, and the availability of a largenumber of supported processor and i/o vme cards.nevertheless, many researchers are quite dissatisfied with the servicesprovided by commercial operating systems, usually for reasons of cost andinefficiency. because these operating systems are designed with generalpurpose requirements in mind, the few requirements essential to ve andteleoperation systems are not well addressed, and the resulting systems arecumbersome to use and clumsy in their design. for these reasons, countlesscustom operating and development systems have been designed andimplemented, with their scope limited to one or to a handful of laboratories.telerobotics353virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.experience shows that no single operating system (and no single cputechnology) will satisfy the varied needs of ve and teleoperation systems. thisis why realtime extensions to the unix systems will most likely remainlimited to the higher levels of the hierarchy (soft real time). for lowlevelcontrol and data acquisition, no single runtime environment has been shown tobe satisfactory to the community. helios is probably one of the most suitableoperating systems for ve and teleoperation systems to date. it provides all theunixlike services on a distributed realtime network, allowing one at the sametime to ''peel away" the higher levels of the operating system on part of thenetwork dedicated to timecritical code. in practice, the response is mixed:some users report sufficient control at the device level, some others don't(poussart, personal communication). systems like helios are clearly going inthe right direction and might be the approach of choice for ve and teleoperationsystems in the future. to obtain full processor control on a network of only afew processors, enhanced c compilers, like 3l c by 3l ltd. are minimalsystems, adequate for providing standard host i/o, memory managementfunctions, and multiprocessing facilities (network loading, communicationmanagement, load balancing, message routing, microkernel for multitasking).research needsa major trend in telerobotics is toward higherlevel supervisory control,that is to say, toward autonomous robotics (robotics for short). major drivers inthis direction include: (1) communication problems (e.g., delays, lowbandwidth, low resolution), especially in space and undersea, (2) burden liftingfrom the operator (e.g., partially automating repetitive tasks), and (3)performance enhancements (e.g., using local sensing when vision is obscured,assisting in obstacle avoidance). hence the needs in telerobotics are often thesame as the needs in robotics. the line between supervisory control androbotics is quite blurred.at the same time, manually controlled telerobots remain important. onereason is safety, particularly in space and medicine applications, due touneasiness about the loss of control. for example, developers of the plannedspace station are concerned that the telerobot not damage the space stationstructure. another reason is the inability of robots to handle unstructuredenvironments, which obviates the possibility of even partially automating a taskšthe reason for having telerobots in the first place.in the control of telerobots, the issues of haptic interfaces, computergenerated environments, and realtime systems are important. althoughpredominantly covered in other sections of the report, these issues are addressedhere to some extent in discussing various needs in telerobotics.telerobotics354virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.handling communication delaysin space applications, the sentiment for groundbased teleoperation isincreasing, for a number of reasons. first, for the planned space station, theactual amount of time spent by astronauts in orbit will be relatively short, and ofthat time the fraction that could be devoted to teleoperation is shorter yet.because internal vehicular robots (ivrs) and external vehicular robots (evrs)will be present the whole time, they would be used more efficiently if operatedfrom the ground when astronauts are not attending. second, humans are muchless effective in space than on the ground; weightlessness seriously affectsconcentration and attention. third, sending humans into space is expensive anddangerous.to communicate from the ground, several satellite and computer systemsmust be traversed. the result is variable delays on the order of 5 s. there is alsoa need to funnel commands through a central computer at the johnson spaceflight center, not only to share the communication channel but also to verifythe commands. again, safety is an overriding concern. the trend is to devolveground control from houston to other sites; for example, the recent rotexexperiment (brunner et al., 1993; hirzinger et al., 1993) involved control of thisgermanbuilt telerobot on the space shuttle from karlsruhe. the canadianspace agency is interested in controlling canadianbuilt telerobots from itsmain facility in st. hubert, quebec. other groups will wish to perform scientificexperiments in space from their home locations on the ground; the wordtelescience has been coined to describe this activity. this devolution willaccentuate the delays.time delays are also important in underseas teleoperation. to avoid powerand data tethers, communication with submersibles must rely on relatively slowsonar signals. for example, at a distance of 1,700 m, sound transmissionimposes a roundtrip delay of 2 s (sheridan, 1992a).there are two main approaches to handling teleoperation with time delays:(1) control theory approaches that incorporate time delays and (2) predictivedisplays. in general, the greater the delays, the lower the system gains may be(e.g., stiffness and viscosity) in order to avoid instability. thus the systemresponse slows down and performance suffers, to an extent that depends on thecontroller. recently developed controllers based on passivity approachesguarantee stability but seem to suffer in performance because their gains are toolow (lawn and hannaford, 1993). exactly how to formulate a controller thataddresses both stability and performance, under various time delays, needsfurther work.force reflection under delays is more problematic than position control.for delays approaching a second or more, position control becomes superior toforce control (lawn and hannaford, 1993). in fact, humanstelerobotics355virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.are sensitive to extremely small delays in terms of fidelity of force reflection(biggers et al., 1989). one solution is to implement a force controller, such asimpedance control, on the slave side and a position controller on the masterside. thus there is no force feedback to the operator, and the slave forcecontroller is presumed capable enough to handle interaction forces. thisrequires that the slave force controller be appropriately tuned from a knowledgeof the environment and the task. this knowledge can either exist from a prioriinformation about the task or from measurements made on the environment. itis therefore important to get several kinds of information: (1) determine whetherto use position control or force reflection as a function of delay and tasks; (2)determine how to configure the slave manipulator's force controller as afunction of a priori knowledge of the task; and (3) identify the environmentalcharacteristics through slave robot sensing, if a priori knowledge is notavailable. the last two needs are essentially robotics problems.for delays greater than 12 s, direct manual control becomes ineffectivewithout the aid of predictive displays (sheridan, 1992a). the main issue thenbecomes how well the remote manipulator and environment are simulated. forknown structured environments, good simulations of the geometrical aspects arequite feasible. force control can also be simulated, but difficulties are presentdue to closedchain dynamics and arbitrary contact conditions. in rotex, thepredictive displays for force were found to be quite close to actual experimentalforces.even in the case of rotex, there are small misalignments andinaccuracies, which are accommodated by local sensing. force, proximity, andtactile sensing are fused to correct such deviations. for less structuredproblems, a model of the environment must be built using vision and othersensors. this generic problem in robot vision and mobile robots is far fromsolved. one envisions that a robot would spend some time mapping itsimmediate environment, from which a simulation model is extracted; theoperator then controls the robot through this simulation model.accurate, realtime simulationsbesides predictive control, other uses for telerobotic simulations aretraining and mission development. training simulators prevent damage frombeing done in an operator's learning phase (e.g., learning a surgical procedure),free expensive telerobotic equipment for actual use (e.g., teleoperation offorestry harvesting equipment), and prepare trainees for equipment that isavailable only at the remote site (e.g., underwater robots) or that cannotfunction under normal conditions (e.g., space robots that cannot lift themselvesagainst gravity). mission development involvestelerobotics356virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.extensive simulation to work out operational scenarios; this is particularlyimportant in space.three needs, which are similar for virtual environments, are reemphasizedhere for telerobotics.(1) construction of a simulated environment. this involves the softwaretools for representing real environments, creating a particular virtualenvironment, and sharing ve modules. as mentioned above, it wouldalso be desirable to "reverse engineer" a real environment into asimulation, by using visual and other sensory recognition andrepresentation methods (see, e.g., oyama et al., 1993). this is a hardproblem, especially if object attributes other than geometry (spaceoccupancy) are to be addressed, such as mass and surface properties.(2) accurate representation of task dynamics. although the laws ofphysics have been around for a long time, often we don't understanddetailed mechanical interactions between objects in sufficient detail forsimulation. a very basic example is motion of threedimensionalobjects under friction, such as sliding, and in collisions (mason, 1989).the field of robotics is just beginning to enumerate how such objectsare expected to behave and to develop appropriate concepts.another difficulty is representing closedchain systems underarbitrary contact conditions. examples are locomotion on differentsurfaces, the behavior of the operator's hand tissues in the interactionwith the haptic interface, and the behavior of the remote manipulatorwith such compliant environmental substances as human tissues (insurgical applications) and rock or soils (in mining applications).furthermore, there is an issue of experimentally verifying such taskdynamics. the particular robot sensing also needs to be simulated(brunner et al., 1993).in the continuous domain, accurate finite element models arerequired, which may be nonlinear. they will require that theconstitutive equations for real objects be known or measured. abruptchanges in boundary conditions, such as simulating the cutting oftissue, are difficult to represent.(3) realtime computation. in addition to generating graphical images,there are substantial difficulties in realtime computation of asimulated environment. often classical multibody simulation is moreconcerned with accurate longterm integration of initial value problemsthan with computational efficiency. other intensive computations aresimulating and detecting collisions and realtime calculation of finiteelement models. needs in realtime computing are discussed in greaterdetail below.realtime computingthe multitude of challenges in realtime computing for ve andtelerobotics357virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.teleoperation systems can be successfully addressed only with computingsolutions offering high performance, ease of use, reconfigurability,expandability, and support for massive and fast i/o. coarsegrained parallelsystems, based on the most powerful computational nodes available, whichcommunicate via highbandwidth, integrated communication links, are the mostpromising contenders to satisfy all requirements. this has been recognized byacademia and industry alike and explains the early strong support for thetransputer processor family and recently the sweeping popularity of c40basedsystems.in this setting, a highspeed intercommunication standard would be highlydesirable, in a similar fashion to existing bus standards. this would offer usersan immense degree of flexibility, since computational nodes could be mixedand matched across vendors and different processor types. naturally, thiscommunication standard must go hand in hand with a physical module standard,akin to texas instrument's tim40 modules.currently, code development is done on traditional host systems connectedto dedicated runtime systems. this results typically in a bottleneck at theinterface between the two systems. in addition, communicating between runtime code on the dedicated architecture and host programs is typicallyproblematic, due to the difference in development tools, architecture, andresponse time. based on the emerging paradigm of coarse grained parallelismbased on standardized communication, the boundary between development andruntime hardware could become transparent. since the host architecture itselfcould be based on the same paradigm, communication between host(development system) and controller (runtime system) could be fast andflexible, as there could exist numerous communication links between the two.furthermore, the boundary could be laid dynamically, depending on therequirements of each system.if the input/output devices for control, teleoperation, and ve hardwareadhere to the same communication standard, much effort in systemdevelopment could be eliminated and progress toward powerful realtimeteleoperation and ve systems would substantially accelerate. it is likely that,with the increased interest in telerobotics and ve, there will be sufficient thrustto finally address the i/o standardization issue in addition to reducing costthrough volume. it should be reemphasized that, in contrast to i/o systeminterfacing, custom software developments for realtime multiprocessingsystems are no longer necessary, and the effort should rather go towardidentifying the most suitable commercial products and focus on industryacademia collaboration to develop one or several compatible standards.telerobotics358virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.better robot hardwarea goal in teleoperation is that performing a task with a remotely controlledmanipulator should feel nearly indistinguishable from directly performing thetask with one's own limbs. shortfalls in this goal are primarily due to themechanical hardware, both on the master and slave sides. often, limitedperformance masters are hooked up to limitedperformance industrial robots.although comparisons of different masterslave control strategies have beenpublished for these systems, it is unlikely that the results generalize beyond theevaluation of these specific lowperformance systems.on the slave side, robot arms and hands are required with sufficientdexterity and responsiveness to match that of the human arm. to achieve suchdevices requires improvements in actuators, sensors, and structures. someparticular needs are discussed below.multiaxis, highresolution tactile sensorsthe robot must have a comparable sense of touch to a human, even beforeconsidering how to transmit the sensation to an operator. tactile sensorscontinue to be a problem in robotics: hardly any robots have them, and thosethat do sense touch only coarsely. although interesting designs have beenproposed, few have yet been realized in a usable form. some roboticsresearchers are working with tactile physiologists to look for correspondencesbetween physiological and robotic designs, for example, the use ofaccelerometers to act like pacinians and piezoelectric strips to sense shear(cutkosky and hyde, 1993). attention to robot skin mechanics is important, forexample, the existence of fingerprintlike ridges as stress amplifiers.robust proximity sensorsas a step toward supervisory control, small adjustments in grasping or anapproach to a surface should be performed with local sensing. the intelligencerequired is much less than for the general case of autonomous control. a highlysophisticated gripper on rotex (hirzinger et al., 1993), incorporatingproximity, tactile, and force sensing, was the key to the success of the predictivedisplay control. proximity sensors have typically been ultrasonic,electromagnetic, or optical. all have limitations in range, accuracy, andsensitivity to different surface conditions. visual timeofflight systems are notyet practical because of complicated electronics, but they could be a futuresolution.telerobotics359virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.multiaxis force sensorsmultiaxis force sensors would typically be mounted on a robot wrist, tomeasure the net force and torque exerted on the end effector. miniature forcesensors would also be useful on finger segments, to control fingertip forceaccurately. key issues at the moment are cost, robustness, and accuracy. forwrist sensors, inaccuracies of a few percentage points due to crosscouplingeffects are typical and are problematic. a possible solution is based on magneticlevitation.highperformance jointsrobot limbs should be strong and fast yet should be able to interactgracefully with the environments. better actuator and transmission designs arethe key. improvements in electric, hydraulic, and pneumatic drive systems arerequired. novel actuators such as shape memory alloy and polymeric actuatorslook promising in terms of force to weight; the result might be lightweight,responsive limbs that accurately track human motion commands and faithfullyreflect back contact conditions.hardware considerations on the master side are very similar. therequirements of a human wearing or interacting with a haptic interface are evenmore demanding and involve concerns of safety, convenience, and bulk. ifreasonably performing devices cannot be obtained at reasonable costs, thespread of such systems will be limited.improved telerobotic controllerseven without time delays, there are unresolved issues about how to bestimplement controllers for masterslave systems. one reason is limitations inhardware, as mentioned above. another reason is a lack of tools with which toquantify and evaluate teleoperator performance. there are deficiencies intaxonomies of tasks: we don't understand well enough at a detailed level whatwe want these systems to do. at a basic level, we don't completely understandhuman sensorimotor abilities, for example, the discrimination of arbitrarymechanical impedances (stiffness, viscosity, and inertia) (jones and hunter,1992). clearly this knowledge is necessary to set design goals for teleroboticsystems. we also need to understand how more complicated tasks decomposeinto more basic tasks, which can then be measured and used as discriminatorsbetween telerobotic controllers. as mentioned earlier in this chapter, roboticsalso has this goal.as an example, it is not fully understood when to apply rate control versusposition control, or how to include force feedback into rate control.telerobotics360virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.other examples were mentioned in the section on lowlevel control. when isforce reflection more advantageous than the position control of a locally forcecontrolled robot? if force reflection is used, what is the exact form of controllerthat best meets goals of robustness and stability? there is still considerablework to be done in this area.a new set of issues arises from scaling: teleoperation of very small andvery large robots. the mechanical behavior of objects in the micro domain isvery different than in the macro domain. one problem is that the dynamics willbe very fast; somehow movements will have to be slowed down for the operator.as robot autonomy improves, so will the level of supervisory control. anumber of functions could be increasingly automated:(1) path planning and collision avoidance. the main issues here areefficient routines and obtaining geometric descriptions of theenvironment (latombe, 1991).(2) trajectory specification. any trajectory has to stay within systemconstraints of joint limits, actuators, and safety concerns.(3) grasping. the robot should be relied on to obtain a stable grasp and toregrasp as necessary.(4) intermittent dynamic environments. trajectories should be modified inreal time subject to changes in the environment. for example, a robotmay swerve to avoid hitting someone entering its workspace. someforms of handeye coordination, such as catching or hitting, mayrequire a speed of response not possible with teleoperation.(5) force control. with more sophisticated abilities to interact with theenvironment and to complete such tasks as the generic peginholeproblem, the need for force reflection will diminish.a step toward such autonomous control capabilities would be a higherlevel transfer of skills between the operator and the telerobot. the idea is toprogram by kinesthetic demonstrations: the human makes a movement, thismovement is measured, and the telerobot extracts symbolic information abouthow to accomplish the task (funda et al., 1992; ikeuchi, 1993). this differsfrom direct manual teleoperation, in that an exact trajectory is not beingcommanded, but rather a strategy for completing a task. difficulties particularlypresent themselves in transferring force control skills.telerobotics361virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.10networking and communicationscomputer networks offer realtime interaction among people and processeswithout regard to their location. this capability coupled with virtualenvironments (ve) makes telepresence applications, distance learning,distributed simulation, and group entertainment possible. it has been suggestedthat the most promising use of ve and networks will be in applications in whichpeople at different locations need to jointly discuss a threedimensional object,such as radiologists using a ve representation of a ct (computerassistedtomography) scan (pausch, 1991) or aeronautical engineers using a distributedvirtual wind tunnel (bryson and levit, 1992). another exciting concept is thatof virtual libraries, like the one being developed at the microelectronics centerof north carolina. this project, cyberlib, will allow patrons to enter "theinformation space independently" or go to a "virtual reference desk" fromanywhere across the united states via the internet (johnson, 1992). we alreadyhave virtual newspapers. the san jose mercury news publishes its entire text(including classified advertisements) via the american online service usinggraphically based software for the macintosh and ibm personal computers.ve and highspeed networks are the tools that will allow us to exploremars and the earth's oceans. the national aeronautics and spaceadministration's ames research center is examining the use of ve to controlrobot explorers. scientists at the monterey bay research institute areintegrating such diverse technologies as computer simulations, robotics, and vewith a sophisticated undersea localarea network to explore the nation's newestmarine sanctuary.networking and communications362virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.the department of defense's advanced research projects agency(arpa) has also recognized the importance of networks with regards to ves inone of its seven science and technology thrusts. since 1984 arpa has fundedthe simulation network (simnet) system, which enables simultaneousparticipation of approximately 300 players in simulated aircraft and groundcombat vehicles located in europe and the united states on the same virtualbattlefield. arpa is currently working on a much larger technologydemonstration to "create a synthetic theater of war" using the defensesimulation internet (dsi). this network will link thousands of real andsimulated forces all across the united states (reddy, 1992).it is anticipated that, in the future, highspeed networks will allow vesystems to take advantage of distributed resources, including shared databases,multimedia sources, and processors, by providing the required computationalpower for building the most demanding applications. highspeed networks willprovide ve applications with access to huge datasets generated by spaceprobes, dynamic climatic information from weather models, and realtimeimaging systems such as ultrasound.one less serious but rather lucrative combination of ve and networkingthat is currently in use is multiuser interactive ve games offered by genie andsierra online services. these games are provided over slow telephone lineswith limited graphics. other upcoming cooperative arrangements are plannedbetween ve and cable television. some examples of these are the promise ofmultiuser games by sega and nintendo and the development of virtualwalmart department stores by the home shopping network.status of the technologydistributed ve will require enormous bandwidth to support multiple users,video, audio, and possibly the exchange of threedimensional graphic primitivesand models in real time. moreover, new protocols and techniques are requiredto appropriately handle the mix of data over a network link. the technologiesproviding these gains in performance blur the traditional distinction betweenlocalarea and widearea networks (lans and wans). there is also aconvergence between networks that have traditionally carried only voice andvideo over pointtopoint links (circuitswitching) and those that have handledonly data such as electronic mail and file transfers (packetswitching).widearea networksthe fabric of our national telecommunications infrastructure is beingradically altered by the rapid installation of fiber optic cabling capable ofnetworking and communications363virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.operating at gigabit speeds for longhaul traffic. the change has come so fastthat at&t wrote off $3 billion of equipment in a single year while replacing itsanalog plant with digital systems. longdistance carriers have been quietlyinstalling synchronous optical network (sonet) switches to support thosespeeds. sonet is a u.s. and international standard for optical signals; it is thesynchronous frame structure for multiplexed digital traffic and the operatingprocedure for fiber optic switching and transmission systems. sonet allowslowerspeed channels such as oc3 (155 mbit/s) to be inserted and extractedfrom the main rate. also, sonet defines data transmission speeds to 2.4 gbit/sand the major carriers believe that this can be extended to 10 gbit/s for a singlefiber link (ballart and ching, 1989). it is likely that data rates will go muchhigher in the next century. japan's telephone company, nt&t, has announcedtransmission of a 20 gbit/s data stream over 600 miles of fiber, and it isworking to increase throughput to 100 gbit/s using soliton technology. bothmci and sprint have announced that they will have sonet completely deployedby 1995.the new switches will also incorporate asynchronous mode technology(atm). atm (2.4 gbit/s) provides fast variablerate packetswitching usingfixedlength 53 byte cells. this permits atm networks to carry bothasynchronous and isochronous (video and voice) transmissions at sonet speeds.atm also supports multicasting, and the consultative committee oninternational telegraph and telephony has written a standard for interfacingatm with sonet. at&t has announced that it will provide wan atmservices in 1994.two other highspeed services are being offered today: switchedmultimegabit data service (smds) and frame relay. smds, based on theinstitute for electrical and electronics engineers 802.6 man standard, isconnectionless, uses frames and fixedlength cells, and offers speeds up to 34mbit/s with plans to upgrade to 155 mbit/s. it is currently being offered only inlocal metropolitan areas. frame relay is connectionoriented (dialup) and offersspeeds up to 1.544 mbit/s. although neither of the services are considered wellsuited for voice or video applications, they are likely to reduce the datatransmission cost of wan services.the major carriers are not alone in this effort to push wide area networkingto faster speeds. the backbone of the internet, nsfnet, has been completelyupgraded to t3 (45 mbit/s) and will transition to oc3 (155 mbit/s) by 1994.the backbone rate is expected to go to oc12 (622 mbit/s) by 1996. thenational science foundation is responsible for this effort as part of the overallnational research and education network (nren) project, which is one of thefour components in the u.s. high performancenetworking and communications364virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.computing and communications (hpcc) program, established by vicepresident gore. part of the project involves the installation of oc12 networksat several regional test beds: aurora, casa, blanca, nectar, and vistanet. thesetest beds will be pursuing ''grand challenge" applications ranging from medicalimaging to interactive visualization using atm, smds, and sonet technologies(johnson, 1992).the internet engineering task force, foreseeing future increases in wanspeeds, has been conducting experiments over the internet multicast backbonenetwork to develop a standard for sending multimedia applications over packetswitching networks. the realtime transport protocol that is currently beingtested supports packet video and audio and could be used with atm as itbecomes more pervasive.at the local loop (the telephone line between the central office andcustomers), intense competitive pressures in the cable and telephone industriesare spurring the development of new technologies to allow the currentlyinstalled copper lines to operate at megabit speeds without expensive repeaters.the highbitrate digital subscriber loop is an encoding scheme being used nowto deliver duplex t1 service. another scheme that is in the trial stage,asymmetric digital subscriber loop, provides 1.5 mbit/s in one direction and 16kbit/s in the other. with the use of new compression standards such as those ofthe consultative committee on international telegraph and telephony (h.261šp × 64) and the motion pictures experts group (mpeg), adslii, a followontechnology with 3 to 4 mbit/s transport capability, could carry realtime video,audio, and ve data (hsing, 1993).the rewiring of the local loop has also begun. telecommunications inc.(tci), the nation's largest cable company, has announced that it intends toupgrade by 1996 the broadband lines to more than 90 percent of its customerswith fiber. tci is doing this in order to support increased channel capacity, highdefinition television (hdtvšwhich, when uncompressed, requires 1.2 gbit/sbandwidth), and ve services such as games from sega/genesis. tci also willtry to counter the threat of the telephone companies entering the lucrativemarket. at&t has had a test bed for developing the fiber optic local loop forseveral years near pittsburgh. bell atlantic, a regional phone carrier, isconducting tests in its employees' homes of a system that delivers movies overthe telephone line. the regional bell operating companies recently proposedto the clinton administration a plan to rewire the local loop with fiber opticcable within 10 years in exchange for permission to enter the informationservices market and manufacture telecommunications equipment. a bill thatfailed to pass congress in 1993 would have permitted the regional belloperating companies to enter the cable business.networking and communications365virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.localarea networksthe connection from a threedimensional graphics workstation to highspeed wans is most likely to come from a lan. (table 101 presents lancapabilities.) most lans use ethernet (10 mbit/s), which is inadequate for thehighperformance demands of ve and multimedia. several companies haveendorsed the proposal of standards for 100 mbit/s ethernet. in addition, throughthe use of switching hubs (which are collapsed backbones with gigabitspeedbackplanes), a workstation can use all the bandwidth available on an ethernetsegment.fddi (fiber distributed data interfaceš100 mbit/s) is used extensivelyin supercomputer centers. however, most host interfaces operate in the 2050mbit/s range. a new standard for fddi over unshielded twisted pair wiringshould make fddi more affordable for general computing. unfortunately, bothfddi and ethernet technologies are not ideal for isochronous data becausethere is no guaranteed data rate or prioritizing in the protocols. the americannational standards institute (ansi) has developed fddiii to address thisproblem by dynamically allocating bandwidth to isochronous applications.ansi is working on fddi followon with plans for completion to be finishedin the middle of the decade. fddi followon is likely to be designed for speedsup to 1.25 gbit/s. in addition ieee has established a working group that issueda final draft of a standard, the integrated services lan interface, which definesa lan that carries voice, data, and video traffic over unshielded twisted pairwires.the highperformance parallel interface (hppi) is an ansi standardsupporting 32 and 64 bit interfaces that run at rates of 800 and 1,600 mbit/s,respectively. it is a switched architecture and operates over a distance of 25 mon copper cables connecting supercomputers and their peripheral devices. aserial version of hppi for fiber optic cables has been proposed to extend therange to 10 km. the nren casa test bed researchers fromtable 101 local area network (lan) capabilitieslan technologycapacity mbit/syear of finalstandardstatusethernet101985in usefddi1001989in usehppi800/16001992in usefiber channel132.8  1064.21993some productsavailableatm456221993some productsavailablefddifo1250approx. 1995not availablenetworking and communications366virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.los alamos, california institute of technology, the jet propulsion laboratory,the san diego supercomputing center, and the university of california, losangeles, are developing hppisonet interfaces to connect supercomputers overmultiple oc3 circuits, providing 1.2 to 2.5 gbit/s bandwidth (cattlett, 1992).fiber channel is a proposed ansi standard for veryhighspeed lans. itis designed to connect more than 4,000 computers and peripherals over severalkilometers at data rates up to 1,062.4 mbit/s. fiber channel will provide anumber of upperlayer network services that hppi does not, and it has thebacking of ibm and sun microsystems. another proposed standard, scalablecoherent interface, has a potential speed of 8 gbit/s (cattlett, 1992).atm has also been deployed for local area networks. several vendors,including fore systems inc. and adaptive corp., are selling atm switches forlans. fore systems sells interfaces card for sgi, dec, and sun workstations.each workstation is linked via fiber optic cable to a switch at 140 mbit/s. theaurora and nectar test beds are investigating the use of atm host interfaces forsupercomputers (cattlett, 1992). the allure of atm is that it might eliminatethe distinction between widearea and localarea networks, providing highspeed connectivity from desktops across the united states.issues to be addresseddespite the apparent momentum in the development of networktechnology, there are still many problems that impede its use for ve. first, thehardware technology is emerging so rapidly that standards are in flux. forexample, atm is not completely specified because the standards bodiesrepresenting the system users and developers have not agreed on all the protocolrequirements. how atm maps to the upperlayer software protocols fortransport services and routing is not yet clear (cavanaugh and salo, 1992).other protocols, like fddiii, probably will not be implemented widely unlessatm does not succeed.operating at gigabit speeds presents a new set of problems for networking.new methods of handling congestion are required because of the high ratio ofpropagation time to cell transmission time (habib and saadawi, 1991). by thetime a computer in new york sends a message telling a host in san francisco tostop sending data, a gigabit of information will have been transmitted. latencyalso becomes a major concern. much as a jet aircraft can be severely damagedby a small bird, short delays can cause major disruptions for highspeednetworks and ve applications that demand realtime performance.the most likely bottlenecks identified in the aurora project at thenetworking and communications367virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.university of pennsylvania will be in the network interfaces, memoryarchitectures, and operating systems of the computers on either end. forexample, the fore systems atm interface for the sgi indigo can handle only20 mbit/s of data, even though the media can deliver 140 mbit/s. the slowprogress in increasing the interface performance of fddi is an example of thelag in technologies we may see as highspeed networks are fully deployed. norhave memory speeds kept up with the strides made in central processing unitand network performance.at the operating system level, most ve applications are built oncommercial versions of unixša system that is not designed for realtimeperformance. additional problems are introduced by questions surrounding theadequacy of current transport protocols, like transmission control protocol(tcp), that provide an interface between the operating system and the network.furthermore, there is strong debate concerning the efficiency of the newgeneration of interface protocols, like versatile message transaction protocol(vmtp) and xpress transfer protocol (xtpšdesigned to be implemented insilicon) (rudin and williamson, 1989). other protocols, such as stiideveloped by bolt beranek and newman (bbn) for the defense simulationsinternet and the osi suite, are challenging the internet protocol (ip) asnetworked applications like ve demand a host of integrated network services,such as multicast support and resource information for dynamic bandwidthallocation.bbn has also developed deadreckoning techniques to abstract data fromsimulators, thus reducing the communications loads on the network (miller etal., 1989). developers of distributed ve should follow this example andexamine how to balance network bandwidth requirements with better efforts atpreprocessing the data.finally, perhaps the greatest impediment to distributed ve is the lack ofoverall standards in the field, extending from file formats for threedimensionalmodels, to graphic and video images, to audio, to application interfaces. theieee standard for distributed interactive simulation applications is in itsinfancy and is incomplete. although it does not define a standard applicable tothe diverse requirements of ve, it marks a milestone because it shows awidening of understanding of the relationship between ve and networkingtechnology.a related concern is networking architecture that will accommodate theconnection of large numbers of communicating devices of different kinds thatare delivering and using different kinds of services. standards are needed forimplementing architectural concepts as well as for achieving interoperability. arecent report of the national research council (1994) points out the importanceof an open data network architecture (based on shared technical standards thatenable users and providers to interact freely and that permit technologyupgrades) in fulfilling the promisenetworking and communications368virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.of the national information infrastructure. this lesson also is applicable todistributed ve.communications softwarecommunications software passes changes in the virtual world model toother players on the network and allows the entry of previously undescribedplayers into the system. it is quite easy to exceed the capabilities of a singleworkstation when constructing a virtual world, especially if one expectsmultiple players in that world. as we move to a networked environment, we gobeyond graphics and interface software issues to a much more complicatedsystem involving database consistency issues. a standard message protocolbetween workstations is needed to communicate changes to the world. forsmall systems, it is important to ensure that all players on the network have thesame world models and descriptions as time moves forward in the ve action.in systems with fewer than 500 players, each node in the virtual world hasa complete model of the world. the current simnet system looks like this anduses ethernet and t1 links. for systems with more players (1,000 to 300,000, asenvisioned for the defense department's louisiana maneuvers project), it is notreasonable to propagate complete models of the world but rather to considerrolling in the world model just as aircraft simulators roll in terrain. althoughonly a few researchers are examining this problem, there is at least anabstraction that might be relevant in the work of gelernter (1991), entitled"mirror worlds." mirror worlds presents the notion of information tuples (likea distributed blackboard) and tuple operations: publish, read, and consume.such an abstraction allows flexibility in communicating any type of informationthroughout a large, distributed system; flexibility that is necessary forconstructing large virtual worlds. although the abstraction appears appropriate,efficient and realtime implementations are an open research problem.simnet, which is a simulation of entities on a battlefield, is currently thelargest communications network in any ve (pope, 1989; thorpe, 1987). it is astandard for distributed interactive simulations developed under arpa auspicesthat began running in 1988. distributed means that the processing of thesimulation can take place on different hosts on a network. interactive meansthat the simulation can be dynamic and guided by human operators. simnet isalso a network protocol with a welldefined set of communication packetscalled protocol data units (pdus). in addition to packet definitions, simnetalso defines algorithms for the dead reckoning of vehicles whose velocities anddirections can be predicted. the purpose of the dead reckoning is to minimizetraffic on the computer network. simnet currently uses t1 links for longdistance andnetworking and communications369virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.ethernet for local communications. the protocol does not use tcp/ip,multicasting or any other network services. simnet sits on top of the devicedriver/link layer and, consequently, it requires that processes reading/writingsimnet packets run as root privileged. simnet is limited in that it is onlycapable of engagements of a maximum of 300 players. also the simnetprotocol does not allow for generalized information transfer.distributed interactive simulation (dis) is the latest standard forcommunicating defense department simulations (ist, 1991), with a latency ofless than 100 ms in situations in which players are expected to interact. this isto minimize human perception of lag, which can induce simulator sickness. ithas common goals with simnet but, as its replacement, is far more ambitious.dis is intended to overcome the limitations imposed by simnet and toinclude packet definitions not present in simnet. dis uses the defensesimulation internet (dsi) as its support network. dsi is currently beinginstalled in more than 150 sites throughout the united states. the npsnetproject is connected to this network and currently displays records propagatedby the dis 2.0.3 draft standard. dis uses ip multicasting services and does notrequire that its processes run under root privilege. dis is planned to grow, withthe eventual communicating player load expanding to 10,000 to 300,000players. a discompliant version of npsnet, npsnetiv, was recentlydemonstrated at siggraph '93; approximately 50 players located inwashington, d.c.; anaheim, california; dayton, ohio; and san diego andmonterey, california, were connected. the hardware communications mediumwas ethernet and a gateway machine to a t1 wan.there are additional players in the development of networked virtualenvironments; a good source is the networked virtual environments andteleoperation special issue of presence (1994).research needsadvances in network hardware and communication software are key to thefull realization of virtual environments. the following paragraphs detail someof the key network and communications needs generated by virtualenvironments.hardwarealthough highspeed networks will provide the required computationalpower to build largescale ve systems, there are several problems to beaddressed. first, the actual cost of wans may present a problem for thedevelopment of large scale ves. the current price of a oneyear t1 (1.5 mbit/s)link is beyond the budget of most ve research groups, and we already know t1is too slow. this is an issue of accessibility. establishingnetworking and communications370virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.a subsidized, nationwide, open ve network may be one way to eliminate thisbottleneck to networked ve development.second, network host interface slowdown problems are another significantproblem for the largescale ves of the future. we have highspeed interfacestoday, but the layers of unix system software make these interfaces hard toutilize at their full rates.third, because latency across long distances is a permanent problem forve systems (i.e., the speed of light), there is a need to build softwaremechanisms to cope with latencyšsuch as dead reckoning or predictivemodeling. singlepacket dis transfers require approximately 300 ms betweenmonterey, california, and washington, d.c., today using the npsnet andunix software layers and ethernet/t1 links. while npsnet deadreckoningalgorithms for vehicles provide some predictive capabilities, these algorithmsare not generalizable to other player paradigms. in addition, there is a long wayto go to reach the 10 ms necessary for two participants on opposite ends of alongdistance network to work together cooperatively in real time.fourth, network hardware technology is evolving so rapidly that standardsare in flux, and we are currently at risk of having them set by the largeentertainment conglomerates. at the present time, there is only onegovernmental effort in network standard setting, the dis packet format, ieeestandard 1278šand this standard can be considered a stopgap. dis wasdeveloped as a standard of communication for military vehicle simulators andoperates with ethernet/t1 links and a particular software architecture for virtualenvironments. it was not designed for generalized information exchangebetween largescale virtual environments; it is not even general enough tohandle the articulations necessary for an animated, walking human figure.packetswitching standards should be set that take into consideration therequirements of distributed ve systems with huge databases and potentiallylarge numbers of participants.softwarethe problem of generalized information distribution in a largescale,virtual environment of more than 300 participants is not yet solved. there areinteresting abstractions but no real solutions (gelernter, 1991). for example, thesolutions offered by the department of defense's dis standard are limited to300 participants or less and are too specific and complex to be useful to thegeneral ve development community. in the near future, both the military andcivilian communities will have requirements for environments capable ofhandling as many as 10,000 to 300,000 simultaneous participants. there areseveral fundamental infrastructure and software problems associated with theresearch to develop the communicationsnetworking and communications371virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.software needed to solve the technical issue of interaction among thousands ofparticipants.one of the primary infrastructure problems is that only a few universityresearch groups are working in networking largescale distributed ves, andthey are constrained in at least two ways. first, the networks are operatingwithin extremely limited boundsšthe department of defense (dis) bounds;and second, they are very expensive to run. dis is an applications protocoldeveloped under arpa and u.s. army contract as the networking protocol fordepartment of defense simulators. although dis is known to have significantproblems (i.e., it is limited in capability and too large for what it needs to do), ithas been made an ieee standard. the department of defense is now putting allof its development resources into using this protocol for moving from thesimnetsized limit of approximately 300 participants (using ethernet/t1links) toward the 10,000 to 300,000 participant level. what is needed is a majorresearch initiative to investigate dis alternativesšalternatives that will allow ageneralized exchange of information between the distributed participants oflargescale virtual environments. the new protocol needs to be extensible, afeature that does not appear to be a part of dis.a second infrastructure issue is the high cost of research into largescalenetworked virtual environments. there are very few universities that can affordto dedicate the t1 lines (with installation expenses of $40,000 and operatingcosts of $140,000 per year) needed to support these activities. at the presenttime, only two universities in the united states have such dedicated resources:the university of central florida and the naval postgraduate school. variousapproaches, such as an open ve network and the necessary applicationsprotocol, should be considered for providing research universities with access tothe needed facilities. unless costs are significantly reduced, a concerteddevelopment effort on software solutions for networked ve cannot begin.a critical ingredient in the development of largescale networks for ve isthe interest of the entertainment industry in introducing telecomputer andinteractive video games into the home. to date, cooperative financialarrangements have been made between manufacturers of video games and largecorporations already in the telecommunications business. the focus of thesearrangements is to provide lowend, relatively inexpensive systems for largenumbers of participants with an eye to making a profit. like the department ofdefense, the video game industry is not interested in generalizability ofinformation transfer, nor is it interested in openness and accessibility. thedanger is that the video game industry will set the networking protocolstandards at the low end and the defense/dis community will set the standardsat the high end. neither of these standards is general enough for the widespreadve application development we would like to see.networking and communications372virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.11evaluation of synthetic environmentsystemsmany questions arise about design, performance, usability, and costeffectiveness as a new system progresses from inspiration to realization. acarefully chosen program of evaluation studies, conducted throughout thedevelopment and operational life of a system, can help provide the answers tosuch questions, as well as reduce development time and minimize the need forexpensive design changes. to the extent that synthetic environments (se)systems are based on a family of new technologies configured in new ways toperform new functions, the need for evaluation studies becomes especiallyimportant. in this chapter we first outline a variety of approaches to evaluationand identify some key issues in the evaluation of systems in general, whether ornot they are sebased. we then comment on special issues in the evaluation ofses, including the current tendency to ignore, or at least minimize, theevaluation problem.general issues in system evaluationthere are many practical reasons why evaluation studies should beconducted. at the outset of development, they can be used to refine therequirements for a system and to compare design concepts. (for simplicity, andunless stated otherwise, we use the term system here to refer to components andsubsystems of se systems as well as to complete se systems.) once a designhas been chosen, evaluation studies can be used to diagnose problems andsuggest alternative approaches. if appropriate, the results of these formativeevaluations can also be used to facilitateevaluation of synthetic environment systems373virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.communication with a sponsor or a customer. to be useful, the findings fromsuch formative studies must be timely; this may require that scientific rigor andprecision be traded for speed. accordingly, rapid prototyping and simulationsare often used to provide representations of the elements to be examined informative evaluations.once a system has been developed, a summative evaluation can be used tomeasure the capability of the system to fulfill its intended function, to compareits performance with that of alternative systems, and to assess its acceptance byintended users. for example, the training effectiveness of a virtual environment(ve) training system might be compared with that of a conventional trainingsimulator. quantitative measures of training performance and training transfer,together with pooled ratings of experts and judgmental information about thefriendliness of the system, all would have a role in such a summativeevaluation. taken together, formative and summative evaluations provide acritique of a system over its entire lifecycle. finally, evaluation studies can beused to estimate the costeffectiveness of an se system in performing aparticular application function. the results of these studies can then be used toinform policy decisions about investment and production.the specific type of evaluation to be conducted will depend, of course, onthe characteristics of the system to be evaluated as well as the purpose of theevaluation. one dimension along which evaluations vary, mentioned in thepreceding paragraph, concerns the extent to which the purpose of the evaluationis to guide system development or to determine various characteristics of asystem that has already been developed (e.g., related to overall performance,costeffectiveness). a second dimension concerns the amount and type ofempirical work involved. the empirical component of the evaluation can berestricted to pure observation (of how the system performs, how the userbehaves, how the market reacts); it can involve surveys in which system usersand other individuals affected by the system are asked questions; or it caninvolve highly structured and controlled scientific experiments. under certainconditions, evaluations can be conducted without any empirical work at all andbe based solely on theoretical analyses, for example, when appropriate modelsare available for describing all relevant components and performance can bereliably estimated solely by human or machine computation. a third dimensionconcerns the extent to which the item being evaluated constitutes a wholesystem or just one of the components in the system of interest. as one wouldexpect, in most cases, the evaluation of a system component is much simplerthan the evaluation of the whole system (particularly when the whole systeminvolves a human operator). a fourth dimension concerns the extent to whichthe evaluation is analytic, in the sense of providing information on how theperformance of the whole system is related toevaluation of synthetic environment systems374virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.the performance of various components and subsystems. obviously, analyticevaluations play an exceedingly important role in guiding the development ofimproved systems. a fifth dimension concerns the distinction between staticand dynamic tests. whereas static evaluation methods focus on nonperformanceattributes of a system, dynamic methods focus on performance attributes.general background on types and methods of evaluation can be found inmeister (1985).special issues in se evaluationas discussed throughout this book, the creation of se systems draws onprevious work in, and provides research and development challenges to, a widevariety of established disciplines, including computer science, electrical andmechanical engineering, sensorimotor psychophysics, cognitive psychology,and human factors. in each discipline, the requirements associated with creatingcosteffective se technology raise new questions that call for evaluations withinthe context of research and development. in general, evaluation studies of sesand se technology are needed to help ensure that: (1) the perceptual andcognitive capabilities and limitations of human beings, as well as the needs ofthe specific tasks under consideration, are being used as driving criteria forsystem design; (2) hardware and software deliver ses in a costeffectivemanner; and (3) se applications represent a significantly better way of doingold things or of doing new things that have never before been possible.despite the clear need for evaluation in the se area, the types and amountsof evaluation currently taking place in this area are rather limited (a brief reviewof the limited work on performance measurement in teleoperator systems isprovided in chapter 9; three highly experimental studies on ve are described inchapter 12). this is undoubtedly due, at least in part, to the high level ofenthusiasm that exists about what the technology is likely to be able toaccomplish, as well as the belief among many individuals in the se field that nospecial evaluation efforts are required. according to this belief, the informalevaluations that take place more or less automatically as one is developing asystem and the evaluation evidenced by the degree of acceptance in themarketplace (i.e., a system is good or bad according to whether it is used or notused) are sufficient. although these forms of evaluation are necessary, thecommittee does not agree that they are sufficient; the costeffectiveness of theresearch and development is likely to be significantly increased if the task ofevaluation is taken more seriously. although many existing evaluation tools canbe adapted for use with ses, a variety of new tools will need to be developed inorder to evaluate the unique properties of this technology. in the followingparagraphs, we comment briefly on some of theevaluation of synthetic environment systems375virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.considerations that are relevant to the design of an analytic evaluation of an sesystem. we discuss evaluation of se system characteristics and issues that arisein observing and measuring human behavior in se systems.evaluation of system characteristicsperhaps the first general evaluation task consists of measuring the physicalcharacteristics of the se system and considering how these characteristics relateto those of the prospective human user. thus, for example, the characteristics ofthe displays and controls (dynamic ranges, resolutions, time lags, distortions,internally generated noises) should be measured and compared with thesensorimotor capabilities of humans as determined from psychophysicalstudies. ideally, some kind of metric of physical fidelity should be developedthat takes account not only of the fidelity that exists in all the relevantsensorimotor channels, but also of the extent to which this fidelity falls short ofthe maximally useful fidelity and of the implications of this shortfall for overallsystem performance. another portion of the evaluation effort should focus onan analysis of the task to be performed by the se system and an examination ofhow well the system is designed to perform this task. such an evaluation wouldtake account of physical fidelity and how such fidelity is expected to influenceperformance on various task components, as well as more central issues, such asthe degree to which the se system has been designed to anticipate the user'sintention by examination of, and extrapolation from, the control signals. afurther set of related issues can perhaps be best grouped under the heading of''cognitive fidelity." such issues arise in connection with interface design andthe interaction metaphors employed in this design, as well as the structure andfunction of the machine (computer or telerobot) to which the human isinterfaced. the classic notion of stimulusresponse compatibility is a pale andrestricted version of the cognitive fidelity factors that require consideration inan analytic evaluation of an se system.observation and measurement of human behavior in sesin addition to evaluating basic system characteristics, it is important toevaluate overall system performance and how well the system performs thetasks for which it was designed. less obvious, but also of considerableimportance, is the need to examine the behavior of the human operator in these system. the most obvious method for accomplishing this involves storingall the signals that occur as part of the se operation (i.e., all the display signalsand all the control signals flowing inevaluation of synthetic environment systems376virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.and out of the interface) and then studying this set of signals. in order to makesuch a procedure meaningful and efficient, however, procedures must bedeveloped for filtering and transforming this mass of data in a manner thataddresses welldefined evaluation questions. one such question, for example,might relate to how the user's behavior (as measured by the relation of thecontrol signals to the previous display signals) compares with the behavior thatwould have been generated under the same circumstances by some modeloperator (e.g., an operator that is ideal according to some welldefined criterion).beyond examining stored se records, evaluation of human behavior in these can make use of supplementing information derived from externalobservations and measurements. such information could be obtained, forexample, from direct observation of the subject by the evaluator, from video oraudio recordings or physiological measurements of the subject, or fromadministering questions to the subject after termination of the se experience.further information can be obtained by performing experiments during these experience. for example, the evaluator can intentionally degrade the systemin some fashion to probe the effects of degradations that are likely to occurduring field use. similarly, the evaluator can introduce special signals into thedisplays and observe the subject's actions in response to these special signals(e.g., to test alertness).additional supplementary information can be obtained by having theevaluator enter the se in which the subject is operating and makingobservations and measurements of subject behavior from within the se. thiscan be done passively and unobtrusively (in the sense that the subject'senvironment remains identical to that which would have existed if the evaluatorhad remained outside) or the evaluator can intentionally interact with thesubject or the subject's environment. in general, observations and experimentscan be performed either from outside the se or from inside the se.among the special features to consider when designing an se evaluationprogram are those related to measurements of the sensation of presence and thesopite syndrome. although it has not yet been demonstrated that the sense ofpresence is an important variable for predicting objective performance (it hasnot yet even been adequately defined), it seems likely that interest in thisvariable will continue. also, it is clear that the extent to which an se systemelicits the sopite syndrome is of major importance. thus, it is important toconsider measuring both these variables in any comprehensive se systemevaluation. (information on both the sense of presence and the sopite syndromeare available in recent issues of presence.)special considerations arise in connection with assessing system usabilityevaluation of synthetic environment systems377virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.and acceptance. for example, in addition to choosing individual test subjectswho truly constitute a representative sample of the anticipated population ofusers as test subjects, because many envisioned se applications involvesimultaneous multiple users communicating and working together on a commontask, attention must be given to appropriate sampling of groups of users as wellas to observations and measurements of relevant group processes.finally, because of the immersive character of ses, special attention in seevaluation must be given to possible negative longterm psychological andsocial effects. illustrative questions in this category include the following: towhat extent, if any, will individuals begin to confuse occurrences in ses withoccurrences in the real world? how will an individual's selfimage beinfluenced by spending large amounts of time in ses that seriously transformthe individual's interactions with the environment? what impact willwidespread use of networked ses have on various types of social institutions?fundamental psychosocial questions of this type are not likely to be addressedadequately (if at all) by the developers of se technology. however, it isimportant that they be seriously addressed by some group.in general and as a consequence of the many special features associatedwith se evaluation, as well as the current tendency to ignore evaluation in these field, the committee believes that it would be extremely useful to develop aspecial evaluation tool kit for this field. such a tool kit could serve to educatepeople in the field, to provide a more or less standardized set of evaluation toolsfor the field, and eventually to help provide a cumulative and shareabledatabase that would constitute both a current snapshot of accomplishments inthe field and a guide for future research and development.evaluation of synthetic environment systems378virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.iiiapplications 379virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved. 380virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.as the chapters of this book have noted, synthetic environment (se)systems present a number of highly challenging scientific and technologicalresearch problems that, in an ideal and unconstrained funding environment,could be supported solely on the basis of intrinsic scientific interest. however,the funding environment is hardly unconstrained. policy makers are moreconcerned with the promise of applications to areas of national need oreconomic significance than they are with the purely scientific or technicalpromise or potential in a given area of investigation. cognizant of this priority,researchers are often tempted to promise a cornucopia of applications evenwhen the state of the art does not permit realization of these applications.avoiding the hyperbole that leads to a later fall is thus of paramount importance.both virtual environments (ve) and augmented reality pose challengingintellectual problems and also evidence considerable potential for a wide varietyof applications; however, with few exceptions (one of which is entertainment),serious commercial applications (as opposed to research demonstrations ofconcept feasibility and promise) are likely to be realizable only in a longtermtime framešperhaps 5 to 10 years. this is not to say that meaningful progresscannot be demonstrated in a shorter time framešonly that according to themetric of commercial viability, major economic and social benefits are notexpected to be demonstrated in the near future.teleoperation, in contrast, has already been used extensively in a variety ofactivities, including handling nuclear materials, operating heavy machinery,exploring space, performing underwater inspections, and removing hazardouswaste. furthermore, there are a number of experimental programs in which theuse of teleoperation is being explored for surgery, patient monitoring, anddelivery of remote treatment. although teleoperation technology can providemany potential benefits, it has not, to date, been demonstrated to have a highcommercial value.that said, the scientific and technical study of se is entirely compatiblewith a tight integration between research and applications development. thehistory of se suggests that many interesting research problems in the field havearisen from difficulties faced by applications developers concerning, forexample, perception, motion sickness, and software development for realtimeinteractive systems.given that applicationsoriented work in areas of national need isappropriate, it is important to choose judiciously which applications should besingled out for nearterm attention. an important consideration in this choice isthe fact that the entertainment industry has been the primary driver ofnonfederal work in ve. for all practical purposes, this industry has supportedthe development of lowcost, lowperformance proprietary hardware andsoftware. this suggests that federal efforts 381virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.should focus more appropriately on the development of medium and highendtechnology that can be readily shared. however it does not specify whichamong the various possible application areas should be most stronglysupported. the committee has developed five criteria for determiningappropriate application areas. specifically, the applications should be:(1) of demonstrable national importance;(2) intellectually challenging;(3) realizable in a relatively short time;(4) an area to which se technology can make unique contributions; and(5) an area in which success can be achieved through modest applicationof additional federal efforts.using these criteria, the committee focused on the areas of design,manufacturing, and marketing; medicine and health care; hazardous operations;training; education; information visualization; and telecommunications andteletravel. our selection of these seven areas should not be construed to meanthat these are the only areas worthy of special interest. a different committeewith different expertise and interests could well have chosen another set.important omissions to this list are entertainment, art, and national defense.entertainment is not discussed in detail because it is receiving significantcommercial support and because the development of technology for thispurpose was not deemed a pressing national need. art is not discussedseparately because ve in its current stage of interface technology developmentprovides only marginal opportunities for artistic expression over those offeredby more traditional computergenerated art. however, as the interfacetechnology improves, we expect a significant increase in the use of ve for art.national defense is not treated separately because it includes functions andtasks that are represented in many of the other application domains discussed.we do, however, briefly mention certain facets of these areas because of theirimportant roles in technology development and implementation.progress in the entertainment industry regarding virtual environments is ofinterest because its contribution to the field will probably generalize to otherapplication areas. moreover, entertainment can be expected to be a primarydriver and test bed for certain aspects of the technology. according to thepopular press, since 1992, many joint ventures have been created among videogame companies, computer graphics companies, motion picture studios, andtelecommunication conglomerates to use ve technology as a new medium forentertainment, education, and artistic expression. although several of theseventures are 382virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.occurring in the united states, large programs are also under development injapan and europe (fisher, 1993). to date, the entertainment industry's efforts inve have been proceeding on several fronts, ranging from lowend systems forhome use to arcade games, locationbased entertainment, and theme parks.at the low end of the technology, several companies, including sony,reflection technology, olympus, and sega, are developing inexpensive vedisplays for use in the home with interactive, threedimensional games. inarcades, ve action games involving one or more players are now appearing.those currently in existence offer motion platforms and realistic interaction, butthe visual quality remains poor. locationbased entertainment differs fromarcade games in that it provides several interactive systems on a commontheme. such systems usually involve several players sharing a virtual spaceover a local area network. in the near future, paramount communication will beintroducing a locationbased star trek game in which players enter the bridgeof a starship, become one of the characters, and interact with other characters.as the industry moves from passive viewing and interactive twodimensionalenvironments to the realism of threedimensional environments in whichmultiple participants actively play in a fantasy world, serious questions areraised about the effects of the content of these worlds on human behavior. ofparticular concern is the use of such worlds to depict violence and sex; both ofthese areas have been heavily hyped in newspapers and magazines.ve also offers a new medium for artistic expression that has only begun tobe explored (see loeffler and anderson, 1994; leonardo, 1994). a number ofindividuals are designing highly imaginative video games. a few artists haveproduced special ve experiences, and some programs are beginning to emergethat encourage artists to create ve art pieces; however, these efforts are in theirinfancy. although ve has the potential to support the creation of a wide varietyof aesthetic experiences, it is too early in the development process to know howthe technology will influence the techniques, effects, genres, and content thatwill emerge to define the medium. in the case of film, another artistic medium,the process of discovering new techniques (e.g., cuts, pans, fades, slow motion,closeups and telephoto shots) and using them to create new experiences for theviewing public has taken many decades to evolve.a few organizations have developed ve art programs or exhibitions overthe last few years. the banff centre for the arts in canada has had an ongoingprogram allowing artists to create ve art pieces since 1991, and about a dozenpieces have been created so far (moser, 1991). the annual exhibition arselectronica in linz, austria, has included ve art pieces in recent years(hattinger et al., 1990). since 1991 the annual 383virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.siggraph computer graphics conference has sponsored an exhibitionincluding ve demos and art pieces (lineham, 1993).currently, most efforts are directed toward building the hardware andsoftware that can effectively generate realistic ves. as adequate equipmentbecomes more widely available and affordable, we expect to see an increasingnumber of artists exploring its potential. although it is difficult to make reliablepredictions, it seems fairly obvious that future ve art will make extensive useof observer participation and interactivity of various kinds. precisely how thesefeatures, as well as the other special features of ve, are used to create trulyartistic experiences, however, remains to be seen. in order to encourage the useof ve for purely artistic purposes, it will undoubtedly be necessary for the artworld to create appropriate supporting attitudes, programs, and fundingopportunities.finally, as noted above, national defense is not treated as a separateapplication area because its scope intersects with functions in all theapplications discussed in chapter 12. for example, information visualizationand distributed collaboration are critical to strategic and tactical engagementplanning; hazardous operations relates to the use of technology in handlingunsafe materials or remotely operating vehicles in hostile environments; andtelemedicine and the eventual promise of remote surgery are important to therapid provision of medical support to soldiers on the battlefield or to thoselocated in isolated or inaccessible locations. the two most relevant applicationsto military concerns may be training and design and manufacturing.according to a recent draft strategic plan (thorpe, 1993), the mostambitious application of se technology in the military is the integration,management, development, and acquisition of new systems such as tanks oraircraft. in this application, the technology will be used to model and testalternative systems in a variety of synthetic exercises. the most promisingconfiguration will then be designed by a computeraided design (cad) system,manufactured in a virtual factory, and tested in an se. once the appropriatetechnology is available, the entire system can be selected, designed (withmanufacturing specifications, time, and cost included) and tested without theneed for physical prototype development. moreover, individuals and units canreceive training on the new system using virtual war games before the systemhas been produced. the discussion of manufacturing in chapter 12 uses thedesign of an aircraft to describe many of the processes involved in theacquisition of a new system.in the area of training, the department of defense (dod) is currently usingve technology to cover a range of instructional experiences, from those of theindividual soldier and small team to theaterlevel synthetic battlefields in whichmore than 10,000 participants interact in real time 384virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.over distributed networks. for all levels of training, ves can be used to safelyprovide exercises in hostile and dangerous environments. moreover, in thefuture, computers will have the capability to generate ves as replacements forand extensions of traditional equipmentbound simulators. a critical issuediscussed in the training application section of this chapter is the problem ofconducting evaluation studies that demonstrate the impact of one method oranother on training success as defined by how well the knowledge and skillsacquired in training transfer to performance on the job.of particular interest to dod is the development of networking hardwareand software for largescale distributed training. the first large networksimulation of realistic battlefield engagements was simnet, developed bydod's advanced research projects agency. in simnet, as many as 300soldiers in tanks and aircraft simulators located at different military bases canengage in a realistic battle against an intelligent enemy on a commonbattlefield. each participant in the battle views the portion of the terrain and theaction that would be visible to him if he were present. as a battle unfolds, thescene changes in real time. once a battle is completed, it can be replayed as anafteraction briefing in which trainees can zoom in on various portions of theengagement or take the perspective of any tank or aircraft.more recently, dod has used its newest softwarešdistributed interactivesimulation (dis)što develop a detailed, true reconstruction of the 73 eastingsbattle that occurred during the persian gulf war. this fully interactivesimulation is based on the events in an actual battle; as a result, it can be used asa benchmark to examine whatif training scenarios when enemy or friendlyweapon system capabilities are changed. the projections are that dis willprovide many of the networking capabilities needed by the military for bothtraining and system acquisition.the final chapter of this report provides a more detailed description of theuse of se technology for each of the application areas selected for review. 385virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved. 386virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.12specific applications of se systemsdesign, manufacturing, and marketingcomputer technology generally, and synthetic environment (se)technology more specifically, are potentially important drivers for futuredevelopments in design, manufacturing, and product marketing. trends alreadyunder way suggest a movement toward the development of manufacturingsystems in which production processes are integrated with all elements of theproduct lifecycle from concept through sales, including quality, cost, schedule,and the determination of user requirements. this process, known as concurrentengineering, provides for parallel development across product lifecycleactivities through the use of technologies such as computeraided design/computeraided manufacturing (cad/cam) and computerintegratedmanufacturing (cim). using shared databases, customers, designers, andproduction managers can simultaneously evaluate a proposed product design.as a result, the design of the product, as it evolves, can incorporate therequirements of the user, special needs for marketing, and any limitations of theproduction process (krishnaswamy and elshennawy, 1992). once developed,advanced visualization technologies such as virtual environments (ve) mayprovide valuable extensions to current practices.in april 1993, manufacturing was named as one of six national initiativesto be administered by the federal coordinating council for science,engineering, and technology (fccset). the primary focus of the fccsetmission on manufacturing is to assess special opportunities forspecific applications of se systems387virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.technology to change the way manufacturing will be carried out in the nextcentury.rationalealthough the ultimate goal of all manufacturing is to produce a tangibleobject or component, informationšin the form of plans, specifications, andprocessesšplays a most important role. thus, it should be expected thatinformation technology, including ve, could have a meaningful role inmanufacturing by enabling people to generate and manage such informationmore effectively. consider the following points in the lifecycle of amanufactured product: developing design requirements. ve could be used as a medium in whicha customer's mental image of a product can be fashioned into a virtualimage of the product. that image could be subsequently manipulated oreven used as the basis for production specifications. examples:architectural walkthroughs of spatial designs, such as proposed buildings,rooms, and aircraft interiors. undertaking detailed design. ve could provide designers with the abilityto reach inside the design and move elements around, to test foraccessibility, and to try out planned maintenance procedures. thedesigner could thus have a comprehensive view of how changes made inthe design or placement of one component could affect the design of othersystem components. producing the artifact. virtual pilot lines could simulate both human andmachine processes on the production line. such a virtual pilot line couldbe used to predict performance and to diagnose the source of faults orfailures. plant management could be improved as engineers are given thecapability of reviewing and modifying various plant layouts in virtualspace. marketing the artifact. by providing potential customers with the abilityto visualize various uses of an artifact, ve could be used for marketing anarray of completed product designs to customers prior to their production.specific manufacturing applicationsbuilding prototypes electronicallybuilding prototypes electronically provides a number of advantages,including the opportunity for sharing data across manufacturing functions andthe ability to modify designs with greater ease than in a physical mockup. afurther advantage is the ability to incorporate stress andspecific applications of se systems388virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.durability test data into the design process without physically performing eachtest. ve promises to enhance the value of prototyping electronically by offeringcustomers, sales staff, and engineers the ability to walk around the product andmanipulate it in virtual space, much the same way as they would explore aphysical mockup in real space. a longrange goal is to create ve systems thatcan be extended to provide groups of individuals in different locations with thecapability to work together in a shared virtual space.researchers at the university of north carolina (airey et. al., 1990) haveworked on the development of software for creating interactive virtual buildingenvironments. this software can be used to present architectural walkthroughsof buildings that have not yet been constructed. in touring a virtual building, anindividual will be provided with changing views and lighting that are consistentwith his or her position relative to the building space. such software can beuseful for design of any interior spaces, including industrial buildings, hospitaloperating rooms, churches, homes, and aircraft passenger compartments, toname a few.electronic configuration and management of production linesanother potentially important area for the application of ve technology isin the design and testing of the processing, fabrication, and assembly lines.virtual pilot lines might be developed instead of real pilot lines to simulatehuman and machine tasks and make predictions about potential problems forhuman performance and safety as well as estimating the probability of failureand the line's expected operating efficiency. the promise is that virtual pilotlines will be far easier to modify in response to diagnosed problems than aphysical pilot line, and they will provide the opportunity to introduceinformation on manufacturing efficiency early in the product design process. inaddition, a virtual line could be run in parallel with an operating line forpurposes of diagnosing failures, retooling for new products, or changing humanmachine interface designs or procedures at points in the process at which errorsor problems are occurring.although ve technology provides more of a promise than an existingcapability for industry, several forces within various government andmanufacturing enterprises will push for its development and use. from theindustry perspective, ve technology has the potential to make themanufacturing process (from planning through sales) more flexible andeconomical. the aerospace, automobile, and textile industries are pursuing vetechnology as a means for speeding development and making productmodification easier. chrysler, ford, and general motors have formed a veconsortium with the u.s. army vehicle center, the automotivespecific applications of se systems389virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.division of united technologies, the university of michigan, and several smallcompanies. in a recent proposal to the advanced research projects agency(arpa), the consortium predicted that ve technology would lead to improvedproduct design, a better market response, and reductions in time or cost (adam,1993).exemplar industriesin the following sections we provide a discussion of the potential for ve inthe textile and aerospace industries. the selection of these industries was notbased on an exhaustive or systematic search of industries and applications;however, both industries offer some interesting illustrations of ve technologythat transfer broadly to other industries.textilesve may have very important applications in the marketing andmanufacture of clothing. the concept is that customers could shop for apparelin a ve in which they would see virtual clothes on virtual images of their ownbodies and feel how the clothes would fit. on the basis of this experience,customers would select and order outfits that would be fabricated on demandand sent out to them within a short time period. the result would be tosignificantly reduce financial losses associated with fabric waste during apparelproduction and with product markdown and liquidation. moreover, thecustomer would be provided with a greater range of choice and an improvedmadetomeasure fit. this approach appears to be a natural extension of thecurrent market trends of increased shopping through catalogs and homeshopping networks and the accompanying decrease in retail outlet shopping.industry efforts ve technology has captured the interest of the textileindustry (steward, 1993). in 1993, a collaborative research and developmentprogram, the american textile partnership (amtex), was initiated betweenthe department of energy (doe), the doe national laboratories, and the fibers,textiles, and apparel industry to improve the competitiveness of the u.s. textileindustry through the application of technology. the national laboratories plan towork together and coordinate with industry through major industrysupportedresearch and technology transfer facilities. matching funds for the partnershipare to be provided by government and industry. the first joint project betweenthe national laboratories and the industry will involve the creation of an industrymodel for integrating hardware and software in a system to provide demandactivated manufacturing architecture (dama). one aspect of thisspecific applications of se systems390virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.effort will involve research on the uses of ve technology (hall and walsh,1993).the u.s. textile industry, which includes fiber producers, textile weavers,apparel makers, and retailers, employs over 1 million workers (10 percent of themanufacturing work force in the united states) and includes 26,000 companies.it is the largest producer of nondurable goods, experiences annual consumersales of approximately $200 billion, and contributes $53 billion to the u.s.gross national product (hall and walsh, 1993; steward, 1993). each year theindustry fails to realize revenues of approximately $25 billion due to inventorymarkdowns and liquidation.most companies are small, with profit margins of 2 percent or less, and soare not in a position to conduct or support research. almost all the research inthe industry is conducted by five large research centers based in universities andjointly funded by industry and government. one of these centers, the apparelcim center, was established in 1988 with the goals of removing barriers toadopting proven cim technology, establishing cim standards, providingassistance to state industry, and conducting broadbased research anddevelopment to keep the industry competitive.the primary charge of the apparel cim center is to investigateapplications of ve to clothing as seen, examined, and purchased by the retailcustomer. a second charge is to apply ve technology to represent the internalview of a textile manufacturing plant, including the position of machines, the airconditioning, the noise level, and the lighting. the goal of the project is tofacilitate the reorganization of a manufacturing plant by providing engineersand factory workers with the ability to walk through a virtual plant; to movemachines around on the basis of requirements to produce new lines of apparel(seasonal changes); to examine spacing, lighting, and noise to ensure goodhuman factors practices; and to assess the effects of various equipmentconfigurations on work flow.technology requirements the technology required for implementing theinternal plant layout includes: (1) building an object database of all equipmentneeded in the plant, (2) creating the capability to determine light andventilation, (3) providing noise levels based on the combination and spacing ofmachines, (4) matching lighting requirements and noise levels against federalrequirements, (5) integrating new software capabilities with existingsimulations of work flow through the plant for manufacturing differentproducts, and (6) developing an interface for engineers that is easy to use andacceptable. according to steward (1993), all of these activities are under way.there are many technologies, including ve, contributing to this applicationšsome existing and some in development. as these activities evolve, there willbe a need for ve technology to rely on and interface with other developinginformation technologies.specific applications of se systems391virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.developing the technology required to fully implement a ve system formarketing clothing is a longterm effort. one area for development is bodymeasurement technology. currently the concept is to have the customer don abody stocking and be electronically scanned. the linear and volumetricdimensions from the scan would be stored on a card that the customer woulduse when entering the virtual shopping space. when a customer's dimensionschange, he or she could be scanned again. cyberware has built anddemonstrated effective full body scanners. however, this technology isproduced on an individual basis and is expensive to acquire.a second area of development is the technology for accuratelyrepresenting material draping. a critical factor in deciding to purchase agarment is appearance: how the jacket hangs, how the folds appear, how thefabric moves when the individual wearing it moves, etc. thus, the draping ofvirtual clothes on a virtual customer must appear real.other areas requiring technology development include providing accuratecolors in the virtual world, giving customers the opportunity to ''feel" fabrics,and providing customers with a sense of how the garment "fits." all of thesefactors are important to customers in selecting clothing. colors must be accurateso that different parts of an outfit can be matched; feel and fit are critical tocomfort and style. of all the research and technology development issuesidentified above, the most complex and long range will be developing the tactilefeedback needed to create a sense of fit.aerospacethe aerospace industry is expected to be a major user of ve technology inthe future. companies such as boeing and rockwell international have longrange plans to develop ve systems that will provide all interested parties withthe ability to view and interact with threedimensional images of prototype partsor assemblies of prototype parts. currently, both companies are using cadtools to create electronic prototypes of parts in lieu of physical mockups.industry efforts staff at rockwell international, through its virtual realitylaboratory (tinker, 1993), are working on virtual prototypes and mockups;virtual world human factors assessment for proposed task environments; andtraining for manual factory workers, maintenance personnel, and equipmentoperators. these efforts are in the early stages of implementation. proprietarysoftware has been developed to read cad data into a virtual reality database.the longrange goal is to provide the ability for multiple participants to worktogether in a shared virtual space interacting with highresolution cad data inreal time.specific applications of se systems392virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.at boeing, the design of the 777 aircraft is being accomplished without aphysical mockup; all of the 6.5 million parts are being prototypedelectronically using cad tools. as a result, designers, engineers, and possiblycustomers see only models of parts or assemblies of parts on a computer screen.although this approach provides for shared databases among design,manufacturing, and sales components and adds significant flexibility to thedesign process, it takes away the ability to walk around, explore, andmanipulate parts. as a result, boeing is working toward the development of ave system that would give these capabilities to designers, engineers, customers,and marketing personnel.according to mizell (1993), the plans for boeing's ve project include: (1)giving designers the ability to reach in and move parts assemblies around, (2)conducting human factors tests in virtual space, using human models, todetermine whether maintenance can be accomplished and control operationscan be easily performed, and (3) providing customers with a variety ofcustomized aircraft interiors to walk through and make modifications in realtime. all of these applications feed directly back into the design process. cabinlayout modifications made by customers influence the placement of wiring,ventilation, windows, seats, etc. mizell believes that boeing would consider thework in virtual reality a success if its only use was to provide customers withthe ability to walk through and experience various configurations of aircraftinteriors.implementation of the project is in its initial stages. but boeing alreadyhas software that reads cad data into a ve preprocessed database. a limitingfactor at this point is the computing and graphics power needed to represent thecad database in a threedimensional virtual space so that realtime interactionand a feeling of presence can be facilitated. it is anticipated that ve technologywill begin to contribute to boeing's productivity in the next two years, butdevelopment will probably need to be continued over a 15year time period (seethe more detailed discussion of implementation issues in the section ontechnology requirements).a second major project area at boeing is the application of augmentedreality to various parts of the manufacturing process. this project seeks toeliminate the need for complex assembly instructions or manually manipulatedtemplates by creating a system in which computerproduced diagrams aresuperimposed onto work pieces. the technology to accomplish this goalinvolves a headmounted seethrough display and a headpositionsensing andrealworld registration system. the augmentedreality project is beingdeveloped to assist factory workers in performing many complex, manual, skillbased tasks that rely heavily on human perception and decision making andtherefore are not easily automated. currently, guidelines are presented toworkers in the form of overlays, templates, or written instructions for each stepin the process. when partsspecific applications of se systems393virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.or processes are modified by designers in the cad system, a substantial amountof time may be required to reflect the appropriate changes in the manufacturingdocumentation. the augmentedreality system would link manufacturinginstructions with the cad system and superimpose these instructions in theform of diagrams on work pieces. the diagrams would appear to be painted on.for each step, a new diagram is projected. the link with the cad system wouldmake it possible to show changes in design or procedures to the workerimmediately. a more detailed description of this project is provided by caudelland mizell (1992).technology requirements in boeing's augmentedreality project, aprototype system has been developed and tested. the primary technology needsare for a comfortable headmounted color display with a field of view widerthan 30 degrees. another goal is a positiontracking system that will leave theworker untethered.in order to implement boeing's vision for using ve, several areas requiretechnology development. one critical problem is the lack of graphics andcomputing power. the cad database for the 777 aircraft contains between 5and 10 billion polygons. even though only a fraction of the database may beneeded at any one time, the existing graphics hardware limits the ability tocreate a scene that is interactive in real time, particularly because of thecomplexity of the geometry in the cad database. the problems created by thesize of the database, the inadequate hardware, and the requirement for a ve thatlooks real and behaves in predictable ways underscore the need for research onrealtime scheduling, assigning reduced workload areas, and developingheuristics to accomplish graceful degradation.another goal requiring technology development is providing engineerswith the ability to interact with objects in virtual space. currently, boeing isworking with a mannequin developed by norman badler at the university ofpennsylvania (badler et al., 1993) that can be put inside the cad geometry andchanged in size or shape. similar technology has been used by the automobileindustry for several years. the next major step is to develop the capability foran engineer to inhabit the mannequin in a virtual space, to move around insidethe cad geometry, perform maintenance checks, and in general, feel presentinside the scene while others monitor from a third person. particularly importantdevelopment areas include the need for collision detection and the requirementto give the individual in the virtual space some sense of force feedback,especially when testing the difficulty of performing various maintenanceoperations. developments in this area, particularly those involving hapticfeedback, are at least 10 to 15 years in the future.creating architectural walkthroughs of customized aircraft interiorsspecific applications of se systems394virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.is another important area for development. these models would providecustomers with the opportunity to see and experience the aircraft they arepurchasing before it is actually built.medicine and health carerationalethe knowledge base of medicine has exploded in the past 30 years, and itcontinues to expand at a staggering rate. as a result, medical practitioners havedifficulty in keeping pace with changes in practice, and medical students andresidents have difficulty in assimilating the information presented in theirmedical educations.as in other informationintensive disciplines, computer andcommunications technologies have important roles to play in reducing thecognitive demands on medical practitioners and students by helping to manage,filter, and process multiple sources of information. the following subset ofmedical knowledge and skill is wellsuited to management and handling by ve,augmented reality, and teleoperator systems: anatomical relations of various organs and systems. knowing that aparticular organ is located underneath another organ is an essential part ofanatomical knowledge. the ability to "walk through" the body and to seeanatomy in its natural state with all of the interrelations of various organsand systems would greatly facilitate the acquisition of certain importantpieces of anatomical knowledge. development of manipulative skills involving precise motor control andhandeye coordination. surgical trainers can be particularly useful inacquiring needed skills. image interpretation. although various imaging devices are common inmedicine, their effective use depends on the skill of the viewer to identifyoften small differences between normal and abnormal images. telemedicine through teleoperation. medical expertise is oftenunavailable in remote areas. telemedicinešwhether through consultationor through remote manipulators that enable teleoperationšoffers somepotential to place medical expertise in locations that might not otherwisehave access to such expertise. teleoperation also enables one toeffectively transform the sensorimotor system of the physician(diagnostician or surgeon) to better match the task.specific medical applicationsthe following discussion focuses on six applications of ve technology tomedicine: medical education, accreditation, surgical planning,specific applications of se systems395virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.telepresence, telesurgery, and rehabilitation. each subsection addresses a longrange vision of how ve might assist in these applications and a description ofpossible nearterm demonstrations.preservice and continuing medical educationmedical education has changed little in the last 30 years, despite enormousadvances in knowledge. most medical schools emphasize learning facts by rote.information is provided in a lecture format, and students study outlines forendless hours in the library. little effort is expended to place the informationinto a context or framework that might help to structure and organize seeminglydisparate facts. as a result, students must use their own, perhaps incompleteexperience to begin assimilating the data and creating a logical, integratedframework of anatomy, physiology, biochemistry, genetics, and the myriadsprings of subspecialized knowledge from contemporary medical research.teaching anatomythe teaching of anatomy is illustrative, and the application of ve andaugmented reality to such teaching has great potential. the static, transparent,twodimensional overlays typical of anatomy textbooks could someday bereplaced by a virtual human. indeed, today the national institutes of health isfunding the visible human, a project to develop a complete static digitalrepresentation of an adult human. once the data are collected, a student wouldbe able to operate a ve system for anatomy that would illustrate the spatialinterrelationships of all body organs relative to each other, selectively enablingor suppressing the display of selected body subsystems (e.g., displaying onlythe digestive system, viewing the complete image without the circulatorysystem).a much more sophisticated version of the visible human would be adynamic model that could illustrate how various organs and systems moveduring normal or diseased states, or how they respond to various externallyapplied forces (e.g., the touch of a scalpel). thus, a student could view the heartin normal and diseased states pumping blood, or observe how the stomach wallmoved while cutting it.today, several virtual worlds have been developed to demonstrate basicanatomy and as rudimentary models of training simulators. one is a model ofthe optic nerve created by vpl (vpl, inc., 1991). this model illustrates, inthree dimensions, the path of the optic nerve from the retina to the optic cortex.by pointing a finger one can fly along this path, looking to either side atadjacent structures. in this way, less effort isspecific applications of se systems396virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.expended in constructing a threedimensional image in the individual's mindand more effort is channeled into learning the anatomical relationships.a second model is a rudimentary simulator for the abdomen created bysatava (1993b). with this simulator, one can travel from the esophagusthroughout the intestine, taking side trips through the biliary system and thepancreas. it is a unique instructional tool that describes anatomy from the insideof the intestines rather than from the outside. it is of considerable benefit intraining individuals to perform colonoscopy and esophagogastroduodenoscopy,as well as teaching students the true anatomic relationships of intraabdominalstructures. basically, one is able to fly around various organs and experiencetheir actual relationshipsšthe model provides the learner with the ability tointerdigitate between organs and behind them without destroying theirrelationships to one another in the process.another educational tool is an augmentedreality system that allows theuser to see virtual information superimposed over real structures. seethroughdisplays provide the user with a view of the surrounding environment, alongwith an image displayed on goggles. investigators in boston and at theuniversity of north carolina (unc) have created seethrough displays usingcomputerassisted tomography (ct) scan, magnetic resonance imaging (mri),or ultrasound technology as imaging techniques (bajura et al., 1992).the work on augmented reality at unc (bajura et al., 1992) is based onthe images from an ultrasound that delineates abdominal structures in threedimensions. specifically, the investigators created a graphic of a threedimensional model and projected it through the headmounted display (hmd),as an overlay onto the user's view of the abdomen. this program, used on apregnant woman, allows the operator to "open a window" into the abdomen andview the fetus in a threedimensional manner without incising the skin.although the application of such programs to view a developing fetus islimited, the technology raises the possibility of visualizing other intraabdominalstructures.seethrough models can be used to teach surgeons where an organ islocated and show its relation to surrounding tissues. novice surgeons often havedifficulty visualizing the location of the gallbladder and the cystic duct inrelation to the common bile duct. despite extensive anatomy instruction, thefirst few operations are difficult because structures in a living body appeardifferent from the illustrations in an anatomical atlas. a seethrough displaygives surgeons in training an opportunity to develop their own internal threedimensional map of living organs, rather than having to operate without one.specific applications of se systems397virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.surgical traininga variety of surgical training applications are plausible as well. surgeonsknow there is no substitute for handson practice and training. considerlaparoscopic procedures, which involve surgery performed through very smallincisions in the body. the advantage of such procedures is that patient recoverytime is greatly reduced over conventional surgery, because of the smallertrauma to the body. however, manipulation of tools through the incision, handeye coordination, and understanding the spatial relationships of the toolsrelative to the organs within the body place high cognitive demands on thesurgeon.today, a surgeon wishing to learn laparoscopic procedures may attend aone or twoday course designed to teach necessary skills. in the course, thesurgeonsintraining begin with laparoscopic cholecystectomy trainers tofamiliarize themselves with the technique. these trainers are quite rudimentary,consisting of a black box in which endoscopic instruments are passed throughrubber gaskets. trainees use these instruments to practice such tasks as tyingknots, grasping structures, and encircling plastic arteries. the currenttechnology of velourandplastic organ models are stiffer and harder tomanipulate than normal tissue; arteries are not easily transected or avulsed anddo not bleed; and damaged organs do not ooze. as a result, the experience is farfrom realistic. following work with the trainer, the surgeonsintraining beginpracticing these techniques on pigsšthe approximation to humans is better, butthe anatomy is dissimilar, and the amount of experience is limited by both thecost and the availability of the pigs.an appropriately constructed ve simulator could obviate some of theseproblems. for example, the abdominal simulator developed by satava (1993b)includes several laparoscopic tools, making it possible for the surgeon topractice some (primitive) endoscopic surgical techniques. the currentdrawbacks of this system are the lowresolution graphics, the lack of realisticdeformation of organs with manipulation, and the lack of tactile input and forcefeedback. however, the model sets the framework for further investigation intosurgical simulators. such a simulator could be used for initial training, as wellas for the additional followup training, which has been shown to significantlyreduce the incidence of postsurgical complications in patients operated on bysurgeons with such training (william et al., 1993).a second application to surgical training is the use of the seethroughaugmentedreality model to support novice surgeons in performing their firstfew appendectomies, cholecystectomies, or arthroscopies. after gainingspecific applications of se systems398virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.confidence in their knowledge of anatomy, these surgeons could proceedwithout the aid of the display (haber, 1986).1close on the horizon are hybrid programs that will combine technologyfrom current surgical simulators with ve technology (stix, 1992). the pressureand tactile feedback provided to the surgeon could be improved by using theactual tools of endoscopic procedures, instrumented to act as interface devices,to train with a simulator. if the ve program did not have to generate theinstruments, computer power could be reserved for producing more intricatedisplays.ve simulators might also serve as mentors to teach residents, interns, andmedical students the basics of surgical practice, such as suturing, ligating bloodvessels, and the basic tenets of dissecting. operating procedures might be storedin a ve library, ready to demonstrate a billroth i, a choledochojejunostomy, thereconstruction of an uncommon craniofacial anomaly, pathologic rarities, anduncommonly encountered trauma scenarios.building on the groundwork of what has been created to date, robertmann's vision of "the ultimate simulator" will be achieved: "a computerenvironment in which a surgeon will not only see but 'touch' and 'feel' as ifactually performing surgery" (mann, 1985).accreditationthe purpose of medical accreditation is to ensure that physicians have aminimum level of skill and knowledge adequate to serve the public. theknowledge and skills acquired by physiciansintraining depend very much onthe idiosyncrasies of the patient population and the epidemiology of diseases towhich they are exposed. thus, whereas residents of different programs haveextremely disparate bases of knowledge and different levels of skill, evenresidents in the same program may acquire very different funds of knowledge.for recently developed surgical techniques, training programs may differ evenmore strongly, lacking standardized training methods and standardizedaccreditation procedures (see bailey et al., 1991).2 how are physicians atvarious points on a spectrum of experience, without any unified teaching, andwithout exposure to a uniform patient population, to be judged against anational standard?1 this training protocol is analogous to the headup displays often used by fighterpilots during their initial flights. the pilot's headup display gives cues about the heightof objects or ground contours while the pilot develops his or her own internalapproximations. once the pilot has developed a scale of altitude approximation, the cuescan be deleted from future flights.specific applications of se systems399virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.today the answer is board certification. knowledge can be tested by means ofpaperandpencil examinations, but proficiency in operative skills cannot bedemonstrated in this way. as a result, individual residency programs are left tojudge a physician competent in the operating room.ve simulators offer some hope for standardizing the surgical accreditationprocess. for example, a more sophisticated version of the abdominal simulatordeveloped by satava (1993b) could offer a set of standardized tests forlaparoscopic procedures. the ability to track the instruments would enable thenational board of medical examiners (or a similar governing body) to monitorperformance during a given procedure and to document the types and frequencyof errors made, thus providing some uniform means of assessing surgical skillsacross programs.surgical planningan actual medical operation is never performed in the abstractšit isperformed on a specific individual whose precise physical dimensions areunique and whose anatomy almost certainly deviates from those found inanatomy textbooks. thus, to a certain extent, a surgeon confronts surprisesevery time he or she undertakes an operation.vebased surgical planning aids offer a way to reduce the uncertainty. inprinciple, imaging data for the patient could be used to update a generic digitalhuman model, allowing the surgeon to understand more fully the specifics ofthe individual. the surgeon could then explore freely various approaches tosolving a surgical problem on the ve simulator and could practice the operationif required.such an application is a promise for the long term. however, jolesz andhis colleagues (gleason, 1993) have developed an augmentedreality system forvideo registration of brain tumors to aid in the planning and performance ofsurgical resection. in this system, the mass is imaged using either ct or mri, athreedimensional construct is created, and the image is projected over thepatient's head to plan the optimal site of skin incision and bone flap to exposethe tumor. this program is then taken into the operating room to provide areference map for the surgeons during the resection. thus, the surgeon is able toconsult the image at any time to assess the remaining tissue and the extent offurther excision.a model of the lower extremity envisioned by mann (1985) and designedby delp (delp et al., 1990) is an example of using virtual reality to test variousprocedures. this model allows the surgeon to "perform" the planned surgery,and then simulate a number of years of walking or other normal activity. thealtered model can be reanalyzed at the end of the simulated activity period andthe outcome of the procedure evaluated.2 in some cases, surgeons have published proposed guidelines for such accreditation.specific applications of se systems400virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.this allows surgeons to refine decisionmaking skills before ever operating on areal patient.the augmentedreality system designed by bajura et. al (1992), describedabove, could also be used for preplanning complex abdominal procedures, suchas complex tumor resections. liver masses, pancreatic pseudocysts, andedematous gallbladders could be viewed in three dimensions, making it easierto estimate size and interrelations with other abdominal organs and to plan andperform invasive procedures.telemedicinetelemedicine is the technology that would allow physicians to interactdirectly from locations thousands of miles apart. they would be in the samevirtual "room" when discussing a case or when performing a procedure andcould refer, simultaneously, to paramedical data, reducing the likelihood that amiscommunication would occur, that information would be missed, or thatresults would be misdirected or lost.the medical college of georgia in augusta has developed and introducedthe first stages of a statewide telemedicine system. the system uses interactivevoice, video telecommunication, and biomedical telemetry to link rural healthcare facilities and primary care physicians with large medical centers. as aresult, primary care physicians and their patients can consult with specialistswithout leaving their communities. in these consultations, the participantswould see each other and share common diagnostic data and images. the ideaalso extends to providing remote assistance to surgeons in rural hospitals duringsurgery. according to sanders and tedesco (1993), the system will eventuallyhave five hubs, each serving several clinics and rural facilities. currently, a testis being conducted with one hubšthe medical college of georgiašservingfive sites. the ultimate goal is to deliver the quality health care available atmajor medical centers to all underserved areas in the state. early results suggestthat the proposed system, when fully developed, will not only make highquality care more accessible but will also reduce costs. telemedicine networksare also under development in iowa, west virginia, and colorado.telesurgerydevelopments are also under way in telesurgery. one example is the greentelepresence system, created at sri international (palo alto, calif.) by phillipgreen. the system based on technology used by the national aeronautics andspace administration (nasa) for remote manipulation consists of a separateoperative worksite and surgical workstation (rosenspecific applications of se systems401virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.et al., 1994). the operative site has a remote manipulator with surgicalinstruments, a stereoscopic camera, and a stereophonic microphone to transmitthe environment to the surgical workstation. the workstation has a threedimensional display on polaroid glasses and an interface for the surgical tools.it also has the capability of accepting digital information from ct, mri, andvital signs monitors.hunter and his colleagues at mcgill (1993) have developed a prototypeteleoperated microsurgical robot and an associated virtual environment that areintended to allow a surgeon to perform remote microsurgery on the eye. thehelmet worn by the surgeon controls a remote camera in the operating room.images from the camera are relayed back to the helmet for the surgeon to view.tools shaped like a microsurgical scalpel and attached to a forcereflectinginterface are provided to the surgeon. as the surgeon moves the tools, he or shecauses the microsurgical tool held by the microrobot to move proportionally.the forces exerted by the microrobot are reflected back to the surgeon. theseforces are amplified, thus providing the surgeon with an experience of cuttingthat would, under normal conditions, be imperceptible. the fact that the masterand slave computers communicate by optical fiber connection will make itpossible for the surgeon and the microsurgical robot to be located at differentsites once the system is implemented.vpl has developed a system, rb2, in which two operators can interact invirtual space (vpl inc., 1989). using this program, two surgeons could operateon a virtual patient, one as the initiate, the other as a mentor. ultimately, asurgeon in new york could help a surgeon in london to do an operationwithout setting foot on a plane. a data highway rather than a commercial airlinewould make possible the transmission of complex knowledge (satava, 1993a).there is an enormous amount of interest in the promise of telesurgery;however, all current work is at the research and development stage.rehabilitationthere are several ongoing programs in which the use of ve technology isbeing tested as a means to assist in the rehabilitation of physically and mentallychallenged persons. some of this work is focused on building better humancomputer interfaces that take into account the specific physical limitations ofthe disabled individual. these studies are examining alternative methods, suchas eye movements or flexion of facial muscles, for sending bioelectric signals toa computer, which will in turn enable the individual to perform a desired task orsee a desired display. at loma linda university medical center, researchersare developing such interfaces using what they call a ''biocybernetic controller"(warnerspecific applications of se systems402virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.et al., 1994). their next series of studies will involve the use of bioelectricsignals based on muscle activity to provide disabled children with the capabilityto play nintendotype computer games now being played by the generalpopulation. other centers working on the same problem, such as the children'shospital in boston, have proposed studies to examine the motor output capacityof disabled individuals and to match that capacity with control interfacesmaking use of multimodal outputs.loma linda researchers are also engaged in having disabled persons workwith virtual objects generated by a computer as a means to begin rehabilitatingtheir motor skills. for example, an individual could practice manipulating avirtual object at a weight they could handle. it is believed that this practice isuseful even if the virtual object weighs less than the real one. in related work,greenleaf medical systems (greenleaf, 1994) is working toward using ves toenable individuals to perform tasks that they could not perform in the realworld. an example provided by researchers at greenleaf is creating a ve inwhich a cerebral palsy sufferer could operate a switch board.weghorst et al. (1994) are experimenting with using virtual objects to treatwalking disorders associated with parkinson's disease. according to the authors,objects placed at the feet of these patients may serve to stimulate a walkingresponse. since using real objects is not a particularly practical approach,virtual objects are being tried with some success. specifically (p. 243):nearnormal walking can be elicited, even in severely akinetic patients, bypresenting collimated virtual images of objects and abstract visual cues movingvertically through the visual field at speeds that emulate normal walking. thecombination of image collimation and animation speed reinforces the illusionof spacestabilized visual cues at the patient's feet.in another example, greenleaf medical systems is in the process ofadapting the vpl dataglove and datasuit for use in measuring the functionalmotion of a disabled person and recording progress over time. moreover,researchers are working on developing a gesture control system designed toenable individuals wearing the dataglove to perform complex control activitieswith simple gestures and on creating a system that will recognize personalizedgestures as speech signals. in the latter example, the dataglove would receivethe hand gestures and translate them into signals sent to a speech synthesizer,which then "speaks" for the individual wearing the dataglove. all of theseproducts are in the early development stage.in yet another example, ve technology is being used by architects todesign living and working spaces for the disabled. the technology nowspecific applications of se systems403virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.enables an individual in a wheelchair wearing a headmounted display and adataglove to travel through a space testing for access, maneuverability, counterheights, and reach distances for doors and cabinets.finally, the use of ve is being explored for behavioral therapy. hodges etal. (1993) report on a project at the graphics, visualization, and usabilitycenter at the georgia institute of technology that makes use of ve technologyto provide acrophobic patients with fearproducing experiences of heights in asafe situation. advantages of this approach are that ve provides the therapistwith control over the height stimulus parameters and with the ability to isolatethose parameters that generate a phobic response in the individual. in anotherproject, kijima et al. (1994) have been exploring the use of ve technology tocreate a virtual sand box to be used for virtual sand play. sand play, a techniqueused in diagnosing an individual's mental state and providing psychotherapy,involves patients creating landscapes and populating them with bridges,buildings, people, animals, and vegetation. an important advantage of using vefor sand play is that the patient's actions are recorded and can be viewed severaltimes by several trained observers.issues to be addressedalthough the longterm potential of vr for medical applications issuggested by an extrapolation from current demonstrations, a number ofresearch problems must be addressed to fulfill this potential. simulations of higher fidelity. the simulated organs and body structuresseen by the simulator user are far from realistic, with graphics that areprimitive and cartoonlike. tactile and force feedback, an importantconsideration in simulating the actual "feel" of a surgical procedure, ismostly absent. changes in visual perspective are not seen by the user inreal time. realistic models of organs and other body structures. in currentsimulations, organs and body structures do not morph as real tissue does;for example, they do not deform with gravity or change shape withmanipulation. blood vessels should bleed; bile ducts should ooze; heartsshould pump. better image registration techniques for augmented reality. in manycases, a vr surgical or diagnostic aid will require the superposition ofimages acquired from a number of different modalities (e.g., a cat scancoupled with an ultrasound image). seethrough displays must superposeartificially generated images on the real image through the user's eyes.techniques for aligning these images so that the right parts correspond toeach other remains a considerable intellectual problem.specific applications of se systems404virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved. appropriate data representation schemes. how can patientspecific databest be combined with generic models of humans in a computationallyefficient manner? reduction in widearea network delays. delays must be significantlyreduced to provide for coordinated long distance work.there are several social issues that will also need to be addressed if vetechnology is to achieve wide acceptance in medicine: acceptability to care providers. physicians generally practice from aperspective of conservatism, refraining from the use of techniques thatmay be unproven, such as the opportunities for surgical training andperformance offered by ve technology. accepted practice, to be encodedin the future as practice guidelines, is widely regarded as a way to ensurethat the wellbeing of patients is not placed at undue risk. public opinion. for understandable reasons, the public may feeluncomfortable with the perception that a robot is undertaking surgicaloperations. negative public reactions to the recently tested robot used inhip replacement surgery are a case in point. a cultural shift thatacknowledges that automation can help care providers do their jobs moreeffectively will be necessary. moreover, the necessary shift isbidirectional: patients will have to change the way they regard their careproviders, and care providers will have to change the way they presentthemselves to patients.to gain physician and public acceptance, convincing demonstrations willbe necessary. these systems will have to demonstrate that their use will resultin better outcomes, fewer complications, and ultimately even less invasive, lessdebilitating procedures.in the current health care environment, a third social consideration must beaddressed: cost. explicit costbenefit analyses for various technologies arelikely to become increasingly common, and ve will be no exception.teleoperation for hazardous operationsrationaleany activity that involves performing tasks in environments that are unsafefor humans, or in which safety is too costly, is a potential application of ateleoperated system. chapter 9 provides a review of the technology underlyingteleoperation; in this section we discuss ways in which this technology has beenapplied in hazardous environments.specific applications of se systems405virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.survey of major applicationsthere are many examples of teleoperation applied to hazardousenvironments. we provide a short survey of some major applications. in thefirst two, toxic environments and space operations, detailed scenarios arepresented to illustrate critical tasks. these scenarios contain examples that canbe considered prototypical of all hazardous environments.toxic environmentsto avoid human exposure to radioactivity or toxic chemicals, teleoperatorshave been used in handling radioactive and chemically toxic materials,maintaining nuclear power plants, and disposing of hazardous wastes. handlingradioactive materials was the first application for which teleoperators weredeveloped in the 1940s by raymond goertz at argonne national laboratorynear chicago. glove boxes are used when more dexterity is required than isafforded by current teleoperators, but advances in telerobotic dexterity shouldeventually replace them.maintenance and cleanup of nuclear power plants typically requires mobiletelerobots as well as large manipulators. a variety of wheeled or tracked mobilerobots have been designed that carry sensors and manipulators through powerplants (fogle, 1992; trivedi and chen, 1993). large manipulators are requiredto reach inside reactor vessels for maintenance (munakata et al., 1993; rolfe,1992). more generally, telerobots are required to identify and handle toxicmaterials at accident sites (stone and edmonds, 1992).the disposal of hazardous wastes is a major public concern. for example,the u.s. department of energy has an extensive program to retrieve low andmediumlevel nuclear and chemically toxic wastes of nuclear weaponfabrication from desert disposal sites and to place these wastes into improvedcontainers and sites (harrigan, 1993). at these desert disposal sites, most of thewaste is stored in large concrete tanks or buried in 100 gallon drums whoselocation is known only approximately. removing these wastes (and any soilthat may have been previously contaminated through leaks) without causingfurther environmental damage and without endangering human life is achallenging task.another challenging task is disposing of the highlevel nuclear waste fromcommercial nuclear reactors and government facilities; highlevel waste iscurrently stored under water in storage pools. as these facilities fill up, currentplans call for placing the wastes in deep underground storage facilities. movingthe material from storage pools to a processing facility where they can be moreeasily handled (e.g., a glass vitrification plant) to the final storage facility willrequire the assistance of remote handling operations.specific applications of se systems406virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.in order to discuss the application of hazardous waste disposal in moredetail, we take as a scenario the removal of hazardous waste from anunderground storage tank, such as at hanford, washington (harrigan, 1993),which has been demonstrated in part in laboratory settings. sensors andmanipulators are needed that are immune to the chemical, radiation, andthermal stresses that would destroy ordinary video camera lenses and sensingelements, manipulator lubricants and mechanisms. moreover, the hazardousnature of the materials being handled places a premium on high reliability.manipulators for gaining access to storage vats must be large yet have theability to work in confined spaces. such systems will be extremely expensive.since there are many waste sites to handle, advances in supervisory control arerequired to speed up operations.the first problem is that the contents of each tank is not known. hencevision and proximity sensors have to be inserted by a manipulator to map thetank's contents. in this mapping phase, the manipulator is manually controlled;proximity sensors are used to approach surfaces without collisions using localcontrol. in this operation, it is critical that the tank sides not be contacted toavoid rupture. from these data, a representation of the tank's contents isconstructed with operator assistance. for recognizable objects such as pipes, anobject model can be specified. for waste surfaces and nonrecognizable objects,a threedimensional surface representation is employed.the model of the tank's contents is then used to generate a graphical imageand to perform simulations of operations. tasks and robot motions are plannedthrough realtime graphical simulations: sequences are developed, operationsare dynamically simulated, and collisions and joint limits are checked. just priorto a real operation, a simulation can be run to verify the expected result.for the actual task, the operator relies heavily on the graphical display andsimulation to control the telerobot because of poor visibility in the tank. sensingduring the operation is employed to update and modify the graphical model. asa motion develops, the simulation is run simultaneously to check for emergingproblems such as collisions or other unsafe operations. approaches to surfacesand objects will be conducted in local mode, using proximity and contactsensors in the end effector. wherever possible, autonomous control will beemployed to speed up such operations as grasping objects, cutting pieces of thestructure loose, and conveying waste from the tank.space operationsachievement of remote manipulation from the earth to the moon datesfrom the 1967 united states surveyor and the russian lunakhod,specific applications of se systems407virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.which followed shortly thereafter (both were unmanned lunar roving vehicles).canadianbuilt remote manipulators have served the space shuttle project onmany separate missions for loading and unloading the cargo bay in space and toprovide a stable base for astronauts performing extravehicular tasks. the deepspace probes have been the most impressive teleoperators, e.g., the viking wascontrolled from earth even when at the very edge of the solar system, severallighthours away.the planned space station construction is motivating a great deal of spacetelerobotics work, particularly in the united states, germany, japan, andcanada. these considerable efforts have resulted in some of the most advancedtelerobotic systems, which are developing the kinds of generic capabilities thatwill be required in applications other than space. uses for space telerobotsinclude construction and maintenance of the space station, satellite servicingand repair, and space laboratory work.for the present scenario we concentrate on space laboratory robotics,motivated in part by the success of the german rotex effort (brunner et al.,1993; hirzinger et al., 1993). there are two main reasons for conductinglaboratory and manufacturing operations in space with robots. first, humans arenot aseptic enough for cleanroom applications such as crystal growing. second,control of the robot from the ground allows full utility of the space laboratory,because operations do not require the active participation of astronauts.a main challenge therefore is teleoperation under variable time delays onthe order of several seconds. predictive displays and local autonomous controlare the approach that has been taken (sheridan, 1993b). space stationenvironments, unlike those of hazardous waste disposal, are likely to be wellknown in advance, so a detailed simulation of the workspace can be fashioned.the main difficulty is to model the dynamics of the manipulator and objects inthe contact tasks; the goal is to accurately simulate the contact forces for thepredictive display.the predictive display is used in the first instance for operator training. anoperator interfaces to the realtime simulation via a stereo graphical display andforcereflecting hand controllers. display enhancements are added to aid theoperator (see discussion in chapter 9).in addition to training, the predictive display is employed for task planningand preview (kim and bejczy, 1993; kim et al., 1993). prior to sending amotion command to the remote robot, the operator can rehearse a task to ensureits outcome. the resultant motion can be replayed or relayed to the slave robot;otherwise the operator may directly control the robot after feeling confidentabout the outcome.during an actual space laboratory task, the operator may view not only thestereo graphical display but also a timedelayed real image superimposed with aphantom display. the phantom display is an outlinespecific applications of se systems408virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.of the graphical image of the robot superimposed on the real image. thisphantom display responds immediately to an operator's commands, indicatingwhere the robot will go from its current position.enhancements on the real image can be performed to improve visibility,such as removing bright specular reflections and increasing the contrast indimly lit areas. remote cameras automatically track the end effector to keep themanipulator in view or to focus on the task. voice commands are employed toswitch a camera onto a monitor or to zoom in on a known location. a separategraphical display provides a more global view for operator orientation.commands to the slave robot include not only position specifications, butalso nominal sensory patterns such as for force. differences between the realand simulated worlds are accommodated by local sensing and intelligence ofthe robot; the degree of autonomy is controllable and restricted, because anyadjustments are small. the local sensing in the robot's end effector includes sixaxis wrist force sensing, optical proximity sensing (both long and short range),and tactile sensing (hirzinger et al., 1993). this local sensing can also beemployed in dynamic tasks, such as catching a freefloating object, which mightnot be possible with long time delays.many elements of the scenario described above were successfullycompleted by the german space agency's rotex telerobot (brunner et al.,1993; hirzinger et al., 1993), during a flight of the space shuttle columbia inapril and may 1993.teleoperated heavy machinerytractors, diggers, cranes, and dump trucks have been controlled remotelyfor construction, demolition, earth moving, forestry, farming, and mining.excavators and cranes are now being fitted with hand controls and sensors sothat the endpoint may be directly controlled instead of the arm joints(wallensteiner et al., 1988). force feedback is being supplied to the operator toaccommodate the hardness of the ground. teleoperated bulldozers and diggershave been employed to remove contaminated soil (fogle, 1992; potemkin et al.,1992). grapple yarders and log loaders have been teleoperated in the forestryindustry (sauder et al., 1992). in mining, teleoperation is being applied forhaulage and drilling (kwitowski et al., 1992); lowbandwidth communicationsthrough mine shafts offer major difficulties.offtheground environmentsmanipulators mounted on teleoperated cherry pickers have been applied inremote power line maintenance, tree trimming (goldenberg et al.,specific applications of se systems409virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.1992), and firefighting. in the last five years, several companies in japan andthe united states have developed teleoperator systems capable of changinginsulators and performing other repairs on active hightension power lines.current methods require the use of rubber gloves (on low voltage lines) orseveralfoot insulated "hot sticks" for high voltage lines. the human lineworkers are boosted to within close proximity to the active wires in buckets onthe ends of long hydraulic arms. however, due to instability of these devices orhuman errors, workers have inadvertently been burned and electrocuted. newermanipulator devices allow workers to stay much farther away in the buckets orto operate from the ground using remote video.windows and outside surfaces of buildings must be cleaned, painted, andrepaired, but the cost of doing so is high and dangers significant. many modernbuildings have platforms that can be raised and lowered by motor drives andmoved laterally around the building. yet since the geometry of the buildings isknown in detail, it should be possible to perform all the necessary tasks byteleoperation. for inspection of structures, experimental vehicles have beendemonstrated that scale walls (hirose, 1987; nishi et al., 1986).a more unusual application of teleoperation is the inspection and cleaningof the insides of pipes. pipes are essential elements of most buildings and plants(especially in nuclear and chemical plants with high pressure pipes), yet theyare often difficult to inspect or clean. experimental prototypes have beendeveloped, but no largescale effort has yet occurred (fujiwara et al., 1993;fukuda et al., 1987; okada and kanade, 1987). eventually such devices couldalso be used in water, gas, and even sewer pipes, many of which are aging andleaking. some teleoperators for this application drag cables behind them,although developers have envisioned autonomous selfpowered devices thatmake surveillance journeys for significant distances before returning with theirfindings.firefighting and securityalthough teleoperated fire rescue and firefighting operations are notcurrently undertaken, it is clear that firefighters could use a remote vehiclecapable of entering a burning building. development of such a fire rescueteleoperator would make an excellent national demonstration project. there isno requirement that is not technologically feasible. a remote firefightingvehicle would be capable of crawling up one or more flights of stairs underremote human control; entering various rooms; and allowing the humanoperator to look around (with better vision than a human eye and most likelywith camera pan and tilt controlled by a headmounted display) to find and togive instructions to persons who arespecific applications of se systems410virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.found. it would include fresh air breathing masks, an insulated compartmentinto which a person could crawl and be brought out to safety, and foamdispensers and other fireextinguishing gear. furthermore, the vehicle wouldprobably be batteryoperated so as not to have to drag a power cord and riskgetting snagged, although conceivably it could unroll a cord behind it to avoidentanglement.several firms are already marketing simple wheeled sentry robots capableof guiding themselves along paths laid out with magnetic or optical markers,listening for unexpected sounds, or looking for unexpected people or objects.future police teleoperator capability should include radiocontrolled stairclimbing teleoperators equipped with lowlightlevel cameras capable of seeingin the dark, which are able to dispense tear gas or even to fire weapons.explosive ordnance disposal is an area that is very well matched toteleoperations, and prototype teleoperators have been developed for thisapplication. a teleoperator could approach a suspected bomb with the intent offirst inspecting it, then disarming it if possible, covering it with something tolimit damage from detonation, or carrying it to another location for detonation.finally, a number of teleoperated systems have been developed formilitary surveillance, sabotage, and warfare. some take the form of aircraft,such as cruise missiles, and remotely piloted vehicles and helicopters. some areland vehicles, capable of laying down kilometers of optical fiber for highbandwidth surveillance and control. still others are undersea vehicles, capableof performing surveillance, sabotage, and weapons delivery. some are manuallycontrolled; others are preprogrammed telerobots.bulk transportation environmentsloading and unloading operationsšwhether within plants or to or fromshipping docks, ships, trucks, trains or aircraftštend to be unique with respectto the relative positioning movement to be made (location of receiving vehiclerelative to initial location of container or product). some bulk materials such asliquids, coal, and grain can be handled by pouring or pumping, so that remotemanipulations are primarily for the purpose of locating the transfer pipe or ductrelative to the receiving container or vehicle. sometimes this manipulation isdynamically tricky, as in airtoair refueling and cargo transfer between twoships on rolling seas. by sensing the (continuously varying) relative position ofthe receiving vehicle with reference to the sending vehicle, the teleoperator canbe commanded to null out the difference in platform positions, while the humanoperator controls the cargo transfer as though the two platforms were fixedrelative to one another. when massive objects are being transferred,specific applications of se systems411virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.manual control must be executed slowly because of large inertias. computeraided control can make use of predictor displays to anticipate the effects ofcontrol actions before they are imposed, thus adding lead stabilization.ocean environmentstwo widely publicized deep oceanteleoperations were used in therecovery of the accidentally airdropped hydrogen bomb off the polomaresislands in 1967 and the discovery and exploration of the ship titanic. in theformer, the navy vehicle curv (cableoperated underwater remote vehicle),equipped with a pressuresealed camera, lights, and manipulator, was draggeduntil the parachute attached to the bomb could be found and grasped. in thesecond case, the woods hole oceanographic institution submersible vehicleargo was passively towed at the end of a severalmilelong cable, and itscameras, lights, and powerful sonar were used to discover the titanic(whitcomb and yoerger, 1993). the small rov (remotely operated vehicle)jason then explored inside the titanic at the end of its power and signalumbilical. deepocean teleoperators have also found a number of other sunkenships, historic artifacts, and buried treasure, as well as discovering the existenceof hydrothermal vents deep on the ocean floor.deepocean vehicles, sensors, and manipulators have been used to surveythe ocean bottom topographically, geologically, and biologically. it has beenestimated by some biologists that more than 90 percent (volumetrically) of theearth's ecosphere has yet to be exploredšnamely, the oceans below the surfacešand it is already clear that the oceans are full of creatures at all depths.teleoperators appear to be an ideal way to perform this exploration.a more commercial application of teleoperation in ocean environments hasbeen in the area of offshore oil exploration. many petroleum wellheadpreparations have been performed by teleoperators. still other operations thatmight have been done by teleoperation have been neglected, such as inspectionand repair of the legs of oil platforms (some of which have broken up in heavyseas because their welds cracked) and inspection of outflow pipe lines (some ofwhich have burst for similar reasons). many associated underwater robotictechnologies still need much development, such as cleaning, inspection ofwelds, and rewelding.infrastructure and research needsthe usefulness of teleoperation in a particular hazardous domain may notbe at issue, but its costs may well be. in many cases, it is simply morespecific applications of se systems412virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.economical to use tried and true practices (and accept but attempt to minimizetheir attendant human losses) than to invest in risky new technologies. a goodexample is that of firefighting, an area in which there is little nationalinfrastructure to encourage application of advanced technology research anddevelopment, and in which local communities buy apparatus from a myriad ofsuppliers of conventional trucks and equipment with funds derived from localtaxes and bond issues.the technology needs for teleoperations for hazardous operations can bedivided into a number of categories: manipulators must today be developed on an applicationbyapplicationbasis that is wellmatched to the task in question. such manipulators mustbe made sufficiently precise and sensitive, producing enough feedbackthat the human operator can control the manipulations appropriately. inaddition, the range of tasks that can be performed by manipulators shouldbe expanded so that unanticipated contingencies can be handled withgreater ease. the survivability of manipulators and sensors is obviously important inhostile environments. as an example, radioactive environments can leadto the breakdown of lubricants, the deterioration of electroniccomponents, and the darkening of glass lenses. the dirt and grime ofmining environments are hard on optics and other sensors and on actuatorjoints and the bearings of robotic arms. cost continues to be a major issue. as noted above, the costeffectivenessof teleoperated systems remains to be demonstrated, and until it is, thecost of building and testing teleoperated systems will remain high anddemand will remain low. using sensors to construct a model of a disposal tank's content ischallenging, particularly when dynamics as well as geometry are to besimulated. during actual operations, the incoming sensory information has to befused with the current model of the tank. higherlevel supervisory control and partial autonomy are required tospeed up the operations and ensure safety.trainingrequirements for training are ubiquitous in today's world. they range fromgeneral childhood education to highly specialized training of military specialforces in preparation for specific critical missions. our purpose here is toconsider the application of ves for training, that is, for the acquisition ofspecial skills for specific purposes.the natural precursor to ve, technologybased task simulation, camespecific applications of se systems413virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.into being in the late 1930s. the archetype for such training devices was thelink trainer, essentially a plywood box on a gimballed stand. the inside of thebox was furnished like the cockpit of an airplane, complete with a functioninginstrument panel. when occupied by a trainee, a large hood was dropped overthe canopy area so the trainee's view of the external world was cut off.activation of the instruments was controlled electronically by an instructor, andeach trainee action was recorded in the same manner. by operating thesimulator, the trainee could practice instrument navigation skills in ways thatcould not or would not be duplicated in the real world because of theirhazardous qualities. while having relatively modest fidelity compared withpresentday flight simulators, the link trainer incorporated nearly all of thetraining concepts that are featured in the most advanced training devices(williams and flexman, 1949). after world war ii, the ideas of dynamic tasksimulation were extended into other task environments: air defense (chapmanet al., 1959), air traffic control (fitts et al., 1958), submarine combat (mclaneand wolf, 1965), and the operation of surface vehicles such as tanks(denenberg, 1954).meanwhile, computer programs were devised that could represent contextsas varied as multifirm commercial markets (kibbee et al., 1961) and theprocesses of municipal governments (guetzkow, 1962). developers formulatedinteractive video representations of such complex situations as the emergencyroom in a large hospital in a way that the trainee would see the patient, see andhear the actions and questions of other members of the health care team, and,most dramatically, experience the consequences of his or her own decisions inthe form of medical outcomes.the advantages of relatively low cost, low hazard, speed, repeatability, andgood transferability of acquired skills were the driving force for progress insimulationbased training. these qualities are likely to be even more prominentin mature vebased training facilities. indeed, the qualities that ve might bringto the whole enterprise of dynamic task simulation are those explicitly called forin the most comprehensive historical review of the use of simulation for pilottraining (orlansky and string, 1977).the idea of using ve technologies for training is a very natural extensionof the use of simulation for training. given the committee's view of ve assimply an extension of the concepts of simulation to include closer couplingbetween the participant and the technology supporting the creation of theartificial world, it is only natural that training, a subject that has benefitedgreatly from advances in simulation, should be a prime candidate forexploration of ve technology.specific applications of se systems414virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.rationalevirtual environments have the potential to extend the scope ofcircumstances that can be satisfactorily simulated and to extend the advantagesof using simulation versus realworld training. when the visual world ispresented to each individual on a personally mounted visual display, when theauditory environment is recreated with perceptually compatible virtual soundsources, when sensorimotor interaction is accomplished through an effectorinterface that simulates the touch and feel of live interaction without requiring aphysical hardware mockup, then the definition and scope of what can berepresented is limited not by what can be presented in a physical mockup, butonly by the quality of the sensors and effectors, the speed and processing powerof the supporting computer, and the bandwidth of the transmission channels.the issue of what can be trained with simulation has therefore moved fromthe physical world to the potential worlds that can be created with software andgeneralpurpose hardware. at this point in time we are not only uncertain ofexactly what those boundaries are, but also confident that they are expandingwith each new technological development in the field. when ve technology ismature, a suite of hardware will be able to be converted from a nuclear physicstraining simulation to a mission rehearsal application or a surgical simulator,simply with the change of software application.vebased training also has the potential for being better than conventionaltraining. for example, realistic training for hazardous, risky, and dangerousemergency situations, in which the trainee feels that he or she is present in thesimulated environment, can be undertaken in ways not possible withconventional training. artificial cues, not realizable dynamically in the physicalworld, may be utilized to augment the training effectiveness of ve worlds.lastly, vebased training may be more costeffective. current experiencewith simulators is instructive. simulators are very expensive because theyrequire the fabrication or acquisition of detailed mockups of the participants'actual operating environments. nevertheless they are extremely costeffectivewhen compared with equivalent training in the corresponding real environment.(one examplešthe cost of a 747 flight approaches $10,000 per hour, whereastoday's 747 simulator can be run for less than a tenth of that cost.) if researchdemonstrates that vebased training is as good as other methods, then theeconomics of running simulators versus mockups and test equipment andvehicles may be such that the former is much cheaper than the latter.specific applications of se systems415virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.the futureit is presumptive to assume that serious applications of ve to training willawait the clear experimental demonstration of costeffectiveness. most trainingtechniques in place today did not have the benefit of such evaluation beforethey were adopted. there is little doubt that ve training will be advocated andperhaps even adopted on the basis of its face validity alone. however, thescientific community should not be satisfied simply with proof from themarketplace.the committee anticipates that training will be a powerful and useful earlyapplication of ve. its success in limited applications, such as passive simulationscenarios, involving threedimensional demonstrations that are difficult tocreate in a twodimensional world, will be the first areas of successfulapplication. then, as haptic interfaces emerge, at first with physicalmanipulators and later with generalpurpose earthreferenced devices or withexoskeletons, the full potential for costeffective applications to training willemerge. at this point we should be able to forecast applications for which vetechnology will be successful and those for which it will not. in the long range,we envision a full range of interactive portable training simulators both foroperational activities and for maintenance activities.specific applicationscommercial aircraft simulators substitute for real aircraft in training flightcrews with respect to standard and emergency procedures in their currentaircraft and in qualifying them to transfer to a new aircraft type. every nuclearpower plant in the united states is required to have a simulator on site forregular training of the controlroom crews. aspiring doctors practice surgery oncadavers, a medical simulation of a real patient.although there is a massive literature on the use of simulation for training,the extension to ve technology has only begun. for now let us address theexperimental literature on ve training. we are aware of one as yet unpublishedand two published experiments; however, we can expect to see many more inthe near future.wes regian had conducted a series of experiments on the use of vetraining for two applications: (1) learning to navigate an unfamiliar building and(2) learning to operate a simple control panel (regian and shebilske, 1992; w.e. regian, personal communication, 1993). he has found conclusively that vetraining is as good as training in the actual environment when the performancemeasures are success in finding one's way in the building unaided byencumbrances or artificial cues or operatingspecific applications of se systems416virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.a physical realization of the control panel. in further experiments in this series,as yet unpublished, regian is investigating what can be accomplished with asimilar training paradigm when a twodimensional display is used as a controlcondition.peter hancock and his students at the university of minnesota (kozak etal., 1993) conducted an experiment that purported to evaluate the usefulness ofvirtual environments for training simple pick and place movements in thelaboratory. the task he chose for training was the movement of five soda cansarranged in a row, one by one, to a position about 6 inches to the rear of theirprior position and then to return them to their original position. the subjects inone control condition practiced moving real cans while a second controlcondition involved no training. the experimental condition practiced using avpl eyephone system and data glove to move virtual cans. the transfercondition required moving the real cans as rapidly and accurately as possible.not surprisingly, early in the transfer trials they found that the virtualtraining condition was worse than training with the real cans when realmovement of the cans was the criterion task. the resolution and viewing angleof the virtual display were inferior to those involving the real cans. the dataglove representation of the hand projected in the virtual world provideddegraded positioning accuracy compared with a real hand and real cans. andthe visualmotor coordination lagged behind the actual hand movements underthe conditions that used as the criterion the time for speeded movement. thetask required during virtual training was simply very different from the criteriontask.the conditions of this experiment were a poor choice for demonstratingsuccessful ve training. the task of picking up and moving soda cans is sohighly overlearned even before beginning the experiment, that all that is left tolearn is the very contextspecific features of the situation. nevertheless, theexperiment illustrates one of the biggest barriers to the application of ve totraining. the fidelity of the visualmotor representation was so poor that thetraining condition did not correspond closely enough to the conditions requiredfor transfer to even hope for positive transfer.lochlan magee and his colleagues of the canadian defense and civilinstitute of environmental medicine have designed a ve system for trainingnaval officers in the piloting tasks of maneuvering ships in formation on theopen sea. the officers currently practice on real ships at considerable cost ofoperation. magee conducted an experiment, as yet unpublished, that comparedlandbased ve training (using a headmounted display of the visual scene andpolhemus head sensing to stabilize the imagery) with training at sea. bothgroups of approximately 13 junior officers transferred to performance at sea. heallowed the instructors tospecific applications of se systems417virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.utilize the ve system in the same way that they used the real ships. they madeno attempt to augment the training with artificial cues or special conditions thatcould be accomplished only in the ve system. his data analysis is stillincomplete, but preliminary results show that the instructor ratings during thetransfer condition for the simplest maneuvers studied were significantly betterwith training using the ve system than with training at sea, and the two trainingconditions were not significantly different for the two more difficult classes ofmaneuvers. because of the very great cost differences between the two trainingmethods, showing equivalence is more than good enough to support use of theve system.training transferthe major concern in designing any training experience is how well theknowledge and skills acquired in the training environment transfer to theworking or operational environment. the two theories that have dominated thethinking about transfer since the turn of the century grew out of the behavioristtradition. the earliest of these was proposed by thorndike and woodworth in1903 (thorndike, 1903). this theory, known as the theory of identical elements,is based on the notion that the degree of transfer is a function of the identity ofstimulusresponse pairs between the original (training) task and the transfertask. that is, the task similarity is determined by the number of sharedelements. this model can be used to give qualitative predictions of positivetransfer; however, it does not address negative transfer.the second theoretical formulation that was offered by osgood in 1949proposed that the amount of transfer is a function of the degree of similaritybetween stimuli and responses in the original and the training tasks. in thistheory the predictions are qualitative and continuous for both positive andnegative transfer. for ve training, the qualitative predictions based on osgood'smodel are: (1) maximum positive transfer will be obtained with stimulus andresponse identity between original and transfer learning, (2) if there is completeresponse identity, negative transfer cannot be generated, and (3) even withmaximum identity of stimuli between original and transfer learning, negativetransfer can still be obtained if the responses are antagonistic or in opposition toone another.a drawback of these approaches is the difficulty of identifying anddefining similar elements and in determining the amount of contribution eachelement makes to transfer. clearly, both theories are most appropriate for thetraining of discrete, concrete, and simple tasks that involve the acquisition ofmotor skills.more recently, with the shift to cognitive approaches, interest hasspecific applications of se systems418virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.begun to focus on the process by which learning occurs and on the developmentof models to explain that process. many of these models include not only arepresentation of the knowledge to be acquired but also a set of rules for usingthe knowledge. for the past 10 years, anderson (1993) has been using such amodel to build intelligent tutors to teach content and procedural knowledge inalgebra, geometry, and computer programming.there is some indication from transfer of training research that singletheories of transfer will not hold for both cognitive and motor tasks (schmidtand young, 1987; ritchie and muckler, 1954). for complex simulations of thetype represented by ve technology, in which complex tasks may be learned andtransferred, hays and singer (1989:319) suggest starting with fidelity analysis:''the first step in the fidelity analysis should be to determine the major emphasis(either cognitive or psychomotor) of the task. if the task is cognitively oriented,it is likely that the training systems should emphasize functional fidelity. on theother hand, if the task has strong psychomotor requirements, physical fidelityshould be emphasized." functional fidelity refers to the accuracy ofrepresentation of the system's procedures and procedural sequences; physicalfidelity refers to the accuracy of representation of the physical design and layoutof the system on which the individual will receive training.ve may be particularly suited to increasing the probability of transferbecause of its flexibility, its feedback capabilities, and its potential formotivating the learner. the promise of flexibility may make it possible todesign individually tailored training experiences that take into accountindividual differences in the skill and knowledge level of trainees as they enterand proceed through training. for example, a particular training program couldbe made more compatible with the specific motor skills of each trainee; asproficiency was gained, the training scenarios could be modified accordingly.further, ve technology is better suited than previous technology to augmentfeedback to the trainee by adding special cues or by providing multimodalstimulation (e.g., haptic, visual, and auditory). this may be most useful as areinforcment strategy for training lowerskilled students. finally, vetechnology has the potential to furnish an intrinsically interesting andmotivating training environment through the presentation of special soundeffects and interesting visual patterns.currently there appears to be no way to predict qualitatively orquantitatively the kinds of transfer that will result from ve. as a result,evaluation will be needed during all stages of system design, development, andimplementation. the following section discusses the problems associated withquantitative demonstrations of transfer of training effectiveness in thelaboratory and the field.specific applications of se systems419virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.issues to be addressedthere are two limitations to demonstrating success in ve training. first arelimitations inherent in experimental demonstration of success in the training ofalmost anything. that is not to say that simulation has not been a successfultraining medium. it has, as measured in the number of simulators sold, thenumber of disciplines that have committed to simulation as an alternative totraining in the real environment, and the number of organizations that haveenthusiastically adopted it as the training medium of choice. commercial pilotsroutinely use simulators to maintain their currency in selected aircraft. nasaused virtual reality to train astronauts and other personnel in preparation for thehubbell telescope repair mission that involved extensive extravehicular activity.what has been difficult to provide are quantitative demonstrations of transfer oftraining effectiveness in either the laboratory or the field.let us examine some of the issues that must be considered whenconducting meaningful training evaluations. since in this section we areconcerned with the use of ve to support training rather than performance, wepresume that, after training is complete, the trainees will begin work on thesame or similar tasks in a different, nonve environment. the issue therefore istransfer of training from the ve conditions to the nonve conditions.for purposes of illustration, let us assume that we are training firefighters.we place a ve helmetmounted display of the visual scene of a fire on thetrainee, together with a treadmill that allows us to simulate walking into a fire.the heat of the fire is represented by adjustable infrared lamps. the trainee thenpractices the procedures associated with fighting fires. after a period oftraining, we observe the trainee's performance fighting real fires. we hope that,for most of the required skills, there is positive transfer, that is, that traineeperformance after practicing in the ve is better at fighting real fires than it wasbefore training. we would expect that for some skills there would be notransfer, perhaps because they were skills for which the ve system could notprovide practice, such as climbing ladders or manipulating hoses. however,there is always the risk of negative transfer, that is, that selected firefightingskills will be performed more poorly after practice in the ve. we would expectnegative transfer under conditions in which students learn the wrong responseassociations to the stimuli to which they are exposed. for example, suppose thetrainee approaches closer to the flames in the ve than he or she should becausethe ve fire leaves out the risk of getting burned. to the extent that the traineegets closer than she should to the flames in a real fire for the same heatstimulus, we would say that the training exhibited negative transfer. a badresponse was learned.specific applications of se systems420virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.it is not enough simply to show that the training improved performancewithout asking, "compared with what?" firefighters are trained today in theclassroom, using controlled fires in mockups or real buildings, or both. unlesswe can show that the ve training is costeffective compared with the currentmethods, we have not accomplished a useful result. this is usuallyaccomplished by training a control group using the standard method andcomparing the transfer of training to the real environment to that obtained withthe experimental method.it is likely in this example that there are some firefighting skills for whichit is costeffective to train in an ve, such as communication and coordinationamong personnel in firefighting teams, whereas others, such as the physicalskills of using hoses and climbing ladders, are best trained in mockups.it is very expensive and timeconsuming to conduct the kind of transfer oftraining experiment that is described here. efficient experiments require thateach traineesubject perform the task twice, once using the experimentalmethod and once using the control method. but there is a dilemma: it is notsensible to compare performance with the experimental method withperformance when the subject is trained again on the same task. humanvariability being what it is, large numbers of different subjects must be tested ineach condition in order to obtain statistically reliable results. furthermore,because we would expect some skills to show transfer and others not, it isimportant to develop detailed performance measurement at the level ofindividual skills so that those that produce positive transfer are detectable.because of the cost and difficulty of conducting such studies, governmentsponsored experiments of this kind are needed to evaluate scientifically thebenefits of ve training.a second limitation of the current state of virtual environment technologyis the problem of successfully representing the real world with satisfactoryfidelity to achieve a training goal. there is a tradeoff between the field of viewand the resolution of helmetmounted displays. there are limitations to thetechnology supporting the computation and display of virtual images. there arelimitations in the acceptability of computational delays that result fromdynamically generated, rapidly changing, complex scenes. the hardware andsoftware supporting the haptic interfaces, which allow the user to touch and feelthe objects with which he or she is interacting, have until now been limited todata gloves that can sense only approximately the position of the hands andfingers and provide no sensory feedback about the objects with which the userin interacting. technology problems also include the technology supporting thecomputation and display of virtual images.in thinking about other barriers, it is possible to ask why existingspecific applications of se systems421virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.simulation techniques are not more widely used in training. despite the numberof successful applications cited above, it is surprising that the use of simulationis not more general. although our review does not support a definitive answer,we can speculate that, until the last few years, the cost of computers and of thespecial hardware required to support realistic displays, together with theinflexibility embodied in specialpurpose hardware, made them not costeffective for many training applications. for example, until recently, drivingsimulators may have been too expensive to be costeffective when comparedwith the cost of training in a real vehicle. this is now changing, and there isgrowing interest in this and related applications.at this stage in the development of ve, the lack of demonstration of costeffectiveness is a clear barrier, but this stems as much from the lack ofdemonstrated effectiveness as from high cost.research needsthere is a need to mature the state of ve technology in order to broadenthe range of training applications for which it is costeffective. we run theserious risk of trying to test potential training applications and failing, notbecause an application is bad, or even one that would not ultimately showusefulness, but because the state of the technology was not yet mature enoughto support it effectively.we also have a need for improved methodologies for evaluating costeffectiveness. we can just as easily fail because we did not conduct anexperiment that was sensitive enough to reliably detect the potential gains thatwere presumed to be there. this can happen for reasons of inadequate control ofthe experiment or because of an injudicious choice of performance measures.finally, since research of this kind has just begun, there is a need todevelop a taxonomy of application areas that promise high leverage in vetraining effectiveness. it seems likely that tasks requiring spatial learning orawareness are good candidates. for example, in isolating faults in the avionicsof highly complex aircraft, the technician has to conduct the logical faultisolation task and also map the location of the fault on a schematic diagram tothe spatial location of the box containing the appropriate circuit on the actualaircraft. for today's aircraft, that is a very difficult task. it also seems likely thattasks for which enhanced artificial and spatially distributed cues could enhancelearning could be important applications. the oftreferred to demonstration ofillustrating the molecular forces associated with the position of atoms in acarbonchain molecule could provide a compelling lesson in molecularstructure. such a taxonomy of application areas should not be based onspeculation, however;specific applications of se systems422virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.it should emerge from a review of a large number of empirical tests of thesuccess of ve training, tests that have yet to be accomplished.educationthe problems of american education are too well known to needelaboration here. our children are not sufficiently engaged in learning the skillsand concepts presented to them by our schools, and the learning opportunitiesafforded are not always well matched with our society's future needs(secretary's commission on achieving necessary skills, 1991). syntheticenvironments may offer opportunities to address both concerns, within thepublic education system as well as in other arenas, such as homebasedentertainment and communications. for students these opportunities include: visiting simulations of ancient india or greece, the paleozoic era or theinner ear, to gather data for presentations, plays, and virtual worldbuilding of their own; observing ("jobshadowing") adults working in information space, such ascad artists and researchers using databases (such as medical librarians,historians, etc.); remotely experiencing such phenomena as the eruption of an underwatervolcano or the live birth of an elephant at the zoo in beijing; experimenting with simulations constructed by experts (e.g., physicists,ecologists, social and econometric model builders); working with other students of different ages and cultures, at differentsites internationally, on a daily basis, to improve one another's languageskills, or on tasks like those described above; building improved tools for their own use and use by others, such aslibraries of images, elemental simulations, stories, local history, anddemographic and geographic data.in essence, students using virtual reality would be able to do what wewould like students to be doing todayšbut with vastly expanded ability toaccess information in the larger world, to experiment, visualize, and understand,and to interpret the information to their own ends.rationaleeducation is an application area that cuts across subjectspecific domains;to the extent that a person can learn something using a ve system developed forany specific area, ve is being used for educational purposes. for example, ascientific visualization of a computer simulation that teaches a researchersomething new about nature is arguably anspecific applications of se systems423virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.educational application. however, for purposes of this discussion, educationalapplications of ve will be focused on the potential use of ve in grades k12, inwhich improvements are of demonstrable and pressing national concern.most commentators on the goals of k12 education would agree that itshould develop a student's capacity to think independently; increase a student'sdesire and motivation to learn; and increase the extent to which a student learnsand retains specific skills and knowledge. in contrast, there is no unanimityabout how to create an environment in which these things happen. forspecificity, the discussion that follows is guided by the philosophy that peoplelearn best when they can integrate what they are learning into the broadercontext of other things they know and care about; that they are more highlymotivated when they can and have reason to influence the course of their ownlearning; and that they learn to think independently when they are givensubstantial opportunities for doing so. much of this educational philosophy hasbeen characterized as constructionism.constructionism is a theory of instructional design, based on constructivisttheory. constructivism is a school of thought among developmentalpsychologists (carey, 1987; piaget and inhelder, 1967) that concerns the way inwhich children develop models of the world. the idea is that the essential stepstoward a mature understanding of a particular subject include a series ofdifferentiations and reintegrations of experiences involving the dissection andreconstruction of internal models. within this philosophical framework,computer and other information technologies, specifically ve, may haveimportant roles to play in improving education. in particular: ve is a potential vehicle through which the range of experience to whichstudents are exposed could be vastly increased. ve can provide immersive and interactive environments that providemacro contexts in which interesting intellectual problems naturally arise.3 ve potentially provides micro worlds in which students can exercise theskills and use the knowledge they learn.3 theories of situated cognition (brown et al., 1989) suggest that learning takes placemost effectively in contexts that are meaningful to the learner. such pedagogicalapproaches contrast sharply with those that assume that the learner learns a general skillthat can then be applied to all relevant situations. one instructional design techniquebased on situated cognition is referred to as anchored instruction (cognition andtechnology group at vanderbilt, 1993). anchored instruction consists of providing arich story line or macro context, within which ve environments may be able to providemacro contexts that are both rich and controllable.specific applications of se systems424virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved. ve potentially expands the peer group among which collaborativelearning experiences are possible.4specific applicationsthe technology of ve, augmented reality, and telepresence is too new tohave any real educational applications, if real is taken to mean an applicationthat is fully integrated into the intellectual substance of a nonexperimentalcurriculum. the following discussion focuses on a number of applications ofve technology to education that are suggested by preliminary experiments andfield trials to date: field trips and telepresence, spatial relations (real space orphase space), playrooms to build things, micro worlds, simulations of thingsthat are too complex or expensive to experience or experiment with, and newconceptualization tools for traditional subjects.each section below addresses a longrange vision of how ve might assistin these applications and a description of possible nearterm demonstrations.simulated field trips and telepresenceschools often use field trips to expose students to unfamiliar physicalenvironments (e.g., an innercity class may visit a farm). however, cost,convenience, and safety can limit such opportunities for students. vetechnology could provide immersive display systems that would enable studentsto experience exotic environmentsšin museum dioramas, in microscopicworlds (see taylor et al., 1993), and in remote and hazardous surroundings.one example, the jason project (see tyre, 1989; ullman, 1993), amassachusetts nonprofit enterprise developed by ocean explorer and geologistrobert ballard, is built around a remotely controlled submarine (jason). livevideo images of grey whales in the ocean off mexico and hydrothermal vents inthe ocean floor were broadcast from jason to sites in schools and universities,where nearly 1 million students could see underwater exploration as it wasoccurring and experience a sense of immediacy and involvement. the jasonproject also permitted 23 students to participate directly and interactively.specifically, these participants4 the value of collaborative work is suggested by a number of projects in whichstudents collaborate via electronic mail, for example, the national geographic society'skidsnet, bank street college's earth lab, and at&t learning circles systems. vecould enable collaborating students to share environments, thus reducing or eliminatingthe difficulty of using words to describe objects or phenomena that they collectivelyneed to reference.specific applications of se systems425virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.were directly involved in the control of the robot's motion in real time, thuscreating an experience of telepresence.spatial relationsin the summers of 1991 and 1992, the pacific science center in seattle,washington, sponsored a technology academy program in which groups ofstudents ages 9 to 15 were introduced to some aspects of se. six oneweek daycamps were held each summer. participation consisted of seeing videotapes anddemonstrations and working with cad software (swivel 3d) on macintoshcomputers to construct graphical models for use in a ve. these models weretaken to the human interface technology laboratory at the university ofwashington for installation by graduate assistants into the laboratory's sesystem, which incorporated gloves, helmet, and sound.project managers meredith bricken and chris byrne (1992) reported thatstudents showed a high degree of comprehension and rapid learning ofcomputer graphics concepts such as cartesian coordinates and threedimensional modeling. students indicated that they would much rather workand play with se than with conventional video games or tv. however, theworkshops were only one week long, and the students' experience of immersivese consisted of parts of one day in which they saw their threedimensionalmodels in the se system.in spring 1992, intel corporation sponsored the creation of an exhibit atthe boston computer museum, which featured a twoparticipant virtual world.this world was based on sense8's worldtoolkit software and intel's dvigraphics cards, running in 486 pcs; the actual demonstration was constructedby sense8's ken pimintel and brian blau at the university of central florida(pimentel and blau, 1994). hundreds of people experienced the rather simplevirtual world, in which it was possible to grasp building blocks using a threedimensional wandlike pointing device and slide the blocks around to assembleone's own toy house.the boston computer museum invented a kind of video swivel chair (forwhich a patent application has been submitted). the chair carries a 13inch tvmonitor and allows the participant to look around in the virtual world. theimagery rotates to correspond to the chair's rotation. this approach avoids thesanitary problems and expense of providing stereo headmounted displays forvisitors. this kind of technology will probably also be useful in school settings.the autodesk cyberspace project has supported several experimentsconcerning multimedia tools in the public schools of novato, calif. in 1990,mark merickel of oregon state university conducted experiments in the oliveelementary school. one team used autosketch andspecific applications of se systems426virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.autocad to develop threedimensional models; another group exploredcyberspacešautodesk's immersive virtual world (merickel, 1990).playrooms to build thingssome education researchers feel that learning is enhanced when studentscan build their own simulations (harel and papert, 1991), but the constructionof virtual worlds with today's tools is technically challenging. bowen loftin atnasa/johnson and the university of houston has constructed a virtual physicslaboratory, built on nasa's se tools (yam, 1993). loftin is in the process ofextending his previous work on intelligent tutoring (loftin et al., 1991) into aricher virtual environment for science education.micro worldsinteractive, textbased, roleplaying environments and games have beendeveloping for several years on both the internet and on bulletin board systems(bruckman, 19921993). known as muds, moos and muses, theseenvironments have gained substantial popularity with a certain segment of thepopulation. mud stands for multiuser dungeon, reflecting the genre's originsin roleplaying games; moo stands for mud, objectoriented, which containsobjects that can be manipulated. muse stands for multiuser simulationenvironment.in a typical mud or moo, a participant types look and receives atextual description of the room or place currently occupied. any other players atthat place can type messages, which are immediately echoed on the screens ofall the others at that place. objects can be created, picked up, dropped, andused. new places can be added to the universe.muds provide a crude sense of space and a lively interaction with otherparticipants. they thus prefigure some of the kinds of interactions that can beexpected in fully immersive ves. some proponents of muds regard them asinstances of virtual reality. the popularity of muds as educational tools israpidly growing. they require only a pc, a modem, and access to the internet.pantelidis (1993) has provided a substantial bibliography on educational uses ofmuds and other ve systems in education.simulationsinteractive modeling and simulation is being pursued as an educationaltool on many fronts (fuerzeig, 1992). simulations of physical, biological, andsocial phenomena can have substantial pedagogic value, especiallyspecific applications of se systems427virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.when the systems being simulated are otherwise inaccessible to students. manysimulations have been successfully implemented on pcs and used in both k12and university environments (see white, 1984; maxis, 1991; glenn and ehman,1987; schug and kepner, 1984). as a research example, tom defanti andcollaborators at the university of illinois at chicago and the center forsupercomputing applications are designing educational applications of thecave display system (cruzneira et al., 1992). this system projects images onthree walls and the floor and tracks a principal viewer's head position todetermine the view direction and content. other viewers in the cave are"along for the ride," which can sometimes be disconcerting. the curriculartopics under consideration at present include scientific visualization and noneuclidean geometry. these topics are at present of more interest to universitythan to k12 educators.issues to be addressedthe discussion above is admittedly speculative. educators and theirconstituents will have to address issues of serious concern in three areas.desirabilityperhaps the most fundamental question is that of desirability. under whatcircumstances does it make senseševen given a technically perfect veeducational applicationšto use that application in the classroom? the questionarises because it is all too easy to imagine a classroom in which the amount ofinteraction that students have with real blocks, real people, and real situations isreduced in favor of simulated experiences.put differently, concrete, manipulable objects enhance children's ability tohandle abstractions, as repeatedly demonstrated in montessori schools. fieldtrips enhance the realism and relevance of lessons. why should children bedeprived of these tools, in favor of virtual building blocks and virtual fieldtrips? moreover, unlike real blocks and environments, the rules that governsimulated systems are limited only by the system developer's imagination, andas such are essentially arbitrary. why is it necessarily desirable for students tohave more experience and interaction with such systems?ultimately, the intellectual challenge is likely to be learning to see ve asone tool of many in the tool chest of responsible educators. ve can obviouslyprovide some experiences that would not otherwise be possible for students tohave, but when a "real" alternative is available, it may make more sense for thelatter to have priority. some judicious mix of handson learning, ve experience,and book learningšvarying fromspecific applications of se systems428virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.classroom to classroom and subject to subjectšwill demonstrate that thesetools can complement each other. such an approach is suggested by thevanderbilt group (cognition and technology group at vanderbilt, 1993); theyargue that handson and anchored instruction techniques are complementary.they use videotape and videodisk segments to establish the story line, and thenuse handson projects for the students to construct their own understanding.such a paradigm can also be explored in ve, with or without virtualreplacement of the handson activities.effectiveness and feasibilitythe ve hypothesis for education is that, for certain purposes, wellintegrated ve systems will achieve results superior to the use of conventionalcapabilities. the hallmarks of such success would be (1) significantimprovement in students' learning and retention of specific skills and concepts,compared with their response to similar content presented without ves and (2)significant increases in students' voluntary use of such systems, compared withtheir response to similar content presented in other ways.some reports suggest that the successful introduction of educationtechnology results in a sustainable increase in the enthusiasm of students, whichincreases the overall chances for educational success (office of technologyassessment, 1988). indeed, the popularity of video games and other suchtechnologies among k12 students outside the educational context stronglyindicates that engagement with technology is not likely to be a significantproblem.however, the eagerness with which students embrace technology shouldnot be taken as an unqualified endorsement of immersive graphics foreducation. rather, some part of the enthusiasm may come from the novelty ofthe medium. it is hard to test the kinds of learning achieved from field trips toexotic ve labs for oneshot viewings of one's geometric models or virtualsubmarine voyages. what is genuinely problematic is the following issue: towhat extent can educators design engaging ve systems (a relatively easy task)that also result in what reasonable people would regard as learning (a muchharder one)? to answer this question, much more empirical study is necessary.practicalitythe concern about practicality is dominated by cost. a number of studieshave established in specific contexts that computers can be costeffectivecompared with other means of delivering instruction (levin et al., 1984; officeof technology assessment, 1988). however, a number of other factorsintervene to complicate their wider adoption by schools.specific applications of se systems429virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.first, the introduction of technology seldom decreases costsšat least inpublic education. thus, even if the provable incremental performance providedby the technology is cheaper per unit than other possible improvements,political will and funding may not be forthcoming.second, costbenefit analysis for technology must necessarily analyze theteaching of skills and concepts that can be taught by other means. certain skills(e.g., accessing remote online databases) are meaningless without thetechnology. most parents want their children to have uptodate skills.schools are not yet able to afford wellsupported stateoftheart personalcomputers in meaningful numbers, and only recently is there broad acceptanceof the idea that computers are useful in education (office of technologyassessment, 1988). in 1993 a typical computer purchased for school use cost,with software, about $3,000šessentially the cost of a wellequipped apple ii in1979. increased performance expectations have offset the decrease incomponent prices, and so the cost of lowend personal computers has tended toremain about constant, even as their capabilities have increased.an entrylevel pcbased ve system can be purchased in 1993 forapproximately $24,000. if current trends continue, such systems should beavailable for around $3,000 in three to five years. current developments inentertainment electronics may advance that time frame somewhat, but displaytechnology is not likely to advance as fast as image generation and simulationtechnology. we believe that ve will not begin to penetrate schools until itsutility for specific purposes generates sufficient interest and desire, and when anindividual station of acceptable performance costs $3,000 or less.assuming that costs have been adequately addressed, it appears that publicopinion would be generally supportive of educational ve applications. but longtime educators have seen education fads come and go; they will need to beconvinced that ve gives them usable capabilities that can enhance education.thus, costs necessarily include those related to training teachers to use newtechnologies effectively and how to judge when the use of new technologies isappropriate. these costs are high, perhaps comparable to those related todeployment of the hardware itself.infrastructure and research needsthe development of highquality software followed the arrival of the pcby more than a decade. only recently has commercial software becomeavailable that meets real education needs beyond drillandpractice. this is dueboth to the maturation of a generation of lessonbuilders and their firms and tothe arrival of mature, wellreasoned national agendasspecific applications of se systems430virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.such as those provided by the national council of teachers of mathematics(1989). the generational shift from emphasizing knowledge to emphasizingcompetence, represented by the national council for teachers of mathematicscurriculum and its siblings, has finally provided educational computing withappropriate subject matter and philosophical focus.a similar evolution will be necessary for ve in education. a critical massof competent ve programmers must develop and educators will need to workwith the hardware and software over a period of years before clear directionswill emerge.on the technology side, no special issues stand out. this is partly becausethe specific requirements of ve systems in education cannot be articulated untilwe have a better understanding of the goals of those systems.although the technology research agenda for education is not separatefrom that of ve in general, a great many questions and issues for educationresearch remain: the identification and characterization of skill and subject matter domainsfor which vebased immersion can be demonstrated to provide cleardidactic advantages over equivalent nonimmersive presentations. the relationship between immersion and nonimmersive representationswithin a given educational environment, as tools to help studentsunderstand their own learning process. immersive presentations may bemore engrossing and lead to intuitive understandings. twodimensionalor schematic presentations may lead to better abstract understandings. the development of a variety of means (user interfaces, languages, cadtools) whereby ve environments can be easily expressed and constructedby lesson designers. the development of concepts and tools (e.g., telepresence) that can beused by students to facilitate their own modelbuilding within thoseenvironments and the educational significance of their use (e.g., asindicated by their ability to embody objective knowledge about theprocesses of science, economics, history, etc. in their models). the educational value of roleplaying adventure games, anchoredlearning, and other shared simulation experiences in fostering thedevelopment of analytical skills such as problem formulation. the extent to which various features of the user interface can substitutefor or enhance the interpersonal interactions among colocated teammembers.5specific applications of se systems431virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved. the role of thirdparty mentors, either explicitly present and visible in thesimulation or behind the scenes, in enhancing the learning experience.with some relatively small modifications, all of these questions and issuescan and should be asked of any information technology in an education context.but the threedimensional immersive environment affords a much richer spacethan a twodimensional screen in which to ask and address such questions.indeed, the richness may well be a significant differentiating feature if answersto these questions depend on the specific technology being considered.information visualizationwith the rapidly increasing amount, types, and sources of informationbeing produced for scientists, engineers, business executives, and the public,there is a pressing need to develop better presentation techniques and formats tosupport everyday tasks of exploration, understanding, and decision making. theinformation explosion, coupled with advances in computer technology, offerssome promising opportunities to develop new approaches to visualizinginformation. currently we are presented with information in a variety of forms,including text and images in two and three dimensions that can be static ordynamic. interaction techniques are limited to pointandclick windowinterfaces that can require as many as 10 to 50 steps to accomplish a single task.possibilities for the future include augmented reality, in which a virtualimage is superimposed on the real world (e.g., view the pipes inside a wall), andves in which the user is immersed in the information and interacts with it inreal time. effectively designed environments should provide individuals witheasier access to the critical elements in the information; the ability to view andexplore interactions among multiple problem dimensions simultaneously; andthe opportunity to examine and test relationships that cannot be presented on atwodimensional display. as an example, wellpresented information may assistdecision makers in a manufacturing plant to understand more readily the effectsof several variables, such as current economic conditions, environmentalimpact, materials flow, staff training, and marketing forecasts, on the feasibilityand cost of producing a particular product. in another example, ve technologycould be used to visualize a proposed factory operation before it is developed.furthermore, creative use of the technology to present financial data couldassist investors in making more informed decisions.designing useful visualizations for different types of information insupport of different user tasks is an enormous undertaking. one major5 for example, convolved sound, touch, and other sensory modalities and transmodaltranspositions (e.g., a keyboarddriven speech synthesizer) may make it possible to getmore involvement from relatively nonverbal people than would be possible in traditionalinperson teamwork. disabled learners could be involved on a more equal basis, wheneveryone is learning new metaphors for motion and control.specific applications of se systems432virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.area of required work is the development of computational software to managelarge and diverse databases in ways that allow users to explore alternatives andmake discoveries. this issue is discussed in more detail in chapter 8, oncomputer generation of virtual environments and in the section below onscientific visualization.a second area of required work is the determination of what informationshould be provided, how it should be formatted, and how we expect the user tointeract with it. researchers have worked for many years to create dynamic twodimensional information displays for such tasks as monitoring the status of anuclear power plant, flying a jet aircraft, controlling aircraft traffic, andanalyzing complex data. this work has provided some insight into how muchinformation can be absorbed at one time, how it should be organized on thescreen, and how frequently it can be updated before the limits of humaninformation processing are exceeded (ellis, 1993). in addition, cognitivescientists have been exploring the relationship between types of tasks and themost appropriate types of information to support those tasks (palmiter andelkerton, 1993).although some of the knowledge about human information processing,learning, and problem solving gained when using twodimensional displays willbe of value in designing information displays in threedimensionalenvironments, we will need to mount a substantial research effort to determinehow to use the capabilities of threedimensional environments effectively. anintegral part of these research efforts will be a determination of the most userfriendly and efficient interaction techniques. other chapters of this bookprovide a discussion of these issues. of particular importance will be researchinto the use of sensory modalities other than vision in increasing or modifyingthe comprehension of information.currently, little progress has been made in the use of virtual or augmentedreality for the purposes of information visualization. however, someinvestigators have begun to explore various aspects of visualization forscientific purposes. a brief description of results in this area are reported below.many of the problems raised will be pertinent to the design of informationpresentation for other types of activities.scientific visualizationscientific visualization (mccormick et al., 1987) is the use of computergraphics to create visual images that aid in the understanding of complex, oftenmassive, numerical representations of scientific concepts or results. suchnumerical representations, or datasets, may be the output of numericalsimulations as in computational fluid dynamics and or molecular modeling,recorded data as in geological and astronomical applications,specific applications of se systems433virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.or constructed shapes as in visualizing topological arguments. thesesimulations may contain highdimensional data in a threedimensional volume,and they often vary in time. different locations in such datasets can exhibitstrikingly and interestingly different features, and difficulty in specifyinglocations will impede exploration. scientific insight into complex phenomenadepends in part on our ability to develop meaningful threedimensional displays.rationaletraditionally, scientific visualization has been based on static or animatedtwodimensional images that have generally required a significant investment intime and expertise to produce.6 as a result, severe limits have been placed onthe number of ways in which a dataset can be explored. that is, an explorerdoes not know a priori what images are unimportant, but when the effort toproduce a visualization is large, there will understandably be a hesitation toproduce a picture that is likely to be discarded.other problems arise with traditional scientific visualization techniquesbecause they are not well suited to the computational datasets associated withmodern engineering simulations. these datasets may be inherently complex,consisting of a time series of threedimensional volumes, with many parametersat each point. also, scientists are often interested in behavior induced by thesedata (i.e., streamlines in a vector field) rather than the data values themselves.under these circumstances, realtime interactive visualization is likely to payoff, due to the complexity of phenomena that can occur in a threedimensionalvolume.ve technology is a natural match for the analysis of complex, timevaryingdatasets. scientific visualization requires the informative display of abstractquantities and concepts, rather than the realistic representation of objects in thereal world. thus, the graphics demands of scientific visualization can beoriented toward accurate, as opposed to realistic, representations. furthermore,as the phenomena being represented are abstract, a researcher can performinvestigations in ve that are impossible or meaningless in the real world. therealtime interactive capabilities6 in the early days of visualization, it was rather difficult for the researcher to producevisualizations beyond conventional drawings and plots. familiarity with computergraphics programming was required to do more sophisticated visualization, a need thatwas addressed through the creation of ''visualization shops," in which a visualization wasproduced to order. a researcher provided data to a visualization programmer, who thenproduced a highquality image or animation. thus, there was a significant investmentinvolved in the production of visualization. this served the purpose of visualization as apresentation medium, but it hindered the use of visualization as an exploratory medium.specific applications of se systems434virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.promised by ve can be expected to make a significant difference in theseinvestigations, with the potential to provide: the ability to quickly sample adatasets volume without cluttering the visualization; no penalty forinvestigating regions that are not expected to be of interest; and the ability tosee the relationship between data nearby in space or time without cluttering upthe visualization. in short, realtime interaction should encourage exploration.just as important, a natural, anthropomorphic threedimensional vebasedinterface can aid the unambiguous display of these structures by providing arich set of spatial and depth cues. ve input interfaces allow the rapid andintuitive exploration of the volume containing the data, enabling the variousphenomena at various places in that volume to be explored, as well as providingsimple control of the visualization environment through controls integrated intothe environment.a properly constructed vebased interface will require very little of theuser's attention; it would be used naturally, using pointing and speechcommands and directions rather than commandline text input. someone usingsuch an interface would see an unambiguous threedimensional display. thiswould contrast with the current interaction paradigm in scientific visualization,which is based on text or twodimensional input via graphical user interfacesand twodimensional projections of threedimensional scenes.specific applicationsve systems for scientific visualization are in many ways like softwarepackages for graphing: tools for displaying and facilitating the interpretation oflarge datasets. but it is too early to describe a single generalpurpose ve systemfor scientific visualization. at the same time, a number of projects havedemonstrated that vr does have significant application potential. aeronautical engineering: the virtual wind tunnel (bryson and levit,1992; bryson and geraldyamasaki, 1992) uses virtual reality to facilitatethe understanding of precomputed simulated flow fields resulting fromcomputational fluid dynamics calculations. the visualization of thesecomputations may be useful to the designers of modern highperformanceaircraft. the virtual wind tunnel is expected to be used by aircraftresearchers in 1994 and provides a variety of visualization techniques inboth singleuser and remotely located multipleuser environments. general relativity: virtual spacetime (bryson, 1992) is an extension ofthe virtual wind tunnel in which curved spacetimes, which are solutionsspecific applications of se systems435virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.to einstein's field equations of gravitation, are visualized using articlepaths in virtual reality. molecular modeling: molecular docking studies using a ve that includeda forcereflecting manipulation device have been performed with thegrope system at the university of north carolina at chapel hill(brooks et al., 1990). investigators employed a headtracked stereodisplay in conjunction with the forcefeedback arm to investigate howvarious molecules dock together. these studies have implications for thedesign of pharmaceuticals. scanning tunneling microscopy: a ve coupled with a telerobot for thecontrol and display of results from a scanning tunneling microscopecalled the nanomanipulator has been developed at the university of northcarolina at chapel hill (taylor et al., 1993). this system uses a headtracked stereo display in conjunction with a forcefeedback arm to displaya surface with molecular resolution via graphics and force reflectionbased on data obtained in nearreal time from a scanning tunnelingmicroscope. in addition, there is the ability to deposit very small amountsof material on the surface via direct manipulation by the user. medical visualization: medical visualization systems using augmentedreality (e.g., bajura et al., 1992) have been developed at several sites. theprimary difficulty with medical visualization at this time involves the verylarge amounts of graphic data being displayed. bajura et al. are designinga system that will map ultrasound imagery in real time onto thephysician's view of the real patient, allowing the location of the featuresshown in the ultrasound imagery to be quickly and intuitively located inthe patient. astrophysics: a system to investigate cosmic structure formation has beenimplemented at the national center for supercomputing applications(song and norman, 1993). this system visualizes structure arising fromsimulations of the formation of galaxies in the early universe. circuit design: the electronic visualization laboratory at the universityof illinois, chicago, has implemented several scientific visualizationapplications in a virtual environment setting. for descriptions of theindividual projects, see cruzneira et al. (1993a, 1993b).issues to be addressedexperience with vebased scientific visualizations has shown that in orderto sustain usable interaction and to make the user feel that a series of picturesintegrates into an insightful animation, a number of criteria must be met. first,the system must provide interactive response times to the user of approximately0.1 s or less. interactive response time is a measure of the speed with which theuser sees the results of actions; if thespecific applications of se systems436virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.interactive response time is too slow, the user will experience difficulty inprecisely placing visualization tools (sheridan and ferrill, 1974). second,effective systems for scientific visualization must have animation rates of atleast 10 frames/s. animation rate is a measure of how fast images are presentedto the user; this rate is particularly relevant with respect to viewing control andfor timevarying datasets. if the rate is too slow, the images will be perceived asa series of still pictures rather than a continuous evolution or movement. thesetwo parameters are psychologically and perceptually related, albeitcomputationally distinct. some ve systems may separate the computation andvisualization processes, so that they run asynchronously. we are at thebeginning of understanding the potential of this technology for scientists.research is needed to answer such questions as when are continuous imagesmore useful than discrete images for scientific insight.scientific visualization also makes particular demands on virtual realitydisplays. the phenomenon to be displayed in a scientific visualizationapplication often involves a delicate and detailed structure, requiring highquality, highresolution, and fullcolor displays. experience has shown thatdisplays with 1,000 × 1,000 pixel resolution are sufficient for manyapplications. in addition, a wide field of view is often desirable, as it allows theresearcher to view how detailed structures are related to larger, more globalphenomena.lastly, user acceptance criteria suggest that few researchers would bewilling to invest the time required to don and doff headmounted displaysavailable at the time of this writing. furthermore, many researchers haveexpressed distaste for donning helmets or strapping displays onto their heads.telecommunications and teletravelrationaleas facilitators of distributed collaboration, the applications oftelecommunications and teletravel cut across all of the other applicationsdiscussed in this chapter. for manufacturing activities of the future, it isanticipated that virtual images of products will be simultaneously shared bygeographically dispersed design engineers, sales personnel, and potentialcustomers, thus providing the means for joint discussion and productmodification. in health care, there are several examples of distributedcollaboration, including remote surgical practice and remote diagnosticconsultation among patients, primary care physicians, and specialists who mayall be viewing common data or threedimensional images. an example of thelatter is the development of a telemedicine system in georgia,specific applications of se systems437virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.in which medical center expertise is shared over a network with rural doctorsand their patients.for education and training, there are many instances in which distributedcollaboration may be useful. one example is the use of a shared virtualbattlefield for mission planning, rehearsal, and training. another potential use isoffering students from several schools around the country the opportunity tocome together through network technology to share a common virtual worldšsuch as a reconstruction of a historic site that no longer exists. finally, inhazardous operations, distributed collaboration is a central feature of humansand telerobots working together is the same remote environment.the discussion in this section focuses on the increasingly collaborativenature of modern business and the potential contribution of se technology tofacilitating this collaboration in a costeffective manner. specifically, wediscuss telecommunication and teletravel. both of these processes usetechnology to reduce unproductive travel time to and from meeting sites orregular work sites.today, many of those who are knowledge workers already use technologyto avoid travel. the nature of a knowledge worker's job is such that by workingat home or in satellite locations using personal computers, modems, and thetelephone network, these telecommuting workers can perform their jobsreasonably well. greater understanding and acceptance of this phenomenon inthe workplace is illustrated by the response of the los angeles work force tothe earthquake of january 1994. in the aftermath of the disaster and theconsequent disruption of customary commuter routes, telecommuting increaseddramatically. however, most workers in the united states do not have jobs thatcan be performed using only a screen, a keyboard, and a mouse. most salespeople must interact facetoface, and others, such as craftspeople, work onsolid objects with their eyes and hands. even telecommuting knowledgeworkers need facetoface meetings for discussions involving more than twopeople, job interviews, and many other work situations in which gestures, facialexpression, and eye contact are critical components of the interchange. theseadded task requirements open the door to the next step in the use of vetechnology.backgroundthe historical evolution of distributed collaboration provides a usefulcontext for this discussion. study of the paths followed in the earlier efforts inboth research and system development can reveal the current robust status oftelecommunication facilities, as well as the potential consequences ofexpanding such facilities to include ve capabilities.specific applications of se systems438virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.distributed collaboration first emerged as a product of advancedtechnology in the form of multiperson telephone conversations. such serviceswere provided by at&t during the 1950s with economic benefits for bothusers and providers. routine use of this technological capability came in the1960s, along with expansion of the concept to include secure conferencesbetween remote participants. a separate and special technology,telecommunication, came to be institutionalized as a consequence. severalmajor systems were developed at this time to serve the needs of the federalgovernment. among such systems were the first models of autosevocom,a secure network that could support remote conferences between toplevelmilitary commanders from their stations around the world (sinaiko and belden,1965). this system also cemented the integration of computers into themultistation communication network. in these early instances, the computer wasused only as a switching device and a tool for signal encryption. however, itspresence in the system was a definite harbinger of things to come.in addition, the design effort for military systems set off programs ofresearch intended to explore the potentials, both positive and negative, ofcomputermediated communication. for example, studies were initiated todetermine the feasibility of using a computermediated network to link northatlantic treaty organization member heads of state for purposes of joint crisisresolution. problems ranging from how to implement rules of diplomaticprotocol to overcoming language barriers were explored. the outcomes of theseresearch programs revealed some of the limitations on communicationeffectiveness imposed by an absence of visual information to augment the directvoice transmissions.digitization, packet switching, and optical fibers, among othertechnological advances, began to open more vistas in the 1970s. it was then thatthe first instances of teletravel began to appear (fordyce, 1974; craig, 1980)after having been forecast several years previously. entire new areas ofeconomic advantage began to be apparent, such as the possible savings ingasoline use and pollution reduction. negatives such as lowered productivitydue to a lack of supervisory presence were played down by early enthusiasts buthave come to be treated as significant matters in presentday applicationinstances. in any case, telecommuting, as a form of distributed collaboration,has become an accepted option for some workers in some organizations(shirazi, 1991).the other critical ingredient in the evolution of distributed collaborationwas the rapid adoption of small but powerful computers by workers in manydifferent occupations. computerized networks began to become widespread inthe 1980s. in a sense, the computer became an actual participant in multipersoncollaborations that were performed on the network. the computer provided aninformation storage and retrieval capabilityspecific applications of se systems439virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.that far exceeded what the humans could contribute. also, the computer couldprovide a dynamic color graphics capability that is not available by any othermeans. objectoriented collaborations, such as designing electronic circuitry,have been quite successful when these technologies have been employed(sheridan, 1993a; fanning and raphael, 1986).in summary, distributed collaboration is not a particularly new idea.working in this manner has gradually expanded over the past four decades, aspeople have accustomed themselves to the concept and its ramifications and asthe technology has progressed to a point at which it supports new modes ofactivity at affordable costs. now the question becomes: what can or will veadd to the process? will ve provide the means to take a few more incrementalsteps in the further expansion of distributed collaborationšor will ve providethe basis for major change in how the concept is actualized?teletravel and virtual environmentsve offers the possibility for one to participate in a meeting in which all theother attendees were present in the form of virtual images. each participant inthe virtual meeting would see and hear the other participants throughlightweight, seethrough ve goggles that resemble eyeglasses, while his or herown appearance was captured by a video camera for broadcast to all the others.different communications channels would support both group communicationsand communications to selected individualsšthe equivalent of whispering tosomeone.the social feasibility of virtual meetings goes beyond the technology. theenormous difficulty of scheduling conference phone calls for more than fourbusy people suggests that a new set of social norms would be needed beforevirtual meetings could be called routinely. for example, people would have tofeel that is was unacceptable to remain in a virtual meeting and attend to otherbusiness simultaneously.shared workspaces refer to the real or virtual gathering of people at aspecific location for the purpose of interacting with an artifact or an object.with the appropriate technology for automatic model generation, any physicalspacešand the relevant artifacts and objectsšcould be turned into a virtualmeeting place. thus a group could travel to any location where sensors existedand meet while observing events taking place in that realworld location.ve also offers the possibility that one could be part of collaborativecommunities at a distance. for example, xerox parc is currentlyexperimenting with technologies that virtually bring together people working indifferent offices. although the current effort is limited to smallscreenspecific applications of se systems440virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.video and audio, vebased collaborative communities could offer illusions ofphysical presence so real that offices of collaborators and colleagues could begeographically dispersed much more than they are today.to facilitate the illusion of shared office spaces, office workers might wearglasses that could give them the feeling that their offices were part of a muchlarger common area in which many other participants were present. eachparticipant would see the other participants sitting at desks in their own offices.in situations in which a traveler is unable to physically go to the target site,virtual travel may be useful. an example would be to enable inmates ofcorrectional facilities to hold jobs or be trained in the outside world while stillbeing controlled and monitored physically. police might travel virtually to themiddle of a jail riot to gather further intelligence. researchers, regulators, andsite planners might virtually meet in a hazardous environment, such as a nucleardumping site, to examine conditions and plan operations for the future. it isconceivable that close observation and review of site conditions could providesuperior input to planning based on video or still pictures.teleoperation and remote accessteleoperated systems controlled through a ve interface would enable anindividual or a group to go beyond mere passive observation. for example, agroup on a virtual visit to a nuclear power plant could be authorized to open andshut valves and make other changes to the physical status of the plant. since thevalves and other actuators in such a plant are electronically controlled from acontrol panel, it is not too farfetched to imagine the sensor data and control ofthe operation of the plant being part of the virtual world inhabited by the plant'shuman operators. the operators might be more aware of the status of the plantin emergencies if they could virtually travel through its radioactive corridors.any device that was connected to the communications grid could becontrolled by a virtually present human. a home security system, for example,could summon home its owner when an alarm went off. (for an example basedin telecommunications prior to ve technology, see taylor, 1980.) withappropriate cameras and sensors, the owner could travel through the house(virtually) to see if intruders were present. with certain actuators present in thehome and linked to the network, the owner might either drive off to escapeconfrontation or try to capture the intruders in the home.a very general means of making changes to the world would be for adistant person to occupy a telerobot to perform some task. discussions ofspecific applications of se systems441virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.telerobotics tend to treat the link between human operator and telerobot assemipermanent. but if telerobots were common, they might be treated more liketelephonesšthat is, known locations into which one could project one's eyesand hands.research needsa key aspect of a virtual meeting is the ability to see body movements andfacial expressions, a feature difficult to achieve with current video conferencingsystems. in real meetings in real places, the participants perceive themselves tobe in a place, surrounded by its walls. they are able to observe the positions ofother participants within that space and hear their voices coming from specificdirections. these perceptual possibilities are not available from videotelecommunication systems. therefore, the use of directional sound in virtualmeetings will be especially important. with as few as two pairs of peoplehaving simultaneous independent conversations, the conversations will bedisrupted without the ability of the hearers to filter out the unwanted sounds.this is done in real life (the "cocktail party effect") by the human ability toselectively filter sounds based on their directionality, and the fact that soundsfrom more distant sources are less loud. both of these properties of real soundscan be supported in shared virtual worlds.to capture an image of a user's facial expression while allowing the user toview the shared virtual world, several display methods are available. one wayto do it is for the user to wear a seethrough hmd, resembling eyeglasses. avideo camera could be trained on the user's face. the seethrough capability ofthe hmd in this case is primarily useful for allowing the camera to see thoughto the user's face, rather than for allowing the user to see the real surroundings.another viewing method is to put the user in an immersive viewingstation, similar to the cave, a room whose walls, ceiling, and floor surround aviewer with projected images (cruzneira et al., 1992). since the cave usespolarized glasses for stereo, a camera is needed that can see through to collectimages of both eyes of the user through the polarization. both of these viewingsetups are adequate to allow a group of people, each at a viewing station, to seeand be seen by the other members of a group of people occupying a virtualmeeting space.the twodimensional image of face and body that would be collected by anormal video camera is adequate but not optimal. the twodimensional facetexture could be mapped onto a virtual mannequin representing the person inthe virtual world, and the same could be done for the twodimensional bodyimage. this would provide a very flat person to the virtual world, but it wouldstill have advantages over video telecommunication,specific applications of se systems442virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.which does not show the locations of the different participants very effectively.a more elaborate body image capture method would be to use rangeimaging techniques to capture a threedimensional model of the body and face.such automatic threedimensional model acquisition is needed by otherbranches of the se field, and various prototype systems exist for range imaging.a threedimensional image of the body of each participant in a virtual meetingbegins to make such meetings sound like they could actually come close toduplicating the perceptual feel of being physically present at a real meeting.these technologies, though still immature, offer the possibility ofelectronically projecting oneself, as easily as one currently makes telephonecalls, into virtual worlds inhabited by other distant human users, with whom onecan have facetoface interactions both oneonone and in groups. these sharedmultiperson virtual worlds create a shared space, in which each humanparticipant has a position, a body image resembling his or her own realappearance, and a viewpoint from which to observe the behaviors and facialexpressions of the other people engaged in the transaction.specific applications of se systems443virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.referencesoverviewcatmull, e., l. carpenter, and r. cook 1984 private and public communication. communication inreference to the number of polygons required to render reality, making certain assumptionsabout depth complexity and display resolution.larijani, l.c., ed. 1994 the virtual reality primer. new york: mcgrawhill, inc.thompson, j., ed. 1993 virtual reality: an international directory of research projects. aldershot,u.k.: jbt publishing.thorpe, j. 1993 synthetic environments strategic plan. draft 3b. alexandria, va.: defense researchprojects agency.chapter 1: some psychlogical considerationsakin, d.l., m.i. minksky, e.d. thiel, and c.r. kurtzman 1983 space applications of automation,robotics, and machine intelligence systems. aramis phase ii, vol. 3. nationalaeronautics and space administration.anderson, j.r., a.t. corbett, k.r. koedinger, and r. pelletier 1993 cognitive tutors: lessonslearned. unpublished paper. a copy may be obtained by contacting john r. anderson atcarnegie mellon university via electronic mail: ja0s@andrew.cmu.edubachyrita, p. 1972 brain mechanism in sensory substitution. new york: academic press.references444virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.bachyrita, p. 1992 invited address: tactile displays. international symposium: digest oftechnical papers. vol: 23, may. boston, mass.: society for information display.baron, s., c. feehrer, r. muralidharam, r. pew, and p. horwitz 1982 an approach to modelingsupervisory control of a nuclear power plant. technical report nureg/cr2988,ornl/sub/8170523/1, oak ridge national laboratory.bartlett, f.c. 1932 remembering: a study in experimental and social psychology. cambridge,england: cambridge university press.bejczy, a.k. 1982 manual control of manipulator forces and torques using graphic display. pp.691698 in ieee international conference on cybernetics and society.benthin, a., p. slovic, and h. severson 1993 a psychometric study of adolescent risk perception.journal of adolescence 16(2):153168.bertsche, w., k. logan, a. pesch, and c. winget 1977 operator performance in underseamanipulator systems: studies in control performance with visual force feedback.technical report whoi776, woods hole oceanographic institution.bregman, a.s. 1990 auditory scene analysis. cambridge, mass.: mit press.bryson, a.e., and y. ho 1975 applied optimal control. hemisphere, washington, d.c.cesarone, b. 1994 video games and children. eric digest. eric clearinghouse on elementaryand early childhood education, urbana, ill., january.chi, m., p. feltovich, and r. glaser 1981 categorization and representation of physics problems byexperts and novices. cognitive science 5:121152.coldevin, g., et al. 1993 influence of instructional control and learner characteristics on factualrecall and procedural learning from interactive video. canadian journal of educationalcommunication 22(2):113130.coombs, n. 1993 global empowerment of impaired learners. educational media international 30(1):2325.dubriel, r. 1993 all things being equal. vocational education journal 68(8):2869.durlach, n.i., l.a. delhorne, a. wong, w.y. ko, w.m. rabinowitz, and j. hollerbach 1989manual discrimination and identification of length by the fingerspan method. perceptionand psychophysics 46(1):2938.references445virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.elkind, j.i. 1956 characteristics of simple manual control systems. technical report 111, lincolnlaboratory, massachusetts institute of technology.fasse, e.d., b.a. kay, and n. hogan 1990 human haptic illusions in virtual object manipulation.proceedings of annual conference of ieee engineering in medicine and biology society12(5):19171918.fling, s., l. smith, t. rodriguez, d. thornton, et al. 1992 videogames, aggression, and selfesteem: a survey. social behavior and personality 20(1):3945.fontaine, g. 1992 the experience of a sense of presence in intercultural and internationalencounters. presence: teleoperators and virtual environments 1(4):482490.funk, j. 1992 video games: benign or malignant? journal of developmental and behavioralpediatrics 13(1):5354.garner, w.r. 1962 uncertainty and structure as psychological concepts. new york: wiley.1974 the processing of information and structure. new york: wiley.glaser, r., a. lesgold, and s. gott 1991 implications of cognitive psychology for measuring jobperformance. pp. 126 in a.k. wigdor and b.f. green, jr., eds., performance assessment for the workplace, vol. 2: technical issues. committee on the performance of militarypersonnel, national research council, washington, d.c.: national academy press.heeter, c. 1992 being there: the subjective experience of presence. presence 1(2):262271.held, r., and n. durlach 1991 telepresence, time delay, and adaptation. in pictorialcommunication in virtual and real environments. london: taylor and francis.1992 telepresence, spotlight on: the concept of telepresence. presence 1(1):109112.hogan, n., b.a. kay, e.d. fasse, and f.a. mussaivaldi 1990 haptic illusions: experiments onhuman manipulation and perception of 'virtual objects.' cold spring harbor symposia onquantitative biology 55:925931.howard, i.p., and w.b. templeton 1966 human spatial orientation. london: john wiley.jex, h.r. 1971 problems in modeling manmachine control behavior in biodynamic environments.in 7th annual conference on manual control, nasa sp281.jones, l.a., and i.w. hunter 1990 influence of the mechanical properties of a manipulandum onhuman operator dynamics: 1. elastic stiffness. biological cybernetics 62:299307.references446virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.1992 human operator perception of mechanical variables and their effects on tracking performance.in asme winter annual meeting: issues in the development of kinesthetic displays forteleoperation and virtual environments, anaheim, nov. 813.1993 influence of the mechanical properties of a manipulandum on human operator dynamics: 2.viscosity. biological cybernetics (in press).kearney, r.e., and i.w. hunter 1987 system identification of human joint dynamics. criticalreviews in biomedical engineering 18(1):5587.kidd, j.s. 1958 social influence phenomena in a taskoriented group situation. journal of abnormaland social psychology 56(1):1317.kleinman, s., d.l. baron, and w.h. levinson. 1974 a control theoretic approach to mannedvehicle systems analysis. ieee transactions on automatic control ac16:824832.kok, j.j., and r.a. van wijk 1977 a model of the human supervisor. pp. 210216 in thirteenthannual conference on manual control, moffett field, calif.kuipers, b.j. 1975 a frame for frames. in d. bobrow and a. collins, eds., representation andunderstanding: studies in cognitive science . new york: academic press.lajoie, s.p., and a.m. lesgold 1992 dynamic assessment of proficiency for solving proceduralknowledge tasks. educational psychologist 27(3):365384.lesgold, a., s.p. lajoie, d. logan, and g.m. eggan 1990 cognitive task analysis approaches totesting. pp. 325350 in n. frederiksen, r. glaser, a. lesgold, and m. shafto, eds.,diagnostic monitoring of skill and knowledge acquisition. hillsdale, n.j.: lawrenceerlbaum.loomis, j.m., and s.j. lederman 1986 tactual perception. chap. 31 in k.r. boff, l. kauffman, andj.p. thomas, eds., handbook of perception and human performance: vol. i  sensoryprocesses and perception. new york: john wiley.loomis, j.m. 1992 distal attribution and presence. presence 1(1):11319.massimino, m.j., and t.b. sheridan 1993 sensory substitution for force feedback in teleoperation.presence 2(4):344352.mcruer, d.t., d. graham, w.s. krendel, and w. reisener, jr. 1965 human pilot dynamics incompensatory systemstheory, models and experiments with controlled element andforcing function variations. technical report affdltr6515, air force flightdynamics laboratory, wrightpatterson air force base, ohio.mead, g.h. 1934 mind, self, and society. chicago: university of chicago press.references447virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.minsky, m. 1975 a framework for representing knowledge. in p.h. winston, ed., the psychologyof computer vision. new york: mcgrawhill.morrison, h., j. mcclure, and c. alewise 1993 the impact of portable computers on pupil'sattitudes to study. journal of computer assisted learning 9(3):130140.neuman, d. 1989 computer based education for learning disabled students: teacher's perceptionsand behaviors. journal of special education technology 9(3):156166.newell, a., and h.a. simon 1972 human problem solving. englewood cliffs, n.j.: prenticehall.novelli, j. 1993 there's never been a better time to use technology. instructor 103(3):34ff.pang, x.d., h.z. tan, and n.i. durlach 1991 manual discrimination of force using active fingermotion. perception and psychophysics 49(6):531540.pepper, r.i., and j.d. hightower 1984 research issues in teleoperator systems. presented at 28thannual human factors society, san antonio, tex.quastler, h. 1955 information theory in psychology: problems and methods. glencoe, ill.: freepress.rasmussen, j. 1983 skills, rules and knowledge: signals, signs and symbols, and other distinctionsin human performance models. ieee transactions on systems, man and cyberneticssmc133:257267.rasmussen, j. 1986 information processing and humanmachine interaction. new york: elsevierpress.reed, c.m., and n.i durlach 1994 note on information transfer rates in human communication.submitted to presence.reed, c.m., n.i. durlach, and l.d. braida 1982 research on tactile communication of speech: areview. asha monograph 20, american speechlanguagehearing association,rockville, md.reed, c.m., n.i. durlach, l.a. delhorne, w.m. rabinowitz, and k.w. grant 1989 research ontactual communication of speech: ideas, issues, and findings. the volta review (91):6578.schloerb, d.w. 1994 a quantitative measure of telepresence. presence, in press.schwartz, j. 1994 a terminal obsession. the washington post. march 27:f1.sheridan, t.b., and w.r. ferrell 1974 manmachine systems: information, control and decisionmodels of human performance. cambridge, mass.: mit press.references448virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.sheridan, t.b. 1976 toward a general model of supervisory control. in t.b. sheridan and g.johansen, eds., monitoring behavior and supervisory control. new york: plenum press.1992a defining our terms. presence 1(2):27274.1992b musings of telepresence and virtual presence. presence 1(1): 12025.1992c telerobotics, automation and human supervisory control. cambridge, mass.: mit press.sjoberg, l., and g. torell 1993 the level of risk acceptance and moral valuation. scandanavian journal of psychology 34(3):223236.stelmach, g.e. 1978 information processing in motor control and learning. new york, academicpress.steuer, j. 1992 defining virtual reality: dimensions determining telepresence. journal ofcommunication (42): 7393.tan, h.z., x.d. pang, and n.i. durlach 1992 manual resolution of length, force and compliance.pp. 1318 in h. kazerooni, ed., advances in robotics, asme dsc vol. 42.tan, h.z., n.i. durlach, y. shao, and m. wei 1993 manual resolution of compliance when workand force cues are minimized. pp. 99104 in h. kazerooni, j.e. colgate, and b.d.adelstein, eds., advances in in robotics, mechatronics and haptic interfaces, asme dscvol. 49.vasu, m.l., and e.s. vasu 1993 computer usage in a research methods course in the social sciencesand education: a conceptual framework. collegiate microcomputer 11(3):177182.warren, d.h., and e.r. strelow 1985 electronic spatial sensing for the blind. dordrecht,netherlands: martinusnijhoff.welch, r.b. 1978 perceptual modification: adapting to altered sensory environments. new york:academic press.zeltzer, d. 1992 autonomy, interaction, and presence. presence 1(1):12732.chapter 2: the visual channelaukstakalnis, s., and d. blatner 1993 silicon mirage: the art and science of virtual reality.berkeley, calif.: peachpit press.benton, s.a. 1982 survey of holographic stereograms. pp. 1519 in j.j. pearson, ed., proceedings ofspie 367: processing and display of threedimensional data.references449virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.1991 experiments in holographic video imaging. in proceedings of the spie institute onholography, bellingham, wash.boff, k.r., l. kaufman, and j.p. thomas, eds. 1986 handbook of perception and humanperformance. new york: john wiley.bridgeman, b. 1991 separate visual representations for perception and for visually guided behavior.in s.r. ellis, m.k. kaiser, and a.c. grunwald, eds., pictorial communication in virtualand real environments. london: taylor & francis.dallas, w.j. 1980 computergenerated holograms. in r. frieden, ed. the computer in opticalresearch. berlin, west germany: springerverlag.dizio, p., and j.r. lackner 1992 altered loading of the head exacerbates motion sickness andspatial disorientation. paper presented at the aerospace medical association meeting,miami, fla.earnshaw, r.a., et al., eds. 1993 virtual reality systems. london: academic press.graham, n.s. 1989 visual pattern analyzers. vol. 16 in the oxford psychology series. d.e.broadbent, s. kosslyn, j.l. mcgaugh, n.j. mackintosh, e. tulving, and l. weiskrantz,eds. new york: oxford university press.gregory, r.l. 1991 seeing by exploring. in s.r. ellis, m.k. kaiser, and a.c. grunwald, eds.,pictorial communication in virtual and real environments. london: taylor & francis.haber, r.n., and m. hershenson 1980 the psychology of visual perception, 2nd ed. new york:holt, rinehart & winston.hallett, p.e. 1986 eye movements. in k.r. boff, l. kaufman, and j.p. thomas, eds., handbook ofperception and human performance. new york: john wiley.held, r., and n. durlach 1991 telepresence, time delay and adaptation. in s.r. ellis, m.k. kaiser,and a.c. grunwald, eds., pictorial communication in virtual and real environments.london: taylor & francis.held, r., and s.j. freedman 1963 plasticity in human sensorimotor control. science 54:3337.holmgren, d.e., and w. robinett 1994 scanned laser displays for virtual reality: a feasibility study.presence 2(3):171.hood, d.c., and m.a. finkelstein 1986 sensitivity to light. in k.r. boff, l. kaufman, and j.p.thomas, eds., handbook of perception and human performance. new york: john wiley.kalawsky, r.s. 1993 the science of virtual reality and virtual environments. workingham,england: addisonwesley.references450virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.kobayashi, t., et al. 1984 polyaniline filmcoated electrodes as electrochromic display devices.journal of electroanalytical chemistry 161:419423.kohler, i. 1964 the formation and transformation of the perceptual world. (translated by h.fiss.) international universities press. originally über aufbau und wandlungen derwahrenehmungswelt. vienna, austria: r.m. rohere (1951).lalonde, j. 1990 warp speed into cyberspace. the seattle times. june 18.latour, p.l. 1962 visual threshold during eye movements. vision research 2:261262.macdowall, i.e., m. bolas, s. pieper, s.s. fisher, and j. humphries 1990 implementation andintegration of a counterbalanced crtbased stereoscopic display for interactive viewpointcontrol in virtual environment applications. in j. merritt, ed., proceedings of spie stereoscopic displays and applications, san jose, calif.matin, l. 1975 eye movements and perceived visual direction. pp. 176 in d. jameson and l.m.hurvich, eds., the handbook of sensory physiology, vol. 1. new york: academic press.matin, e., a. clymer, and l. matin 1972 metacontrast and saccadic suppression. science178:179182.mckenna, m., and d. zeltzer 1992 three dimensional visual display systems for virtualenvironments. presence 1(4):421458.merritt, j. 1991 innovations in optics. paper presented at the southeast c41 conference, tampa, fla.meyer, k., h.l. applewhite, and f.a. biocca 1992 a survey of position trackers. presence 1(2):173200.mittelstaedt, h. 1991 interactions of form and orientation. in s.r. ellis, m.k. kaiser, and a.c.grunwald, eds., pictorial communication in virtual and real environments. london:taylor & francis.miyashita, t., and t. uchida 1990 causes of fatigue and its improvement in stereoscopic displays.proceedings of the society for information display 31(3):249254.oyama, e., n. tsunemoto, t. susumu, and y. inoue 1993 experimental study on remotemanipulation using virtual reality. presence 2(2):112124.piantanida, t., d.k. boman, and j. gille 1993 human perceptual issues and virtual reality. virtualreality systems 1(1):4352.picart, b., et al. 1989 visualization of vlsi integrated circuits by means of ferroelectric liquidreferences451virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.crystals. liquid crystal chemistry, physics, and applications spie 180:131139.pirenne, m., and f. mariott 1962 visual functions in man. pp. 3217 in h. davson, ed., the eye. vol ii. the visual process. new york: academic press.roufs, a.j., and i.m.j. goossens 1988 the effect of gamma on perceived image quality. ieeeproceedings: idrc:2731.sekuler, r., and r. blake 1990 perception. new york: mcgrawhill.soltan, p., j. trias, w. robinson, and w. dahlke 1992 laser based 3d volumetric display system.in proceedings of spie, san jose, calif., february 914.spain, e.h. 1992 effects of camera aiming technique and display type on unmanned groundvehicle performance. final report to the department of defense unmanned groundvehicle joint program office, huntsville, alabama.spottiswoode, r. 1967 a grammar of the film: an analysis of film technique. berkeley and losangeles: university of california press.st. hilaire, p., s.a. benton, m. lucente, m.l. jepsen, j. kollin, h. yoshikawa, and j. underkoffler1990 electronic display system for computational holography. in spie proceedingspractical holography iv, vol. 1212, bellingham, wash.tricoles, g. 1987 computer generated holograms: an historical review. applied optics 26(20):43514360.westerink, j.h., and a.j. roufs 1989 subjective image quality as a function of viewing distance,resolution, and picture size. smpte journal (feb):113119.westheimer, g. 1986 the eye as an optical instrument. in k.r. boff, l. kaufman, and j.p. thomas,eds., handbook of perception and human performance. new york: john wiley.wiegand, t.e.v., d.c. hood, n.vs. graham 1994 testing a computational model of lightadaptation dynamics. submitted to vision research.chapter 3: the auditory channelallen, j.b., and d.a. berkley 1979 image method for efficiently simulating smallroom acoustics.journal of the acoustical society of america 65(4):943950.applied acoustics 1992 special issue on auditory virtual environments and telepresence. 36(34).references452virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.1993 special issue on computer modeling and auralization of sound fields in rooms. 38(24).blattner, m.m., d.a. sumikawa, and r.m. greenberg 1989 earcons and icons: their structure andcommon design principles. humancomputer interaction 4:1144.blauert, j. 1983 spatial hearing. cambridge, mass.: mit press.boff, k.r., and j. lincoln, eds. 1988 engineering data compendium: human perception andperformance. henry g. armstrong aerospace medical research laboratory, wrightpatterson air force base, ohio.boff, k.r., l. kaufman, and j.p. thomas, eds. 1986 handbook of perception and humanperformance, vols. 1 and 2. new york: john wiley.borin, g., g. depoli, and a. sarti 1992 algorithms and structures for synthesis using physicalmodels. computer music journal 16(4):3042.borish, j. 1984 extension of the image model to arbitrary polyhedra. journal of the acousticalsociety of america 75:18271836.bregman, a.s. 1990 auditory scene analysis. cambridge, mass.: mit press.buxton, w., w. gaver, and s. bly 1989 the use of nonspeech audio at the interface. (tutorial no.10). in chi'89, acm conference on human factors in computing systems. new york:acm press.cadoz, c., and c. ramstein 1991 capture, representation, and ''composition" of the instrumentalgame. icmc proceedings 5356, glasgow, scotland.cadoz, c., a. luciani, and j. florens 1984 responsive input devices and sound synthesis bysimulation of instrumental mechanisms: the cordis systems. computer music journal 8(3):6073.1993 cordisanimaa modeling and simulation system for sound and image synthesisthegeneral formalism. computer music journal 117(1):1929.cadoz, c., l. lisowski, and j. florens 1990 a modular feedback keyboard design. computer musicjournal 14(2):4751.carterette, e.c., and m.p. friedman 1978 handbook of perception, vol. 4, hearing. new york:academic press.chandler, d.w., and d.w. grantham 1992 minimum audible movement angle in the horizontalplane as a function of stimulus frequency and bandwidth, source azimuth, and velocity.journal of the acoustical society of america 91:16241636.references453virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.chowning, j. 1973 the synthesis of complex audio spectra by means of frequency modulation.journal of the audiological engineering society 21(7):526534.chowning, j., and d. bristow 1986 fm theory and applications by musicians for musicians.yamaha music foundation, tokyo, japan.cohen, m., and l.f. ludwig 1991 multidimensional audio window management. internationaljournal of manmachine studies 34:319336.colburn, h.s., and n.i. durlach 1978 models of binaural interaction. pp. 467518 in e.c. carteretteand m.p. friedman, eds., handbook of perception: vol. 4, hearing. new york: academicpress.colburn, h.s., p.m. zurek, and n.i. durlach 1987 binaural directional hearingimpairments andaids. pp. 261278 in w.a. yost and g. gourevitch, eds., directional hearing. new york:springerverlag.cook, p.r. 1993 spasm, a realtime vocal tract physical model controllerand singer, thecompanion software synthesis system. computer music journal 17(1):3044.cook, p.r., s. hirschman, and j.o. smith 1991 physical modeling. presentation at the internationalcomputer music conference, ccrma.cooper, d.h., and j.l. bauck 1989 prospects for transaural recording. journal of the audioengineering society 37:319.cruzneira, c., d.j. sandin, and t.a. de fanti 1993 surround screen projectionbased virtualreality: the design and implementation of the cave. pp. 135142 in computer graphics.proceedings of siggraph.dannenbring, g.l. 1976 perceived auditory continuity with alternately rising and falling frequencytransitions. canadian journal of psychology 30:99114.djoharian, p. 1993 generating models for modal synthesis. computer music journal 17(1):5765.dolson, m. 1986 the phase vocoder: a tutorial. computer music journal 10:1427.durlach, n.i. 1991 auditory localization in teleoperator and virtual environment systems: ideas,issues, and problems. perception 20:543554.durlach, n.i., and h.c. colburn 1978 binaural phenomena. pp. 365466 in e.c. carterette and m.p.friedman, eds., handbook of perception, vol. 4: hearing. new york: academic press.references454virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.durlach, n.i., and x.d. pang 1986 interaural magnification. journal of the acoustical society ofamerica 80:18491850.durlach, n.i., r. held, and b. shinncunningham 1992a super auditory localization displays. inproceedings of the society for information displays, boston, mass.durlach, n.i., r.w. pew, w.a. aviles, p.a. dizio, and d.l. zeltzer 1992b virtual environmenttechnology for training (vett). report no. 7661. cambridge, mass.: bolt, beranek, andnewman.durlach, n.i., a. rigopulos, x.d. pang, w.s. woods, a. kulkarni, h.s. colburn, and e.m. wenzel1992c on the externalization of auditory images. presence 1(2):251257.durlach, n.i., b.g. shinncunningham, and r.m. held 1993 supernormal auditory localization: i.general background. presence 2(2):1103.eargle, j. 1986 stereophonic techniques: an anthology of reprinted articles on sterophonictechniques. audio engineering society.fay, r.r., and a.n. popper, eds. 1994 handbook of auditory research. new york: springerverlag.foster, s.h., e.m. wenzel, and r.m. taylor 1991 realtime synthesis of complex acousticenvironments. in proceedings of the ieee workshop on applications of signal processingto audio and acoustics, new paltz, n.y.gaver, w.w. 1993 synthesizing auditory icons. pp. 228235 in proceedings of nterchi'93, conference on human factors in computing systems, amsterdam, netherlands.gaver, w.w., r.b. smith, and t. o'shea 1991 effective sounds in complex systems: the arkolasimulation. pp. 8590 in proceedings of chi'91, acm conference on computerhuman interaction.gierlich, h.w., and k. genuit 1989 processing artificialhead recordings. journal of the audio engineering society 37:3439.gilkey, r., and t. anderson, eds. 1994 binaural and spatial hearing. hillsdale, n.j.: lawrenceerlbaum associates.grantham, d.w. 1986 detection and discrimination of simulated motion of auditory targets in thehorizontal plane. journal of the acoustical society of america 79:19391949.1989 motion aftereffects with horizontally moving sound sources in the free field. perception andpsychophysics 45:129136.1992 adaptation to auditory motion in the horizontal plane: effect of prior exposure to motion onmotion detectability. perception and psychophysics 52:144150.references455virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.grey, j.m., and j.a. moorer 1977 perceptual evaluations of synthesized musical instrument tones.journal of acoustical society of america 63:454462.griesinger, d. 1989 theory and design of a digital audio signal processor for home use. journal ofthe audio engineering society 37:4050.handel, s. 1989 listening: an introduction to the perception of auditory events. cambridge, mass.:mit press.hartman, w.m. 1988 pitch perception and the segregation and integration of auditory entities. in g.edelman, w. gall, and w. cowan, eds., auditory function. new york: john wiley.howard, j.h. 1983 perception of simulated propeller cavitation. human factors 25:643656.jaffe, d., and j. smith 1983 extensions of the karplusstrong pluckedstring algorithm. computer music journal 7(2):4355.kay, l. 1974 a sonar aid to enhance spatial perception of the blind: engineering design andevaluation. radio and electronics engineer 44:4062.keefe, d.h. 1992 physical modeling using digital waveguides. computer music journal 16(4):5773.kendall, g.s., and w.l. martens 1984 simulating the cues of spatial hearing in naturalenvironments. in proceedings of the 1984 international computer music conference,paris, france.killion, m.c. 1981 earmold options for wideband hearing aids. journal of speech and hearingdisorders 46:1020.killion, m.c., and t.w. tillman 1982 evaluation of highfidelity hearing aids. journal of speech and hearing research 25:1525.kramer, g., ed. 1994 auditory display: sonification, audification, and auditory interfaces. santafe institute studies in the sciences of complexity, proceedings volume 18. reading,mass.: addisonwesley.krokstadt, a., s. strom, and s. sorsdal 1968 calculating the acoustical room response by the use ofa ray tracing technique. journal of sound and vibrations 8:118.lehnert, h. 1993a systematic errors of the raytracing algorithm. applied acoustics 38:207221.1993b auditory spatial impression. pp. 4046 in proceedings of the 12th international aesconference, copenhagen, denmark.references456virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.lehnert, h., and j. blauert 1989 a concept for binaural room simulation [summary]. inproceedings of the ieee assp workshop on applications of signal processing to audioand acoustics, new paltz, n.y.1991 virtual auditory environment. proceedings of the 5th international conference on advancedrobotics ieeeicar 1:211216.1992 principles of binaural room simulation. applied acoustics 36:325333.li, x., r.j. logan, and r.e. pastore 1991 perception of acoustic source characteristics. journal ofthe acoustical society of america 90:30363049.liberman, a.m., f.s. cooper, d.p. shankweiler, and m. studdertkennedy 1968 why are speechspectrograms hard to read? american annals of the deaf 113:127133.loomis, j.m., c. hebert, and j.g. cicinelli 1990 active localization of virtual sounds. journal ofthe acoustical society of america 88:17571764.ludwig, l.f., n. pincever, and m. cohen 1990 extending the notion of a window system to audio.computer 23:6672.maccabe, c., and d.j. furlong 1994 virtual imaging capabilities of surround sound systems.journal of the audio engineering society 42:3849.massimino, m.j., and t.b. sheridan 1993 sensory substitution for force feedback in teleoperation.presence 2(4):344352.mathews, m.v., and j.r. pierce 1989 current directions in computer music research. cambridge,mass.: mit press.mckinley, r.l., and m.a. ericson 1988 digital synthesis of binaural auditory localization azimuthcues using headphones. journal of the acoustical society of america 83:s18.mills, a.w. 1958 on the minimum audible angle. journal of the acoustical society of america30:237246.1972 auditory localization. pp. 301345 in j.v. tobias, ed., foundations of modern auditorytheory, vol. ii. new york: academic press.moore, b.c.j. 1989 an introduction to the psychology of hearing, 3rd edition. new york:academic press.moore, f.r. 1990 elements of computer music. englewood cliffs, n.j.: prenticehall.moorer, j.a. 1978 the use of the phase vocoder in computer music applications. journal ofaudiological engineering society 24:717727.morrison, j.d., and j.m. adrien 1993 mosaic  a framework for modal synthesis. computermusic journal 17(1):4556.references457virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.morse, p.m., and k.u. ingard 1968 theoretical acoustics. new york: mcgrawhill.patterson, r.r. 1982 guidelines for auditory warning systems on civil aircraft. paper no. 82017 .london: civil aviation authority.perrott, d.r. 1982 studies in the perception of auditory motion. pp. 169193 in r.w. gatehouse,ed., localization of sound: theory and applications. groton, conn.: amphora press.perrott, d.r., and j. tucker 1988 minimum audible movement angle as a function of signalfrequency and the velocity of the source. journal of the acoustical society of america83:15221527.perrott, d.r., k. marlborough, p. merrill, and t.z. strybel 1989 minimum audible angle thresholdsobtained under conditions in which the precedence effect is assumed to operate. journal ofthe acoustical society of america 85:282287.pickles, j.d. 1988 an introduction to the physiology of hearing, 2nd ed. new york: springerverlag.portnoff, m.r. 1980 shorttime fourier analysis of sampled speech. ieee transactions onacoustics, speech and signal processing 29:5569.richards, w. 1988 natural computation. cambridge, mass.: mit press.risset, j.c., and m.v. mathews 1969 analysis of musical instrument tones. physics today 22:2340.roads, c., and j. strawn 1988 foundations of computer music. cambridge, mass.: mit press.scaletti, c. 1994 sound synthesis algorithms for auditory data representations. in g. kramer, ed.,auditory display: the proceedings of icad'92, the first international conference onauditory display. santa fe institute studies in the sciences of complexity, volume 18,reading, mass.: addisonwesley.scaletti, c., and a.b. craig 1991 using sound to extract meaning from complex data. pp. 207219in proceedings of the spie (1459), san jose, calif.schottstaedt, b. 1977 the simulation of natural instrument tones using frequency modulation with acomplex modulating wave. computer music journal 1:4650.schroeder, m.r. 1962 natural sounding artificial reverberation. journal of the audio engineeringsociety 10(3).searle, c.l., l.d. braida, d.r. cuddy, and m.f. davis 1976 binaural pinna disparity: anotherauditory localization cue. journal of the acoustical society of america 57:448455.references458virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.shaw, e.a.g. 1966 earcanal pressure generated by circumaural and supraaural earphones. journalof the acoustical society of america 39:471479.shinncunningham, b.g. 1994 adaptation to supernormal auditory localization cues in anauditory virtual environment. unpublished doctoral dissertation, department ofelectrical engineering, massachusetts institute of technology.shinncunningham, b.g., h. lehnert, g. kramer, e. wenzel, and n.i. durlach 1994 auditorydisplays. in r. gilkey and t. anderson, eds., binaural and spatial hearing. hillsdale,n.j.: lawrence erlbaum.smith, j.o. 1992 physical modeling using digital waveguides. computer music journal 16(4):7498.smith, s., r.d. bergeron, and g.g. grinstein 1990 stereophonic and surface sound generation forexploratory data analysis. pp. 125132 in proceedings of chi'90, acm conference on human factors in computing systems, seattle, wash.stelmachowicz, p.g., k.a. beauchaine, a. kalberer, t. langer, and w. jesteadt 1988 thereliability of auditory thresholds in the 8 to 20 khz range using a prototype audiometer.journal of the acoustical society of america 83:15281535.strelow, e.r., and d.h. warren 1985 sensory substitution in blind children and neonates. pp.273298 in d.h. warren and e.r. strelow, eds., electronic spatial sensing for the blind.dordrecht, netherlands: martinusnijhoff.trahiotis, c., and l.r. bernstein 1987 some modern techniques and devices used to preserve andenhance the spatial qualities of sounds. pp. 279290 in w.a. yost and g. gourevitch, eds.,directional hearing. new york: springerverlag.van veen, b.d., and r.l. jenison 1991 auditory space expansion via linear filtering. journal of the acoustical society of america 90:231240.vian, j., and j. martin 1992 binaural room acoustics simulation: practical uses and applications.applied acoustics 36:293306.warren, d.h., and e.r. strelow 1984 learning spatial dimensions with a visual sensory aid:molyneaux revisited. perception 13:331350.1985 training the use of artificial spatial displays. in d.h. warren and e.r. strelow, eds.,electronic spatial sensing for the blind. dordrecht, netherlands: martinusnijhoff.warren, h., and r.r. verbrugge 1984 auditory perception of breaking and bouncing events: a casestudy in ecological acoustics . journal of experimental psychology: human perceptionand performance 10:704712.wawrzynek, j. 1991 vlsi models for sound synthesis. pp. 113148 in g.d. poli, a. piccialli,references459virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.and c. roads, eds., representations of musical signals. cambridge, mass.: mit press.welch, r.b. 1978 perceptual modification: adapting to altered sensory environments. new york:academic press.wenzel, e.m. 1992 localization in virtual acoustic displays. presence 1:80107.1994 spatial sound and sonification. in g. kramer, ed., auditory display: sonification,audification, and auditory interfaces. santa fe institute studies in the sciences ofcomplexity, proceedings vol. 18, reading, mass.: addisonwesley.wenzel, e.m., and s.h. foster 1993 perceptual consequences of interpolating headrelated transferfunctions during spatial synthesis. proceedings of the assp (ieee) workshop onapplications of signal processing to audio & acoustics, new paltz, n.y., oct. 1720.wenzel, e.m., f.l. wightman, and s.h. foster 1988 a virtual display system for conveying threedimensional acoustic information. pp. 8690 in proceedings of the human factors society,anaheim, calif.wenzel, e.m., p.k. stone, s.s. fisher, and s.h. foster 1990 a system for threedimensionalacoustic "visualization" in a virtual environment workstation. pp. 329337 in proceedingsof the ieee visualization '90 conference, san francisco, calif.wenzel, e.m., m. arruda, d.j. kistler, and f.l. wightman 1993a localization using nonindividualized headrelated transfer functions. journal of the acoustical society ofamerica 94:111123.wenzel, e.m., w. gaver, s.h. foster, h. levkowitz, and r. powell 1993b perceptual vs. hardwareperformance in advanced acoustic interface design. pp. 363366 in proceedings ofinterchi'93, conference on human factors in computing systems, amsterdam,netherlands.wightman, f.l., and d.j. kistler 1989a headphone simulation of freefield listening i: stimulussynthesis. journal of the acoustical society of america 85:858867.1989b headphone simulation of freefield listening ii: psychophysical validation. journal of theacoustical society of america 85:868878.wightman, f.l., d.j. kistler, and m. arruda 1992 perceptual consequences of engineeringcompromises in synthesis of virtual auditory objects [abstract] . journal of the acoustical society of america 92:2332.woodhouse, j. 1992 physical modeling of bowed strings. computer music journal 16(4):4356.yost, w.a. 1991 auditory image perception and analysis: the basis for hearing. hearing research56:818.1994 fundamentals of hearing, 3rd ed. new york: academic press.references460virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.yost, w.a., and g. gourevitch, eds. 1987 directional hearing. new york: springerverlag.chapter 4: haptic interfacesadelstein, b.d., and m.j. rosen 1992 design and implementation of a force reflectingmanipulandum for manual control research. pp. 112 in m. kazerooni, ed., asme 1992:advances in robotics dsc vol. 42.bachyrita, p. 1982 sensory substitution in rehabilitation. pp. 361383 in l. illis, m. sedgwick,and h. granville, eds., rehabilitation of the neurological patient. oxford, england:blackwell scientific.bliss, j.c., and j.w. hill 1971 tactile perception studies related to teleoperator systems. stanfordresearch institute, menlo park, calif., final report, 20, nasacr114346, june.bolanowski, s.j., jr., g.a. gescheider, r.t. verrillo, and c.m. checkosky 1988 four channelsmediate the mechanical aspects of touch. journal of the acoustical society of america 84(5):16801694.brooks, f.p., m. ouhyoung, and j. batter 1990 project grope: haptic displays for scientificvisualization. computer graphics 24(4):177185.brooks, t.l. 1990 telerobotic response requirements. pp. 113120 in proceedings of the ieeeinternational conference on systems, man and cybernetics, los angeles; also report no.stx/rob/9003, stx corporation, lanham, md.brooks, t.l., and a.k. bejczy 1985 hand controllers for teleoperation. technical report jplpublication 8511, jet propulsion laboratory, pasadena, calif., march 1, 1985.burdea, g., j. zhuang, e. roskos, d. silver, and n. langrana 1992 a portable dextrous master withforce feedback. presence 1(1):1828.burgess, p.r., and f.j. clark 1969 characteristics of knee joint receptors in the cat. journal ofphysiology (london) 203:317335.clark, f.j., and k.w. horch 1986 kinesthesia. chap. 13 in k.r. boff, l. kauffman, and j.p.thomas, eds., handbook of perception and human performance, vol. 1: sensory processes and perception. new york: john wiley.cybernet systems corporation 1992 perforce: programmable environment reality throughforce. sales literature, cybernet systems corporation.dariansmith, i. 1984 the sense of touch: performance and peripheral neural processes. pp.739788 in handbook of physiologythe nervous systemiii. england: oxforduniversity press.references461virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.dariansmith, i., i. davidson, and k.o. johnson 1980 peripheral neural representation of spatialdimensions of a textured surface moving across the monkey's fingerpad. journal of physiology 309:135146.durlach, n.i., l.a. delhorne, a. wong, w.y. ko, w.m. rabinowitz, and j. hollerbach 1989manual discrimination and identification of length by the fingerspan method. perceptionand psychophysics 46(1):2938.durlach, n.i., r.w. pew, w.a. aviles, p.a. dizio, and d.l. zeltzer 1992 virtual environmenttechnology for training. virtual environment and teleoperator research consortium(vetrec), massachusetts institute of technology, bbn report 7661, march 1992.edin, b.b. 1993 the capacity of hairy skin mechanoreceptors to provide information about jointconfigurations in humans. 32nd congress of the international union of physiologicalsciences, glasgow, aug. 16.fasse, e.d. 1992 on the use and representation of sensory information of the arm by robots andhumans. unpublished dissertation, department of mechanical engineering, massachusettsinstitute of technology.fasse, e.d., b.a. kay, and n. hogan 1990 human haptic illusions in virtual object manipulation.proceedings of annual conference of ieee engineering in medicine and biology society12(5):19171918.ferrel, w.r. 1980 the adequacy of stretch receptors in the cat knee joint for signalling joint anglethroughout a full range of movement. journal of physiology (london) 299:8599.ferrel, w.r., gandevia, s.c. and d.i. mccloskey 1987 the role of joint receptors in humankinaesthesia when intramuscular receptors cannot contribute. journal of physiology386:6371.friskengibson, s.f., p. bachyrita, w.j. tompkins, and j.g. webster 1987 a 64solenoid, fourlevel fingertip search display for the blind. ieee transactions on biomedical engineeringbme34(12):963965.goertz, r. 1964 some work on manipulator systems at anl: past, present and a look at the future.in seminars on remotely operated special equipment, vol. 1.goodwin, g.m., d.i. mccloskey, and p.b.c. matthews 1972 the contribution of muscle afferentsto kinaesthesia shown by vibration induced illusions of movement and by the effects ofparalysing the joint afferents. brain 95:705748.grigg, p., and b.j. greenspan 1977 response of primate joint afferent neurons to mechanicalstimulation of knee joint. journal of neurophysiology 40:18.grigg, p., g.a. finerman, and l.h. riley 1973 jointposition sense after total hip replacement.journal of bone and joint surgery 55a:10161025.references462virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.hill, j.w. 1979 study of modeling and evaluation of remote manipulation tasks with forcefeedback. final report, jpl contract 955170, march.hogan, n., b.a. kay, e.d. fasse, and f.a. mussaivaldi 1990 haptic illusions: experiments onhuman manipulation and perception of 'virtual objects.' cold spring harbor symposia onquantitative biology 55:925931.honeywell inc. 1989 hand controller commonality study. technical report, honeywell inc.,avionics division, clearwater, fla., february 1989. prepared for mcdonnell douglasspace systems, co., huntington beach, calif.howe, r.d. 1992 a forcereflecting teleoperated hand system for the study of tactile sensing inprecision manipulation. pp. 13211326 in ieee international conference on robotics andautomation, nice, france.hunter, i.w., s. lafontaine, p.m.f. nielsen, p.j. hunter, and j.m. hollerbach 1990 manipulationand dynamic mechanical testing of microscopic objects using a telemicrorobot system.ieee control systems magazine 10(2):39.hunter, i., t.d. doukoglou, s.r. lafontaine, p.g. charrette, l.a. jones, m.a. sagar, g.d.mallinson, and p.j. hunter 1994 a teleoperated microsurgical robot and associated virtualenvironment for eye surgery. presence 2(4):265280.iwata, h. 1990 artificial reality with forcefeedback: development of desktop virtual space withcompact master manipulator. computer graphics 24(4):165170.iwata, h., t. nakagawa, and t. nakashima 1992 force display for presentation of rigidity of virtualobjects. journal of robotics and mechatronics 4(1):3942.jacobsen, s.c., e.k. iversen, c.c. davis, d.m. potter, and t.w. mclain 1989 design of a multipledegree of freedom, force reflective hand master/slave with a high mobility wrist.paper presented at the third topical meeting on robotics and remote systems, march1316, charleston, south carolina.jacobus, h.n., a.j. riggs, c.j. jacobus, and y. weinstein 1992 implementation issues fortelerobotic handcontrollers: humanrobot ergonomics. pp. 284314 in m. rahimi and w.karwowski, eds., humanrobot interaction. new york: taylor & francis.jau, b.m. 1992 manequivalent telepresence through four fingered humanlike hand system. pp.843848 in ieee international conference on robotics and automation, nice, france.johansson, r.s., and k.j. cole 1992 sensorymotor coordination during grasping and manipulativeactions. current opinion in neurobiology 2:815823.johansson, r.s., and a.b. vallbo 1979 tactile sensibility in the human hand: relative and absolutedensitiesreferences463virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.of four types of mechanoreceptive units in glabrous skin. journal of physiology286:283300.johansson, r.s., and g. westling 1984 roles of glabrous skin receptors and sensorimotor memoryin automatic control of precision grip when lifting rougher or more slippery objects.experimental brain research 56:550564.johnson, k.o., and s.s. hsiao 1992 neural mechanisms of tactual form and texture perception.annual review of neuroscience 15:227250.johnson, k.o., and j.r. phillips 1981 tactile spatial resolution  i. two point discrimination, gapdetection, grating resolution and letter recognition. journal of neurophysiology 46(6):11771191.jones, l.a. 1986 perception of force and weight: theory and research. psychological bulletin 100(1):2942.1989 matching forces: constant errors and differential thresholds. perception 18(5):681687.jones, l.a., and i.w. hunter 1992 human operator perception of mechanical variables and theireffects on tracking performance. pp. 4953 in h. kazerooni, ed., advances in robotics,asme dsc vol. 42.kaczmarek, k.a., and p. bachyrita 1993 tactile displays. in w. barfield and t. furness iii, eds.,virtual environments and advanced interface design. oxford, england: oxforduniversity press.knibestol, m., and a.b. vallbo 1970 single unit analysis of mechanoreceptor activity from thehuman glabrous skin. acta physiological scandinavia 80(6):178195.lamb, g.d. 1983a tactile discrimination of textured surfaces: psychophysical performancemeasurements in humans. journal of physiology 338:551565.1983b tactile discrimination of textured surfaces: peripheral neural coding in the monkey. journalof physiology 338:567587(a).lamotte, r.h., and m.a. srinivasan 1991 surface microgeometry: tactile perception and neuralencoding. in o. franzen and j. westman, eds., information processing in the somatosensory system. wennergren international symposium series. london:macmillan press.1993 responses of cutaneous mechanoreceptors to the shape of objects applied to the primatefingerpad. acta psychological 84:4151.lamotte, r.h., and j. whitehouse 1986 tactile detection of a dot on a smooth surface. journal ofneurophysiology 56:11091128.lederman, s.j., and m.m. taylor 1972 fingertip force, surface geometry, and the perception ofroughness by active touch. perception psychophysics 12:401408.references464virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.loomis, j.m. 1979 an investigation of tactile hyperacuity. sensory processes 3:289302.loomis, j.m., and s.j. lederman 1986 tactual perception. chap. 31 in k.r. boff, l. kauffman, andj.p. thomas, eds., handbook of perception and human performance: vol. i  sensoryprocesses and perception. new york: john wiley.massie, t.h., and j.k. salisbury 1994 probing virtual objects with the phantom hapticinterface. paper submitted to the asme wam, nov. 611, 1994, chicago, illinois.matthews, p.b.c. 1982 where does sherrington's ''muscular sense" originate? muscles, joints orcorollary discharges? annual reviews of neuroscience 5:189218.1988 proprioceptors and their contribution to somatosensory mapping: complex messages requirecomplex processing. canadian journal of physiological pharmacology 66:430438.mcaffee, d.a., and p. fiorini 1991 hand controller design requirements and performance issuesin telerobotics . proceedings of international conference on advanced robotics (icar),pisa, june.meyer, k., h.l. applewhite, and f.a. biocca 1992 a survey of position trackers. presence 1(2):173200.minsky, m., m. ouhyoung, o. steele, f.p. brooks, jr., and m. behensky 1990 feeling and seeing:issues in force display. computer graphics 24(2):235243.monkman, g.j. 1992 an electrorheological tactile display. presence 1(2):219228.morley, j.w., a.w. goodwin, and i. dariansmith 1983 tactile discrimination of gratings.experimental brain research 49:291299.mountcastle, v.b., and t.p.s. powell 1959 central nervous mechanisms subserving position senseand kinesthesis. bulletin of the johns hopkins hospital 105:173200.mountcastle, v.b., r.h. lamotte, and g. carli 1972 detection threshold for stimuli in humans andmonkeys: comparison with threshold events in mechanoreceptive afferent nerve fibersinnervating monkey hand. journal of neurophysiology 35:122136.noma, h., and h. iwata 1993 presentation of multiple dimensional data by 6.d.o.f force display.pp. 14951500 in ieee/rsj international conference on intelligent robots and systems,yokohama, japan.pang, x.d., h.z. tan, and n.i. durlach 1991 manual discrimination of force using active fingermotion. perception and psychophysics 49(6):531540.phillips, j.r., and k.o. johnson 1981a tactile spatial resolution  ii. neural represntation of bars,edges and gratings in monkey primary afferents. journal of neurophysiology 46(6):11921203.1981b tactile spatial resolution  iii. a continuum mechanics model of skinreferences465virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.predicting mechanoreceptor responses to bars, edges, and gratings. journal ofneurophysiology 46(6):12041225.phillips, j.r., k.o. johnson, and h.m. browne 1983 a comparison of visual and two modes oftactual letter recognition. perception and psychophysics 34:243249.phillips, j.r., k.o. johnson, and s.s. hsiao 1988 spatial pattern representation and transformationin monkey somatosensory cortex. proceedings of the national academy of sciences (usa)85(6):13171321.poulton, e.c. 1974 tracking skill and manual control. new york: academic press.pubols, b.h.p., jr. 1980 onversusoff responses of raccoon glabrous skin rapidly adaptingcutaneous mechanoreceptors. journal of neurophysiology 43:15581570.pubols, b.h.p., jr., and l.m. pubols 1976 coding of mechanical stimulus velocity and indentationdepth by squirrel monkey and raccoon skin mechanoreceptors. journal of neurophysiology39:773787.1983 tactile receptor discharge and mechanical properties of glabrous skin. fed. proc.42:25282535.rabinowitz, w.m., a.j.m. houtsma, n.i. durlach, and l.a. delhorne 1987 multidimensionaltactile displays: identification of vibratory intensity, frequency, and contactor area. journalof the acoustical society of america 82(4):12431252.reed, c.m., n.i. durlach, and l.d. braida 1982 research on tactile communication: a review.asha monographs 20.salcudean, s.e., n.m. wong, and r.l. hollis 1992 a forcereflecting teleoperation system withmagnetically levitated master and wrist. pp. 14201426 in proceedings of ieeeinternational conference on robotics and automation, nice, france.sathian, k., a.w. goodwin, k.t. john, and i. dariansmith 1989 perceived roughness of a grating:correlation with responses of mechanoreceptive afferents innervating the monkey'sfingerpad. journal of neuroscience 9:12731279.schmult, b., and r. jebens 1993 application areas for a forcefeedback joystick. pp. 4754 in h.kazerooni, j.e. colgate, and b.d. adelstein, eds., advances in robotics, mechatronicsand haptic interfaces, asme dsc vol. 49.schneider, w. 1988 the tactile array stimulator. johns hopkins api technical digest 9(1):3943.sheridan, t.b. 1992 telerobotics, automation, and supervisory control. cambridge, mass.: mitpress.sherrick, c.e., and r.w. cholewiak 1986 cutaneous sensitivity. ch. 12 in k.r. boff, l. kauffman,and j.p. thomas, eds., handbook of perception and human performance, vol. 1: sensoryprocesses and perception. new york: john wiley.references466virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.shimoga, k.b. 1992 finger force and touch feedback issues in dexterous telemanipulation. inproceedings of nasacirsse international conference on intelligent robotic systems forspace exploration, troy, n.y.skoglund, s. 1956 anatomical and physiological studies of knee joint innervation in the cat. actaphysiological scandinavica 36(suppl 124):1101.srinivasan, m.a. 1989 surface deflection of primate fingertip under line load. journal ofbiomechanics 22(4):343349.srinivasan, m.a., and j.s. chen 1993 human performance in controlling normal forces of contactwith rigid objects. pp. 119125 in h. kazerooni, j.e. colgate, and b.d. adelstein, eds.,advances in robotics, mechatronics and haptic interfaces, asme dsc vol. 49.srinivasan, m.a., and k. dandekar 1992 role of fingertip geometry in transmission of tactilemechanical signals. advances in bioengineering 22:569572.srinivasan, m.a., and r.h. lamotte 1991 encoding of shape in the responses of cutaneousmechanoreceptors. pp. 13231332 in o. franzen and j. westman, eds., informationprocessing in the somatosensory system. wennergren international symposium series.london: macmillan press.1994 tactual discrimination of softness. manuscript submitted to journal of neurophysiology.srinivasan, m.a., j.m. whitehouse, and r.h. lamotte 1990 tactile detection of slip: surfacemicrogeometry and peripheral neural codes . journal of neuorophysiology 63(6):13231332.sutherland, i.e. 1965 the ultimate display. proceedings of the ifip congress 2:506508.talbot, w.h., i. dariansmith, h.h. kornhuber, and v.b. mountcastle 1968 the sense of fluttervibration: comparison of the human capacity with response patterns of mechanoreceptiveafferents from the monkey hand. journal of neurophysiology 31:301334.tan, h.z., x.d. pang, and n.i. durlach 1992 manual resolution of length, force and compliance.pp. 1318 in h. kazerooni, ed., advances in robotics, asme dsc vol. 42.tan, h.z., n.i. durlach, y. shao, and m. wei 1993 manual resolution of compliance when workand force cues are minimized. pp. 99104 in h. kazerooni, j.e. colgate, and b.d.adelstein, eds., advances in in robotics, mechatronics and haptic interfaces, asme dscvol. 49.tan, h.z., m.a. srinivasan, b.s. eberman, and b. cheng 1994 human factors for the design offorcereflecting haptic interfaces. proceedings of asme winter annual meeting (in press).tini alloy company 1990 sales literature. oakland, calif.references467virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.trimmer, w.s., k.j. gabriel, and r. mahadevan 1987 silicon electrostatic motors. pp. 857860 inproceedings of transducers '87, tokyo, japan.valéry, p. 1938 discours d'ouverture au congrès de chirurgie, paris, n.r.f. cited on p. 82 in r.tubiana, ed., the hand, vol. 1. philadelphia, pa., w.b. saunders co., 1981.vallbo, a.b., and k.e. hagbarth 1968 activity from skin mechanoreceptors recordedpercutaneously in awake human subjects. experimental neurology 21:270289.chapter 5: position tracking and mappingagronin, m.l. 1987 the design of a ninestring sixdegreeoffreedom forcefeedback joystick fortelemanipulation. pp. 341348 in proceedings of the nasa workshop on spacetelerobotics, pasadena, calif.andersson, r.l. 1993 a real experiment in virtual environments: a virtual batting cage. presence:teleoperators and virtual environments 2:1633.applewhite, h. 1994 a new vr positioning method yielding pseudoabsolute location. vrst '94:the acm symposium on virtual reality systems and technology.atkeson, c.g., and j.m. hollerbach 1985 kinematic features of unrestrained vertical armmovements. journal of neuroscience 5:23182330.atkinson, w.d., k.e. bond, g.l. tribble, and k.r. wilson 1977 computing with feeling.computers and graphics 2:97103.badler, n.i., m.j. hollick, and j.p. granieri 1993 realtime control of a virtual human usingminimal sensors. presence: teleoperators and virtual environments 2:8286.bahill, a.t., a. brockenbrough, and b.t. troost 1981 variability and development of a normativedata base for saccadic eye movements. investigative ophthalmology and visual science21:116125.beraldin, j.a., m. rioux, f. blais, l. cournoyer, and j. domey 1992 registered intensity andrange imaging at 10 megasamples per second. optical engineering 31:8894.beraldin, j.a., s.f. elhakim, and l. cournoyer 1993 practical range camera calibration.videometrics ii. spie vol. 2067.besl, p.j. 1988 active, optical range imaging sensors. machine vision and applications 1:127152.blais, f., m. rioux, and j. domey 1991 optical range image acquisition for the navigation of amobile robot. pp. 25742580 in proceedings of the ieee international conference robotics and automation, sacramento, calif.references468virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.brooks, t.l. 1990a telerobot response requirements. report no. stx/rob/9003. lanham, md.:stx robotics.1990b telerobot response requirements. pp. 113120 in proceedings of the ieee conference onsystems, man, and cybernetics, los angeles, calif.canepa, g., j.m. hollerbach, and a.j.m. boelen 1994 kinematic calibration by means of a triaxialaccelerometer. proceedings of the ieee international conference on robotics and automation, san diego, calif.chen, y.d., j. ni, and s.m. wu 1993 dynamic calibration and compensation of a 3d laser radarscanning system. ieee international conference robotics and automation 3:652658.durlach, n.i., r.w. pew, w.a. aviles, p.a. dizio, and d.l. zeltzer 1992 virtual environmenttechnology for training. bbn report no. 7661. cambridge, mass.: bbn systems andtechnologies.fischer, p., r. daniel, and k. siva 1990 specification and design of input devices for teleoperation.pp. 540545 in proceedings of the ieee international conference robotics andautomation, cincinnati, ohio.fohanno, t. 1982 assessment of the mechanical performance of industrial robots. pp. 349358 inproceedings of the 12th international symposium on industrial robots, paris, france.foxlin, e. 1993 inertial headtracking. electrical engineering and computer science,massachusetts institute of technology.foxlin, e., and n. durlach 1994 an inertial headorientation tracker with automatic driftcompensation for use with hmds. paper submitted to vrst '94: the acm symposium onvirtual reality systems and technology.franklin, g.f., j.p. powell, and m.l. workman 1990 digital control of dynamic systems. reading,mass.: addisonwesley.friedmann, m., t. starner, and a. pentland 1992 synchronization in virtual realities. presence:teleoperators and virtual environments 1:139144.hebert, m., and t. kanade 1989 3d vision for outdoor navigation by an autonomous vehicle. pp.208226 in m. brady, ed., robotics science. cambridge, mass.: mit press.held, r., and n. durlach 1987 telepresence, time delay, and adaptation. nasa conferencepublication 10023.hollerbach, j.m., l. giugovaz, m. buehler, and y. xu 1993 screw axis measurement for kinematiccalibration of the sarcos dextrous arm. pp. 16171621 in proceedings of the ieee/rsjinternational conference on intelligent robots and systems, yokohama, japan.references469virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.inchingolo, p., and m. spanio 1985 on the identification and analysis of saccadic eye movementsa quantitative study of the processing procedures. ieee transactions on biomedicalengineering bme32:683695.inoue, h. 1993 vision based behavior: observation and control of robot behavior by realtimetracking vision. in preprints of the 6th international symposium robotics research,hidden valley, pa.iwata, h. 1991 force display for virtual worlds. pp. 111116 in proceedings of the internationalconference artificial reality and teleexistence.1992 force displays for walkthrough simulation. pp. 481486 in proceedings of the secondinternational symposium on measurement and control in robotics, tsukuba science city,japan.jiang, b.c., j.y. black, and r. duraisamy 1988 a review of recent developments in robotmetrology. journal of manufacturing systems 7:339357.jones, l.a., i.w. hunter, s. lafontaine, j.m. hollerbach, and r. kearney 1991 widebandwidthmeasurements of human elbow joint mechanics. in proceedings of the canadian medicaland biological engineering conference 17:135136.kanade, t. 1993 very fast 3d sensing hardware. in preprints of the 6th international symposiumrobotics research, hidden valley, pa.kang, s.b., and k. ikeuchi 1993 toward automatic robot instruction from perceptionrecognizinga grasp from observation. ieee transactions on robotics and automation 9:432443.kawamura, s., and k. ito 1993 a new type of master robot for teleoperation using a radial wiredrive system. pp. 5560 in proceedings of the ieee/rsj international conference onintelligent robots and systems, yokohama, japan.kyle, s.a. 1993 noncontact measurement for robot calibration. pp. 79100 in r. bernhardt ands.l. albright, eds., robot calibration. london: chapman and hall.lau, k., r. hocken, and l. haynes 1985 robot performance measurements using automatic lasertracking techniques. robotics and computerintegrated manufacturing 2:227236.liang, j., c. shaw, and m. green 1991 on temporalspatial realism in the virtual realityenvironment. pp. 1925 in fourth annual symposium on user interface software and technology.maclean, s.g., m. rioux, f. blais, j. grodski, p. milgram, h.f.l. pinkney, and b.a. aikenhead1990 vision system development in a space simulation laboratory. closerangephotogrammetry meets machine vision spie 1395:815.references470virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.maeda, t., and s. tachi 1992 development of lightweight binocular headmounted displays. pp.281288 in proceedings of the second international symposium on measurement andcontrol in robotics, tsukuba science city, japan.meyer, k., h.l. applewhite, and f.a. biocca 1992 a survey of position trackers. presence:teleoperators and virtual environments 1:173200.mooring, b.w., z.s. roth, and m.r. driels 1991 fundamentals of manipulator calibration. newyork: wiley interscience.mulligan, i.j., a.k. mackworth, and p.d. lawrence 1989 a modelbased vision system formanipulator position sensing. pp. 186193 in proceedings of the workshop oninterpretation of 3d scenes, austin, tx.neilson, p.d. 1972 speed of response or bandwidth of voluntary system controlling elbow positionin intact man. medical and biological engineering 10:450459.payannet, d., m.j. aldon, and a. liegeois 1985 identification and compensation of mechanicalerrors for industrial robots. pp. 857864 in proceedings of the 15th internationalsymposium on industrial robots, tokyo, japan.prenninger, j., m. vincze, and h. gander 1993a contactless position and orientation measurementof robot endeffectors. ieee international conference robotics and automation 1:180185.1993b measuring dynamic robot movements in 6 dof and real time. pp. 124153 in r. bernhardtand s.l. albright, eds., robot calibration. london: chapman and hall.rioux, m. 1984 laser range finder based on synchronized scanners. applied optics 23:38373844.rohling, r., and j.m. hollerbach 1993 calibrating the human hand for haptic interfaces. presence: teleoperators and virtual environments 2(4).rohling, r., j.m. hollerbach, and s.c. jacobsen 1993 optimized fingertip mapping: a generalalgorithm for robotic hand teleoperation. presence: teleoperators and virtualenvironments 2(3).sato, m. 1991 virtual work space for 3dimensional modeling. pp. 103110 in proceedings of theinternational conference artificial reality and teleexistence.soechting, j.f., and m. flanders 1989 sensorimotor representations for pointing to targets in threedimensional space. journal of neurophysiology 62:582594.soechting, j.f., and f. lacquaniti 1988 quantitative evaluation of the electromyographic responsesto multidirectional load perturbations of the human arm. journal of neurophysiology59:12961313.references471virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.stone, h.w. 1987 kinematic modeling, identification, and control of robotic manipulators.boston: kluwer academic publishers.truong, d.m., and s.e. feldon 1987 sources of artifact in infrared recording of eye movement.investigative ophthalmology and visual science 28(6):10181022.ward, m., r. azuma, r. bennett, s. gottschalk, and h. fuchs 1992 a demonstrated optical trackerwith scalable work area for headmounted display system . pp. 4352 in proceedings of the1992 symposium interactive 3d graphics, cambridge, mass.wells, m.j., and m.w. haas 1990 head movement during simulated airtoair engagements. in r.j.lewandowski, ed., helmetmounted displays ii spie 1290:1629.xu, y., i.w. hunter, j.m. hollerbach, and d.j. bennett 1991 an airjet actuator system foridentification of the human arm joint mechanical properties. ieee transactions onbiomedical engineering 38:11111122.zhuang, h., b. li, z.s. roth, and x. xie 1992 selfcalibration and mirror center offset eliminationof a multibeam laser tracking system. robotics and autonomous systems 9:255269.chapter 6: wholebody motion, motion sickness,and locomotion interfacesbarany, r. 1906 untersuchungen uber den vom vestibular appat des chres reflek torisch ausgelostenrythmischer nystagmus und seine begleiterscheinunger. mschr chrenheilkd 40:193197.berthoz, a., b. pavard, and l.r. young 1975 perception of linear horizontal selfmotion induced byperipheral vision (linear vection): basic characteristics and visualvestibular interactions.experimental brain research 23:471489.biocca, f. 1992 will simulation sickness slow down the diffusion of virtual environmenttechnology? presence 1(3):334343.bles, w. 1981 stepping around: circular vection and coriolis effects. in j. long and a. baddeley,eds., attention and performance ix. hillsdale, n.j.: lawrence erlbaum.bles, w., and t.s. kapteyn 1977 circular vection and human posture. 1. does the proprioceptivesystem play a role? agressologie 18:325328.bles, w., t.s. kapteyn, t. brandt, and f. arnold 1980 the mechanism of physiological heightvertigo. ii. posturography. acta otolaryngologica stockholm, sweden 89:534540.references472virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.brandt, t., w. büchele, and f. arnold 1977 arthrokinetic nystagmus and egomotion sensation.experimental brain research 30:331338.calkins, d.s., m.f. reschke, r.s. kennedy, and w.p. dunlop 1987 reliability of provocative testsof motion sickness susceptibility. aviation space and environmental medicine 58(9suppl):a5054.coats, a.c., and s.h. smith 1967 body position and the intensity of caloric nystagmus. actaotolaryngologica 63:515532.cohen, b., j.i. suzuki, and t. raphan 1977 role of the otolith organs in generation of horizontalnystagmus: effects of selective labyrinthine lesions. brain research 276:159164.cowings, p.s., k.h. naifeh, and w.b. toscano 1990 the stability of individual patterns ofautonomic responses to motion sickness stimulation. aviation, space, and environmental medicine 61:5.crampton, g. 1990 motion and space sickness. boca raton, fla.: crc press.dichgans, j., and t. brandt 1978 visualvestibular interaction: effects on selfmotion perceptionand postural control. pp. 755804 in handbook of sensory physiology, vol 8. new york:springer.dizio, p., and j.r. lackner 1991 motion sickness susceptibility in parabolic flight and velocitystorage activity. aviation, space, and environmental medicine 62:300307.fukuda, t. 1975 postural behavior in motion sickness. acta otolaryngologica (stockholm) 330:914.goodwin, g.m., d.i. mccloskey, and p.b.c. matthews 1972 proprioceptive illusions induced bymuscle vibration: contribution by muscle spindles to perception? science 175:13821384.graybiel, a. 1952 the oculogravic illusion. american medical association ophthalmology48:605615.graybiel, a., and d.i. hupp 1946 the oculogyral illusion, a form of apparent motion which may beobserved following stimulation of the semicircular canals. journal of aviation medicine17:327.graybiel, a., and j. knepton 1976 sopite syndrome: a sometimes sole manifestation of motionsickness. aviation space and environmental medicine 47(8):873882.graybiel, a., and j.r. lackner 1980 evaluation of the relationship between motion sicknesssymptomatology and blood pressure, heart rate, and body temperature. aviation, space, and environmental medicine 51: 2112141983 motion sickness: acquisition and retention of adaptation effects in three motion environments.aviation, space, and environmental medicine 54:307311.references473virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.1987 treatment of severe motion sickness with antimotion sickness drug injections. aviation,space, and environmental medicine 58:773776.graybiel, a., c. wood, e. miller, and d. cramer 1968 diagnostic criteria for grading the severity ofacute motion sickness. aerospace medicine 39:453455.guignard, j.c., and m.e. mccauley 1990 the accelerative stimulus for motion sickness. pp.123152 in g. crampton, ed., motion and space sickness. boca raton, fla.: crc press.held, r., j. dichgans, and j. bauer 1975 characteristics of moving visual areas influencing spatialorientation. vision research 15:357365.howard, i.p. 1982 human visual orientation. new york: john wiley.howard, i.p., and w.b. templeton 1966 human spatial orientation. london: john wiley.kennedy, r.s., a. graybiel, r.c. mcdonough, and f.d. beckwith 1968 symptomatology understorm conditions in the north atlantic in control subjects and in persons with labyrinthinedefects. acta otolaryngologica 66:533.kennedy, r.s., l.j. hettinger, and m.g. lilienthal 1990 simulator sickness. pp. 317341 in g.h.crampton, ed., motion and space sickness. florida: crc press.kennedy, r.s., n.e. lane, m.g. lilienthal, k.s. berbaum, and l.j. hettinger 1992 profile analysisof simulator sickness symptoms: application to virtual environment systems. presence 1(3):295301.lackner, j.r. 1977 induction of illusory selfrotation and nystagmus by a rotating soundfield.aviation, space, and environmental medicine 48: 129131.1985 human sensorymotor adaptation to the terrestrial force environment. pp. 175210 in d. ingle,m. jeannerod, and d. lee, eds., brain mechanisms and spatial vision. amsterdam: nijhoff.1988 some proprioceptive influences on the perceptual representation of body shape andorientation. brain 111:281297.1989 human orientation, adaptation, and movement control. pp. 2950 in national researchcouncil, motion sickness, visual displays, and armored vehicle design. washington,d.c.: national academy press.1990 sensorymotor adaptation to high force levels in parabolic flight maneuvers. pp. 527548 inm. jeannerod, ed., attention and performance. hillsdale, n.j.: lawrence erlbaum.1992a spatial orientation in weightless environments. perception 21: 803812.1992b sense of body position in parabolic flight. annals of the new york academy of sciences656:329339.1992c multimodal and motor influences on orientation: implications for adapting to weightless andvirtual environments. journal of vestibular research 2:307322.1993 orientation and movement in unusual force environments. psychological science 4(3):134142.references474virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.lackner, j.r., and p. dizio 1984 some efferent and somatosensory influences on body orientationand oculomotor control. pp. 283301 in l. spillman and b.r. wooten, eds., sensoryexperience, adaptation, and perception. hillsdale, n.j.: lawrence erlbaum.1988 visual stimulation affects the perception of voluntary leg movements during walking.perception 17:7180.1989 altered sensorymotor control of the head as an etiological factor in space motion sickness.perceptual and motor skills 68:784786.1992 sensorymotor calibration processes constraining the perception of force and motion duringlocomotion. posture and gait: control mechanisms 1:93104.1993 spatial stability, voluntary action and causal attribution during selflocomotion. journal ofvestibular research 3:1523.1994 rapid adaptation to coriolis force perturbations of arm movement trajectory. to appear injournal of neurophysiology.lackner, j.r., and a. graybiel 1981 illusions of postural, visual, and aircraft motion elicited bydeep knee bends in the increased gravitoinertial force phase of parabolic flight.experimental brain research 44:312316.1984a elicitation of motion sickness by head movements in the microgravity phase of parabolicflight maneuvers. aviation, space, and environmental medicine 55:513520.1984b influence of gravitoinertial force level on apparent magnitude of coriolis crosscoupledangular accelerations and motion sickness. natoagard aerospace medical panelsymposium on motion sickness: mechanisms, prediction, prevention and treatmentagardcp372, 22:17.1984c perception of body weight and body mass at twice earthgravity acceleration levels. brain107:133144.1985 head movements elicit motion sickness during exposure to microgravity and macrogravityacceleration levels. pp. 170176 in m. igarashi and f.o. black, eds., proceedings of thevii international symposium: vestibular and visual control on posture and locomotorequilibrium. basel, switzerland: krager.1986 the effective intensity of coriolis, crosscoupling stimulation is gravitoinertial forcedependent: implications for space motion sickness. aviation, space, and environmentalmedicine 57:229235.1987 head movements in low and high gravitoinertial force environments elicit motion sickness:implications for space motion sickness. aviation, space, and environmental medicine58:a212a217.lackner, j.r., and m.s. levine 1979 changes in apparent body orientation and sensory localiztioninduced by vibration of postural muscles: viratory myesthetic illusions. aviation, space,and environmental medicine 50:346354.lackner, j.r., and d. lobovits 1978 incremental exposure facilitates adaptation to sensoryrearrangement. aviation, space, and environmental medicine 49:362364.references475virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.lackner, j.r., and r.a. teixeira 1977 optokinetic motion sickness: continuous head movementsattenuate the visual induction of apparent selfrotation and symptoms of motion sickness.aviation, space, and environmental medicine 48:248253.lackner, j.r., a. graybiel, and p. dizio 1991 altered sensorymotor control of the body as anetiological factor in space motion sickness. aviation, space, and environmental medicine62:765771.lawson, b.d., f.a. sunahara, and j.r. lackner 1991 physiological responses to visually inducedmotion sickness. society for neuroscience abstracts, 21st annual meeting 17(1):317.mccauley, m.e., ed. 1984 research issues in simulator sickness: proceedings of a workshop.committee on human factors, national research council, washington, d.c.: nationalacademy press.mccauley, m.e., and t.j. sharkey 1992 cybersickness: perception of selfmotion in virtualenvironments. presence 1(3):311318.miller, e.f., and a. graybiel 1972 semicircular canals as a primary etiological factor in motionsickness. aerospace medicine 43:10651074.minor, l.b., and j.m. goldberg 1990 influence of static head position on the horizontal nystagmusevoked by caloric, rotational, and optokinetic stimulation in the squirrel monkey.experimental brain research 82:113.previc, f.h., r.v. kenyon, and k.k. gillingham 1991 the effects of dynamic visual roll onpostural and manual control and selfmotion perception. paper presented at the 2ndannual scientific meeting of the aerospace medical society, cincinnati, ohio.reason, j.t., and j.j. brand 1975 motion sickness. new york: academic press.robinson, d.a. 1977 linear addition of optokinetic and vestibular signals in the vestibular nucleus.experimental brain research 30:447450.stern, r.m., k.l. koch, w.r. stewart, and i.m. lindbland 1987 spectral analysis of tachygastriarecorded during motion sickness. gastroenterology 92:9297.teixeira, r.a., and j.r. lackner 1979 optokinetic motion sickness: attenuation of visuallyinducedapparent selfrotation by passive head movements. aviation, space, and environmentalmedicine 50:264266.warren, w.h. 1993 perception and control of egomotion. in w. epstein and j.j. rogers, eds.,handbook of perception and cognition, vol. 5: perception of space and motion. orlando,fla.: academic press.references476virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.welch, r.b., b. bridgeman, s. anand, and k.e. browman 1993 alternating prism exposure causesdual adaptation and generalization to a novel displacement. perception and psychophysics54:195204.chapter 7: other interface components: speech,physiology, smell, and tastebahl, l.r., f. jelinek, and r. l. mercer 1983 a maximum likelihood approach to continuousspeech recognition. ieee transactions on pattern analysis and machine intelligence,pami5 179190.cacioppo, j.t., and r.e. petty 1983 social psychophysiology: a sourcebook. new york: guilford.cacioppo, j.t., and l.g. tassinary 1990 psychophysiology and psychophysiological inference. inj.t. cacioppo and l.g. tassinary, eds., principles of psychophysiology: physical, socialand inferential elements. new york: cambridge university press.calhoun, g., g. mcmillan, and p. morton 1993 brain actuated control: a candidate enablingtechnology. submitted to ieee transactions on rehabilitation engineering.cater, j.p., d.c. varner, s.d. huffman, k. miller, and e. peterson 1994 development of acomputercontrolled olfactory and radiant heat delivery system for virtualenvironments. project 059766, southwest research institute, san antonio, tex.chow, y.l., r. schwartz, s. roucos, o. kimball, p.price, g. kubala, m. dunham, m. krasner, andj. makhoul 1986 the role of worddependent coarticulatory effects in a phonemebasedspeech recognition system. proceedings of icassp 86:15931596. tokyo, japan.coles, m.g.h., g. gratton, t.r. bashore, c.w. eriksen, and e. donchin 1985 apsychophysiological investigation of the continuous flow model of human informationprocessing. journal of experimental psychology: human perception and performance11:529553.davis, s., and p. mermelstein 1980 comparison of parametric representations for monosyllabicword recognition in continuously spoken sentences. ieee transactions on acoustics,speech and signal processing, assp28 357366.dixon, n.r., and t. martin, eds. 1979 automatic speech and speaker recognition. new york: ieee.donchin, e., d. karis, t. bashores, m. coles, and g. gratton 1986 cognitive psychophysiology andhuman information processing. in m.g.h. coles, e. donchin, and s. porges, eds.psychophysiology: systems, processes, and applications. new york: guilford press.druckman, d., and j.i. lacey, eds. 1989 brain and cognition: some new technologies. committeeon new technologies in cognitive psychophysiology, commission on behavioralreferences477virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.and social sciences and education, national research council. washington, d.c.:national academy press.farry, k.a., and i.d. walker 1993 myoelectric teleoperation of a complex robotic hand. ieeeinternational conference on robotics and automation 3:502509.hartman, b.o. 1980 technical evaluation report of the aerospace medical panel working groupwg08 on evaluation of methods to assess workload. report no. agardar139.neuillysurseine, france: advisory group for aerospace research and development.hassett, j. 1978 a primer of psychophysiology. san francisco: w.h. freeman.hiraiwa, a., n. uchida, n. sonehara, and k. shimohara 1992 emg pattern recognition by neuralnetworks for prosthetic fingers control ''cyber finger." pp. 535542 in proceedings of thesecond international symposium on measurement and control in robotics, tsukubascience city, japan, nov. 1619.jacobsen, s.c., d.f. knutti, r.t. johnson, and h.h. sears 1982 development of the utah artificialarm. ieee transactions on biomedical engineering bme29:249269.junker, a.m., j.h. schnurer, d.f. ingle, and c.w. downey 1988 loopclosure of the visualcortical response (u). summary report number aamrltr88014. wrightpattersonair force base, ohio: armstrong aerospace medical research laboratory.1989 brain actuated control of a roll axis tracking simulator. pp. 714717 in proceedings of the1989 ieee international conference on systems, man, and cybernetics.karis, d., m. fabiani, and e. donchin 1984 p300 and memory: individual differences in the vonrestorff effect. cognitive psychology 16:177216.klatt, d.h. 1987 a review of texttospeech conversion of english. journal of the acousticalsociety of america 82:737793.kramer, a. 1993 event related potentials and mental workload. unpublished paper presented atthe workshop on virtual environment and teleoperator technology, february 2627,1993, to the committee on virtual reality research and development, commission onbehavioral and social sciences and education, national research council, washington,d.c.krantz, d., and s. manuck 1984 acute psychophysiological reactivity and risk of cardiovasculardisease: a review and methodological critique. psychological bulletin 96:465490.makhoul, j., and r. schwartz 1994 state of the art in continuous speech recognition. pp. 165198 invoice communication between humans and machines. washington, d.c.: nationalacademy press.references478virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.martin, j.h., j.c.m. brust, and s. hilal 1991 imaging the living brain. pp. 309324 in eric r.kandel, james h. schwartz, and thomas m. jessell, eds., principles of neural science, third edition. new york: elsevier.meek, s.g., j.e. wood, and s.c. jacobsen 1990 modelbased, multimuscle emg control of upperextremity prostheses. pp. 360376 in j.m. winters and s.ly. woo, eds., multiple muscle systems: biomechanics and movement organization. new york: springerverlag.neville, h.j., m. kutas, g. chesney, and a.l. schmidt 1986 eventrelated brain potentials duringinitial encoding and recognition memory of congruous and incongruous words. journal ofmemory and language 25:7592.nguyen, l., r. schwartz, f. kubala, and p. placeway 1993 search algorithms for softwareonly realtime recognition with very large vocabularies. pp. 9195 in proceedings of the arpahuman language technology workshop, plainsboro, n.j., march. san mateo, calif.:morgankaufmann.o'shaughnessy, d. 1987 speech communication, human and machine. reading, mass.: addisonwesley.pallett, d., j. fiscus, w. fisher, j. garafolo, b. lund, and m. pryzbocki 1994 1993 benchmark testsfor the arpa spoken language program. proceedings of the arpa human languagetechnology workshop, plainsboro, n.j., march. san mateo, calif.: morgankaufmann.rabiner, l.r. 1983 tutorial on isolated and connected word recognition. in h.w. sdchiissler, ed.,signal processing ii: theories and applications. new york: elsevier science publishers.schwartz, r., and s. austin 1991 a comparison of several approximate algorithms for findingmultiple (nbest) sentence hypotheses. pp. 701704 in proceedings of icassp, toronto,canada.smith, j.j., and j.p. kampine 1984 circulatory physiology: the essentials. baltimore, md.:williams and wilkins.walter, w.g., r. cooper, a.j. aldridge, w.c. mccallum, and a.l. winter 1964 contingentnegative variation: an electrical sign of sensorimotor association and expectancy in thehuman brain. nature 203:380384.warner, d., t. anderson, and j. johanson 1994 biocybernetics: a biologically responsiveinteractive interfacethe next paradigm of human computer interaction. pp. 237241 inproceedings of medicine and virtual reality. san diego, calif.weber, m.a., and i.m. drayer 1984 ambulatory blood pressure monitoring. dannstadt, germany:steinhopffverlag.references479virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.wierwille, w.w. 1979 physiological measures of aircrew mental workload. human factors 21(5):579594.zeffiro, t. 1993 brain imaging technology. unpublished paper presented at the workshop onvirtual environment and teleoperator technology, february 2627, 1993, to thecommittee on virtual reality research and development, commission on behavioral andsocial sciences and education, national research council, washington, d.c.chapter 8: computer hardware and softwarefor the generation of virtual environmentsairey, j.m., j.h. rohlf, and f.p. brooks, jr. 1990 towards image realism with interactive updaterates in complex virtual building environments. computer graphics 24(2):41.akeley, k. 1993 realityengine graphics. paper submitted to siggraph.badler, n.i., b.l. webber, j.k. kalita, and j. esakov 1991 animation from instructions. pp. 5193in n.i. badler, b.a. barsky, and d. zeltzer, eds., making them move: mechanics,control, and animation of articulated figures. san mateo, calif.: morgankaufmann.badler, n.i., c. phillips, and b.l. webber 1993 simulating humans: computer graphics,animation, and control. oxford, england: oxford university press.baraff, d. 1989 analytical methods for dynamic simulation of nonpenetrating rigid bodies.computer graphics 23(3):223232.baraff, d., and a. witkin 1992 dynamic simulation of nonpenetrating flexible bodies. computer graphics 26:303308. (proceedings of siggraph 1992.)barzel, r., and a.h. barr 1988 a modeling system based on dynamic constraints. computergraphics 22:179188.baumgarte, j. 1972 stabilization of constraints and integrals of motion in dynamical systems.computer methods in applied mechanics and engineering 1:116.beckett, w., and n. badler 1993 integrated behavioral agent architecture. proceedings of the workshop on computer generated forces and behavior representation, orlando, fla.bhargava, h.k., and w.c. branley, jr. 1993 what would agax have observed? or, introducingimperfections in the belief systems of autonomous agents. proceedings of the twentysixth hawaii international conference on system sciences vol. iii, january, kauai, hawaii.(los alamitos, calif: ieee computer society press.)references480virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.blanchard, c., s. burgess, y. harvill, j. lanier, a. lasko, m. oberman, and m. teitel 1990 realitybuilt for two: a virtual reality toolkit. computer graphics 24(2):3536.blattner, m., d. sumikawa, and r. greenberg 1989 earcons and icons: their structure and commondesign principles. humancomputer interaction 4(1). hillsdale, n.j.: lawrence erlbaum.brooks, f.p., jr. 1986 walkthrougha dynamic graphics system for simulating virtual buildings.pages 921 in proceedings of the 1986 workshop on interactive 3d graphics, october2324.brooks, r.a. 1989 the whole iguana. pp. 432456 in m. brady, ed., robotics science. cambridge,mass.: mit press. bryson, s.1992a paradigms for the shaping of surfaces in a virtual environment. proceedings of hicss,kauai, hawaii.1992b virtual spacetime. in proceedings of visualization 92, boston.bryson, s., and m. geraldyamasaki 1992 the distributed virtual wind tunnel. in proceedings ofsupercomputing, minneapolis, minn.bryson, s., and c. levit 1992 virtual wind tunnel: an environment for the exploration of threedimensional unsteady flows. ieee computer graphics and applications 12(4):2534.catmull, e., l. carpenter, and r. cook 1984 private and public communication. communication inreference to the number of polygons required to render reality, making certain assumptionsabout depth complexity and display resolution.celniker, g., and d. gossard 1992 deformable curve and surface finite elements for freeformshape design. ieee computer graphics and applications 25(4):25766.ching, w., and n. badler 1992 fast motion planning for anthropometric figures with many degreesof freedom. in ieee international conference on robotics and automation, may.clark, j.h. 1976 hierarchical geometric models for visible surface algorithms. communications ofthe acm 19(10):547554.cohen, m.f. 1992 interactive spacetime control for animation. in ieee international conferenceon robotics and automation 26(2):293303.conner, d.b., s.s. snibbe, k.p. herndon, d.c. robbins, r.c. zelenik, and a. van dam 1992 threedimensional widgets. pp. 197208 in proceedings of 1992 siggraph symposium oninteractive 3d graphics.references481virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.dehaemer, m.j., jr., and m.j. zyda 1991 simplification of objects rendered by polygonalapproximations. computers and graphics 15(2):175184.de mey, v., and s. gibbs 1993 a multimedia testbed. in proceedings of acm multimedia,anaheim, calif.ellis, s.r., m.k. kaiser, and a.j. grunwald, eds. 1991 pictorial communication in virtual andreal environments. london: taylor and francis.esakov, j., and n.i. badler 1991 an architecture for highlevel human task animation control. inp.a. fishwick and r.b. modjeski, eds., knowledgebased simulation. new york:springerverlag.fisher, s.s., m. mcgreevy, j. humphries, and w. robinett 1986 virtual environment displaysystem. pp. 7787 of proceedings acm workshop on interactive graphics.ford, l. 1985 intelligent computer aided instruction. pp. 106126 in m.y.a.a. narayanan, ed.,artificial intelligence: human effects. new york: wiley halsted press.frisch, u., d. d'homieres, b. hasslacher, p. lallemand, y. pomeau, and j.p. rivet 1987 latticegas hydrodynamics in two and three dimensions. complex systems 1:649701.fuchs, h., j. goldfeather, j. hultquist, s. spach, j. austin, f. brooks, j. eyles, and j. poulton 1985fast spheres, shadows, textures, transparencies, and image enhancements in pixel planes.pp. 111120 in siggraph '89 23(3).fuchs, h., j. poulton, j. eyles, t. greer, j. goldfeather, d. ellsworth, s. molnar, g. turk, b. tebbs,and l. israel 1989 pixel planes 5: a heterogeneous multiprocessor graphics system usingprocessorenhanced memories. pp. 7988 in siggraph '89 23(3).fuchs, h., g. bishop, k. arthur, l. mcmillan, r. bajcsy, s.w. lee, h. farid, and t. kanade 1994virtual space teleconferencing using a sea of cameras. robotic applications intelemedicine, september.funkhouser, t.a., c.h. sequin, and s. teller 1992 management of large amounts of data ininteractive building walkthroughs. p. 11 of proceedings of the computer graphicssymposium on interactive 3d graphics.geib, c. 1993 a consequence of incorporating intentions in meansend planning. aaai springsymposium series, foundations of automatic planning: the classical approach andbeyond, stanford, calif.hahn, j.k. 1988 realistic animation of rigid bodies. computer graphics 22:299308.references482virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.held, r., and n. durlach 1991 telepresence, time delay, and adaptation. in s.r. ellis, m.k. kaiser,and a.c. grunwald, eds., pictorial communication in real and virtual environments.bristol, penn.: taylor and francis.holloway, r., h. fuchs, and w. robinett 1992 virtualworlds research at the university of northcarolina at chapel hill. course notes, course #9: implementation of immersive virtualenvironments, siggraph '92, chicago, july.jacoby, r.h. 1992 using virtual menus in a virtual environment. proceedings of the symposium onelectronic imaging science and technology, vol. 1668. international society for opticalengineering/society for imaging science and technology .kramer, g. 1992 some organizing principles for representing data with sound. in g. kramer, ed.,auditory display: proceedings of icad '92, first international conference on auditorydisplay. sfi studies in the sciences of complexity, proceedings volume 18. reading,mass.: addisonwesley.lin, m.c., and f.c. canney 1992 efficient collision detection for animation. proceedings of thethird eurographics workshop on animation and simulation. cambridge, england:eurographics workshop on animation and simulation.maes, p. 1990 situated agents can have goals. journal of robotics and autonomous systems 6(1&2):4970.magnenatthalmann, n., and d. thalmann, eds. 1990 synthetic actors in computergenerated 3dfilms. berlin: springerverlag.1991 complex models for animating synthetic actors. ieee computer graphics and applications 11(5):3244.maiocchi, r., and b. pernici 1990 directing an animated scene with autonomous actors. pp. 4160in n. magnenatthalmann and d. thalmann, eds., computer animation. tokyo, japan:springerverlag.mckenna, m., and d. zeltzer 1990 dynamic simulation of autonomous legged locomotion.computer graphics 24(4):2938.metaxas, d. 1992 physicsbased modeling of nonrigid objects for vision and graphics. ph.d.thesis, department of computer science, university of toronto.1993 fast dynamic pointtopoint constraint algorithm for deformable bodies. in proceedings of the2nd international conference on discrete element methods, massachusetts institute oftechnology, cambridge, mass.metaxas, d., and d. terzopoulos 1992a dynamic deformation of solid primitives with constraints.computer graphics 26(2):309312.references483virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.1992b flexible multibody dynamics techniques for shape and nonrigid motion estimation andsynthesis. pp. 147155 in proceedings of asme symposium on the dynamics and controlof flexible multibody systems, anaheim, calif.1993 shape and nonrigid motional estimation through physicsbased synthesis. pattern analysis andmachine intelligence 15(6):580591.miller, g.p. 1988 the motion dynamics of snakes and worms. computer graphics 22:169178.molnar, s., j. eyles, and j. poulton 1992 pixelflow: highspeed rendering using imagecomposition. pp. 231240 in siggraph '92 26(2).moore, m., and j. wilhelms 1988 collision detection and response for computer animation.computer graphics 22:289298.ohya, j., y kitamura, h. takemura, f. kishino, and n. terashima 1993 realtime reproduction of3d human images in virtual space teleconferencing. pp. 408414 in ieee virtual realityannual international symposium, september 1822.pentland, a., and j. williams 1989 good vibrations: modal dynamics for graphics and animation.computer graphics 23(3):215222.platt, j., and a. barr 1988 constraint methods for flexible models. computer graphics 22:279288.raibert, m.h., and j.k. hodgkins 1992 animation of dynamic legged locomotion. computergraphics 25(4)34958.ridsdale, g. 1990 connectionist modeling of skill dynamics. journal of visualization andcomputer animation 1(2):6672.rijpkema, h., and m. girard 1991 computer animation of knowledgebased human grasping. pp.339348 in proceedings of acm siggraph 91, las vegas, nev.robinett, w., and r. holloway 1992 implementation of flying, scaling and grabbing in virtualworlds. computer graphics 26(3):189.schroeder, p., and d. zeltzer 1990 the virtual erector set: dynamic simulation with linear recursiveconstraint propagation. computer graphics 24(2):2332.schroeder, w.j., j.a. zarge, and w.e. lorensen 1992 decimation of triangle meshes. computergraphics 26(2):6569.sheridan, t.b., and w.r. ferrell 1974 man machine systems. cambridge, mass.: mit press.snibbe, s.s., k.p. herndon, d.c. robbins, d.b. conner, and a. van dam 1992 using deformationsto explore 3d widget design. pp. 351352 in proceedings of siggraph '92, chicago, ill.references484virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.szeliski, r., and d. tonnesen 1992 surface modeling with oriented particle systems. computergraphics 26(2):18594.teller, s.j., and c.h. sequin 1991 visibility preprocessing for interactive walkthroughs. computer graphics 25(4):6169.terzopoulos, d., and d. metaxas 1991 dynamic 3d models with local and global deformations:deformable superquadrics. ieee trans. pattern analysis and machine intelligence 13(7):703714.toffoli, t. 1983 cellular automata as an alternative to differential equations in modeling physics.interdisciplinary workshop, los alamos, n.m. march. p. 117.turk, g. 1992 retiling polygonal surfaces. computer graphics 26(2):5564.van de panne, m., e. fiume, and z. vranesic 1990 reusable motion synthesis using statespacecontrollers. computer graphics 24(4):225234.webber, b., n. badler, f.b. baldwin, w. becket, b. dieugenio, c. geib, m. jung, l. levison, m.moore, and m. white 1993 doing what you're told: following task instructions inchanging, but hospitable environments. submitted to ai journal.welch, w., and a. witkin 1992 variational surface modeling. computer graphics 26(2):157166.wilson, k., m. zyda, and d. pratt 1992 npsgdl: an object oriented graphics description languagefor virtual world appplication support. in proceedings of the third eurographics workshop on objectoriented graphics, champery, switzerland.witkin, a., m. gleicher, and w. welch 1990 interactive dynamics. computer graphics 2:1124.witkin, a., and m. kass 1988 spacetime constraints, computer graphics. computer graphics22:159168.witkin, a., and w. welch 1990 fast animation and control of nonrigid structures. computer graphics 24(4):243252.zeltzer, d. 1983 knowledgebased animation. pp. 187192 in proceedings acm siggraph/sigart workshop on motion.zyda, m.j., j.g. monahan, and d.r. pratt 1992 npsnet: physicallybased modeling enhancementsto an object file format. pp. 3552 in nadia magnenatthalmann and daniel thalmann,eds., creating and animating the virtual world. tokyo, japan: springerverlag.references485virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.zyda, m.j., k.p. wilson, d.r. pratt, j.g. monahan, and j.s. falby 1993a npsoff: an objectdescription language for supporting virtual world construction. computers and graphics 17(4):457464.zyda, m.j., d.r. pratt, w.d. osborne, and j.g. monahan 1993b npsnet: realtime collisiondetection and response. the journal of visualization and computer animation 4(1):1324.chapter 9: teleroboticsadvanced research projects agency 1992 proceedings of the image understanding workshop.defense advanced research projects agency, san diego, calif., january.ahmad, s. 1988 issues in the design of multiprocessor robot control hardware. pp. 127337 inproceedings of the ieee workshop on special computer architecture for robotics.an, c.h., c.g. atkeson, and j.m. hollerbach 1988 modelbased control of a robot manipulator.cambridge, mass.: mit press.andary, j.f., and p.d. spidaliere 1993 the development test flight of the flight telerobotic servicer:design description and lessons learned. ieee transactions on robotics and automation9:664674.anderson, r.j., and m.w. spong 1988 bilateral control of teleoperators with time delay. inproceedings of the ieee international conference on systems, man, & cybernetics 1:131.1989 bilateral control of teleoperators with time delay. ieee transactions on automatic control34:494501.anderson, t., and m. donath 1991 animal behavior as a paradigm for developing robot autonomy.pp. 145168 in p. maes, ed., design autonomous agents. cambridge, mass.: mit press.andriot, c., and r. fournier 1991 on the bilateral control of teleoperators with flexible joints andtime delay by the passive approach. in proceedings of the 5th icar, pisa, italy.arai, t., r. stoughton, and j.p. merlet 1992 teleoperator assisted hybrid control for parallel linkmanipulator and its application to assembly task. pp. 817822 in proceedings of thesecond international symposium on measurement and control in robotics, tsukubascience city, japan, november 1619.arkin, r.c. 1992 behaviorbased robot navigation for extended domains. adaptive behavior 1(2):201225.armstronghelouvry, b. 1991 control of machines with friction. boston, mass.: kluwer academic.references486virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.asada, h., and s.k. lim 1985 design of torque sensors and torque feedback control for directdrive arms. pp. 277284 in asme winter annual meeting: robotics and manufacturing automation, pedvol. 15, miami beach, fla. november 1722.asada, h., and k. youceftoumi 1987 direct drive robots: theory and practice. cambridge,mass.: mit press.aviles, w.a., t.w. hughes, h.r. everett, a.y. umeda, s.w. martin, a.h. koyamatsu, m.r.solorzano, r.t. laird, and s.p. mcarthur 1990 issues in mobile robotics: the unmannedground vehicle program teleoperated vehicle (tov). pp. 587597 in spie vol. 1388,mobile robots v.backes, p., s. hayati, v. hayward, and k. tso 1989 the kali multiarm robot programming andcontrol environment. pp. 173182 in proceedings of the nasa conference on spacetelerobotics. pp. 173182.badler, n.i., b.a. barsky, and d. zeltzer, eds. 1991 making them move: mechanics, control, andanimation of articulated figures . san mateo, calif.: morgankaufmann.baron, s., g. zacharias, r. muralhidaran, and r. lancraft 1980 procru: a model for analyzingflight crew procedures in approach to landing. pp. 7176 in proceedings of 8th ifaccongress, tokyo, japan. (see also nasa report cr152397.)bederson, b.b., r.s. wallace, and e.l. schwartz 1992 two miniature pantilt devices. pp. 658663in ieee international conference on robotics and automation, nice, france, may 1015.bejczy, a.k., and m. handlykken 1981 experimental results with a sixdegreeoffreedom forcereflecting hand controller. in proceedings of the 17th annual conference on manualcontrol, los angeles, calif.bejczy, a.k., and j.k. salisbury 1980 kinesthetic coupling between operator and remotemanipulator. in proceedings: asme international computer technology conference1:197211.1983 kinesthetic coupling for remote manipulators. cime (computers in mechanical engineering)2:4862.bejczy, a.k., and z. szakaly 1987 universal computer control system (uccs) for space telerobots.pp. 318324 in proceedings of the ieee international conference on robotics andautomation.bejczy, a.k., b. hannaford, and z. szakaly 1988 multimode manual control in telerobotics. inproceedings of romany '88, udine, italy, september 1215.belanger, p.r. 1992 estimation of angular velocity and acceleration from shaft encodermeasurements. pp. 585592 in ieee international conference on robotics andautomation, nice, france, may 1015.references487virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.besl, p.j. 1988 range image sensors. technical report gmr6090. general motors researchlaboratory, march.biggers, k.b., s.c. jacobsen, and c.c. davis 1989 linear analysis of a force reflective teleoperator.in proceedings of nasa conference on space telerobotics. pasadena, calif., january 31.book, w.j., and d.p. hannema 1980 masterslave manipulator performance for various dynamiccharacteristics and positioning task parameters. ieee transactions on systems, man, andcybernetics 10:764771.brenan, c.j.h., t.d. doukoglou, i.w. hunter, and s. lafontaine 1993 characterization and use of anovel position for microposition control of a linear motor. review of scientific instruments63:349356.brooks, r. 1986 a robust layer control systems for a mobile robot. ieee journal of robotics andautomation 2(1):1423.1990 a robot that walks: emergent behaviors from a carefully evolved network. pp. 2939 in p.h.winston and s.a. shellard, eds., artificial intelligence at mit. cambridge, mass.: mitpress.1991 intelligence without representation. artificial intelligence 47(13):139159.brooks, t.l., and i. ince 1992 operator vision aids for telerobotic assembly and servicing in space.pp. 886891 in proceedings of the ieee international conference on robotics andautomation, nice, france, may 1015.brown, r.h., s.c. schneider, and m.g. mulligan 1992 analysis of algorithms for velocityestimation from discrete position versus time data. ieee transactions on industrialelectronics 39:1119.brunner, b., g. hirzinger, k. landzettel, and j. heindl 1993 multisensory shared autonomy andtelesensorprogrammingkey issues in the space robot technology experiment rotex.pp. 21232131 in proceedings of the ieee/rsj international conference on intelligent robots and systems, yokohama, july 2630.buehler, m., l. l. whitcomb, f. levin, and d. koditscheck 1989 a distributed message passingcomputation and i/o engine for realtime motion control. pp. 478483 in proceedings ofthe american control conference.busby associates, inc. 1990 undersea vehicles directory  19901991. arlington, va.: busbyassoc., inc.charette, p.g., i.w. hunter, and c.j.h. brenan 1993 a complete high performance heterodyneinterferometer displacement transducer for microactuator control. review of scientificinstruments 63:241248.chen, j.b., r.s. fearing, b.s. armstrong, and j.w. burdick 1986 nymph: a multiprocessor formanipulation applications. pp. 17211736references488virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.in proceedings of the ieee international conference on robotics and automation.cheng, c.c. 1991 predictor displays: theory, development and application to towedsubmersibles. doctoral thesis, massachusetts institute of technology, june.chu, y.y., and w.b. rouse 1979 adaptive allocation of decision making responsibility betweenhuman and computer in multitask situations. ieee transactions on systems, man andcybernetics 9(12):769788.clark, d. 1989 hic: an operating system for hierarchical servo loops. pp. 10041009 inproceedings of the ieee international conference on robotics and automation.colgate, j.e. 1991 power and impedance scaling in bilateral manipulation. pp. 22922297 inproceedings of ieee international conference on robotics and automation, sacramento,calif.colgate, j.e., and n. hogan 1988 robust control of dynamically interacting systems. international journal of control 48:6588.connell, j.h. 1990 minimalist mobile robotics. san diego: academic press.conway, l., r. volz, and m. walker 1987 teleautonomous systems: methods and architectures forintermingling autonomous and telerobotic technology. pp. 11211130 in proceedings 1987ieee international conference on robotics and automation, march 31april 3, raleigh,n.c.cutkosky, m., and j.m. hyde 1993 manipulation control with dynamic tactile sensing. in 6thinternational symposium on robotics research, hidden valley, pa., oct. 25.dario, p. 1989 tactile sensing for robots: present and future. pp. 133146 in o. khatib, j.j. craig,and t. lozanoperez, eds., the robotics review 1. cambridge, mass.: mit press.1992 microbiotics: shifting robotics technology toward a different scale world . pp. 343358 in o.khatib, j.j. craig, and t. lozanoperez, eds., the robotics review 2. cambridge, mass.:mit press.dario, p., and d. de rossi 1985 tactile sensors and the gripping challenge. ieee spectrum 22(8):4652.dario, p., r. valleggi, m. pardini, and a. sabatini 1991 a miniature device for medical intracavityintervention. pp. 171175 in proceedings of the ieee workshop on micro electromechanical systems. nara, japan, jan. 30feb. 2.das, h. 1989 kinematic control and visual display of redundant teleoperators. ph.d. thesis,massachusetts institute of technology.references489virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.das, h., h. zak, w.s. kim, a.k. bejczy, and p.s. schenker 1992 operator performance withalternative manual control modes in teleoperation. presence 1:201218.de rossi, d., g. canepa, f. germagnoli, g. magenes, a. caiti, and t. parisini 1993 skinliketactile sensor arrays for contact stress field extraction. materials science and engineeringc: biomimetic materials, sensors, and systems 1(1):23.desai, r.s., b. wilcox, and r. bedard 1992 robotic vehicle overview. unmanned systems 10(4):1621.desilva, c.w. 1989 control sensors and actuators. englewood cliffs, n.j.: prenticehall.engelberger, j.l. 1989 robotics in service. cambridge, mass.: mit press.espiau, b. 1987 the advanced teleoperation project. pp. 1927 in a. morecki, g. bianchi, and k.kedzior, eds., romansy 6: proceedings of the 6th cismiftomm symposium on theoryand practice of robots and manipulators. cambridge, mass.: mit press.everett, h.r., g.a. gilbreath, t. tran, and j.m. nieusma 1990 modeling the environment of amobile security robot. technical document 1835, naval ocean systems center, sandiego, calif.everett, h.r., e.h. stitz, and d.e. demuth 1992 survey of a collision avoidance and rangingsensors for mobile robots. naval command, control and ocean surveillance center,rdt&e division. tr 1194, december.everett, h.r., g.a. gilbreath, r.t. laird, and t.a. heathpastore 1993 multiple robot hostarchitecture: mobile detection, assessment, and response system (mdars). technicalnote 1710, revision 1. nccosc rdt&e division. san diego, calif.ferrell, w.r. 1965 remote manipulation with transmission delay. pp. 2432 in ieee transactionson human factors in electronics, september.feynman, r.p. 1960 there's plenty of room at the bottom: an invitation to open up a new field ofphysics. engineering and science 23(feb.):2226.flash, t., and n. hogan 1985 the coordination of arm movement: an experimentally confirmedmathematical model. journal of neuroscience 5(july):16881703flatau, c.r. 1973 the manipulator as a means of extending our dextrous capabilities to larger andsmaller scales. pp. 4750 in proceedings of the 21st conference on remote systemstechnology.fletcher, b.e., and j.l. fuqua 1991 advanced security vehicle (asv) major bid and proposal,naval ocean system center. tr 1421. san diego, calif., april.references490virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.freund, j.f. 1986 new technologies in u.s. navy undersea vehicles. naval sea systemscommand, washington, d.c.fukuda, t., k. tanie, and t. mitsuka 1987 a new method of master slave type teleoperation for amicromanipulator. in proceedings of the ieee micro robots and teleoperatorsworkshop , hyannis, mass., november.funda, j., t.s. lindsay, and r.p. paul 1992 teleprogramming: toward delayinvariant remotemanipulation. presence: teleoperators and virtual environments 1:2944.goertz, r.c. 1964 manipulator systems development at anl. pp. 117136 in proceedings of the12th conference on remote systems technology, ans, november.goertz, r.c., and w.m. thompson 1954 electronically controlled manipulator. nucleonics(november):4647.goldenberg, a.a., and d. bastas 1990 on the bilateral control of force reflecting teleoperation. inproceedings ifac '90. august.gopinath, g., and s. thakkar 1990 chaos: why one cannot have only an operating system forrealtime applications. computer (june):6069.gordon, a. 1993 underwater laser line scan technology. pp. 164170 in underwater intervention93: the 11th annual rov conference and exposition, marine technology society, neworleans, la.gosselin, c.m., and e. lavoie 1993 on the kinetic design of spherical threedegreeoffreedomparallel manipulators. international journal of robotics research 12:394402.graves, s., j. mollenhauer, and b.d. morgan 1994 dynamic session management for teleroboticcontrol and simulation. to appear in the proceedings of the ieee internationalconference on robotics and automation.griffin, k.a., and j.w. murphy 1992 low cost gps/ins system for uav navigation. auvs92.huntsville. june.handlykken, m., and t. turner 1980 control system analysis and synthesis for a six degreeoffreedom universal forcereflecting hand controller. pp. 11971205 in proceedings of theieee conference on decision and control.hannaford, b. 1989 a design framework for teleoperators with kinesthetic feedback. ieeetransactions on robotics and automation 5:426434.1990 scaling, impedance, and power flows in force reflecting teleoperation. robotics research:proceedings of asme winter annual meeting 26:229232.1991 kinesthetic feedback techniques in teleoperated systems. in c.references491virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.leondes, ed., advances in control and dynamic systems. san diego, calif.: academicpress.hannaford, b., and w.s. kim 1989 force reflection, shared control, and time delay intelemanipulation. in proceedings of the ieee international conference on systems, man, and cybernetics . cambridge, mass., november.hannaford, b., and z. szakaly 1992 forcefeedback cursor control. nasa tech briefs 16(5):item 21.hannaford, b., l. wood, b. guggisberg, d. mcaffee, and h. zak 1989 performance evaluation ofa sixaxis generalized forcereflecting teleoperator. jpl publication 8918, pasadena,calif., june 15.hannaford, b., l. wood, d. mcaffee, and h. zak 1991 performance evaluation of a sixaxisgeneralized force reflecting teleoperator. in ieee transactions on systems, man, andcybernetics 21:620633.hashimoto, m., y. kiyosawa, h. hirabayashi, and r.p. paul 1991 a joint torque sensing techniquefor robots with harmonic drives. pp. 10341039 in proceedings of the ieee internationalconference on robotics and automation, sacramento, calif., april 911.hashimoto, t., t.b. sheridan, and m.v. noyes 1986 effects of predicted information inteleoperation through a time delay. japanese journal of ergonomics 22(2).hatamura, y., and h. morishita 1990 direct coupling system between nanometer world and humanworld. pp. 203208 in proceedings of the ieee workshop on micro electro mechanicalsystems, napa valley, calif., feb. 1114.hayati, s., t. lee, k. tso, p. backes, and j. lloyd 1990 a testbed for a unified teleoperatedautonomous dualarm robotic system. pp. 10901095 in proceedings of the ieeeinternational conference on robotics and automation. cincinnati, ohio, may 1318.hayati, s., h. seraji, w.s. kim, and j. balarum 1992 remote surface inspection for spaceapplications. aiaa 921017. aiaa aerospace design conference, irvine, calif., feb. 36.hayward, v., and r.p. paul 1984 introduction to rccl: a robot control library. pp. 283294 inproceedings of the ieee international conference on robotics and automation.hayward, v., n. chafye, x. chen, and b. duplat 1993 kinematic decoupling in mechanisms andapplication to a passive hand controller design. journal of robotic systems 10(5).heath, t.a. 1992 humanmachine interface development for teleoperated vehicles: improvedattitude awareness with gravity references sensors. technical report in process by thenaval command, control and ocean surveillance center.hebert, m., t. kanade, and i. kweon 1988 3d vision techniques for autonomous vehicles.technical report cmuritr8812. carnegie mellon university.references492virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.held, r.m., and n.i. durlach 1992 telepresence. presence: teleoperators and virtualenvironments 1(1):109112.henrion, w., et al. 1990 wide dynamic range direct digital accelerometer. pp. 153157 in ieeesolidstate sensor and actuator workshop, hilton head.hill, j.w. 1979 study of modeling and evaluation of remote manipulation tasks with force feedback.final report 5,6 jpl contract 955718, stanford research institute.hirose, m., and k. yokoyama 1992 vr application for transmission of synthetic sensation. pp.145154 in proceedings of the second international conference on artificial reality andteleexistence, tokyo, japan.hirose, s., and k. yoneda 1989 robotic sensors with photodetecting technology. pp. 271287 inproceedings of the international symposium on industrial robots, tokyo, japan.1990 development of optical 6axial force sensor and its signal calibration considering nonlinearinterference. pp. 4653 in proceedings of the ieee international conference on roboticsand automation, cincinnati, ohio.hirose, m., k. hirota, and k. yokoyama 1992 a study on the transmission of synthetic sensation.pp. 487492 in proceedings of the second international symposium on measurement andcontrol in robotics, tsukuba science city, japan.hirzinger, g. 1987 the space and telerobotic concepts of the dfvlr rotex. pp. 443449 inproceedings of the ieee international conference on robotics and automation, raleigh,n.c.hirzinger, g., and j. dietrich 1986 multisensory robots and sensor based path generation. pp.19922001 in proceedings of the ieee international conference on robotics and automation, san francisco, calif.hirzinger, g., b. brunner, j. dietrich, and j. heindl 1993 sensorbased space robotics rotex andits telerobotic features. ieee transactions on robotics and automation 9: 649663.hogan, n. 1985a impedance control: an approach to manipulation: part itheory. journal ofdynamic systems, measurement, and control 107:17.1985b impedance control: an approach to manipulation: part iiimplementation. journal ofdynamic systems, measurement, and control 107:816.1985c impedance control: an approach to manipulation: part iiiapplications. journal of dynamicsystems, measurement, and control 107:1724.hollerbach, j.m. 1982 a recursive formulation of manipulator dynamics and comparativereferences493virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.study of dynamic formulations and complexity. pp. 7387 in brady et al., eds, robotmotion, cambridge, mass.: mit press.1985 optimum kinetic design for a seven degree of freedom manipulator. pp. 215222 in h.hanafusa and h. inoue, eds., robotics research: the second international symposium.cambridge, mass.: mit press.1987 robot hands and tactile sensing. pp. 317342 in w.e.l. grimson and r.s. patil, eds., ai in the1980's and beyond. cambridge, mass.: mit press.hollerbach, j.m., i.w. hunter, and j. ballantyne 1992 a comparative analysis of actuatortechnologies for robotics. pp. 299342 in o. khatib, j.j. craig, and t. lozanoperez, eds.,the robotics review 2. cambridge, mass.: mit press.hollerbach, j.m., j. lang, e. vaaler, i. garabieta, r. sepe, s. umans, and i.w. hunter 1993 themcgill/mit direct drive motor project. in proceedings of the ieee internationalconference on robotics and automation, atlanta, georgia.hollis, r.l., s. salcudean, and d.w. abraham 1990 toward a telenanorobotic manipulation systemwith atomic scale force feedback and motion resolution. pp. 115119 in proceedings of theieee workshop on micro electro mechanical systems, napa valley, calif.hong, j., and x. tan 1989 calibrating a vpl datagrove for teleoperating the utah/mit hand. pp.17521757 in proceedings of the ieee international conference on robotics andautomation, scottsdale, ariz.howe, r.d., and m.r. cutkosky 1992 touch sensing for robotic manipulation and recognition. pp.55112 in o. khatib, j.j. craig, and t. lozanoperez, eds., the robotics review 2.cambridge, mass.: mit press.hunter, i.w., s. lafontaine, p.m.f. nielsen, p.j. hunter, and j.m. hollerbach 1990 manipulationand dynamic mechanical testing of microscopic objects using a telemicrobot system.ieee control systems magazine 10(2):39.hunter, i.w., s. lafontaine, j.m. hollerbach, and p.j. hunter 1991 fast reversible niti fibers foruse in microbotics. pp. 166170 in proceedings of the ieee workshop on micro electromechanical systems, nara, japan.ikeuchi, k. 1993 assembly plan from observation. in 6th international symposium on roboticsresearch , hidden valley, pa.ikuta, k., m. tsukamoto, and s. hirose 1988 shape memory alloy servo actuator system withelectric resistance feedback and application for active endoscope. pp. 427430 inproceedings of the ieee international conference on robotics and automation,philadelphia, pa.izaguirre, a., m. hashimoto, r.p. paul, and v. hayward 1992 a new computational structure forrealtime dynamics. international journal of robotics research 11(4):346362.references494virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.jacobsen, s.c., e.k. iversen, d.f. knutti, r.t. johnson, and k.b. biggers 1986 design of the utah/mit dextrous hand. pp. 15201532 in proceedings of the ieee international conferenceon robotics and automation, san francisco, calif.jacobsen, s.c., r.h. price, j.e. wood, t.h. rytting, and m. rafaelof 1989 the wobble motor:design, fabrication, and testing of an eccentricmotion electrostatic microactuator. pp.15361546 in proceedings of the ieee international conference on robotics andautomation, scottsdale, ariz.jacobsen, s.c., e.k. iverson, c.c. davis, d.m. potter, and t.w. mclain 1990a design of amultiple degree of freedom, force reflective hand master/slave with a high mobility wrist .in 3rd topical meeting on robotics and remote systems, charleston, s.c.jacobsen, s.c., f.m. smith, e.k. iverson, and d.k. backman 1990b high performance, highdexterity, force reflective teleoperator. in proceedings of the 38th conference on remotesystems technology, washington, d.c.jacobsen, s.c., f.m. smith, d.k. backman, and e.k. iverson 1991 high performance, highdexterity, force reflective teleoperator ii. in ans topical meeting on robotics and remotesystems, albuquerque, n.m.jansen, j.f., and j.n. herndon 1990 design of a telerobotic controller with joint torque sensors. pp.11091115 in proceedings of the ieee international conference on robotics andautomation, cincinnati, ohio.jansen, j.f., and r.l. kress 1991 control of a teleoperator system with redundancy based onpassivity conditions. pp. 478484 in proceedings of the ieee international conference onrobotics and automation, sacramento, calif.jau, b.m. 1992 manequivalent telepresence through fourfingered humanlike hand system. pp.843848 in ieee international conference on robotics and automation, nice, france.jones, j.l., and a.m. flynn 1993 mobile robots: inspiration to implementation. wellesley, mass.:a.k. peters.jones, l.a., and i.w. hunter 1990 influence of the mechanical properties of a manipulandum onhuman operator dynamics: 1. elastic stiffness. biological cybernetics 62:299307.1992 human operator perception of mechanical variables and their effects on tracking performance.in asme winter annual meeting: issues in the development of kinesthetic displays forteleoperation and virtual environments, anaheim, calif.1993 influence of the mechanical properties of a manipulandum on human operator dynamics: 2.viscosity. biological cybernetics (in press).juliussen, e. 1994 which lowend workstation? ieee spectrum 31(4):5159.references495virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.kan, e.p., j. lee, and m. junod 1990 an integrated frhcpumaee teleautonomous system. pp.397404 in m. jamshidi and m. saif, eds., robotics and manufacturing (vol.3). new york:asme press.kanade, t. 1992 cmu image understanding program. pp. 5161 in darpa image understanding workshop, san diego, calif.kazerooni, h., and j. guo 1993 human extenders. journal of dynamic systems, measurements, and control 115(2b):281290.kazerooni, h., t.i. tsay, and k. hollerbach 1993 a controller design framework for teleroboticsystems. ieee transactions on control systems technology 1:5062.kelley, a.j., and s.e. salcudean 1993 magicmouse: tactile and kinesthetic feedback in the humancomputer interface using an electromagnetically actuated input/output device. ieeetransactions on robotics and automation (in press).khoshzaban, m., f. sassani, and p.d. lawrence 1992 autonomous kinematic calibration ofindustrial hydraulic manipulators. in m. jamshidi, r. lumia, j. mullins, and m.shahinpoor, eds., robotics and manufacturing 4:577584. new york: asme press.khosla, p.k. 1988 some experimental results on modelbased control schemes. in proceedings ofthe ieee international conference on robotics and automation 3:13801385.philadelphia, pa.kim, w.s., f. tendick, s.r. ellis, and l.w. stark 1987 a comparison of position and rate controlfor telemanipulations with consideration of manipulator system dynamics. ieee journal of robotics and automation 3:426436.kim, w.s., b. hannaford, and a.k. bejczy 1992 forcereflection and shared complaint control inoperating telemanipulators with time delay. in ieee transactions on robotics andautomation 8:176185.kim, w.s., k.s. tso, and s. hayati 1993 an operator interface design for a telerobotic inspectionsystem. aiaa 931160. aiaa aerospace design conference, irvine, calif.kruthmicrowave electronics company 1989 final report phase i. naval sea systems commandnssc contract no. n0002488c5155. hanover, md.kvasnica, m. 1992 sixcomponent forcetorque sensing by means of one square ccd or psdelement. pp. 213219 in proceedings of the 2nd international symposium on measurementand control in robotics, tsukuba science city, japan.laird, r.t., h.r. everett, and g.a. gilbreath 1993 a host architecture for multiple robot control. inamerican nuclearreferences496virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.society 5th topical meeting on robotics and remote handling, knoxville, tenn.landsberger, s., and t.b. sheridan 1985 a new design for parallel link manipulators. inproceedings of the ieee international conference on cybernetics and society, tuscon,ariz.lathrop, r. h. 1985 parallelism in manipulator dynamics. international journal of roboticsresearch 4(2):346362. cambridge, mass.: mit press.latombe, j.c. 1991 robot motion planning. norwell, mass.: kluwer academic.laughlin, d.r., a.a. ardaman, and h.r. sebesta 1992 inertial angular rate sensors: theory andapplications. sensors 9(10):2024.lawn, c.a., and b. hannaford 1993 performance testing of passive communication and control inteleoperation with time delay. ieee international conference on robotics and automation3:776783.lee, t.s., s. hayati, k.s. tso, and p.g. backes 1990 dualarm voicecontrolled forcereflectingteleoperation system. pp. 419426 in m. jamshidi and m. saif, eds., robotics andmanufacturing (vol. 3). new york: asme press.long, g.l., and c.l. collins 1992 a pantograph linkage parallel platform master hand controllerfor force reflection. pp. 390395 in ieee international conference on robotics andautomation, nice, france.luh, j.y.s., m.w. walker, and r.p.c. paul 1980 online computational scheme for mechanicalmanipulators. journal of dynamic systems, measurement, and control 102:6970.luh, j.y.s., w.d. fisher, and r.p.c. paul 1983 joint torque control by a direct feedback forindustrial robots. pp. 153161 in ieee transactions on automatic control ac28.ma, d., j.m. hollerbach, and y. xu 1994 gravity based autonomous calibration for robotmanipulators. in proceedings of the ieee international conference on robotics and automation.machida, k., y. toda, t. iwata, m. kawachi, and t. nakamura 1988 development of a graphicsimulator augmented teleoperator system for space applications. pp. 358364 inproceedings of 1988 aiaa conference on guidance, navigation, and control.mack, b., s. mcclure, and r. ravindran 1991 a ground testbed for evaluating concepts for thespecial purpose dextrous manipulator. pp. 884889 in proceedings of the ieeeinternational conference on robotics and automation, sacramento, calif.maekawa, h., k. tanie, k. komoriya, m. kaneko, k. horiguchi, and t. sugawara 1992development of a fingershaped tectile sensor and its evaluation by active touch. pp.13271334 in ieee international conference on robotics and automation, nice, france.references497virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.mason, m.t. 1981 compliance and force control for computer controlled manipulators. pp. 418432in ieee transactions on systems, man, and cybernetics, vol. smc11.1989 robotic manipulation: mechanics and planning. pp. 262290 in m. brady, ed., roboticsscience. cambridge, mass.: mit press.mccammon, i.d., and s.c. jacobsen 1990 tactile sensing and control for the utah/mit hand. pp.239266 in s.t. venkataraman and t. iberall, eds., dextrous robot hands. new york:springerverlag.mcgovern, d.w. 1990 experiences and results in teleoperation of land vehicles. sandia nationallaboratory report sand900299.uc515, april.mclean, g.f., b. prescott, and r. podhorodeski 1994 teleoperated system performance evaluation.ieee transactions on systems, man, and cybernetics 24:796804.mettala, e.g. 1992 cmu image understanding program. pp. 159171 in darpa image understanding workshop.milgram, p., s. zhai, and d. drascic 1993 applications of augmented reality for humanrobotcommunication. pp. 14671472 in proceedings of the ieee/rsj international conference on intelligent robots and systems, yokohama, japan, july 2630.millitech corporation 1989 modular millimeter wave fmcw sensor. final report for naval seasystems command nssc contract no. n0002488c5144. south deerfield, mass.mitsuishi, m., s. warisawa, y. hatamura, t. nagao, and b. kramer 1992 trial of a remote realitybased manufacturing system in japan operated from the united states. pp. 14811498 injapanusa international workshop on flexible automation.morinaga, w.s., and r.t. hoffman 1991 advanced tethered vehicle. ieee0780302028/91/00001268.nakamura, y., and h. hanafusa 1986 inverse kinematic solutions with singularity robustness forrobot manipulator control. journal of dynamic systems, measurement, and control108:163171.nakamura, y., t. yoshikawa, and i. futamata 1988 design and signal processing of sixaxis forcesensors. pp. 7582 in r. bolles and b. roth, eds., robotics research: the fourth international symposium. cambridge, mass.: mit press.narasimham, s., d.m. siegel, and j.m. hollerback 1988 condor: a revised architecture forcontrolling the utahmit hand. pp. 446449 in proceedings of the ieee internationalconference on robotics and automation.newman, w.s., and j.j. patel 1991 experiments in torque control of the adept one robot. pp.18671872references498virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.in proceedings of the ieee international conference on robotics and automation,sacramento, calif.nicholls, h.r., and m.h. lee 1989 a survey of robot tactile sensing technology. international journal of robotics research 8(3):330.nielsen, p., i. hunter, s. lafontaine, and s. martel 1989 a parallel computation and controlcomputer for microrobotics. pp. 472477 in proceedings of the american controlconference.niemeyer, g., and j.j. slotine 1991 stable adaptive teleoperation. ieee journal of oceanicengineering 16:152162.niino, r., s. egawa, and t. higuchi 1993 highpower and highefficiency electrostatic actuator. pp.236241 in proceedings of the ieee workshop on micro electro mechanical systems, fortlauderdale, fla.novak, j.l., and j.t. feddema 1992 a capacitancebased proximity sensor for whole arm obstacleavoidance. pp. 13071314 in ieee international conference on robotics and automation,nice, france.noyes, m.v. 1984 superposition of graphics on low bitrate video as an aid to teleoperation.master's thesis, massachusetts institute of technology.okada, t. 1982 development of an optical distance sensor for robots. international journal ofrobotics research 1(4):314.okada, t., and u. rembold 1991 proximity sensor using a spiralshaped lightemitting mechanism.ieee transactions robotics and automation 7:798805.ollendorf, s. 1990 robotics for space applications. pp. 413418 in m. jamshidi and m. saif, eds.,robotics and manufacturing, vol. 3. new york: asme press.ouhyoung, m., m. pique, j. hughes, n. srinivasan, and f.p. brooks, jr. 1988 using a manipulatorfor force display in molecular docking. pp. 18241829 in proceeding of the ieeeinternational conference on robotics and automation. philadelphia, pa.oyama, e., n. tsunemoto, s. tachi, and y. inoue 1993 remote manipulation using virtualenvironment. presence: teleoperators and virtual environments 2:112124.paden, b., and s. sastry 1988 optimal kinematic design of 6r manipulators. international journal of robotics research 7(2):4361.pao, l., and t.h. speeter 1989 transformation of human hand positions for robotic hand control.pp. 17581763 in proceedings of the ieee international conference on robotics andautomation, scottsdale, ariz.references499virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.papanikolopoulos, n.p., and p.k. khosla 1992 shared and traded telerobotic visual control. pp.878885 in ieee international conference on robotics and automation, nice, france.park, j.h. 1991 supervisory control of robot manipulators for gross motions. ph.d. thesis,massachusetts institute of technology.parker, n.r., s.e. salcudean, p.d. lawrence 1993 application of force feedback to heavy dutymachines. proceedings of ieee international conference on robotics and automation3:375381.partaatmdja, o., b. benhabib, a. sun, and a.a. goldenberg 1992 an electrooptic orientation sensorfor robotics. ieee transactions robotics and automation 8:111119.paul, h.a., b. mittlestadt, w.l. bargar, b. musits, r.h. taylor, p. kazanzides, b. williamson, andw. hanson 1992 a surgical robot for total hip replacement. pp. 606611 in ieee international conference on robotics and automation, nice, france.pennington, j.e. 1986 space telerobotics: a few more hurdles. pp. 813816 in proceedings on theieee international conference on robotics and automation, san francisco.pepper, r.l., and j.d. hightower 1984 research issues in teleoperator systems. a paper presentedat the 28th annual human factors society meeting, san antonio, texas.pfeffer, l., o. khatib, and j. hake 1989 joint torque sensory feedback in the control of a pumamanipulator. ieee transactions robotics and automation 5:418425.pugh, a., ed. 1986 robot sensors, volume 2tactile and nonvision. new york: springerverlag.raibert, m.h. 1990 pp. 149179 in winston and shellard, eds., legged robots, artificial intelligence at mit. cambridge, mass.: mit press.raibert, m.h., and j.j. craig 1981 hybrid position/force control of manipulators. asme journal ofdynamic systems, measurement, and control 102:126133.raju, g.j. 1988 operator adjustable impedance in bilateral remote manipulation. ph.d.dissertation, department of mechanical engineering, massachusetts institute oftechnology.ramstein, c., and v. hayward 1994 the pantograph: a large workspace haptic device for amultimodal humancomputer interaction. computerhuman interaction chi'94, acm,boston, april 2428.rasmussen, d. 1992 a natural visual interface for precision telerobot control. pp. 170179 in spievol. 1833 telemanipulator technology, boston, november.references500virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.rizzi, a., l. whitcomb, and d. koditschek 1992 distributed realtime control of a spatial robotjuggler. pp. 1224. computer may.rohling, r.n., and j.m. hollerbach 1993 optimized fingertip mapping for teleoperation of dextrousrobot hands. in proceedings of the ieee international conference on robotics andautomation, atlanta, ga.rosheim, m.e. 1990 design of an omnidirectional arm. pp. 21622167 in proceedings of the ieeeinternational conference of robotics and automation, cincinnati, ohio.russell, m. 1983 odex 1: the first functionoid. robotics age 5(5):1218.salcudean, s.e., n.m. wong, and r.l. hollis 1992 a forcedeflecting teleoperation system withmagnetically levitated master and wrist. pp. 14201426 in ieee international conference on robotics and automation, nice, france.salisbury, j.k. 1985 kinematic and force analysis of articulated hands. pp. 2167 in m.t. masonand j.k. salisbury, eds., robot hands and the mechanics of manipulation. cambridge,mass.: mit press.salisbury, k. 1980 active stiffness control of a manipulator in cartesian coordinates. in conferenceon decision and control, 19th annual symposium on adaptive processes, albuquerque,n.mex.salisbury, k., b. eberman, m. levin, and w. townsend 1990 the design and control of anexperimental wholearm manipulator. pp. 233242 in h. miura and s. arimoto, eds.,robotics research: the 5th international symposium. cambridge, mass.: mit press.sato, t., and s. hirai 1988 meister: a model enhanced intelligent and skillful teleoperationalrobot system. pp. 155162 in r. bolles and b. roth, eds., robotics research: the 4thinternational symposium. cambridge, mass.: mit press.sayers, c., and r. paul 1993 synthetic fixturing. pp. 3746 in advances in robotics, mechatronics and haptic interfaces, dsc49, asme winter annual meeting, new orleans, nov. 28dec. 3.sheridan, t.b. 1960 human metacontrol. in proceedings of annual conference on manual control, wrightpatterson air force base, ohio.1970 on how often the supervisor should sample. ieee transactions on systems science andcybernetics 6(2):140145.1989 telerobotics. automatica 25(4):487507.1992a telerobotics, automation, and human supervisory control. cambridge, mass.: mit press.1992b musings on telepresence and virtual presence. presence: teleoperators and virtualenvironments 1(1):120125.references501virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.1992c defining our terms. presence: teleoperators and virtual environments 1:272274.1993 space teleoperation through time delay: review and prognosis. ieee transactions onrobotics and automation 9:592606.sheridan, t.b., and r.t. hennessy, eds. 1984 research and modeling of supervisory controlbehavior. national research council, commission on behavioral and social sciences andeducation, committee on human factors. washington, d.c.: national academy press.sheridan, t.b., g.j. raju, f.t. buzan, w. yared, and j. park 1989 adjustable impedance, forcefeedback and command language aids for telerobotics . parts 14 in proceedings of thenasa conference on space telerobotics, 8part mit progress report. cambridge, mass.:mit press.smurlo, r.p., and h.r. everett 1992 intelligent security assessment for a mobile robot. inproceedings of sensors exposition. chicago, ill.song, s., and k.j. waldron 1989 machines that walk. cambridge, mass.: mit press.spain, e.h. 1992 effects of camera aiming technique and display type on unmanned groundvehicle performance. final report to the department of defense unmanned groundvehicle joint program office.speeter, t.h. 1992 transforming human hand motion for telemanipulation. presence: teleoperators and virtual environments 1:6379.spooner, m.g., and c.h. weaver 1955 an analysis and analogcomputer study of a forcereflecting positional servomanipulator. technical report reprint no. 264. university ofwisconsin engineering station.starr, g.p., and c.w. wilson 1992 design of a torque controller for the adept2 robot. journal ofintelligent and robotic systems 6:183201.stewart, d.b., d.e. schmitz, and p.k. khosla 1989 chimera ii: a realtime multiprocessingenvironment for sensorbased roto control. pp. 265271 in proceedings of the ieeeinternational symposium on intelligent control.strassberg, y., a.a. goldenberg, and j.k. mills 1992 a new control scheme for bilateralteleoperating systems: lyapunov stability analysis. pp. 837852 in proceedings of theieee international conference on robotics and automation.swartz, b.a. 1993 diver and rov deployable laser range gate underwater imaging systems,underwater intervention 93. pp. 193197 in the 11th annual rov conference andexposition, marine technology society, new orleans, la.tachi, s. 1991 toward virtual existence in real and/or virtual worlds. pp. 8594 inreferences502virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.proceedings of the international conference on artificial reality and teleexistence.tachi, s., h. arai, and t. maeda 1989 development of an anthropomorphic teleexistence slaverobot. pp. 385390 in proceedings of the international conference on advanced mechatronics, tokyo, japan.1990a teleexistence master slave system for remote manipulation. pp. 343348 in proceedings ofthe ieee international workshop on intelligent robots and systems.1990b teleexistence master slave system for remote manipulation (ii). pp. 8590 in proceedings ofthe 29th ieee conference on decision and control.tendick, f., r.w. jennings, g. tharp, and l. stark 1993 sensing and manipulation problems inendoscopic surgery: experiment, analysis, and observation. presence: teleoperators andvirtual environments 2:6681.thorpe, c.e. 1990 vision and navigation: the carnegie mellon navlab. norwell, mass.: kluweracademic publishers.todd, d.j. 1986 fundamentals of robot technology. new york: john wiley.trimmer, w. 1989 micromechanical systems. pp. 1.11.32 in 3rd toyota conference on integratedmicro motion systems, tokyo, japan.tsach, u., t.b. sheridan, and a. buharali 1983 failure detection and location in process control:integrating a new modelbased technique with other methods. in proceedings of americancontrol conference, san francisco.tsai, l.w., and a.p. morgan 1985 solving the kinematics of the most general six and fivedegreeoffreedom manipulators by continuation methods. asme journal of mechanisms, transmissions, and automation in design 107:189200.uhrich, r.w., and j.m. walton 1993 aussnavy advanced unmanned search system. seatechnology (february).vanzandt, t.r., t.w. kenny, and w.j. kaiser 1992 novel position sensor technologies formicroaccelerometers. spie 1694:165171.vertut, j., and p. coiffet 1985a teleoperation and robotics: evolution and development. london:kogan page.1985b teleoperation and robotics: applications and technology. london: kogan page.vijaykumar, r., m.j. tsai, and k.j. waldron 1985 geometric optimization of manipulatorstructures for working volume and dexterity. pp. 228237 in proceedings of the ieeeconference on robotics and automation, st. louis, missouri.references503virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.vischer, d., and o. khatib 1990 design and development of torquecontrolled joints. pp. 271286in v. hayward and o. khatib, eds., experimental robotics 1the first internationalsymposium. new york: springerverlag.waldron, j.j., and r.b. mcgee 1986 the adaptive suspension vehicle. pp. 712 in ieee controlsystems magazinewaldron, k.j., and k.h. hunt 1991 series parallel dualities in actively coordinated mechanisms.international journal of robotics research 10.wallace, r.s., and d.g. taylor 1991 lowtorqueripple switched reluctance motors for directdriverobotics. ieee transactions on robotics and automation 7:733742.walton, j., m. cook, and r. urich 1993 advanced unmanned search systems. pp. 243249 inunderwater intervention 93: the 11th annual rov conference and exposition, marinetechnology society, new orleans, la.wampler, c.w. 1986 manipulator inverse kinematic solutions based on vector formulations anddamped leastsquare methods. ieee transactions on systems, man, and cyberneticssmc16:93101.wen, j.t. 1988 controller synthesis for infinite dimensional systems based on a passivity approach.pp. 724729 in proceedings of the 27th conference on decision and control, austin, tex.whitney, d. 1969 resolved motion rate control of manipulators and human prosthesis. ieeetransactions on manmachine systems mms10:4753.1985 historical perspectives and state of the art in robot force control. pp. 262268 in proceedingsof the ieee conference on robotics and automation.wilcox, b.h. 1992 robotic vehicles for planetary exploration. in c. weisbin, ed., robotic systemsto augment man's capability in space. a special issue of international journal of appliedintelligence 2. norwell, mass.: kluwer academic publishers.wittenburg, r.c. 1987 automobile collision avoidance systems capitalizes on bean steerableantenna. electronic engineering times 2(february).wu, e.c., j.c. hwang, and j.t. chlakek 1993 faulttolerant joint development for the space shuttleremote manipulator system: analysis and experiment. ieee transactions on robotics andautomation 9:675684.yokokohji, y., and t. yoshikawa 1992 bilateral control of masterslave manipulators for idealkinesthetic couplingformulation and experiment. pp. 849885 in proceedings of theieee international conference on robotics and automation.references504virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.yoshichida, f., and k. michitaka 1993 2400mflops reconfigurable parallel vlsi processor forrobot control. pp. 149154 in proceedings of the ieee international conference onrobotics and automation.yoshikawa, t., and t. miyazaki 1989 a sixaxis force sensor with threedimensional crossshapestructure. pp. 249255 in proceedings of the ieee international conference on roboticsand automation, scottsdale, ariz.yun, w., and r.t. howe 1992 recent developments in silicon microaccelerometers. sensors 9(10).yun, w., et al. 1992 surface micromachined, digitally forcebalanced accelerometer with integratedcmos detection circuitry. pp. 126131 in ieee solidstate sensor actuator workshop,hilton head, s.c.zai, l.c., l.f. durfee, d.g. manzer, j.p. karadis, m.p. mastro, and l.w. landerman 1992 controlof a hummingbird minipositioner with a multitransputer marc controller. pp. 543541in proceedings of the ieee international conference on robotics and automation.zhai, s., and p. milgram 1993a human performance evaluation of manipulation schemes in virtualenvironments. proceedings of the ieee virtual reality annual international symposium,seattle, sept.1993b human performance evaluation of isometric and elastic rate controllers in a 6 dof trackingtask. proceedings of the spie, vol. 2057: telemanipulator technology boston, mass.sept. 710.chapter 10: networking and communicationsballart, r., and y.c. ching 1989 sonet: now it's the standard optical network. ieeecommunications magazine 29(3):815.bryson, s., and c. levit 1992 virtual wind tunnel: an environment for the exploration of threedimensional unsteady flows. ieee computer graphics and applications 12(4):2534.cattlett, c.e. 1992 balancing resources. ieee spectrum 29(9):4855.cavanaugh, j.d., and t.j. salo 1992 internetworking with atm wans. research report,minnesota supercomputer center, inc.gelernter, d. 1991 mirror worlds: or the day software puts the universe in a shoebox–how itwill happen and what it will mean. new york: oxford university press .references505virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.habib, i.w., and t.n. saadawi 1991 controlling flow and avoiding congestion in broadbandnetworks. ieee communications magazine 29(10):4653.hsing, r.t., c.t. chen, and j. bellisio 1993 video communications and services in the copperloop. ieee communications magazine 31(1):6268.institute for simulation and training 1991 protocol data units for entity information and entityinteraction in a distributed interactive simulation. military standard (draft). documentnumber iistpd902. orlando, fla.johnson, j.t. 1992 turning the clock ahead on tomorrow's network. data communications 21(12):4362.miller, d.c., a.r. pope, and r.m. waters 1989 longhaul networking of simulators. bbnsystems and technologies corporation research paper.national research council 1994 realizing the information future: the internet and beyond.nrenaissance committee; computer science and telecommunications board;commission on physical sciences, mathematical, and applications, washington, d.c.:national academy press.pausch, r. 1991 virtual reality on five dollars a day. pp. 265270 in proceedings of chi '91. neworleans: acm press.pope, a. 1989 the simnet network and protocols. bbn report no. 7102. cambridge, mass.:bbn systems and technologies.presence 1994 networked virtual environments and teleoperation. special issue.reddy, r. 1992 advanced distributed simulation concept briefing. ads concept brief.rudin, h., and r. williamson 1989 faster, more efficient streamlined protocols. ieeecommunications magazine 27(6):1012.thorpe, j. 1987 the new technology of large scale simulator networking: implications for masteringthe art of warfighting. in proceedings of the ninth interservice industry training systemsconference.chapter 11: evaluation of syntheticenvironment systemsmeister, d. 1985 behavioral analysis and measurement methods. new york: john wiley.references506virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.chapter 12: specific applications of se systemsadam, j.a., ed. 1993 virtual reality is for real. ieee spectrum (october):2229.airey, j.m., j.h. rohlf, and f. p. brooks, jr. 1990 towards image realism with interactive updaterates in complex virtual building environments. computer graphics 24(2):41.anderson, j.r. 1993 cognitive tutors: lessons learned. technical report. pittsburgh, pa.: carnegiemellon university.badler, n.i., c. phillips, and b.l. webber 1993 simulating humans: computer graphics,animation, and control. cambridge, england: cambridge university press.bailey, r.w., a.l. imbmbo, and k.a. zucker 1991 establishment of a laparoscopiccholecystectomy training program. the american surgeon 57(4):231236.bajura, m., h. fuchs, and r. ohbuchi 1992 merging virtual objects with the real world: seeingultrasound images. computer graphics 26(2):203210.bricken, m., and c.m. byrne 1992 summer students in virtual reality: a pilot study oneducational applications of virtual reality technology. technical report from humaninterface technology laboratory of the washington technology center, university ofwashington.brooks, f.p., jr., m. ouhyoung, j.j. blatter, and p.j. kilpatrick 1990 project gropehapticdisplays for scientific visualization. computer graphics: proceedings of siggraph '9024(4).brown, j.s., a. collins, and p. duguid 1989 situated cognition and the culture of learning.educational researcher 18(1):3242.bruckman, a. 199293 identity workshop: emergent social and psychological phenomena in textbased virtual reality. in proceedings of inet 93. available via anonymous ftp frommedia.mit.edu in pub/mediamoo/papers/identityworkshop.{ps,rtf}brunner, b., g. hirzinger, k. landzettel, and j. heindl 1993 multisensory shared autonomy andtelesensorprogrammingkey issues in the space robot technology experiment rotex.pp. 21232131 in proceedings of the ieee/rsj international conference on intelligent robots and systems, yokohama, japanbryson, s. 1992 virtual spacetime: an environment for the visualization of curved spacetimes viageodesic flows. in proceedings of the ieee visualization t92, boston.bryson, s., and m. geraldyamasaki 1992 the distributed virtual wind tunnel. in proceedings ofsupercomputing '92, minneapolis, minn.references507virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.bryson, s., and c. levit 1992 the virtual wind tunnel: an environment for the exploration of threedimensional unsteady flows. computer graphics and applications, julycarey, s. 1987 conceptual change in childhood. cambridge, mass.: mit press.caudell, t.p., and d.w. mizell 1992 augmented reality: an application of headsup displaytechnology to manual manufacturing processes. ieee (january):659669.chapman, r.l., j.l. kennedy, a. newell, and w.c. biel 1959 the systems research laboratory'sair defense experiments. management science 5(3):251269.cognition and technology group at vanderbilt 1993 anchored instruction and situated cognitionrevisited. educational technology 22(3):5270.craig, l.c. 1980 office automation at texas instruments, incorporated. pp. 202214 in m.l. moss,ed., telecommunications and productivity. reading, mass.: addisonwesley.cruzneira, c., d.j. sandin, t.a. defanti, r.v. kenyon, and j.c. hart 1992 the cave: audiovisual experience automatic virtual environment. communications of the acm 35(6):6572.cruzneira, c., d.j. sandin, and t.a. defanti 1993a surroundscreen projectionbased virtualreality: the design and implementation of the cave. computer graphics: proceedings of siggraph '93. august.cruzneira, c., j. leigh, c. barnes, s. cohen, s. das, r. englemann, r. hudson, m. papka, l.siegel, c. vasilakis, d.j. sandin, and t.a. defanti 1993b scientists in wonderland: areport on visualization applications in the cave virtual reality environment. inproceedings of the ieee symposium on research frontiers in virtual reality, october.delp, s., p. loan, m. hoy, f. zajac, e. topp, and j. rosen 1990 an interactive graphicsbasedmodel of the lower extremity to study orthopaedic surgical procedures. ieee transactionson biomedical engineering 37(8 august):757767.denenberg, v.h. 1954 the training effectiveness of a tank hull trainer. humrro technicalreport no. 3., washington, d.c.ellis, s.r., ed. 1993 pictorial communication in virtual and real environments. london: taylorand francis.fanning, t., and b. raphael 1986 computer teleconferencing: experience at hewlettpackard.proceedings of a conference on computersupported cooperative work. new york: theassociation for computing machinery.fisher, s. 1993 current status of vr and entertainment. presentation to the committeereferences508virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.on virtual reality research and development. woods hole, mass., august.fitts, p.m., l. schipper, j.s. kidd, m. shelly, and c. kraft 1958 some concepts and methods for theconduct of system research in a laboratory setting. in g. finch and f. cameron, eds., airforce human engineering, personnel and training research. washington d.c.: nationalacademy of sciences.fogle, r.f. 1992 the use of teleoperators in hostile environment applications. pp. 6166 in ieeeinternational conference on robotics and automation, nice, france.fordyce, s.w. 1974 nasa experiences in telecommunications as a substitute for transportation. washington, d.c.: national aeronautics and space administration. april.fuerzeig, w. 1992 visualization tools for modelbased inquiry. in proceedings of the conferenceon technology assessment. ucla center for technology assessment, los angeles, calif.fujiwara, s., r. kanehara, t. okada, and t. sanemori 1993 an articulated multivehicle robot forinspection and testing of pipeline interiors. pp. 509516 in proceedings of the ieee/rsj international conference on intelligent robots and systems, yokohama, japan.fukuda, t., h. hosokai, and m. otsuka 1987 autonomous pipeline inspection and maintenancerobot with inch worm mobile mechanism. pp. 539544 in proceedings of the ieeeinternational conference on robotics and automation. raleigh, n.c.glenn, a., and l. ehman 1987 computerbased education in social studies. social studiesdevelopment center, indiana university and eric clearinghouse for social studies/social science education.goldenberg, a.a., j. wiercienski, p. kuzan, c. szymczyk, r.g. fenton, and b. shaver 1992 aremote manipulator for forestry operations. pp. 27922795 in ieee internationalconference on robotics and automation, nice, france.greenleaf, w.j. 1994 dataglove and datasuit: virtual reality technology applied to themeasurement of human movement. pp. 6369 in proceedings of medicine and virtualreality. san diego, calif.guetzkow, h., ed. 1962 simulation in the social sciences. englewood cliffs, n.j.: prenticehall.haber, r.n. 1986 flight simulation. scientific american 255(1):96103.hall, d.m., and w.k. walsh 1993 the changing face of textile research. textile world(september):90100.harel, i., and s. papert, eds. 1991 constructionism. norwood, n.j.: ablex publishing corp.references509virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.harrigan, r.w. 1993 automating the operation of robots in hazardous environments. pp. 12111219in proceedings of the ieee/rsj international conference on intelligent robots andsystems, yokohama, japan.hattinger, g., et al., eds. 1990 ars electronica 1990 band ii: virtuelle welten (in german andenglish). linzer veranstaltungsgesellschaft mbh. linz, austria: brucknerhaus.hays, r.t., and m.j. singer 1989 simulation fidelity in training system design. new york:springerverlag.hirose, s. 1987 wall climbing vehicle using internally balanced magnetic unit, romansy 6. pp.420427 in a. morecki, g. bianchi, and k. kedzior, eds., proceedings of the 6th cismiftomm symposium on theory and practice of robots and manipulators. cambridge,mass.: mit press.hirzinger, g., b. brunner, j. dietrich and j. heindl 1993 sensorbased space roboticsrotex andits telerobotic features. ieee transactions on robotics and automation 9:649663.hodges, l.f., j. bolter, e. mynatt, w. ribarsky, and r. van teylingen 1993 virtual environmentsresearch at the georgia tech gvu center. presence 2(3):234243.hunter, i.w., l.a. jones, m.a. sagar, t.d. doukoglou, s.r. lafontaine, p.g. charette, g.d.mallinson, and p.j. hunter 1993 a teleoperated microsurgical robot and associatedvirtual environment for eye surgery. technical report. montreal: mcgill university.kibbee, j.m., c.j. craft, and b. nanus 1961 management games. new york: reinhold publishingcorp.kijima, r., k. shirakawa, m. hirose, and k. nihei 1994 virtual sand box: development of anapplication of virtual environments for clinical medicine. presence 3(1): 4559.kim, w.s., and a.k. bejczy 1993 demonstration of a highfidelity predictive/preview displaytechnique for telerobotic servicing in space. ieee transactions on robotics andautomation 9:698702.kim, w.s., p.s. schenker, a.k. bejczy, and s. hayati, s. 1993 advanced graphics interfaces fortelerobotic servicing and inspection. pp. 303309 in proceedings of the ieee/rsjinternational conference on intelligent robots and systems, yokohama, japan.kozak, j.j., p.a. hancock, e. arthur, and s. chrysler 1993 transfer of training from virtual reality.ergonomics 36:777784.krishnaswamy, g.m., and a.k. elshennawy 1992 concurrent engineering deployment: anenhanced ''customer product" approach. computers and industrial engineering 23(14):503506.references510virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.kwitowski, a.j., w.d. mayercheck, and a.l. brautigam 1992 teleoperation for continuous minersand haulage equipment. ieee transactions on industry applications 28:11181126.leonardo 1994 virtual reality: venus return or vanishing point. leonardo 27:277302.levin, h., et al. 1984 cost effectiveness of four educational interventions. ifg report no. 84a11. stanford university.lineham, t.e., ed. 1993 computer graphics visual proceedings. new york: association forcomputing machinery.loeffler, l.e., and t. anderson, eds. 1994 the virtual reality casebook. new york: van nostrandreinhold.loftin, r.b., b. lee, s. mueller, and r. way 1991 an intelligent tutoring system for physicsproblem solving. in proceedings of physics computer '91, san jose, calif.mann, r.w. 1985 computer aided surgery. in resna 8th annual conference, memphis, tenn.maxis corp. 1991 simant, simearth, simcity: software products and reference manuals. orinda,calif.mccormick, b., t.a. defanti, and m.d. brown, eds. 1987 visualization in scientific computing.computer graphics 21(6).mclane, r.c., and j.d. wolf 1965 final research report on the experimental evaluation ofsymbolic and pictorial displays for submarine control. st. paul, minn.: honeywell, inc.merickel, m.l. 1990 the creative technologies project: will training in 2d/3d graphics enhancekids' cognitive skills? t.h.e. journal 18(5):5558.mizell, d. 1993 virtual reality research at boeing. presentation to the national research council'scommittee on virtual reality research and development.moser, m.a., ed. 1991 virtual seminar on the bioapparatus. banff, alberta: banff centre for thearts.munakata, t., s. murakami, y. matsumoto, s. nakagaki, t. honda, k. shibanuma, s. kakudate, k.oka, t. terakado, and m. kondoh 1993 manipulator for invessel remote maintenance offusion experimental reactor. pp. 12201224 in proceedings of the ieee/rsj international conference on intelligent robots and systems, yokohama, japan.national council of teachers of mathematics 1989 curriculum and evaluation standards forschool mathematics. national council of teachers of mathematics. reston, virginia.references511virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.nishi, a., y. wakasugi, and k. watanabe 1986 design of a robot capable of moving on a verticalwall. advanced robotics 1(1):3346.office of technology assessment 1988 power on! new tools for teaching and learning. otaset379. washington, d.c.: government printing office.okada, t., and t. kanade 1987 a threewheeled selfadjusting vehicle in a pipe, ferret1.international journal of robotics research 6(4):6075.orlansky, j., and j. string 1977 costeffectiveness of flight simulators for military training, vol.1. arlington, va.: the institute for defense analyses.palmiter, s., and j. elkerton 1993 animated demonstrations for learning procedural computerbasedtasks. humancomputer interaction 8:193216.pantelidis, v.s. 1993 virtual reality and education: a bibliography. available via anonymous ftpfrom u.washington.edu in the directory /pub/usersupported/virtualreality/misc/papersunder the filename pantelidsvreducationbibl.txt.piaget, j., and b. inhelder 1967 the child's conception of space. new york: norton.pimentel, k., and b. blau 1994 system architecture issues related to multiple user vr systems:teaching your system to share. ieee computer graphics and applications, january.potemkin, e., p. astafurov, a. osipov, m. malenkov, v. mishkenyuk, and p. sologub 1992remotecontrolled robots for repair and recovery in the zones of high radiation levels. pp.8082 in ieee international conference on robotics and automation, nice, france.regian, w.e., and w.i. shebilske 1992 virtual reality: an instructional medium for visualspatialtasks. journal of communication 42(4):136149.ritchie, m.l., and f.a. muckler 1954 retroaction as a function of discrimination and motorvariables. journal of experimental psychology 48(6):409415.rolfe, a. 1992 the problem of maintenance of the joint european torus tokamak fusion nuclearexperiment. spar journal of engineering and technology 1:713.rosen j., a. laskoharvill, and r. satava 1994 virtual reality and surgery. in g. burdea, ed.,computer integrated surgery. cambridge, mass.: mit press.sanders, j.h., and f.j. tedesco 1993 telemedicine: bringing medical care to isolated communities.journal of the medical association of georgia (may):237241.references512virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.satava, r. 1993a surgery 2001: a technologic framework for the future. surgical endoscopy7:111113.1993b virtual reality surgical simulator: the first steps. surgical endoscopy 7:203205.sauder, b.j., p.d. lawrence, and u. wallensteiner 1992 coordinated control systems applied to theforest industry. pp. 5255 in proceedings of the forest sector 2000, istc.schmidt, r.a., and d.e. young 1987 transfer of movement control in motor skill learning. pp.4759 in s.m. cormier and j.d. hagman, eds., transfer of learning: contemporary research and applications. san diego: academic press.schug, m.c., and h.s. kepner, jr. 1984 choosing computer simulations in social studies. thesocial studies 75(sept/oct):211215.secretary's commission on achieving necessary skills 1991 what work requires of schools: ascans report for america 2000 . secretary's commission on achieving necessary skills,u.s. department of labor.sheridan, t.b. 1993a cooperation among humans and among humans and machines. talkdelivered at woods hole, mass., august.1993b space teleoperation through time delay: review and prognosis. ieee transactions onrobotics and automation 9:592606.sheridan, t.b., and w.r. ferrill 1974 man machine systems. cambridge, mass.: mit press.shirazi, e. 1991 telecommuting: moving the work to the workers. washington, d.c.: u.s.department of transportation.sinaiko, h.w., and t.g. belden 1965 the indelicate experiment. pp. 343348 in spiegel, j., andd.e. walker, eds., second congress on the information system sciences. washington,d.c.: spartan books.song, d., and m.l. norman 1993 cosmic explorer: a virtual reality environment for exploringcosmic data. in proceedings of ieee symposium on research frontiers in virtual reality,october.steward, a. 1993 virtual reality applications to the textile industry. presentation to the nationalresearch council's committee on virtual reality research and development. woodshole, mass., august.stix, g. 1992 reach out: touch is added to virtual reality simulations. scientific american 264(2):134.stone, h.w., and g. edmonds 1992 hazbot: a hazardous materials emergency response mobilerobot.references513virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.pp. 6773 in ieee international conference on robotics and automation, nice, france.taylor, e.r., w. robinett, v.l. chi, f.p. brooks, jr., w.v. wright, r.s. williams, and e.s. snyder1993 the nanomanipulator: a virtual reality interface for a scanning tunnelingmicroscope. proceedings of siggraph 1993. anaheim, calif.taylor, t. 1980 telecommunications and public safety. pp. 280288 in m.l. moss, ed.,telecommunications and productivity. reading, mass.: addisonwesley.thorndike, e.l. 1903 educational psychology. new york: lemcke and buechner.thorpe, j. 1993 synthetic environments strategic plan. draft 3b. defense advanced researchprojects agency, alexandria, va.tinker, p. 1993 the rockwell international virtual reality laboratory. rockwell internationalcorporation, thousand oaks, calif.trivedi, m.m., and c.x. chen 1993 developing telerobotic systems using virtual reality concepts.pp. 352359 in proceedings of the ieee/rsj international conference on intelligentrobots and systems, yokohama, japan.tyre, t. 1989 live t.v. broadcasts from ocean floor bring new depth to science education. t.h.e.journal august:4246.ullman, n. 1993 hightech connection between schools and science expeditions enlivens classes.wall street journal march 17:b1 and b10.vpl research, inc. 1989 virtual reality at texpo '89. redwood city, calif.: vpl research, inc.1991 brain tour. videotape for lederle, inc., demonstration. may.wallensteiner, u., p. stager, and p. lawrence 1988 a human factors evaluation of teleoperatedhand controllers. pp. 291296 in international symposium on teleoperation and control.warner, d., t. anderson, and j. johanson 1994 biocybernetics: a biologically responsiveinteractive interfacethe next paradigm of human computer interaction. pp. 237241 inproceedings of medicine and virtual reality. san diego, calif.weghorst, s., j. prothero and t. furness 1994 virtual images in the treatment of parkinson's diseaseakinesia. conference proceedings: virtual reality meets medicine ii. san diego, calif.whitcomb, l.l., and d.r. yoerger 1993 a new distributed realtime control system for the jasonunderwater robot. pp. 368374 in proceedings of the ieee/rsj international conference on intelligent robots and systems, yokohama, japan.white, b. 1984 designing computer games to help physics students understand newton's laws ofmotion. cognition and instruction 1(1):69108.references514virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.william, a., c.s. cooper, and r.j. fisher 1993 predictors of laparoscopic complications afterformal training in laparoscopic surgery. journal of the american medical association 270(22):26892692.williams, a.c., and r.e. flexman 1949 an evaluation of the link snj operational trainer as anaid on contact flight training. port washington, n.y.: naval special devices center.yam, p. 1993 surreal science: virtual reality finds a place in the classroom. scientific american 268(february):103ff.references515virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.references516virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.appendixesappendixes517virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.appendixes518virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.abiographical sketchesnathaniel durlach (chair) is a senior scientist in the departmentof electrical engineering and computer science at the massachusetts instituteof technology and has been codirector of the sensory communication groupin the research laboratory of electronics there for over 20 years. he has alsobeen a visiting scientist in the biomedical engineering department of bostonuniversity for five years. he received an m.a. degree from columbiauniversity in mathematics and took courses at harvard university inpsychology and biology. he is the author (or coauthor) of numerous bookchapters and refereed articles in such journals as perception and psychophysicsand the journal of the acoustical society of america; he continues to reviewarticles, proposals, and research programs in the field of psychophysics; and hehas received the silver medal award for outstanding work in psychoacoustics bythe acoustical society of america. recently, his research interests have focusedon teleoperator and virtual environment systems, with special emphasis on thehumanmachine interfaces used in such systems. he is cofounder and directorof the mit virtual environment and teleoperator research consortium, aswell as cofounder and managing editor of the new mit press journal presence:teleoperators and virtual environments.steve bryson is an employee of computer sciences corporationworking under contract for the applied research office of the numericalaerodynamics simulation systems division at the nasaames researchcenter. his current research involves the application of virtual reality519biographical sketchesvirtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.techniques for scientific visualization, of which the virtual wind tunnel is themain focus. he began work in the virtual reality field in 1984 at vpl research,working on a graphicsbased programming environment using the prototypedataglove for input; later he was involved in work on the dataglove model ii.he joined the view lab at the nasaames research center in 1987, where hewas involved in integrating the various inputoutput and graphics systems into avirtual environment. this included research in software architectures for virtualreality systems and human factors. in 1991, he was cochair of the ieeesymposium on research frontiers in virtual reality and is program cochair ofthe ieee virtual reality annual international symposium.norman hackerman is chairman of the scientific advisory boardat the robert a. welch foundation. he is president emeritus of rice university,where he was president and a professor of chemistry for 15 years. prior to that,he had a long and distinguished career at the university of texas at austin,where he served as president and held various positions, including director ofthe corrosion research laboratory and on state boards and on committeesfocusing on various aspects of research and education. he was technical editorand then editor of the journal of the electrochemical society from 1950 to1990. he has served on a dozen national academy of science/nationalresearch council panels and committees and is a past chairman of the board onenergy studies and of the commission on physical sciences, mathematics, andapplications. he is the author or coauthor of more than 200 publications. hehas a ph.d. in chemistry from johns hopkins university.john m. hollerbach is professor of computer science at theuniversity of utah. from 1989 to 1994 (including his time of membership onthe committee) he was the natural sciences and engineering/canadian institutefor advanced research professor of robotics at mcgill university. from 1982to 1989 he was on the faculty of the department of brain and cognitivesciences and a member of the artificial intelligence laboratory at themassachusetts institute of technology. he received a ph.d. in computerscience from mit in 1978. he was a member of the administrative committeeof the ieee robotics and automation society from 1989 to 1993. he is atechnical editor of the ieee transactions on robotics and automation,treasurer of the ieee/asme journal of microelectromechanical systems, and asenior editor of presence.james r. lackner is riklis professor of physiology and director ofthe ashton graybiel spatial orientation laboratory at brandeis university. hereceived b.sc. and ph.d. degrees from the massachusetts institute of520biographical sketchesvirtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.technology. he has served on the national research council's committee onvision from 1986 to 1992 and the committee on space biology and medicinefrom 1990 to the present. from 1986 to 1990 he was provost of brandeisuniversity. he is the section editor for spatial orientation of the journal ofvestibular research and is on the editorial board of presence. his researchinterests concern human movement and orientation control in unusual sensoryand force environments.herbert s. lin is senior staff officer for the computer science andtelecommunications board. previously he was a professional staff member forthe house armed services committee under representative les aspin workingon strategic modernization and arms control issues. he has also worked as aresearcher, instructor, and visiting scholar at the massachusetts institute oftechnology, cornell university, and the university of washington. he receiveda ph.d. in physics from the massachusetts institute of technology.anne s. mavor is study director for the committee on virtual realityresearch and development and for the committee on human factors. herprevious work as a national research council senior staff officer has included astudy of modeling cost and performance of military enlistment, a review offederally sponsored education research activities, and a study to evaluateperformance appraisal for merit pay. for the past 25 years her work hasconcentrated on human factors, cognitive psychology, and information systemdesign. prior to joining the nrc she worked for the essex corporation, ahuman factors research firm, and served as a consultant to the college board.she has an m.s. in experimental psychology from purdue university.j. michael moshell is associate professor of computer science atthe university of central florida. he received a ph.d. in computer science fromohio state university in 1975 and spent the next 10 years at the university oftennessee. he currently serves as chief scientist of the visual systemslaboratory in the institute for simulation and training of the university ofcentral florida. he is active in the association for computing machinery and isan associate editor for the journal presence. his research interests concern theapplication of simulation and virtual environments to education and training.randy pausch is an associate professor of computer science at theuniversity of virginia. he received a b.s. in computer science from brownuniversity and a ph.d. in computer science from carnegie mellon university.he is a national science foundation presidential young investigator521biographical sketchesvirtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.and a lilly foundation teaching fellow. his primary interests are humancomputer interaction and undergraduate education.richard w. pew is a psychologist with bolt beranek and newman,inc., where he is principal scientist and manager of the cognitive sciences andsystems department. from 1960 to 1974 he was at the university of michigan,where he received a ph.d. and was on the faculty for 11 years. he was the firstchairman of the national research council's committee on human factors.throughout his career he has been concerned with issues related to humanperformance and system design, ranging from the design of a control panel foran advanced music synthesizer to the impact of the introduction of automationinto aircraft cockpits.warren robinett is a designer of interactive computer graphicssoftware and hardware and president and founder of virtual reality games,inc., a developer of virtual reality video games for the home market. in 1978, hedesigned the atari video game adventure, the first graphical adventure game.in 1980, he was cofounder and chief software engineer at the learningcompany, a publisher of educational software. there he designed rocky'sboots, a computer game that teaches digital logic design to 11yearoldchildren. rocky's boots won software of the year awards from three magazinesin 1983. in 1986 robinett worked as a research scientist at the nasaamesresearch center, where he designed the software for the virtual environmentworkstation, nasa's pioneering virtual reality project. from 1989 to 1992 atthe university of north carolina, he directed the virtual reality andnanomanipulator projects. he is an associate editor for the journal presence.joseph rosen is an associate professor of plastic and reconstructivesurgery at dartmouthhitchcock medical center and chief of the section onplastic and reconstructive surgery at the veterans administration medicalcenter in white river junction, vermont. he is also adjunct professor ofengineering at the thayer school of engineering at dartmouth college. hisclinical specialty is complex microsurgical reconstructions of congenital andacquired problems in plastic and reconstructive surgery. he operates on andreconstructs all parts of the body and interacts with surgeons in every specialty.his research interests include bionics, humanmachine interfaces, nerve repairand evaluation, artificial nerve grafts, transplantation of limbs, and computersimulations of complex surgical procedures.mandayam a. srinivasan is a principal research scientist in thedepartment of mechanical engineering at the massachusetts institute oftechnology and a member of the sensory communication group in the522biographical sketchesvirtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.research laboratory of electronics at mit. after receiving degrees in civil andaeronautical engineering in india, he received a ph.d. in mechanicalengineering from yale university. before moving to mit in 1987, he was amember of the research faculty in the department of anesthesiology at the yaleuniversity school of medicine. his research interests include all aspects ofhuman sensorimotor interactions with environments, especially in the context ofhuman and machine haptics. he is a founding member of the mit virtualenvironment and teleoperator consortium and is on the editorial board of thejournal presence.james j. thomas is a technology manager at the applied physicscenter at battelle pacific northwest laboratories in richland, washington. hespecializes in the research, design, and implementation of innovativeinformation systems technology. prior to working at battelle, he worked atgeneral motors research laboratories. he has been named among the top 100scientific innovators by science digest and among the top 100 innovators inscience and technology by research and development. in addition, he wasawarded the federal laboratories consortium technology transfer award forinnovation in transferring research technology to industry and universities. hepublishes, has been editor of scientific journals, gives invited talks, teaches asan adjunct associate professor for washington state university, lectures forprofessional short courses, and is actively involved as a motivator and leader incomputer graphics and human interface technology at the professional societysiggraph. he was cochair for the siggraph annual conference in 1987,vice chair for siggraph from 1987 to 1989, and chair of acm siggraphfrom 1989 to 1992.andries van dam is l. herbert ballou university professor andprofessor of computer science at brown university. he has been on brown'sfaculty since 1965 and was one of the founders of the department of computerscience and its first chairman, from 1979 to 1985. his research has concernedcomputer graphics, textprocessing and hypermedia systems, and workstations.he has been working for more than 25 years on systems for creating andreading electronic books, based on highresolution interactive graphics systems,for use in teaching and research. he received a ph.d. from the university ofpennsylvania. introduction to computer graphics, coauthored with j. foley, s.feiner, j. hughes, and r. phillips, was published in 1993. he has also authoredor coauthored numerous other books and papers.elizabeth wenzel is director of the spatial auditory displays labin the aerospace human factors research division at the nasaamesresearch center, directing development of realtime display technology and523biographical sketchesvirtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.conducting basic and applied research in auditory perception and localization inthreedimensional virtual acoustic displays. she is an associate editor of thejournal presence and has published a number of articles and spoken at manyconferences on the topics of virtual acoustic environments. she received a ph.d.in cognitive psychology with an emphasis in psychoacoustics from theuniversity of california, berkeley. from 1985 to 1986 she was a nationalresearch council postdoctoral research associate at nasaames, working onthe auditory display of information for aviation systems.andrew witkin is a professor of computer science at carnegiemellon university. previously he was director of the perception and graphicsgroups at schlumberger's palo alto research lab. he received a b.a. inpsychology from columbia college and a ph.d. in psychology from themassachusetts institute of technology. he has published extensively in theareas of computer vision and computer graphics. he serves as an associateeditor for acm transactions on graphics, has served on numerous conferenceprogram committees, and is a fellow of the american association for artificialintelligence. his awards include best paper prizes at the national conference onartifical intelligence and the international joint conference on artificialintelligence; the grand prix for animation at the 1987 parigraph competition inparis, france; and the grand prix for computer graphics at the prix arselectronica 1992 in linz, austria.eugene wong is the pro vice chancellor for research and developmentat the hong kong university of science and technology; during his time ofservice on the committee he was at the university of california, berkeley. heassumed his current position upon his recent retirement as professor ofelectrical engineering and computer science at the university of california,where he had a 32year career as a faculty member, most recently as departmentchairman. in january 1993, he completed a threeyear stint as associate directorof the white house office of science and technology policy. his researchinterests have spanned a wide range of topics, two principal ones beingstochastic processes and their applications and database management systems.he was one of the architects of ingres, a pioneering relational databasemanagement system, and contributed to the development of query processing,database semantics, and distributed systems. he has been a consultant to anumber of major corporations and was a founder of the ingres corporation.he received b.s., a.m., and ph.d. degrees in electrical engineering fromprinceton university.524biographical sketchesvirtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.michael zyda is a professor in the department of computer scienceat the naval postgraduate school. he is also the academic associate andassociate chair for academic affairs in that department. his main focus inresearch is in the area of computer graphics, specifically the development oflargescale, networked, threedimensional virtual environments and visualsimulation systems. he is the senior editor for virtual environments for thequarterly presence; for that journal, he has coedited special issues on pacificrim virtual reality and telepresence.525biographical sketchesvirtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.bcontributorsmany individuals contributed to the committee's thinking and its draftingof various sections of the report by serving as presenters, consultants, andadvisers. the list below acknowledges these contributors and their affiliations.bishnu atal, american telephone & telegraphkurt akeley, silicon graphics, inc.walter aviles, massachusetts institute of technologynorman badler, university of pennsylvaniapaul barham, naval postgraduate schoolklaus biggers, university of utahwilliam bricken, university of washingtonmartin buehler, mcgill universitymatt conway, university of virginiahari das, jet propulsion laboratorythomas defanti, university of illinoischicagopaul dizio, brandeis universityjohn falby, naval postgraduate schoolscott foster, crystal river engineeringeric foxlin, massachusetts institute of technologyblake hannaford, university of washingtonvincent hayward, mcgill universityeric howlett, leep systemsian hunter, mcgill universitycharles hutchenson, dartmouth college526contributorsvirtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.bonnie john, carnegie mellon universitykenneth kaplan, harvard universitykristen m. kelleher, naval postgraduate schoolthomas knight, massachusetts institute of technologyarthur kramer, university of illinoisronald kruk, cae electronicsjaron lanier, consultantjengfeng lee, massachusetts institute of technologyming c. lin, naval postgraduate schoolmichael r. macedonia, naval postgraduate schooljohn makhoul, bbn laboratories, inc.david mizell, boeingjoshua mogal, silicon graphics, inc.steve molnar, university of north carolinathomas piantanida, sri internationaldavid pratt, naval postgraduate schoolrichard satava, advanced research projects agencybarbara shinncunningham, massachusetts institute of technologykenneth stevens, massachusetts institute of technologyalan steward, apparel cim centersusumu tachi, university of tokyothomas wiegand, massachusetts institute of technologyjames winget, silicon graphics, inc.thomas zeffiro, national institutes of healthdavid zeltzer; massachusetts institute of technologyadvisersscott fisher, telepresencerichard held, massachusetts institute of technologythomas sheridan, massachusetts institute of technologygeorge zweig, los alamos national laboratoryconsultantharold van cott, national research councilliaisonlawrence w. stark, committee on human factorssponsorsbernard corona, human research and engineering directorate, armyresearch laboratory527contributorsvirtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.brenda thein, human research and engineering directorate, armyresearch laboratoryjohn tangney, air force office of scientific researchcol. william strickland, human resource directorate, armstronglaboratory, brooks afbhendrick ruck, human resource directorate, armstrong laboratory,brooks afbkenneth boff, human engineering division, crew systems directorate,armstrong laboratory, wrightpatterson afbrobert eggleston, human engineering division, crew systemsdirectorate, armstrong laboratory, wrightpatterson afbclaire gordon, u.s. army natick r&d centerjames sampson, u.s. army natick r&d centerjames jenkins, national aeronautics and space administrationy. t. chien, national science foundationjohn hestenes, national science foundationgeorge cotter, national security agencynorman glick, national security agencysharon stanfield, sandia national laboratory528contributorsvirtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.indexaabdominal simulators and models, 397,398, 400, 401acoustetron, 148acoustic trackers, 51, 199200, 203actuators, 6, 63, 80, 81, 310313adaptation and readaptation, 4344, 71,102, 103, 104, 112, 143144,147148, 159, 207, 219additive synthesis, 154155adoptive control of thought (act) theory, 106advanced research projects agency(arpa), 235, 363, 385, 390advanced technology program, 84advanced unmanned search system(auss), 323aerospace applications, 389, 392395, 435airline travel information service(atis), 235236american national standards institute(ansi), 366american textile partnership (amtex),390anatomical modeling, 38, 6970, 395,396397, 398399, 400401, 404405anchored instruction, 424n, 429, 431apparel cim center, 391applications, 34, 15, 3537, 6871,381385.see also defense applications; design, manufacturing, and marketing;education applications;entertainment and games;hazardous operations;information visualization;medical and health care applications;scientific visualization;telecommunications;teletravel;training applicationsarchitectural visualization, 57, 251, 271,388, 389aerospace applications, 394395for disabled persons, 403404argonne national laboratory, 176, 406argo submersible vehicle, 412arm movements, 188189, 209210index529virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.artistic expression, 382, 383384ascension bird sensor, 193, 194astrophysics, 436asynchronous mode technology (atm),364, 367atomic force microscopes (afms), 313at&t, 175, 364, 365, 439attitudes, 45, 108, 109auditory channel, 23, 46, 4850, 79, 80,82, 134144, 248, 249in anatomical modeling, 70in augmentedreality systems, 145, 153display and hardware, 23, 47, 4849,134, 136137, 141, 144153, 159information transfer rates, 96, 135136movement and directional cues, 214,225226, 229, 442resolution, 94, 135scene analysis, 50, 141143and sensory substitution, 50, 95, 100,143144, 159synthetic display, 49, 50, 153159, 160in training applications, 415augmentedreality systems, 2, 1921, 95,145, 153, 432image registration and alignment, 5, 15,48, 74, 75, 79, 404medical applications, 38, 397, 398399,400401, 404, 436in production line, 37, 393394software, 5, 60, 79, 275276, 295296autocad, 273autodesk cyberspace project, 426427automobile industry, 389390, 394autonomic nervous system, 240243autonomous agents, 5, 22, 60, 77, 7980,281286, 413in teleoperations, 306, 354, 361autosevocom network, 439autostereoscopic displays, 121, 124128,132bbehavioral therapy, 404binocular vision, 118biocybernetic controllers, 402boeing, 392, 393394boom viewers, 130, 132, 192boston computer museum, 426brain activity, 243245bulk transport, 411412bulletin board systems, 427ccad, see computeraided designcae electronics, 128cam, see computeraided manufacturingcanada, 355, 383, 408, 417carnegie mellon university (cmu), 324cathode ray tubes (crts), 122, 123, 124,132cave display system, 152153, 428, 442celesco transducer products, 200chargedcouple device (ccd) arrays,194, 197, 204children's hospital, boston, 403chrysler corp., 389cim, see computerintegrated manufacturingcircuit design, 436, 440circular vection, 213cleanroom operations, 408coding systems, 9697cognitive function, 99, 243cognitive models, 4, 4445, 71, 72, 93,99, 105107, 110training applications, 45, 71, 106, 110,418419collaboration, 97in design and manufacturing, 387, 388,389, 392, 393, 437distributed, 42, 437441interdisciplinary, 1314, 65, 8586, 375learning, 425index530virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.collision, 156157, 185, 326328, 394, 407color sim eye, 129comfort, see ergonomics and comfortissuescommunications, see networks and distributed systems;telecommunicationscompetition and market forces, 89, 86, 88compression algorithms, 256257computeraided design (cad), 59, 273,295, 384, 387aerospace, 392394computeraided manufacturing (cam),37, 387computerassisted tomography (ct), 397,400computerintegrated manufacturing(cim), 387, 391computers, see hardware technologies;softwareconcurrent model, 259consortia, 64, 389390constructionism, 424425continuous motion illusion, 9495contrast sensitivity function (csf),117118convolvotron, 148, 149coriolis forces, 208, 209, 210, 221costeffectiveness, 36, 37, 65, 67, 374, 375educational applications, 41, 429430and human performance characteristics,43, 94, 110medical applications, 405teleoperator systems, 39, 412413training applications, 40, 70, 415, 416,422crossover model, 98cruise missiles, 411curv remote vehicle, 412cyberface, 128, 129, 130ddataglove, 177, 242, 314, 403404, 463datasuit, 177, 403decisionmaking, 432modeling, 45, 9899defense applications, 36, 73, 87, 128129,382, 384385networks, 36, 6364, 83, 363, 369370,385, 439teleoperator systems, 36, 305, 320321,384, 411, 438defense simulation internet, 363, 370definitions, see terminology and definitionsdeleterious effects, see motion sickness;psychological issues, side effects;sopite syndromedemand activated manufacturing architecture (dama), 390department of defense (dod), 87, 320,363, 384385department of energy (doe), 86, 87,390, 406depth perception, 118, 119120design, manufacturing, and marketing, 3,3738, 68, 384, 387395collaboration and data sharing, 387,388, 389, 392, 393, 437shopping vignette, 2628see also aerospace applications;production lines;prototypes;textile industrydextrous arm, 309, 312dextrous hand, 177, 242, 313, 314, 315dextrous teleoperation system master,176disability and rehabilitation, 3839, 100,402404education applications, 107, 431nphysiological interfaces, 239, 242, 245,402403psychological, 39, 404tactile communication aids, 169, 177index531virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.discomfort, see ergonomics and comfortissuesdisplays, see auditory channel;forcefeedback display systems;graphical displays and imaging;haptic channel;headmounted display systems;offhead displays;seethrough displays;spatial display and resolution;tactile information and feedback;visual channeldistributed interactive simulation (dis),64, 385distributed systems, see networks anddistributed systemsdynamic time warping (dtw), 232233eearphones, 23, 47, 49, 138139, 145149education applications, 34, 4041, 6869,423432cognitive modeling, 45, 71, 110in entertainment domain, 41, 69, 107,423, 427, 430, 431medical, 6970, 395, 396397networks, 40, 69, 107, 425426, 438research agenda, 430432vignette, 2832see also training applicationselectrochromic polymers, 124electrodermal activity (eda), 242243electromagnetic sensors, 317electromyography (emg), 242electronic visualization laboratory, 152,436electrostatic actuators, 312electrotactile displays, 177emotional response, 240241emu morpheus synthesizer, 158endoscopy, 398, 399engineering applications, 387, 394,434435enhancing distortions, 44, 48, 71, 93, 103,119120, 409entertainment and games, 3, 4, 15, 3536,46, 67, 381, 382383education applications, 41, 69, 107, 423,427, 430, 431networks, 63, 88, 363, 365, 372, 383roleplaying, 45, 109, 427, 431violence in, 45, 109, 383ergonomics and comfort issues, 3, 47, 67,94, 110, 239240haptic interfaces, 175176, 186headmounted displays, 47, 73,104105, 121, 131, 437ethernet, 64, 366europe, 16, 383evaluation and testing, 7, 6566, 8384,373378costeffectiveness, 37, 65, 374, 375interfaces, 75, 375, 376378positiontracking devices, 204standardization, 7, 83, 84, 378teleoperations, 337338, 345346training applications, 70, 374, 375, 415,416, 420, 421422evans & sutherland, 299, 302eventdriven model, 258259eventrelated potentials (erps), 243exos, 177, 242, 314exoskeleton, 52, 172, 173174, 176177,178, 180explosive ordnance disposal, 411eyegen3, 129eyephone, 128, 129, 417eyes, see visual channelffacilities design, see production linesfake space labs, 128, 130, 192federal coordinating council for science,engineering, and technology (fccset), 387388federal research and development, 8, 16,7677, 79, 8687, 381382fiber channel, 367fiber distributed data interface (fddi),6465, 366fidelity, 45, 376, 419, 421index532virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.field of view, 73, 117, 121, 132firefighting and fire rescue, 246, 410411,413, 420421fixation point, 112, 113flicker, 117flight simulators, 210, 220222, 251,271272, 414, 416, 420forcefeedback display systems, 16,2324, 5253, 172, 173176, 178,184, 394, 436sensory substitution in, 100in teleoperator systems, 332, 333334,355356, 409tracking in, 189, 193ford motor co., 389fovea, 48, 112, 114, 115frame rates, 57, 117, 249250, 287288freedom graphics hardware, 299, 302frequency modulation (fm), 155, 157functional fidelity, 419ggalvanic skin reflex (gsr), 242games, see entertainment and gamesgams tracker, 199200general electric, 128general motors, 389general relativity, 435436geometric modeling, 273275, 294295georgia institute of technology, 404germany, 16, 408, 409glasses, see headmounted display systemsgloves, 52, 177, 180, 191193, 199, 417,421dataglove, 177, 242, 314, 403404, 463goniometers, 51, 191192, 202government policy and infrastructure,89, 8488gp83d tracker, 199, 200graphical displays and imaging, 42, 56,77, 111, 249250hardware, 5758, 251261, 296303,394, 430medical applications, 38, 395, 398, 404in teleoperation systems, 8182,344345, 407, 409see also augmentedreality systems;information visualizationgraphics, visualization, and usabilitycenter, 404graphic user interface (gui), 264, 265, 266greenleaf medical systems, 403green telepresence system, 401402grope project, 176, 436groundbased linkage devices, 51, 52,173174, 175176, 192193, 203in robotics, 309gustatory channel, 23, 55, 245gyrochip, 200gyroengine, 200hhand tracking, 188189, 192, 193, 266267haptic channel, 5, 16, 23, 46, 5253, 72,74, 161174, 248249hand and arm tracking, 188189, 192,193, 266267hardware development, 53, 174177,180182, 184185, 186, 394illusions, 95, 166, 209information transfer rates, 9697in medical applications, 70, 398, 399, 404sensorimotor resolution, 94, 177and sensory substitution, 100, 186software development, 53, 79, 80,178180, 185in telecommunications, 42teleoperator systems, 52, 62, 81, 176,177, 179180, 189, 304, 305,315317, 345see also augmentedreality systems;information visualizationhardware technologies, 3, 5, 5758, 67,7576, 251261, 289290index533virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.auditory channel, 23, 47, 4849, 134,136137, 141, 144153, 159computation and data management capability, 254260, 290graphics, 5758, 251261, 296303, 394,430haptic, 53, 174177, 180182, 184185,186, 394networks, 367, 370371telerobotics, 6, 6263, 8081, 349353,359360training applications, 415, 421hazardous operations, 39, 100, 405412research agenda, 3, 68, 412413telerobotics, 4, 39, 46, 102, 304, 305,381, 405413, 438, 441see also firefighting and fire rescue;security applications;space applications;underwater applicationshazardous waste disposal, 406407headmounted display (hmd) systems,23, 4748, 73, 104, 120121, 123,128130, 131, 426auditory, 23, 47, 49, 138139, 145149in augmentedreality systems, 47, 73,121, 393, 394, 397comfort issues, 47, 73, 104105, 121,131, 437in entertainment, 3536, 47, 73ergonomics issues, 47, 73, 104105,121, 131, 437and motion sickness, 208209, 215in scientific visualization, 436, 437seethrough glasses, 47, 73, 119, 440,441, 442head movement and tracking, 47, 59, 121,131, 151, 189190, 193, 201,207209, 912headrelated transfer functions (hrtfs),147148, 159health care, see medical and health careapplications;telemedicineheat, 55, 245, 246heavy machinery, 305, 409helios, 353hidden markov modeling (hmm),232233, 234, 235, 236237high performance computing and communications (hpcc) program, 15,64, 84, 364365highperformance parallel interface(hppi), 366367holographic displays, 124, 125, 127128hubbell telescope, 420human interface technology (hit) laboratory, 124, 426humanmachine interface, 1, 45, 21,4647, 7275in production lines, 389in rehabilitation systems, 3839, 402403in teleoperator systems, 1718, 2122,305307, 329330in virtual environment systems, 2, 18,2122see also auditory channel;gloves;haptic channel;headmounted display systems;locomotion interfaces;multimodal interfaces;offhead displays;visual channelhuman operator models, 43, 97100human performance characteristics, 3, 15,43, 65, 67, 375, 376378, 392, 393information transfer rates, 43, 9597manual control, 43, 93, 9799research needs, 4, 71, 110, 422sensorimotor resolution, 43, 9495,99100, 104see also illusions;sensorimotor loop alterationshumidity, 55, 245hydraulic actuators, 312hypermedia integration, 61, 286287iic sensors, 200identical elements, 418index534virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.illusions, 43, 9495, 264haptic channel, 95, 166, 209wholebody motion, 95, 206207,213215imaging, see augmentedreality systems;graphical displays and imaging;information visualizationincarceration, 43, 441inertial tracking, 51, 74, 200201, 203204information transfer rates, 43, 9597auditory channel, 96, 135136visual channel, 96, 112information visualization, 3, 4, 4142, 68,69, 251, 254, 256, 384, 432433cognitive modeling, 45, 71, 110scientific, 42, 45, 433437infrared light emitting diodes (ireds),194, 195integrated services local area networkinterface, 65intel corporation, 426intelligent tutors, 106, 282interdisciplinary cooperation, 1314, 65,8586, 375and communication difficulties, 2, 14,17, 86interface, see forcefeedback display systems;humanmachine interface;locomotion interfacesinterior design, 388, 389international competition, 1517, 390internet, 64, 364, 365, 368, 427jjapan, 1617, 128, 176, 383, 408jason remote vehicle, 412, 425426joints, 165167, 171, 191192, 202robotic, 6, 81, 308310, 314316, 360joysticks, 23, 52, 172, 173, 175176, 180kkaiser electronics, 128, 129kemar mannequin, 148kinematics, 308310kinesthetics and proprioception, 100,163164, 165167, 170, 171, 226227knowledge, see cognitive modelskopin, inc., 123124llaparoscopic surgery, 38, 398, 400laser interferometers, 198199, 203laser radar (ladar), 194, 197198, 203,327laser scanning, 60, 124, 194, 196199,203, 274, 295learning, see cognitive models;education applications;training applicationsleep optics systems, 128, 130lenticular displays, 124125libraries and data bases, 85, 261262,362, 399limb position and motion, 99, 165167,171, 202linear operator models, 98linear vection, 213214link trainer, 414liquid crystal displays (lcds), 122, 123,124loading equipment, 409, 411412localarea networks (lans), 6465,366367locomotion interfaces, 5, 5455, 72, 74,205, 206, 211212, 223224, 228229loma linda university medical center,402, 403loudspeakers, 49, 145, 151153lunakhod lunar rover, 407408lunar operations, 407408mmagnetic bearings and levitation, 311312magnetic resonance angiography (mra),245index535virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.magnetic resonance imaging (mri), 243,244245, 397, 400magnetic tracking, 51, 193194, 203magnification, 44, 103, 116117, 144manipulation, 52, 162163, 172, 207manipulators, robotic, 20, 307317,347348, 406, 407, 408, 409410,413, 436manual control and tracking, 43, 93,97101manufacturing, see design, manufacturing, and marketingmarker systems, 194195marketing, see design, manufacturing,and marketingmassachusetts general hospital, 245massachusetts institute of technology(mit), 123, 127, 128, 175, 176, 178,201measurement systems inc., 175mechatronics, 334medical and health care applications,3839, 395405accreditation, 399400computational models, 85imaging, 38, 395, 397, 398, 398399,400401, 404, 436planning, 400research agenda, 3, 68, 6970, 404405training, 38, 6970, 395, 396400, 414,416see also disability and rehabilitation;telemedicinemedical college of georgia, 401merged display effects, 73, 95metamodeling, 78microelectromechanical systems(mems), 6, 63, 180, 185, 312313microrobotics, 63, 80, 81, 176, 312313,402midi (musical instrument digital interface) technology, 157, 158military uses, see defense applicationsmillimeter wave (mmw) radar, 326327mining applications, 409, 413mobile detection assessment andresponse system (mdars), 322323mobile robots, 317330, 406mockups, see prototypes;simulator systemsmodeling software, 5, 5960, 77, 78, 85,272281, 294296, 369molecular modeling, 436moos (mud, objectoriented), 427motion blur, 113motion sickness, 94, 103, 104, 110, 121,206, 207209, 215220, 229230motivation, 107108, 424, 429muds (multiuser dungeons), 109, 427mullerlyer illusion, 95multiaxis highresolution tactile sensors,6, 80, 8081, 359multimedia, see hypermedia integrationmultimodal interfaces, 3, 37, 53, 67, 71,72, 76, 77, 78, 82, 133, 247249,287289merged display effects, 73, 95modeling of, 179181, 185in telerobotics, 61muse (multiuser simulation environment), 427music, 135136, 154, 157158nnanomanipulator, 436national aeronautics and space administration (nasa), 64, 87, 128, 140,222, 362, 401, 420, 427national board of medical examiners, 400national center for supercomputingapplications, 428, 436national council for teachers of mathematics, 431national defense, see defense applicationsnational information infrastructure, 8,8485index536virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.national institute of standards and technology, 84national institutes of health, 244, 396national research and education network (nren), 64, 364365, 366367national science and technology council, 84national standards and regulations, 8,8788national television standard code(ntsc), 129naval ocean systems center, 128, 176navigation, 5, 59, 77, 79, 267272,293294, 324326navlab, 324negative transfer, 418, 420networks and distributed systems, 15,6365, 362363collaborative, 42, 384, 437441, 439,440441defense applications, 36, 6364, 83, 363,369370, 385, 439education applications, 40, 69, 107, 425,425426, 438entertainment, 63, 88, 363, 365, 372, 383hardware, 367, 370371research agenda, 67, 8283, 370372software, 64, 369370, 371372standards, 7, 64, 65, 82, 8283, 364,366, 368, 372telerobotics, 6, 62, 63, 64, 80, 8182vignette, 3233see also telemedicineneural stimulation, 56, 239noise, see signal variation and noisenonanthropomorphic robots, 44, 71, 93,103northern digital, 195npsnet system, 274, 284286, 370, 371nuclear power facilities, 406, 416, 441nuclear waste disposal, 406oocean, see underwater applicationsodetics 3d laser imaging system, 198offground environments, 409410offhead displays (ohds), 4748, 49, 73,104, 120, 121122, 130, 131, 132,151153offshore oil operations, 412olfactory channel, 23, 55, 245, 246opensystem environment, 79operating systems, 61, 77, 7879, 291292telerobotics, 353354optical technologies, 51, 60, 74, 75, 124,194199, 203, 274, 295, 317.see also visual channeloptic nerve model, 396397optimal control model, 98optotrak, 195196orientation (body), 212213ppacific science center, 426packet switching, 363, 365, 371parallax barrier displays, 124126parallel computing, 58, 257258, 260, 290in teleoperations, 8182, 351353, 358paramount communication, 383passive stereo vision systems, 194patient counseling, 38, 69, 70perforce hand controller, 176personal computers (pcs), 260261,350351phantom interface device, 176, 178photodiodes, 194, 195physical fidelity, 419physiological responses and interfaces,56, 75, 99, 239245pixelflow, 299, 302303pixel planes, 299, 302play facilities, 41, 404, 427pneumatic actuators, 312polhemus fastrak, 193, 246index537virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.police operations, 411polygon flow minimization, 269272polygon processing, 58position mapping, 50, 190, 262264, 392positionsensing detectors (psds), 194,195positiontracking systems, 5, 5051, 59,7475, 188204, 262263acoustic, 51, 199200, 203in augmentedreality production line, 394eye movements, 48, 5152, 112113,115, 117, 132, 189, 201, 204head movements, 47, 121, 131, 151,189190, 193, 201, 912inertial, 51, 74, 200201, 203204magnetic, 51, 193194, 203mechanical, 51, 74, 190193, 202203optical sensing, 51, 74, 75, 194199, 203in teleoperation systems, 62, 411412positron emission tomography (pet),243, 244245power line maintenance, 409410predictive displays, 6, 19, 80, 355356,371, 408409production lines, 388, 389390augmentedreality systems, 37, 393394textile industry, 391proprioception, see kinesthetics and proprioceptionprostheses, 242prototypes, 3738, 388389aerospace, 392395psychological issues, 4, 40, 4346, 7172,93110side effects, 4, 3335, 4546, 66, 71, 72,108109, 110, 377378see also cognitive models;human performance characteristics;motivation;sensorimotor loop alterationspublic opinion and support, 8485educational applications, 41, 430hazardous operations applications, 39medical applications, 405pupil dilation, 241, 243pursuit tracking, 98, 99rrasterization, 253254reality built for two (rb2) system, 274,402realityengine (re2), 58, 254, 298301realtime interaction, 2, 18, 77, 250,254260, 408, 434435haptic interfaces, 78, 185in teleoperations, 6, 6263, 80, 346354,357358registration and alignment, 5, 15, 48, 74,75, 79, 275276, 295296, 404rehabilitation, see disability and rehabilitationremote manipulator system (rms), 308,332, 408, 409remote operations, see teleoperator systems and teleroboticsrepresentation, simulation, and rendering(rsr) techniques, 272273research agenda, 23, 6668applications, 34, 6871computer generation, 56, 7580, 289296evaluation, 8384humanmachine interfaces, 45, 7275networks, 67, 8283, 370372psychological issues, 4, 7172, 110telerobotics, 6, 8082, 354361response bias, 95, 104risk tolerance, 108109robotics, see teleoperator systems andteleroboticsrockwell international, 392rocky iii, 322roleplaying games, 45, 109, 427, 431rotex, 311, 315, 317, 355, 408, 409runtime software architecture, 258260index538virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.ssand play, 404sarcos, inc., 176, 309, 312scanning tunneling microscopy (stm),313, 436scara robot, 308scientific visualization, 42, 45, 433437security applications, 63, 322323, 411,441seethrough displays, 20, 47, 73, 117,119, 404, 440, 441, 442selfimage, 108109, 378selspot, 195sense8, 262, 426sensorimotor loop alterations, 4344,103104adaptation and readaptation, 4344, 71,102, 103, 104, 147148, 159enhancing distortions, 44, 71, 93, 103multimodal interfaces, 71, 72research needs, 4, 71, 72, 110subjective telepresence, 22, 44, 45, 71,101103, 181technology deficiency and distortions,44, 71, 94, 103104, 230see also illusions;sopite syndromesensors (telerobotic), 6, 1718, 6263,8081, 313317, 407, 409, 413force, 6, 80, 81, 315316, 317, 360joint motion and torque, 314315navigational, 324326proximity, 6, 80, 359sensory modeling, 99100sensory substitution, 44, 50, 71, 100101,159auditory, 50, 95, 100, 143144, 159haptic channel, 100, 186sentry robots, 411shape memory alloys (smas), 180, 185,313shared workspaces, 440441sherlock computer system, 106siggraph conference, 199, 383384signal variation and noise, 43, 44, 48, 77,9596, 103104, 230haptic channel, 172, 178silicon graphics, 58, 254, 261, 298, 299,300simnet battle simulation, 6364, 274,282, 363, 369370, 372, 385simulation frameworks, 5, 78, 294simulation loops, 258simulator systems, 2, 14, 2223educational applications, 427428training applications, 40, 356, 413414,415, 416, 420, 421422situated cognition, 424nslicestacking displays, 125, 126127social and ethical issues, 9, 43, 88, 378social interaction and influence, 108, 109,440software, 3, 56, 67, 7680, 261, 291296augmentedreality systems, 5, 60, 79,275276, 295296autonomous agents, 5, 22, 60, 77, 7980,281286, 413communications, 64, 369370, 371372hypermedia integration, 61, 286287interaction, 5859, 261267, 292293modeling, 5, 5960, 77, 78, 85, 272281,294296, 369navigation, 5, 59, 77, 79, 267272,293294operating system, 61, 77, 7879, 291292training applications, 415, 421sonet standard, 364sonification, 136, 158sopite syndrome, 44, 47, 48, 71, 116, 220,377sound, see auditory channelspace applications, 87, 308, 332, 354,362, 407409space automation/robotics consortium,64spatial display and resolution, 37, 44, 94,116, 265, 426427index539virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.auditory, 4950, 137140, 144, 146151,159tactile, 169170, 171, 172, 184185spatiotemporal blur, 113speech, 75, 96, 135136, 142recognition, 5556, 231, 232237synthesis, 20, 55, 56, 154, 231232,237238, 403sri international, 401standards, 8, 8788networks, 7, 64, 65, 8283, 364, 366,368, 372star trek game, 383statistical variation, see signal variationand noisestereo displays, 48, 117, 119120, 121,133, 194, 196, 203, 275, 442stereographics corp., 128, 130structured light systems, 196197subtractive synthesis, 155, 156supervisory control, 6, 80, 81, 306, 307,338346, 354, 413surgeryand anatomical modeling, 6970, 396,397, 398399, 400401, 404planning, 38, 400401training, 2526, 38, 395, 398399, 405surveyor lunar rover, 407408switched multimegabit data service(smds), 364sympathetic nervous response, 240synthetic environment (se) systems, definition, 1n, 2, 13, 21systron donner, 200ttactile information and feedback, 99100,163164, 165, 167171, 177, 179,180, 184185, 392communication aids, 169, 177robotic sensors, 316317, 359see also forcefeedback display systems;haptic channeltechnology academy program, 426technology innovation group, 128, 129technology reinvestment program(trp), 292telecommunications, 3, 4, 42, 43, 68, 69,437443research agenda, 442443see also networks and distributed systemstelecommuting, 438, 439teleconferences, 42, 439, 440, 442telemedicine, 38, 395communication networks, 38, 64, 395,401, 405, 437438surgical applications, 38, 80, 305, 308,381, 395, 401402, 405, 437teleoperated vehicle (tov), 320321teleoperator systems and telerobotics, 6,14, 2122, 4243, 6163, 304307,441442autonomous agents, 306, 354, 361costeffectiveness, 39, 412413defense applications, 36, 305, 320321,384, 411, 438definition, 12, 13, 1719distributed and networked, 6, 62, 63, 64,80, 8182haptic interfaces, 52, 62, 81, 176, 177,179180, 189, 304, 305, 315317, 345hardware development, 6, 6263, 8081,349353, 359360in hazardous operations, 4, 39, 46, 102,304, 305, 381, 405413, 438, 441lowlevel control, 330338, 347348microrobotics, 63, 80, 81, 176, 312313,402nonanthropomorphic models, 44, 71, 93,103remote vehicles, 317330, 406research agenda, 6, 8082, 354361tracking, 62, 411412see also sensors (telerobotic);telemedicinetelepresence, 22, 44, 45, 71, 101103,181, 377index540virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.teletravel, 3, 4, 4243, 68, 69, 437443and distributed collaboration, 439,440441educational applications, 40, 107,425426, 438terminology and definitions, 13, 1724difficulties with, 2, 14, 17testing, see evaluation and testingtexas instruments, 126textile industry, 389, 390392shopping vignette, 2628time delays, 44, 57, 58, 7778, 103104,250, 288in auditory channel systems, 140intermodal, 288position tracking, 205206in scientific visualization, 436437in telerobotics, 6, 62, 80, 335336, 344,355356, 408in visual channel systems, 48, 117118tonic vibration reflex, 214toolhandle interfaces, 52, 74, 173,180181touch, see forcefeedback display systems;haptic channel;tactile information and feedbacktoxic materials and environments, 406407training applications, 3940, 87, 384385,392, 413418cognitive modeling, 45, 71, 106, 110,418419evaluation, 70, 374, 375, 415, 416, 420,421422hazardous operations, 39, 415medical, 38, 395, 398400, 414, 416research agenda, 3, 4, 68, 69, 7071,422423transfer, 40, 45, 69, 7071, 385, 417422see also education applications;flight simulatorstransport delays, see time delaystreadmills, 55, 211, 223224, 228229uultrasonic technology, 199200, 317, 397,436underground storage facilities, 406407underwater applications, 304, 319, 321,323, 327328, 355, 362, 409educational, 425426military, 411united technologies, 390university of illinois, 152, 428, 436university of michigan, 390university of minnesota, 417university of north carolina (unc), 128,176, 290, 299, 302, 389, 397, 436university of washington, 128, 426unix, 61, 78, 291, 293, 353, 354, 368, 371unmanned ground vehicle program, 320update rates, 117, 140, 287288user interface, see ergonomics and comfort issues;humanmachine interfacevvariable gravity displays, 222223velocity storage, 218vertigo, 212213very large scale integrated (vlsi) technologies, 123, 236vestibular displays, 226vibration, 169, 170171, 177, 184viking deepspace probe, 408violence, 45, 109, 383virtex cyberglove, 177virtual actor, 60, 282see also autonomous agentsvirtual environment (ve) systems, definition, 12, 13, 1819, 2223virtual reality (vr) systems, definition,1n, 13virtual research, 128, 129virtual spacetime, 435436index541virtual reality: scientific and technological challengescopyright national academy of sciences. all rights reserved.visible human, 396visual channel, 5, 23, 46, 4748, 57, 70,72, 7374, 91, 95, 111120, 415ambient system, 115116in augmentedreality systems, 48, 95,117, 118119color, 113114, 392display resolution and distortion, 47, 48,57, 73, 94, 113114, 116118,119120, 132display technologies, 122131, 276281eye movement and tracking, 48, 5152,112113, 115, 117, 132, 189, 201, 204in headmounted displays, 23, 4748,73, 120121illusions, 9495, 264information transfer rates, 96, 112line of sight, 264and movement, 213214, 215, 224225,229and sensory substitution, 50, 100see also information visualizationvpl research, inc., 128, 129, 158, 177,274, 314, 396, 402, 403, 417vxworks, 353wwholebody motion, 5354, 55, 205206illusions, 95, 206207, 213215inertial displays, 54, 220222, 228motion sickness, 215220noninertial displays, 54, 224227stability, 95, 206207see also locomotion interfaceswidearea networks (wan), 7, 64, 65,363365, 405windows nt, 61, 78wind tunnels, 255256, 435woods hole oceanographic institution,412world toolkit, 262wrightpatterson air force base, 128,148, 222xxerox parc, 440index542
detailsdistribution, posting, or copying of this pdf is strictly prohibited without written permission of the national academies press. (request permission) unless otherwise indicated, all materials in this pdf are copyrighted by the national academy of sciences.copyright © national academy of sciences. all rights reserved.the national academies pressvisit the national academies press at nap.edu and login or register to get:œ œ 10% off the price of print titlesœ special offers and discountsget this bookfind related titlesthis pdf is available at sharecontributorshttp://nap.edu/11480catalyzing inquiry at the interface of computing and biology468 pages | 8.5 x 11 | paperbackisbn 9780309096126 | doi 10.17226/11480john c. wooley and herbert s. lin, editors; committee on frontiers at theinterface of computing and biology; computer science and telecommunicationsboard; division on engineering and physical sciences; national research councilcatalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.introductionijohn c. wooley and herbert s. lin, editorscommittee on frontiers at the interface of computing and biologycomputer science and telecommunications boarddivision on engineering and physical sciencescatalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.the national academies press 500 fifth street, n.w. washington, dc 20001notice: the project that is the subject of this report was approved by the governing board ofthe national research council, whose members are drawn from the councils of the nationalacademy of sciences, the national academy of engineering, and the institute of medicine. themembers of the committee responsible for the report were chosen for their special competencesand with regard for appropriate balance.support for this project was provided by the defense advanced research projects agencyunder contract no. mda9720010005, the national science foundation under contract no.dbi0094528, the department of health and human services/national institutes of health(including the national institute of general medical sciences and the national center forresearch resources) under contract no. n01od42139, the department of energy undercontract no. defg0202er63336, the department of energyõs office of science (ber) underinteragency agreement no. defg0204er63934, and national research council funds. anyopinions, findings, conclusions, or recommendations expressed in this publication are those ofthe author(s) and do not necessarily reflect the views of the organizations or agencies thatprovided support for the project.international standard book number 030909612xlibrary of congress control number: 2005936580cover designed by jennifer m. bishop.this report is available fromcomputer science and telecommunications boardnational research council500 fifth street, n.w.washington, dc 20001additional copies of this report are available from the national academies press, 500 fifthstreet, n.w., lockbox 285, washington, dc 20055; (800) 6246242 or (202) 3343313 (in thewashington metropolitan area); internet, http://www.nap.edu.copyright 2005 by the national academy of sciences. all rights reserved.printed in the united states of americacatalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.the national academy of sciences is a private, nonprofit, selfperpetuating societyof distinguished scholars engaged in scientific and engineering research, dedicated to thefurtherance of science and technology and to their use for the general welfare. upon theauthority of the charter granted to it by the congress in 1863, the academy has a mandatethat requires it to advise the federal government on scientific and technical matters.dr. ralph j. cicerone is president of the national academy of sciences.the national academy of engineering was established in 1964, under the charter of thenational academy of sciences, as a parallel organization of outstanding engineers. it isautonomous in its administration and in the selection of its members, sharing with thenational academy of sciences the responsibility for advising the federal government. thenational academy of engineering also sponsors engineering programs aimed at meetingnational needs, encourages education and research, and recognizes the superior achievements of engineers. dr. wm. a. wulf is president of the national academy of engineering.the institute of medicine was established in 1970 by the national academy of sciences tosecure the services of eminent members of appropriate professions in the examination ofpolicy matters pertaining to the health of the public. the institute acts under the responsibility given to the national academy of sciences by its congressional charter to be an adviser tothe federal government and, upon its own initiative, to identify issues of medical care,research, and education. dr. harvey v. fineberg is president of the institute of medicine.the national research council was organized by the national academy of sciences in 1916to associate the broad community of science and technology with the academyõs purposes offurthering knowledge and advising the federal government. functioning in accordance withgeneral policies determined by the academy, the council has become the principal operating agency of both the national academy of sciences and the national academy of engineering in providing services to the government, the public, and the scientific and engineering communities. the council is administered jointly by both academies and the institute ofmedicine. dr. ralph j. cicerone and dr. wm. a. wulf are chair and vice chair, respectively,of the national research council.www.nationalacademies.orgcatalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.committee on frontiers at the interface ofcomputing and biologyjohn c. wooley, university of california at san diego, chairadam p. arkin, university of california at berkeley and lawrence berkeleynational laboratoryeric brill, microsoft research labsrobert m. corn, university of california at irvinechris diorio, university of washingtonleah edelsteinkeshet, university of british columbiamark h. ellisman, university of california at san diegomarcus w. feldman, stanford universitydavid k. gifford, massachusetts institute of technologytakeo kanade, carnegie mellon universitystephen s. laderman, agilent laboratoriesjames s. schwaber, thomas jefferson medical collegestaffherbert lin, senior scientist and study directorgeoff cohen, consultant to cstbmitchell waldrop, consultant to cstbdaehee hwang, consultant to board on biologyrobin schoen, senior staff officerelizabeth grossman, senior staff officer (through march 2001)jennifer bishop, program associated.c. drake, senior program assistant (through march 2003)ivcatalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.computer science and telecommunications boardjoseph traub, columbia university, chaireric benhamou, benhamou global ventures, llcdavid d. clark, massachusetts institute of technology, cstb chair emerituswilliam dally, stanford universitymark e. dean, ibm almaden research centerdeborah estrin, university of california, los angelesjoan feigenbaum, yale universityhector garciamolina, stanford universitykevin kahn, intel corporationjames kajiya, microsoft corporationmichael katz, university of california, berkeleyrandy h. katz, university of california, berkeleywendy a. kellogg, ibm t.j. watson research centersara kiesler, carnegie mellon universitybutler w. lampson, microsoft corporation, cstb member emeritusteresa h. meng, stanford universitytom m. mitchell, carnegie mellon universitydaniel pike, gci cable and entertainmenteric schmidt, google inc.fred b. schneider, cornell universitywilliam stead, vanderbilt universityandrew j. viterbi, viterbi group, llcjeannette m. wing, carnegie mellon universityrichard rowberg, acting directorkristen batch, research associatejennifer m. bishop, program associatejanet briscoe, manager, program operationsjon eisenberg, senior program officer and associate directorrenee hawkins, financial associatemargaret marsh huynh, senior program assistantherbert s. lin, senior scientistlynette i. millett, senior program officerjanice sabuda, senior program assistantgloria westbrook, senior program assistantbrandye williams, staff assistantfor more information on cstb, see its web site at http://www.cstb.org, write tocstb, national research council, 500 fifth street, n.w., washington, dc 20001, orcall (202) 3342605, or email the cstb at cstb@nas.edu.vcatalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.prefaceviiin the last decade of the 20th century, computer science and biology both emerged as fields capableof remarkable and rapid change. moreover, they evolved as fields of inquiry in ways that draw attention to their areas of intersection. the continuing advancements in technology and the pace of scientificresearch present the means for computing to help answer fundamental questions in the biologicalsciences and for biology to demonstrate that new approaches to computing are possible.advances in the power and ease of use of computing and communications systems have fueledcomputational biology (e.g., genomics) and bioinformatics (e.g., database development and analysis).modeling and simulation of biological entities such as cells have joined biologists and computer scientists (and mathematicians, physicists, and statisticians too) to work together on activities from pharmaceutical design to environmental analysis.on the other side, computer scientists have pondered the significance of biology for their field. forexample, computer scientists have explored the use of dna as a substrate for new computing hardwareand the use of biological approaches in solving hard computing problems. exploration of biologicalcomputation suggests a potential for insight into the nature of and alternative processes for computation, and it also gives rise to questions about hybrid systems that achieve some kind of synergy ofbiological and computational systems. and there is also the fact that biological systems exhibit characteristics such as adaptability, selfhealing, evolution, and learning that would be desirable in the information technologies that humans use.making the most of the research opportunities at the interface of computing and biologyñwhat weare calling the biocomp interfaceñrequires illuminating what they are and effectively engaging peoplefrom both computing and biology. as in other contexts, the challenges of interdisciplinary educationand of collaboration are significant, and each will require attention, together with substantive workfrom both policy makers and researchers. at the start of the 1990s, attempts were made to stimulatemutual interest and collaboration among young researchers in computing and biology. those earlyefforts yielded nontrivial successes, but in retrospect represented a version 1.0 prototype for the potential in bringing the two fields together. circumstances today seem much more favorable for progress.new research teams and training programs have been formed as individual investigators from therespective communities, government agencies, and private foundations have become increasingly engaged. similarly, some larger groups of investigators from different backgrounds have been able tocatalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.viiiprefaceobtain funding to work together to address crossdisciplinary research problems. it is against thisbackground that the committee sees a version 2.0 of the biocomp interface emerging that will yieldunprecedented progress and advance.the range of possible activities at the biocomp interface is broad, and accordingly so is the range ofinterested agencies, which include the defense advanced research projects agency (darpa), thenational science foundation (nsf), the department of energy (doe), and the national institutes ofhealth (nih). these agencies have, to varying degrees, recognized that truly crossdisciplinary workwould build on both computing and biology, and they have sought to advance activities at the interface.this report by the committee on frontiers at the interface of computing and biology seeks toestablish the intellectual legitimacy of a fundamentally crossdisciplinary collaboration between biologists and computer scientists. that is, while some universities are increasingly favorable to research atthe intersection, life science researchers at other universities are strongly impeded in their efforts tocollaborate. this report addresses these impediments and describes some strategies for overcomingthem.in addition, this report provides a wealth of welldocumented examples. as a rule, these exampleshave generally been selected to illustrate the breadth of the topic in question, rather than to identify themost important areas of activity. that is, the appropriate spirit in which to view these examples is òleta thousand flowers bloom,ó rather than one of òfinding the prettiest flowers.ó it is hoped that theseexamples will encourage students in the life sciences to start or to continue study in computer sciencethat will enable them to be more effective users of computing in their future biological studies. in theopposite direction, the report seeks to describe a rich and diverse domainñbiologyñwithin whichcomputer scientists can find worthy problems that challenge current knowledge in computing. it ishoped that this awareness will motivate interested computer scientists to learn about biological phenomena, data, experimentation, and the likeñso that they can engage biologists more effectively.to gather information on such a broad area, the committee took input from a wide variety ofsources. the committee convened two workshops in march 2001 and may 2001, and committee members or staff attended relevant workshops sponsored by other groups. the committee mined the published literature extensively. it solicited input from other scientists known to be active in biocompresearch. an early draft of the report was examined by a number of reviewers far larger than usual fornational research council (nrc) reports, and the draft was modified in accordance with their extensive input, which helped the committee to sharpen its message and strengthen its presentation.the result of these efforts is the first comprehensive nrc study that suggests a highlevel intellectual structure for federal agencies for supporting work at the biocomp interface. although workshopreports have been supported by individual agencies on the subject of computing applied to variousaspects of biological inquiry, the nrc has not until now undertaken a study whose intent was to beinclusive.within the nrc, the lead unit on this project was the computer science and telecommunicationsboard (cstb), and marjory blumenthal and elizabeth grossman launched the project. the committeealso acknowledges with gratitude the contribution of the board on biologyñrobin schoen continuedwork on the project after elizabeth grossmanõs departure. geoff cohen and mitch waldrop, consultants to cstb, made major substantive contributions to this report. a variety of project assistants,including d.c. drake, jennifer bishop, gloria westbrook, and margaret huynh, provided research andadministrative support. finally, grateful thanks are offered to darpa, nih, nsf, and doe for theirfinancial support for this project as well as their patience in awaiting the final report. no single agencycan respond to the challenges and opportunities at the interface, and the committee hopes that itsanalysis will facilitate agency efforts to define their own priorities, set their own path, and participate inwhat will be a continuing adventure along the frontier at this exciting and promising interface, whichwill continue to develop throughout the 21st century.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.prefaceixa personal note from the chairthe committee found the scope of the study and the need to achieve an adequate level of balance inboth directions around the biocomp interface to be a challenge. this challenge, i hope, has been met,but this was only possible due to the recruitment of an outstanding physicist turned computer sciencepolicy expert from the nrc. specifically, after the original series of meetings, herb lin from the cstbside of the nrc joined the effort, and most notably, followed up on the committeeõs earlier analyses byinterviewing numerous individuals engaged in both biocomputing (applications of biology to computing) and computational biology (applications of computing to biology). this was invaluable, as washerbõs never ending enthusiasm, insight into the nature of the interdisciplinary discussions that aregrowing, and his willingness to engage in learning a lot about biology. the report could never havebeen completed without his persistence. his expertise in editing and analytical treatment of policy andtechnical material allowed us to sustain a broad vision. (even with the length and breadth of this study,we were able to cover only selected areas at the interface.) the committeeõs efforts were sustained andaccelerated by herbõs determination that we stay the course despite the size of the task, and by hisinsightful comments, criticisms, and suggestions on every aspect of the study and the report.john wooley, chaircommittee on frontiers at the interfaceof computing and biologycatalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.this report has been reviewed in draft form by individuals chosen for their diverse perspectivesand technical expertise, in accordance with procedures approved by the national research councilõsreport review committee. the purpose of this independent review is to provide candid and criticalcomments that will assist the institution in making its published report as sound as possible and toensure that the report meets institutional standards for objectivity, evidence, and responsiveness to thestudy charge. the review comments and draft manuscript remain confidential to protect the integrity ofthe deliberative process. we wish to thank the following individuals for their review of this report:harold abelson, massachusetts institute of technology,eric benhamou, benhamou global ventures, llc,mina bissell, lawrence berkeley national laboratory,gaetano borriello, university of washington,dennis bray, university of cambridge,steve burbeck, ibm,andrea califano, columbia university,charles cantor, boston university,david d. clark, massachusetts institute of technology,g. bard ermentrout, university of pittsburgh,lisa fauci, tulane university,david galas, keck graduate institute,leon glass, mcgill university,mark d. hill, university of wisconsinmadison,tony hunter, the salk institute for biological studies,sara kiesler, carnegie mellon university,isaac kohane, childrenõs hospital,nancy kopell, boston university,bud mishra, new york university,william noble, university of washington,acknowledgment of reviewersxicatalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.xiiacknowledgment of reviewersalan s. perelson, los alamos national laboratory,robert j. robbins, fred hutchinson cancer research center,lee segel, the weizmann institute of science,larry l. smarr, university of california, san diego,sylvia spengler, national science foundation,william stead, vanderbilt university,suresh subramani, university of california, san diego,charles taylor, university of california, los angeles, andandrew j. viterbi, viterbi group, llc.although the reviewers listed above have provided many constructive comments and suggestions,they were not asked to endorse the conclusions or recommendations, nor did they see the final draft ofthe report before its release. the review of this report was overseen by russ altman, stanford university. appointed by the national research council, he was responsible for making certain that an independent examination of this report was carried out in accordance with institutional procedures and thatall review comments were carefully considered. responsibility for the final content of this report restsentirely with the authoring committee and the institution.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.executive summary11introduction91.1excitement at the interface of computing and biology, 91.2perspectives on the biocomp interface, 101.2.1from the biology side, 111.2.2from the computing side, 121.2.3the role of organization and culture, 131.3imagine whatõs next, 141.4some relevant history in building the interface, 161.4.1the human genome project, 161.4.2the computingtobiology interface, 161.4.3the biologytocomputing interface, 171.5background, organization, and approach of this report, 19221st century biology232.1what kind of science?, 232.1.1the roots of biological culture, 232.1.2molecular biology and the biochemical basis of life, 242.1.3biological components and processes in context, and biological complexity, 252.2toward a biology of the 21st century, 272.3roles for computing and information technology in biology, 312.3.1biology as an information science, 312.3.2computational tools, 332.3.3computational models, 332.3.4a computational perspective on biology, 332.3.5cyberinfrastructure and data acquisition, 342.4challenges to biological epistemology, 34contentsxiiicatalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.xivcontents3on the nature of biological data353.1data heterogeneity, 353.2data in high volume, 373.3data accuracy and consistency, 383.4data organization, 403.5data sharing, 443.6data integration, 473.7data curation and provenance, 494computational tools574.1the role of computational tools, 574.2tools for data integration, 584.2.1desiderata, 594.2.2data standards, 604.2.3data normalization, 604.2.4data warehousing, 624.2.5data federation, 624.2.6data mediators/middleware, 654.2.7databases as models, 654.2.8ontologies, 674.2.8.1ontologies for common terminology and descriptions, 674.2.8.2ontologies for automated reasoning, 694.2.9annotations and metadata, 734.2.10a case study: the cell centered database, 754.2.11a case study: ecological and evolutionary databases, 794.3data presentation, 814.3.1graphical interfaces, 814.3.2tangible physical interfaces, 834.3.3automated literature searching, 844.4algorithms for operating on biological data, 874.4.1preliminaries: dna sequence as a digital string, 874.4.2proteins as labeled graphs, 884.4.3algorithms and voluminous datasets, 894.4.4gene recognition, 894.4.5sequence alignment and evolutionary relationships, 924.4.6mapping genetic variation within a species, 944.4.7analysis of gene expression data, 974.4.8data mining and discovery, 1004.4.8.1the first known biological discovery from mining databases, 1004.4.8.2a contemporary example: protein family classification and dataintegration for functional analysis of proteins, 1014.4.9determination of threedimensional protein structure, 1034.4.10protein identification and quantification from mass spectrometry, 1064.4.11pharmacological screening of potential drug compounds, 1074.4.12algorithms related to imaging, 1074.4.12.1image rendering, 1104.4.12.2image segmentation, 1104.4.12.3image registration, 1134.4.12.4image classification, 1144.5developing computational tools, 114catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.contentsxv5computational modeling and simulation as enablers forbiological discovery1175.1on models in biology, 1175.2why biological models can be useful, 1195.2.1models provide a coherent framework for interpreting data, 1205.2.2models highlight basic concepts of wide applicability, 1205.2.3models uncover new phenomena or concepts to explore, 1215.2.4models identify key factors or components of a system, 1215.2.5models can link levels of detail (individual to population), 1225.2.6models enable the formalization of intuitive understandings, 1225.2.7models can be used as a tool for helping to screen unpromising hypotheses, 1225.2.8models inform experimental design, 1225.2.9models can predict variables inaccessible to measurement, 1235.2.10models can link what is known to what is yet unknown, 1245.2.11models can be used to generate accurate quantitative predictions, 1245.2.12models expand the range of questions that can meaningfully be asked, 1245.3types of models, 1255.3.1from qualitative model to computational simulation, 1255.3.2hybrid models, 1295.3.3multiscale models, 1305.3.4model comparison and evaluation, 1315.4modeling and simulation in action, 1345.4.1molecular and structural biology, 1345.4.1.1predicting complex protein structures, 1345.4.1.2a method to discern a functional class of proteins, 1345.4.1.3molecular docking, 1365.4.1.4computational analysis and recognition of functional andstructural sites in protein structures, 1365.4.2cell biology and physiology, 1395.4.2.1cellular modeling and simulation efforts, 1395.4.2.2cell cycle regulation, 1465.4.2.3a computational model to determine the effects of snps inhuman pathophysiology of red blood cells, 1485.4.2.4spatial inhomogeneities in cellular development, 1495.4.2.4.1unraveling the physical basis of microtubule structure andstability, 1495.4.2.4.2the movement of listeria bacteria, 1505.4.2.4.3morphological control of spatiotemporal patterns ofintracellular signaling, 1515.4.3genetic regulation, 1525.4.3.1cisregulation of transcription activity as process control computing, 1525.4.3.2genetic regulatory networks as finitestate automata, 1535.4.3.3genetic regulation as circuits, 1575.4.3.4combinatorial synthesis of genetic networks, 1585.4.3.5identifying systems responses by combining experimental data withbiological network information, 1595.4.4organ physiology, 1615.4.4.1multiscale physiological modeling, 1615.4.4.2hematology (leukemia), 162catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.xvicatalyzing inquiry5.4.4.3immunology, 1635.4.4.4the heart, 1665.4.5neuroscience, 1725.4.5.1the broad landscape of computational neuroscience, 1725.4.5.2largescale neural modeling, 1735.4.5.3muscular control, 1755.4.5.4synaptic transmission, 1815.4.5.5neuropsychiatry, 1875.4.6virology, 1895.4.7epidemiology, 1915.4.8evolution and ecology, 1935.4.8.1commonalities between evolution and ecology, 1935.4.8.2examples from evolution, 1945.4.8.2.1reconstruction of the saccharomyces phylogenetic tree, 1955.4.8.2.2modeling of myxomatosis evolution in australia, 1975.4.8.2.3the evolution of proteins, 1985.4.8.2.4the emergence of complex genomes, 1995.4.8.3examples from ecology, 2005.4.8.3.1impact of spatial distribution in ecosystems, 2005.4.8.3.2forest dynamics, 2015.5technical challenges related to modeling, 2026a computational and engineering view of biology2056.1biological information processing, 2056.2an engineering perspective on biological organisms, 2106.2.1biological organisms as engineered entities, 2106.2.2biology as reverse engineering, 2116.2.3modularity in biological entities, 2136.2.4robustness in biological entities, 2176.2.5noise in biological phenomena, 2206.3a computational metaphor for biology, 2237cyberinfrastructure and data acquisition2277.1cyberinfrastructure for 21st century biology, 2277.1.1what is cyberinfrastructure? 2277.1.2why is cyberinfrastructure relevant? 2287.1.3the role of highperformance computing, 2317.1.4the role of networking, 2357.1.5an example of using cyberinfrastructure for neuroscience research, 2357.2data acquisition and laboratory automation, 2377.2.1todayõs technologies for data acquisition, 2377.2.2examples of future technologies, 2417.2.3future challenges, 2458biological inspiration for computing2478.1the impact of biology on computing, 2478.1.1biology and computing: promise and skepticism, 2478.1.2the meaning of biological inspiration, 2498.1.3multiple roles: biology for computing insight, 250catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.contentsxvii8.2examples of biology as a source of principles for computing, 2538.2.1swarm intelligence and particle swarm optimization, 2538.2.2robotics 1: the subsumption architecture, 2558.2.3robotics 2: bacteriuminspired chemotaxis in robots, 2568.2.4selfhealing systems, 2578.2.5immunology and computer security, 2598.2.5.1why immunology might be relevant, 2598.2.5.2some possible applications of immunologybased computer security, 2598.2.5.3immunological design principles for computer security, 2608.2.5.4an example: immunology and intruder detection, 2628.2.5.5interesting questions and challenges, 2638.2.5.5.1definition of self, 2638.2.5.5.2more immunological mechanisms, 2638.2.5.6some possible difficulties with an immunological approach, 2648.2.6amorphous computing, 2648.3biology as implementer of mechanisms for computing, 2658.3.1evolutionary computation, 2658.3.1.1what is evolutionary computation? 2658.3.1.2suitability of problems for evolutionary computation, 2678.3.1.3correctness of a solution, 2688.3.1.4solution representation, 2698.3.1.5selection of primitives, 2698.3.1.6more evolutionary mechanisms, 2708.3.1.6.1coevolution, 2708.3.1.6.2development, 2708.3.1.7behavior of evolutionary processes, 2718.3.2robotics 3: energy and compliance management, 2728.3.3neuroscience and computing, 2738.3.3.1neuroscience and architecture in broad strokes, 2748.3.3.2neural networks, 2748.3.3.3neurally inspired sensors, 2778.3.4ant algorithms, 2778.3.4.1ant colony optimization, 2788.3.4.2other ant algorithms, 2798.4biology as physical substrate for computing, 2808.4.1biomolecular computing, 2808.4.1.1description, 2818.4.1.2potential application domains, 2848.4.1.3challenges, 2858.4.1.4future directions, 2868.4.2synthetic biology, 2878.4.2.1an engineering approach to building living systems, 2888.4.2.2cellular logic gates, 2888.4.2.3broader views of synthetic biology, 2908.4.2.4applications, 2918.4.2.5challenges, 2918.4.3nanofabrication and dna selfassembly, 2928.4.3.1rationale, 2928.4.3.2applications, 296catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.xviiicontents8.4.3.3prospects, 2978.4.3.4hybrid systems, 2989illustrative problem domains at the interface ofcomputing and biology2999.1why problemfocused research? 2999.2cellular and organismal modeling, 3009.3a synthetic cell with physical form, 3039.4neural information processing and neural prosthetics, 3069.5evolutionary biology, 3119.6computational ecology, 3139.7genomeenabled individualized medicine, 3179.7.1disease susceptibility, 3189.7.2drug response and pharmacogenomics, 3209.7.3nutritional genomics, 3229.8a digital human on which a surgeon can operate virtually, 3239.9computational theories of selfassembly and selfmodification, 3259.10a theory of biological information and complexity, 32710culture and research infrastructure33110.1setting the context, 33110.2organizations and institutions, 33210.2.1the nature of the community, 33210.2.2education and training, 33310.2.2.1 general considerations, 33310.2.2.2 undergraduate programs, 33410.2.2.3 the bio2010 report, 335 10.2.2.3.1engineering, 336 10.2.2.3.2quantitative training, 336 10.2.2.3.3computer science, 33710.2.2.4 graduate programs, 34110.2.2.5 postdoctoral programs, 343 10.2.2.5.1the sloan/doe postdoctoral awards forcomputational molecular biology, 343 10.2.2.5.2the burroughswellcome career awards at thescientific interface, 344 10.2.2.5.3keck center for computational andstructural biology: the research training program, 34410.2.2.6 faculty retraining in midcareer, 34510.2.3academic organizations, 34610.2.4industry, 34910.2.4.1 major it corporations, 35010.2.4.2 major life science corporations, 35010.2.4.3 startup and smaller companies, 35110.2.5funding and support, 35210.2.5.1general considerations, 35210.2.5.1.1the role of funding institutions, 35210.2.5.1.2the review process, 352catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.contentsxix10.2.5.2 federal support, 35310.2.5.2.1national institutes of health, 35310.2.5.2.2national science foundation, 35610.2.5.2.3department of energy, 35710.2.5.2.4defense advanced research projects agency, 35910.3barriers, 36110.3.1differences in intellectual style, 36110.3.1.1historical origins and intellectual traditions, 36110.3.1.2different approaches to education and training, 36210.3.1.3the role of theory, 36310.3.1.4data and experimentation, 36510.3.1.5a caricature of intellectual differences, 36710.3.2differences in culture, 36710.3.2.1the nature of the research enterprise, 36710.3.2.2publication venue, 36910.3.2.3organization of human resources, 36910.3.2.4devaluing the contributions of the other, 36910.3.2.5attitudinal issues, 37010.3.3barriers in academia, 37110.3.3.1academic disciplines and departmental structure, 37110.3.3.2structure of educational programs, 37210.3.3.3coordination costs, 37310.3.3.4risks of retraining and conversion, 37410.3.3.5rapid but uneven changes in biology, 37410.3.3.6funding risk, 37510.3.3.7local cyberinfrastructure, 37510.3.4barriers in commerce and business, 37510.3.4.1importance assigned to shortterm payoffs, 37510.3.4.2reduced workforces, 37610.3.4.3proprietary systems, 37610.3.4.4cultural differences between industry and academia, 37610.3.5issues related to funding policies and review mechanisms, 37710.3.5.1scope of supported work, 37710.3.5.2scale of supported work, 37910.3.5.3the review process, 38010.3.6issues related to intellectual property and publication credit, 38111conclusions and recommendations38311.1disciplinary perspectives, 38311.1.1the biologycomputing interface, 38311.1.2other emerging fields at the biocomp interface, 38411.2moving forward, 38511.2.1building a new community, 38611.2.2core principles for practitioners, 38711.2.3core principles for research institutions, 38811.3the special significance of educational innovation at the biocomp interface, 38911.3.1content, 38911.3.2mechanisms, 390catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.xxcontents11.4recommendations for research funding agencies, 39211.4.1core principles for funding agencies, 39211.4.2national institutes of health, 39511.4.3national science foundation, 39711.4.4department of energy, 39711.4.5defense advanced research projects agency, 39811.5conclusions regarding industry, 39811.6closing thoughts, 399appendixesathe secrets of life: a mathematicianõs introduction to molecular biology403bchallenge problems in bioinformatics and computational biology from other reports429cbiographies of committee members and staff437dworkshop participants443what is cstb?445catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.executive summary11executive summarydespite some apparent differences, biology and information technology (it) have much in common. they are two of the most rapidly changing fields todayñthe former because of enormous influxesof new, highly heterogeneous data, and the latter because of exponentially decreasing priceperformance ratios. they both deal with entities of astounding complexity (organisms in the case of biology,networks and computer systems in the case of information technology), although in the it context, thesignificance of the constituent connections and components is much better understood than in thebiological context. also, they both have profound and revolutionary implications for science and society. biological science and technology have the potential to contribute strongly to society in improvinghuman health and wellbeing. the potential impacts include earlier diagnoses and more powerfultreatments for diseases, rapid environmental cleanup, and more robust food production. computingand information technology enable human beings to acquire, store, process, and interpret enormousamounts of information that continue to underpin much of modern society.against that backdrop, this report considers potential interactions between biology and computingñthe òbiocompó interface. to understand better the potential synergies at the biocomp interfaceand to facilitate the development of new collaborations between the scientific communities in bothfields that can better exploit these synergies, the national research council established the committeeon frontiers at the interface of computing and biology. for simplicity, this report uses òcomputingó torefer to the broad domain encompassed collectively by terms such as computing, computation, modeling and simulation, computer science, computer engineering, informatics, information technology, scientific computing, and computational science. (analytical techniques without a strong machineassisted computational dimension are generally excluded from this study, although they are mentionedfrom time to time when there is an interesting relationship to computing.) similarly, the report uses theterm ò21st century biologyó to refer to all fields of endeavor in the biological, biochemical, and biomedical sciences.obviously, the union of computing with biology results in an extraordinarily broad area of interest.thus, this report is not intended to be comprehensive in the sense of seeing how every subfield ofbiology might connect to every topic in computing. instead, it seeks to sample the intellectual terrain inenough places so as to give the reader a sense of the kinds of activities under way, and its spirit shouldcatalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.2catalyzing inquirybe understood as òletting a thousand flowers bloomó rather than òidentifying the prettiest flowers inthe landscape.ócomputingõs impact on biology twentyfirst century biology will integrate a number of diverse intellectual notions. one integration is that of the reductionist and systems approachesña focus on components of biological systemscombined with a focus on interactions among these components. a second integration is that of manydistinct strands of biological research: taxonomic studies of many species, the enormous progress inmolecular genetics, steps toward understanding the molecular mechanisms of life, and a considerationof biological entities in relationship to their larger environment. a third integration is that computingwill become highly relevant to both hypothesis testing and hypothesis generation in empirical work inbiology. finally, 21st century biology will also encompass what is often called discovery scienceñtheenumeration and identification of the components of a biological system independently of any specifichypothesis about how that system functions (a canonical example being the genomic sequencing ofvarious organisms). twentyfirst century biology will embrace the study of an inclusive set of biologicalentities, their constituent components, the interactions among components, and the consequences ofthose interactions, from molecules, genes, cells, and organisms to populations and even ecosystems.how will computing play in 21st century biology? life scientists have exploited computing formany years in some form or another. yet what is different todayñand will increasingly be so in thefutureñis that the knowledge of computing needed to address many interesting biological problemscan no longer be learned and exploited simply by òhackingó and reading the manuals. indeed, the kindsand levels of expertise needed to address the most challenging problems of 21st century biology stretchthe current state of knowledge of the fieldña point that illuminates the importance of real computingresearch in a biological context.this report identifies four distinct but interrelated roles of computing for biology.1.computational tools are artifactsñusually implemented as software but sometimes hardwareñthat enable biologists to solve very specific and precisely defined problems. such biologicallyoriented tools acquire, store, manage, query, and analyze biological data in a myriad of formsand in enormous volume for its complexity. these tools allow biologists to move from the studyof individual phenomena to the study of phenomena in a biological context; to move across vastscales of time, space, and organizational complexity; and to utilize properties such as evolutionary conservation to ascertain functional details.2.computational models are abstractions of biological phenomena implemented as artifacts that canbe used to test insights, to make quantitative predictions, and to help interpret experimentaldata. these models enable biological scientists to understand many types of biological data incontext, even in very large volume, and to make modelbased predictions that can then be testedempirically. such models allow biological scientists to tackle difficult problems that could notreadily be posed without visualization, rich databases, and new methods for making quantitative predictions. biological modeling itself has become possible because data are available inunprecedented richness and because computing itself has matured enough to support the analysis of such complexity.3.a computational perspective on or metaphor for biology applies the intellectual constructs of computer science and information technology as ways of coming to grips with the complexity ofbiological phenomena that can be regarded as performing information processing in differentways. this perspective is a source of information and computing abstractions that can be used tointerpret and understand biological mechanisms and function. because both computing andbiology are concerned with function, information and computing abstractions can provide wellunderstood constructs that can be used to characterize the biological function of interest. further,catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.executive summary3such abstractions may well provide an alternative and more appropriate language and set ofabstractions for representing biological interactions, describing biological phenomena, or conceptualizing some characteristics of biological systems.4.cyberinfrastructure and data acquisition are enabling support technologies for 21st century biology.cyberinfrastructureñhighend generalpurpose computing centers that provide supercomputingcapabilities to the community at large; wellcurated data repositories that store and make available to all researchers large volumes and many types of biological data; digital libraries thatcontain the intellectual legacy of biological researchers and provide mechanisms for sharing,annotating, reviewing, and disseminating knowledge in a collaborative context; and highspeednetworks that connect geographically distributed computing resourcesñwill become an enabling mechanism for largescale, dataintensive biological research that is distributed over multiple laboratories and investigators around the world. new data acquisition technologies such asgenome sequencers will enable researchers to obtain larger amounts of data of different typesand at different scales, and advances in information technology and computing will play keyroles in the development of these technologies.why is computing in all of these roles needed for 21st century biology? the answer, in a word, isdata. the data relevant to 21st century biology are highly heterogeneous in content and format,multimodal in method of collection, multidimensional in time and space, multidisciplinary in creationand analysis, multiscale in organization, international in relevance, and the product of collaborationsand sharing. consider, for example, that biological data may consist of sequences, graphs, geometricinformation, scalar and vector fields, patterns of organization, constraints, images, scientific prose, andeven biological hypotheses and evidence. these data may well be of very high dimension, since datapoints that might be associated with the behavior of an individual unit must be collected for thousandsor tens of thousands of comparable units.these data are windows into structures of immense complexity. biological entities (and systemsconsisting of multiple entities) are sufficiently complex that it may well be impossible for any humanbeing to keep all of the essential elements in his or her head at once; if so, it is likely that computers willbe the vessel in which biological theories are held, formed, and evaluated. furthermore, because ofevolution and a long history of environmental accidents that have driven processes of natural selection,biological systems are more properly regarded as engineered entities than as objects whose existencemight be predicted on the basis of the first principles of physics, although the evolutionary contextmeans that an artifact is never òfinishedó and rather has to be evaluated on a continuous basis. the taskof understanding thus becomes one of òreverse engineeringóñattempting to understand the construction of a device about whose design little is known but from which much indicative empirical data canbe extracted.twentyfirst century biology will be an information science, and it will use computing and information technology as a language and a medium in which to manage the discrete, nonsymmetric, largelynonreducible, unique nature of biological systems and observations. in some ways, computing andinformation will have a relationship to the language of 21st century biology that is similar to therelationship of calculus to the language of the physical sciences. computing itself can provide biologistswith an alternative, and possibly more appropriate, language and sets of intellectual abstractions forcreating models and data representations of higherorder interactions, describing biological phenomena, and conceptualizing some characteristics of biological systems.biologyõs impact on computingfrom the computing side (i.e., for the computer scientist), there is an asyetunfulfilled promise thatbiology may have significant potential to influence computer design, component fabrication, and software. the essential premise is that biological systems possess many qualities that would be desirable incatalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.4catalyzing inquirythe information technology that humans use. for example, computer and information scientists arelooking for ways to make computers more adaptive, reliable, òsmarter,ó faster, and resilient. biologicalsystems excel at finding and learning goodñbut not necessarily optimalñsolutions to illposed problems on time scales short enough to be useful to them. they efficiently store òdata,ó integrate òhardwareó and òsoftware,ó selfcorrect, and have many other properties that computing and informationscience might capture in order to achieve its future goals. especially for areas in which computer sciencelacks a welldeveloped theory or analysis (e.g., the behavior of complex systems or robustness), biologymay have the most to contribute.the impact of biology and biological sciences on advances in computing is, however, more speculative than the reverse, because such considerations are, with only a few exceptions, relevant to futureoutcomes and not to what has been or is already being delivered. humans understand computingartifacts much better than they do biological organisms, largely because humans have been responsiblefor the design of computing artifacts. absent a comparable base of understanding of biological organisms, the historical and contemporary contributions from biology to computing have been largelymetaphorical and can be characterized more readily as inspiration, rather than advances having astraightforward or linear impact.this difference may be one of time scale. because todayõs computing already contributes directly inan essential way to advancing biological knowledge, a path for the nearterm future can be readilydescribed. contemporary advances in computing provide new opportunities for understanding biology, and this will continue to be true for the foreseeable future. advances in biological understandingmay yet have enormous value for changing computing paradigms (e.g., as may be the case if neuralinformation processing is understood more fully)ñbut these advances are themselves contingent onwork done over a considerably longer time scale.illustrative problem domains at the biocomp interfaceboth life scientists and computer scientists will draw inspiration and derive utility from otherfieldsñincluding each otherõsñas they see fit. nevertheless, one way of making progress is to addressproblems that emerge naturally at the biocomp interface. problemfocused research carries the majoradvantage that problems offered by nature do not respect disciplinary boundaries; hence, in makingprogress against challenging problems, practitioners of different disciplines must learn to work onproblems that are shared.the biocomp interface drives many problem domains in which the expenditure of serious intellectual effort can reasonably be expected to generate significant new knowledge in biology and/or computing. compared to many of grand challenges in computational biology outlined over the past twodecades, making significant progress in these problem domains will call for a longer time scale, greaterresources, and more extensive basic progress in computing and in biology.biological insight could take different formsñthe ability to make new predictions, the understanding of some biological mechanism, the construction of a synthetic biological mechanism. the same istrue for computingñinsight might take the form of a new biologically inspired approach to somecomputing problem, different hardware, or novel architecture.this report discusses a number of interesting problem domains at the biocomp interface, but giventhe breadth of the cognizant scientific arenas, no attempt is made to be exhaustive. rather, topics havebeen selected to span a space of possible problem domains, and no inferences should be made concerning the omission of any problem from this list. the problem domains discussed in this report includehighfidelity cellular modeling and simulation, the development of a synthetic cell, neural informationprocessing and neural prosthetics, evolutionary biology, computational ecology, models that facilitateindividualized medicine, a digital human on which a surgeon can operate virtually, computationaltheories of selfassembly and selfmodification, and a theory of biological information and complexity.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.executive summary5the role of organization and infrastructure in creatingopportunities at the interfacethe committee believes that over time, computing will assume an increasing role in the workinglives of nearly all biologists. but given the societal benefits that accompany a fuller and more systematicunderstanding of biological phenomena, it is better if the computingenabled 21st century biologyarrives sooner rather than later.this point suggests that cultural and organizational issues have at least as much to do with thenature and scope of the biological embrace of computing as do intellectual ones. the report discussesbarriers to cooperation arising from differences in organizational culture and differences in intellectualstyle.consider organizational cultures. in many universities, for example, it is difficult for scholars working at the interface between two fields to gain recognition (e.g., tenure, promotion) from eitherña factthat tends to drive such individuals toward one discipline or another. the shortterm goals in industrialsettings also inhibit partnerships along the interface because of the longer time frame for payoff. nonetheless, the committee believes that a synergistic cooperation between practitioners in each field, in bothbasic and applied settings, will have enormous payoffs despite the real differences in intellectual style.coordination costs are another issue, because they increase with interdisciplinary work. computerscientists and biologists are likely to belong to different departments or universities, and when they tryto work together, the lack of physical proximity makes it harder for collaborators to meet, to coordinatestudent training, and to share physical resources. in addition, bigger projects increase coordinationcosts, and interdisciplinary projects are often larger than unidisciplinary projects. such costs are reflected in delays in project schedules, poor monitoring of progress, and an uneven distribution ofinformation and awareness of what others in the project are doing. they also reduce peopleõs willingness to tolerate logistical problems that might be more tolerable in their home contexts, increase thedifficulty of developing mutual regard and common ground, and can lead to more misunderstandings.differences of intellectual style occur because the individuals involved are first and foremost intellectuals. for example, for the computer scientist, the notions of modeling systems and using abstractions are central to his or her work. using these abstractions and models, computer scientists are able tobuild some of the most complex artifacts known. but manyñperhaps mostñbiologists today have adeep skepticism about theory and models, at least as represented by mathematicsbased theory andcomputational models. and many computer scientists, mathematicians, and other theoretically inclinedresearchers fail to recognize the complexity inherent in biological systems. as a result, there is often anintellectual tension between simplification in service of understanding and capturing details in serviceof fidelityñand such a tension has both positive and negative consequences.cooperation will require that practitioners in each field learn enough about the other to engage insubstantive conversations about hard biological problems. to take one of the most obvious examples,the different fields place different emphases on the role of empirical data vis‹vis theory. accurate datafrom biological organisms impose òhardó constraints on the biologist in much the same way that resultsfrom theoretical computer science impose hard constraints on the computer scientist. a second exampleis that whereas computer scientists are trained to develop general solutions that give guarantees aboutevents in terms of their worstcase performance, biologists are interested in specific solutions that relateto very particular (though voluminous) datasets.finally, institutional difficulties often arise in academic settings for work that is not traditional ornot easily identified with existing departments. these differences derive from the structure and cultureof departments and disciplines, and they lead to scientists in different disciplines having differentintellectual and professional goals and experiencing different conditions for their career success. collaborators from different disciplines must find and maintain common ground, such as agreeing ongoals for a joint project, but must also respect one anotherõs separate priorities, such as having topublish in primary journals, present at particular conferences, or obtain tenure in their respectivecatalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.6catalyzing inquirydepartments according to departmental criteria. such crosspressures and expectations from homedepartments and disciplinary colleagues remain even if the participants in a collaboration developsimilar goals for a project.findings and recommendations at the outset, the committee had hoped to identify a deep symmetry between computing andbiology. that is, it is clear that the impact of computing on biology is increasingly profound, and thesymmetrical notion would be that biology would have a comparable effect on computing. however,this proved not to be the case. the impact of computing on biology will be deep and profound, andindeed will span virtually all areas of life sciences research, and in this direction a focus on interestingproblem domains (some of which are illustrated above) is a reasonable way to proceed. by contrast,research that explores the impact of biology on computing falls much more into the òhighrisk, highpayoffó category. that is, the ultimate value of biology for changing computing paradigms in deep andfundamental ways is as yet unproven. nevertheless, various biological attributesñrobustness, adaptation, damage recovery, and so onñare so desirable from a computing point of view that any intellectualinquiry is valuable if it can contribute to humanengineered artifacts with these attributes.it is also clear that a number of other areas of inquiry are associated with the biocomp interface; inaddition to biology and computing, the interface also draws from chemistry, materials science, bioengineering, and biochemistry. three of the most important efforts, which can be loosely characterized asdifferent flavors of biotechnology, are (1) analytical biotechnology (which involves the application ofbiotechnological tools for the creation of chemical measurement systems); (2) materials biotechnology(which entails the use of biotechnological methods for the fabrication of novel materials with uniqueoptical, electronic, rheological, and selective transport properties); and (3) computational biotechnology(which focuses on the potential replacement of silicon devices with nanoscale biomolecularbased computational systems).the committee underscores the importance of building human capital and, within that enterprise,the special significance of educational innovation at the biocomp interface. the committee endorses thecall from other reports that recommend greater training in quantitative sciences (e.g., mathematics,computer sciences) for biologists, but it also believes that students of the new biology would benefitgreatly from some study of engineering. just as engineers must construct physical systems to operate inthe real world, so also must nature operate under these same constraintsñphysical lawsñto òdesignósuccessful organisms. despite this fundamental similarity, biology students rarely learn the importantanalysis, modeling, and design skills common in engineering curricula. the committee believes that theparticular area of engineering (electrical, mechanical, computer, etc.) is probably much less relevantthan exposure to essential principles of engineering design: the notion of tradeoffs in managing competing objectives, control systems theory, feedback, redundancy, signal processing, interface design,abstraction, and the like.of course, more than education will have to change. fifty years ago, academic biology had tochoose between altering the thendominant styles of research to embrace molecular biology or riskingobsolescence. the committee believes that a new dawn is visibleñand just as molecular biology hasbecome simply part of the biological sciences as a whole, so also will computational biology ultimatelybecome simply a part of the biological sciences. in the interim, however, considerable effort will berequired to build and sustain the infrastructure and to train a generation of biologists and computerscientists who can choose the right collaborators to thrive at the biocomp interface.the committee believes that 21st century biology will be based on a synergistic mix of reductionistand systems biologies. for systems biology researchers, the committee emphasizes that empirical andexperimental hypothesistesting research will continue to be central in providing experimental verification of putative discoveriesñand indeed, relevant as much to studies of how components interact as tostudies of components themselves. thus, disparaging rhetoric about the inadequacies and failures ofcatalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.executive summary7reductionist biology and overheated zeal in promoting systems biology should be avoided. for researchers more oriented toward experimental or empirical work, the committee emphasizes that systems biology will be central in formulating novel, interesting, and in some cases counterintuitive hypotheses to test. the point suggests that agencies that have traditionally supported hypothesistestingresearch would do well to cast a wide òdiscoveryó net that supports the development of alternativehypotheses as well as research that supports traditional hypothesis testing.twentyfirst century biology will require leadership from both biology and computing that linkstogether firstclass research efforts in their respective domains. these efforts will necessarily crosstraditional institutional boundaries. for example, research efforts in scientific computing will have toexist in both clinical and biological environments if they are to couple effectively to problem domains inthe life sciences. establishment of a pervasive national infrastructure for life sciences research (including the construction of interdisciplinary teams) and development of the requisite itenabled tools forthe larger community will require both sustained funding and rigorous oversight. likewise, the departmental imperatives that characterize much of academe will have to be modified if work at the biocompinterface is to flourish.in general, the committee believes that the most important change in funding policy for the supporters of this area would be to broaden the kinds of work for which they offer support to include thedevelopment of technology for data acquisition and analysis and exploratory research that results in thegeneration of interesting hypotheses to be tested. that said, there is a direct relationship between thespeed with which research frontiers advance and the levels of funding allocated to them. although itunderstands the realities of a budgetconstrained environment, the committee would gladly endorse anincreased flow of funding to the furtherance of a truly integrated 21st century biology.as for the support of biologically inspired computing, the committee believes that its highrisk,highpayoff nature means that supporting agencies should take a broad view of what òbiological inspirationó means and should support the field on a levelofeffort basis, recognizing the longterm natureof such work and taking into account the number of researchers doing and likely to do good work inthis area and the potential availability of other avenues to improved computing.from the committeeõs perspective, the highlevel goals articulated by the agencies and programsthat support work related to biologyõs potential contribution to computing seem generally sensible.this is not to say that every proposal supported under the auspices of these agenciesõ programs wouldnecessarily have garnered the support of the committeeñbut that would be true of any research portfolio associated with any program.one important consequence of supporting highrisk research is that it is unlikely to be successful inthe short term. researchñparticularly of the highrisk varietyñis often more òmessyó and takes longerto succeed than managers would like. managers understandably wish to terminate unproductive linesof inquiry, especially when budgets are constrained. but shortterm success cannot be the only metric ofthe value of research, because when it is, funding managers invite hyperbole and exaggeration on thepart of proposal submitters, and unrealistic expectations begin to characterize the field. those believingthe hyperbole (and those contributing to it as well) thus overstate the importance of the research and itscentrality to the broader goal of improving computing. when unrealistic expectations are not met (andthey will not be met, almost by definition), disillusionment sets in, and the field becomes disfavoredfrom both a funding and an intellectual standpoint.from this perspective, it is easy to see why support for certain fields rises rapidly and then dropsprecipitously. wild budget fluctuations and an unpredictable funding environment that changes goalsrapidly can damage the longterm prospects of a field to produce useful and substantive knowledge.funding levels do matter, but programs that provide steady funding in the context of broadly stated butconsistent intellectual goals are more likely to yield useful results than those that do not.thus, the committee believes that in the area of biologically inspired computing, funding agenciesshould have realistic expectations, and these expectations should be relatively modest in the near term.intellectually, their programs should continue to take a broad view of what òbiological inspirationócatalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.8catalyzing inquirymeans. funding levels in these areas ought to be established on a levelofeffort basis (i.e., what theagency believes is a reasonable level of effort to be expended in this area), by taking into account thenumber of researchers doing and likely to do good work in an area and the potential availability of otheravenues to improved computing. in addition, programmatic continuity for biologically inspired computing should be the rule, with playing rules and priorities remaining more or less constant in theabsence of profound scientific discovery or technology advances in the area.closing thoughtsthe impact of computing on biology can fairly be considered a paradigm change as biology entersthe 21st century. twentyfive years ago, biology saw the integration of multiple disciplines from thephysical and biological sciences and the application of new approaches to understand the mechanismsby which simple bacteria and viruses function. the impact of the early efforts was so significant that anew discipline, molecular biology, emerged, and many biologists, including those working at the levelof tissues or systems and whole organisms, came to adopt the approaches and even often the techniques. molecular biology has had such success that it is no longer a discipline but simply part of lifesciences research itself.today, the revolution lies in the application of a new set of interdisciplinary tools: computationalapproaches will provide the underpinning for the integration of broad disciplines in developing aquantitative systems approach, an integrative or synthetic approach to understanding the interplay ofbiological complexes as biological research moves up in scale. bioinformatics provides the glue forsystems biology, and computational biology provides new insights into key experimental approachesand how to tackle the challenges of nature. in short, computing and information technology applied tobiological problems is likely to play a role for 21st century biology that is in many ways analogous to therole that molecular biology has played across all fields of biological research for the last quartercenturyñand computing and information technology will become embedded within biological research itself.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.introduction991introduction1.1excitement at the interface of computing and biologysustained progress across all areas of science and technology over the last halfcentury has transformed the expectations of society in many ways. yet, even in this context of extraordinary advances,both the biological sciences and the computer and information sciences share a number of characteristics that are compelling.first, both fields have been characterized by exponential growth, with doubling times on the orderof 12 years. in information technology (it), both the component density of microprocessors and theinformation storage density on hard disk drives have increased exponentially with doubling times from9 to 18 months. in biology, the rate of growth of the biological literature is characterized by exponentialgrowth as well (e.g., the growth in genbank is on the order of 60 percent per year, a rate comparable tomooreõs law for microprocessors). while these growth rates cannot continue indefinitely, exponentialgrowth is likely at least in the short term.second, both fields deal with organisms and phenomena or artifacts of astounding complexity.both biological organisms and sophisticated computer systems involve very large numbers of components and interconnections between them, and out of these assemblages of components and connections emerges interesting and useful functionality. in the information technology context, the significance of these connections and components is much better understood than in the biological context,not least because human beings have been responsible for the design of information technologysystems such as operating systems and computer systems. still, the capabilities of existing computingmethodologies to design or characterize largescale information systems and networks are beingstretched, and in the biological domain, a systemslevel understanding of biological or computernetworks is both highly important and difficult to achieve. in addition, information technology is anecessary and enabling technology for the study of complex objects. computers are the scientificinstruments that let us see genomes just as electron microscopes let us see viruses, or radio telescopeslet us see quasars.third, both biology and information technology have profound and revolutionary implications forscience and society. from an intellectual standpoint, biology offers at least partial answers to eternalquestions such as, what is life? also, biological science and technology have the potential for greatimpact on human health and wellbeing, including improved disease treatments, rapid environmentalcatalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.10catalyzing inquirycleanup, and more robust food production. computing and information technology enable humanbeings to acquire, store, process, and interpret enormous amounts of information, and continue tounderpin much of modern society.finally, several important areas of interaction between the two fields have already emerged, andthere is every expectation that more will emerge in the future. indeed, the belief of the committee thatthere are many more synergies at the interface between these two fields than have been exploited todate is the motivation for this report. against this backdrop, it makes good sense to consider potentialinteractions between the two fieldsñwhat this report calls the òbiocompó interface.as for the nature of computing that can usefully be exploited by life scientists, there is a range ofpossibilities. for some problems encountered by biology researchers, a very rudimentary knowledge ofcomputing and information technology is quite sufficient. however, as problems become bigger and/ormore complex, what one may pick up by hacking and reading manuals is no longer sufficient. toaddress such problems, the kinds and levels of expertise needed are more likely to require significantformal study of computer science (e.g., as an undergraduate major in the field). and for still moredifficult, larger, or more complex problems, the kinds and levels of expertise needed stretch the currentstate of knowledge of the fieldña point that illuminates the importance of real computer scienceresearch in a biological context.nor is the utility of computing limited to providing tools or modelsñno matter how sophisticatedñfor biologists to use. as discussed in chapter 6, computing can also provide intellectual abstractions that may provide insight into biological phenomena and a useful language for describing suchphenomena. as one example, notions of circuit and network and modularityñoriginally conceptualized in the world of engineering and computer scienceñhave much applicability to understandingbiological phenomena.on the other side, biology refers to the scientific study of the activities, processes, mechanisms,and other attributes of living organisms. for the purposes of this report, biology, biomedicine, lifesciences, and other descriptions of research into how living systems work should be regarded assynonymous. in this context, for the past decade, researchers have spoken increasingly of a newbiology, a biology of the 21st century, one that is driven by new technologies, that is more automatedwith tools and methods provided by industrial models, and that often entails highthroughput dataacquisition.1 this report examines the biocomp interface from the perspective of 21st century biology, as a science that integrates traditional empirical and experimental biology with a systemslevelbiology that considers the multiscale, hierarchical, highly interwoven, or interactive aspects intrinsicto living systems.1.2perspectives on the biocomp interfacethis report addresses computationally inspired ways of understanding biology and biologicallyinspired ways of understanding computing. although the committee started its work with the idea thatit would discover a single community and intellectual synthesis of biology and computing, closerexamination showed that the appropriate metaphor is one of an interface between the two fields ratherthan a common, shared area of inquiry. thus, the adventures along the frontier cannot be treated ascoming from a single community, and the different objectives have to be recognized.1for example, see national research council, opportunities in biology, national academy press, washington, dc, 1989. highthroughput data acquisition is an approach that relies on the largescale parallel interrogation of many similar biological entities.such an approach is essential for the conduct of global biological analyses, and it is often the approach of choice for rapid andcomprehensive assessment of biological system properties and dynamics. see, for example, t. ideker, t. galitski, and l. hood,òa new approach to decoding life: systems biology,ó annual review of genomics and human genetics 2:343372, 2001. a numberof the highthroughput data acquisition technologies mentioned in that article are discussed in chapter 7 of his report.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.introduction111.2.1from the biology sidebiologists have a long history of applying tools from other disciplines to provide more powerfulmethods to address or even solve their research problems. for example, anton van leeuwenhoekõsinvention of the optical microscope in the late 1600s opened up a previously unknown world andultimately brought an entirely new vista to biologyñnamely, the existence of cells and cellular structures. this remarkable revolutionary discovery would have been impossible without the study ofopticsñand leeuwenhoek was a clockmaker.the biological sciences have drawn heavily from chemistry, physics, and more recently, mathematical modeling. indeed, the reductionist revolution in biological sciencesñwhich led to the current stateof understanding of biological function and mechanism at the molecular level or of specific areas suchas neurophysiologyñin the past five decades began as chemists, physicists, microbiologists, and othersinteracted and created what is now known as molecular biology. the applications from the physicalsciences are already so well established that it is unnecessary to discuss them at length.mathematics and statistics have at times played important roles in designing and optimizing biological experiments. for example, statistical analysis of preliminary data can lead to improved datacollection and interpretation in subsequent experiments. in many cases, simple mathematical or physical ideas, accompanied by calculations or models, can suggest experiments or lead to new ideas that arenot easily identified with biological reasoning alone. an example of this category of contribution iswilliam harveyõs estimation of the volume of the blood and his finding that a closed circulatory systemwould explain the anomaly in such calculations. traditionally, biologists have resisted mathematicalapproaches for various reasons discussed at length in chapter 10. to some extent, this history is beingchanged in modern biology, and it is the premise of this report that an acceleration of this change ishighly worthwhile.approaches borrowed from another discipline may provide perspectives that are unavailable frominside the disciplinary research program itself. in some cases, these lead to a new integrative explanation or to new ways of studying and appreciating the intricacies of biology. in other cases, this borrowing opens an entirely new subfield of biology. the discovery of the helical structure and the òcodeó ofdna, impossible without crystallography and innovative biological thinking, is one example. theunderstanding of electrical signaling in neurons by voltagegated channels, and the hodgkinhuxleyequations (based on the theory of electrical circuits), constitute another example. both of these approaches revolutionized the way biology was conducted and required significant, skilled input fromother fields.the most dramatic scenarios arise when major subfields emerge. an example dating back somedecades, and described above in another context, is molecular biology, whose tools and techniques(using advanced chemistry, physics, and equipment based on the above) changed the face of biology. amore recent, current example is genomics with its indelible mark on the way that biology as a disciplineis conducted and will be conducted for years to come.the committee believes that from the perspective of the biology researcher, there is both substantiallegacy and future promise regarding the application of computing to biological problems. some of thislegacy is manifested in a severaldecade development of privatesector databases (mostly those ofpharmaceutical companies) and software for data analysis, in publicsector genetic databases, in the useof computergenerated visualization, and in the use of computation to determine the crystal structuresof increasingly complex biomolecules.2several life sciences research fields have begun to take computational approaches. for example,ecology and evolution were among the first subfields of biology to develop advanced computationalsimulations based on theory and models of ecosystems and evolutionary pathways. cardiovascular2see, for example, t. headgordon and j.c. wooley, òcomputational challenges in structural and functional genomics,ó ibmsystems journal 40(2):265296, 2001, available at http://www.research.ibm.com/journal/sj/402/headgordon.pdf.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.12catalyzing inquiryphysiology and studies of the structure and function of heart muscle have involved bioengineeringmodels and combined experimental and computational approaches. all of these computational approaches would have been impossible without solid preexisting mathematical models that led to theintuition and formed the basis for the emerging computational aspects.nevertheless, genomics research is simply not possible without information technology. it is not anexaggeration to say that it was the sequencing of complete genomes, more than any other researchactivity, that brought computational and informatics approaches to the forefront of life sciences research, as well as identifying the need for basic underlying algorithms to tackle biological problems.only through computational analysis have researchers begun to uncover the implications of genomicscale sequence data. apart from specific results thereby obtained, such analysis, coupled with theavailability of complete genomic sequences, has changed profoundly how many biologists think, conduct research, and plan strategically to address central research problems.today, computing is essential to every aspect of molecular and cell biology, as researchers expandtheir scope of inquiry from gene sequence analysis to broader investigations of biological complexity.this scope includes the structure and function of proteins in the context of metabolic, genetic, andsignaling networks, the sheer complexity of which is overwhelming. future challenges include theintegration of organ physiology, catalogs of specieswide phenotypic variations, and understanding ofdifferences in gene expression in various states of health and disease.1.2.2from the computing sidefrom the viewpoint of the computer scientist, there is an asyetunfulfilled promise that biologymay have significant potential to influence computer design, component fabrication, and software.today, the impact of biology and biological sciences on advances in computing is more speculative thanthe reverse (as described in section 1.2.1), because such considerations are, with only a few exceptions,relevant to future outcomes and not to what has been or is already being delivered.in one sense, this should not be very surprising. computing is a òscience of the artificial,ó3 whereasbiology is a science of the natural, and in general, it is much easier for humans to understand both thefunction and the behavior of a system that they have designed to fulfill a specific purpose than tounderstand the internal machinery of a biological black box that evolved as a result of forms andpressures that we can only sketchily guess.4 thus, paths along which biology may influence computingare less clear than the reverse, and work in this area should be expected to have longer time horizonsand to take the form of many largely independent threads, rather than a hierarchy of interrelated orintellectual thrusts.nevertheless, exploring why the biological sciences might be relevant to computing is worthwhilein particular because biological systems possess many qualities that would be desirable in the information technology that humans use. for example, computer and information scientists are looking forways to make computers more adaptive, reliable, òsmarter,ó faster, and resilient. biological systemsexcel at finding and learning adequateñbut not necessarily optimalñsolutions to illposed problemson time scales short enough to be useful to them. they efficiently store òdata,ó integrate òhardwareóand òsoftware,ó selfcorrect, and have many other properties that computing and information science3òwe speak of engineering as concerned with ôsynthesis,õ while science is concerned with ôanalysis.õ synthetic or artificial objectsñand more specifically prospective artificial objects having desired propertiesñare the central objective of engineering activityand skill. the engineer, and more generally the designer, is concerned with how things ought to beñhow they ought to be in orderto attain goals, and to function.ó h.a. simon, sciences of the artificial, 3rd ed., mit press, cambridge, ma, 1996, pp. 45.4this is what neuroscientist valentino braitenberg called his law of uphill analysis and downhill synthesis, in vehicles: experiments in synthetic psychology, mit press/a bradford book, cambridge, ma, 1984. cited in daniel c. dennett, òcognitive scienceas reverse engineering: several meanings of ôtopdownõ and ôbottomupõ,ó proceedings of the ninth international congress of logic,methodology and philosophy of science, d. prawitz, b. skyrms, and d. westerstahl, eds., elsevier science northholland, 1994.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.introduction13might capture in order to achieve its future goals. especially for areas in which computer science lacksa welldeveloped theory or analysis (e.g., the behavior of complex systems or robustness), biology mayhave the most to contribute.to hint at some current threads of inquiry, some researchers envision a hybrid deviceña biologicalcomputerñessentially, an organic tool for accomplishing what is now carried out in silicon. as aninformation storage and processing medium, dna itself may someday be the substance of a massivelydense memory storage device, although today the difficulties confronting the work in this area aresignificant. dna may also be the basis of nanofabrication technologies.biomimetic devices are mechanical, electrical, or chemical systems in which an attempt has beenmade to mimic the way that a biological system solves a particular problem. successes include roboticlocomotion (based on legged movements of arthropods), artificial blood or skin, and others. approacheswith generalpurpose applicability are less clearly successes, though they are still intriguing. theseinclude attempts to develop approaches to computer security that are modeled on the mammalianimmune system and approaches to programming based on evolutionary concepts.hybrid systems are a promising new technology for measurement of or interaction with smallbiological systems. in this case, hybrid systems refer to silicon chips or other devices designed tointeract directly with a biological sample (e.g., record electrical activity in the flight muscles of a moth)or analyze a small biological sample under field conditions. here the applications of the technologyboth to basic scientific problems and to industrial and commercially viable products are exciting.in the domain of algorithms, swarm intelligence (a property of certain systems of nonintelligent,independently acting agents that collectively exhibit intelligent behavior) and neural nets offer approaches to programming that are radically different from many of todayõs models. such applications ofbiological principles to nonbiological computing could have much value, and chapter 8 addresses ingreater detail some possible biological inspirations for computing. yet it is also possible that a betterunderstanding of informationprocessing principles in biological systems will lead as well to greaterbiological insight; so the dividing line between òapplying biological principles to information processingó and òunderstanding biological information processingó is not as clear as it might appear atfirst glance. moreover, even if biology ultimately proves unhelpful in providing insight into potentialcomputing solutions, it is still a problem domain par excellenceñone that offers interesting intellectual challenges in which progress will require that the state of computing research be stretchedimmeasurably.1.2.3the role of organization and culturethe possibilityñor even the factñthat one field may be well positioned to make or facilitate significant intellectual contributions to the other does not, by itself, lead to harmonious interchange betweenpractitioners in the two fields. cultural and organizational issues are also very much relevant to thesuccess or failure of collaborations across different fields. for example, one important issue is the factthat much of todayõs biological research is done in individual laboratories, whereas many interestingproblems of 21st century biology will require interdisciplinary teams and physical or virtual centerswith capable scientists, distributed wherever they work, involved in addressing difficult problems.twentyfirst century biology will also see the increasing importance of research programs that havea more industrial flavor and involve greater standardization of instruments and procedures. a smallexample is that reagent kits are becoming more and more popular, as labs realize that the small advantages that might accrue through the use of a set of customized reagents are far outweighed by thesavings in effort associated with the use of such kits. a larger example might be shared devices andequipment of largerscale and assemblylinelike processes that replace the craft work of individualtechnicians.as biologists recognize the inherent difficulties posed by the dataintensive nature of these newresearch strategies, they will require differentñand additionalñtraining in quantitative methods andcatalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.14catalyzing inquiryscience. computing is likely to be central, but since the nature and scope of the computing required willgo far beyond what is typically taught in an introductory computing course, real advancement of thefrontier will require that computer scientists and biologists recognize and engage each other as intellectual coequals. at the same time, computer scientists will have to learn enough about biology to understand the nature of problems interesting to biologists and must refrain from regarding the problemdomain as a òmereó application of computing.the committee believes that such peerlevel engagement happens naturally, if slowly. but accelerating the cultural and organizational changes needed remains one of the key challenges facing the communities today and is one that this report addresses. such considerations are the subject of chapter 10.1.3imagine whatõs nextin the long term, achievements in understanding and harnessing the power of biological systemswill open the door to the development of new, potentially farreaching applications of computing andbiologyñfor example, the capability to use a blood or tissue sample to predict an individualõs susceptibility to a large number of afflictions and the ability to monitor disease susceptibility from birth,factoring in genetics and aging, diet, and other environmental factors that influence the bodyõs functions over time and ultimately to treat such ailments.likewise, 21st century biology will advance the abilities of scientists to model, before a treatment isprescribed, the likely biological response of an individual with cancer to a proposed chemotherapyregime, including the likelihood of the effectiveness of the treatment and the side effects of the drugs.indeed, the promise of 21st century biology is nothing less than a systemwide understanding of biological systems both in the aggregate and for individuals. such understanding could have dramaticeffects on health and medicine. for example, detailed computational models of cellular dynamics couldlead to mechanismbased target identification and drug discovery for certain diseases such as cancer,5to predictions of drug effects in humans that will speed clinical trials,6 and to a greater understandingof the functional interactions between the key components of cells, organs, and systems, as well as howthese interactions change in disease states.7on another scale of knowledge, it may be possible to trace the genetic variability in the worldõshuman populations to a common ancestral set of genesñto discover the origins of the earliest humans,while learning, along the way, about the earliest diseases that arose in humans, and about the biologicalforces that shape the worldõs populations. work toward all of these capabilities has already begun, asbiologists and computer scientists compile and consider vast amounts of information about the geneticvariability of humans and the role of that variability in relation to evolution, physiological functions,and the onset of disease.at the frontiers of the interface, remarkable new devices can be pictured that draw on biology forinspiration and insight. it is possible to imagine, for example, a walking machineñan independent setof legs as agile, stable, and energyefficient as those of humans or animalsñable to negotiate unknownterrain and recover from falls, capable of exploring and retrieving materials. such a machine wouldovercome the limitations of presentday rovers that cannot do such things. biologists and computerscientists have begun to examine the locomotion of living creatures from an engineering and biologicalperspective simultaneously, to understand the physical and biological controls on balance, gait, speed,and energy expended and to translate this information into mechanical prototypes.5j.b. gibbs, òmechanismbased target identification and drug discovery in cancer research,ó science 287:1969, 2000.6c. sander, ògenomic medicine and the future of health care,ó science 287:1977, 2000.7d. noble, òmodeling the heartñfrom genes to cells to the whole organ,ó science 295:1678, 2002.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.introduction15box 1.1illustrative research areas at the interface of computer science and biology¥structure determination of biological molecules and complexes¥simulation of protein folding¥whole genome sequence assembly¥whole genome modeling and annotation¥full genomegenome comparison¥rapid assessment of polymorphic genetic variations¥complete construction of orthologous and paralogous groups of genes¥relating gene sequence to protein structure¥relating protein structure to function¥in silico drug design¥mechanistic enzymology¥cell network analysissimulation of genetic networks and the sensitivity of these pathways to componentstoichiometry and kinetics¥dynamic simulation of realistic oligomeric systems¥modeling of cellular processes¥modeling of physiological systems in health and disease¥modeling behavior of schools, swarms, and their emergent behavior¥simulation of membrane structure and dynamic function¥integration of observations across scales of vastly different dimension and organization for modelcreation purposes¥development of bioinspired autonomous locomotive devices¥development of biomimetic devices¥bioengineering prostheticswe can further imagine an extension of presentday bioengineering from mechanical hearts andtitanium hip joints to an entirely new level of devices, such as an implantable neural prosthetic thatcould assist stroke patients in restoring speech or motor control or could enhance an individualõscapability to see more clearly in the dark or process complex information quickly under pressure. sucha prosthetic would marry the speed of computing with the brainõs capacity for intelligence and wouldbe a powerful tool with many applications.with the advancement of computational power and other capabilities, there is a great opportunityand challenge in whether human functions can be represented in digital computational forms. one formof representation of a human being is how it is constructed, starting with genes and proteins. anotherform of representation is how a human being functions. human functions can be viewed at manydifferent levelsñphysioanatomical, motionmechanical, and psychocognitive, for example. if it werepossible to represent a human being at any or all of these functional levels, then a òdigital humanó couldbe created inside the computer, to be used for many applications such as medical surgical training,humancentered design of products, and societal simulation. (there are already such simulations atvarying levels of fidelity for particular organs such as the heart.)the potential breadth and depth of the interface of computing and biology are vast. box 1.1 is arepresentative list of research areas already being pursued at the interface; appendix b at the end of thisreport provides references to more detailed discussions of these efforts. the excitement and challenge ofall of these possibilities drive the increasing interest in and enthusiasm for research at the interface.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.16catalyzing inquiry1.4some relevant history in building the interface1.4.1the human genome projectaccording to cookdeegan,8 the human genome project resulted from the collective impact ofthree independent public airings of the idea that the human genome should be sequenced. in 1985,robert sinsheimer and others convened a group of scientists to discuss the idea.9 in 1986, renatodulbecco noted that sequencing the genome would be an important tool in probing the genetic originsof cancer.10 then in 1988, charles delisi developed the idea of sequencing the genome in the context ofunderstanding the biological and genetic effects of ionizing radiation on survivors of the hiroshima andnagasaki atomic bombs.11in 1990, the international human genome consortium was launched with the intent to map andsequence the totality of human dna (the genome).12 on april 14, 2003, not quite 50 years to the dayafter james watson and francis crick first published the structure of the dna double helix,13 officialsannounced that the human genome project was finished.14 after 13 years and $2.7 billion, the international effort had yielded a virtually complete listing of the human genetic code: a sequence some 3billion base pairs long.151.4.2the computingtobiology interfacefor most of the electronic computing age, biological computing applications have been secondarycompared to those associated with the physical sciences and the military. however, over the last twodecades, use by the biological sciencesñin the form of applications related to protein modeling andfoldingñwent from virtually nonexistent to being the largest user of cycles at the national sciencefoundation centers for high performance computing by fy 1998. nor has biological use of computingcapability been limited to supercomputing applicationsña plethora of biological computing applications have emerged that run on smaller machines.during the last two decades, federal agencies also held a number of workshops on computationalbiology and bioinformatics, but until relatively recently, there was no prospect for significant support8cookdeeganõs perspective on the history of the human genome project can be found in r.m. cookdeegan, the gene wars:science, politics, and the human genome, w.w. norton and company, new york, 1995.9r. sinsheimer, òthe santa cruz workshop,ó genomics 5(4):954956, 1989.10r. dulbecco, òa turning point in cancer research: sequencing the human genome,ó science 231(4742):10551056, 1986.11c. delisi, òthe human genome project,ó american scientist 76:488493, 1988.12cookdeegan identifies three independent public airings of the idea that the human genome should be sequenced, airingsthat collectively led to the establishment of the hgp. in 1985, robert sinsheimer and others convened a group of scientists todiscuss the idea. (see r. sinsheimer, òthe santa cruz workshop,ó genomics 5(4):954956, 1989.) in 1986, renato dulbecco notedthat sequencing the genome would be an important tool in probing the genetic origins of cancer. (see r. dulbecco, òa turningpoint in cancer research: sequencing the human genome,ó science 231(4742):10551056, 1986.) in 1988, charles delisi developed the idea of sequencing the genome in the context of understanding the biological and genetic effects of ionizing radiationon survivors of the hiroshima and nagasaki atomic bombs. (see c. delisi, òthe human genome project,ó american scientist76:488493, 1988.) cookdeeganõs perspective on the history of the human genome project can be found in r. cookdeegan, thegene wars: science, politics, and the human genome, w.w. norton and company, new york, 1995.13j.d. watson and f.h. crick, òmolecular structure of nucleic acids: a structure for deoxyribose nucleic acid,ó nature171(4356):737738, 1953.14the òcompletionó of the project had actually been announced once before, on june 26, 2000, when u.s. president bill clintonand british prime minister tony blair jointly hailed the release of a preliminary, draft version of the sequence with loud mediafanfare. however, while that draft sequence was undoubtedly useful, it contained multiple gaps and had an error rate of onemistaken base pair in every 10,000. the muchrevised sequence released in 2003 has an error rate of only 1 in 100,000, and gaps inonly those very rare segments of the genome that cannot reliably be sequenced with current technology. see http://www.genome.gov/11006929.15various histories of the human genome project can be found at http://www.ornl.gov/sci/techresources/humangenome/project/hgp.shtml.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.introduction17for academic work at the interface. the keck foundation and the sloan foundation supported training,and numerous database activities have been supported by federal agencies. as the impact of the humangenome project and comparative genomics began to reach the community as a whole, the situationchanged. an important step came from the howard hughes medical institute, which in 1999 held aspecial competition to select professors in bioinformatics and thus provided a strong endorsement ofthe role of computing in biology.in 1999, the national institutes of health (nih) also took a first step toward integrating ad hocsupport by requesting an analysis of the opportunities, requirements, and challenges from computingfor biomedicine. in june 1999, the botsteinsmarr working group on biomedical computing presenteda report to the nih entitled the biomedical information science and technology initiative.16 specificallytasked with investigating the needs of nihsupported investigators for computing resources, includinghardware, software, networking, algorithms, and training, the working group made recommendationsfor nih actions to support the needs of nihfunded investigators for biomedical computing.that report embraces a vision of computing as the hallmark of tomorrowõs biomedicine. to accelerate the transition to this new world of biomedicine, the working group sought to find ways òto discover,encourage, train, and support the new kinds of scientists needed for tomorrowõs science.ó much of thereport focuses on national programs to create òthe best opportunities that can be created for doing andlearning at the interfaces among biology, mathematics, and computation,ó and argues that òwith suchnew and innovative programs in place, scientists [would] absorb biomedical computing in due course,while supporting the mission of the nih.ó the report also identifies a variety of barriers to the fullexploitation of computation for biological needs.in the intervening 4 years, the validity of the botsteinsmarr working group report vision has notbeen in question; if anything, the expectations, opportunities, and requirements have grown. computation in various forms is rapidly penetrating all aspects of life sciences research and practice.¥stateoftheart radiology (and along with it other fields dependent on imagingñneurology, forexample) is highly dependent on information technology: the images are filtered, processed reconstructions that are acquired, stored, and analyzed computationally.¥genomics and proteomics are completely dependent on computation.¥integrative biology aimed at predictive modeling is not just computationally enabledñit literallycannot occur in a noncomputational environment.biomedical scientists of all stripes are increasingly using public resources and computational toolsat high levels of intensity such that very significant fractions of the overall effort are in this domain, andit is highly likely that these trends will continue. yet many of the barriers to full exploitation of computation in the biological sciences that were identified in the botsteinsmarr report still remain. oneprimary focus of the present report is accordingly to consider the intellectual, organizational, andcultural barriers that impede or even prevent the full benefits of computation from being realized forbiomedical research.1.4.3the biologytocomputing interfacethe application of biological ideas to the design of computing systems appears through much of thehistory of electronic computers, in most cases as an outgrowth of attempts to model or simulate abiological system. in the early 1970s, john h. holland (the first person in the united states to beawarded a ph.d. in computer science) pioneered the idea of genetic algorithms, which use simulatedgenetic processes (crossover, mutation, and inversion) to search a large solution space of algorithms.1716available at http://www.nih.gov/about/director/060399.htm.17j.h. holland, adaptation in natural and artificial systems, university of michigan press, ann arbor, 1975.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.18catalyzing inquirythis work grew out of research in the 1950s and 1960s to simulate just such processes in the naturalworld. a second wave of popularity of this technique came after john koza described genetic programming, which used similar techniques to modify symbolic expressions that comprised entire programs.18both of these approaches are in use today, especially in research and academic settings.the history of artificial neural networks also shows a strong relationship between attempts tosimulate biology and attempts to construct a new software tool. this research predates even the modernelectronic digital computers, since warren mcculloch and walter pitts published a model of a neuronthat incorporated analog weights into a binary logic scheme in 1943.19 this was meant to be used as amodel of biological neurons, not merely as an abstract computational processing approach. research onneural nets continued throughout the next decades, focusing on network architectures (particularlyrandom and layered), mechanisms of selfassembly, and pattern recognition and classification. significant among this research was rosenblattõs work on perceptrons.20 however, lack of progress caused aloss of interest in neural networks in the late 1970s and early 1980s. hopfield revived interest in the fieldin 1982,21 and progress throughout the 1980s and 1990s established neural networks as a standard toolfor learning and classifying patterns.a similar pattern characterizes research into cellular automata. john von neumannõs attempts toprovide a theory of biological selfassembly inspired him to apply traditional automata theory to a twodimensional grid;22 similar work was being done at the same time by stanislaw ulam (who may havesuggested the approach to von neumann). von neumann also showed that cellular automata couldsimulate a turing machine, meaning that they were a system that could provide universal computation.a boom of popularity for cellular automata followed the publication of the details of john conwayõsgame of life.23 in the early 1980s, stephen wolfram made important contributions to formalizingcellular automata, especially in their role in computational theory,24 and toffoli and margolus stressedthe general applicability of automata as systems for modeling.25at a more metaphorical level, ibm has taken initiatives in biologically inspired computing. specifically, ibm launched its autonomic computing initiative in 2001. autonomic computing is inspired bybiology in the sense that biological systemsñand in particular the autonomic nervous systemñarecapable of doing many things that would be desirable in complex computing systems. autonomiccomputing is conceived as a way to manage increasingly complex and distributed computing environments as traditional approaches to system management reach their limits. ibm takes special note of thefact that òthe autonomic nervous system frees our conscious brain from the burden of having to dealwith vital but lowerlevel functions.ó26 autonomic computing, by ibmõs definition, requires that asystem be able to configure and reconfigure itself under varying and unpredictable conditions, tocontinually optimize its workings, to recover from routine and extraordinary events that might cause18j.r. koza, ògenetically breeding populations of computer programs to solve problems in artificial intelligence,ó pp. 819827in proceedings of the second international conference on tools for artificial intelligence, ieee computer society press, los alamitos,ca, 1990.19w.s. mcculloch and w.h. pitts, òa logical calculus of the ideas immanent in nervous activity,ó bulletin of mathematicalbiophysics 5:115137, 1943.20r. rosenblatt, principles of neurodynamics: perceptrons and the theory of brain mechanisms, spartan books, washington, dc,1962.21j.j. hopfield, òneural networks and physical systems with emergent collective computational abilities,ó proceedings of thenational academy of sciences (usa) 79(8):25542558, 1982.22j. von neumann, theory of selfreproducing automata (edited and completed by a. w. burks), university of illinois press, 1966.23m. gardner, òmathematical games: the fantastic combinations of john conwayõs new solitaire game ôlifeõ,ó scientific american 223(october):120123, 1970.24s. wolfram, òcomputation theory of cellular automata,ó communications in mathematical physics 96:1557, 1984.25t. toffoli and n. margolus, cellular automata machines: a new environment for modeling, mit press, cambridge, ma, 1987.26g. ganek and t.a. corbi, òthe dawning of the autonomic computing era,ó ibm systems journal 42(1):518, 2003.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.introduction19some parts to malfunction in a manner analogous to the healing of a biological system, and to protectitself against dangers in its (open) environment.1.5background, organization, and approach of this reportto better understand potential synergies at the biocomp interface and to facilitate the developmentof collaborations between scientific communities in both fields that can better exploit these synergies,the national research council established the committee on frontiers at the interface of computingand biology. the committee hopes that this report will be valuable and important to a variety ofinterested parties and constituencies and that scientists who read it will be attracted by the excitementof research at the interface. to researchers in computer science, the committee hopes to demonstratethat biology represents an enormously rich problem domain in which their skills and talents can be ofenormous value in ways that go far beyond their value as technical consultants and also that they mayin turn be able to derive inspiration for solving computing problems from biological phenomena andinsights. to researchers in the biological sciences, the committee hopes to show that computing andinformation technology have enormous value in changing the traditional intellectual paradigms ofbiology and allowing interesting new questions to be posed and answered. to academic administrators,the committee hopes to provide guidance and principles that facilitate the conduct of research andeducation at the biocomp interface. finally, to funding agencies and organizations, the committeehopes to provide both a rationale for broadening the kinds of work they support at the biocompinterface and practices that can enhance and create links between computing and biology.a note on terminology and scope is required for this report. within the technology domain are anumber of interconnecting aspects implied by terms such as computing, computation, modeling, computer science, computer engineering, informatics, information technology, scientific computing, andcomputational science. today, there is no one term that defines the breadth of the science and technology within the computing and information sciences and technologies. the intent is to use any of theseterms with a broad rather than narrow construction and connotation and to consider the entire domainof inquiry in terms of an interface to life science. for simplicity, this report uses the term òcomputingóto refer to intellectual domains characterized by roots in the union of the terms above.although the words òcomputingó and òcomputationó are used throughout this report, biology inthe new millennium connects with a number of facets of the exact sciences in a way that cannot beseparated from computer science per se. in particular, biology has a synergistic relationship with mathematics, statistics, physics, chemistry, engineering, and theoretical methodsñincluding modeling andanalysis as well as computation and simulation. in this relationship, blind computation is no surrogatefor insight and understanding. in many cases, the fruits of computation are reaped only after carefuland deliberate theoretical analysis, in which the physics, biology, and mathematics underlying a givensystem are carefully considered. although much of the focus of this report is on the exchange betweenbiology and computing, the reader should consider how the same ideas may be extended to encompassthese other aspects.consider, for example, the fact that mathematics plays an essential role in the interpretation ofexperimental data and in developing algorithms for machineassisted computing. computing is implicitly mathematical, and as techniques for mathematical analysis evolve and develop, so will new opportunities for computing.these points suggest that any specific limits on the range of coverage of this report are artificial andsomewhat forced. yet practicality dictates that some limits be set, and thus the committee leaves systematic coverage of certain important dimensions of the biologycomputing interface to other reports.for example, a 2005 report of the board on mathematical sciences (bms) of the national researchcouncil (nrc) recommends a mathematical sciences research program that allows biological scientiststo make the most effective use of the large amount of existing genomic information and the much largerand more diverse collections of structural and functional genomic information that are being created,catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.20catalyzing inquirycovering both current research needs and some higherrisk research that might lead to innovativeapproaches for the future.27 the bms study takes a very broad look at what will be required forbioinformatics, biophysics, pattern matching, and almost anything related to the mathematical foundations of computational biology; thus, it is that bms report, rather than the present report, that addressesanalytical techniques.similar comments apply to the present reportõs coverage of medical devices based on embeddedinformation technologies and medical informatics. medical devices such as implanted defibrillators relyon realtime analysis of biological data to decide when to deliver a potentially lifesaving shock. medicalinformatics can be regarded as computer science applied directly to problems of medicine and healthcare, focusing on the management of medical information, data, and knowledge for medical problemsolving and decision making. medical devices and medical informatics have many links and similaritiesto the subject matter of this report, but they, too, are largely outside its scope, although from time totime issues and challenges from the medical area are mentioned. comprehensive studies describingfuture needs in medical informatics and medical devices must await future nrc work.yet another area of concern unaddressed in this report is the area of ethics associated with the issuesdiscussed here. to ask just a few questions: who will own dna data? what individual biomedical datawill be collected and retained? what are the ethics involved in using this data? what should individualsbe told about their genetic futures? what are the ethical implications of creating new biological organisms or of changing the genetics of already living individuals? all of these questions are important, andphilosophers and ethicists have begun to address some of them, but they are outside the scope of thisreport or the expertise of the committee.in developing this report, the committee chose to characterize the overarching opportunities at theinterface of biology and the computer and information sciences, and to highlight several diverse examples of activities at the interface. these points of intersection broadly represent and illustrate characteristics of research along the interface and include promising areas of exploration, some exciting froma basic science perspective and others from the point of view of novel applications.chapter 2 presents perspectives on 21st century biology, a synthesis among a variety of differentintellectual approaches to biological research. chapter 3 is a discussion of the nature of biological dataand the requirements that biologists put on data.chapter 4 discusses computational tools for biology that help to solve specific and precisely definedproblems. chapter 5 focuses on models and simulations in biology as approaches for exploring andpredicting biological phenomena.chapter 6 describes the value of a computational and engineering perspective in characterizingbiological functionality of interest. chapter 7 addresses roles in biological research for cyberinfrastructure and technologies for data acquisition.chapter 8 describes the potential of computer science applications and processes to utilize biological systemsñto emulate, mimic, or otherwise draw inspiration from the organization, behavior, andstructure of living things or to make use of the physical substrate of biological material in hybridsystems or other informationprocessing applications.chapter 9 presents a number of illustrative problem domains. these are technical challenges, potential future applications, and specific research questions that exemplify points along the interface ofcomputing and biology. they illustrate the two overarching themes described in chapter 2, and describe in detail the specific technological goals that must be met in order to successfully meet thechallenge.chapter 10 is a discussion of the research infrastructureñpeople and resources need to vitalize theinterface. the chapter examines the requisite scientific expertise, the false starts of the past, cultural andother barriers that must be addressed, and the coordinated effort needed to move research at theinterface forward.27national research council, mathematics and 21st century biology, the national academies press, washington, dc, 2005.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.introduction21finally, chapter 11 summarizes key findings about opportunities and barriers to progress at theinterface and provides recommendations for priority areas of research, tools, education, and resourcesthat will propel progress at the interface.appendix a is a reprint of a chapter from a 1995 nrc report entitled calculating the secrets of life.the chapter, òthe secrets of life: a mathematicianõs introduction to molecular biology,ó is essentiallya short primer on the fundamentals of molecular biology for nonbiologists. appendix b lists some of theresearch challenges in computational biology discussed in other reports. short biographies of committee members, staff, and the review coordinator are given in appendix c.throughout this report, examples of relevant work are provided quite liberally where they arerelevant to the topic at hand. the reader should note that these examples have generally been selectedto illustrate the breadth of the topic in question, rather than to identify the most important areas ofactivity. that is, the appropriate spirit in which to view these examples is òletting a thousand flowersbloom,ó rather than one of òfinding the prettiest flowers.ócatalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.21st century biology2323221st century biologybiology, like any science, changes when technology introduces new tools that extend the scope andtype of inquiry. some changes, such as the use of the microscope, are embraced quickly and easily,because they are consonant with existing values and practices. others, such as the introduction of multivariate statistics as performed by computers in the 1960s, are resisted, because they go against traditions ofintuition, visualization, and conceptions of biology that separate it clearly from mathematics.this chapter attempts to frame the challenges and opportunities created by the introduction ofcomputation to the biological sciences. it does so by first briefly describing the existing threads ofbiological culture and practice, and then by showing how different aspects of computational scienceand technology can support, extend, or challenge the existing framework of biology.computing is only one of a large number of fields playing a role in the transformation of biology,from advanced chemistry to new fields of mathematics. and yet, in many ways, computers have proventhe most challenging and the most transformative, rooted as they are in a tradition of design andabstraction so different from biology. just as computers continue to radically change society at large,however, there is no doubt that they will change biology as well. as it has done so many times before,biology will change with this new technology, adopting new techniques, redefining what makes goodscience and good training, and changing which inquiries are important, valued, or even possible.2.1what kind of science?2.1.1the roots of biological culturebiology is a science with a deep history that can be linked to the invention of agriculture at the verydawn of civilization and, even earlier, to the first glimmerings of oral culture: òis that safe to eat?ó assuch, it is a broad field, rich with culture and tradition, that encompasses many threads of observational, empirical, and theoretical research and spans scales from single molecules to continents. such abroad field is impossible to describe simply; nevertheless, this section attempts to identify a number ofthe main threads of the activity and philosophy of biology.first, biology is an empirical and a descriptive science. it is rooted in a tradition of qualitative observation and description dating back at least to aristotle. biological researchers have long sought tocatalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.24catalyzing inquirycatalog the characteristics, behaviors, and variations of individual biological organisms or populationsthrough the direct observation of organisms in their environments, rather than trying to identify generalprinciples through mathematical or abstract modeling. for this reason, the culture of biology is bothstrongly visual and specific. identifying a new species, and adequately describing its physical appearance, environment, and life cycle, remains a highly considered contribution to biological knowledge.it is revealing to contrast this philosophy with that of modern physics, where the menagerie of newsubatomic particles discovered in the 1960s and 1970s was a source of faint embarrassment and discomfort for physicists. only with the introduction of quarks, and the subsequent reduction in the number offundamental particles, did physicists again feel comfortable with the state of their field. biology, instrong contrast, not only prizes and embraces the enormous diversity of life, but also considers suchdiversity a prime focus of study.second, biology is an ontological science, concerned with taxonomy and classification. from the timeof linnaeus, biologists have attempted to place their observations into a larger framework of knowledge, relating individual species to the identified span of life. the methodology and basis for thiscatalog is itself a matter of study and controversy, and so research activity of this type occurs at twolevels: specific species are placed into the tree of life (or larger taxa are relocated), still a publishableevent, and the science of taxonomy itself is refined.biology is a historical science. life on earth apparently arose just once, and all life today is derivedfrom that single instance. a complete history of life on earthñwhich lineage arose from which, andwhenñis one of the great, albeit possibly unachievable, goals of biology. coupled to this inquiry, butseparate, are the questions, how? and why? what are the forces that cause species to evolve in certainways? are there secular trends in evolution, for example, as is often claimed, toward increasing complexity? does evolution proceed smoothly or in bursts? if we were to òreplay the tapeó of evolution,would similar forms arise? just as with taxonomy (and closely related to it), there are two levels here:what precisely happened and what the forces are that cause things to happen.these three strandsñempirical observations of a multitude of life forms, the historical facts ofevolution, and the ordering of biological knowledge into an overarching taxonomy of lifeñserved todefine the central practices of biology until the 1950s and still in many ways affect the attitudes, training,philosophy, and values of the biological sciences. although biology has expanded considerably withthe advent of molecular biology, these three strands continue as vital areas of biological research andinterest.these three intellectual strands have been reflected in biological research that has been qualitativeand descriptive throughout much of its early history. for example, empirical and ontological researchers have sought to catalog the characteristics, behaviors, and variations of individual biological organisms or populations through the direct observation of organisms in their environments.yet as important and valuable as these approaches have been for biology, they have not providedñand cannot provideñvery much detail about underlying mechanisms. however, in the last halfcentury, an intellectual perspective provided by molecular biology and biochemistry has served as thebasis for enormous leaps forward.2.1.2molecular biology and the biochemical basis of lifein the past 50 years, biochemical approaches to analyzing biological questions and the overallapproaches now known as molecular biology have led to the increased awareness, identification, andknowledge of the central role of certain mechanisms, such as the digital code of dna as the mechanismunderlying heredity, the use of adenosine triphospate (atp) for energy storage, common protein signaling protocols, and many conserved genetic sequences, some shared by species as distinct as humans,sponges, and even singlecell organisms such as yeast.this new knowledge both shaped and was shaped by changes in the practice of biology. twoimportant threads of biological inquiry, both existing long before the advent of molecular biology, camecatalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.21st century biology25to the forefront in the second half of the 20th century. these threads were biological experimentationand the search for the underlying mechanics of life.biological experimentation and the collection of data are not new, but they acquired a new importance and centrality in the late 20th century. the identification of genes and mutations exemplified byexperiments on drosophila became an icon of modern biological science, and with this a new focusemerged on collecting larger amounts of quantitative data.biologists have always been interested in how organisms live, a question that ultimately comesdown to the very definition of life. a great deal of knowledge regarding anatomy, circulation, respiration, and metabolism was gathered in the 18th and 19th centuries, but without access to the instrumentsand knowledge of biochemistry and molecular biology, there was a limit to what could be discovered.with molecular biology, some of the underlying mechanisms of life have been identified and analyzedquantitatively.the effort to uncover the basic chemical features of biological processes and to ascertain all aspectsof the components by way of experimental design will continue to be a major aspect of basic biologicalresearch, and much of modern biology has sought to reduce biological phenomena to the behavior ofmolecules.however, biological researchers are also increasingly interested in a systemslevel view in whichcompletely novel relationships among system components and processes can be ascertained. that is, adetailed understanding of the components of a biological organism or phenomenon inevitably leads tothe question of how these components interact with each other and with the environment in which theorganism or phenomenon is embedded.2.1.3biological components and processes in context, and biological complexitythere is a long tradition of studying certain biological systems in context. for example, ecology hasalways focused on ecosystems. physiology is another example of a life science that has generally considered biological systems as whole entities. animal behavior and systematics science also considersbiological phenomena in context. however, data acquisition technologies, computational tools, andeven new intellectual paradigms are available today that enable a significantly greater degree of incontext understanding of many more biological components and processes than was previously possible, and the goal today is to span the space of biological entities from genes and proteins to networksand pathways, from organelles to cells, and from individual organisms to populations and ecosystems.following kitano,1 a systems understanding of a biological entity is based on insights regardingfour dimensions: (1) system structures (e.g., networks of gene interactions and biochemical pathwaysand their relationship to the physical properties of intracellular and multicellular structures), (2) systemdynamics (e.g., how a system behaves over time under various conditions and the mechanisms underlying specific behaviors), (3) control mechanisms (e.g., mechanisms that systematically control the stateof the cell), and (4) design principles (e.g., principles underlying the construction and evolution ofbiological systems that have certain desirable properties).2as an example, consider advances in genomic sequencing. sequence genomics has created a pathfor establishing the òparts listó for living cells, but to move from isolated molecular details to a comprehensive understanding of phenomena from cell growth up to the level of homeostasis is widely recog1h. kitano, òsystems biology: a brief overview,ó science 295(5560):16621664, 2002.2for example, such principles might occur as the result of convergent evolution, that is, the evolution of species with differentorigins toward similar forms or characteristics, and an understanding of the likely ways that evolution can take to solve certainproblems. alternatively, principles might be identified that can explain the functional behavior of some specific biologicalsystem under a wide set of circumstances without necessarily being an accurate reflection of what is going on inside the system.such principles may prove useful from the standpoint of being able to manipulate the behavior of a larger system in which thesmaller system is embedded, though they may not be useful in providing a genuine understanding of the system with whichthey are associated.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.26catalyzing inquirynized as requiring a very different approach. in the highly interactive systems of living organisms, themacromolecular, cellular, and physiological processes, themselves at different levels of organizationalcomplexity, have both temporal and spatial components. interactions occur between sets of similarobjects, such as two genes, and between dissimilar objects, such as genes and their environment.a key aspect of biological complexity is the role of chance. one of the most salient instances ofchance in biology is evolution, in which chance events affect the fidelity of genetic transmission fromone generation to the next. the hand of chance is also seen in the development of an organismñchanceevents affect many of the details of development, though generally not the broad picture or trends. butperhaps the most striking manifestation is that individual biological organismsñeven as closely relatedas sibling cellsñare unlikely to be identical because of stochastic events from environmental input tothermal noise that affect molecularlevel processes. if so, no two cells will have identical macromolecular content, and the dynamic structure and function of the macromolecules in one cell will never be thesame as even a sibling cell. this fact is one of the largest distinctions between living systems and mostsilicon devices or almost any other manufactured or humanengineered artifact.put differently, the digital òcode of lifeó embedded in dna is far from simple. for example, thebiological òparts listó that the genomic sequence makes available in principle may be unavailable inpractice if all of the parts cannot be identified from the sequence. segments of the genome once assumedto be evolutionary òjunkó are increasingly recognized as the source of novel types of rna molecules thatare turning out to be major actors in cellular behavior. furthermore, even a complete parts list provides alot less insight into a biological system than into an engineered artifact, because human conventions forassembly are generally well understood, whereas natureõs conventions for assembly are not.a second example of the complexity is that a single gene can sometimes produce many proteins. ineukaryotes, for example, mrna cannot be used as a blueprint until special enzymes first cut out theintrons, or noncoding regions, and splice together the exons, the fragments that contain useful code.3 insome cases, however, the cell can splice the exons in different ways, producing a series of proteins withvarious pieces added or subtracted but with the same linear ordering (these are known as splice variants). a process known as rna editing can alter the sequence of nucleotides in the rna after transcription from dna but before translation into a protein, resulting in different proteins. an individualnucleotide can be changed into a different one (òsubstitution editingó), or nucleotides can be inserted ordeleted from the rna (òinsertiondeletion editingó). in some cases (however rare), the cellõs translationmachinery might introduce an even more radical change by shifting its òreading frame,ó meaning thatit starts to read the threebasepair genetic code at a point displaced by one or two base pairs from theoriginal. the result will be a very different sequence of amino acids and, thus, a very different protein.furthermore, even after the proteins are manufactured at the ribosome, they undergo quite a lot ofpostprocessing as they enter the various regulatory networks. some might have their shapes and activitylevels altered by the attachment, for example, of a phosphate group, a sugar molecule, or any of a varietyof other appendages, while others might come together to form a multiprotein structure. in short, knowing the complete sequence of base pairs in a genome is like knowing the complete sequence of 1s and 0sthat make up a computer program: by itself, that information does not necessarily yield insight into whatthe program does or how it may be organized into functional units such as subroutines.4a third illustration of biological complexity is that few, if any, biological functions can be assignedto a single gene or a single protein. indeed, the unique association between the hemoglobin moleculeand the function of oxygen transport in the bloodstream is by far the exception rather than the rule.3virtually all introns are discarded by the cell, but in a few cases, an intron has been found to codeñby itselfñfor anotherprotein.4a meaningful analogy can be drawn to the difference between object code and source code in a computer. object code,consisting of binary digits, is what runs on the computer. source code, usually written in a highlevel programming language, iscompiled into object code so that a program will run, but source codeñand therefore program structure and logicñis muchmore comprehensible to human beings. source code is also much more readily changed.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.21st century biology27much more common is the situation in which biological function depends on interactions among manybiological components. a cellõs metabolism, its response to chemical and biological signals from theoutside, its cycle of growth and cell divisionñall of these functions and more are generally carried outand controlled by elaborate webs of interacting molecules.fran“ois jacob and jacque monod won the 1965 nobel prize in medicine for the discovery that dnacontained regulatory regions that governed the expression of individual genes.5 (they further emphasized the importance of regulatory feedback and discussed these regulatory processes using the language of circuits, a point of relevance in section 5.4.3.3.) since then, it has become understood thatproteins and other products of the genome interact with the dna itself (and with each other) in aregulatory web.for example, rna molecules have a wide range of capabilities beyond their roles as messengersfrom dna to protein. some rna molecules can selectively silence or repress gene transcription; othersoperate as a combination chemoreceptorgene transcript (òriboswitchó) that gives rise to a protein atone end of the molecule when the opposite end comes in contact with the appropriate chemical target.indeed, it may even be that a significant increase in the number of regulatory rnas on an evolutionarytime scale is largely responsible for the increase in eukaryotic complexity without a large increase in thenumber of proteincoding genes. understanding the role of rna and other epigenetic phenomena thatresult in alternative states of gene expression, molecular function, or organizationñòsystems [that] arefar more complex than any problem that molecular biology, genetics or genomics has yet approached,ó6is critical to realizing genomicsõ promise.a fourth example of biological complexity is illustrated by the fact that levels of biological complexity extend beyond the intricacies of the genome and protein structures through supramolecular complexes and organelles to cellular subsystems and assemblies of these to form often functionally polarized cells that together contribute to tissue form and function and, thereby to an organismõs properties.although the revolution of the last half of the last century in biochemistry and molecular biology hascontributed significantly to our knowledge of the building blocks of life, we have only begun to scratchthe surface of a datadense and gordian knotlike puzzle of complex and dynamic molecular interactions that give rise to the complex behaviors of organisms. in short, little is known about how thecomplexities of physiological processes are governed by molecular, cellular, and transcellular signalingsystems and networks. available information is deep only in limited spatial or temporal domains, andscarce in other key domains, such the middle spatial scales (e.g., 10 †10 µm), and there are no tools thatmake intelligent links between relatable pieces of scientific knowledge across these scales.complexity, then, appears to be an essential aspect of biological phenomena. accordingly, thedevelopment of a coherent intellectual approach to biological complexity is required to understandsystemslevel interactionsñof molecules, genes, cells, organisms, populations, and even ecosystems. inthis intellectual universe, both ògenome syntaxó (the letters, words, and grammar associated with thedna code) and ògenome semanticsó (what the dna code can express and do) are central foci forinvestigation. box 2.1 describes some of the questions that will arise in cell biology.2.2toward a biology of the 21st centurya biology of the 21st century will integrate a number of diverse intellectual themes.7 one integration is that of the reductionist and systems approaches. where the componentcentered reductionist5f. jacob and j. monod, ògenetic regulatory mechanisms in the synthesis of proteins,ó journal of molecular biology 3:318356,1961.6f.s. collins et al., òa vision for the future of genomic research,ó nature 422:835847, 2003.7what this report calls 21st century biology has also been called òbringing the genome to life,ó an intentional biology, anintegrative biology, synthetic biology, the new biology or even the next new biology, biology 21, beyond the genome, postgenomicbiology, genomeenabled science, and industrialized biology.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.28catalyzing inquirybox 2.1some questions for cell biology in the 21st centuryin the human genome instituteõs recently published agenda for research in the postgenome era, franciscollins and his coauthors repeatedly emphasized how little biologists understand about the data already inhand. collins et al. argue that biologists are a very long way from knowing everything there is to know abouthow genes are structured and regulated, for example, and they are virtually without a clue as to whatõs goingon in the other 95 percent of the genome that does not code for genes. this is why the agendaõs very first grandchallenge was to systematically endow those data with meaningñthat is, to òcomprehensively identify thestructural and functional components encoded in the human genome.ó1the challenge, in a nutshell, is to understand the cellular information processing systemñall of itñfrom thegenome on up. weng et al. suggest that the essential defining feature of a cell, which makes the system as awhole extremely difficult to analyze, is the following:2[the cell] is not a machine (however complex) drawn to a welldefined design, but a machine that can and doesconstantly rebuild itself within a range of variable parameters. for a systematic approach, what is needed is arelatively clear definition of the boundary of this variability. in principle, these boundaries are determined by an asyetunknown combination of intrinsic capability and external inputs. the balance between intrinsic capability andthe response to external signals is likely to be a central issue in understanding gene expression. . . . a large body ofemerging data indicates that early development occurs through signaling interactions that are genetically programmed, whereas at the later stages, the development of complex traits is dependent on external inputs as well. aquantitative description of this entire process would be a culmination and synthesis of much of biology.some of the questions raised by this perspective include the following:¥what is the proteome of any given cell? how do these individual protein molecules organize themselvesinto functional subnetworksñand how do these subnetworks then organize themselves into higher andhigherlevel networks?3 what are the functional design principles of these systems? and how, precisely, dothe products of the genome react back on the genome to control their own creation?¥to what extent are active elements (such as rna) present in the noncoding portions of the genome? whatis the inventory of epigenetic mechanisms (e.g., rna silencing, dna methylation, histone hypoacetylation,chromatin modifications, imprinting) that cells use to control gene expression? these mechanisms play important roles in controlling an organismõs development and, in some lower organisms, are defense responsesagainst viruses and transposable elements. however, epigenetic phenomena have also been implicated inseveral human diseases, particularly cancer development due to the repression of tumor suppressor genes.what activates these mechanisms?¥how do these dynamically selforganizing networks vary over the course of the cell cycle (even thoughmost cells in an organism are not proliferating and have exited from the cell cycle)? how do they change asthe cell responds to its surroundings? how do they encode and process information? also, what accounts forlifeõs robustnessñthe ability of these networks to adapt, maintain themselves, and recover from a wide varietyof environmental insults?1f.s. collins, e.d. green, a.e. guttmacher, and m.s. guyer, òa vision for the future of genomic research,ó nature 422(6934):835847,2003. to help achieve this grand challenge, the institute has launched the encode project, a public research consortium dedicated tobuilding an annotated encyclopedia of all known functional dna elements. see http://www.genome.gov/10005107.2g. weng, u.s. bhalla, and r. iyengar, òcomplexity in biological signaling systems,ó science 284(5411):9296, 1999.3the hierarchy of levels obviously doesnõt stop at the cell membrane. although deciphering the various cellular regulatory networks is ahuge challenge in itself, systems biology ultimately has to deal as well with how cells organize themselves into tissues, organs, and the wholeorganism. one group that is trying to lay the groundwork for such an effort is the physiome project at the university of auckland in newzealand. see http://www.webopedia.com/term/w/webservices.html.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.21st century biology29approach is based on identifying the constituent parts of an organism and understanding the behaviorof the organism in terms of the behavior of those parts (in the limit, a complete molecularlevel characterization of the biological phenomena in question), systems biology aims to understand the mechanisms of a living organism across all relevant levels of hierarchy.8 these different fociña focus oncomponents of biological systems versus a focus on interactions among these componentsñare complementary, and both will be essential for intellectual progress in the future.twentyfirst century biology will bring together many distinct strands of biological research: taxonomic studies of many species, the enormous progress in molecular genetics, steps towards understanding the molecular mechanisms of life, and an emerging systems biology that will consider biological entities in relationship to their larger environment. twentyfirst century biology aims to understandfully the mechanisms of a living cell and the increasingly complex hierarchy of cells in metazoans, up to¥how do cells develop spatial structure? the cytoplasm is far from a uniform mixture of all of the biomolecules that exist in a cell; proteins and other macromolecules are often bound to membranes or isolated insidevarious cellular compartments (especially eukaryotes). a full account of the regulatory networks has to takethis compartmentalization into account, along with such spatial factors as diffusion and the transport of various species through the cytoplasm and across membranes.¥how do the networks organize and reorganize themselves over the course of embryonic development, aseach cell decides whether its progeny are going to become skin, muscle, brain, or whatever?4 then, once thecells are through differentiating, how do the networks actually vary from one cell type to the next? whatconstitutes the difference, and what happens to the networks as cells age or are damaged? how do flaws in thenetworks manifest themselves as maladies such as cancer?¥how do the networks vary between individuals? how do those variations account for differences in morphology and behavior? alsoñespecially in humansñhow do those variations account for individual differences in the response to drugs and other therapies?¥how do multicellular organisms operate? a full account of multicellular organisms will have to include anaccount of signaling (in all its varieties, including cellcell; cellsubstratum; autocrine, paracrine, and exocrinesignaling), cellular differentiation, cell motility, tissue architecture, and many other òcommunityó issues.¥how do the networks vary between species? to put it another way, how have they changed over thecourse of evolution? since the òblueprintó genes for proteins and rna seem to be quite highly conserved fromone species to the next, is it possible that most of evolution is the result of rearrangements in the geneticregulatory system?54physiological processes such as metabolism, signal transduction, and the cell cycle take place on a time scale that ranges frommilliseconds to days and are reversible in the sense that an activity flickers on, gene expression is adjusted as needed, and then everythingreturns to some kind of equilibrium. but the commitments that the cell makes during development are effectively irreversible. becoming aparticular cell line means that the genetic regulatory networks in each successive generation of cells have to go through a cascade ofdecisions that end up turning genes on and off by the thousands. unless there is some drastic intervention, as in the cloning experiments thatcreated dolly the sheep, those genes are locked in place for the life span of the organism. of course, the developmental program does notproceed in an isolated, òopenloopó fashion, as a computer scientist might say. very early in the process, for example, the growing embryolays out its basic body planñfront versus back, top versus bottom, and so onñby establishing embryowide chemical gradients, so that theconcentration of the appropriate compound tells each cell what to do. similar tricks are used at every stage thereafter: each cell is alwaysreceiving copious feedback from its neighbors, with chemical signals providing a constant stream of instructions and course corrections.5after all, even very small changes in the timing of events during development, and in the rates at which various tissues grow, can havea profound impact on the final outcome.8as a philosophical matter, the notion of reductionist explanation has had a long history in the philosophy of science. life iscomposed of matter, and matter is governed by the laws of physics. so, the ultimate in reductionist explanation would suggestthat life can be explained by the properties of schrıdingerõs equation.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.30catalyzing inquiryprocesses operating at the level of the organism and even populations and ecosystems. however, thiskind of understanding is fundamentally dependent on synergies between a systems understanding asdescribed above and the reductionist tradition.twentyfirst century biology also brings together empirical work in biology with computationalwork. empirical work is undertaken in laboratory experiments or field observations and has led to bothhypothesis testing and hypothesis generation. hypothesis testing relies on the data provided by empirical work to accept or reject a candidate hypothesis. however, data collected in empirical work can alsosuggest new hypotheses, leading to work that is exploratory in nature. in 21st century biology, computational work provides a variety of tools that support empirical work, but also enables much of systemsbiology through techniques such as simulation, data mining, and microarray analysisñand thus underlies the generation of plausible candidate hypotheses that will have to be tested. note also that hypothesis testing is relevant to both reductionist and systems biology, in the sense that both types of biologyare formulated around hypotheses (about components or about relationships between components)that mayñor may notñbe consistent with empirical or experimental results.in this regard, a view expressed by walter gilbert in 1991 seems prescient. gilbert noted that òin thecurrent paradigm [i.e., that of 1991], the attack on the problems of biology is viewed as being solelyexperimental. the ôcorrectõ approach is to identify a gene by some direct experimental procedureñdetermined by some property of its product or otherwise related to its phenotypeñto clone it, tosequence it, to make its product and to continue to work experimentally so as to seek an understandingof its function.ó he then argued that òthe new paradigm [for biological research], now emerging [i.e., in1991], is that all the genes will be known (in the sense of being resident in databases available electronically), and that the starting point of a biological investigation will be theoretical. an individual scientistwill begin with a theoretical conjecture, only then turning to experiment to follow or test that hypothesis. the actual biology will continue to be done as ôsmall scienceõñdepending on individual insightand inspiration to produce new knowledge but the reagents that the scientist uses will include aknowledge of the primary sequence of the organism, together with a list of all previous deductions fromthat sequence.ó9finally, 21st century biology encompasses what is often called discovery science. discovery sciencehas been described as òenumerat[ing] the elements of a system irrespective of any hypotheses on howthe system functionsó and is exemplified by genome sequencing projects for various organisms.10 asecond example of discovery science is the effort to determine the transcriptomes and proteomes ofindividual cell types (e.g., quantitative measurements of all of the mrnas and protein species).11 suchefforts could be characterized as providing the building blocks or raw materials out of which hypotheses can be formulatedñmetaphorically, words of a biological òlanguageó for expressing hypotheses.yet even here, the human genome project, while unprecedented in its scope, is comfortably part of along tradition of increasingly fine description and cataloging of biological data.all told, 21st century biology will entail a broad spectrum of research, from laboratory work directed by individual principal investigators, to projects on the scale of the human genome that generatelarge amounts of primary data, to the òmesoscienceó in between that involves analytical or syntheticwork conducted by multiple collaborating laboratories. for the most part, these newer research strategies involving discovery science and analytical work will complement rather than replace the traditional, relatively small laboratory focusing on complementary empirical and experimental methods.9w. gilbert, òtowards a paradigm shift in biology,ó nature 349(6305):99, 1991.10r. aebersold, l.e. hood, and j.d. watts, òequipping scientists for the new biology,ó nature biotechnology 18:359, 2000.11these examples are taken from t. ideker, t. galitski, and l. hood, òa new approach to decoding life: systems biology,óannual review of genomics and human genetics 2:343372, 2001. the transcriptome is the complete collection of transcribedelements of the genome, including all of the genetic elements that code for proteins, all of the mrnas, and all noncoding rnasthat are used for structural and regulatory purposes. the proteome is the complete collection of all proteins involved in aparticular pathway, organelle, cell, tissue, organ, or organism that can be studied in concert to provide accurate and comprehensive data about that system.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.21st century biology31grand questions, such as those concerning origins of life, the story of evolution, the architecture ofthe brain, and the interactions of living things with each other in populations and ecosystems, are up forgrabs in 21st century biology, and the applications to health, agriculture, and industry are no lessambitious. for example, 21st century biology may enable the identification of individuals who are likelyto develop cancer, alzheimerõs, or other diseases, or who will respond to or have a side effect from aparticular disease treatment. pharmaceutical companies are making major investments intranscriptomics to screen for plausible drug targets. forwardthinking companies want to develop morenutritious plants and animals, commandeer the machinery of cells to produce materials and drugs, andbuild interfaces to the brain to correct impaired capabilities or produce enhanced abilities. agenciesinterested in fighting bioterrorism want to be able to rapidly identify the origins and ancestry of pathogen outbreaks, and stewards of natural systems would like to make better predictions about the impactsof introduced species or global change.2.3roles for computing and information technology in biologyto manage biological data, 21st century biology will integrate discovery science, systems biology,and the empirical tradition of biological science and provide a quantitative framework within which theresults of efforts in each of these areas may be placed. the availability of large amounts of biologicaldata is expected to enable biological questions to be addressed globally, for example, examining thebehavior of all of the genes in a genome, all of the proteins produced in a cell type, or all of themetabolites created under particular environmental conditions. however, enabling the answering ofbiological questions by uncovering the raw data is not the same as answering those questionsñthe datamust be analyzed and used in intellectually meaningful and significant ways.2.3.1biology as an information sciencethe dataintensive nature of 21st century biology underlies the dependence of biology on information technology (it). for example, even in 1990 it was recognized that it would play a central role in theinternational human genome consortium for the storage and retrieval of biological gene sequencedatañrecording the signals, storing the sequence data, processing images of fluorescent traces specificto each base, and so on. also, as biology unfolds in the 21st century, it is clear that the rate of productionof biological data will not abate. data acquisition opportunities will emerge in most or all life sciencesubdisciplines and fields, and life scientists will have to cope with the coming deluge of highly multivariate, largely nonreducible data, including highresolution imaging and time series data of complexdynamic processes.yet beyond data management issues, important and challenging though they are, it has also becomeclear that computing and information technology will play crucial roles in identifying meaningfulstructures and patterns in the genome (e.g., genes, genetic regulatory elements), in understanding theinterconnections between various genomic elements, and in uncovering functional biological information about genes, proteins, and their interactions. this focus on informationñon acquiring, processing,structuring, and representing informationñplaces genomic studies squarely in the domain of computing and information science.of course, genomic studies are not the whole of modern biology. for life sciences ranging fromecology, botany, zoology, and developmental biology to cellular and molecular biologyñall of whichcan be characterized as science with diverse data types and high degrees of data heterogeneity andhierarchyñit is essential to collect key information and organize biological data in methodical ways inorder to draw meaningful observations. massive computing power, novel modeling approaches, newalgorithms and mathematical or statistical techniques, and systematic engineering approaches willprovide biologists with vital and essential tools for managing the heterogeneity and volume of the dataand for extracting meaning from those data.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.32catalyzing inquiryultimately, what calculus is to the language of the physical sciences, computing and informationwill be to the language of 21st century biology, or at least to its systems biology thread.12 the processesof biology, the activities of living organisms, involve the usage, maintenance, dissemination, transformation or transduction, replication, and transmittal of information across generations. biological systems are characterized by individuality, contingency, historicity, and high digital information contentñevery living thing is unique. furthermore, the uniqueness and historical contingency of life means thatfor populationscale problems, the potential state space that the population actually inhabits is huge.13as an information science, the life sciences use computing and information technology as a languageand a medium in which to manage the discrete, asymmetric, largely irreducible, unique nature ofbiological systems and observations.in the words above, those even marginally familiar with the history of biology will recognize hintsof what was once called theoretical biology or mathematical biology, which in earlier days meantmodels and computer simulations based on such thenfashionable ideas as cybernetics and generalsystems theory.14 the initial burst of enthusiasm waned fairly quickly, as it became clear that theavailable experimental data were not sufficient to keep the mathematical abstractions tethered to reality. indeed, reliable models are impossible when many or most of the quantitative values are missing.moreover, experience since then has indicated that biological systems are much more complex andinternally interlinked than had been imaginedña fact that goes a long way towards explaining why themodels of that era were not very successful in driving productive hypothesis generation and research.the story is radically different today. highthroughput data acquisition technologies (themselvesenabled and made practical by todayõs information technologies), change a paucity of data into a delugeof it, as illustrated by the use of these technologies for sequencing of many eukaryotic organisms. thisis not to say that more data are not needed, merely that the acquisition of necessary data now seems tobe possible in reasonable amounts of time.the same is true for the information technologies underpinning 21st century biology. in the past,even if data had been available, the it then available would have been inadequate to make sense out ofthose data. but todayõs information technologies are vastly more powerful and hold considerable promise for enabling the kinds of data management and analytical capabilities that are necessary for asystemslevel approach. moreover, information technology as an underlying medium has the advantage of growing ever more capable over time at exponential rates. as information technology becomesmore capable, biological applications will have an increasingly powerful technology substrate on whichto draw.12biological sciences advisory committee on cyberinfrastructure for the biological sciences, building a cyberinfrastructure forthe biological sciences (cibio): 2005 and beyond: a roadmap for consolidation and exponentiation, july 2003. available from http://research.calit2.net/cibio/archived/cibiofinal.pdf. this is not to deny that calculus also has application in systems biology(mostly through its relevance to biochemistry and thermodynamics), but calculus is not nearly as central to systems biology as itis to the physical sciences nor as central as computing and information technology are to systems biology.13the number of possible different 3billionbasepair genomes, assuming only simple base substitution mutations, is 4 to the3billionth power. thatõs a big number. in fact, it is so big that the ratio of that number (big) to the number of particles in theknown universe (small) is much greater than the ratio of the diameter of the universe to the diameter of a carbon atom. thus,exhaustive computer modeling of that state space is effectively precluded. even more tractable state spaces, such as the numberof different possible human haploid genotypes, still produce gigantic numbers. for example, if we assume that the entire humanpopulation is heterozygous at just 500 locations throughout the genome (a profound underestimate of existing diversity), witheach site having only two states, then the number of possible human haplotypes is 2 to the 500th power, which also exceeds thenumber of electrons in the known universe. these backoftheenvelope calculations also show that it is impossible for the statespace of existing human genotypes to exist in anything approaching linkage equilibrium.14n. wiener, cybernetics, or control and communication in the animal and the machine, 2nd ed., mit press, cambridge, ma, 1961;l. von bertalanffy, general systems theory: foundations, development, applications, george braziller, new york, 1968. this historywas recently summarized in o. wolkenhauer, òsystems biology: the reincarnation of systems theory applied in biology?óbriefings in bioinformatics 2(3):258270, 2001.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.21st century biology33in short, the introduction of computing into biology has transformed, and continues to transform,the practice of biology. the most straightforward, although often intellectually challenging, way involves computing tools with which to acquire, store, process, and interpret enormous amounts ofbiological data. but computing (when used wisely and in combination with the tools of mathematicsand physics) will also provide biologists with an alternative and possibly more appropriate languageand set of abstractions for creating models and data representations of higherorder interactions, describing biological phenomena, and conceptualizing some characteristics of biological systems.finally, it should be noted that although computing and information technology will become anincreasingly important part of life science research, researchers in different subfields of biology arelikely to understand the role of computing differently. for example, researchers in molecular biology orbiophysics may focus on the ability of computing to make more accurate quantitative predictions aboutenzyme behavior, while researchers in ecology may be more interested in the use of computing toexplore relationships between ecosystem behavior and perturbations in the ambient environment. theseperspectives will become especially apparent in the chapters of this report dealing with the impact ofcomputing and it on biology (see chapter 4 on tools and chapter 5 on models).this report distinguishes between computational tools, computational models, information abstractions and a computational perspective on biology, and cyberinfrastructure and data acquisition technologies. each of these is discussed in chapters 4 through 7, respectively, preceded by a short chapteron the nature of biological data (chapter 3).2.3.2computational toolsin the lexicon of this report, computational tools are artifactsñusually implemented as software,but sometimes as hardwareñthat enable biologists to solve very specific and precisely defined problems. for example, an algorithm for gene finding or a database of genomic sequences is a computationaltool. as a rule, these tools reinforce and strengthen biological research activities, such as recording,managing, analyzing, and presenting highly heterogeneous biological data in enormous quantity. chapter 4 focuses on computational tools.2.3.3computational modelscomputational models apply to specific biological phenomena (e.g., organisms, processes) and areused for several purposes. they are used to test insight; to provide a structural framework into whichobservations and experimental data can be coherently inserted; to make hypotheses more rigorous,quantifiable, and testable; to help identify key or missing elements or important relationships; to helpinterpret experimental data; to teach or present system behavior; and to predict dynamical behavior ofcomplex systems. predictive models provide some confidence that certain aspects of a given biologicalsystem or phenomenon are understood, when their predictions are validated empirically. chapter 5focuses on computational models and simulations.2.3.4a computational perspective on biologycoming to grips with the complexity of biological phenomena demands an array of intellectualtools to help manage complexity and facilitate understanding in the face of such complexity. in recentyears, it has become increasingly clear that many biological phenomena can be understood as performing information processing in varying degrees; thus, a computational perspective that focuses on information abstractions and functional behavior has potentially large benefit for this endeavor. chapter 6focuses on viewing biological phenomena through a computational lens.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.34catalyzing inquiry2.3.5cyberinfrastructure and data acquisitioncyberinfrastructure for science and engineering is a term coined by the national science foundation to refer to distributed computer, information, and communication technologies and the associatedorganizational facilities to support modern scientific and engineering research conducted on a globalscale. cyberinfrastructure for the life sciences is increasingly an enabling mechanism for a largescale,dataintensive biological research effort, inherently distributed over multiple laboratories and investigators around the world, that facilitates the integration of experimental data, enables collaboration, andpromotes communication among the various actors involved.obtaining primary biological data is a separate question. as noted earlier, 21st century biology isincreasingly a dataintensive enterprise. as such, tools that facilitate acquisition of the requisite datatypes in the requisite amounts will become ever more important in the future. although they are not byany means the whole story, advances in it and computing will play key roles in the development ofnew data acquisition technologies that can be used in novel ways.chapter 7 focuses on the roles of cyberinfrastructure and data acquisition for 21st century biology.2.4challenges to biological epistemologythe forthcoming integration of computing into biological research raises deep epistemologicalquestions about the nature of biology itself. for many thousands of years, a doctrine known as vitalismheld that the stuff of life was qualitatively different from that of nonlife and, consequently, that livingorganisms were made of a separate substance than nonliving things or that some separate life forceexisted to animate the materials that composed life.while this belief no longer holds sway today (except perhaps in bad science fiction movies), thequestion of how biological phenomena can be understood has not been fully settled. one stance is basedon the notion that the behavior of a given system is explained wholly by the behaviors of the components that make up that systemña view known as reductionism in the philosophy of science. a contrasting stance, known as autonomy in the philosophy of science, holds that in addition to understanding its individual components, understanding of a biological system must also include an understandingof the specific architecture and arrangement of the systemõs components and the interactions amongthem.if autonomy is accepted as a guiding worldview, introducing the warp of computing into the weftof biology creates additional possibilities for intellectual inquiry. just as the invention of the microscopeextended biological inquiry into new arenas and enlarged the scope of questions that were reasonable toask in the conduct of biological research, so will the computer. computing and information technologywill enable biological researchers to consider heretofore inaccessible questions, and as the capabilities ofthe underlying information technologies increase, such opportunities will continue to open up.new epistemological questions will also arise. for example, as simulation becomes more pervasiveand common in biology, one may ask, are the results from a simulation equivalent to the data output ofan experiment? can biological knowledge ever arise from a computer simulation? (a practical exampleis the following: as largescale clinical trials of drugs become more and more expensive, under whatcircumstances and to what extent might a simulation based on detailed genomic and pharmacologicalknowledge substitute for a largescale trial in the drug approval process?) as simulations become moreand more sophisticated, preloaded with more and more biological data, these questions will becomeboth more pressing and more difficult to answer definitively.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.on the nature of biological data35353on the nature of biological datatwentyfirst century biology will be a dataintensive enterprise. laboratory data will continue tounderpin biologyõs tradition of being empirical and descriptive. in addition, they will provide confirmingor disconfirming evidence for the various theories and models of biological phenomena that researchersbuild. also, because 21st century biology will be a collective effort, it is critical that data be widelyshareable and interoperable among diverse laboratories and computer systems. this chapter describes thenature of biological data and the requirements that scientists place on data so that they are useful.3.1data heterogeneityan immense challengeñone of the most central facing 21st century biologyñis that of managingthe variety and complexity of data types, the hierarchy of biology, and the inevitable need to acquiredata by a wide variety of modalities. biological data come in many types. for instance, biological datamay consist of the following:1¥sequences.sequence data, such as those associated with the dna of various species, have grownenormously with the development of automated sequencing technology. in addition to the humangenome, a variety of other genomes have been collected, covering organisms including bacteria, yeast,chicken, fruit flies, and mice.2 other projects seek to characterize the genomes of all of the organismsliving in a given ecosystem even without knowing all of them beforehand.3 sequence data generally1this discussion of data types draws heavily on h.v. jagadish and f. olken, eds., data management for the biosciences, report ofthe nsf/nlm workshop of data management for molecular and cell biology, february 23, 2003, available at http://www.eecs.umich.edu/~jag/wdmbio/wdmbrpt.pdf. a summary of this report is published as h.v. jagadish and f. olken,òdatabase management for life science research,ó omics: a journal of integrative biology 7(1):131137, 2003.2see http://www.genome.gov/11006946.3see, for example, j.c. venter, k. remington, j.f. heidleberg, a.l. halpern, d. rusch, j.a. eisen, d. wu, et al., òenvironmentalgenome shotgun sequencing of the sargasso sea,ó science 304(5667):6674, 2004. venterõs team collected microbial populationsen masse from seawater samples originating in the sargasso sea near bermuda. the team subsequently identified 1.045 billionbase pairs of nonredundant sequence, which they estimated to derive from at least 1,800 genomic species based on sequencerelatedness, including 148 previously unknown bacterial phylotypes. they also claimed to have identified more than 1.2 millionpreviously unknown genes represented in these samples.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.36catalyzing inquiryconsist of text strings indicating appropriate bases, but when there are gaps in sequence data, gaplengths (or bounds on gap lengths) must be specified as well.¥graphs.biological data indicating relationships can be captured as graphs, as in the cases ofpathway data (e.g., metabolic pathways, signaling pathways, gene regulatory networks), genetic maps,and structured taxonomies. even laboratory processes can be represented as workflow process modelgraphs and can be used to support formal representation for use in laboratory information managementsystems.¥highdimensional data.because systems biology is highly dependent on comparing the behaviorof various biological units, data points that might be associated with the behavior of an individual unitmust be collected for thousands or tens of thousands of comparable units. for example, gene expressionexperiments can compare expression profiles of tens of thousands of genes, and since researchers areinterested in how expression profiles vary as a function of different experimental conditions (perhapshundreds or thousands of such conditions), what was one data point associated with the expression ofone gene under one set of conditions now becomes 106 to 107 data points to be analyzed.¥geometric information.because a great deal of biological function depends on relative shape (e.g.,the òdockingó behavior of molecules at a potential binding site depends on the threedimensionalconfiguration of the molecule and the site), molecular structure data are very important. graphs are oneway of representing threedimensional structure (e.g., of proteins), but ballandstick models of proteinbackbones provide a more intuitive representation.¥scalar and vector fields.scalar and vector field data are relevant to natural phenomena that varycontinuously in space and time. in biology, scalar and vector field properties are associated with chemical concentration and electric charge across the volume of a cell, current fluxes across the surface of acell or through its volume, and chemical fluxes across cell membranes, as well as data regarding charge,hydrophobicity, and other chemical properties that can be specified over the surface or within thevolume of a molecule or a complex.¥patterns.within the genome are patterns that characterize biologically interesting entities. forexample, the genome contains patterns associated with genes (i.e., sequences of particular genes) andwith regulatory sequences (that determine the extent of a particular geneõs expression). proteins arecharacterized by particular genomic sequences. patterns of sequence data can be represented as regularexpressions, hidden markov models (hmms), stochastic contextfree grammars (for rna sequences),or other types of grammars. patterns are also interesting in the exploration of protein structure data,microarray data, pathway data, proteomics data, and metabolomics data.¥constraints.consistency within a database is critical if the data are to be trustworthy, and biological databases are no exception. for example, individual chemical reactions in a biological pathwaymust locally satisfy the conservation of mass for each element involved. reaction cycles in thermodynamic databases must satisfy global energy conservation constraints. other examples of nonlocal constraints include the prohibition of cycles in overlap graphs of dna sequence reads for linear chromosomes or in the directed graphs of conceptual or biological taxonomies.¥images.imagery, both natural and artificial, is an important part of biological research. electronand optical microscopes are used to probe cellular and organ function. radiographic images are used tohighlight internal structure within organisms. fluorescence is used to identify the expressions of genes.cartoons are often used to simplify and represent complex phenomena. animations and movies areused to depict the operation of biological mechanisms over time and to provide insight and intuitiveunderstanding that far exceeds what is available from textual descriptions or formal mathematicalrepresentations.¥spatial information.real biological entities, from cells to ecosystems, are not spatially homogeneous, and a great deal of interesting science can be found in understanding how one spatial region isdifferent from another. thus, spatial relationships must be captured in machinereadable form, andother biologically significant data must be overlaid on top of these relationships.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.on the nature of biological data37¥models.as discussed in section 5.3.4, computational models must be compared and evaluated.as the number of computational models grows, machinereadable data types that describe computational modelsñboth the form and the parameters of the modelñare necessary to facilitate comparisonamong models.¥prose.the biological literature itself can be regarded as data to be exploited to find relationshipsthat would otherwise go undiscovered. biological prose is the basis for annotations, which can beregarded as a form of metadata. annotations are critical for researchers seeking to assign meaning tobiological data. this issue is discussed further in chapter 4 (automated literature searching).¥declarative knowledge such as hypotheses and evidence.as the complexity of various biologicalsystems is unraveled, machinereadable representations of analytic and theoretical results as well as theunderlying inferential chains that lead to various hypotheses will be necessary if relationships are to beuncovered in this enormous body of knowledge. this point is discussed further in section 4.2.8.1.in many instances, data on some biological entity are associated with many of these types: forexample, a protein might have associated with it twodimensional images, threedimensional structures, onedimensional sequences, annotations of these data structures, and so on.overlaid on these types of data is a temporal dimension. temporal aspects of data types such asfields, geometric information, highdimensional data, and even graphsñimportant for understandingdynamical behaviorñmultiply the data that must be managed by a factor equal to the number of timesteps of interest (which may number in the thousands or tens of thousands). examples of phenomenawith a temporal dimension include cellular response to environmental changes, pathway regulation,dynamics of gene expression levels, protein structure dynamics, developmental biology, and evolution.as noted by jagadish and olken,4 temporal data can be taken absolutely (i.e., measured on an absolutetime scale, as might be the case in understanding ecosystem response to climate change) or relatively(i.e., relative to some significant event such as division, organism birth, or environmental insult). notealso that in complex settings such as disease progression, there may be many important events againstwhich time is reckoned. many traditional problems in signal processing involve the extraction of signalfrom temporal noise as well, and these problems are often found in investigating biological phenomena.all of these different types of data are needed to integrate diverse witnesses of cellular behavior intoa predictive model of cellular and organism function. each data source, from highthroughputmicroarray studies to mass spectroscopy, has characteristic sources of noise and limited visibility intocellular function. by combining multiple witnesses, researchers can bring biological mechanisms intofocus, creating models with more coverage that are far more reliable than models created from onesource of data alone. thus, data of diverse types including mrna expression, observations of in vivoproteindna binding, proteinprotein interactions, abundance and subcellular localization of smallmolecules that regulate protein function (e.g., second messengers), posttranslational modifications, andso on will be required under a wide variety of conditions and in varying genetic backgrounds. inaddition, dna sequence from diverse species will be essential to identify conserved portions of thegenome that carry meaning.3.2data in high volumedata of all of the types described above contribute to an integrated understanding of multiple levelsof a biological organism. furthermore, since it is generally not known in advance how various components of an organism are connected or how they function, comprehensive datasets from each of these4h.v. jagadish and f. olken, òdatabase management for life science research,ó omics: a journal of integrative biology 7(1):131137, 2003.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.38catalyzing inquirytypes are required. in cellular analysis, data comprehensiveness includes three aspects, as noted bykitano: 51.factor comprehensiveness, which reflects the numbers of mrna transcripts and proteins that canbe measured at once;2.timeline comprehensiveness, which represents the time frame within which measurements aremade (i.e., the importance of highlevel temporal resolution); and3.item comprehensivenessñthe simultaneous measurement of multiple items, such as mrna andprotein concentrations, phosphorylation, localization, and so forth.for every one of the many proteins in a given cell type, information must be collected about proteinidentity, abundance, processing, chemical modifications, interactions, turnover time, and so forth. spatial localization of proteins is particularly critical. to understand cellular function in detail, proteinsmust be localized on a scale finer than that of cell compartments; moreover, localization of specificprotein assemblies to discrete subcellular sites through anchoring and scaffolding proteins is important.all of these considerations suggest that in addition to being highly heterogeneous, biological datamust be voluminous if they are to support comprehensive investigation.3.3data accuracy and consistencyall laboratories must deal with instrumentdependent or protocoldependent data inconsistencies.for example, measurements must be calibrated against known standards, but calibration methods andprocedures may change over time, and data obtained under circumstances of heterogeneous calibrationmay well not be comparable to each other. experiments done by multiple independent parties almostalways result in inconsistencies in datasets.6 different experimental runs with different technicians andprotocols in different labs inevitably produce data that are not entirely consistent with each other, andsuch inconsistencies have to be noted and reconciled. also, the absolute number of data errors that mustbe reconciledñboth within a single dataset and across datasetsñincreases with the size of the dataset.for such reasons, statistical data analysis becomes particularly important in analyzing data acquired viahighthroughput techniques.to illustrate these difficulties, consider the replication of microarray experiments. experience withmicroarrays suggests that such replication can be quite difficult. in principle, a microarray experimentis simple. the raw output of a microarray experiment is a listing of fluorescent intensities associatedwith spots in an array; apart from complicating factors, the brightness of these spots is an indication ofthe expression level of the transcript associated with them.on the other hand, the complicating factors are many, and in some cases ignoring these factors canrender oneõs interpretation of microarray data completely irrelevant. consider the impact of the following:¥background effects, which are by definition contributions to spot intensity that do not originatewith the biological material being examined. for example, an empty microarray might result in some5h. kitano, òsystems biology: a brief overview,ó science 295(5560):16621664, 2002.6as an example, there is only limited agreement between the datasets generated by multiple methods regarding proteinprotein interactions in yeast. see, for example, the following set of papers: y. ho, a. gruhler, a. heilbut, g.d. bader, l. moore,s.l. adams, a. miller, et al., òsystematic identification of protein complexes in saccharomyces cerevisiae by mass spectrometry,ónature 415(6868):180183, 2002; a.c. gavin, m. bosche, r. krause, p. grandi, m. marzioch, a. bauer, j. schultz, et al., òfunctionalorganization of the yeast proteome by systematic analysis of protein complexes,ó nature 415(6868):141147, 2002; t. ito, t.chiba, r. ozawa, m. yoshida, m. hattori, and y. sakaki, òa comprehensive two hybrid analysis to explore the yeast proteininteractome,ó proceedings of the national academy of sciences 98(8):45694574, 2001; p. uetz, l. giot, g. cagney, t.a. mansfield,r.s. judson, j.r. knight, d. lockshon, et al., òa comprehensive analysis of proteinprotein interactions in saccharomycescerevisiae,ó nature 403(6770):623627, 2000.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.on the nature of biological data39background level of fluorescence and even some variation in background level across the entire surfaceof the array.¥noise dependent on expression levels of the sample. for example, tu et al. found that hybridizationnoise is strongly dependent on expression level, and in particular the hybridization noise is mostlypoissonlike for high expression levels but more complex at low expression levels.7¥differential binding strengths for different probetarget combinations. the brightness of a spot is determined by the amount of target present at a probe site and the strength of the binding between probe andtarget. held et al. found that the strength of binding is affected by the free energy of hybridization,which is itself a function of the specific sequence involved at the site, and they developed a model toaccount for this finding.8¥lack of correlation between mrna levels and protein levels. the most mature microarray technologymeasures mrna levels, while the quantity of interest is often protein level. however, in some cases ofinterest, the correlation is small even if overall correlations are moderate. one reason for small correlations is likely to be the fact that some proteins are regulated after translation, as noted in ideker et al.9¥lack of uniformity in the underlying glass surface of a microarray slide. lee et al. found that the specificlocation of a given probe on the surface affected the expression level recorded.10other difficulties arise when the results of different microarray experiments must be compared.11¥variations in sample preparation. a lack of standardized procedure across experiments is likely toresult in different levels of random noiseñand procedures are rarely standardized very well when theyare performed by humans in different laboratories. indeed, sample preparation effects may dominateeffects that arise from the biological phenomenon under investigation.12¥insufficient spatial resolution. because multiple cells are sampled in any microarray experiment,tissue inhomogeneities may result in more of a certain kind of cell being present, thus throwing off thefinal result.¥cellcycle starting times. identical cells are likely to have moreorless identical clocks, but there isno assurance that all of the clocks of all of the cells in a sample are started at the same time. becauseexpression profile varies over time, asynchrony in cell cycles may also throw off the final result.13to deal with these difficulties, the advice offered by lee et al. and novak et al., among others, isfairly straightforwardñrepeat the experiment (assuming that the experiment is appropriately struc7y. tu, g. stolovitzky, and u. klein, òquantitative noise analysis for gene expression microarray experiments,ó proceedingsof the national academy of sciences 99(22):1403114036, 2002.8g.a. held, g. grinstein, and y. tu, òmodeling of dna microarray data by using physical properties of hybridization,óproceedings of the national academy of sciences 100(13):75757580, 2003.9t. ideker, v. thornsson, j.a. ranish, r. christmas, j. buhler, j.k. eng, r. bumgarner, et al., òintegrated genomic and proteomicanalyses of a systematically perturbed metabolic network,ó science 292(5518):929934, 2001. (cited in rice and stolovitzky,òmaking the most of it,ó 2004, footnote 11.)10m.l. lee, f.c. kuo, g.a. whitmore, and j. sklar, òimportance of replication in microarray gene expression studies: statistical methods and evidence from repetitive cdna hybridizations,ó proceedings of the national academy of sciences 97(18):98349839, 2000.11j.j. rice and g. stolovitzky, òmaking the most of it: pathway reconstruction and integrative simulation using the data athand,ó biosilico 2(2):7077, 2004.12j.p. novak, r. sladek, and t.j. hudson, òcharacterization of variability in largescale gene expression data: implicationsfor study design,ó genomics 79(1):104113, 2002.13r.j. cho, m.j. campbell, e.a. winzeler, l. steinmetz, a. conway, l. wodicka, t.g. wolfsberg, et al., òa genomewidetranscriptional analysis of the mitotic cell cycle,ó molecular cell 2(1):6573, 1998; p.t. spellman, g. sherlock, m.q. zhang, v.r.iyer, k. anders, m.b. eisen, p.o. brown, et al., òcomprehensive identification of cell cycleregulated genes of the yeast saccharomyces cerevisiae by microarray hybridization,ó molecular biology of the cell 9(12):32733297, 1998. (cited in rice and stolovitzky,òmaking the most of it,ó 2004, footnote 11.)catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.40catalyzing inquirytured and formulated in the first place). however, the expense of microarrays may be an inhibitingfactor in this regard.3.4data organizationthe acquiring of experimental data by some researcher is only the first step in making them usefulto the wider biological research community. data are useless if they are inaccessible or incomprehensible to others, and given the heterogeneity and large volumes of biological data, appropriate dataorganization is central to extracting useful information from the data. indeed, it would not be anexaggeration to identify data management and organization issues as a key ratelimiting step in doingscience for the small to mediumsized laboratory, where òscienceó covers the entire intellectual waterfront from laboratory experiment to data that are useful to the community at large. this is especiallytrue in laboratories using highthroughput data acquisition technologies.in recent years, biologists have taken significant steps in coming to terms with the need to thinkcollectively about databases as research tools accessible to the entire community. in the field of molecular biology, the first widely recognized databases were the international archival repositories for dnaand genomic sequence information, including genbank, the european molecular biology laboratory(embl) nucleotide sequence database, and the dna databank of japan (ddj). subsequent databaseshave provided users with information that annotated the genomic sequence data, connecting regions ofa genome with genes, identifying proteins associated with those genes, and assigning function to thegenes and proteins. there are databases of scientific literature, such as pubmed; databases on singleorganisms, such as flybase (the drosophila research database); and databases of protein interactions,such as the general repository for interaction datasets (grid). in their research, investigators typicallyaccess multiple databases (from the several hundred webaccessible biological databases). table 3.1provides examples of key database resources in bioinformatics.data organization in biology faces significant challenges for the foreseeable future, given the levelsof data being produced. each year, workshops associated with major conferences in computationalbiology are held to focus on how to apply new techniques from computer science into computationalbiology. these include the intelligent systems for molecular biology (ismb) conference and the conference on research in computational biology (recomb), which have championed the cause of creatingtools for database development and integration.14 the longterm vision for biology is for a decentralized collection of independent and specialized databases that operate as one large, distributed information resource with common controlled vocabularies, related user interfaces, and practices. much research will be needed to achieve this vision, but in the short term, researchers will have to make do withmore specialized tools for the integration of diverse data types as described in section 4.2.what is the technological foundation for managing and organizing data? in 1998, jeff ullman notedthat òthe common characteristic of [traditional business databases] is that they have large amounts ofdata, but the operations to be performed on the data are simple,ó and also that under such circumstances, òthe modification of the database scheme is very infrequent, compared to the rate at whichqueries and other data manipulations are performed.ó15the situation in biology is the reverse. modern information technologies can handle the volumes ofdata that characterize 21st century biology, but they are generally inadequate to provide a seamlessintegration of biological data across multiple databases, and commercial database technology has provento have many limitations in biological applications.16 for example, although relational databases haveoften been used for biological data management, they are clumsy and awkward to use in many ways.14t. headgordon and j. wooley, òcomputational challenges in structural and functional genomics,ó ibm systems journal40(2):265296, 2001.15j.d. ullman, principles of database and knowledgebase systems, vols. i and ii, computer science press, rockville, md, 1988.16h.v. jagadish and f. olken, òdatabase management for life science research,ó omics: a journal of integrative biology7(1):131137, 2003.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.on the nature of biological data41table 3.1examples of key database resources in bioinformaticscategorydatabases and urlscomprehensive datancbi (national center for biotechnology and information):center: broad contenthttp://www.ncbi.nlm.nih.gov/including sequence,structure, function, etc.ebi (european bioinformatics institute): http://www.ebi.ac.uk/european molecular biology laboratory (embl):http://www.emblheidelberg.de/tigr (the institute of genome research): http://www.tigr.org/whitehead/massachusetts institute of technology genome center:http://wwwgenome.wi.mit.edu/dna or protein sequencegenbank: http://www.ncbi.nlm.nih.gov/genbankddbj (dna data bank of japan): http://www.ddbj.nig.acjp/embl nucleotide sequence databank:http://www.ebi.ac.uk/embl/index.htmlpir (protein information resource): http://pir.georgetown.edu/swissprot: http://www.expasy.ch/sprot/sprottop.htmlbiomolecular interactionsbind (biomolecular interaction network database):http://www.blueprint.org/bind/bind.phpthe contents of bind include highthroughput data submissions andhandcurated information gathered from the scientific literature.genomes: completeentrez complete genomes:genome sequences andhttp://www.ncbi.nlm.nih.gov/entrez/genome/org.htmlrelated information forspecific organismscomplete genome at ebi: http://www.ebi.ac.uk/genomes/university of california, santa cruz, human genome working draft:http://genome.ucsc.edu/mgd (mouse genome database): http://www.informaticsjax.org/sgd (saccharomyces genome database):http://genomewww.stanford.edu/saccharomyces/flybase (a database of the drosophila genome):http://flybase.bio.indiana.edu/wormbase (the genome and biology of caenorhabditis elegans):http://www.wormbase.org/genetics: gene mapping,gdb (genome database): http://gdbwww.gdb.org/gdb/mutations, and diseasesomim (online mendelian inheritance in man):http://www3.ncbi.nlm.nih.gov/omim/searchomim.htmlhgmd (human gene mutation database):http://archive.uwcm.ac.uk/uwcm/mg/hgmdo.htmlcontinuedcatalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.42catalyzing inquirytable 3.1continuedcategorydatabases and urlsgene expression:unigene: http://www.ncbi.nlm.nih.gov/unigene/microarray and cdnagene expressiondbest (expression sequence tag database):http://www.ncbi.nlm.nih.gov/dbest/index.htmlbodymap: http://bodymap.ims.utokyo.ac.jp/geo (gene expression omnibus): http://www.ncbi.nlm.nih.gov/geo/structure: threepdb (protein data bank): http://www.rcsb.org/pdb/index.htmldimensional structures ofsmall molecules, proteins,ndb (nucleic acid database):nucleic acids (both rnahttp://ndbserver.irutgers.edu/ndb/ndb.htmland dna) foldingpredictionscsd (cambridge structural database):http://www.ccdc. cam. ac.uk/prods/csd/csd.htmlclassification of proteinscop (structure classification of proteins):family and proteinhttp://scop.mrcimb.cam.ac.uk/scop/domainscath (protein structure classification database):http://www.biochem.ucl.ac.uk/bsm/cathnew/index.htmlpfam: http://pfam.wustl.edu/prosite database for protein family and domains:http://www.expasy.ch/prosite/block: http://www.blocks.fhcrc.org/protein pathwaykegg (kyoto encyclopedia of genes and genomes):proteinproteinhttp://www.genome.ad.jp/kegg/kegg2.html#pathwayinteractions andmetabolic pathwaybind (biomolecular interaction network database):http://www.binddb.org/dip (database of interacting proteins): http: hdip.doembi.ucla.edu/ecocyc (encyclopedia of escherichia coli genes and metabolism):http://ecocyc.org/ecocyc/ecocyc.htmlwit (metabolic pathway): http://hwit.mcs.anl.gov/wit2/proteomics: proteins,afcs (alliance for cellular signaling): http://cellularsignaling.org/protein familyjcsg (joint center for structure genomics):http://www.jcsg.org/scripts/prod/home.htmlpkr (protein kinase resource): http://pkr.sdsc.edu/html/index.shtmlcatalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.on the nature of biological data43the size of biological objects is often not constant. more importantly, relational databases presume theexistence of welldefined and known relationships between data records, whereas the reality of biological research is that relationships are imprecisely knownñand this imprecision cannot be reduced toprobabilistic measures of relationship that relational databases can handle.jagadish and olken argue that without specialized life sciences enhancements, commercial relational database technology is cumbersome for constructing and managing biological databases, andmost approximate sequence matching, graph queries on biopathways, and threedimensional shapesimilarity queries have been performed outside of relational data management systems. moreover, therelational data model is an inadequate abstraction for representing many kinds of biological data (e.g.,pedigrees, taxonomies, maps, metabolic networks, food chains). box 3.1 provides an illustration of howbusiness database technology can be inadequate.objectoriented databases have some advantages over relational databases since the natural foci ofstudy are in fact biological objects. yet jagadish and olken note that objectoriented databases have alsohad limited success in providing efficient or extensible declarative query languages as required forspecialized biological applications.because commercial database technology is of limited help, research and development of databasetechnology that serves biological needs will be necessary. jagadish and olken provide a view of requirements that will necessitate further advances in data management technology, requirements that includepharmacogenomics,pharmgkb (pharmacogenetics knowledge base):pharmaco genetics, singlehttp://pharmgkb.orgnucleotide polymorphism(snp), genotypingsnp consortium: http://snp.cshl.orgdbsnp (single nucleotide polymorphism database):http://www.ncbi.nlm.nih.gov/snp/locuslink: http://www.ncbi.nlm.nih.gov/locuslinkafred (allele frequency database):http://alfred.med.yale. edu/alfred/index.aspceph genotype database: http://www.cephb.fr/cephdb/tissues, organs, andvisible human project database:organismshttp://www.nlm.nih.gov/research/visible/visiblehuman.htmlbraid (brain image database): http://hbraid.rad.jhu.edu/interface.htmlneurodb (neuroscience federated database):http://www.npaci.edu/dice/neuro/the whole brain atlas:http://www.med.harvard.edu/aanlib/home.htmlliterature referencepubmed medline:http://www.ncbi.nlm.nih.gov/entrez/query.fcgiuspto (u.s. patent and trademark office): http://www.uspto.gov/table 3.1continuedcategorydatabases and urlscatalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.44catalyzing inquirya great diversity of data types: sequences, graphs, threedimensional structures, images; unconventional types of queries: similarity queries, (e.g., sequence similarity), patternmatching queries, patternfinding queries; ubiquitous uncertainty (and sometimes even inconsistency) in the data; data curation(data cleaning and annotation); largescale data integration (hundreds of databases); detailed dataprovenance; extensive terminology management; rapid schema evolution; temporal data; and management for a variety of mathematical and statistical models of organisms and biological systems.data organization and management present major intellectual challenges in integration and presentation, as discussed in chapter 4.3.5data sharingthere is a reasonably broad consensus among scientists in all fields that reproducibility of findingsis central to the scientific enterprise. one key component of reproducibility is thus the availability ofdata for community examination and inspection. in the words of the national research council (nrc)committee on responsibilities of authorship in the biological sciences, òan authorõs obligation is notbox 3.1probabilistic onetomany database entry linkingone purpose of database technology is the creation and maintenance of links between items in differentdatabases. thus, consider the problem in which a primary biological database of genes contains an object(call it a) that subsequent investigation and research reveal to be two objects. for example, what was thoughtto be a single gene might upon further study turn out to be two closely linked genes (a1 and a2) with anoncoding region in between (a3). another database (e.g., a database of clones known to hybridize to variousgenes) may have contained a link to añcall the clone in question c. research reveals that it is impossible forc to hybridize to both a1 and a2 individually, but that it does hybridize to the set taken collectively (i.e., a1,a2, and a3).how should this relationship now be represented? before the new discovery, the link was simple: c to a. nowthat new knowledge requires that the primary database (or at least the entry for a) be restructured, how shouldthis new knowledge be reflected in the original simple link? that is, what should one do with links connectedto the previously single object, now that that single object has been divided into two?the new information in the primary database has three components, a1, a2, and a3. to which of these, ifany, should the original link be attached? if the link is discarded entirely, the database loses the fact that chybridizes to the collection. if the link from c is now attached to all three equally, that link represents information contrary to fact, since experiment shows that c does not hybridize to both a1 and a2. the necessaryrelationship that must be reflected calls for the clone entry c to link to a1, a2, and a3 simultaneously but alsoprobabilistically. that is, what must be represented is that the probability of the match in the set of three is oneand that the probability of match for two or one in the set is zero.as a general rule, such relationships (i.e., onetomany relationships that are probabilistic) are not supportedby business database technology. however, they are required in scientific databases once this kind of splittingoperation has occurred on a hypothetical biological objectñand such splitting is commonplace in scientificliterature. as indicated, it can occur in the splitting of a gene, or in other cases, it can occur in the splitting ofa species on the basis of additional findings on the biology of what was believed to be one species.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.on the nature of biological data45only to release data and materials to enable others to verify or replicate published findings but also toprovide them in a form on which other scientists can build with further research.ó17however, in practice, this ethos is not uniformly honored. an old joke in the life science researchcommunity comments on data mining in biologyñòthe data are mine, mine, mine.ó for a field whoseroots are in empirical description, it is not hard to see the origins of such an attitude. for most of itshistory, the life sciences research community has granted primary intellectual credit to those who havecollected data, a stance that has reinforced the sentiment that those that collect the data are its rightfulowners. while some fields such as evolutionary biology generally have an ethos of data sharing, thedatasharing ethos is honored with much less uniformity in many other fields of biology. requests fordata associated with publications are sometimes (even often) denied, ignored, or fulfilled only afterlong delay or with restrictions that limit how the data may be used.18the reasons for this state of affairs are multiple. the upside report called attention to the growingrole of the forprofit sector (e.g., the pharmaceutical, biotechnology, researchtool, and bioinformaticscompanies) in basic and applied research over the last two decades, and the resulting circumstance thatincreasing amounts of data are developed by and held in private hands. these forprofit entitiesñwhose primary responsibilities are to their investorsñhope that their data will provide competitiveadvantages that can be exploited in the marketplace.nor are universities and other nonprofit research institutions immune to commercial pressures. anincreasing amount of life sciences research in the nonprofit sector is supported directly by funds fromthe forprofit sector, thus increasing the prospect of potentially conflicting missions that can impedeunrestricted data sharing as nonprofit researchers are caught up in commercial concerns. universitiesthemselves are encouraged as a matter of public law (the bayhdole act of 1980) to promote the use,commercialization, and public availability of inventions developed through federally funded researchby allowing them to own the rights to patents they obtain on these inventions. university researchersalso must confront the publishorperish issue. in particular, given the academic premiums on beingfirst to publish, researchers are strongly motivated to take steps that will preserve their own ability topublish followup papers or the ability of graduate students, postdoctoral fellows, or junior facultymembers to do the same.another contributing factor is that the nature of the data in question has changed enormously sincethe rise of the human genome project. in particular, the enormous volumes of data collected are acontinuing resource that can be productively òminedó for a long time and yield many papers. thus,scientists who have collected such data can understandably view relinquishing control of them as a stiffpenalty in light of the time, cost, and effort needed to do the research supporting the first publication.19although some communities (notably the genomics, structural biology, and clinical trials communities)have established policies and practices to facilitate data sharing, other communities (e.g., those workingin brain imaging or gene and protein expression studies) have not yet done so.17national research council, sharing publicationrelated data and materials: responsibilities of authorship in the life sciences,national academies press, washington, dc, 2003. hereafter referred to as the upside report. much of the discussion in section3.5 is based on material found in that report.18for example, a 2002 survey of geneticists and other life scientists at 100 u.s. universities found that of geneticists who hadasked other academic faculty for additional information, data, or materials regarding published research, 47 percent reportedthat at least one of their requests had been denied in the preceding 3 years. twelve percent of geneticists themselves acknowledged denying a request from another academic researcher. see e.g. campbell, b.r. clarridge, m. gokhale, l. birenbaum, s.hilgartner, n.a. holtzen, and d. blumenthal, òdata withholding in academic genetics: evidence from a national survey,ójournal of the american medical association 287(4):473480, 2002. (cited in the upside report; see footnote 17.)19data provenance (the concurrent identification of the source of data along with the data itself as discussed in section 3.7) hasan impact on the social motivation to share data. if data sources are always associated with data, any work based on that datawill automatically have a link to the original source; hence proper acknowledgment of intellectual credit will always be possible.without automated data provenance, it is all too easy for subsequent researchers to lose the connection to the original source.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.46catalyzing inquiryfinally, raw biological data are not the only commodities in question. computational tools andmodels are increasingly the subject of publication in the life sciences (see chapters 4 and 5), and it isinevitable that similar pressures will arise (indeed, have arisen) with respect to sharing the software andalgorithms that underlie these artifacts. when software is at issue, a common concern is that the releaseof softwareñespecially if it is released in source codeñcan enable another party to commercialize thatcode. some have also argued that mandatory sharing of source code prevents universities from exercising their legal right to develop commercial products from federally funded research.considering these matters, the nrc committee on responsibilities of authorship in the biologicalsciences concluded:the act of publishing is a quid pro quo in which authors receive credit and acknowledgment in exchange for disclosure of their scientific findings. all members of the scientific communityñwhetherworking in academia, government, or a commercial enterpriseñhave equal responsibility for upholding community standards as participants in the publication system, and all should be equally able toderive benefits from it.the upside report also explicated three principles associated with sharing publicationrelated dataand software:20¥authors should include in their publications the data, algorithms, or other information that is centralor integral to the publicationñthat is, whatever is necessary to support the major claims of the paper andwould enable one skilled in the art to verify or replicate the claims.¥if central or integral information cannot be included in the publication for practical reasons (for example, because a dataset is too large), it should be made freely (without restriction on its use for researchpurposes and at no cost) and readily accessible through other means (for example, on line). moreover,when necessary to enable further research, integral information should be made available in a form thatenables it to be manipulated, analyzed, and combined with other scientific data. . . . [however, m]akingdata that is central or integral to a paper freely obtainable does not obligate an author to curate andupdate it. while the published data should remain freely accessible, an author might make available animproved, curated version of the database that is supported by user fees. alternatively, a valueaddeddatabase could be licensed commercially.¥if publicly accessible repositories for data have been agreed on by a community of researchers and arein general use, the relevant data should be deposited in one of these repositories by the time of publication. . . . [t]hese repositories help define consistent policies of data format and content, as well as accessibility to the scientific community. the pooling of data into a common format is not only for the purposeof consistency and accessibility. it also allows investigators to manipulate and compare datasets, synthesize new datasets, and gain novel insights that advance science.when a publication explicitly involves software or algorithms to solve biological problems, theupside report pointed out that the principle enunciated for data should also apply: software or algorithms that are central or integral to a publication òshould be made available in a manner that enables itsuse for replication, verification, and furtherance of science.ó the report also noted that one option is toprovide in the publication a detailed description of the algorithm and its parameters. a second option isto make the relevant source code available to investigators who wish to test it, and either optionupholds the spirit of the researcherõs obligation.since the upside report was released in 2003, editors at two major life science journals, science andnature, have agreed in principle with the idea that publication entails a responsibility to make datafreely available to the larger research community.21 nevertheless, it remains to be seen how widely theupside principles will be adopted in practice.20the upside report contained five principles, but only three were judged relevant to the question of data sharing per se. theprinciples described in the text are quoted directly from the upside report.21e. marshall, òthe upside of good behavior: make your data freely available,ó science 299(5609):990, 2003.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.on the nature of biological data47as for the technology to facilitate the sharing of data and models, the state of the art today is thateven when the will to share is present, data or model exchange between researchers is generally anontrivial exercise. data and models from one laboratory or researcher must be accompanied by enoughmetadata that other researchers can query the data and use the model in meaningful ways without a lotof unproductive overhead in òfutzing around doing stupid things.ó technical dimensions of this pointare discussed further in section 4.2.3.6data integration as noted in chapter 2, data are the sine qua non of biological science. the ability to share datawidely increases the utility of those data to the research community and enables a higher degree ofcommunication between researchers, laboratories, and even different subfields. data incompatibilitiescan make data hard to integrate and to relate to information on other variables relevant to the samebiological system. further, when inquiries can be made across large numbers of databases, there is anincreased likelihood that meaningful answers can be found. largescale data integration also has thesalutary virtue that it can uncover inconsistencies and errors in data that are collected in disparate ways.in digital form, all biological data are represented as bits, which are the underlying electronicrepresentation of data. however, for these data to be useful, they must be interpretable according tosome definitions. when there is a single point of responsibility for data management, the definitions arerelatively easy to generate. when responsibility is distributed over multiple parties, they must agree onthose definitions if the data of one party are to be electronically useful to another party. in other words,merely providing data in digital form does not necessarily mean that they can be shared readilyñthesemantics of differing data sets must be compatible as well.another complicating factor is the fact that nearly all databasesñregardless of scaleñhave theirorigins in smallscale experimentation. researchers almost always obtain relatively small amounts ofdata in their first attempts at experimentation. small amounts of data can usually be managed in flatfilesñtypically, spreadsheets. flat files have the major advantage that they are quick and easy toimplement and serve smallscale data management needs quite well.however, flat files are generally impractical for large amounts of data. for example, queries involving multiple search criteria are hard to make when a flatfile database is involved. relationships between entries are concealed in a flatfile format. also, flat files are quite poor for handling heterogeneous data types.there are a number of technologies and approaches, described below, that address such issues. inpractice, however, the researcher is faced with the problem of knowing when to abandon the smallscale flat file in favor of a more capable and technically sophisticated arrangement that will inevitablyentail higher overhead, at least initially.the problem of largescale data integration is extraordinarily complex and difficult to solve. in2003, lincoln stein noted that òlife would be much simpler if there was a single biological database, butthis would be a poor solution. the diverse databases reflect the expertise and interests of the groups thatmaintain them. a single database would reflect a series of compromises that would ultimately impoverish the information resources that are available to the scientific community. a better solution wouldmaintain the scientific and political independence of the databases, but allow the information that theycontain to be easily integrated to enable crossdatabase queries. unfortunately, this is not trivial.ó22consider, for example, what might be regarded as a straightforward problemñthat of keepingstraight vocabularies and terminologies and their associated concepts. in reality, when new biologicalstructures, entities, and events have been uncovered in a particular biological context, they are often22reprinted by permission from l.d. stein, òintegrating biological databases,ó nature reviews genetics 4(5):337345, 2003.copyright 2005 macmillan magazines ltd.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.48catalyzing inquirydescribed with novel terminology or measurements that do not reveal much about how they might berelated to similar entities in other contexts or how they quantitatively function in the contexts in whichthey exist, for example:¥biological concepts may clash as users move from one database to another. stein discussesseveral examples:231.to some research communities, òa pseudogene is a genelike structure that contains inframestop codons or evidence of reverse transcription. to others, the definition of a pseudogene isexpanded to include gene structures that contain full open reading frames (orfs) but are nottranscribed. some members of the neisseria gonorrhea research community, meanwhile, usepseudogene to mean a transposable cassette that is rearranged in the course of antigenic variation.ó2.òthe human genetics community uses the term allele to refer to any genomic variant, including silent nucleotide polymorphisms that lie outside of genes, whereas members of many modelorganism communities prefer to reserve the term allele to refer to variants that change genes.ó3.òeven the concept of the gene itself can mean radically different things to different researchcommunities. some researchers treat the gene as the transcriptional unit itself, whereas othersextend this definition to include up and downstream regulatory elements, and still others usethe classical definitions of cistron and genetic complementation.ó¥evolving scientific understandings may drive changes in terminology. for example, diabeteswas once divided into the categories of juvenile and adult onset. as the role of insulin became clearer,the relevant categories evolved into òinsulin dependentó and ònoninsulin dependent.ó the relationship is that almost all juvenile cases of diabetes are insulin dependent, but a significant fraction of adultonset cases are as well.¥names of the same biological object may change across databases. òfor example, consider thednadamage checkpointpathway gene that is named rad24 in saccharomyces cerevisiae (budding yeast).[schizo]saccharomyces pombe (fission yeast) also has a gene named rad24 that is involved in the checkpoint pathway, but it is not the orthologue of the s. cerevisiae rad24. instead, the correct s. pombeorthologue is rad17, which is not to be confused with the similarly named rad17 gene in s. cerevisiae.meanwhile, the human checkpointpathway genes are sometimes named after the s. cerevisiaeorthologues, sometimes after the s. pombe orthologues, and sometimes have independently derivednames. in c. elegans, there are a series of rad genes, none of which is orthologous to s. cerevisiae rad17.the closest c. elegans match to rad17 is, in fact, a dnarepair gene named mrt2.ó24¥implicit meanings can be counterintuitive. for example, the international classification of disease (icd) code for òanginaó means òangina occurring in the past.ó25 a condition of current angina isindicated by the code for òchest pain not otherwise specified.ó¥data transformations from one database to another may destroy useful information. for example, a clinical order in a hospital may call for a òpa [posterioranterior] and lateral chest xray.ówhen that order is reflected in billing, it may be collapsed into òchest xray: 2 views.ó¥metadata may change when databases originally created for different purposes are conceptuallyjoined. for example, medline was developed to facilitate access to the printed paper literature by23reprinted by permission from l.d. stein, òintegrating biological databases,ó nature reviews genetics 4(5):337345, 2003.copyright 2005 macmillan magazines ltd.24reprinted by permission from l.d. stein, òintegrating biological databases,ó nature reviews genetics 4(5):337345, 2003.copyright 2005 macmillan magazines ltd.25icd codes refer to a standard international classification of diseases. for more information, see http://www.cdc.gov/nchs/about/otheract/icd9/abticd9.htm.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.on the nature of biological data49scientists. the data were assembled in medline to help users find citations. as a result, authors inmedline were originally treated as text strings, not as people. there was no effort, to identify individual people, so òsmith, jó could be john smith, jim smith, or joan smith. however, the name of anindividual is not necessarily constant over his or her professional lifetime. thus, one cannot usemedline to search for all papers authored by an individual who has undergone a name changewithout independent knowledge of the specifics of that change.experience suggests that left to their own devices, designers of individual databases generally makelocally optimal decisions about data definitions and formats for entirely rational reasons, and local decisions are almost certain to be incompatible in some ways with other such decisions made in other laboratories by other researchers.26 nearly 10 years ago, robbins noted that òa crisis occurred in the [biological]databases in the mid 1980s, when the data flow began to outstrip the ability of the database to keep up. aconceptual change in the relationship of databases to the scientific community, coupled with technicaladvances, solved the problem. . . . now we face a dataintegration crisis of the 1990s. even if the variousseparate databases each keep up with the flow of data, there will still be a tremendous backlog in theintegration of information in them. the implication is similar to that of the 1980s: either a solution willsoon emerge or biological databases collectively will experience a massive failure.ó27 box 3.2 describessome of the ways in which communitywide use of biological databases continues to be difficult today.two examples of research areas requiring a large degree of data integration are cellular modeling andpharmacogenomics. in cellular modeling (discussed further in section 5.4.2), researchers need to integratethe plethora of data available today about cellular function; such information includes the chemical,electrical, and regulatory features of cells; their internal pathways; mechanisms of cell motility; cell shapechanges; and cell division. box 3.3 provides an example of a celloriented database. in pharmacogenomics(the study of how an individualõs genetic makeup affects his or her specific reaction to drugs, discussed insection 9.7), databases must integrate data on clinical phenotypes (including both pharmacokinetic andpharmacodynamic data) and profiles (e.g., pulmonary, cardiac, and psychological function tests, andcancer chemotherapeutic side effects); dna sequence data, gene structure, and polymorphisms in sequence (and information to track haploid, diploid, or polyploid alleles, alternative splice sites, and polymorphisms observed as common variants); molecular and cellular phenotype data (e.g., enzyme kineticmeasurements); pharmacodynamic assays; cellular drug processing rates; and homology modeling ofthreedimensional structures. box 3.4 illustrates the pharmacogenetics research network and knowledgebase (pharmgkb), an important database for pharmacogenetics and pharmacogenomics.3.7data curation and provenance28biological research is a fastpaced, quickly evolving discipline, and data sources evolve with it: newexperimental techniques produce more and different types of data, requiring database structures tochange accordingly; applications and queries written to access the original version of the schema must26in particular, a scientist working on the cutting edge of a problem almost certainly requires data representations and modelswith more subtlety and more degrees of resolution in the data relevant to the problem than someone who has only a passinginterest in that field. almost every dataset collected has a lot of subtlety in some areas of the data model and less subtletyelsewhere. merging these datasets into a commondenominator model risks throwing away the subtlety, where much of thevalue resides. yet, merging these datasets into a uniformly datarich model results in a database so rich that it is not particularlyuseful for general use. an exampleñbiomedical databases for human beings may well include coding for gender as a variable.however, in a laboratory or medical facility that does a lot of work on transgendered individuals who may have undergone sexchange operations, the notion of gender is not necessarily as simple as òmaleó or òfemale.ó27r.j. robbins, òcomparative genomics: a new integrative biology,ó in integrative approaches to molecular biology, j. colladovides, b. magasanik, and t.f. smith, eds., mit press, cambridge, ma, 1996.28section 3.7 embeds excerpts from s.y. chung and j.c. wooley, òchallenges faced in the integration of biological information,ó bioinformatics: managing scientific data, z. lacroix and t. critchlow, eds., morgan kaufmann, san francisco, ca, 2003.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.50catalyzing inquirybox 3.2characteristics of biological databasesbiological databases have several characteristics that make them particularly difficult to use by the communityat large. biological databases are¥autonomous. as a point of historical fact, most biological databases have been developed and maintainedby individual research groups or research institutions. initially, these databases were developed for individualuse by these groups or institutions, and even when they proved to have value to the larger community, datamanagement practices peculiar to those groups remained. as a result, biological databases almost alwayshave their own governing body and infrastructure.¥inconsistent in format (syntax). in addition to the heterogeneity of data types discussed in section 3.1,databases that contain the same types of data still may be (and often are) syntactically heterogeneous. forexample, the scientific literature, images, and other freetext documents are commonly stored in unstructuredor semistructured formats (plain text files, html or xml files, binary files). genomic, microarray gene expression, and proteomic data are routinely stored in conventional spreadsheet programs or in structured relationaldatabases (oracle, sybase, db2, informix, etc.). major data depository centers have also adopted differentstandards for data formats. for example, the u.s. national center for biotechnology information (ncbi) hasadopted the highly nested data asn.1 (abstract syntax notation) for the general storage of gene, protein, andgenomic information, while the u.s. department of agricultureõs plant genome data and information centerhas adopted the objectoriented acedb data management systems and interface.¥inconsistent in meaning (semantics). biological databases containing the same types of data are also oftensemantically inconsistent. for example, in the database of biological literature known as medline, multiplealiases for genes are the norm, rather than the exception. there are cases in which the same name refers todifferent genes that have no relationship to each other. a gene that codes for an enzyme might be namedaccording to its mutant phenotype by a geneticist and its enzymatic function by a biochemist. a vector to amolecular biologist refers to a vehicle, as in a cloning vector, whereas vector to a parasitologist is an organismthat is an agent in the transmission of disease. research groups working with different organisms will oftengive the same molecule a different name. finally, biological knowledge is often represented only implicitly, inthe shared assumptions of the community that produced the data source, and not explicitly via metadata thatcan be used either by human users or by integration software.¥dynamic and subject to continual change. as biological research progresses and better understandingemerges, it is common that new data are obtained that contradict old data. often, new data organizationalschemes become necessary, even new data types or entirely new databases may become necessary.¥diverse in the query tools they support. the queries supported by a database are what give the database itsutility for a scientist, for only through the making of a query can the appropriate data be returned. yet databases vary widely in the kinds of query they supportñor indeed that they can support. user interfaces to queryengines may require specific input and output formats. for example, blast (the basic local alignment searchtool), the most frequently used program in the molecular biology community, requires a specific format(fasta) for input sequence and outputs a list of pairwise sequence alignments to the end users. output fromone database query often is not suitable as direct input for a query on a different database. finally, applicationsemantics vary widely. leaving aside the enormous variety of different applications for different biologicalproblems (e.g., applications for nucleic and protein sequence analysis, genome comparison, protein structureprediction, biochemical pathway and genetic network analysis, construction of phylogenetic trees, modelingand simulation of biological systems and processes), even applications nominally designed for the sameproblem domain can make different assumptions about the underlying data and the meaning of answers toqueries. at times, they require nontrivial domain knowledge from different fields. for example, protein foldingcan be approached using ab initio prediction based on first principles (physics) or using knowledgebased(computer science) threading methods.¥diverse in the ways they allow users to access data. some databases provide large text dumps of theircontents, others offer access to the underlying database management system and still others provide only webpages as their primary mode of access.source: derived largely from s.y. chung and j.c. wooley, òchallenges faced in the integration of biological information,óbioinformatics: managing scientific data, z. lacroix and t. critchlow, eds., morgan kaufmann, san francisco, ca, 2003.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.on the nature of biological data51be rewritten to match the new version. incremental updates to data warehouses (as opposed to wholesale rebuilding of the warehouse from scratch) are difficult to accomplish efficiently, particularly whencomplex transformations or aggregations are involved.a most important point is that most broadly useful databases contain both raw data and data thatare either the result of analysis or derived from other databases. in this environment, databases becomeinterdependent. errors due to data acquisition and handling in one database can be propagated quicklyinto other databases. data updated in one database may not be propagated immediately to relateddatabases.thus, data curation is essential. curation is the process through which the community of users canhave confidence in the data on which they rely. so that these data can have enduring value, informationrelated to curation must itself be stored within the database; such information is generally categorizedas annotation data. data provenance and data accuracy are central concerns, because the distinctionsbetween primary data generated experimentally, data generated through the application of scientificbox 3.3the alliance for cellular signalingthe alliance for cellular signaling (afcs), partly supported by the national institute of general medicalsciences and partly by large pharmaceutical companies, seeks to build a publicly accessible, comprehensivedatabase on cellular signaling that makes available virtually all significant information about molecules ofinterest. this database will also be one enabler for pathway analysis and facilitate an understanding of howmolecules coordinate with one another during cellular responses. the database seeks to identify all of theproteins that constitute the various signaling systems, assess timedependent information flow through thesystems in both normal and pathological states, and reduce the mass of detailed data into a set of interactingtheoretical models that describe cellular signaling. to the maximum extent possible, the information contained in the database is intended to be machinereadable.the complete database is intended to enable researchers to:¥query the database about complex relationships between molecules;¥view phenotypealtering mutations or functional domains in the context of protein structure;¥view or create de novo signaling pathways assembled from knowledge of interactions between moleculesand the flow of information among the components of complex pathways;¥evaluate or establish quantitative relationships among the components of complex pathways;¥view curated information about specific molecules of interest (e.g., names, synonyms, sequence information, biophysical properties, domain and motif information, protein family details, structure and gene data, theidentities of orthologues and paralogues, blast results) through a òmolecule home pageó devoted to eachmolecule of interest, and¥read comprehensive, peerreviewed, expertauthored summaries, which will include highly structuredinformation on protein states, interactions, subcellular localization, and function, together with references tothe relevant literature.the afcs is motivated by a desire to understand as completely as possible the relationships between sets ofinputs and outputs in signaling cells that vary both temporally and spatially. yet because there are many researchers engaged in signaling research, the cultural challenge faced by the alliance is the fact that informationin the database is collected by multiple researchers in different laboratories and from different organizations.today, it involves more than 50 investigators from 20 academic and industrial institutions. however, as of thiswriting, it is reported that the nigms will reduce funding sharply for the alliance following a midproject reviewin early 2005 (see z. merali and j. giles, òdatabases in peril,ó nature 435:10101011, 23 june 2005).catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.52catalyzing inquirybox 3.4the pharmacogenetics research network and knowledge basesupported by the national institute of general medical sciences (nigms) of the national institutes of health,the pharmacogenetics research network and knowledge base (pharmgkb) is intended as a national resourcecontaining highquality structured data linking genomic information, molecular and cellular phenotype information, and clinical phenotype information. the ultimate aim of this project is to produce a knowledge basethat provides a public infrastructure for understanding how variations in the human genome lead to variationsin clinical response to medications.sample inquiries to this database might include the following:1. for gene x, show all observed polymorphisms in its sequence;2. for drug y, show the variability in pharmacokinetics; and3. for phenotype z, show the variability in association with drug y and/or gene x.such queries require a database that can model key elements of the data, acquire data efficiently, providequery tools for analysis, and deliver the resulting system to the scientific community.a central challenge for pharmgkb is that data contained it must be crossreferenced and integrated with avariety of other webaccessible databases. thus, pharmgkb provides mechanisms for surveillance of andintegration with these databases, allowing users to submit one query with the assurance that other relevantdatabases are being accessed at the same time. for example, pharmgkb monitors dbsnp, the national centerfor biotechnology information (ncbi)supported repository for single nucleotide polymorphisms and shortdeletion and insertion polymorphisms. these monitoring operations search for new information about thegenes of interest to the various research groups associated with the pharmacogenetics research network. inaddition, pharmgkb provides users with a tool for comparative genomic analysis between human and mousethat focuses on longrange regulatory elements. such elements can be difficult to find experimentally, but areoften conserved in syntenic regions between mice and humans, and may be useful in focusing polymorphismstudies on noncoding areas that are more likely to be associated with detectable phenotypes.another important issue for the pharmgkb database is that because it contains clinical data derived fromindividual patients, it must have functionality that enforces the rights of those individuals to privacy andconfidentiality. thus, data flow must be limited both into and out of the knowledge base, based on evolvingrules defining what can be stored in pharmgkb and what can be disseminated. no identifying informationabout an individual patient can be accepted into the knowledge base, and the data must be òmassagedó sothat patient identity cannot be reconstructed from publicly available data records.29p. buneman, s. khanna, and w.c. tan, òwhy and where: a characterization of data provenance,ó 8th international conference on database theory (icdt), pp. 316330, 2001. cited in chung and wooley, òchallenges faced in the integration of biologicalinformation,ó 2003, footnote 28.analysis programs, and data derived from database searches are blurred. users of databases containingthese kinds of data must be concerned about where the data come from and how they are generated. adatabase may be a potentially rich information resource, but its value is diminished if it fails to keep anadequate description of the provenance of the data it contains.29 although proponents of online accesscatalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.on the nature of biological data53to databases frequently tout it as an advantage that òthe user does not need to know where the datacame from or where the data are located,ó in fact it is essential for quality assurance reasons that the userbe able to ascertain the source of all data accessed in such databases.data provenance addresses questions such as the following: where did the characterization of agiven genbank sequence originate? has an inaccurate legacy annotation been òtransitivelyó propagated to similar sequences? what is the evidence for this annotation?a complete record of a datumõs history presents interesting intellectual questions. for example, it isdifficult to justify filling a database with errata notices correcting simple errors when the actual entriesgenomicgenomicinformationinformationmolecular &molecular &cellularcellularphenotypephenotypeclinicalclinicalphenotypephenotypeallelesallelesmoleculesmoleculesindividualsindividualsdrugdrugresponseresponsesystemssystemsdrugsdrugsenvironmentenvironmentisolated isolated functional functional measuresmeasurescodingcodingrelationshiprelationshippharmacologicpharmacologicactivitiesactivitiesproteinproteinproductsproductsrole inrole inorganismorganismvariationsvariationsin genomein genomemolecularmolecularvariationsvariationstreatmenttreatmentprotocolsprotocolsobservableobservablephenotypesphenotypesgeneticgeneticmakeupmakeupphysiologyphysiologynonnongeneticgeneticfactorsfactorsintegratedintegratedfunctional functional measuresmeasuresobservableobservablephenotypesphenotypesfigure 3.4.1complexity of relationships in pharmacogenetics.source: figure reprinted and text adapted by permission from t.e. klein, j.t. chang, m.k. cho, k.l. easton, r. fergerson, m. hewett, z.lin, y. liu, s. liu, d.e. oliver, d.l. rubin, f. shafa, j.m. stuart, and r.b. altman, òintegrating genotype and phenotype information: anoverview of the pharmgkb project,ó the pharmacogenomics journal 1:167170, 2001. copyright 2001 macmillan publishers ltd.pharmgkb integrates data on clinical phenotypes (including both pharmacokinetic and pharmacodynamicdata) and profiles (e.g., pulmonary, cardiac, and psychological function tests; cancer chemotherapeutic sideeffects), dna sequence data, gene structure, and polymorphisms in sequence (and information to track haploid, diploid, or polyploid alleles; alternative splice sites; and polymorphisms observed as common variants),molecular and cellular phenotype data (e.g., enzyme kinetic measurements), pharmacodynamic assays, cellular drug processing rates, and homology modeling of threedimensional structures. figure 3.4.1 illustrates thecomplex relationships that are of interest for this knowledge base.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.54catalyzing inquirycan be updated. however, the original data themselves might be important, because subsequent research might have been based on them. one view is that once released, electronic database entries, likethe pages of a printed journal, must stand for all time in their original condition, with errors andcorrections noted only by the additional publication of errata and commentaries. however, this mightquickly lead to a situation in which commentary outweighs original entries severalfold. on the otherhand, occasional efforts to òimproveó individual entries might inadvertently result in important information being mistakenly expunged. a middle ground might be to require that individual releasedentries be stable, no matter what the type of error, but that change entries be classified into differenttypes (correction of data entry error, resubmission by original author, correction by different author,etc.), thus allowing the user to set filters to determine whether to retrieve all entries or just the mostrecent entry of a particular type.to illustrate the need for provenance, consider that the output of a program used for scientificanalysis is often highly sensitive to the parameters used and the specifics of the input datasets. in thecase of genomic analysis, a finding that two sequences are òsimilaró or not may depend on the specificalgorithms used and the different cutoff values used to parameterize matching algorithms, in whichcase other evidence is needed. furthermore, biological conclusions derived by inference in one databasewill be propagated and may no longer be reliable after numerous transitive assertions. repeated transitive assertions inevitably degrade data, whether the assertion is a transitive inference or the result of asimple òjoinó operation. in the absence of data perfection, additional degradation occurs with eachconnection.for a new sequence that does not match any known sequence, gene prediction programs can beused to identify open reading frames, to translate dna sequence into protein sequence, and to characterize promoter and regulatory sequence motifs. gene prediction programs are also parameterdependent, and the specifics of parameter settings must be retained if a future user is to make sense of theresults stored in the database.neuroscience provides a good example of the need for data provenance. consider the response ofrat cortical cells to various stimuli. in addition to the òprimaryó data themselvesñthat is, voltages as afunction of timeñit is also important to record information about the rat: where the rat came from, howthe rat was killed, how the brain was extracted, how the neurological preparation was made, whatbuffers were present, the temperature of the preparation, how much time elapsed between the sacrificeof the rat and the actual experiment being done, and so on. while all of this òextraó information seemsirrelevant to the primary question, neuroscience has not advanced to the point where it is known whichof these variables might have an effect on the response of interestñthat is, on the evoked corticalpotential.box 3.5 provides two examples of wellcharacterized and wellcurated data repositories.finally, how far curation can be carried is an open question. the point of curation is to providereliable and trustworthy datañwhat might be called biological truths. but the meaning of such òtruthsómay well change as more data is collected and more observations are madeñsuggesting a growingburden of constant editing to achieve accuracy and internal consistency. indeed, every new entry in thedatabase would necessarily trigger extensive validity checks of all existing entries individually andperhaps even for entries taken more than one at a time. moreover, assertions about the real world maybe initially believed, then rejected, then accepted again, albeit in a modified form. catastrophism ingeology is an example. thus, maintaining a database of all biological truths would be an editorialnightmare, if not an outright impossibilityñand thus the scope of any single database will necessarilybe limited.a database of biological observations and experimental results provides different challenges. anindividual datum or result is a standalone contribution. each datum or result has a recognized partyresponsible for it, and inclusion in the database means that it has been subject to some form of editorialreview, which presumably assures its adherence to current scientific practices (and does not guaranteecatalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.on the nature of biological data55box 3.5two examples of wellcurated data repositoriesgenbankgenbank is a public database of all known nucleotide and protein sequences, distributed by the nationalcenter for biotechnology information (ncbi), a division of the national library of medicine (nlm). as ofjanuary 2003, genbank contained over 20 billion nucleotide bases in sequences from more than 55,000speciesñhuman, mice, rat, nematode, fruit fly, and the model plant arabidopsis are the most represented.genbank and its collaborating european (embl) and japanese (jppl) databases are built with data submittedelectronically by individual investigators (using bankit or sequin submission programs) and largescale sequencing centers (using batch procedures). each submission is reviewed for quality assurance and assigned anaccession number; sequence updates are designated as new versions. the database is organized by a sequencebased taxonomy into divisions (e.g., bacteria, viruses, primates) and categories (e.g., expressed sequence tags, genome survey sequences, highthroughput genomic data). genbank makes available derivativedatabases, for example of putative new genes, from these data.investigators use the entrez retrieval system for crossdatabase searching of genbankõs collections of dna,protein, and genome mapping sequence data, population sets, the ncbi taxonomy, protein structures fromthe molecular modeling database (mmdb), and medline references (from the scientific literature). a popular tool is blast, the sequence alignment program, for finding genbank sequences similar to a query sequence. the entire database is available by anonymous ftp in compressed flatfile format, updated every 2months. ncbi offers its toolkit to software developers creating their own interfaces and specialized analyticaltools.the research resource for complex physiologic signalsthe research resource for complex physiologic signals was established by the national center for researchresources of the national institutes of health to support the study of complex biomedical signals. the creationof this threepart resource (physiobank, physiotoolkit, and physionet) overcomes longstanding barriers tohypothesistesting research in this field by enabling access to validated, standardized data and software.1physiobank comprises databases of multiparameter, cardiopulmonary, neural, and other biomedical signalsfrom healthy subjects and patients with pathologies such as epilepsy, congestive heart failure, sleep apnea,and sudden cardiac death. in addition to fully characterized, multiply reviewed signal data, physiobankprovides online access to archival data that underpin results reported in the published literature, significantlyextending the contribution of that published work. physiobank provides theoreticians and software developers with realistic data with which to test new algorithms.the physiotoolkit includes software for the detection of physiologically significant events using both classicmethods and novel techniques from statistical physics, fractal scaling analysis, and nonlinear dynamics; theanalysis of nonstationary processes; interactive display and characterization of signals; the simulation of physiological and other signals; and the quantitative evaluation and comparison of analysis algorithms.physionet is an online forum for the dissemination and exchange of recorded biomedical signals and thesoftware for analyzing such signals; it provides facilities for the cooperative analysis of data and the evaluationof proposed new algorithms. the database is available at http://www.physionet.org/physiobank.1a.l. goldberger, l.a. amaral, l. glass, j.m. hausdorff, p.c. ivanov, r.g. mark, j.e. mietus, g.b. moody, c.k. peng, and h.e. stanley,òphysiobank, physiotoolkit, and physionet: components of a new research resource for complex physiologic signals,ó circulation101(23):e215e220, 2000.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.56catalyzing inquiryits absolute truth value). without the existence of databases with differing editorial policies, someimportant but iconoclastic data or results might never be published. on the other hand, there is noguarantee of consistency among these data and results, which means that progress at the frontiers willdepend on expert judgment in deciding which data and results will constitute the foundation fromwhich to build.in short, reconciling the tension between truth and diversityñboth desirable, but for differentreasonsñis implicitly a part of the construction of every largescale database.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.computational tools57574computational toolsas a factual science, biological research involves the collection and analysis of data from potentiallybillions of members of millions of species, not to mention many trillions of base pairs across differentspecies. as data storage and analysis devices, computers are admirably suited to the task of supportingthis enterprise. also, as algorithms for analyzing biological data have become more sophisticated andthe capabilities of electronic computers have advanced, new kinds of inquiries and analyses havebecome possible.4.1the role of computational toolstoday, biology (and related fields such as medicine and pharmaceutics) are increasingly dataintensiveña trend that arguably began in the early 1960s.1 to manage these large amounts of data, andto derive insight into biological phenomena, biological scientists have turned to a variety of computational tools.as a rule, tools can be characterized as devices that help scientists do what they know they must do.that is, the problems that tools help solve are problems that are known by, and familiar to, the scientistsinvolved. further, such problems are concrete and well formulated. as a rule, it is critical that computational tools for biology be developed in collaboration with biologists who have deep insights into theproblem being addressed.the discussion below focuses on three generic types of computational tools: (1) databases and datamanagement tools to integrate large amounts of heterogeneous biological data, (2) presentation toolsthat help users comprehend large datasets, and (3) algorithms to extract meaning and useful information from large amounts of data (i.e., to find meaningful a signal in data that may look like noise at firstglance). (box 4.1 presents a complementary view of advances in computer sciences needed for nextgeneration tools for computational biology.)1the discussion in section 4.1 is derived in part from t. lenoir, òshaping biomedicine as an information science,ó proceedingsof the 1998 conference on the history and heritage of science information systems, m.e. bowden, t.b. hahn, and r.v. williams, eds.,asis monograph series, information today, inc., medford, nj, 1999, pp. 2745.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.58catalyzing inquirythese examples are drawn largely from the area of cell biology. the reason is not that these are theonly good examples of computational tools, but rather that a great deal of the activity in the field hasbeen the direct result of trying to make sense out of the genomic sequences that have been collected todate. as noted in chapter 2, the human genome projectñcompleted in draft in 2000ñis arguably thefirst largescale project of 21st century biology in which the need for powerful information technologywas manifestly obvious. since then, computational tools for the analysis of genomic data, and byextension data associated with the cell, have proliferated wildly; thus, a large number of examples areavailable from this domain.4.2tools for data integration2as noted in chapter 3, data integration is perhaps the most critical problem facing researchers asthey approach biology in the 21st century.box 4.1tool challenges for computer sciencedata representation¥nextgeneration genome annotation system with accuracy equal to or exceeding the best humanpredictions¥mechanism for multimodal representation of dataanalysis tools¥scalable methods of comparing many genomes¥tools and analyses to determine how molecular complexes work within the cell¥techniques for inferring and analyzing regulatory and signaling networks¥tools to extract patterns in mass spectrometry datasets¥tools for semantic interoperabilityvisualization¥tools to display networks and clusters at many levels of detail¥approaches for interpreting data streams and comparing highthroughput data with simulation outputstandards¥good softwareengineering practices and standard definitions (e.g., a common component architecture)¥standard ontology and dataexchange format for encoding complex types of annotationdatabases¥large repository for microbial and ecological literature relevant to the ògenomes to lifeó effort.¥big relational database derived by automatic generation of semantic metadata from the biological literature¥databases that support automated versioning and identification of data provenance¥longterm support of public sequence databasessource: u.s. department of energy, report on the computer science workshop for the genomes to life program, gaithersburg, md,march 67, 2002; available at http://doegenomestolife.org/compbio/.2sections 4.2.1, 4.2.4, 4.2.6, and 4.2.8 embed excerpts from s.y. chung and j.c. wooley, òchallenges faced in the integration ofbiological information,ó in bioinformatics: managing scientific data, z. lacroix and t. critchlow, eds., morgan kaufmann, sanfrancisco, ca, 2003. (hereafter cited as chung and wooley, 2003.)catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.computational tools594.2.1desiderataif researcher a wants to use a database kept and maintained by researcher b, the òquick and dirtyósolution is for researcher a to write a program that will translate data from one format into another. forexample, many laboratories have used programs written in perl to read, parse, extract, and transformdata from one form into another for particular applications.3 depending on the nature of the datainvolved and the structure of the source databases, writing such a program may require intensivecoding.although such a fix is expedient, it is not scalable. that is, pointtopoint solutions are not sustainable in a large community in which it is assumed that everyone wants to share data with everyone else.more formally, if there are n data sources to be integrated, and pointtopoint solutions must bedeveloped, n (n ð 1)/2 translation programs must be written. if one data source changes (as is highlylikely), n ð 1 programs must be updated.a more desirable approach to data integration is scalable. that is, a change in one database shouldnot necessitate a change on the part of every research group that wants to use those data. a number ofapproaches are discussed below, but in general, chung and wooley argue that robust data integrationsystems must be able to1.access and retrieve relevant data from a broad range of disparate data sources;2.transform the retrieved data into a common data model for data integration;3.provide a rich common data model for abstracting retrieved data and presenting integrated dataobjects to the enduser applications;4.provide a highlevel expressive language to compose complex queries across multiple datasources and to facilitate data manipulation, transformation, and integration tasks; and5.manage query optimization and other complex issues.sections 4.2.2, 4.2.4, 4.2.5, 4.2.6, and 4.2.8 address a number of different approaches to dealing withthe data integration problem. these approaches are not, in general, mutually exclusive, and they may beusable in combination to improve the effectiveness of a data integration solution.finally, biological databases are always changing, so integration is necessarily an ongoing task. notonly are new data being integrated within the existing database structure (a structure established on thebasis of an existing intellectual paradigm), but biology is a field that changes quicklyñthus requiringstructural changes in the databases that store data. in other words, biology does not have some òclassical core frameworkó that is reliably constant. thus, biological paradigms must be redesigned from timeto time (on the scale of every decade or so) to keep up with advances, which means that no ògoldstandardsó to organize data are built into biology. furthermore, as biology expands its attention toencompass complexes of entities and events as well as individual entities and events, more coherentapproaches to describing new phenomena will become necessaryñapproaches that bring some commonality and consistency to data representations of different biological entitiesñso that relationshipsbetween different phenomena can be elucidated.as one example, consider the potential impact of òomicó biology, biology that is characterized bya search for data completenessñthe complete sequence of the human genome, a complete catalog ofproteins in the human body, the sequencing of all genomes in a given ecosystem, and so on. thepossibility of such completeness is unprecedented in the history of the life sciences and will almostcertainly require substantial revisions to the relevant intellectual frameworks.3the perl programming language provides powerful and easytouse capabilities to search and manipulate text files. becauseof these strengths, perl is a major component of much bioinformatics programming. at the same time, perl is regarded by manycomputer scientists as an unsafe language in which it is easy to make programs do dangerous things. in addition, many regardthe syntax and structure of most perl programs to be of a nature that is hard to understand much after the fact.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.60catalyzing inquiry4.2.2 data standardsone obvious approach to data integration relies on technical standards that define representations ofdata and hence provide an understanding of data that is common to all database developers. for obviousreasons, standards are most relevant to future datasets. legacy databases, which have been built aroundunique data definitions, are much less amenable to a standardsdriven approach to data integration.standards are indeed an essential element of efforts to achieve data integration of future datasets,but the adoption of standards is a nontrivial task. for example, communitywide standards for datarelevant to a certain subject almost certainly differ from those that might be adopted by individuallaboratories, which are the focus of the òsmallinstrument, multidatasourceó science that characterizesmost publicsector biological research.ideally, source data from these projects flow together into larger national or international dataresources that are accessible to the community. adopting community standards, however, entails localcompromises (e.g., nonoptimal data structuring and semantics, greater expense), and the budgets thatcharacterize smallinstrument, singledatasource science generally do not provide adequate supportfor local data management and usually no support at all for contributions to a national data repository.if data from such diverse sources are to be maintained centrally, researchers and laboratories must haveincentives and support to adopt broader standards in the name of the communityõs greater good. in thisregard, funding agencies and journals have considerable leverage and through techniques such as requiringresearchers to deposit data in conformance to community standards may be able to provide such incentives.at the same time, data standards cannot resolve the integration problem by themselves even forfuture datasets. one reason is that in some fastmoving and rapidly changing areas of science (such asbiology), it is likely that the data standards existing at any given moment will not cover some newdimension of data. a novel experiment may make measurements that existing data standards did notanticipate. (for example, sequence databasesñby definitionñdo not integrate methylation data; and yetmethylation is an essential characteristic of dna that falls outside primary sequence information.) asknowledge and understanding advance, the meaning attached to a term may change over time. a secondreason is that standards are difficult to impose on legacy systems, because legacy datasets are usually verydifficult to convert to a new data standard and conversion almost always entails some loss of information.as a result, data standards themselves must evolve as the science they support changes. becausestandards cannot be propagated instantly throughout the relevant biological community, database amay be based on version 12.1 of a standard, and database b on version 12.4 of the òsameó standard. itwould be desirable if the differences between versions 12.1 and 12.4 were not large and a basic level ofintegration could still be maintained, but this is not ensured in an environment in which options varywithin standards, different releases and versions of products, and so on. in short, much of the devil ofensuring data integration is in the detail of implementation.experience in the database world suggests that standards gaining widespread acceptance in thecommercial marketplace tend to have a long life span, because the marketplace tends to weed out weakstandards before they become widely accepted. once a standard is widely used, industry is often motivated to maintain compliance with this accepted standard, but standards created by niche players in themarket tend not to survive. this point is of particular relevance in a fragmented research environment andsuggests that standards established by strong consortia of multiple players are more likely to endure.4.2.3 data normalization4an important issue related to data standards is data normalization. data normalization is the processthrough which data taken on the òsameó biological phenomenon by different instruments, procedures, orresearchers can be rendered comparable. such problems can arise in many different contexts:4section 4.2.3 is based largely on a presentation by c. ball, òthe normalization of microarray data,ó presented at the aaas2003 meeting in denver, colorado.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.computational tools61¥microarray data related to a given cell may be taken by multiple investigators in different laboratories.¥ecological data (e.g., temperature, reflectivity) in a given ecosystem may be taken by differentinstruments looking at the system.¥neurological data (e.g., timing and amplitudes of various pulse trains) related to a specificcognitive phenomenon may be taken on different individuals in different laboratories.the simplest example of the normalization problem is when different instruments are calibrateddifferently (e.g., a scale in georgeõs laboratory may not have been zeroed properly, rendering massmeasurements from georgeõs laboratory noncomparable to those from maryõs laboratory). if a largenumber of readings have been taken with georgeõs scale, one possible fix (i.e., one possible normalization) is to determine the extent of the zeroing required and to add or subtract that correction to thealready existing data. of course, this particular procedure assumes that the necessary zeroing wasconstant for each of georgeõs measurements. the procedure is not valid if the zeroing knob was jiggledaccidentally after half of the measurements had been taken.such biases in the data are systematic. in principle, the steps necessary to deal with systematic biasare straightforward. the researcher must avoid it as much as possible. because complete avoidance isnot possible, the researcher must recognize it when it occurs and then take steps to correct for it.correcting for bias entails determining the magnitude and effect of the bias on data that have been takenand identifying the source of the bias so that the data already taken can be modified and correctedappropriately. in some cases, the bias may be uncorrectable, and the data must be discarded.however, in practice, dealing with systematic bias is not nearly so straightforward. ball notes thatin the real world, the process goes something like this:1.notice something odd with data.2.try a few methods to determine magnitude.3.think of many possible sources of bias.4.wonder what in the world to do next.there are many sources of systematic bias, and they differ depending on the nature of the datainvolved. they may include effects due to instrumentation, sample (e.g., sample preparation, samplechoice), or environment (e.g., ambient vibration, current leakage, temperature). section 3.3 describes anumber of the systematic biases possible in microarray data, as do several references provided by ball.5there are many ways to correct for systematic bias, depending on the type of data being corrected.in the case of microarray studies, these ways include use of dye swap strategies, replicates and referencesamples, experimental controls, consistent techniques, and sensible array and experiment design. yet all5ballõs aaas presentation includes the following sources: t.b. kepler, l. crosby, and k.t. morgan, ònormalization andanalysis of dna microarray data by selfconsistency and local regression,ó genome biololgy 3(7), research0037.1 research0037.12, 2002. available at http://genomebiology.org/2002/3/7/research/0037.1; r. hoffmann, t. seidl, m. dugas.òprofound effect of normalization on detection of differentially expressed genes in oligonucleotide microarray data analysis,ó genome biolology 3(7):research0033.1research0033.111. available at http://genomebiology.com/2002/3/7/research/0033; c. colantuoni, g. henry, s. zeger, and j. pevsner, òlocal mean normalization of microarray element signalintensities across an array surface: quality control and correction of spatially systematic artifacts,ó biotechniques 32(6):13161320, 2002; b.p. durbin, j.s. hardin, d.m. hawkins, and d.m. rocke, òa variancestabilizing transformation for geneexpression microarray data,ó bioinformatics 18 (suppl. 1):s105s110, 2002; p.h. tran, d.a. peiffer, y. shin, l.m. meek, j.p. brody, andk.w. cho, òmicroarray optimizations: increasing spot accuracy and automated identification of true microarray signals,ónucleic acids research 30(12):e54, 2002, available at http://nar.oupjournals.org/cgi/content/full/30/12/e54; m. bilban, l.k.buehler, s. head, g. desoye, and v. quaranta, ònormalizing dna microarray data,ó current issues in molecular biology 4(2):5764, 2002; j. quackenbush, òmicroarray data normalization and transformation,ó nature genetics supplement 32:496501, 2002.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.62catalyzing inquiryof these approaches are laborintensive, and an outstanding challenge in the area of data normalizationis to develop approaches to minimize systematic bias that demand less labor and expense.4.2.4 data warehousingdata warehousing is a centralized approach to data integration. the maintainer of the data warehouse obtains data from other sources and converts them into a common format, with a global dataschema and indexing system for integration and navigation. such systems have a long track record ofsuccess in the commercial world, especially for resource management functions (e.g., payroll, inventory). these systems are most successful when the underlying databases can be maintained in a controlled environment that allows them to be reasonably stable and structured. data warehousing isdominated by relational database management systems (rdbms), which offer a mature and widelyaccepted database technology and a standard highlevel standard query language (sql).however, biological data are often qualitatively different from the data contained in commercialdatabases. furthermore, biological data sources are much more dynamic and unpredictable, and fewpublic biological data sources use structured database management systems. data warehouses are oftentroubled by a lack of synchronization between the data they hold and the original database from whichthose data derive because of the time lag involved in refreshing the data warehouse store. data warehousing efforts are further complicated by the issue of updates. stein writes:6one of the most ambitious attempts at the warehouse approach [to database integration] was the integrated genome database (igd) project, which aimed to combine human sequencing data with the multiple genetic and physical maps that were the main reagent for human genomics at the time. at its peak,igd integrated more than a dozen source databases, including genbank, the genome database (gdb)and the databases of many human geneticmapping projects. the integrated database was distributed toendusers complete with a graphical front end. . . . the igd project survived for slightly longer than ayear before collapsing. the main reason for its collapse, as described by the principal investigator on theproject (o. ritter, personal communication, as relayed to stein), was the database churn issue. on average, each of the source databases changed its data model twice a year. this meant that the igd dataimport system broke down every two weeks and the dumping and transformation programs had to berewrittenña task that eventually became unmanageable.also, because of the breadth and volume of biological databases, the effort involved in maintaininga comprehensive data warehouse is enormousñand likely prohibitive. such an effort would have tointegrate diverse biological information, such as sequence and structure, up to the various functions ofbiochemical pathways and genetic polymorphisms.still, data warehousing is a useful approach for specific applications that are worth the expense ofintense data cleansing to remove potential errors, duplications, and semantic inconsistency.7 two current examples of data warehousing are genbank and the international consortium for brain mapping(icbm) (the latter is described in box 4.2).4.2.5 data federationthe data federation approach to integration is not centralized and does not call for a òmasteródatabase. data federation calls for scientists to maintain their own specialized databases encapsulatingtheir particular areas of expertise and retain control of the primary data, while still making it availableto other researchers. in other words, the underlying data sources are autonomous. data federation often6reprinted by permission from l.d. stein, òintegrating biological databases,ó nature reviews genetics 4(5)337345, 2003. copyright 2005 macmillan magazines ltd.7r. resnick, òsimplified data mining,ó pp. 5152 in drug discovery and development, 2000. (cited in chung and wooley, 2003.)catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.computational tools63box 4.2the international consortium for brain mapping (icbm):a probabilistic atlas and reference system for the human brainin the human population, the brain varies structurally and functionally in currently undefined ways. it is clearthat the size, shape, symmetry, folding pattern, and structural relationships of the systems in the human brainvary from individual to individual. this has been a source of considerable consternation and difficulty inresearch and clinical evaluations of the human brain from both the structural and the functional perspective.current atlases of the human brain do not address this problem. cytoarchitectural and clinical atlases typicallyuse a single brain or even a single hemisphere as the reference specimen or target brain to which other brainsare matched, typically with simple linear stretching and compressing strategies. in 1992, john mazziotta andarthur toga proposed the concept of developing a probabilistic atlas from a large number of normal subjectsbetween the ages of 18 and 90. this data acquisition has now been completed, and the value of such an atlasis being realized for both research and clinical purposes. the mathematical and software machinery requiredto develop this atlas of normal subjects is now also being applied to patient populations including individualswith alzheimerõs disease, schizophrenia, autism, multiple sclerosis, and others.talairach atlasto date, more than 7,000 normal subjects have been entered into the talairach atlas project and a wide rangeof datasets. these datasets contain detailed demographic histories of the subjects, results of general medicaland neurological examinations, neuropsychiatric and neuropsychological evaluations, quantitative òhandedness measurementsó, and imaging studies. the imaging studies include multispectra 1 mm3 voxelsize magnetic resonance imaging (mri) evaluations of the entire brain (t1, t2, and proton density pulse sequences). asubset of individuals also undergo functional mri, cerebral blood flow position emission tomography (pet)and electroencephalogram (eeg) examinations (evoked potentials). of these subjects, 5,800 individuals havealso had their dna collected and stored for future genotyping. as such, this database represents the mostcomprehensive evaluation of the structural and functional imaging phenotypes of the human brain in thenormal population across a wide age span and very diverse social, economic, and racial groups. participatinglaboratories are widely distributed geographically from asia to scandinavia, and include eight laboratories, inseven countries, on four continents.world map of sitesa component of the world map of sites project involves the post mortem mri imaging of individuals whohave willed their bodies to science. subsequent to mri imaging, the brain is frozen and sectioned at a resolution of approximately 100 microns. block face images are stored, and the sectioned tissue is stained forcytoarchitectural, chemoarchitectural, and differential myelin to produce microscopic maps of cellular anatomy, neuroreceptor or transmitter systems, and white matter tracts. these datasets are then incorporated intoa target brain to which the in vivo brain studies are warped in three dimensions and labeled automatically.the 7,000 datasets are then placed in the standardized space, and probabilistic estimates of structural boundaries, volumes, symmetries, and shapes are computed for the entire population or any subpopulation (e.g.,age, gender, race). in the current phase of the program, information is being added about in vivo chemoarchitecture (5ht2a [5hydroxytryptamine2a] in vivo pet receptor imaging), in vivo white matter tracts (mridiffusion tensor imaging), vascular anatomy (magnetic resonance angiography and venography), and cerebralconnections (transcranial magnetic stimulationpet cerebral blood flow measurements).target brainthe availability of 342 twin pairs in the dataset (half monozygotic and half dizygotic) along with dna forgenotyping provides the opportunity to understand structurefunction relationships related to genotype and,therefore, provides the first largescale opportunity to relate phenotypegenotype in behavior across a widerange of individuals in the human population.continuedcatalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.64catalyzing inquirycalls for the use of objectoriented concepts to develop data definitions, encapsulating the internaldetails of the data associated with the heterogeneity of the underlying data sources.8 a change in therepresentation or definition of the data then has minimal impact on the applications that access thosedata.an example of a data federation environment is biomoby, which is based on two ideas.9 the firstis the notion that databases provide bioinformatics services that can be defined by their inputs andoutputs. (for example, blast is a service provided by genbank that can be defined by its inputñthatis, an uncharacterized sequenceñand by its output, namely, described gene sequences deposited ingenbank.) the second idea is that all database services would be linked to a central registry (mobycentral) of services that users (or their applications) would query. from moby central, a user couldmove from one set of inputoutput services to the nextñfor example, moving from one database that,given a sequence (the input), postulates the identity of a gene (the output), and from there to a databasethat, given a gene (the input), will find the same gene in multiple organisms (the output), and so on,picking up information as it moves through database services. there are limitations to the biomobysystemõs ability to discriminate database services based the descriptions of inputs and outputs, andmoby central must be up and running 24 hours a day.10box 4.2 continuedthe development of similar atlases to evaluate patients with welldefined disease states allows the opportunityto compare the normal brain with brains of patients having cerebral pathological conditions, thereby potentially leading to enhanced clinical trials, automated diagnoses, and other clinical applications. such exampleshave already emerged in patients with multiple sclerosis and epilepsy. an example in alzheimerõs diseaserelates to a current hotly contested research question. individuals with alzheimerõs disease have a greaterlikelihood of having the genotype apoe 4 (as opposed to apoe 2 or 3). having this genotype, however, isneither sufficient nor required for the development of alzheimerõs disease. individuals with alzheimerõs disease also have small hippocampi, presumably because of atrophy of this structure as the disease progresses.the question of interest is whether individuals with the highrisk genotype (apoe 4) have small hippocampi tobegin with. this would be a very difficult hypothesis to test without the dataset described above. with theicbm database, it is possible to study individuals from, for example, ages 20 to 40 and identify those with thesmallest (lowest 5 percent) and largest (highest 5 percent) hippocampal volumes. this relatively small numberof subjects could then be genotyped for apoe alleles. if individuals with small hippocampi all had the genotype apoe 4 and those with large hippocampi all had the genotype apoe 2 or 3, this would be strong supportfor the hypothesis that individuals with the highrisk genotype for the development of alzheimerõs diseasehave small hippocampi based on genetic criteria as a prelude to the development of alzheimerõs disease.similar genotypeimaging phenotype evaluations could be undertaken across a wide range of human conditions, genotypes, and brain structures.source: modified from john c. mazziotta and arthur w. toga, department of neurology, david geffen school of medicine, universityof california, los angeles, personal communication to john wooley, february 22, 2004.8r.g.g. cattell, object data management: objectoriented and extended relational database systems, revised edition, addisonwiley, reading, ma, 1994. (cited in chung and wooley, 2003.)9m.d. wilkinson and m. links, òbiomoby: an opensource biological web services proposal,ó briefings in bioinformatics3(4):331341, 2002.10l.d. stein, òintegrating biological databases,ó nature reviews genetics 4(5):337345, 2003.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.computational tools654.2.6 data mediators/middlewarein the middleware approach, an intermediate processing layer (a òmediatoró) decouples the underlying heterogeneous, distributed data sources and the client layer of end users and applications.11 themediator layer (i.e., the middleware) performs the core functions of data transformation and integration, and communicates with the database òwrappersó and the user application layer. (a òwrapperó isa software component associated with an underlying data source that is generally used to handle thetasks of access to specified data sources, extraction and retrieval of selected data, and translation ofsource data formats into a common data model designed for the integration system.)the common model for data derived from the underlying data sources is the responsibility of themediator. this model must be sufficiently rich to accommodate various data formats of existing biological data sources, which may include unstructured text files, semistructured xml and html files, andstructured relational, objectoriented, and nested complex data models. in addition, the internal datamodel must facilitate the structuring of integrated biological objects to present to the user applicationlayer. finally, the mediator also provides services such as filtering, managing metadata, and resolvingsemantic inconsistency in source databases.there are many flavors of mediator approaches in life science domains. ibmõs discoverylink for thelife sciences is one of the best known.12 the kleisli system provides an internal nested complex datamodel and a highpower query and transformation language for data integration.13 k2 shares manydesign principles with kleisli in supporting a complex data model, but adopts more objectorientedfeatures.14 opm supports a rich object model and a global schema for data integration.15 tambisprovides a global ontology (see section 4.2.8 on ontologies) to facilitate queries across multiple datasources.16 tsimmis is a mediation system for information integration with its own data model (objectexchange model, oem) and query language.174.2.7 databases as modelsa natural progression for databases established to meet the needs and interests of specializedcommunities, such as research on cell signaling pathways or programmed cell death, is the evolution of11g. wiederhold, òmediators in the architecture of future information systems,ó ieee computer 25(3):3849, 1992; g.wiederhold and m. genesereth, òthe conceptual basis for mediation services,ó ieee expert, intelligent systems and their applications 12(5):3847, 1997. (both cited in chung and wooley, 2003.)12l.m. haas et al., òdiscoverylink: a system for integrated access to life sciences data sources,ó ibm systems journal 40(2):489511, 2001.13s. davidson, c. overton, v. tannen, and l. wong, òbiokleisli: a digital library for biomedical researchers,ó internationaljournal of digital libraries 1(1):3653, 1997; l. wong, òkleisli, a functional query system,ó journal of functional programming10(1):1956, 2000. (both cited in chung and wooley, 2003.)14j. crabtree, s. harker, and v. tannen, òthe information integration system k2,ó available at http://db.cis.upenn.edu/k2/k2.doc; s.b. davidson, j. crabtree, b.p. brunk, j. schug, v. tannen, g.c. overton, and c.j. stoeckert, jr., òk2/kleisli and gus:experiments in integrated access to genomic data sources,ó ibm systems journal 40(2):489511, 2001. (both cited in chung andwooley, 2003.)15im.a. chen and v.m. markowitz, òan overview of the objectprotocol model (opm) and opm data management tools,óinformation systems 20(5):393418, 1995; im.a. chen, a.s. kosky, v.m. markowitz, and e. szeto, òconstructing and maintainingscientific database views in the framework of the objectprotocol model,ó proceedings of the ninth international conference onscientific and statistical database management, institute of electrical and electronic engineers, inc., new york, 1997, pp. 237ð248.(cited in chung and wooley, 2003.)16n.w. paton, r. stevens, p. baker, c.a. goble, s. bechhofer, and a. brass, òquery processing in the tambis bioinformaticssource integration system,ó proceedings of the 11th international conference on scientific and statistical database management, ieee,new york 1999, pp. 138147; r. stevens, p. baker, s. bechhofer, g. ng, a. jacoby, n.w. paton, c.a. goble, and a. brass,òtambis: transparent access to multiple bioinformatics information sources,ó bioinformatics 16(2):184186, 2000. (both cited inchung and wooley, 2003.)17y. papakonstantinou, h. garciamolina, and j. widom, òobject exchange across heterogeneous information sources,óproceedings of the ieee conference on data engineering, ieee, new york, 1995, pp. 251260. (cited in chung and wooley, 2003.)catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.66catalyzing inquirydatabases into models of biological activity. as databases become increasingly annotated with functional and other information, they lay the groundwork for model formation.in the future, such òdatabase modelsó are envisioned as the basis of informed predictions anddecision making in biomedicine. for example, physicians of the future may use biological informationsystems (biss) that apply known interactions and causal relationships among proteins that regulate celldivision to changes in an individualõs dna sequence, gene expression, and proteins in an individualtumor.18 the physician might use this information together with the bis to support a decision onwhether the inhibition of a particular protein kinase is likely to be useful for treating that particulartumor.indeed, a major goal in the forprofit sector is to create richly annotated databases that can serve astestbeds for modeling pharmaceutical applications. for example, entelos has developed physiolab, acomputer model system consisting of a large set (more than 1,000) of ordinary nonlinear differentialequations.19 the model is a functional representation of human pathophysiology based on currentgenomic, proteomic, in vitro, in vivo, and ex vivo data, built using a topdown, diseasespecific systemsapproach that relates clinical outcomes to human biology and physiology. starting with major organsystems, virtual patients are explicit mathematical representations of a particular phenotype, based onknown or hypothesized factors (genetic, lifestyle, environmental). each model simulates up to 60separate responses previously demonstrated in human clinical studies.in the neuroscience field, bower and colleagues have developed the modelerõs workspace,20 whichis based on a notion that electronic databases must provide enhanced functionality over traditionalmeans of distributing information if they are to be fully successful. in particular, bower et al. believethat computational models are an inherently more powerful medium for the electronic storage andretrieval of information than are traditional online databases.the modelerõs workspace is thus designed to enable researchers to search multiple remote databases for model components based on various criteria; visualize the characteristics of the componentsretrieved; create new components, either from scratch or derived from existing models; combine components into new models; link models to experimental data as well as online publications; and interactwith simulation packages such as genesis to simulate the new constructs.the tools contained in the workspace enable researchers to work with structurally realistic biological models, that is, models that seek to capture what is known about the anatomical structure andphysiological characteristics of a neural system of interest. because they are faithful to biologicalanatomy and physiology, structurally realistic models are a means of storing anatomical and physiological experimental information.for example, to model a part of the brain, this modeling approach starts with a detailed descriptionof the relevant neuroanatomy, such as a description of the threedimensional structure of the neuronand its dendritic tree. at the singlecell level, the model represents information about neuronal morphology, including such parameters as soma size, length of interbranch segments, diameter of branches,bifurcation probabilities, and density and size of dendritic spines. at the neuronal network level, themodel represents the cell types found in the network and the connectivity among them. the model mustalso incorporate information regarding the basic physiological behavior of the modeled structureñforexample, by tuning the model to replicate neuronal responses to experimentally derived data.with such a framework in place, a structural model organizes data in ways that make manifestlyobvious how those data are related to neural function. by contrast, for many other kinds of databases itis not at all obvious how the data contained therein contribute to an understanding of function. bower18r. brent and d. endy, òmodelling cellular behaviour,ó nature 409:391395, 2001.19see, for example, http://www.entelos.com/science/physiolabtech.html.20m. hucka, k. shankar, d. beeman, and j.m. bower, òthe modelerõs workspace: making modelbased studies of the nervoussystem more accessible,ó computational neuroanatomy: principles and methods, g.a. ascoli, ed., humana press, totowa, nj, 2002,pp. 83103.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.computational tools67and colleagues argue that òas models become more sophisticated, so does the representation of the data.as models become more capable, they extend our ability to explore the functional significance of thestructure and organization of biological systems.ó214.2.8 ontologiesvariations in language and terminology have always posed a great challenge to largescale, comprehensive integration of biological findings. in part, this is due to the fact that scientists operate, witha data and experiencedriven intuition that outstrips the ability of language to describe. as early as1952, this problem was recognized:geneticists, like all good scientists, proceed in the first instance intuitively and . . . their intuition hasvastly outstripped the possibilities of expression in the ordinary usages of natural languages. they knowwhat they mean, but the current linguistic apparatus makes it very difficult for them to say what theymean. this apparatus conceals the complexity of the intuitions. it is part of the business of geneticalmethodology first to discover what geneticists mean and then to devise the simplest method of sayingwhat they mean. if the result proves to be more complex than one would expect from the current expositions, that is because these devices are succeeding in making apparent a real complexity in the subjectmatter which the natural language conceals.22in addition, different biologists use language with different levels of precision for different purposes. for instance, the notion of òidentityó is different depending on context.23 two geneticists maylook at a map of human chromosome 21. a year later, they both want to look at the same map again. butto one of them, òsameó means exactly the same map (same data, bit for bit); to the other, it means thecurrent map of the same biological object, even if all of the data in that map have changed. to a proteinchemist, two molecules of betahemoglobin are the same because they are composed of exactly the samesequence of amino acids. to a biologist, the same two molecules might be considered different becauseone was isolated from a chimpanzee and the other from a human.to deal with such contextsensitive problems, bioinformaticians have turned to ontologies. anontology is a description of concepts and relationships that exist among the concepts for a particulardomain of knowledge.24 ontologies in the life sciences serve two equally important functions. first,they provide controlled, hierarchically structured vocabularies for terminology that can be used todescribe biological objects. second, they specify object classes, relations, and functions in ways thatcapture the main concepts of and relationships in a research area.4.2.8.1ontologies for common terminology and descriptionsto associate concepts with the individual names of objects in databases, an ontology tool mightincorporate a terminology database that interprets queries and translates them into search terms consistent with each of the underlying sources. more recently, ontologybased designs have evolved fromstatic dictionaries into dynamic systems that can be extended with new terms and concepts withoutmodification to the underlying database.21m. hucka, k. shankar, d. beeman, and j.m. bower, òthe modelerõs workspace,ó 2002.22j.h. woodger, biology and language, cambridge university press, cambridge, uk, 1952.23r.j. robbins, òobject identity and life science research,ó position paper submitted for the semantic web for life sciencesworkshop, october 2728 2004, cambridge, ma, available at http://lists.w3.org/archives/public/publicswlsws/2004sep/att0050/position01.pdf.24the term òontologyó is a philosophical term referring to the subject of existence. the computer science community borrowedthe term to refer to òspecification of a conceptualizationó for knowledge sharing in artificial intelligence. see, for example, t.r.gruber, òa translation approach to portable ontology specification,ó knowledge acquisition 5(2):199220, 1993. (cited in chungand wooley, 2003.)catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.68catalyzing inquirya feature of ontologies that facilitates the integration of databases is the use of a hierarchicalstructure that is progressively specialized; that is, specific terms are defined as specialized forms ofgeneral terms. two different databases might not extend their annotation of a biological object to thesame level of specificity, but the databases can be integrated by finding the levels within the hierarchythat share a common term.the naming dimension of ontologies has been common to research in the life sciences for much ofits history, although the term itself has not been widely used. chung and wooley note the following, forexample:¥the linnaean system for naming of species and organisms in taxonomy is one of the oldestontologies.¥the nomenclature committee for the international union of pure and applied chemistry (iupac)and the international union of biochemistry and molecular biology (iubmb) make recommendationson organic, biochemical, and molecular biology nomenclature, symbols, and terminology.¥the national library of medicine medical subject headings (mesh) provides the most comprehensive controlled vocabularies for biomedical literature and clinical records.¥a division of the college of american pathologists oversees the development and maintenanceof a comprehensive and controlled terminology for medicine and clinical information known assnomed (systematized nomenclature of medicine).¥the gene ontology consortium25 seeks to create an ontology to unify work across many genomic projectsñto develop controlled vocabulary and relationships for gene sequences, anatomy, physical characteristics, and pathology across the mouse, yeast, and fly genomes.26 the consortiumõs initialefforts focus on ontologies for molecular function, biological process, and cellular components of geneproducts across organisms and are intended to overcome the problems associated with inconsistentterminology and descriptions for the same biological phenomena and relationships.perhaps the most negative aspect of ontologies is that they are in essence standards, and hence takea long time to developñand as the size of the relevant community affected by the ontology increases, sodoes development time. for example, the ecological and biodiversity communities have made substantial progress in metadata standards, common taxonomy, and structural vocabulary with the help ofnational science foundation and other government agencies.27 by contrast, the molecular biologycommunity is much more diverse, and reaching a communitywide consensus has been much harder.an alternative to seeking communitywide consensus is to seek consensus in smaller subcommunities associated with specific areas of research such as sequence analysis, gene expression, protein pathways, and so on.28 these efforts usually adopt a usecase and opensource approach for communityinput. the ontologies are not meant to be mandatory, but instead to serve as a reference frameworkfrom which further development can proceed.25see www.geneontology.org.26m. ashburner, c.a. ball, j.a. blacke, d. botstein, h. butler, j.m. cherry, a.p. davis, et al., ògene ontology: tool for theunification of biology,ó nature genetics 25(1):25ð29, 2000. (cited in chung and wooley, 2003.)27j.l. edwards, m.a. lane, and e.s. nielsen, òinteroperability of biodiversity databases: biodiversity information on everydesk,ó science 289(5488):23122314, 2000; national biological information infrastructure (nbii), available at http://www.nbii.gov/disciplines/systematics.html; federal geographic data committee (fgdc), available at http://www.fgdc.gov/.(all cited in chung and wooley, 2003.)28gene expression ontology working group, see http://www.mged.org/; p.d. karp, m. riley, s.m. paley, and a. pellegrinitoole, òthe metacyc database,ó nucleic acids research 30(1):5961, 2002; p.d. karp, m. riley, m. saier, i.t. paulsen, j. colladovides, s.m. paley, a. pellegrinitoole, et al., òthe ecocyc database,ó nucleic acids research 30(1):5658, 2002; d.e. oliver, d.l.rubin, j.m. stuart, m. hewett, t.e. klein, and r.b. altman, òontology development for a pharmacogenetics knowledge base,ópacific symposium on biocomputing 6576, 2002. (all cited in chung and wooley, 2003.)catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.computational tools69an ontology developed by one subcommunity inevitably leads to interactions with related ontologies and the need to integrate. for example, consider the concept of homology. in traditional evolutionary biology, òanalogyó is used to describe things that are identical by function and òhomologyó is usedto identify things that are identical by descent. however, in considering dna, function and descent areboth captured in the dna sequence, and therefore to molecular biologists, homology has come to meansimply similarity in sequence, regardless of whether this is due to convergence or ancestry. thus, theterm òhomologousó means different things in molecular biology and evolutionary biology.29 morebroadly, a brain ontology will inevitably relate to ontologies of other anatomic structures or at themolecular level sharing ontologies for genes and proteins.30difficulties of integrating diverse but related databases thus are transformed into analogous difficulties in integrating diverse but related ontologies, but since each ontology represents the integrationof multiple databases relevant to the field, the integration effort at the higher level is more encompassing. at the same time, it is also more difficult, because the implications of changes in fundamentalconceptsñwhich will be necessary in any integration effortñare much more farreaching than analogous changes in a database. that is, design compromises in the development of individual ontologiesmight make it impossible to integrate the ontologies without changes to some of their basic components.this would require undoing the ontologies, then redoing them to support integration.these points relate to semantic interoperability, which is an active area of research in computerscience.31 information integration across multiple biological disciplines and subdisciplines would depend on the close collaborations of domain experts and information technology professionals to develop algorithms and flexible approaches to bridge the gaps between multiple biological ontologies. inrecent years, a number of life science researchers have come to believe in the potential of the semanticweb for integrating biological ontologies, as described in box 4.3.a sample collection of ontology resources for controlled vocabulary purposes in the life sciences islisted in table 4.1.4.2.8.2ontologies for automated reasoningtoday, it is standard practice to store biological data in databases; no one would deny that thevolume of available data is far beyond the capabilities of human memory or written text. however, evenas the volume of analytic and theoretical results drawn from these data (such as inferred geneticregulatory, metabolic, and signaling network relationships) grows, it will become necessary to storesuch information as well in a format suitable for computational access.the essential rationale underlying automated reasoning is that reasoning oneõs way through all ofthe complexity inherent in biological organisms is very difficult, and indeed may be, for all practicalpurposes, impossible for the knowledge bases that are required to characterize even the simplest organisms. consider, for example, the networks related to genetic regulation, metabolism, and signaling of anorganism such as escherichia coli. these networks are too large for humans to reason about in theirtotality, which means that it is increasingly difficult for scientists to be certain about global networkproperties. is the model complete? is it consistent? does it explain all of the data? for example, thedatabase of known molecular pathways in e. coli contains many hundreds of connections, far more thanmost researchers could remember, much less reason about.29for more on the homology issue, see w.m. fitch, òhomology: a personal view on some of the problems,ó trends in genetics16(5):227231, 2000.30a. gupta, b. lud−scher, and m.e. martone, òknowledgebased integration of neuroscience data sourcesó conference onscientific and statistical database management, berlin, ieee computer society, july 2000. (cited in chung and wooley, 2003.)31p. mitra, g. wiederhold, and m. kersten, òa graphoriented model for articulation of ontology interdependencies,ó proceedings of conference on extending database technology konstanz, germany, march 2000. (cited in chung and wooley, 2003.)catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.70catalyzing inquirybox 4.3biological data and the semantic webthe semantic web seeks to create a universal medium for the exchange of machineunderstandabledata of all types, including biological data. using semantic web technology, programs can share andprocess data even when they have been designed totally independently. the semantic web involves aresource description framework (rdf), an rdf schema language, and the web ontology language(owl). rdf and owl are semantic web standards that provide a framework for asset management,enterprise integration and the sharing and reuse of data on the web. furthermore, a standardized querylanguage for rdf enables the òjoiningó of decentralized collections of rdf data. the underlying technology foundation of these languages is that of urls, xml, and xml name spaces.within the life sciences, the notion of a life sciences identifier (lsid) is intended to provide a straightforward approach to naming and identifying data resources stored in multiple, distributed data stores ina manner that overcomes the limitations of naming schemes in use today. lsids are persistent, locationindependent, resource identifiers for uniquely naming biologically significant resources including butnot limited to individual genes or proteins, or data objects that encode information about them.the life sciences pose a particular challenge for data integration because the semantics of biologicalknowledge are constantly changing. for example, it may be known that two proteins bind to each other.but this fact could be represented at the cellular level, the tissue level, and the molecular level depending on the context in which that fact was important.the semantic web is intended to allow for evolutionary change in the relevant ontologies as newscience emerges without the need for consensus. for example, if researcher a states (and encodesusing semantic web technology) a relationship between a protein and a signaling cascade withwhich researcher b disagrees, researcher b can instruct his or her computer to ignore (perhapstemporarily) the relationship encoded by researcher a in favor (perhaps) of a relationship that isdefined only locally.an initiative coordinated by the world wide web consortium seeks to explore how semantic webtechnologies can be used to reduce the barriers and costs associated with effective data integration,analysis, and collaboration in the life sciences research community, to enable disease understanding,and to accelerate the development of therapies. a meeting in october 2004 on the semantic web andthe life sciences concluded that work was needed in two highpriority areas.¥in the area of ontology development, collaborative efforts were felt required to define core vocabularies that can bridge data and ontologies developed by individual communities of practice. thesevocabularies would address provenance and context (e.g., identifying data sources, authors, publications names, and collection conditions), terms for crossreferences in publication and other reporting ofexperimental results, navigation, versioning, and geospatial/temporal quantifiers.¥with respect to lsids, the problem of sparse implementation was regarded as central, and participants believed that work should focus on how to implement lsids in a manner that leverages existingweb resource resolution mechanisms such as http servers.sources: the semantic web activity statement, available at http://www.w3.org/2001/sw/activity; life sciences identifiers rfpresponse, omg document lifesci/20031202, january 12, 2004, available at http://www.omg.org/docs/lifesci/031202.doc#toc61702471; john wilbanks, science commons, massachusetts institute of technology, personal communication, april4, 2005.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.computational tools71table 4.1biological ontology resourcesorganizationdescriptionshuman genome organization (hugo) genehgnc is responsible for the approval of a unique symbolnomenclature committee (hgnc):for each gene and designate description of genes. aliaseshttp://www.gene.ucl.ac.uk/nomenclature/for genes are also listed in the database.gene ontology consortium (go):the purpose of go is to develop ontologies describing thehttp://www.geneontology.orgmolecular function, biological process, and cellularcomponent of genes and gene products for eukaryotes.members include genome databases of fly, yeast, mouse,worm, and arabidopsis.plant ontology consortium:this consortium will produce structured, controlledhttp://www.plantontology.orgvocabularies applied to plantbased database information.microarrey gene expression data (mged)the mged group facilitates the adoption of standards forsociety ontology working group:dnamicroarray experiment annotation and datahttp://www.mged.org/representation, as well as the introduction of standardexpertmental controls and data normalization methods.nibii (national biological informationnbii provides links to taxonomy sites for all biologicalinfrastructure):disciplines.http://www.nbii.gov/disciplines/systematics.htmlitis (integrated taxonomic information system):itis provides taxonomic information on plants, animals,http://www.itis.usda.gov/and microbes of north america and the world.mesh (medical subject headings):mesh is a controlled vocabulary established by thehttp://www.nlm.nih.gov/mesh/national library of medicine (nlm) and used for indexingmeshhome.htmlarticles, cataloging books and other holdings, and searchingmeshindexed databases, including medline.snomed (systematized nomenclature ofsnomed is recognized globally as a comprehensive,medicine):multiaxial, controlled terminology created for the indexinghttp://www.snomed.org/of the entire medical record.international classification of diseases,icd9cm is the official system of assigning codes toninth revision, clinical modificationdiagnoses and procedures associated with hospital(icd9cm):utilization in the united states. it is published by the u.s.http://www.cdc.gov/nchs/about/national center for health statistics.otheract/lcd9/abtlcd9.htminternational union of pure and appliediupac and iubmb make recommendations on organic,chemistry (iupaq)biochemical, and molecular biology nomenclature,symbols, and terminology.international union of biochemistry andmolecular biology (iubmb) nomenclaturecommittee:http://www.chem.qmul.ac.uk/iubmb/pharmgkb ( pharmacogenetics knowledge base:pharmgkb, develops ontologies for pharmacogenetics andhttp://pharmgkb.org/pharmacogenomics.continuedcatalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.72catalyzing inquiryby representing working hypotheses, derived results, and the evidence that supports and refutesthem in machinereadable representations, researchers can uncover correlations in and make inferencesabout independently conducted investigations of complex biological systems that would otherwiseremain undiscovered by relying simply on serendipity or their own reasoning and memory capacities.32 in principle, software can read and operate on these representations, determining properties in away similar to human reasoning, but able to consider hundreds or thousands of elements simultaneously. although automated reasoning can potentially predict the response of a biological system to aparticular stimulus, it is particularly useful for discovering inconsistencies or missing relations in thedata, establishing global properties of networks, discovering predictive relationships between elements,and inferring or calculating the consequences of given causal relationships.33 as the number of discovered pathways and molecular networks increases and the questions of interest to researchers becomemore about global properties of organisms, automated reasoning will become increasingly useful.symbolic representations of biological knowledgeñontologiesñare a foundation for such efforts.ontologies contain names and relationships of the many objects considered by a theory, such as genes,enzymes, proteins, transcription, and so forth. by storing such an ontology in a symbolic machinetable 4.1continuedorganizationdescriptionsmmcef (macromolecular crystallographicthe information file mmcef is sponsored by iucrinformation file):(international union of crystallography) to provide ahttp://pdb.rutgers.edu/mmcif/dictionary for data items relevant to macromolecularhttp://www.iucr.ac.ukliucrtop/cif/index.htmlcrystallographic experiments.locuslink:locuslink contains genecentered resources, includinghttp://www.ncbi.nlm.nih.gov/locuslink/nomenclature and aliases for genes.prot”g”2000:prot”g”2000 is a tool that allows the user to construct ahttp://protege.stanford.edudomain ontology that can be extended to access embeddedapplications in other knowledgebased systems. a numberof biomedical ontologies have been constructed with thissystem, but it can be applied to other domains as well.tambis:tambis aims to aid researchers in the biological scienceshttp://imgproj.cs.man.ac.uk/tambis/by providing a single access point for biologicalinformation sources around the world. the access point willbe a single webbased interface that acts as a singleinformation source. it will find appropriate sources ofinformation for user queries and phrase the user questionsfor each source, returning the results in a consistent mannerwhich will include details of the information source.32l. hunter, òontologies for programs, not people,ó genome biology 3(6):1002.11002.2, 2002.33as shown in chapter 5, simulations are also useful for predicting the response of a biological system to various stimuli. butsimulations instantiate procedural knowledge (i.e., how to do something), whereas the automated reasoning systems discussedhere operate on declarative knowledge (i.e., knowledge about something). simulations are optimized to answer a set of questionsthat is narrower than those that can be answered by automated reasoning systemsñnamely, predictions about the subsequentresponse of a system to a given stimulus. automated reasoning systems can also answer such questions (though more slowly),but in addition they can answer questions such as, what part of a network is responsible for this particular response?, presumingthat such (declarative) knowledge is available in the database on which the systems operate.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.computational tools73readable form and making use of databases of biological data and inferred networks, software based onartificial intelligence research can make complex inferences using these encoded relationships, for example, to consider statements written in that ontology for consistency or to predict new relationshipsbetween elements.34 such new relationships might include new metabolic pathways, regulatory relationships between genes, signaling networks, or other relationships. other approaches rely on logicalframeworks more expressive than database queries and are able to reason about explanations for agiven feature or suggest plans for intervention to reach a desired state.35developing an ontology for automated reasoning can make use of many different sources. forexample, inference from geneexpression data using bayesian networks can take advantage of onlinesources of information about the likely probabilistic dependencies among expression levels of variousgenes.36 machinereadable knowledge bases can be built from textbooks, review articles, or even theoxford dictionary of molecular biology. the rapidly growing volume of publications in the biologicalliterature is another important source, because inclusion of the knowledge in these publications helps touncover relationships among various genes, proteins, and other biological entities referenced in theliterature.an example of ontologies for automated reasoning is the ontology underlying the ecocyc database.the ecocyc pathway database (http://ecocyc.org) describes the metabolic transport, and genetic regulatory networks of e. coli. ecocyc structures a scientific theory about e. coli within a formal ontology sothat the theory is available for computational analysis.37 specifically, ecocyc describes the genes andproteins of e. coli as well as its metabolic pathways, transport functions, and gene regulation. theunderlying ontology encodes a diverse array of biochemical processes, including enzymatic reactionsinvolving small molecule substrates and macromolecular substrates, signal transduction processes,transport events, and mechanisms of regulation of gene expression.384.2.9 annotations and metadataannotation is auxiliary information associated with primary information contained in a database.consider, for example, the human genome database. the primary database consists of a sequence ofsome 3 billion nucleotides, which contains genes, regulatory elements, and other material whose function is unknown. to make sense of this enormous sequence, the identification of significant patternswithin it is necessary. various pieces of the genome must be identified, and a given sequence might beannotated as translation (e.g., òstopó), transcription (e.g., òexonó or òintronó), variation (òinsertionó),structural (òcloneó), similarity, repeat, or experimental (e.g., òknockout,ó òtransgenicó). identifying aparticular nucleotide sequence as a gene would itself be an annotation, and the protein correspondingto it, including its threedimensional structure characterized as a set of coordinates of the proteinõsatoms, would also be an annotation. in short, the sequence database includes the raw sequence data,and the annotated version adds pertinent information such as gene coded for, amino acid sequence, orother commentary to the database entry of raw sequence of dna bases.3934p.d. karp, òpathway databases: a case study in computational symbolic theories,ó science 293(5537):20402044, 2001.35c. baral, k. chancellor, n. tran, n.l. tran, a. joy, and m. berens, òa knowledge based approach for representing andreasoning about signaling networks,ó bioinformatics 20(suppl. 1):i15i22, 2004.36e. segal, b. taskar, a. gasch, n. friedman, and d. koller, òrich probabilistic models for gene expression,ó bioinformatics17(supp. 1):s243s252, 2001. (cited in hunter, òontologies for programs, not people,ó 2002, footnote 32.)37p.d. karp, òpathway databases: a case study in computational symbolic theories,ó science 293(5537):20402044, 2001; p.d.karp, m. riley, m. saier, i.t. paulsen, j. colladovides, s.m. paley, a pellegrinitoole, et al., òthe ecocyc database,ó nucleicacids research 30(1):5658, 2002.38p.d. karp, òan ontology for biological function based on molecular interactions,ó bioinformatics 16(3):269ð285, 2000.39see http://www.biochem.northwestern.edu/holmgren/glossary/definitions/defa/annotation.html.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.74catalyzing inquiryalthough the genomic research community uses annotation to refer to auxiliary information thathas biological function or significance, annotation could also be used as a way to trace the provenanceof data (discussed in greater detail in section 3.7). for example, in a protein database, the utility of anentry describing the threedimensional structure of a protein would be greatly enhanced if entries alsoincluded annotations that described the quality of data (e.g., their precision), uncertainties in the data,the physical and chemical properties of the protein, various kinds of functional information (e.g., whatmolecules bind to the protein, location of the active site), contextual information such as where in a cellthe protein is found and in what concentration, and appropriate references to the literature.in principle, annotations can often be captured as unstructured natural language text. but formaximum utility, machinereadable annotations are necessary. thus, special attention must be paid tothe design and creation of languages and formats that facilitate machine processing of annotations. tofacilitate such processing, a variety of metadata tools are available. metadatañor literally òdata aboutdataóñare anything that describes data elements or data collections, such as the labels of the fields, theunits used, the time the data were collected, the size of the collection, and so forth. they are invaluablenot only for increasing the life span of data (by making it easier or even possible to determine themeaning of a particular measurement), but also for making datasets comprehensible to computers. thenational biological information infrastructure (nbii)40 offers the following description:metadata records preserve the usefulness of data over time by detailing methods for data collection anddata set creation. metadata greatly minimize duplication of effort in the collection of expensive digitaldata and foster sharing of digital data resources. metadata supports local data asset management such aslocal inventory and data catalogs, and external user communities such as clearinghouses and websites. itprovides adequate guidance for enduse application of data such as detailed lineage and context. metadata makes it possible for data users to search, retrieve, and evaluate data set information from the nbiiõsvast network of biological databases by providing standardized descriptions of geospatial and biologicaldata.a popular tool for the implementation of controlled metadata vocabularies is the extensible markuplanguage (xml).41 xml offers a way to serve and describe data in a uniform and automatically parsableformat and provides an opensource solution for moving data between programs. although xml is alanguage for describing data, the descriptions of data are articulated in xmlbased vocabularies.such vocabularies are useful for describing specific biological entities along with experimentalinformation associated with those entities. some of the vocabularies have been developed in associationwith specialized databases established by the community. because of their common basis in xml,however, one vocabulary can be translated to another using various tools, for example, the xml stylesheet language transformation, or xslt.42examples of such xmlbased dialects include the biopolymer markup language (bioml),43 designed for annotating the sequences of biopolymers (e.g., genes, proteins), in such a way that all information about a biopolymer can be logically and meaningfully associated with it. much like html, thelanguage uses tags such as <protein>, <subunit>, and <peptide> to describe elements of a biopolymeralong with a series of attributes.the microarray markup language (maml) was created by a coalition of developers(www.beahmish.lbl.gov) to meet community needs for sharing and comparing the results of geneexpression experiments. that community proposed the creation of a microarray gene expression database and defined the minimum information about a microarray experiment (miame) needed to enable40see http://www.nbii.gov/datainfo/metadata/.41h. simon, modern drug discovery, american chemical society, washington, dc, 2001, pp. 6971.42see http://www.w3c./tr/xslt.43see http://www.bioml.com/bioml.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.computational tools75sharing. consistent with the miame standards proposed by microarray users, maml can be used todescribe experiments and results from all types of dna arrays.the systems biology markup language, (sbml) is used to represent and model information insystems simulation software, so that models of biological systems can be exchanged by different software programs (e.g., ecell, stochsim). the sbml language, developed by the caltech erato kirantosystems biology project,44 is organized around five categories of information: model, compartment,geometry, specie, and reaction.a downside of xml is that only a few of the largest and most used databases (e.g., a genbank)support an xml interface. other databases whose existence predates xml keep most of their data inflat files. but this reality is changing, and database researchers are working to create conversion toolsand new database platforms based on xml. additional xmlbased vocabularies and translation toolsare needed.the data annotation process is complex and cumbersome when large datasets are involved, andsome efforts have been made to reduce the burden of annotation. for example, the distributed annotation system (das) is a web service for exchanging genome annotation data from a number of distributed databases. the system depends on the existence of a òreference sequenceó and gathers òlayersó ofannotation about the sequence that reside on thirdparty servers and are controlled by each annotationprovider. the data exchange standard (the das xml specification) enables layers to be provided in realtime from the thirdparty servers and overlaid to produce a single integrated view by a das client.success in the effort depends on the willingness of investigators to contribute annotation informationrecorded on their respective servers, and on usersõ learning about the existence of a das server (e.g.,through ad hoc mechanisms such as link lists). das is also more or less specific to sequence annotationand is not easily extended to other biological objects.today, when biologists archive a newly discovered gene sequence in genbank, for example, theyhave various types of annotation software at their disposal to link it with explanatory data. nextgeneration annotation systems will have to do this for many other genome features, such as transcriptionfactor binding sites and single nucleotide polymorphisms (snps), that most of todayõs systemsdonõt cover at all. indeed, these systems will have to be able to create, annotate, and archive models ofentire metabolic, signaling, and genetic pathways. nextgeneration annotation systems will have to bebuilt in a highly modular and open fashion, so that they can accommodate new capabilities and newdata types without anyoneõs having to rewrite the basic code.4.2.10 a case study: the cell centered database45to illustrate the notions described above, it is helpful to consider an example of a database effortthat implements many of them. techniques such as electron tomography are generating large amountsof exquisitely detailed data on cells and their macromolecular organization that have to be exposed tothe greater scientific community. however, very few structured data repositories for community useexist for the type of cellular and subcellular information produced using light and electron microscopy.the cell centered database (ccdb) addresses this need by developing a database for threedimensional light and electron microscopic information.4644see http://www.cds.caltech.edu/erato.45section 4.2.10 is adapted largely from m.e. martone, s.t. peltier, and m.h. ellisman, òbuilding grid based resources forneurosciences,ó national center for microscopy and imaging research, department of neurosciences, university of california,san diego, unpublished and undated working paper.46m.e. martone, a. gupta, m. wong, x. qian, g. sosinsky, b. ludascher, and m.h. ellisman, òa cellcentered database forelectron tomographic data,ó journal of structural biology 138(12):145155, 2002; m.e. martone, s. zhang, s. gupta, x. qian, h.he, d.a. price, m. wong, et al., òthe cell centered database: a database for multiscale structural and protein localization datafrom light and electron microscopy,ó neuroinformatics 1(4):379396, 2003.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.76catalyzing inquirythe ccdb contains structural and protein distribution information derived from confocal, multiphoton, and electron microscopy, including correlated microscopy. its main mission is to provide ameans to make highresolution data derived from electron tomography and highresolution light microscopy available to the scientific community, situating itself between whole brain imaging databasessuch as the map project47 and protein structures determined from electron microscopy, nuclear magnetic resonance (nmr) spectroscopy, and xray crystallography (e.g., the protein data bank and embl).the ccdb serves as a research prototype for investigating new methods of representing imagingdata in a relational database system so that powerful datamining approaches can be employed for thecontent of imaging data. the ccdb data model addresses the practical problem of image managementfor the large amounts of imaging data and associated metadata generated in a modern microscopylaboratory. in addition, the data model has to ensure that data within the ccdb can be related to datataken at different scales and modalities.the data model of the ccdb was designed around the process of threedimensional reconstructionfrom twodimensional micrographs, capturing key steps in the process from experiment to analysis.(figure 4.1 illustrates the schemaentity relationship for the ccdb.) the types of imaging data stored inthe ccdb are quite heterogeneous, ranging from largescale maps of protein distributions taken byconfocal microscopy to threedimensional reconstruction of individual cells, subcellular structures, andorganelles. the ccdb can accommodate data from tissues and cultured cells regardless of tissue oforigin, but because of the emphasis on the nervous system, the data model contains several featuresspecialized for neural data. for each dataset, the ccdb stores not only the original images and threedimensional reconstruction, but also any analysis products derived from these data, including segmented objects and measurements of quantities such as surface area, volume, length, and diameter.users have access to the full resolution imaging data for any type of data, (e.g., raw data, threedimensional reconstruction, segmented volumes), available for a particular dataset.for example, a threedimensional reconstruction is viewed as one interpretation of a set of raw datathat is highly dependent on the specimen preparation and imaging methods used to acquire it. thus, asingle record in the ccdb consists of a set of raw microscope images and any volumes, images, or dataderived from it, along with a rich set of methodological details. these derived products include reconstructions, animations, correlated volumes, and the results of any segmentation or analysis performedon the data. by presenting all of the raw data, as well as reconstructed and processed data with athorough description of how the specimen was prepared and imaged, researchers are free to extractadditional content from micrographs that may not have been analyzed by the original author or employadditional alignment, reconstruction, or segmentation algorithms to the data.the utility of image databases depends on the ability to query them on the basis of descriptiveattributes and on their contents. of these two types of query, querying images on the basis of theircontents is by far the most challenging. although the development of computer algorithms to identifyand extract image features in image data is advancing,48 it is unlikely that any algorithm will be able tomatch the skill of an experienced microscopist for many years.the ccdb project addresses this problem in two ways. one currently supported way is to store theresults of segmentations and analyses performed by individual researchers on the data sets stored in theccdb. the ccdb allows each object segmented from a reconstruction to be stored as a separate objectin the database along with any quantitative information derived from it. the list of segmented objectsand their morphometric quantities provides a means to query a dataset based on features contained inthe data such as object name (e.g., dendritic spine) or quantities such as surface area, volume, andlength.47a. mackenziegraham, e.s. jones, d.w. shattuck, i. dinov, m. bota, and a.w. toga, òthe informatics of a c57bl/6 mousebrain atlas,ó neuroinformatics 1(4):397410, 2003.48u. sinha, a. bui, r. taira, j. dionisio, c. morioka, d. johnson, and h. kangarloo, òa review of medical imaging informatics,óannals of the new york academy of sciences 980:168197, 2002.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.computational tools77it is also desirable to exploit information in the database that is not explicitly represented in theschema.49 thus, the ccdb project team is developing specific data types around certain classes of segmented objects contained in the ccdb. for example, the creation of a òsurface data typeó will enable usersto query the original surface data directly. the properties of the surfaces can be determined through verygeneral operations at query time that allow the user to query on characteristics not explicitly modeled inthe schema (e.g., dendrites from striatal medium spiny cells where the diameter of the dendritic shaftshows constrictions of at least 20 percent along its length). in this example, the schema does not containexplicit indication of the shape of the dendritic shaft, but these characteristics can be computed as part ofthe query processing. additional data types are being developed for volume data and protein distributiondata. a data type for tree structures generated by neurolucida has recently been implemented.the ccdb is being designed to participate in a larger, collaborative virtual data federation. thus, anapproach to reconciling semantic differences between various databases must be found.50 scientificprojectexperimentsubjecttissueproducttissueprocessingfixationproteinlocalizationstainingembeddingmicroscopyproductmicroscopyimage detailsanatomicaldetailsregion ofinterestreconstructionreconstructionimage detailssegmentationtree trackingmorphometricsfigure 4.1the schema and entity relationship in the cell centered database.source: see http://ncmir.ucsd.edu/ccdb.49z. lacroix, òissues to address while designing a biological information system,ó pp. 45 in bioinformatics: managing scientific data, z.t. lacroix , ed., morgan kaufmann, san francisco, 2003.50z. lacroix, òissues to address while designing a biological information system,ó pp. 45 in bioinformatics: managing scientific data, 2003.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.78catalyzing inquiryterminology, particularly neuroanatomical nomenclature, is vast, nonstandard, and confusing. anatomical entities may have multiple names (e.g., caudate nucleus, nucleus caudates), the same term mayhave multiple meanings (e.g., spine [spinal cord] versus spine [dendritic spine]), and worst of all, thesame term may be defined differently by different scientists (e.g., basal ganglia). to minimize semanticconfusion and to situate cellular and subcellular data from the ccdb in a larger context, the ccdb ismapped to several shared knowledge sources in the form of ontologies.concepts in the ccdb are being mapped to the unified medical language system (umls), a largemetathesaurus and knowledge source for the biomedical sciences.51 the umls assigns each concept inthe ontology a unique identifier (id); thus, all synonymous terms can then be assigned the same id. forexample, the umls id number for the synonymous terms purkinje cell, cerebellar purkinje cell, andpurkinjeõs corpuscle is c0034143. thus, regardless of which term is preferred by a given individual, ifthey share the same id, they are asserted to be the same. conversely, even if two terms share the samename, they are distinguishable by their unique ids. in the example given above, spine (spinal cord) =c0037949, whereas spine (dendritic spine) = c0872341.in addition, an ontology can support the linkage of concepts by a set of relationships. these relationships may be simple òis aó and òhas aó relationships (e.g., purkinje cell is a neuron, neuron has anucleus), or they may be more complex.52 from the above statements, a search algorithm could inferthat òpurkinje cell has a nucleusó if the ontology is encoded in a form that would allow such reasoningto be performed. because the knowledge required to link concepts is contained outside of the sourcedatabase, the ccdb is relieved of the burden of storing exhaustive taxonomies for individual datasets,which may become obsolete as new knowledge is discovered.the umls has recently incorporated the neuronames ontology53 as a source vocabulary.neuronames is a comprehensive resource for gross brain anatomy in the primate. however, for thetype of cellular and subcellular data contained in the ccdb, the umls does not contain sufficientdetail. ontologies for areas such as neurocytology and neurological disease are being built on top of theumls, utilizing existing concepts wherever possible and constructing new semantic networks andconcepts as needed.54in addition, imaging data in the ccdb is mapped to a higher level of brain organization by registering their location in the coordinate system of a standard brain atlas. placing data into an atlasbasedcoordinate systems provides one method by which data taken across scales and distributed acrossmultiple resources can reliably be compared.55through the use of computerbased atlases and associated tools for warping and registration, it ispossible to express the location of anatomical features or signals in terms of a standardized coordinatesystem. while there may be disagreement among neuroscientists about the identity of a brain areagiving rise to a signal, its location in terms of spatial coordinates is at least quantifiable. the expressionof brain data in terms of atlas coordinates also allows them to be transformed spatially to offer alternative views that may provide additional information (such as flat maps or additional parcellation51b.l. humphreys, d.a. lindberg, h.m. schoolman, and g.o. barnett, òthe unified medical language system: an informaticsresearch collaboration,ó journal of the american medical informatics association 5(1):111, 1998.52a. gupta, b. ludascher, j.s. grethe, and m.e. martone, òtowards a formalization of a disease specific ontology forneuroinformatics,ó neural networks 16(9):12771292, 2003.53d.m. bowden and m.f. dubach, òneuronames 2002,ó neuroinformatics 1:4359, 2002.54a. gupta, b. ludascher, j.s. grethe, and m.e. martone, òtowards a formalization of a disease specific ontology forneuroinformatics,ó neural networks 6(9):12771292, 2003.55a. brevik, t.b. leergaard m. svanevik, j.g. bjaalie, òthreedimensional computerised atlas of the rat brain stemprecerebellar system: approaches for mapping, visualization, and comparison of spatial distribution data,ó anatomy andembryology 204(4):319332, 2001; j.g. bjaalie, òopinion: localization in the brain: new solutions emerging,ó nature reviews:neuroscience 3(4):322325, 2003; d.c. van essen, h.a. drury, j. dickson, j. harwell, d. hanlon, and c.h. anderson, òan integrated software suite for surfacebased analyses of cerebral cortex,ó journal of the american medical informatics association8(5):443459, 2001; d.c. van essen, òwindows on the brain: the emerging role of atlases and databases in neuroscience,ócurrent opinion in neurobiology 12(5):574579, 2002.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.computational tools79schemes).56 finally, because individual experiments can study only a few aspects of a brain region atone time, a standard coordinate system allows the same brain region to be sampled repeatedly to allowdata to be accumulated over time.4.2.11 a case study: ecological and evolutionary databasesalthough genomic databases such as genbank receive the majority of attention, databases andalgorithms that operate on databases are key tools in research into ecology and biodiversity as well.these tools can provide researchers with access to information regarding all identified species of a giventype, such as algaebase57 or fishbase;58 they also serve as a repository for submission of new information and research. other databases go beyond species listings to record individuals: for example, theornis database of birds seeks to provide access to nearly 5 million individual specimens held innatural history collections, which includes data such as recordings of vocalizations and egg and nestholdings.59the data associated with ecological research are gathered from a wide variety of sources: physicalobservations in the wild by both amateurs and professionals; fossils; natural history collections; zoos,botanical gardens, and other living collections; laboratories; and so forth. in addition, these data mustplaced into contexts of time, geographic location, environment, current and historical weather andclimate, and local, regional, and global human activity. needless to say, these data sources are scatteredthroughout many hundreds or thousands of different locations and formats, even when they are indigitally accessible format. however, the need for integrated ecological databases is great: only by beingable to integrate the totality of observations of population and environment can certain key questions beanswered. such a facility is central to endangered species preservation, invasive species monitoring,wildlife disease monitoring and intervention, agricultural planning, and fisheries management, in addition to fundamental questions of ecological science.the first challenge in building such a facility is to make the individual datasets accessible bynetworked query. over the years, hundreds of millions of specimens have been recorded in museumrecords. in many cases, however, the data are not even entered into a computer; they may be stored asa set of index cards dating from the 1800s. natural history collections, such as a museumõs collection offossils, may not even be indexed, and they are available to researchers only by physically inspecting thedrawers. very few specimens have been geocoded.museum records carry a wealth of image and text data, and digitizing these records in a meaningful and useful way remains a serious challenge. for this reason, funding agencies such as thenational science foundation (nsf) are emphasizing integrating database creation, curation, andsharing into the process of ecological science: for example, the nsf biological databases andinformatics program60 (which includes research into database algorithms and structures, as well asdeveloping particular databases) and the biological research collections program, which providesaround $6 million per year for computerizing existing biological data. similarly, the nsf partnerships for enhancing expertise in taxonomy (peet) program,61 which emphasizes training in taxonomy, requires that recipients of funding incorporate collected data into databases or other sharedelectronic formats.56d.c. van essen, òwindows on the brain: the emerging role of atlases and databases in neuroscience,ó current opinion inneurobiology 12:574579, 2002.57see http://www.algaebase.org.58see http://www.fishbase.org.59see http://www.ornisnet.org.60nsf program announcement nsf 02058; see http://www.nsf.gov/pubsys/ods/getpub.cfm?nsf02058.61see http://web.nhm.ku.edu/peet/.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.80catalyzing inquiryecological databases also rely on metadata to improve interoperability and compatibility amongdisparate data collections.62 ecology is a field that demands access to large numbers of independentdatasets such as geographic information, weather and climate records, biological specimen collections,population studies, and genetic data. these datasets are collected over long periods of time, possiblydecades or even centuries, by a diverse set of actors for different purposes. a commonly agreeduponformat and vocabulary for metadata is essential for efficient cooperative access.furthermore, as data increasingly are collected by automated systems such as embedded systemsand distributed sensor networks, the applications that attempt to fuse the results into formats amenableto algorithmic or human analysis must deal with high (and always on) data rates, likely contained inshifting standards for representation. again, early agreement on a basic system for sharing metadatawill be necessary for the feasibility of such applications.in attempting to integrate or crossquery these data collections, a central issue is the naming ofspecies or higherlevel taxa. the linnean taxonomy is the oldest such effort in biology, of course, yetbecause there is not yet (nor likely can ever be) complete agreement on taxa identification, entries indifferent databases may contain different tags for members of the same species, or the same tag formembers that were later determined to be of different species. taxa are often moved into differentgroups, split, or merged with others; names are sometimes changed. a central effort to manage this isthe integrated taxonomic information system (itis),63 which began life as a u.s. interagency task force,but today is a global cooperative effort between government agencies and researchers to arrive at arepository for agreedupon species names and taxonomic categorization. itis data are of varying quality, and entries are tagged with three different quality indicators: credibility, which indicates whether ornot data have been reviewed; latest review, giving the year of the last review; and global completeness,which records whether all species belonging to a taxon were included at the last review. these measurements allow researchers to evaluate whether the data are appropriate for their use.in constructing such a database, many data standards questions arise. for example, itis uses namingstandards from the international code of botanical nomenclature and the international code of zoological nomenclature. however, for the kingdom protista, which at various times in biological science hasbeen considered more like an animal and more like a plant, both standards might apply. dates and dateranges provide another challenge: while there are many international standards for representing a calendar date, in general these did not foresee the need to represent dates occurring millions or billions of yearsago. itis employs a representation for geologic ages, and this illustrates the type of challenge encounteredwhen stretching a set of data standards to encompass many data types and different methods of collection.for issues of representing observations or collections, an important element is the darwin core, aset of xml metadata standards for describing a biological specimen, including observations in the wildand preserved items in natural history collections. where itis attempts to improve communicability byachieving agreement on precise name usage, darwin core64 (and similar metadata efforts) concentratesthe effort on labeling and markup of data. this allows individual databases to use their own datastructures, formats, and representations, as long as the data elements are labeled by darwin corekeywords. since the design demands on such databases will be substantially different, this is a usefulapproach. another attempt to standardize metadata for ecological data is the access to biologicalcollections data (abcd) schema,65 which is richer and contains more information. these two approaches indicate a common strategic choice: simpler standards are easier to adopt, and thus will likelybe more widespread, but are limited in their expressiveness; more complex standards can successfully62for a more extended discussion of the issues involved in maintaining ecological data, see w.k. michener and j.w. brunt,eds., ecological data: design, management and processing, methods in ecology, blackwell science, maryland, 2000. a useful onlinepresentation can be found at http://www.soest.hawaii.edu/pfrp/dec03mtg/michener.pdf.63see http://www.itis.usda.gov.64see http://speciesanalyst.net/docs/dwc/.65see http://www.bgbm.org/tdwg/codata/schema/default.htm.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.computational tools81support a wider variety of queries and data types, but may be slower to gain adoption. another effort toaccomplish agreement on data and metadata standards is the national biological information initiative(nbii), a program of the u.s. geological surveyõs center for biological informatics.agreement on standard terminology and data labeling would accomplish little if the data sourceswere unknown. the most significant challenge in creating largescale ecological information is theintegration and federation of the potentially vast number of relevant databases. the global biodiversityinformation facility (gbif)66 is an attempt to offer a singlequery interface to cooperating data providers; in december of 2004, it consisted of 95 providers totaling many tens of millions of individualrecords. gbif accomplishes this query access through the use of data standards (such as the darwincore) and web services, an information technology (it) industry standard way of requesting information from servers in a platformindependent fashion. a similar international effort is found at theclearinghouse mechanism (chm),67 an instrumentality of the convention on biodiversity. the chm isintended as a way for information on biodiversity to be shared among signatory states and madeavailable as a way to monitor compliance and as a tool for policy.globally integrated ecological databases are still in embryonic form, but as more data becomedigitized and made available by the internet in standard fashions, their value will increase. integrationwith phylogenetic and molecular databases will add to their value as research tools, in both the ecological and the evolutionary fields.4.3 data presentation4.3.1 graphical interfacesbiological processes can take place over a vast array of spatial scales, from the nanoscale inhabitedby individual molecules, to the everyday, metersized human world. they can take place over an evenvaster range of time scales, from the nanosecond gyrations of a folding protein molecule to the sevendecade (or so) span of a human lifeñand far beyond, if evolutionary time is included. they also can beconsidered at many levels of organization, from the straightforward realm of chemical interaction to theabstract realm of, say, signal transduction and information processing.much of 21st century biology must deal with these processes at every level and at every scale,resulting in data of high dimensionality. thus, the need arises for systems that can offer vivid and easilyunderstood visual metaphors to display the information at each level, showing the appropriate amountof detail. (such a display would be analogous to, say, a circuit diagram, with its widely recognized iconsfor diodes, transistors, and other such components.) a key element of such systems is easily understoodmetaphors that present signals containing multiple colors over time on more than one axis. as anempirical matter, these metaphors are hard to find. indeed, the problem of finding a visually (orintellectually!) optimal display layout for highdimensional data is arguably combinatorially hard,because in the absence of a welldeveloped theory of display, it requires exploring every possiblecombination of data in a multitude of arrangements.the system would likewise offer easy and intuitive ways to navigate between levels, so that the usercould drill down to get more detail or pop up to higher abstractions as needed. also, it would offer goodways to visualize the dynamical behavior of the system over timeñwhatever the appropriate time scalemight be. currentgeneration visualization systems such as those associated with biospice68 andcytoscape69 are a good beginningñbut, as their developers themselves are the first to admit, only abeginning.66see http://www.gbif.org/.67see http://www.biodiv.org/chm/default.aspx.68see http://biospice.lbl.gov/home.html.69see http://www.cytoscape.org/.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.82catalyzing inquirybiologists use a variety of different data representations to help describe, examine, and understanddata. biologists often use cartoons as conceptual, descriptive models of biological events or processes. acartoon might show a time line of events: for example, the time line of the phosphorylation of a receptorthat allows a protein to bind to it. as biologists take into account the simultaneous interactions of largernumbers of molecules, events over time become more difficult to represent in cartoons. new ways toòseeó interactions and associations are therefore needed in life sciences research.the most complex data visualizations are likely to be representations of networks. the completegraph in figure 4.2 contains 4,543 nodes of approximately 6,000 proteins encoded by the yeast genome,along with 12,843 interactions. the graph was developed using the osprey network visualization system.figure 4.2from genomics to proteomics. visualization of combined, largescale interaction data sets in yeast. atotal of 14,000 physical interactions obtained from the grid database were represented with the osprey networkvisualization system (see http://biodata.mshri.on.ca/grid). each edge in the graph represents an interaction between nodes, which are colored according to gene ontology (go) functional annotation. highly connected complexes within the dataset, shown at the perimeter of the central mass, are built from nodes that share at least threeinteractions within other complex members. the complete graph contains 4,543 nodes of ~6,000 proteins encodedby the yeast genome, 12,843 interactions and an average connectivity of 2.82 per node. the 20 highly connectedcomplexes contain 340 genes, 1,835 connections, and an average connectivity of 5.39.source: reprinted by permission from m. tyers and m. mann, òfrom genomics to proteomics,ó nature 422:193197, 2003. copyright 2003 macmillan magazines ltd.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.computational tools83other diagrammatic simulations of complex cell networks use tools such as the diagrammatic celllanguage (dcl) and visual cell. these software tools are designed to read, query, and edit cell pathways, and to visualize data in a pathway context. visual cell creates detailed drawings by compactlyformatting thousands of molecular interactions. the software uses dcl, which can visualize and simulate largescale networks such as interconnected signal transduction pathways and the gene expressionnetworks that control cell proliferation and apoptosis. dcl can visualize millions of chemical states andchemical reactions.a second approach to diagrammatic simulation has been developed by efroni et al.70 these researchers use the visual language of statecharts, which makes specification of the simulation precise,legible, and machineexecutable. behavior in statecharts is described by using states and events thatcause transitions between states. states may contain substates, thus enabling description at multiplelevels and zooming in and zooming out between levels. states may also be divided into orthogonalstates, thus modeling concurrency, allowing the system to reside simultaneously in several differentstates. a cell, for example, may be described orthogonally as expressing several receptors, no receptors,or any combination of receptors at different stages of the cell cycle and in different anatomical compartments. furthermore, transitions take the system from one state to another. in cell modeling, transitionsare the result of biological processes or the result of user intervention. a biological process may be theresult of an interaction between two cells or between a cell and various molecules. statecharts providea controllable way to handle the enormous dataset of cell behavior by enabling the separation of thatdataset into orthogonal states and allowing transitions.still another kind of graphical interface is used for molecular visualization. interesting biomoleculesusually consist of thousands of atoms. a list of atomic coordinates is useful for some purposes, but anactual image of the molecule can often provide much more insight into its propertiesñand an imagethat can be manipulated (e.g., viewed from different angles) is even more useful. virtual reality techniques can be used to provide the viewer with a large field of view, and to enable the viewer to interactwith the virtual molecule and compare it to other molecules. however, many problems in biomolecularvisualization tax the capability of current systems because of the diversity of operations required andbecause many operations do not fit neatly into the current architectural paradigm.4.3.2 tangible physical interfacesas useful as graphical visualizations are, even in simulated threedimensional virtual realitythey are still twodimensional. tangible, physical models that a human being can manipulate directly with his or her hands are an extension of the twodimensional graphical environment. aproject at the molecular graphics laboratory at the scripps research institute is developing tangible interfaces for molecular biology.71 these interfaces use computerdriven autofabrication technology (i.e., threedimensional printers) and result in physical molecular representations that onecan hold in oneõs hand.these efforts have required the development and testing of software for the representation ofphysical molecular models to be built by autofabrication technologies, linkages between moleculardescriptions and computeraided design and manufacture approaches for enhancing the models withadditional physical characteristics, and integration of the physical molecular models into augmentedreality interfaces as inputs to control computer display and interaction.70s. efroni, d. harel, and i.r. cohen, òtoward rigorous comprehension of biological complexity: modeling, execution, andvisualization of thymic tcell maturation,ó genome research 13(11):24852497, 2003.71a. gillet, m. sanner, d. stoffler, d. goodsell, and a. olson, òaugmented reality with tangible autofabricated models formolecular biology applications,ó proceedings of the ieee visualization 2004 (visõ04), october 1015, 2004, austin, pp. 235242.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.84catalyzing inquiry4.3.3 automated literature searching72still another form of data presentation is journal publication. it has not been lost on the scientificbioinformatics community that vast amounts of functional information that could be used to annotategene and protein sequences are embedded in the written literature. rice and stolovitzky go so far as tosay that mining the literature on biomolecular interactions can assist in populating a network model ofintracellular interaction (box 4.4).73so far, however, the availability of fulltext articles in digital formats such as pdf, html, or tiffiles has limited the possibilities for computer searching and retrieval of full text in databases. in thefuture, wider use of structured documents tagged with xml will make intelligent searching of full textfeasible, fast, and informative and will allow readers to locate, retrieve, and manipulate specific parts ofa publication.in the meantime, however, natural language provides a considerable, though not insurmountable,challenge for algorithms to extract meaningful information from natural text. one common applicationof natural language processing involves the extraction from the published literature of informationabout proteins, drugs, and other molecules. for example, fukuda et al. (1998) pioneered identificationof protein names using properties of the text such as the occurrence of uppercase letters, numerals, andspecial endings to pinpoint protein names.74other work has investigated the feasibility of recognizing interactions between proteins and othermolecules. one approach is based on simultaneous occurrences of gene names and their use to predicttheir connections based on their occurrence statistics.75 a second approach to pathway discovery was72the discussion in section 4.3.3 is based on excerpts from l. hirschman, j.c. park, j. tsujii, l. wong, and c.h. wu, òaccomplishments and challenges in literature data mining for biology,ó bioinformatics review 18(12):15531561, 2002. available athttp://pir.georgetown.edu/pirwww/aboutpir/doc/datamining.pdf.73j.j. rice and g. stolovitzky, òmaking the most of it: pathway reconstruction and integrative simulation using the data athand,ó biosilico 2(2):7077, 2004.74k. fukuda, et al., òtoward information extraction: identifying protein names from biological papers,ó pacific symposium onbiocomputing 1998, 707718. (cited in hirschman et al., 2002.)75b. stapley and g. benoit, òbiobibliometrics: information retrieval and visualization from cooccurrences of gene names inmedline abstracts,ó pacific symposium on biocomputing 2000, 529540; j. ding et al., òmining medline: abstracts, sentences,or phrases?ó pacific symposium on biocomputing 2002, 326337. (cited in hirschman et al., 2002.)box 4.4text mining and populating a network model of intracellular interactionother methods [for the construction of largescale topological maps of cellular networks] have sought to minemedline/pubmed abstracts that are considered to contain concise records of peerreviewed published results. thesimplest methods, often called ôguilt by association,õ seek to find cooccurrence of genes or protein names in abstracts or even smaller structures such as sentences or phrases. this approach assumes that cooccurrences areindicative of functional links, although an obvious limitation is that negative relations (e.g., a does not regulate b) arecounted as positive associations. to overcome this problem, other natural language processing methods involvesyntactic parsing of the language in the abstracts to determine the nature of the interactions. there are obviouscomputation costs in these approaches, and the considerable complexity in human language will probably renderany machinebased method imperfect. even with limitations, such methods will probably be required to makeknowledge in the extant literature accessible to machinebased analyses. for example, prebind used support vectormachines to help select abstracts likely to contain useful biomolecular interactions to ôbackfillõ the bind database.source: reprinted by permission from j.j. rice and g. stolovitzky, òmaking the most of it: pathway reconstruction and integrativesimulation using the data at hand,ó biosilico 2(2):7077. copyright 2004 elsevier. (references omitted.)catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.computational tools85based on templates that matched specific linguistic structures to recognize and extract of protein interaction information from medline documents.76 more recent work goes beyond the analysis of singlesentences to look at relations that span multiple sentences through the use of coreference. for example,putejovsky and castano focused on relations of the word inhibit and showed that it was possible toextract biologically important information from free text reliably, using a corpusbased approach todevelop rules specific to a class of predicates.77 hahn et al. described the medsyndikate system foracquiring knowledge from medical reports, a system capable of analyzing coreferring sentences andextracting new concepts given a set of grammatical constructs.78box 4.5 describes a number of other information extraction successes in biology. in a commentary in embo reports on publication mining, les grivell, manager of the european electronicpublishing initiative, ebiosci, sums up the challenges this way:79the detection of gene symbols and names, for instance, remains difficult, as researchers have seldomfollowed logical rules. in some organismsñthe fruit fly drosophila is an exampleñscientists have enjoyedapplying gene names with primary meaning outside the biological domain. names such as vamp, eve,disco, boss, gypsy, zip or ogre are therefore not easily recognized as referring to genes.80also, both synonymy (many different ways to refer to the same object) and polysemy (multiple meanings for a given word) cause problems for search algorithms. synonymy reduces the number of recalls ofa given object, whereas polysemy causes reduced precision. another problem is ambiguities of a wordõssense. the word insulin, for instance, can refer to a gene, a protein, a hormone or a therapeutic agent,depending on the context. in addition, pronouns and definite articles and the use of long, complex ornegative sentences or those in which information is implicit or omitted pose considerable hurdles for fulltext processing algorithms.grivell points out that algorithms exist (e.g., the vector space model) to undertake text analysis,theme generation, and summarization of computerreadable texts, but adds that òapart from the considerable computational resources required to index terms and to precompute statistical relationships forseveral million articles,ó an obstacle to fulltext analysis is the fact that scientific journals are owned bya large number of different publishers, so computational analysis will have to be distributed acrossmultiple locations.76s.k. ng and m. wong, òtoward routine automatic pathway discovery from online scientific text abstracts,ó genomeinformatics 10:104112, 1999. (cited in hirschman et al., 2002.)77j. putejovsky and j. castano, òrobust relational parsing over biomedical literature: extracting inhibit relations,ó pacificsymposium on biocomputing 2002, 362373. (cited in hirschman et al., 2002.)78u. hahn, et al., òrich knowledge capture from medical documents in the medsyndikate system,ó pacific symposium onbiocomputing 2002, 338349. (cited in hirschman et al., 2002.)79l. grivell, òmining the bibliome: searching for a needle in a haystack? new computing tools are needed to effectivelyscan the growing amount of scientific literature for useful information,ó embo report 3(3):200203, 2002.80d. proux, f. rechenmann, l. julliard, v. pillet. and b. jacq, òdetecting gene symbols and names in biological texts: a firststep toward pertinent information extraction,ó genome informatics 9:7280, 1999. (cited in grivell, 2002.) note also that whilegene names are often italicized in print (so that they are more readily recognized as genes), neither verbal discourse nor textsearch recognizes italicization. in addition, because some changes of name are made for political rather than scientific reasons,and because these political revisions are done quietly, even identifying the need for synonym tracking can be problematic. anexample is a gene mutation, discovered in 1963, that caused male fruit flies to court other males. over time, the assigned genename of òfruityó came to be regarded as offensive, and eventually the genes name was changed to òfruitlessó after much publicdisapproval. a similar situation arose more recently, when scientists at princeton university found mutations in flies that causedthem to be learning defective or, in the vernacular of the investigators, òvegged out.ó they assigned names such as cabbage,rutabaga, radish, and turnipñwhich some other scientists found objectionable. see, for example, m. vacek, òa gene by anyother name,ó american scientist 89(6), 2001.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.86catalyzing inquirybox 4.5selected information extraction successes in biologybesides the recognition of protein interactions from scientific text, natural language processing has been applied toa broad range of information extraction problems in biology.capturing of specific relations in databases.. . . we begin with systems that capture specific relations in databases. hahn et al. (2002) used natural languagetechniques and nomenclatures of the unified medical language system (umls) to learn ontological relations for amedical domain. baclawski et al. (2000) is a diagrammatic knowledge representation method called keynets. theumls ontology was used to build keynets.using both domainindependent and domainspecific knowledge, keynets parsed texts and resolved references tobuild relationships between entities. humphreys et al. (2000) described two information extraction applications inbiology based on templates: empathie extracted from journal articles details of enzyme and metabolic pathways;pasta extracted the roles of amino acids and active sites in protein molecules. this work illustrated the importanceof template matching, and applied the technique to terminology recognition. rindflesch et al. (2000) describededgar, a system that extracted relationships between cancerrelated drugs and genes from biomedical literature.edgar drew on a stochastic partofspeech tagger, a syntactic parser able to produce partial parses, a rulebasedsystem, and semantic information from the umls. the metathesaurus and lexicon in the knowledge base were usedto identify the structure of noun phrases in medline texts. thomas et al. (2000) customized an information extraction system called highlight for the task of gathering data on protein interactions from medline abstracts. theydeveloped and applied templates to every part of the texts and calculated the confidence for each match. theresulting system could provide a costeffective means for populating a database of protein interactions.information retrieval and clustering.the next papers [in this volume] focus on improving retrieval and clustering in searching large collections. chang etal. (2001) modified psiblast to use literature similarity in each iteration of its search. they showed that supplementing sequence similarity with information from biomedical literature search could increase the accuracy ofhomology search result. illiopoulos et al. (2001) gave a method for clustering medline abstracts based on a statistical treatment of terms, together with stemming, a ògolist,ó and unsupervised machine learning. despite the minimal semantic analysis, clusters built here gave a shallow description of the documents and supported conceptdiscovery.wilbur (2002) formalized the idea of a òthemeó in a set of documents as a subset of the documents and a subset ofthe indexing terms so that each element of the latter had a high probability of occurring in all elements of the former.an algorithm was given to produce themes and to cluster documents according to these themes.classification.. . . text processing has been used for classification. stapley et al. (2002) used a support vector machine to classifyterms derived by standard term weighting techniques to predict the cellular location of proteins from description inabstracts. the accuracy of the classifier on a benchmark of proteins with known cellular locations was better thanthat of a support vector machine trained on amino acid composition and was comparable to a handcrafted rulebased classifier (eisenhaber and bork, 1999).source: reprinted by permission from l. hirschman, j.c. park, j. tsujii, l. wong, and c.h. wu, òaccomplishments and challenges inliterature data mining for biology, bioinformatics review 18(12):15531561, 2002, available at http://pir.georgetown.edu/pirwww/aboutpir/doc/datamining.pdf. copyright 2002 oxford university press.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.computational tools874.4 algorithms for operating on biological data4.4.1 preliminaries: dna sequence as a digital string the digital nature of dna is a central evolutionary innovation for many reasonsñthat is, theòvaluesó of the molecules making up the polymer are discrete and indivisible units. just as an electronicdigital computer abstracts various continuous voltage levels as 0 and 1, dna abstracts a threedimensional organization of atoms as a, t, g, and c. this has important biological benefits, including veryhighaccuracy replication, common and simplified ways for associated molecules to bind to sites, andlow ambiguity in coding for proteins.for human purposes in bioinformatics, however, the use of the abstraction of dna as a digitalstring has had other equally significant and related benefits. it is easy to imagine the opposite case, inwhich dna is represented as the threedimensional locations of each atom in the macromolecule, andcomparison of dna sequences is a painstaking process of comparing the full structures. indeed, this isvery much the state of the art in representing proteins (which, although they can be represented as adigital string of peptides, are more flexible than dna, so the digital abstraction leaves out the criticallyimportant features of folding). the digital abstraction includes much of the essential information of thesystem, without including complicating higher and lowerorder biochemical properties.81 the comparison of the state of the art in computational analysis of dna sequences and protein sequences speaksin part to the enormous advantage that the digital string abstraction offers when appropriate.the most basic feature of the abstraction is that it treats the arrangement of physical matter asinformation. an important advantage of this is that informationtheoretic techniques can be applied tospecific dna strings or to the overall alphabet of codonpeptide associations. for example, computersciencedeveloped concepts such as hamming distance, parity, and errorcorrecting codes can be usedto evaluate the resilience of information in the presence of noise and close alternatives.82a second and very practical advantage is that as strings of letters, dna sequences can be storedefficiently and recognizably in the same format as normal text.83 an entire human genome, for example, canbe stored in about 3 gigabytes, costing a few dollars in 2003. more broadly, this means that a vast array oftools, software, algorithms, and software packages that were designed to operate on text could be adaptedwith little or no effort to operate on dna strings as well. more abstract examples include the long history ofresearch into algorithms to efficiently search, compare, and transform strings. for example, in 1974, analgorithm for identifying the òedit distanceó of two strings was discovered,84 measuring the minimumnumber of changes, transpositions, and insertions necessary to transform one string into another. althoughthis algorithm was developed long before the genome era, it is useful to dna analysis nonetheless.85finally, the very foundation of computational theory is the turing machine, an abstract model ofsymbolic manipulation. some very innovative research has shown that the dna manipulations of somesinglecelled organisms are turingcomplete,86 allowing the application of a large tradition of formallanguage analysis to problems of cellular machinery.81a. regev and e. shapiro, òcellular abstractions: cells as computation,ó nature 419(6905): 343, 2002.82d.a. macdonaill, òa parity code interpretation of nucleotide alphabet composition,ó chemical communications 18:20622063, 2002.83ideally, of course, a nucleotide could be stored using only two bits (or three to include rna nucleotides as well). asciitypically uses eight bits to represent characters.84r.a. wagner and m.j. fischer, òthe stringtostring correction problem,ó journal of the association for computing machinery21(1):168173, 1974.85see for example, american mathematical society, òmathematics and the genome: near and far (strings),ó april 2002.available at http://www.ams.org/newinmath/cover/genome5.html; m.s. waterman, introduction to computational biology:maps, sequences and genomes, chapman and hall, london, 1995; m.s. waterman, òsequence alignments,ó mathematical methodsfor dna sequences, crc, boca raton, fl, 1989, pp. 5392.86l.f. landweber and l. kari, òthe evolution of cellular computing: natureõs solution to a computational problem,óbiosystems 52(13):313, 1999.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.88catalyzing inquirythese comments should not be taken to mean that the abstraction of dna into a digital string is costfree. although digital coding of dna is central to the mechanisms of heredity, the nucleotide sequencecannot deal with nondigital effects that also play important roles in protein synthesis and function.proteins do not necessarily bind only to one specific sequence; the overall proportions of at versus cg ina region affect its rate of transcription; and the state of methylation of a region of dna is an importantmechanism for the epigenetic control of gene expression (and can indeed be inherited just as the digitalcode can be inherited).87 there are also numerous posttranslational modifications of proteins by processessuch as acetylation, glycosylation, and phosphorylation, which by definition are not inherent in thegenetic sequence.88 the digital abstraction also cannot accommodate protein dynamics or kinetics. because these nondigital properties can have important effects, ignoring them puts a limit on how far thedigital abstraction can support research related to gene finding and transcription regulation.last, dna is often compared to a computer program that drives the functional behavior of a cell.although this analogy has some merit, it is not altogether accurate. because dna specifies whichproteins the cell must assemble, it is at least one step removed from the actual behavior of a cell, sincethe proteinsñnot the dnañthat determine (or at least have a great influence on) cell behavior.4.4.2 proteins as labeled graphsa significant problem in molecular biology is the challenge of identifying meaningful substructuralsimilarities among proteins. although proteins, like dna, are composed of strings made from a sequence of a comparatively small selection of types of component molecules, unlike dna, proteins canexist in a huge variety of threedimensional shapes. such shapes can include helixes, sheets, and otherforms generally referred to as secondary or tertiary structure.since the structural details of a protein largely determine its functions and characteristics, determining a proteinõs overall shape and identifying meaningful structural details is a critical element of proteinstudies. similar structure may imply similar functionality or receptivity to certain enzymes or othermolecules that operate on specific molecular geometry. however, even for proteins whose threedimensional shape has been experimentally determined through xray crystallography or nuclear magneticresonance, finding similarities can be difficult due to the extremely complex geometries and largeamount of data.a rich and mature area of algorithm research involves the study of graphs, abstract representationsof networks of relationships. a graph consists of a set of nodes and a set of connections between nodescalled òedges.ó in different types of graphs, edges may be oneway (a òdirected graphó) or twoway(òundirectedó), or edges may also have òweightsó representing the distance or cost of the connection.for example, a graph might represent cities as nodes and the highways that connect them as edgesweighted by the distance between the pair of cities.graph theory has been applied profitably to the problem of identifying structural similarities amongproteins.89 in this approach, a graph represents a protein, with each node representing a single aminoacid residue and labeled with the type of residue, and edges representing either peptide bonds or closespatial proximity. recent work in this area has combined graph theory, data mining, and informationtheoretic techniques to efficiently identify such similarities.9087for more on the influence of dna methylation on genetic regulation, see r. jaenisch and a. bird, òepigenetic regulation ofgene expression: how the genome integrates intrinsic and environmental signals,ó nature genetics 33 (suppl):245254, 2003.88indeed, some work even suggests that dna methylation and histone acetylation may be connected. see j.r. dobosy and e.u.selker, òemerging connections between dna methylation and histone acetylation,ó cellular and molecular life sciences 58(56):721727, 2001.89e.m. mitchell, p.j. artymiuk, d.w. rice, and p. willet, òuse of techniques derived from graph theory to compare secondary structure motifs in proteins,ó journal of molecular biology 212(1):151166, 1989.90j. huan, w. wang, a. washington, j. prins, r. shah, and a. tropsha, òaccurate classification of protein structural familiesusing coherent subgraph analysis,ó pacific symposium on biocomputing 2004:411422, 2004.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.computational tools89a significant computational aspect of this example is that since the general problem of identifyingsubgraphs is npcomplete,91 the mere inspiration of using graph theory to represent proteins is insufficient; sophisticated algorithmic research is necessary to develop appropriate techniques, data representations, and heuristics that can sift through the enormous datasets in practical times. similarly, the probleminvolves subtle biological detail (e.g., what distance represents a significant spatial proximity, whichamino acids can be classified together), and could not be usefully attacked by computer scientists alone.4.4.3algorithms and voluminous datasetsalgorithms play an increasingly important role in the process of extracting information from largebiological datasets produced by highthroughput studies. algorithms are needed to search, sort, align,compare, contrast, and manipulate data related to a wide variety of biological problems and in supportof models of biological processes on a variety of spatial and temporal scales. for example, in thelanguage of automated learning and discovery, research is needed to develop algorithms for active andcumulative learning; multitask learning; learning from labeled and unlabeled data; relational learning;learning from large datasets; learning from small datasets; learning with prior knowledge; learningfrom mixedmedia data; and learning causal relationships.92the computational algorithms used for biological applications are likely to be rooted in mathematicaland statistical techniques used widely for other purposes (e.g., bayesian networks, graph theory, principalcomponent analysis, hidden markov models), but their adaptation to biological questions must addressthe constraints that define biological events. because critical features of many biological systems are notknown, algorithms must operate on the basis of working models and must frequently contend with a lackof data and incomplete information about the system under study (though sometimes simulated datasuffices to test an algorithm). thus, the results they provide must be regarded as approximate and provisional, and the performance of algorithms must be tested and validated by empirical laboratory studies.algorithm development, therefore, requires the joint efforts of biologists and computer scientists.sections 4.4.4 through 4.4.9 describe certain biological problems and the algorithmic approaches tosolving them. far from giving a comprehensive description, these sections are intended to illustrate thecomplex substrate on which algorithms must operate and, further, to describe areas of successful andprolific collaboration between computer scientists and biologists.some of the applications described below are focused on identifying or measuring specific attributes, such as the identity of a gene, the threedimensional structure of a protein, or the degree ofgenetic variability in a population. at the heart of these lines of investigation is the quest to understandbiological function, (e.g., how genes interact, the physical actions of proteins, the physiological resultsof genetic differences). further opportunities to address biological questions are likely to be as diverseas biology itself, although work on some of those questions is only nascent at this time.4.4.4 gene recognitionalthough the complete genomic sequences of many organisms have been determined, not all of the geneswithin those genomes have been identified. difficulties in identifying genes from sequences of uncharacterizeddna stem mostly from the complexity of gene organization and architecture. just a small fraction of thegenome of a typical eukaryote consists of exons, that is, blocks of dna that, when arranged according to theirsequence in the genome, constitute a gene; in the human genome, the fraction is estimated at less than 3 percent.91the notion of an npcomplete problem is rooted in the theory of computational complexity and has a precise technicaldefinition. for purposes of this report, it suffices to understand an npcomplete problem as one that is very difficult and wouldtake a long time to solve.92s. thurn, c. faloutsos, t. mitchell, and l. wasseterman, òautomated learning and discovery: stateoftheart and researchtopics in a rapidly growing field,ó summary of a conference on automated learning and discovery, center for automated learning and discovery, carnegie mellon university, pittsburgh, pa, 1998.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.90catalyzing inquiryregions of the genome that are not transcribed from dna into rna include biological signals (such aspromoters) that flank the coding sequence and regulate the geneõs transcription. other untranscribed regions ofunknown purpose are found between genes or interspersed within coding sequences.genes themselves can occasionally be found nested within one another, and overlapping genes havebeen shown to exist on the same or opposite dna strands.93 the presence of pseudogenes (nonfunctionalsequences resembling real genes), which are distributed in numerous copies throughout a genome, furthercomplicates the identification of true proteincoding genes.94 finally, it is known that most genes are ultimately translated into more than one protein through a process that is not completely understood. in theprocess of transcription, the exons of a particular gene are assembled into a single mature mrna. however,in a process known as alternate splicing, various splicings omit certain exons, resulting in a family of variants(òsplice variantsó) in which the exons remain in sequence, but some are missing. it is estimated that at leasta third of human genes are alternatively spliced,95 with certain splicing arrangements occurring morefrequently than others. protein splicing and rna editing also play an important role. to understand genestructures completely, all of these sequence features have to be anticipated by gene recognition tools.two basic approaches have been established for gene recognition: the sequence similarity search, orlookup method, and the integrated compositional and signal search, or template method (also known asab initio gene finding).96 sequence similarity search is a wellestablished computational method for generecognition based on the conservation of gene sequences (called homology) in evolutionarily relatedorganisms. a sequence similarity search program compares a query sequence (an uncharacterized sequence) of interest with already characterized sequences in a public sequence database (e.g., databases ofthe institute of genomic research (tigr)97) and then identifies regions of similarity between the sequences. a query sequence with significant similarity to the sequence of an annotated (characterized) genein the database suggests that the two sequences are homologous and have common evolutionary origin.information from the annotated dna sequence or the protein coded by the sequence can potentially beused to infer gene structure or function of the query sequence, including promoter elements, potentialsplice sites, start and stop codons, and repeated segments. alignment tools, such as blast, 98 fasta, andsmithwaterman, have been used to search for the homologous genes in the database.although sequence similarity search has been proven useful in many cases, it has fundamentallimitations. manning et al. note in their work on the protein kinase complement of the human genome93i. dunham, l.h. matthews, j. burton, j.l. ashurst, k.l. howe, k.j. ashcroft, d.m. beare, et al., òthe dna sequence ofhuman chromosome 22,ó nature 402(6982):489495, 1999.94a mitigating factor is that pseudogenes are generally not conserved between species (see, for example, s. caenepeel, g.charydezak, s. sudarsanam, t. hunter, and g. manning, òthe mouse kinome: discovery and comparative genomics of allmouse protein kinases,ó proceedings of the national academy of sciences 101(32):1170711712, 2004). this fact provides another cluein deciding which sequences represent true genes and which represent pseudogenes.95d. brett, j. hanke, g. lehmann, s. haase, s. delbruck, s. krueger, j. reich, and p. bork, òest comparison indicates 38% ofhuman mrnas contain possible alternative splice forms,ó febs letters 474(1):8386, 2000.96j.w. fickett, òfinding genes by computer: the state of the art,ó trends in genetics 12(8):316320, 1996.97see http://www.tigr.org/tdb/.98the blast 2.0 algorithm, perhaps the most commonly used tool for searching large databases of gene or protein sequences, isbased on the idea that sequences that are truly homologous will contain short segments that will match almost perfectly. blast wasdesigned to be fast while maintaining the sensitivity needed to detect homology in distantly related sequences. rather than aligningthe full length of a query sequence against all of the sequences in the reference database, blast fragments the reference sequences intosubsequences or òwordsó (11 nucleotides long for gene search) constituting a dictionary against which a query sequence is matched.the program creates a list of all the reference words that show up in the query sequence and then looks for pairs of those words thatoccur at adjacent positions on different sequences in the reference database. blast uses these òseedó positions to narrow candidatematches and to serve as the starting point for the local alignment of the query sequence. in local alignment, each nucleotide position inthe query receives a score relative to how well the query and reference sequence match; perfect matches score highest, substitutions ofdifferent nucleotides incur different penalties. alignment is continued outward from the seed positions until the similarity of queryand reference sequences drops below a predetermined threshold. the program reports the highest scoring alignments, described by anevalue, the probability that an alignment with this score would be observed by chance. see, for example, s.f. altschul, t.l. madden,a.a. schaffer, j. zhang, z. zhang, w. miller, and d.j. lipman, ògapped blast and psiblast: a new generation of proteindatabase search programs,ó nucleic acids research 25(17):33893402, 1997.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.computational tools91that although òall 518 [kinase] genes are covered by some est [expressed sequence tag] sequence, and~90% are present in gene predictions from the celera and public genome databases, . . . those predictions are often fragmentary or inaccurate and are frequently misannotated.ó99there are several reasons for these limitations. first, only a fraction of newly discovered sequences haveidentifiable homologous genes in the current databases.100 the proportion of vertebrate genes with no detectable similarity in other phyla is estimated to be about 50 percent,101 and this is supported by a recent analysisof human chromosome 22, where only 50 percent of the proteins are found to be similar to previously knownproteins.102 also, the most prominent vertebrate organisms in genbank have only a fraction of their genomespresent in finished (versus draft, errorprone) sequences. hence, it is obvious that sequence similarity searchwithin vertebrates is currently limited. second, sequence similarity searches are computationally expensivewhen query sequences have to be matched against a large number of sequences in the databases.to resolve this problem, a dictionarybased method, such as identifier of coding exons (ice), is oftenemployed. in this method, gene sequences in the reference database are fragmented into subsequences oflength k, and these subsequences make up the dictionary against which a query sequence is matched. if thesubsequences corresponding to a gene have at least m consecutive matches with a query sequence, the gene isselected for closer examination. fulllength alignment techniques are then applied to the selected gene sequences. the dictionarybased approach significantly reduces the processing time (down to seconds per gene).in compositional and signal search, a model (typically a hidden markov model) is constructed thatintegrates coding statistics (measures indicative of protein coding functions) with signal detection intoone framework. an example of a simple hidden markov model for a compositional and signal searchfor a gene in a sequence sampled from a bacterial genome is shown in figure 4.3. the model is firstòtrainedó on sequences from the reference database and generates the probable frequencies of differentnucleotides at any given position on the query sequence to estimate the likelihood that a sequence is ina different òstateó (such as a coding region). the query sequence is predicted to be a gene if the productof the combined probabilities across the sequence exceeds a threshold determined by probabilitiesgenerated from sequences in the reference database.the discussion above has presumed that biological understanding does not play a role in generecognition. this is often untrueñgenerecognition algorithms make errors of omission and commission when run against genomic sequences in the absence of experimental biological data. that is, theyfail to recognize genes that are present, or misidentify starts or stops of genes, or mistakenly insert ordelete segments of dna into the putative genes. improvements in algorithm design will help to reducethese difficulties, but all the evidence to date shows that knowledge of some of the underlying sciencehelps even more to identify genes properly.10399g. manning, d.b. whyte, r. martinez, t. hunter, and s. sudarsanam, òthe protein kinase complement of the humangenome,ó science 298(5600):19121934, 2002.100i. dunham, n. shimizu, b.a. roe, s. chissoe, a.r. hunt, j.e. collins, r. bruskiewich, et al. òthe dna sequence of humanchromosome 22,ó nature 402(6761):489495, 1999.101j.m. claverie, òcomputational methods for the identification of genes in vertebrate genomic sequences,ó human moleculargenetics 6(10):17351744, 1999.102i. dunham, n. shimizu, b.a. roe, s. chissoe, a.r. hunt, j.e. collins, r. bruskiewich, et al., òthe dna sequence of humanchromosome 22,ó nature 402(6761):489495, 1999.103this discussion is further complicated by the fact that there is no scientific consensus on the definition of a gene. robertrobbins (vice president for information technology at the fred hutchinson cancer research center in seattle, washington, personal communication, december 2003) relates the following story: òseveral times, iõve experienced a situation where something likethe following happens. first, you get biologists to agree on the definition of a gene so that a computer could analyze perfect dataand tell you how many genes are present in a region. then you apply the definition to a fairly complex region of dna to determinethe number of genes (letõs say the result is 11). then, you show the results to the biologists who provided the rules and you say,ôaccording to your definition of a gene there are eleven genes present in this region.õ the biologists respond, ôno, there are justthree. but they are related in a very complicated way.õ when you then ask for a revised version of the rules that would provide aresult of three in the present example, they respond, ôno, the rules i gave you are fine.õó in short, robbins argues with considerablepersuasion that if biologists armed with perfect knowledge and with their own definition of a gene cannot produce rules that willalways identify how many genes are present in a region of dna, computers have no chance of doing so.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.92catalyzing inquiry4.4.5 sequence alignment and evolutionary relationshipsa remarkable degree of similarity exists among the genomes of living organisms.104 informationabout the similarities and dissimilarities of different types of organisms presents a picture of relatednessbetween species (i.e., between reproductive groups), but also must provide useful clues to the importance, structure, and function of genes and proteins carried or lost over time in different species.òcomparative genomicsó has become a new discipline within biology to study these relationships.intergenicregionstart codon (atg)coding regionstop codon(taa)emission probabilitya c t g tp = 1.0tp = 1.0emission probabilitya c t g tp =.9tp =.9tp =.1tp = .1.25.25.25.25.9.03.03.04figure 4.3hidden markov model of a compositional signal and search approach for finding a gene in a bacterialgenome.the model has four features: (1) state of the sequence, of which four states are possible (coding, intergenic, start,and stop); (2) outputs, defined as the possible nucleotide(s) that can exist at any given state (a, c, t, g at codingand intergenic states; atg and taa at start and stop states, respectively); (3) emission possibilitiesñthe probability that a given nucleotide will be generated in any particular state; and (4) transition probability (tp)ñthe probability that the sequence is in transition between two states.to execute the model, emission and transition probabilities are obtained by training on the characterized genesin the reference database. the set of all possible combinations of states for the query sequence is then generated,and an overall probability for each combination of states is calculated. if the combination having the highestoverall probability exceeds a threshold determined using gene sequences in the reference database, the querysequence is concluded to be a gene.104for example, 9 percent of e. coli genes, 9 percent of rice genes, 30 percent of yeast genes, 43 percent of mosquito genes, 75percent of zebrafish genes, and 94 percent of rat genes have homologs in humans. see http://iubio.bio. indiana.edu:8089/all/hgsummary.html (summary table august 2005).catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.computational tools93alignments of gene and protein sequences from many different organisms are used to find diagnosticpatterns to characterize protein families; to detect or demonstrate homologies between new sequencesand existing families of sequences; to help predict the secondary and tertiary structures of new sequences; and to serve as an essential prelude to molecular evolutionary analysis.to visualize relationships between genomes, evolutionary biologists develop phylogenetic treesthat portray groupings of organisms, characteristics, genes, or proteins based on their common ancestries and the set of common characters they have inherited. one type of molecular phylogenetic tree, forexample, might represent the amino acid sequence of a protein found in several different species. thetree is created by aligning the amino acid sequences of the protein in question from different species,determining the extent of differences between them (e.g., insertions, deletions, or substitutions of aminoacids), and calculating a measure of relatedness that is ultimately reflected in a drawing of a tree withnodes and branches of different lengths.the examination of phylogenetic relationships of sequences from several different species generally uses a method known as progressive sequence alignment, in which closely related sequencesare aligned first, and more distant ones are added gradually to the alignment. attempts at tacklingmultiple alignments simultaneously have been limited to small numbers of short sequences because of the computational power needed to resolve them. therefore, alignments are most oftenundertaken in a stepwise fashion. the algorithm of one commonly used program (clustalw) consists of three main stages. first, all pairs of sequences are aligned separately in order to calculate adistance matrix giving the divergence of each pair of sequences; second, a guide tree is calculatedfrom the distance matrix; and third, the sequences are progressively aligned according to thebranching order in the guide tree.alignment algorithms that test genetic similarity face several challenges. the basic premise of amultiple sequence alignment is that, for each column in the alignment, every residue from every sequence is homologous (i.e., has evolved from the same position in a common ancestral sequence). in theprocess of comparing any two amino acid sequences, the algorithm must place gaps or spaces at pointsthroughout the sequences to get the sequences to align. because inserted gaps are carried forward intosubsequent alignments with additional new sequences, the cumulative alignment of multiple sequencescan become riddled with gaps that sometimes result in an overall inaccurate picture of relationshipsbetween the proteins. to address this problem, gap penalties based on a weight matrix of differentfactors are incorporated into the algorithm. for example, the penalty for introducing a gap in aligningtwo similar sequences is greater that that for aligning two dissimilar sequences. gap penalties differdepending on the length of the sequence, the types of sequence, and different regions of sequence.based on the weight matrix and rules for applying penalties, the algorithm compromises in the placement of gaps to obtain the lowest penalty score for each alignment.the placement of a gap in a protein sequence may represent an evolutionary changeñif a gap,reflecting the putative addition or subtraction of an amino acid to a proteinõs structure, is introduced,the function of the protein may change, and the change may have evolutionary benefit. however, thechange may also be insignificant from a functional point of view. today, it is known that most insertionsand deletions occur in loops on the surface of the protein or between domains of multidomain proteins,which means that knowledge of the threedimensional structure or the domain structure of the proteincan be used to help identify functionally important deletions and insertions.as the structures of different protein domains and families are increasingly determined by othermeans, alignment algorithms that incorporate such information should become more accurate. morerecently, stochastic and iterative optimization methods are being used to refine individual alignments.also, some algorithms (e.g., bioedit) allow users to manually edit the alignment when other informationor òeyeballingó suggests logical placement of gaps.exploitation of complete genomic knowledge across closely related species can play an importantrole in identifying the functional elements encoded in a genome. kellis et al. undertook a comparativeanalysis of the yeast saccharomyces cerevisiae based on highquality draft sequences of three relatedcatalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.94catalyzing inquiryspecies (s. paradoxus, s. mikatae, and s. bayanus).105 this analysis resulted in significant revisions of theyeast gene catalogue, affecting approximately 15 percent of all genes and reducing the total count byabout 500 genes. seventytwo genomewide elements were identified, including most known regulatory motifs and numerous new motifs, and a putative function was inferred for most of these motifs.the power of the comparative genomic approach arises from the fact that sequences that are positivelyselected (i.e., confer some evolutionary benefit or have some useful function) tend to be conserved as aspecies evolves, while other sequences are not conserved. by comparing a given genome of interest toclosely related genomes, conserved sequences become much more obvious to the observer than if thefunctional elements had to be identified only by examination of the genome of interest. thus, it ispossible, at least in principle, that functional elements can be identified on the basis of conservationalone, without relying on previously known groups of coregulated genes or without using data fromgene expression or transcription factor binding experiments.molecular phylogenetic trees that graphically represent the differences between species are usuallydrawn with branch lengths proportional to the amount of evolutionary divergence between the twonodes they connect. the longer the distance between branches, the more relatively divergent are thesequences they represent. methods for calculating phylogenetic trees fall into two general categories: (1)distancematrix methods, also known as clustering or algorithmic methods, and (2) discrete data methods. in distancematrix methods, the percentage of sequence difference (or distance) is calculated forpairwise combinations of all points of divergence; then the distances are assembled into a tree. incontrast, discrete data methods examine each column of the final alignment separately and look for thetree that best accommodates all of the information, according to optimality criteriañfor example, thetree that requires the fewest character state changes (maximum parsimony), the tree that best fits anevolutionary model (maximum likelihood), or the tree that is most probable, given the data (bayesianinference). finally, òbootstrappingó analysis tests whether the whole dataset supports the proposed treestructure by taking random subsamples of the dataset, building trees from each of these, and calculatingthe frequency with which the various parts of the proposed tree are reproduced in each of the randomsubsamples.among the difficulties facing computational approaches to molecular phylogeny is the fact thatsome sequences (or segments of sequences) mutate more rapidly than others.106 multiple mutations atthe same site obscure the true evolutionary difference between sequences. another problem is thetendency of highly divergent sequences to group together when being compared regardless of their truerelationships. this occurs because of a background noise problemñwith only a limited number ofpossible sequence letters (20 in the case of amino acid sequences), even divergent sequences will notinfrequently present a false phylogenetic signal due strictly to chance.4.4.6 mapping genetic variation within a speciesthe variation that occurs between different species represents the product of reproductive isolationand population fission over very long time scales during which many mutational changes in genes andproteins occur. in contrast, variation within a single species is the result of sexual reproduction, genetic105m. kellis, n. patterson, m. endrizzi, b. birren, and e.s. lander, òsequencing and comparison of yeast species to identifygenes and regulatory elements,ó nature 423(6937):241254, 2003.106a number of interesting references to this problem can be found in the following: m.t. holder and p.o. lewis, òphylogenyestimation: traditional and bayesian approaches,ó nature reviews genetics 4:275284, 2003; i. holmes and w.j. bruno, òevolutionary hmms: a bayesian approach to multiple alignment,ó bioinformatics 17(9):803820, 2001; a. siepel and d. haussler,òcombining phylogenetic and hidden markov models in biosequence analysis,ó in proceedings of the seventh annual international conference on computational molecular biology, berlin, germany, pp. 277286, 2003; r. durbin, s. eddy, a. krogh, and g.mitchison, biological sequence analysisñprobabilistic models of proteins and nucleic acids, cambridge university press, new york,1998.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.computational tools95recombination, and smaller numbers of relatively recent mutations.107 examining the variation of geneor protein sequences between different species helps to draw a picture of the pedigree of a particulargene or protein over evolutionary time, but scientists are also interested in understanding the practicalsignificance of such variation within a single species.geneticists have been trying for decades to identify the genetic variation among individuals in thehuman species that result in physical differences between them. there is an increasing recognition ofthe importance of genetic variation for medicine and developmental biology and for understanding theearly demographic history of humans.108 in particular, variation in the human genome sequence isbelieved to play a powerful role in the origins of and prognoses for common medical conditions.109the total number of unique mutations that might exist collectively in the entire human populationis not known definitively and has been estimated at upward of 10 million,110 which in a 3 billion basepair genome corresponds to a variant every 300 bases or less. included in these are singlenucleotidepolymorphisms (snps), that is, singlenucleotide sites in the genome where two or more of the fourbases (a, c, t, g) occur in at least 1 percent of the population. many snps were discovered in theprocess of overlapping the ends of dna sequences used to assemble the human genome, when thesesequences came from different individuals or from different members of a chromosome pair from thesame individual. the average number of differences observed between the dna of any two unrelatedindividuals represented at 1 percent or more in the population is one difference in every 1,300 bases; thisleads to the estimation that individuals differ from one another at 2.4 million places in their genomes.111in rare cases, a single snp has been directly associated with a medical condition, such as sickle cellanemia or cystic fibrosis. however, most common diseases such as diabetes, cancer, stroke, heart disease, depression, and arthritis (to name a few) appear to have complex origins and involve the participation of multiple genes along with environmental factors. for this reason there is interest in identifyingthose snps occurring across the human genome that might be correlated with common medical conditions. snps found within exons that contain genes are of greatest interest because they are believed to bepotentially related to changes in proteins that affect a predisposition to disease, but because most of thegenome does not code for proteins (and indeed a number of noncoding snps have been found112), thefunctional impact of many snps is unknown.armed with rapid dna sequencing tools and the ability to detect singlebase differences, an international consortium looked for snps in individuals over the last several years, ultimately identifyingmore than 3 million unique snps and their locations on the genome in a public database. snp maps ofthe human genome with a density of about one snp per thousand nucleotides have been developed. aneffort under way in iceland known as decode seeks to correlate snps with human diseases.113 however, determining which combinations of the 10 million snps are associated with particular diseasestates, predisposition to disease, and genes that contribute to disease remains a formidable challenge.some research on this problem has recently on focused on the discovery that specific combinationsof snps on a chromosome (called òhaplotypesó) occur in blocks that are inherited together; that is, they107d. posada and k.a. crandall, òintraspecific gene genealogies: trees grafting into networks,ó trends in ecology and evolution 16(1):3745, 2001.108l.l. cavallisforza and m.w. feldman, òthe application of molecular genetic approaches to the study of human evolution,ó nature genetics 33 (suppl.):266275, 2003.109s.b. gabriel, s.f. schaffner, h. nguyen, j.m. moore, j. roy, b. blumenstiel, j. higins, et al., òthe structure of haplotypeblocks in the human genome,ó science 296(5576):22252229, 2002.110l. kruglyak and d.a. nickerson, òvariation is the spice of life,ó nature genetics 27(3):234236, 2001, available at http://nucleus.cshl.edu/agsa/papers/snp/kruglyak2001.pdf.111the international snp map working group, òa map of human genome sequence variation containing 1.42 million singlenucleotide polymorphisms,ó nature 409:928933, 2001.112see, for example, d. trikka, z. fang, a. renwick, s.h. jones, r. chakraborty, m. kimmel, and d.l. nelson, òcomplex snpbased haplotypes in three human helicases: implications for cancer association studies,ó genome research 12(4):627639, 2002.113see www.decode.com.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.96catalyzing inquiryare unlikely to be separated by recombination that takes place during reproduction. further, only arelatively small number of haplotype patterns appear across portions of a chromosome in any givenpopulation.114 this discovery potentially simplifies the problem of associating snps with disease because a much smaller number of òtagó snps (500,000 versus the estimated 10 million snps) might beused as representative markers for blocks of variation in initial studies to find correlations betweenparts of the genome and common diseases. in october 2002, the national institutes of health (nih)launched the effort to map haplotype patterns (the hapmap) across the human genome.developing a haplotype map requires determination of all of the possible tag snp combinationsthat are common in a population, and therefore relies on data from highthroughput screening of snpsfrom a large number of individuals. a difficulty is that a haplotype represents a specific group of snpson a single chromosome. however, with the exception of gametes (sperm and egg), human cells containtwo copies of each chromosome (one inherited from each parent). highthroughput studies generallydo not permit the separate, parallel examination of each snp site on both members of an individualõspair of chromosomes. snp data obtained from individuals represent a combination of information(referred to as the genotype) from both of an individualõs chromosomes. for example, genotyping anindividual for the presence of a particular snp will result in two data values (e.g., a and t). each valuerepresents an snp at the same site on both chromosomes, and recently it has become possible todetermine the specific chromosomes to which a and t belong.115there are two problems in creating a hapmap. the first is to extract haplotype informationcomputationally from genotype information for any individual. the second is to estimate haplotypefrequencies in a population. although good approaches to the first problem are known,116 the secondremains challenging. algorithms such as the expectationmaximization approach, gibbs samplingmethod, and partitionligation methods have been developed to tackle this problem.some algorithmic programs rely on the concept of evolutionary coalescence or a perfect phylogenyñthat is, a rooted tree whose branches describe the evolutionary history of a set of sequences (orhaplotypes) in sample individuals. in this scenario, each sequence has a single ancestor in the previousgeneration, under the presumption that the haplotype blocks have not been subject to recombination,and takes as a given that only one mutation will have occurred at any one snp site. given a set ofgenotypes, the algorithm attempts to find a set of haplotypes that fit a perfect phylogeny (i.e., couldhave originated from a common ancestor). the performance of algorithms for haplotype predictiongenerally improves as the number of individuals sampled and the number of snps included in theanalysis increases. this area of algorithm development will continue to be a robust area of research inthe future as scientists and industry seek to associate genetic variation with common diseases.direct haplotyping is also possible, and can circumvent many of the difficulties and ambiguitiesencountered when a statistical approach is used.117 for example, ding and cantor have developed atechnique that enables direct molecular haplotyping of several polymorphic markers separated by asmany as 24 kb.118 the haplotype is directly determined by simultaneously genotyping several polymorphic markers in the same reaction with a multiplex pcr and base extension reaction. this approachdoes not rely on pedigree data and does not require previous amplification of the entire genomic regioncontaining the selected markers.114e.s. lander, l.m. linton, b. birren, c. nusbaum, m.c. zody, j. baldwin, et al., òinitial sequencing and analysis of thehuman genome,ó nature 409(6822):860921, 2001.115c. ding and c.r. cantor, òdirect molecular haplotyping of longrange genomic dna with m1pcr,ó proceedings of thenational academy of sciences 100(13):74497453, 2003.116see, for example, d. gusfield, òinference of haplotypes from samples of diploid populations: complexity and algorithms,ó journal of computational biology 8(3):305323, 2001.117j. tost, o. brandt, f. boussicault, d. derbala, c. caloustian, d. lechner, and i.g. gut, òmolecular haplotyping at highthroughput,ó nucleic acids research 30(19):e96, 2002.118c. ding and c.r. cantor, òdirect molecular haplotyping of longrange genomic dna with m1pcr,ó proceedings of thenational academy of sciences 100(13):74497453, 2003.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.computational tools97finally, in early 2005, national geographic and ibm announced a collaboration known as the thegenographic project to probe the migratory history of the human species.119 the project seeks to collect 100,000blood samples from indigenous populations, with the intent of analyzing dna in these samples. ultimately,the project will create a global database of human genetic variation and associated anthropological data(language, social customs, etc.) that provides a snapshot of human genetic variation before the cultural contextof indigenous populations is lostña context that is needed to make sense of the variations in dna data.4.4.7 analysis of gene expression dataalthough almost all cells in an organism contain the same genetic material (the genomic blueprint forthe entire organism), only about onethird of a given cellõs genes are expressed or òswitched onóñthat is,are producing proteinsñat a given time. expressed genes account for differences in cell types; for example, dna in skin cells produces a different set of proteins than dna in nerve cells. similarly, adeveloping embryo undergoes rapid changes in the expression of its genes as its body structure unfolds.differential expression in the same types of cells can represent different cellular òphenotypesó (e.g.,normal versus diseased), and modifying a cellõs environment can result in changed levels of expression ofa cellõs genes. in fact, the ability to perturb a cell and observe the consequential changes in expression is akey to understanding linkages between genes and can be used to model cell signaling pathways.a powerful technology for monitoring the activity of all the genes in a cell is the dna microarray(described in box 7.5 in chapter 7). many different biological questions can be asked with microarrays,and arrays are now constructed in many varieties. for example, instead of dna across an entiregenome, the array might be spotted with a specific set of genes from an organism or with fabricatedsequences of dna (oligonucleotides) that might represent, for example, a particular snp or a mutatedform of a gene. more recently, protein arrays have been developed as a new tool that extends the reachof gene expression analysis.the ability to collect and analyze massive sets of data about the transcriptional states of cells is anemerging focus of molecular diagnostics as well as drug discovery. profiling the activation or suppression of genes within cells and tissues provides telling snapshots of function. such information is criticalnot only to understand disease progression, but also to determine potential routes for disease intervention. new technologies that are driving the field include the creation of òdesigneró transcription factorsto modulate expression, use of laser microdissection methods for isolation of specific cell populations,and technologies for capturing mrna. among the questions asked of microarrays (and the computational algorithms to decipher the results) are the discrimination of genes with significant changes inexpression relative to the presence of a disease, drug regimen, or chemical or hormonal exposure.to illustrate the power of largescale analysis of gene data, an article in science by gaudet andmango is instructive.120 a comparison of microarray data taken from caenorhabditis elegans embryoslacking a pharynx with microarray data from embryos having excess pharyngeal tissue identified 240genes that were preferentially expressed in the pharynx, and further identified a single gene as directlyregulating almost all of the pharynxspecific genes that were examined in detail. these results suggestthe possibility that direct transcriptional regulation of entire gene networks may be a common feature oforganspecification genes.121119more information on the project can be found at http://www5.nationalgeographic.com/genographic/.120j. gaudet and s.e. mango, òregulation of organogenesis by the caenorhabditis elegans foxa protein pha4,ó science295(5556):821825, 2002.121for example, it is known that a specific gene activates other genes that function at two distinct steps of the regulatoryhierarchy leading to wing formation in drosophila (k.a. guss, c.e. nelson, a. hudson, m. e. kraus and s. b. carroll, òcontrol ofa genetic regulatory network by a selector gene,ó science 292(5519):11641167, 2001), and also that the presence of specificfactor is both necessary and sufficient for specification of eye formation in drosophila imaginal discs, where it directly activatesthe expression of both early and lateacting genes (w.j. gehring and k. ikeo, òpax 6: mastering eye morphogenesis and evolution,ó trends in genetics 15(9):371377, 1999).catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.98catalyzing inquirymany analytic techniques have been developed and applied to the problem of revealing biologicallysignificant patterns in microarray data. various statistical tests (e.g., ttest, ftest) have been developedto identify genes with significant changes in expression (out of thousands of genes); such genes havehad widespread attention as potential diagnostic markers or drug targets for disease, stages of development, and other cellular phenotypes. many classification tools (e.g., fisherõs discriminant analysis,bayesian classifier, artificial neural networks, tools from signal processing) have also been developed tobuild a phenotype classifier with the genes differentially expressed. these classification tools are generally used to discriminate known sample groups from each other using differentially expressed genesselected by statistical testing.other algorithms are necessary because data acquired through microarray technology often haveproblems that must be managed prior to use. for example, the quality of microarray data is highlydependent on the way in which a sample is prepared. many factors can affect the extent to which a dotfluoresces, of which the transcription level of the particular gene involved is only one. such extraneousfactors include the sampleõs spatial homogeneity, its cleanliness (i.e., lack of contamination), the sensitivity of optical detectors in the specific instrument, varying hybridization efficiency between clones,relative differences between dyes, and so forth. in addition, because different laboratories (and differenttechnicians) often have different procedures for sample preparation, datasets taken from different laboratories may not be strictly comparable. statistical methods of analysis of variance (anova) have beenapplied to deal with these problems, using models to estimate the various contributions to relativesignal from the many potential sources. importantly, these models not only allow researchers to attachmeasures of statistical significance to data, but also suggest improved experimental designs.122an important analytical task is to identify groups of genes with similar expression patterns. thesegroups of genes are more likely to be involved in the same cellular pathways, and many datadrivenhypotheses about cellular regulatory mechanisms (e.g., disease mechanisms) have been drawn underthis assumption. for this purpose, various clustering methods, such as hierarchical clustering methods,selforganizing maps (trained neural networks), and cosa (clustering objects on subsets of attributes),have been developed. the goal of cluster analysis is to partition a dataset of n objects into subgroupssuch that these objects are more similar to those in their subgroups than to those in other groups.clustering tools are generally used to identify groups of genes that have similar expression patternacross samples; thus, it is reasonable to suppose that the genes in each group (or cluster) are involved inthe same biological pathway. most clustering methods are iterative and involve the calculation of anotional distance between any two data points; this distance is used as the measure of similarity. inmany implementations of clustering, the distance is a function of all of the attributes of each sample.agglomerative hierarchical clustering begins with assigning n clusters for n samples, where allsamples are defined as different individual clusters. potential clusters are arranged in a hierarchydisplayed as a binary tree or òdendrogram.ó euclidian distance or pearson correlation is used withòaverage linkingó to develop the dendrogram. for example, two clusters that are closest to each other interms of euclidean distance are combined to form a new cluster, which is represented as the average oftwo groups combined (average linkage). this process is continued until there is one cluster to which allsamples belong. in the process of forming the single cluster, the overall structure of clusters is evaluatedfor whether the merging of two clusters into one new cluster decreases both the sum of the similaritywithin all of the clusters and the sum of differences between all of the clusters. the clustering procedurestops at the level at which these are equal.selforganizing maps (soms)123 are another form of cluster analysis. with soms, a number ofdesired clusters is decided in advance, and a geometry of nodes (such as an n × m grid) is created,where each node represents a single cluster. the nodes are randomly placed in the data space. then, in122m. kerr, m. martin, and g. churchill, òanalysis of variance for gene expression microarray data,ó journal of computationalbiology 7(6):819837, 2000.123t. kohonen, selforganizing maps, second edition, springer, berlin, 1997.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.computational tools99a random order, each data point is selected. at each iteration, the nodes move closer to the selected datapoint, with the distance moved influenced by the distance from the data point to the node and theiteration number. thus, the closest node will move the most. over time, the initial geometry of thenodes will deform and each node will represent the center of an identified cluster. experimentation isoften necessary to arrive at a useful number of nodes and geometry, but since soms are computationallytractable, it is feasible to run many sessions. the properties of somsñpartially structured, scalable tolarge datasets, unsupervised, easily visualizableñmake them well suited for analysis of microarraydata, and they have been used successfully to detect patterns of gene expression.124in contrast to the above two methods, cosa is based on the assumption that better clustering canbe achieved if only relevant genes are used in individual clusters. this is consistent with the idea ofidentifying differentially expressed genes (relevant genes) and then using only those genes to build aclassifier. the search algorithm in cosa identifies an optimal set of variables that should be used togroup individual clusters and which clusters should be merged when their similarity is assessed usingthe optimal set of variables identified. this idea was implemented by adding weights reflecting contributions of all genes to producing a particular set of sample clusters, and the search algorithm is thenformulated as an optimization problem. the clustering results by cosa indicate that a subset of genesmakes a greater contribution to a particular sample cluster than to other clusters.125clustering methods are being used in many types of studies. for example, they are particularlyuseful in modeling cell networks and in clustering disparate kinds of data (e.g., rna data and nonrna data; sequence data and protein data). clustering can be applied to evaluate how feasible a givennetwork structure is. also, clustering is often combined with perturbation analysis to explore a set ofsamples or genes for a particular purpose. in general, clustering can be useful in any study in whichlocal analyses with groups of samples or genes identified by clustering improve the understanding ofthe overall system.biclustering is an alternate approach to revealing meaningful patterns in the data.126 it seeks toidentify submatrices in which the set of values has a low meansquared residue, meaning that the eachvalue is reasonably coherent with other members in its row and column. (however, excluding meaningless solutions with zero area, this problem is unfortunately npcomplete.) advantages of this approachinclude that it can reveal clusters based on a subset of attributes, it simultaneously clusters genes withsimilar expression patterns and conditions with similar expression patterns, and most importantly,clusters can overlap. since genes are often involved in multiple biological pathways, this can be used toreveal linkages that otherwise would be obscured by traditional cluster analysis.while many analyses of microarray data consider a single snapshot in time, of course expressionlevels vary over time, especially due to the cellular life cycle. a challenge in analyzing microarray timeseries data is that cell cycles may be unsynchronized, making it difficult to correctly identify correlations between data samples that have similar expression behavior. statistical techniques can identifyperiodicity in series and look for phaseshifted correlations between pairs of samples,127 as well as moretraditional clustering analysis.a separate set of analytic techniques is referred to as supervised methods, in contrast to clusteringand similar methods that run with no incoming assumptions. supervised methods, in contrast, useexisting knowledge of the dataset to classify data into one of a set of classes. in general, these techniques124p. tamayo, d. slonim, j. mesirov, q. zhu, s. kitareewwan, e. dmitrovsky, e.s. lander, and t.r. golub, òinterpretingpatterns of gene expression with selforganizing maps: methods and application to hematopoietic differentiation,ó proceedingsof the national academy of sciences 96(6):29072912, 1999.125j.h. friedman and j.j. meulman, òclustering objects on subsets of attributes,ó journal of the royal statistical society series b66(4):815849(34), 2004.126y. cheng and g.m. church, òbiclustering of expression data,ó proceedings of the eighth international conference on intelligentsystems for molecular biology 8:93103, 2000.127v. filkov, s. skiena, and j. zhi, òanalysis techniques for microarray timeseries data,ó journal of computational biology9(2):317330. available at http://www.cs.ucdavis.edu/~filkov/papers/spellmananalysis.pdf.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.100catalyzing inquiryrely on training sets provided by the researchers, where the class membership of data is provided. then,when presented with experimental data, supervised methods apply the learning from the training set toperform similar classifications. one such technique is support vector machines (svms), which areuseful for highly multidimensional data. svms map the data into a òfeature spaceó and then create(through one of a large number of possible algorithms) a hyperplane that separates the classes. anothercommon method is artificial neural nets (see xref), which train on a dataset with defined classmembership; if the neural network classifies a member of the training set incorrectly, the error backpropagates through the system and updates the weightings. unsupervised and supervised methods canbe combined for òsemisupervisedó learning methods, in which heterogeneous training data can be bothclassified and unclassified.128however, there is no analytic method optimal to any dataset. thus, it would be useful to develop ascheme that can guide users to choose an appropriate method (e.g., in hierarchical clustering, an appropriate set of similarity measure, linkage method, and the measure used to determine the number ofclusters) to achieve a reasonable analysis of their own datasets.ultimately, it is desirable to go beyond correlations and associations in the analysis of gene expression data to seek causal relationships. it is an elementary truism of statistics that indications of correlation are not by themselves indicators of causalityñan experimental manipulation of one of more variables is always necessary to conclude a causal relationship. nevertheless, analysis of microarray datacan be helpful in suggesting experiments that might be particularly fruitful in uncovering causal relationships. bayesian analysis allows one to make inferences about the possible structure of a geneticregulatory pathway on the basis of microarray data, but even advocates of such analysis recognize theneed for experimental test. one work goes so far as to suggest that it is possible that automatedprocessing of microarray data can suggest interesting experiments that will shed light on causal relationships, even if the existing data themselves donõt support causal inferences.1294.4.8 data mining and discovery4.4.8.1 the first known biological discovery from mining databases130by the early 1970s, the simian sarcoma virus had been determined to cause cancer in certain speciesof monkeys. in 1983, the responsible oncogene within the virus was sequenced. at around the sametime, and entirely independently, a partial amino acid sequence of an important growth factor inhumansñthe plateletderived growth factor (pdgf) was also determined. pdgf was known to causecultured cells to proliferate in a cancerlike manner. russell doolittle compared the two sequences andfound a high degree of similarity between them, indicating a possible connection between an oncogeneand a normal human gene. in this case, the indication was that the simian sarcoma virus acted on cellsin monkeys in a manner similar to the action of pdgf on human cells.128t. li, s. zhu, q. li, and m. ogihara, ògene functional classification by semisupervised learning from heterogeneousdata,ó pp. 7882 in proceedings of the acm symposium on applied computing, acm press, new york, 2003.129c. yoo and g. cooper, òan evaluation of a system that recommends microarray experiments to perform to discovergeneregulation pathways,ó artificial intelligence in medicine 31(2):169182, 2004, available at http://www.phil.cmu.edu/projects/genegroup/papers/yoo2003a.pdf.130adapted from s.g.e. andersson and l. klasson, ònavigating through the databases,ó available at http://artedi.ebc.uu.se/course/overview/navigatingdatabases.html. the original doolittle article was published as r.f. doolittle, m.w. hunkapiller,l.e. hood, s.g. davare, k.c. robbins, s.a. aaronson, and h.n. antoniades, òsimian sarcoma virus onc gene, vsis, is derivedfrom the gene (or genes) encoding a plateletderived growth factor,ó science 221(4607):275277, 1983.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.computational tools1014.4.8.2 a contemporary example: protein family classification anddata integration for functional analysis of proteinsnew bioinformatics methods allow inference of protein function using associative analysis (òguiltby associationó) of functional properties to complement the traditional sequence homologybased methods.131 associative properties that have been used to infer function not evident from sequence homology include cooccurrence of proteins in operons or genome context; proteins sharing common domainsin fusion proteins; proteins in the same pathway, subcellular network, or complex; proteins with correlated gene or protein expression patterns; and protein families with correlated taxonomic distribution(common phylogenetic or phyletic patterns).coupling protein classification and data integration allows associative studies of protein family,function, and structure.132 an example is provided in figure 4.4, which illustrates how the collectiveuse of protein family, pathway, and genome context in bacteria helped researchers to identify a longsought human gene associated with the methylmalonic aciduria disorder.domainbased or structural classificationbased searches allow identification of protein familiessharing domains or structural fold classes. functional convergence (unrelated proteins with the sameactivity) and functional divergence are revealed by the relationships between the enzyme classificationand protein family classification. with the underlying taxonomic information, protein families thatoccur in given lineages can be identified. combining phylogenetic pattern and biochemical pathwayinformation for protein families allows identification of alternative pathways to the same end productin different taxonomic groups, which may present attractive potential drug targets. the systematicapproach for protein family curation using integrative data leads to novel prediction and functionalinference for uncharacterized òhypotheticaló proteins, and to detection and correction of genome annotation errors (a few examples are listed in table 4.2). such studies may serve as a basis for furtheranalysis of protein functional evolution, and its relationship to the coevolution of metabolic pathways,cellular networks, and organisms.underlying this approach is the availability of resources that provide analytical tools and data. forexample, the protein information resource (pir) is a public bioinformatics resource that provides anadvanced framework for comparative analysis and functional annotation of proteins. pir recentlyjoined the european bioinformatics institute and swiss institute of bioinformatics to establishuniprot,133 an international resource of protein knowledge that unifies the pir, swissprot, and trembldatabases. central to the piruniprot functional annotation of proteins is the pirsf (superfamily)classification system134 that provides classification of whole proteins into a network structure to reflecttheir evolutionary relationships. this framework is supported by the iproclass integrated database ofprotein family, function, and structure,135 which provides valueadded descriptions of all uniprot proteins with rich links to more than 50 other databases of protein family, function, pathway, interaction,modification, structure, genome, ontology, literature, and taxonomy. as a core resource, the pir environment is widely used by researchers to develop other bioinformatics infrastructures and algorithmsand to enable basic and applied scientific research, as shown by examples in table 4.3.131e.m. marcotte, m. pellegrini, m.j. thompson, t.o. yeates, and d. eisenberg, òcombined algorithm for genomewideprediction of protein function,ó nature 402(6757):8386, 1999.132c.h. wu, h. huang, a. nikolskaya, z. hu, and w.c. barker, òthe iproclass integrated database for protein functionalanalysis,ó computational biology and chemistry 28(1):8796, 2004.133r. apweiler, a. bairoch, c.h. wu, w.c. barker, b. boeckmann, s. ferro, e. gasteiger, et al., òuniprot: universal proteinknowledgebase,ó nucleic acids research 32(database issue):d115d119, 2004.134c.h. wu, a. nikolskaya a, h. huang, l.s. yeh, d.a. natale, c.r. vinayaka, z.z. hu, et al., òpirsf family classificationsystem at the protein information resource,ó nucleic acids research 32(database issue):d112d114, 2004.135c.h. wu, h. huang, a. nikolskaya, z. hu, and w.c. barker, òthe iproclass integrated database for protein functionalanalysis,ó computational biology and chemistry 28(1):8796, 2004.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.102catalyzing inquiryfigure 4.4integration of protein family, pathway, and genome context data for disease gene identification.the atr enzyme (ec 2.5.1.17) converts inactive cobalamins to adocbl (a), a cofactor for enzymes in severalpathways, including diol/glycerol dehydratase (ec 4.2.1.28) (b) and methylmalonylcoa mutase (mcm) (ec5.4.99.2) (c). many prokaryotic atrs are predicted to be required for ec 4.2.1.28 based on the genome context ofthe corresponding genes. however, in at least one organism (archaeoglobus fulgidus), the atr gene is adjacent tothe mcm gene, which provided a clue for cloning the human and bovine atrs.source: courtesy of cathy wu, georgetown university.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.computational tools1034.4.9 determination of threedimensional protein structureone central problem of proteomics is that of protein folding. protein folding is one of the mostimportant cellular processes because it produces the final conformation required for a protein to attainbiological activity. diseases such as alzheimerõs disease or bovine spongiform encephalopathy (bse, oròmad cowó disease) are associated with the improper folding of proteins. for example, in bse theprotein (called the scrapie prion), which is soluble when it folds properly, becomes insoluble when oneof the intermediates along its folding pathway misfolds and forms an aggregation that damages nervecells.136due to the importance of the functional conformation of proteins, many efforts have been attempted to predict computationally a threedimensional structure of a protein from its amino acidsequence. although experimental determination of protein structure based on xray crystallographyand nuclear magnetic resonance yields protein structures in high resolution, it is slow, laborintensive,and expensive and thus not appropriate for largescale determination. also, it can apply only to alreadysynthesized or isolated proteins, while an algorithm could be used to predict the structure of agreat number of potential proteins.table 4.2protein family classification and integrative associative analysis for functionalannotationsuperfamily classificationdescriptiona. functional inference of uncharacterized hypothetical proteinssf034452timbarrel signal transduction proteinsf004961metaldependent hydrolasesf005928nucleotidyltransferasesf005933atpase with chaperone activityand inactive lon protease domainsf005211alpha/beta hydrolasesf014673lipid carrier proteinsf005019[ni,fe]hydrogenase3type complex,membrane protein ehaab. correction or improvement of genome annotationssf025624ligandbinding protein with an act domainsf005003inactive homologue of metaldependentproteasesf000378glycyl radical cofactor protein yfidsf000876chemotaxis response regulatormethylesterase chebsf000881thioesterase, type iisf002845bifunctional tetrapyrrole methylase andmazg ntpasec. enhanced understanding of structure, function, evolutionary relationshipssf005965chorismate mutase, aroh classsf001501chorismate mutase, aroq class,prokaryotic typenote: pirsf protein family reports detail supporting evidence for both experimentally validated and computationally predicted annotations.136see, for example, c.m. dobson, òprotein misfolding, evolution and disease,ó trends in biochemical science 24(9):329332,1999; c.m. dobson, òprotein folding and its links with human disease.ó biochemical society symposia 68:126, 2001; c.m. dobson, òprotein folding and misfolding,ó nature 426(6968):884890, 2003.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.104catalyzing inquiryprotein structures predicted in high resolution can help characterize the biological functions ofproteins. biotechnology companies are hoping to accelerate their efforts to discover new drugs thatinteract with proteins by using structurebased drug design technologies. by combining computationaland combinatorial chemistry, researchers expect to find more viable leads. algorithms create molecularstructure built de novo to optimize interactions within the proteinõs active sites. the use of socalledvirtual screening in combination with studies of cocrystallized drugs and proteins could be a powerfultool for drug development.a number of tools for protein structure prediction have been developed, and progress in predictionby these methods has been evaluated by the critical assessment of protein structure prediction (casp)experiment held every two years since 1994.137 in a casp experiment, the amino acid sequences ofproteins whose experimentally determined structures have not yet been released are published, andcomputational research groups are then invited to predict structures of these target sequences usingtheir methods and any other publicly available information (e.g., known structures that exist in theprotein data bank (pdb), the data repository for protein structures). the methods used by the groupstable 4.3algorithms, databases, analytical systems, and scientific research enabled by the pirresourceresourcetopicreferencealgorithmbenchmarking for sequence similarity searchpearson, j. mol. biol. 276:7184,statistics1998pandora keywordbased analysis of proteinskaplan, nucleic acids research31:56175626, 2003computing motif correlations for structurehorng et al., j. comp. chem.prediction24(16):20322043, 2003databasenesbase database of nuclear export signalsla cour et al., nucleic acidsresearch 31(l):393396, 2003tmpdb database of transmembrane topologiesikeda et al., nucleic acidsresearch 31:406409, 2003sdap database and tools for allergenic proteinsivanciuc et al., nucleic acidsresearch 31:359362, 2003systemspine 2 system for collaborative structuralgoh et al., nucleic acidsproteomicsresearch 31:28332838, 2003ergotm genome analysis and discovery systemoverbeek et al., nucleic acidsresearch 31(l):164171, 2003automated annotation pipeline and cdnakasukawa et al., genome res.annotation system13(6b):15421551, 2003systers, genenest, splicenest from genome tokrause et al., nucleic acidsproteinresearch 30(l):299300, 2002researchintermediate filament proteins duringprasad et al., int. j. oncol.carcinogenesis or apoptosis14(3):563570, 1999conserved pathway by global protein networkkelley et al., pnasalignment100(20):1139411399, 2003membrane targeting of phospholipase csingh and murray, protein sci.pleckstrin12:19341953, 2003analysis of human and mouse cdna sequencesstrausberg et al., pnas99(26):1689916903, 2002a novel schistosoma mansoni g proteincoupledhamdan et al., mol. biochem.receptorparasitol. 119(l):7586, 2002proteomics reveals open reading frames (orfs)jungblut et al., infect. immunol.in mycobacterium tuberculosis69(9):59055907, 2001137see http://predictioncenter.llnl.gov/.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.computational tools105can be divided into three areas depending on the similarity of the target protein to proteins of knownstructure: comparative (also known as homology) modeling, fold recognition (also known as threading), and de novo/new fold methods (also known as ab initio). this traditional division of predictionmethods has become blurred as the methods in each category incorporate detailed information used bymethods in the other categories.in comparative (or homology) modeling, one or more template proteins of known structure withhigh sequence homology (greater than 25 percent sequence identity) to the target sequence are identified. the target and template sequences are aligned through multiple sequence alignment (similar tocomparative genomics), and a threedimensional structure of the target protein is generated from thecoordinates of the aligned residues of the template proteins. finally, the model is evaluated using avariety of criteria, and if necessary, the alignment and the threedimensional model are refined until asatisfactory model is obtained.if no reliable template protein can be identified from sequence homology alone, the predictionproblem is denoted as a fold recognition (or threading) problem. the primary goal is to identify one ormore folds in the template proteins that are consistent with the target sequence. in the classical threading methods, known as òrigid body assembly,ó a model is constructed from a library of known coreregions, loops, side chains, and folds, and the target sequence is then threaded onto the known folds.after evaluating how well the model fits the known folds, the best fit is chosen. the assumption in foldrecognition is that only a finite number of folds exist and most existing folds can be identified fromknown structures in the pdb. indeed, as new sequences are deposited and more protein structures aresolved, there appear to be fewer and fewer unique folds. when two sequences share more than 25percent similarity (or sequence identity), their structures are expected to have similar folds. however,there are still remaining issues such as the high rate of false positives in fold recognition, and therefore,the resulting alignment with the fold structure is poor. at 30 percent sequence identity, the fraction ofincorrectly aligned residues is about 20 percent, and the number rises sharply with further decreases insequence similarity. this limits the usefulness of comparative modeling.138if no template structure (or fold) can be identified with confidence by sequence homology methods,the target sequence may be modeled using new fold prediction methods. the goal in this predictionmethod rests on the biological assumption that proteins adopt their lowest free energy conformation astheir functional state. thus, computational methods to predict structure ab initio comprise three elements: (1) protein geometry, (2) potential energy functions, and (3) an energy space search method(energy minimization method). first, setting protein geometry involves determining the number ofparticles to be used to represent the protein structure (for example, allatom, unitedatom, or virtualatom model) and the nature of the space where atoms can be allocated (e.g., continuous (offlattice) ordiscrete (lattice) model). in a simple ab initio folding such as a virtualatom lattice model, one virtualatom represents a number of atoms in a protein (i.e., the backbone is represented as a sequence of alphacarbons) and an optimization method searches only the predetermined lattice points for positions of thevirtual atoms to minimize the energy functions. second, the potential energy functions in ab initiomodels include covalent terms, such as bond stretching, bond angle stretching, improper dihedrals, andtorsional angles, and noncovalent terms, such as electrostatic and van der waals forces. the use ofmolecular mechanics for refinement in comparative modeling is equivalent to ab initio calculation usingall atoms in an offlattice model. third, many optimizations tools, such as genetic algorithms, montecarlo, simulated annealing, branch and bound, and successive quadratic programming (sqp), havebeen used to search for the global minimum in the energy (or structure) spaces with a number of localminima. these approaches have provided encouraging results, although the performance of eachmethod may be limited by the shape of the energy space.138t. headgordon and j. wooley, òcomputational challenges in structural and functional genomics,ó ibm systems journal40(2):265296, 2001.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.106catalyzing inquirybeyond studies of protein structure is the problem of describing a solvent environment (such aswater) and its influence on a proteinõs conformational behavior. the importance of hydration in proteinstability and folding is widely accepted. models are needed to incorporate the effects of solvents inprotein threedimensional structure.4.4.10 protein identification and quantification from mass spectrometrya second important problem in proteomics is protein identification and quantification. that is,given a particular biological sample, what specific proteins are present and in what quantities? thisproblem is at the heart of studying proteinðprotein interactions at proteomic scale, mapping variousorganelles, and generating quantitative protein profiles from diverse species. making inferences aboutprotein identification and abundance in biological samples is often challenging, because cellularproteomes are highly complex and because the proteome generally involves many proteins at relativelylow abundances. thus, highly sensitive analytical techniques are necessary.today, techniques based on mass spectrometry increasingly fill this need. the mass spectrometerworks on a biological sample in ionized gaseous form. a mass analyzer measures the masstochargeratio (m/z) of the ionized analytes, and a detector measures the number of ions at each m/z value. inthe simplest case, a procedure known as peptide mass fingerprinting (pmf) is used. pmf is based on thefact that a protein is composed of multiple peptide groups, and identification of the complete set ofpeptides will with high probability characterize the protein in question. after enzymatically breakingup the protein into its constituent peptides, the mass spectrometer is used to identify individual peptides, each of which has a known mass. the premise of pmf is that only a very few (one in the ideal case)proteins will correspond to any particular set of peptides, and protein identification is effected byfinding the best fit of the observed peptide masses to the calculated masses derived from, say, a sequence database. of course, the òbest fitó is an algorithmic issue, and a variety of approaches have beentaken to determine the most appropriate algorithms.the applicability of pmf is limited when samples are complex (that is, when they involve largenumbers of proteins at low abundances). the reason is that only a small fraction of the constituentpeptides are typically ionized, and those that are observed are usually from the dominant proteins inthe mixture. thus, for complex samples, multiple (tandem) stages of mass spectrometry may be necessary. in a typical procedure, peptides from a database are scored on the likelihood of their generating atandem mass spectrum, and the top scoring peptide is chosen. this computational approach has showngreat success, and contributed to the industrialization of proteomics.however, much remains to be done. first, the generation of the spectrum is a stochastic processgoverned by the peptide composition, and the mass spectrometer. by mining data to understand thesefragmentation propensities, scoring and identification can be further improved. second, if the peptide isnot in the database, de novo or homologybased methods must be developed for identification. manyproteins are posttranslationally modified, with the modifications changing the mass composition.enumeration and scoring of all modifications leads to a combinatorial explosion that must be addressedusing novel computational techniques. it is fair to say that computation will play an important role inthe success of mass spectrometry as the tool of choice for proteomics.mass spectrometry is also coming into its own for protein expression studies. the major problem hereis that the intensity of a peak depends not only on the peptide abundance, but also on the physicochemical properties of the peptide. this makes it difficult to measure expression levels directly. however,relative abundance can be measured using the proven technique of stableisotope dilution. this methodmakes use of the facts that pairs of chemically identical analytes of different stableisotope compositioncan be differentiated in a mass spectrometer owing to their mass difference, and that the ratio of signalintensities for such analyte pairs accurately indicates the abundance ratio for the two analytes.this approach shows great promise. however, computational methods are needed to correlate dataacross different experiments. if the data were produced using liquid chromatography coupled withcatalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.computational tools107mass spectrometry, a peptide pair could be approximately labeled by its retention time in the column,and its masstocharge ratio. such pairs can be matched across experiments using geometric matching.combining the relative abundance levels from different experiments using statistical methods willgreatly help in improving the reliability of this approach.4.4.11pharmacological screening of potential drug compounds139the national cancer institute (nci) has screened more than 60,000 compounds against a panel of60 human cancer cell lines. the extent to which any single compound inhibits growth in any given cellline is simply one data point relevant to that compoundcell line combinationñnamely the concentration associated with a 50 percent inhibition in the growth of that cell line. however, the pattern of suchvalues across all 60 cell lines can provide insight into the mechanisms of drug action and drug resistance. combined with molecular structure data, these activity patterns can be used to explore the ncidatabase of 460,000 compounds for growthinhibiting effects in these cell lines, and can also provideinsight into potential target molecules and modulators of activity in the 60 cell lines. based on thisapproach, five compounds have been screened in this manner and selected for entry into clinical trials.this approach to drug discovery and molecular pharmacology serves a number of useful functions.according to weinstein et al.,(i)it suggests novel targets and mechanisms of action or modulation.(ii)it detects inhibition of integrated biochemical pathways not adequately represented by any singlemolecule or molecular interaction. (this feature of cellbased assays is likely to be more important in thedevelopment of therapies for cancer than it is for most other diseases; in the case of cancer, one is fightingthe plasticity of a poorly controlled genome and the selective evolutionary pressures for development ofdrug resistance.)(iii)it provides candidate molecules for secondary testing in biochemical assays; conversely, it provides awellcharacterized biological assay in vitro for compounds emerging from biochemical screens.(iv)it ôôfingerprintsõõ tested compounds with respect to a large number of possible targets and modulators of activity.(v)it provides such fingerprints for all previously tested compounds whenever a new target is assessedin many or all of the 60 cell lines. (in contrast, if a battery of assays for different biochemical targets wereapplied to, for example, 60,000 compounds, it would be necessary to retest all of the compounds for anynew target or assay.)(vi)it links the molecular pharmacology with emerging databases on molecular markers in microdissected human tumorsñwhich, under the rubric of this article, constitute clinical (c) databases.(vii)it provides the basis for pharmacophore development and searches of an s [structure] database foradditional candidates. if an agent with a desired action is already known, its fingerprint patterns ofactivity can be used by . . . [various] patternrecognition technologies to find similar compounds.box 4.6 provides an example of this approach.4.4.12 algorithms related to imagingbiological science is rich in images. most familiar are images taken through optical microscopes, butthere are many other imaging modalitiesñelectron microscopes, computed tomography scans, xrays,magnetic resonance imaging, and so on. for most of the history of life science research, images have139section 4.4.11 is based heavily on j.n. weinstein, t.g. myers, p.m. oõconnor, s.h. friend, a.j. fornace, jr., k.w. kohn, t.fojo, et al., òan informationintensive approach to the molecular pharmacology of cancer,ó science 275(5298):343349, 1997.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.108catalyzing inquirybeen a source of qualitative insight.140 while this is still true, there is growing interest in using imagedata more quantitatively.consider the following applications:¥automated identification of fungal spores in microscopic digital images and automated estimation of spore density;141¥automated analysis of liver mri images from patients with putative hemochromatosis to determine the extent of iron overload, avoiding the need for an uncomfortable liver biopsy;142box 4.6an informationintensive approach to cancer drug discoverygiven one compound as a òseed,ó [an algorithm known as] compare searches the database of screened agents forthose most similar to the seed in their patterns of activity against the panel of 60 cell lines. similarity in pattern oftenindicates similarity in mechanism of action, mode of resistance, and molecular structure. . . .a formulation of this approach in terms of three databases [includes databases for] the activity patterns [a], . . .molecular structural features of the tested compounds [s], and . . . possible targets or modulators of activity in thecells [t]. . . . the (s) database can be coded in terms of any set of twodimensional (2d) or 3d molecular structuredescriptors. the nciõs drug information system (dis) contains chemical connectivity tables for approximately460,000 molecules, including the 60,000 tested to date. 3d structures have been obtained for 97% of the discompounds, and a set of 588 bitwise descriptors has been calculated for each structure by use of the chemxcomputational chemistry package. this data set provides the basis for pharmacophoric searches; if a tested compound, or set of compounds, is found to have an interesting pattern of activity, its structure can be used to search forsimilar molecules in the dis database.in the target (t) database, each row defines the pattern (across 60 cell lines) of a measured cell characteristic that maymediate, modulate, or otherwise correlate with the activity of a tested compound. when the term is used in this generalshorthand sense, a òtargetó may be the site of action or part of a pathway involved in a cellular response. among thepotential targets assessed to date are oncogenes, tumorsuppressor genes, drug resistancemediating transporters, heatshock proteins, telomerase, cytokine receptors, molecules of the cell cycle and apoptotic pathways, dna repair enzymes, components of the cytoarchitecture, intracellular signaling molecules, and metabolic enzymes.in addition to the targets assessed one at a time, others have been measured en masse as part of a protein expressiondatabase generated for the 60 cell lines by 2d polyacrylamide gel electrophoresis.each compound displays a unique òfingerprintó pattern, defined by a point in the 60d space (one dimension for eachcell line) of possible patterns. in information theoretic terms, the transmission capacity of this communication channel is very large, even after one allows for experimental noise and for biological realities that constrain the compounds to particular regions of the 60d space. although the activity data have been accumulated over a 6yearperiod, the experiments have been reproducible enough to generate . . . patterns of coherence.source: reprinted by permission from j.n. weinstein, t.g. myers, p.m. oõconnor, s.h. friend, a.j. fornace, jr., k.w. kohn, t. fojo, etal., òan informationintensive approach to the molecular pharmacology of cancer,ó science 275(5298):343349, 1997. copyright 1997aaas.140note also that biological imaging itself is a subset of the intersection between biology and visual techniques. in particular, otherbiological insight can be found in techniques that consider spectral information, e.g., intensity as a function of frequency and perhaps afunction of time. processing microarray data (discussed further in section 7.2.1) ultimately depends on the ability to extract interestingsignals from patterns of fluorescing dots, as does quantitative comparison of patterns obtained in twodimensional polyacrylamide gelelectrophoresis. (see s. veeser, m.j. dunn, and g.z. yang, òmultiresolution image registration for twodimensional gel electrophoresis,ó proteomics 1(7):856870, 2001, available at http://vip.doc.ic.ac.uk/2dgel/2dgelfinalrevision.pdf.)141t. bernier and j.a. landry, òalgorithmic recognition of biological objects,ó canadian agricultural engineering 42(2):101109, 2000.142george reeke, rockefeller university, personal communication to john wooley, october 8, 2004.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.computational tools109¥fluorescent speckle microscopy, a technique for quantitatively tracking the movement, assembly, and disassembly of macromolecules in vivo and in vitro, such as those involved in cytoskeletondynamics;143and¥establishing metrics of similarity between brain images taken at different times.144these applications are only an infinitesimal fraction of those that are possible. several researchareas associated with increasing the utility of biological images are discussed below. box 4.7 describesthe open microscopy environment, an effort intended to automate image analysis, modeling, and mining of large sets of biological images obtained from optical microscopy.145as a general rule, biologists need to develop better imaging methods that are applicable across theentire spatial scale of interest, from the subcellular to the organismal. (in this context, òbetteró meansimaging that occurs in real time (or nearly so) with the highest possible spatial and temporal resolution.)these methods will require new technologies (such as the multiphoton microscope) and also newprotein and nonprotein reporter molecules that can be expressed or introduced into cells or organisms.143c.m. watermanstorer and g. danuser, ònew directions for fluorescent speckle microscopy,ó current biology 12(18):r633r640, 2002.144m.i. miller, a. trouve, and l. younes, òon the metrics and eulerlagrange equations of computational anatomy,ó annualreview of biomedical engineering 4:375405, 2002, available at http://www.cis.jhu.edu/publications/papersindatabase/eulerlagrangeeqnscompuanatomy.pdf.145j.r. swedlow, i. goldberg, e. brauner, and p.k. sorger, òinformatics and quantitative analysis in biological imaging,óscience 300(5616):100102, 2003.box 4.7the open microscopy environment1responding to the need to manage a large number of multispectral movies of mitotic cells in the late 1990s,sorger and swedlow began work on the open microscopy environment (ome). the ome is designed asinfrastructure that manages optical microscopy images, storing both the primary image data and appropriatemetadata on those images, including data on the optics of the microscope, the experimental setup and sample, and information derived by analysis of the images. ome also permits data federation that allows information from multiple sources (e.g., genomic or chemical databases) to be linked to image records.in addition, the ome provides an extensible environment that enables users to write their own applications forimage analysis. consider, for example, the task of tracking labeled vesicles in a timelapse movie. as noted byswedlow et al., this problem requires the following: a segmentation algorithm to find the vesicles and toproduce a list of centroids, volumes, signal intensities, and so on; a tracker to define trajectories by linkingcentroids at different time points according to a predetermined set of rules; and a viewer to display the analyticresults overlaid on the original movie.2ome provides a mechanism for linking together various analytical modules by specifying data semantics thatenable the output of one module to be accepted as input to another. these semantic data types of omedescribe analytic results such as òcentroid,ó òtrajectory,ó and òmaximum signa,ó and allow users, rather thana predefined standard, to define such concepts operationally, including in the machinereadable definitionand the processing steps that produce it (e.g., the algorithm and the various parameter settings used).1see www.openmicroscopy.org.2j.r. swedlow, i. goldberg, e. brauner, and p.k. sorger, òinformatics and quantitative analysis in biological imaging,ó science300(5616):100102, 2003.source: based largely on the paper by swedlow et al. cited in footnote145 and on the ome web page at www.openmicroscopy.org.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.110catalyzing inquirythe discussion below focuses only on a narrow slice of the very general problem of biological imaging,as a broader discussion would go beyond the scope of this report.4.4.12.1 image rendering146images have been central to the study of biological phenomena ever since the invention of themicroscope. today, images can be obtained from many sources, including tomography, mri, xrays,and ultrasound. in many instances, biologists are interested in the spatial and geometric properties ofcomponents within a biological entity. these properties are often most easily understood when viewedthrough an interactive visual representation that allows the user to view the entity from different anglesand perspectives. moreover, a single analysis or visualization session is often not sufficient, and processing across many image volumes is often required.the requirement that a visual representation be interactive places enormous demands on thecomputational speed of the imaging equipment in use. today, the data produced by imaging equipment are quickly outpacing the capabilities offered by the image processing and analysis softwarecurrently available. for example, the ge evsrs9 ct scanner is able to generate image volumes withresolutions in the 2090 mm range, which results in a dataset size of multiple gigabytes. datasets ofsuch size require software tools specifically designed for the imaging datasets of today and tomorrow(see figure 4.5) so that researchers can identify subtle features that can otherwise be missed or misrepresented. also with increasing dataset resolution comes increasing dataset size, which translates directly to lengthening dataset transfer, processing, and visualization times.new algorithms that take advantage of stateoftheart hardware in both relatively inexpensiveworkstations and multiprocessor supercomputers must be developed and moved into easytoaccesssoftware systems for the clinician and researcher. an example is raytracing, a method commonly usedin computer graphics that supports highly efficient implementations on multiple processors for interactive visualization. the resulting volume rendition permits direct inspection of internal structures,without a precomputed segmentation or surface extraction step, through the use of multidimensionaltransfer functions. as seen in the visualizations in figure 4.6, the resolution of the ct scan allowssubtleties such as the definition of the cochlea, the modiolus, the implanted electrode array, and the leadwires that connect the array to a headmounted connector. the colinear alignment of the path of thecochlear nerve with the location of the electrode shanks and tips is the necessary visual confirmation ofthe correct surgical placement of the electrode array.in both of the studies described in figure 4.5 and figure 4.6, determination of threedimensionalstructure and configuration played a central role in biological inquiry. volume visualization createddetailed renderings of changes in bone morphology due to a pax3 mutation in mice, and it providedvisual confirmation of the precise location of an electrode array implanted in the feline skull. thescientific utility of volume visualization will benefit from further improvements in its interactivity andflexibility, as well as simultaneous advances in highresolution image acquisition and the developmentof volumetric imageprocessing techniques for better feature extraction and enhancement.4.4.12.2 image segmentation147an important problem in automated image analysis is image segmentation. digital images arerecorded as a set of pixels in a two or threedimensional array. images that represent natural scenesusually contain different objects, so that, for example, a picture of a park may depict people, trees, and146section 4.4.12.1 is based on material provided by chris johnson, university of utah.147section 4.4.11.2 is adapted from and includes excerpts from national research council, mathematics and physics of emergingbiomedical imaging, national academy press, washington, dc, 1996.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.computational tools111figure 4.5visualizations of mutant (left) and normal (right) mice embryos.ct values are inspected by maximum intensity projection in (a) and with standard isosurface rendering in (b).volume rendering (c) using multidimensional opacity functions allows more accurate bone emphasis, depth cueing, and curvaturebased transfer functions to enhance bone contours in image space. in this case, drs. keller andcapecchi are investigating the birth defects caused by a mutation in the pax3 gene, which controls musculoskeletaldevelopment in mammalian embryos. in their model, they have activated a dominantly acting mutant pax3 geneand have uncovered two of its effects: (1) abnormal formation of the bones of the thoracolumbar spine andcartilaginous rib cage and (2) cranioschisis, a more drastic effect in which the dermal and skeletal covering of thebrain is missing. imaging of mutant and normal mouse embryos was performed at the university of utah smallanimal imaging facility, producing two 1.2 gb 16bit volumes of 769 × 689 × 1173 samples, with resolution of 21 ×21 × 21 microns.source: courtesy of chris johnson, university of utah; see also http://www.sci.utah.edu/stories/2004/sprimaging.html.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.112catalyzing inquirybenches. similarly, a scanned image of a magazine page may contain text and graphics (e.g., a picture ofa park). segmentation refers to the process by which an object (or characteristics of the object) in animage is extracted from image data for purposes of visualization and measurement. (extraction meansthat the pixels associated with the object of interest are isolated.) in a biological context, a typicalproblem in image segmentation might involve extracting different organs in a ct scan of the body.segmentation research involves the development of automatic, computerexecutable rules that canisolate enough of these pixels to produce an acceptably accurate segmentation. segmentation is a centralproblem of image analysis because segmentation must be accomplished before many other interesting(a) (b)(c)figure 4.6volume renderings of electrode array implanted in feline skull.in this example, scanning produced a 131 mb 16bit volume of 425 × 420 × 385 samples, with resolution of 21 ×21 × 21 microns. renderings of the volume were generated using a raytracing algorithm across multiple processors allowing interactive viewing of this relatively large dataset. the resolution of the scan allows definition of theshanks and tips of the implanted electrode array. volumetric image processing was used to isolate the electrodearray from the surrounding tissue, highlighting the structural relationship between the implant and the bone.there are distinct ct values for air, soft tissue, bone, and the electrode array, enabling the use of a combination ofray tracing and volume rendering to visualize the array in the context of the surrounding structures, specificallythe bone surface. the volume is rotated gradually upward in columns (a), (b), and (c), from seeing the side of thecochlea exterior in (a), to looking down the path of the cochlear nerve in (c). from top to bottom, each row usesdifferent rendering styles: (1), summation projections of ct values (green) and gradients (magenta); (2), volumerenderings with translucent bone, showing the electrode leads in magenta.source: courtesy of chris johnson, university of utah; see also http://www.sci.utah.edu/stories/2004/sprimaging.html.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.computational tools113problems in image analysis can be solved, including image registration, shape analysis, and volume andarea estimation. a specific laboratory example would be the segmentation of spots on twodimensionalelectrophoresis gels.there is no common method or class of methods applicable to even the majority of images. segmentation is easiest when the objects of interest have intensity or edge characteristics that allow them to beseparated from the background and noise, as well as from each other. for example, an mri image of thehuman body would be relatively easy to segment for bones: all pixels with intensity below a giventhreshold would be eliminated, leaving mostly the pixels associated with highsignalintensity bone.generally, edge detection depends on a search for intensity gradients. however, it is difficult to findgradients when, as is usually the case in biomedical images, intensities change only gradually betweenthe structure of interest and the surrounding structure(s) from which it is to be extracted. continuityand connectivity are important criteria for separating objects from noise and have been exploited quitewidely.a number of different approaches to image segmentation are described in more detail by pham et al.1484.4.12.3 image registration149different modes of imaging instrumentation may be used on the same object because they aresensitive to different object characteristics. for example, an xray of an individual will produce differentinformation than a ct scan. for various purposes, and especially for planning surgical and radiationtreatment, it can be important for these images to be aligned with each other, that is, for informationfrom different imaging modes to be displayed in the same locations. this process is known as imageregistration.there are a variety of techniques for image registration, but in general they can be classified basedon the features that are being matched. for example, such features may be external markers that arefixed (e.g., on a patientõs body), internal anatomic markers that are identifiable on all images, the centerof gravity for one or more objects in the images, crestlines of objects in the images, or gradients ofintensity. another technique is minimization of the distance between corresponding surface points of apredefined object. image registration often depends on the identification of similar structures in theimages to be registered. in the ideal case, this identification can be performed through an automatedsegmentation process.image registration is well defined for rigid objects but is more complicated for deformable objects orfor objects imaged from different angles. when soft tissue deforms (e.g., because a patient is lying on hisside rather than on his back), elastic warping is required to transform one dataset into the other. thedifficulty lies in defining enough common features in the images to enable specifying appropriate localdeformations.an example of an application in which image registration is important is the cellcentered database(ccdb).150 launched in 2002, the ccdb contains structural and protein distribution information derivedfrom confocal, multiphoton, and electron microscopy for use by the structural biology and neurosciencecommunities. in the case of neurological images, most of the imaging data are referenced to a higher levelof brain organization by registering their location in the coordinate system of a standard brain atlas.placing data into an atlasbased coordinate system provides one method by which data taken across scales148d.l. pham, c. xu, and j.l. prince, òcurrent methods in medical image segmentation,ó annual review of biomedical engineering 2:315338, 2000.149section 4.4.12.3 is adapted from national research council, mathematics and physics of emerging biomedical imaging, nationalacademy press, washington, dc, 1996.150see m.e. martone, s.t. peltier, and m.h. ellisman, òbuilding grid based resources for neurosciences,ó unpublished paper2003, national center for microscopy and imaging research, department of neurosciences, university of california, san diego,san diego, ca, and http://ccdb.ucsd.edu/ccdb/about.shtml.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.114catalyzing inquiryand distributed across multiple resources can be compared reliably. through the use of atlases and toolsfor surface warping and image registration, it is possible to express the location of anatomical features orsignals in terms of a standardized and quantitative coordinate system, rather by using terms that describeobjects in the field of view. the expression of brain data in terms of atlas coordinates also allows it to betransformed spatially to provide alternative views that may offer additional information (e.g., flat maps oradditional parcellation schemes). finally, a standard coordinate system allows the same brain region to besampled repeatedly to allow data to be accumulated over time.4.4.12.4 image classificationimage classification is the process through which a set of images can be sorted into meaningfulcategories. categories can be defined through lowlevel features such as color mix and texture patterns orthrough highlevel features such as objects depicted. as a rule, lowlevel features can be computed withlittle difficulty, and a number of systems have been developed that take advantage of such features.151however, users are generally much more interested in semantic content that is not easily represented in such lowlevel features. the easiest method to identify interesting semantic content is simplyto annotate an image manually with text, although this process is quite tedious and is unlikely tocapture the full range of content in an image. thus, automated techniques hold considerable interest.the general problem of automatic identification of such image content has not been solved. oneapproach described by huang et al. relies on supervised learning to classify images hierarchically.152this approach relies on using good lowlevel features and then performing featurespace reconfigurationusing singular value decomposition to reduce noise and dimensionality. a hierarchical classificationtree can be generated from training data and subsequently used to sort new images into categories.a second approach is based on the fact that biological images often contain branching structures. (for example, both muscle and neural tissue contain blood vessels and dendrites that arefound in branching structures.) the fractal dimensionality of such structures can then be used as ameasure of similarity, and images that contain structures of similar fractal dimension can begrouped into categories.1534.5 developing computational toolsthe computational tools described above were once gleams in the eye of some researcher. despitethe joy and satisfaction felt when a prototype program supplies the first useful results to its developer,it is a long, long way to converting that program into a genuine product that is general, robust, anduseful to others. indeed, in his classic text the mythical manmonth (addisonwesley, reading, ma,1995), frederick p. brooks, jr., estimates the difference in effort necessary to create a programmingsystems product from a program as an order of magnitude.some of the software engineering considerations necessary to turn a program into a product includethe following:¥quality. the program, of course, must be as free of defects as possible, not only in the sense ofrunning without faults, but also of precisely implementing the stated algorithm. it must be tested for all151see, for example, m. flickner, h. sawhney, w. niblack, j. ashley, q. huang, b. dom, m. gorkani, j. hafner, d. lee, d.petkovic, d. steele, and p. yanker, òquery by image and video content: the qbic system,ó ieee computer 28(9):2332, 1995,available at http://wwwqbic.almaden.ibm.com/.152j. huang, s.r. kumar, and r. zabih, òan automatic hierarchical image classification scheme,ó acm conference onmultimedia, bristol, england, september 1998. a revised version appears in eurasip journal on applied signal processing, 2003,available at http://www.cs.cornell.edu/rdz/papers/archive/mm98.pdf.153d. cornforth, h. jelinek, and l. peich, òfractop: a tool for automated biological image classification,ó available at http://csu.edu.au/~dcornfor/fractopv7.pdf.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.computational tools115potential inputs, and combinations of factors, and must be robust even in the face of invalid usage. theprogram should have wellunderstood and bounded resource demands, including memory, inputoutput, and processing time.¥maintenance. when bugs are discovered, they must be tracked, patched, and provided to users.this often means that the code should be structured for maintainability; for example, perl, which isextremely powerful, is often written in a way that is incomprehensible to programmers other than theauthor (and often even to the author). differences in functionality between versions must be documented carefully.¥documentation. if the program is to be usable by others, all of the functionality must be clearlydocumented, including data file formats, configuration options, output formats, and of course programusage. if the source code of the program is made available (as is often the case with scientific tools), thecode must be documented in such a way that users can check the validity of the implementation as wellas alter it to meet their needs.¥user interface. the program must have a user interface, although not necessarily graphical, that isunambiguous and able to access the full range of functions of the program. it should be easy to use,difficult to make mistakes, and clear in its instructions and display of state.¥system integration and portability. the program must be distributed to users in a convenient way,and be able to run on different platforms and operating systems in a way that does not interfere withexisting software or system settings. it should be easily configurable and customizable for particularrequirements, and should install easily without access to specialized software, such as nonstandardcompilers.¥general. the program should accept a wide selection of data types, including common formats,units, precisions, ranges, and file sizes. the internal coding interfaces should have precisely definedsyntax and semantics, so that users can easily extend the functionality or integrate it into other tools.tool developers address these considerations to varying degrees, and users may initially be moretolerant of something that is more program than product if the functionality it confers is essential andunique. over time, however; such programs will eventually become more productlike because userswill not tolerate significant inconvenience.finally, there is an issue of development methodology. a proprietary approach to development canbe adopted for a number of competitive reasons, ranging from the ultimate desire to reap financialbenefit to staying ahead of competing laboratories. under a proprietary approach, source code for thetools would be kept private, so that potential competitors would be unable to exploit the code easily fortheir own purposes. (source code is needed to make changes to a program.) an open approach todevelopment calls for the source code to be publicly available, on the theory that broad communityinput strengthens the utility of the tools being made available and better enables one team to build onanother teamõs work.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.computational modeling and simulation as enablers for biological discovery1171175computational modeling and simulation asenablers for biological discoverywhile the previous chapter deals with the ways in which computers and algorithms could supportexisting practices of biological research, this chapter introduces a different type of opportunity. thequantities and scopes of data being collected are now far beyond the capability of any human, or teamof humans, to analyze. and as the sizes of the datasets continue to increase exponentially, even existingtechniques such as statistical analysis begin to suffer. in this datarich environment, the discovery oflargescale patterns and correlations is potentially of enormous significance. indeed, such discoveriescan be regarded as hypotheses asserting that the pattern or correlation may be importantña mode ofòdiscovery scienceó that complements the traditional mode of science in which a hypothesis is generated by human beings and then tested empirically.for exploring this datarich environment, simulations and computerdriven models of biologicalsystems are proving to be essential.5.1on models in biologyin all sciences, models are used to represent, usually in an abbreviated form, a more complex anddetailed reality. models are used because in some way, they are more accessible, convenient, or familiarto practitioners than the subject of study. models can serve as explanatory or pedagogical tools, represent more explicitly the state of knowledge, predict results, or act as the objects of further experiments.most importantly, a model is a representation of some reality that embodies some essential and interesting aspects of that reality, but not all of it.because all models are by definition incomplete, the central intellectual issue is whether the essential aspects of the system or phenomenon are well represented (the term òessentialó has multiplemeanings depending on what aspects of the phenomenon are of interest). in biological phenomena,what is interesting and significant is usually a set of relationshipsñfrom the interaction of two molecules to the behavior of a population in its environment. human comprehension of biological systemsis limited, among other things, by that very complexity and by the problems that arise when attemptingto dissect a given system into simpler, more easily understood components. this challenge is compounded by our current inability to understand relationships between the components as they occur inreality, that is, in the presence of multiple, competing influences and in the broader context of time andspace.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.118catalyzing inquirydifferent fields of science have traditionally used models for different purposes; thus, the nature ofthe models, the criteria for selecting good or appropriate models, and the nature of the abbreviation orsimplification have varied dramatically. for example, biologists are quite familiar with the notion ofmodel organisms.1 a model organism is a species selected for genetic experimental analysis on thebasis of experimental convenience, homology to other species (especially to humans), relative simplicity, or other attractive attributes. the fruit fly drosophila melanogaster is a model organism attractive atleast in part because of its short generational time span, allowing many generations in the course of anexperiment.at the most basic level, any abstraction of some biological phenomenon counts as a model. indeed,the cartoons and block diagrams used by most biologists to represent metabolic, signaling, or regulatorypathways are modelsñqualitative models that lay out the connectivity of elements important to thephenomenon. such models throw away details (e.g., about kinetics) implicitly asserting that omission ofsuch details does not render the model irrelevant.a second example of implicit modeling is the use of statistical tests by many biologists. all statistical tests are based on a null hypothesis, and all null hypotheses are based on some kind of underlyingmodel from which the probability distribution of the null hypothesis is derived. even those biologistswho have never thought of themselves as modelers are using models whenever they use statistical tests.mathematical modeling has been an important component of several biological disciplines formany decades. one of the earliest quantitative biological models involved ecology: the lotkavolterramodel of species competition and predatorprey relationships described in section 5.2.4. in the contextof cell biology, models and simulations are used to examine the structure and dynamics of a cell ororganismõs function, rather than the characteristics of isolated parts of a cell or organism.2 such modelsmust consider stochastic and deterministic processes, complex pleiotropy, robustness through redundancy, modular design, alternative pathways, and emergent behavior in biological hierarchy.in a cellular context, one goal of biology is to gain insight into the interactions, molecular orotherwise, that are responsible for the behavior of the cell. to do so, a quantitative model of the cellmust be developed to integrate global organismwide measurements taken at many different levels ofdetail.the development of such a model is iterative. it begins with a rough model of the cell, based onsome knowledge of the components of the cell and possible interactions among them, as well as priorbiochemical and genetic knowledge. although the assumptions underlying the model are insufficientand may even be inappropriate for the system being investigated, this rough model then provides azerothorder hypothesis about the structure of the interactions that govern the cellõs behavior.implicit in the model are predictions about the cellõs response under different kinds of perturbation.perturbations may be genetic (e.g., gene deletions, gene overexpressions, undirected mutations) orenvironmental (e.g., changes in temperature, stimulation by hormones or drugs). perturbations areintroduced into the cell, and the cellõs response is measured with tools that capture changes at therelevant levels of biological information (e.g., mrna expression, protein expression, protein activationstate, overall pathway function). box 5.1 provides some additional detail on cellular perturbations.the next step is comparison of the modelõs predictions to the measurements taken. this comparisonindicates where and how the model must be refined in order to match the measurements more closely.if the initial model is highly incomplete, measurements can be used to suggest the particular components required for cellular function and those that are most likely to interact. if the initial model isrelatively well defined, its predictions may already be in good qualitative agreement with measurement, differing only in minor quantitative ways. when model and measurement disagree, it is often1see, for example, http://www.nih.gov/science/models for more information on model organisms.2section 5.1 draws heavily on excerpts from t. ideker, t. galitski, and l. hood, òa new approach to decoding life: systemsbiology,ó annual review of genomics and human genetics 2:343372, 2001; and h. kitano, òsystems biology: a brief overview,óscience 295(5560):16621664, 2002.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.computational modeling and simulation as enablers for biological discovery119necessary to create a number of more refined models, each incorporating a different mechanism underlying the discrepancies in measurement.with the refined model(s) in hand, a new set of perturbations can be applied to the cell. note thatnew perturbations are informative only if they elicit different responses between models, and they aremost useful when the predictions of the different models are very different from one another. nevertheless, a new set of perturbations is required because the predictions of the refined model(s) will generallyfit well with the old set of measurements.the refined model that best accounts for the new set of measurements can then be regarded as theinitial model for the next iteration. through this process, model and measurement are intended toconverge in such a way that the modelõs predictions mirror biological responses to perturbation. modeling must be connected to experimental efforts so that experimentalists will know what needs to bedetermined in order to construct a comprehensive description and, ultimately, a theoretical frameworkfor the behavior of a biological system. feedback is very important, and it is this feedback, along withthe globalñor, loosely speaking, genomicscaleñnature of the inquiry that characterizes much of 21stcentury biology.5.2why biological models can be usefulin the last decade, mathematical modeling has gained stature and wider recognition as a useful toolin the life sciences. most of this revolution has occurred since the era of the genome, in which biologistswere confronted with massive challenges to which mathematical expertise could successfully be broughtto bear. some of the success, though, rests on the fact that computational power has allowed scientists toexplore ever more complex models in finer detail. this means that the mathematicianõs talent forabstraction and simplification can be complemented with realistic simulations in which details notamenable to analysis can be explored. the visual realtime simulations of modeled phenomena givebox 5.1perturbation of biological systemsperturbation of biological systems can be accomplished through a number of genetic mechanisms, such as thefollowing:¥highthroughput genomic manipulation. increasingly inexpensive and highly standardized tools are available that enable the disruption, replacement, or modification of essentially any genomic sequence. furthermore, these tools can operate simultaneously on many different genomic sequences.¥systematic gene mutations. although random gene mutations provide a possible set of perturbations, therandom nature of the process often results in nonuniform coverage of possible genotypesñsome genes aretargeted multiple times, others not at all. a systematic approach can cover all possible genotypes and thecoverage of the genome is unambiguous.¥gene disruption. while techniques of genomic manipulation and systematic gene mutation are often useful in analyzing the behavior of model organisms such as yeast, they are not practical for application toorganisms of greater complexity (i.e., higher eukaryotes). on the other hand, it is often possible to inducedisruptions in the function of different genes, effectively silencing (or deleting) them to produce a biologicallysignificant perturbation.source: adapted from t. ideker, t. galitski, and l. hood, òa new approach to decoding life: systems biology,ó annual review ofgenomics and human genetics 2:343372, 2001.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.120catalyzing inquirymore compelling and more accessible interpretations of what the models predict.3 this has made iteasier to earn the recognition of biologists.on the other hand, modelingñespecially computational modelingñshould not be regarded as anintellectual panacea, and models may prove more hindrance than help under certain circumstances. inmodels with many parameters, the state space to be explored may grow combinatorially fast so that noamount of data and brute force computation can yield much of value (although it may be the case thatsome algorithm or problemrelated insight can reduce the volume of state space that must be exploredto a reasonable size). in addition, the behavior of interest in many biological systems is not characterizedas equilibrium or quasisteadystate behavior, and thus convergence of a putative solution may neverbe reached. finally, modeling presumes that the researcher can both identify the important state variables and obtain the quantitative data relevant to those variables.4computational models apply to specific biological phenomena (e.g., organisms, processes) and areused for a number of purposes as described below.5.2.1models provide a coherent framework for interpreting dataa biologist surveys the number of birds nesting on offshore islands and notices that the numberdepends on the size (e.g., diameter) of the island: the larger the diameter d, the greater is the number ofnests n. a graph of this relationship for islands of various sizes reveals a trend. here the mathematicallyinformed and uninformed part ways: simple linear leastsquares fit of the data misses a central point.a trivial ònull modeló based on an equal subdivision of area between nesting individuals predicts thatn~ d2, (i.e., the number of nests should be roughly proportional to the square of island area). this simplegeometric property relating area to population size gives a strong indication of the trend researchersshould expect to see. departures from this trend would indicate that something else may be important.(for example, different parts of islands are uninhabitable, predators prefer some islands to others, andso forth.)although the above example is elementary, it illustrates the idea that data are best interpretedwithin a context that shapes oneõs expectations regarding what the data òoughtó to look like; often amathematical (or geometric) model helps to create that context.5.2.2models highlight basic concepts of wide applicabilityamong the earliest applications of mathematical ideas to biology are those in which populationlevels were tracked over time and attempts were made to understand the observed trends. malthusproposed in 1798 the fitting of population data to exponential growth curves following his simplemodel for geometric growth of a population.5 the idea that simple reproductive processes produce3as one example, ramon felciano studied the use of òdomain graphicsó by biologists. felciano argued that certain visualrepresentations (known as domain graphics) become so ingrained in the discourse of certain subdisciplines of biology that theybecome good targets for user interfaces to biological data resources. based on this notion, felciano constructed a reusableinterface based on the standard twodimensional layout of rna secondary structure. see r. felciano, r. chen, and r. altman,òrna secondary structure as a reusable interface to biological information resources,ó gene 190:5970, 1997.4in some cases, obtaining the quantitative data is a matter of better instrumentation and higher accuracy. in other cases, thedata are not available in any meaningful sense of practice. for example, richard lewontin notes that the probability of survivalps of a particular genotype is an ensemble property, rather than the property of a single individual who either will or will notsurvive. but if what is of interest is ps as a function of the alternative genotypes deriving from a single locus, the effects of theimpacts deriving from other loci must be randomized. however, in sexually reproducing organisms, there is no way known toproduce an ensemble of individuals that are all identical with respect to a single locus but randomized over other loci. thus, aquantitative characterization of ps is in practice not possible, and no alternative measurement technologies will be of much valuein solving this problem. see r. lewontin, the genetic basis of evolutionary change, columbia university press, new york, 1974.5t.r. malthus, an essay on the principle of population, first edition, e.a. wrigley and d. souden, eds., penguin books,harmondsworth, england, 1798.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.computational modeling and simulation as enablers for biological discovery121exponential growth (if birth rates exceed mortality rates) or extinction (in the opposite case) is a fundamental principle: its applicability in biology, physics, chemistry, as well as simple finance, is central.an important refinement of the malthus model was proposed in 1838 to explain why most populations do not experience exponential growth indefinitely. the refinement was the idea of the densitydependent growth law, now known as the logistic growth model.6 though simple, the verhulst modelis still used widely to represent population growth in many biological examples. both malthus andverhulst models relate observed trends to simple underlying mechanisms; neither model is fully accurate for real populations, but deviations from model predictions are, in themselves, informative, because they lead to questions about what features of the real systems are worthy of investigation.more recent examples of this sort abound. nonlinear dynamics has elucidated the tendency ofexcitable systems (cardiac tissue, nerve cells, and networks of neurons) to exhibit oscillatory, burst, andwavelike phenomena. the understanding of the spread of disease in populations and its sensitivedependence on population density arose from simple mathematical models. the same is true of thediscovery of chaos in the discrete logistic equation (in the 1970s). this simple model and its mathematical properties led to exploration of new types of dynamic behavior ubiquitous in natural phenomena.such biologically motivated models often crossfertilize other disciplines: in this case, the phenomenonof chaos was then found in numerous real physical, chemical, and mechanical systems.5.2.3models uncover new phenomena or concepts to exploresimple conceptual models can be used to uncover new mechanisms that experimental science hasnot yet encountered. the discovery of chaos mentioned above is one of the clearest examples of thiskind. a second example of this sort is turingõs discovery that two chemicals that interact chemically ina particular way (activate and inhibit one another) and diffuse at unequal rates could give rise to òpeaksand valleysó of concentration. his analysis of reactiondiffusion (rd) systems showed precisely whatranges of reaction rates and rates of diffusion would result in these effects, and how properties of thepattern (e.g., distance between peaks and valleys) would depend on those microscopic rates. laterresearch in the mathematical community also uncovered how other interesting phenomena (travelingwaves, oscillations) were generated in such systems and how further details of patterns (spots, stripes,etc.) could be affected by geometry, boundary conditions, types of chemical reactions, and so on.turingõs theory was later given physical manifestation in artificial chemical systems, manipulatedto satisfy the theoretical criteria of pattern formation regimes. and, although biological systems did notproduce simple examples of rd pattern formation, the theoretical framework originating in this workmotivated later more realistic and biologically based modeling research.5.2.4models identify key factors or components of a systemsimple conceptual models can be used to gain insight, develop intuition, and understand òhowsomething works.ó for example, the lotkavolterra model of species competition and predatorprey7 islargely conceptual and is recognized as not being very realistic. nevertheless, this and similar modelshave played a strong role in organizing several themes within the discipline: for example, competitiveexclusion, the tendency for a species with a slight advantage to outcompete, dominate, and take overfrom less advantageous species; the cycling behavior in predatorprey interactions; and the effect of6p.f. verhulst, ònotice sur la loi que la population suit dans son accroissement,ó correspondence math”matique et physique, 1838.7a.j. lotka, elements of physical biology, williams & wilkins co., baltimore, md, 1925; v. volterra, òvariazioni e fluttuazionidel numero dõindividui in specie animali conviventi,ó mem. r. accad. naz. dei lincei., ser. vi, vol. 2, 1926. the lotkavolterramodel is a set of coupled differential equations that relate the densities of prey and predator given parameters involving thepredatorfree rate of prey population increase, the normalized rate at which predators can successfully remove prey from thepopulation, the normalized rate at which predators reproduce, and the rate at which predators die.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.122catalyzing inquiryresource limitations on stabilizing a population that would otherwise grow explosively. all of theseconcepts arose from mathematical models that highlighted and explained dynamic behavior within thecontext of simple models. indeed, such models are useful for helping scientists to recognize patternsand predict system behavior, at least in gross terms and sometimes in detail.5.2.5models can link levels of detail (individual to population)biological observations are made at many distinct hierarchies and levels of detail. however, thelinks between such levels are notoriously difficult to understand. for example, the behavior of singleneurons and their response to inputs and signaling from synaptic connections might be well known.the behavior of a large assembly of such neurons in some part of the central nervous system can beobserved macroscopically by imaging or electrode recording techniques. however, how the two levelsare interconnected remains a massive challenge to scientific understanding. similar examples occur incountless settings in the life sciences: due to the complexity of nonlinear interactions, it is nearly impossible to grasp intuitively how collections of individuals behave, what emergent properties of thesegroups arise, or the significance of any sensitivity to initial conditions that might be magnified at higherlevels of abstraction. some mathematical techniques (averaging methods, homogenization, stochasticmethods) allow the derivation of macroscopic statements based on assumptions at the microscopic, orindividual, level. both modeling and simulation are important tools for bridging this gap.5.2.6models enable the formalization of intuitive understandingsmodels are useful for formalizing intuitive understandings, even if those understandings are partialand incomplete. what appears to be a solid verbal argument about cause and effect can be clarified andput to a rigorous test as soon as an attempt is made to formulate the verbal arguments into a mathematical model. this process forces a clarity of expression and consistency (of units, dimensions, forcebalance, or other guiding principles) that is not available in natural language. as importantly, it cangenerate predictions against which intuition can be tested.because they run on a computer, simulation models force the researcher to represent explicitlyimportant components and connections in a system. thus, simulations can only complement, but neverreplace, the underlying formulation of a model in terms of biological, physical, and mathematicalprinciples. that said, a simulation model often can be used to indicate gaps in oneõs knowledge of somephenomenon, at which point substantial intellectual work involving these principles is needed to fill thegaps in the simulation.5.2.7models can be used as a tool for helping to screen unpromising hypothesesin a given setting, quantitative or descriptive hypotheses can be tested by exploring the predictionsof models that specify precisely what is to be expected given one or another hypothesis. in some cases,although it may be impossible to observe a sequence of biological events (e.g., how a receptorligandcomplex undergoes sequential modification before internalization by the cell), downstream effects maybe observable. a model can explore the consequences of each of a variety of possible sequences can andhelp scientists to identify the most likely candidate for the correct sequence. further experimentalobservations can then refine oneõs understanding.5.2.8models inform experimental designmodeling properly applied can accelerate experimental efforts at understanding. theory embeddedin the model is an enabler for focused experimentation. specifically, models can be used alongsideexperiments to help optimize experimental design, thereby saving time and resources. simple modelscatalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.computational modeling and simulation as enablers for biological discovery123give a framework for observations (as noted in section 5.2.1) and thereby suggest what needs to bemeasured experimentally and, indeed, what need not be measuredñthat is how to refine the set ofobservations so as to extract optimal knowledge about the system. this is particularly true when modelsand experiments go handinhand. as a rule, several rounds of modeling and experimentation arenecessary to lead to informative results.carrying these general observations further, selinger et al.8 have developed a framework for understanding the relationship between the properties of certain kinds of models and the experimentalsampling required for òcompletenessó of the model. they define a model as a set of rules that maps a setof inputs (e.g., possible descriptions of a cellõs environment) to a set of outputs (e.g., the resultingconcentrations of all of the cellõs rnas and proteins). from these basic properties, selinger et al. are ableto determine the order of magnitude of the number of measurements needed to populate the space of allpossible inputs (e.g., environmental conditions) with enough measured outputs (e.g., transcriptomes,proteomes) to make prediction feasible, thereby establishing how many measurements are needed toadequately sample input space to allow the rule parameters to be determined.using this framework, salinger et al. estimate the experimental requirements for the completenessof a discrete transcriptional network model that maps all n genes as inputs to all n genes as outputs inwhich the genes can take on three levels of expression (low, medium, and high) and each gene has, atmost, k direct regulators. applying this model to three organismsñmycoplasma pneumoniae, escherichiacoli, and homo sapiensñthey find that 80, 40,000, and 700,000 transcriptome experiments, respectively,are necessary to fill out this model. they further note that the upperbound estimate of experimentalrequirements grows exponentially with the maximum number of regulatory connections k per gene,although genes tend to have a low k, and that the upperbound estimate grows only logarithmicallywith the number of genes n, making completeness feasible even for large genetic networks.5.2.9models can predict variables inaccessible to measurementtechnological innovation in scientific instrumentation has revolutionized experimental biology.however, many mysteries of the cell, of physiology, of individual or collective animal behavior, and ofpopulationlevel or ecosystemlevel dynamics remain unobservable. models can help link observationsto quantities that are not experimentally accessible. at the scale of a few millimeters, mar”e andhogeweg recently developed9 a computational model based on a cellular automaton for the behavior ofthe social amoeba dictyostelium discoideum. their model is based on differential adhesion between cells,cyclic adenosine monophosphate (camp) signaling, cell differentiation, and cell motion. using detailedtwo and threedimensional simulations of an aggregate of thousands of cells, the authors showed howa relatively small set of assumptions and òrulesó leads to a fully accurate developmental pathway.using the simulation as a tool, they were able to explore which assumptions were blatantly inappropriate (leading to incorrect outcomes). in its final synthesis, the mar”ehogeweg model predicts dynamicdistributions of chemicals and of mechanical pressure in a fully dynamic simulation of the culminatingdictyostelium slug. some, but not all, of these variables can be measured experimentally: those that aremeasurable are well reproduced by the model. those that cannot (yet) be measured are predicted insidethe evolving shape. what is even more impressive: the model demonstrates that the system has selfcorrecting properties and accounts for many experimental observations that previously could not beexplained.8d.w. selinger, m.a. wright, and g.m. church, òon the complete determination of biological systems,ó trends in biotechnology 21(6):251254, 2003.9a.f.m. mar”e and p. hogeweg, òhow amoeboids selforganize into a fruiting body: multicellular coordination indictyostelium discoideum,ó proceedings of the national academy of sciences 98(7):38793883, 2001.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.124catalyzing inquiry5.2.10models can link what is known to what is yet unknownin the words of pollard, òany cellular process involving more than a few types of molecules is toocomplicated to understand without a mathematical model to expose assumptions and to frame thereactions in a rigorous setting.ó10 reviewing the state of the field in cell motility and the cytoskeleton,he observes that even with many details of the mechanism as yet controversial or unknown, modelingplays an important role. referring to a system (of actin and its interacting proteins) modeled by mogilnerand edelsteinkeshet,11 he points to advantages gained by the mathematical framework: òa mathematical model incorporating molecular reactions and physical forces correctly predicts the steadystaterate of cellular locomotion.ó the model, he notes, correctly identifies what limits the motion of the cell,predicts what manipulations would change the rate of motion, and thus suggests experiments to perform. while details of some steps are still emerging, the model also distinguishes quantitatively between distinct hypotheses for how actin filaments are broken down for purposes of recycling theircomponents.5.2.11models can be used to generate accurate quantitative predictionswhere detailed quantitative information exists about components of a system, about underlyingrules or interactions, and about how these components are assembled into the system as a whole,modeling may be valuable as an accurate and rigorous tool for generating quantitative predictions.weather prediction is one example of a complex model used on a daily basis to predict the future. onthe other hand, the notorious difficulties of making accurate weather predictions point to the need forcaution in adopting the conclusions even of classical models, especially for more than shortterm predictions, as one might expect from mathematically chaotic systems.5.2.12models expand the range of questions that can meaningfully be asked12for much of life science research, questions of purpose arise about biological phenomena. forinstance, the question, why does the eye have a lens? most often calls for the purpose of the lensñtofocus light raysñand only rarely for a description of the biological mechanism that creates the lens.that such an answer is meaningful is the result of evolutionary processes that shape biological entitiesby enhancing their ability to carry out fitnessenhancing functions. (put differently, biological entitiesare the result of natureõs engineering of devices to perform the function of survival; this perspective isexplored further in chapter 6.)lander points out that molecular biologists traditionally have shied away from teleological matters,and that geneticists generally define function not in terms of the useful things a gene does, but by whathappens when the gene is altered. however, as the complexity of biological mechanism is increasinglyrevealed, the identification of a purpose or a function of that mechanism has enormous explanatorypower. that is, what purpose does all this complexity serve?as the examples in section 5.4 illustrate, computational modeling is an approach to exploring theimplications of the complex interactions that are known from empirical and experimental work. landernotes that one general approach to modeling is to create models in which networks are specified interms of elements and interactions (the network òtopologyó), but the numerical values that quantifythose interactions (the parameters) are deliberately varied over wide ranges to explore the functionalityof the networkñwhether it acts as a òswitch,ó òfilter,ó òoscillator,ó òdynamic range adjuster,ó òproducer of stripes,ó and so on.10t.d. pollard, òthe cytoskeleton, cellular motility and the reductionist agenda,ó nature 422(6933):741745, 2003.11a. mogilner and l. edelsteinkeshet, òregulation of actin dynamics in rapidly moving cells: a quantitative analysis,óbiophysical journal 83(3):12371258, 2002.12section 5.2.12 is based largely on a.d. lander, òa calculus of purpose,ó plos biology 2(6):e164, 2004.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.computational modeling and simulation as enablers for biological discovery125lander explains the intellectual paradigm for determining function as follows:by investigating how such behaviors change for different parameter setsñan exercise referred to asòexploring the parameter spaceóñone starts to assemble a comprehensive picture of all the kinds ofbehaviors a network can produce. if one such behavior seems useful (to the organism), it becomes acandidate for explaining why the network itself was selected; i.e., it is seen as a potential purpose for thenetwork. if experiments subsequently support assignments of actual parameter values to the range ofparameter space that produces such behavior, then the potential purpose becomes a likely one.5.3types of models135.3.1from qualitative model to computational simulationbiology makes use of many different types of models. in some cases, biological models are qualitative or semiquantitative. for example, graphical models show directional connections between components, with the directionality indicating influence. such models generally summarize a great deal ofknown information about a pathway and facilitate the formation of hypotheses about network function.moreover, the use of graphical models allows researchers to circumvent data deficiencies that might beencountered in the development of more quantitative (and thus dataintensive) models. (it has alsobeen argued that probabilistic graphical models provide a coherent, statistically sound framework thatcan be applied to many problems, and that certain models used by biologists, such as hidden markovmodels or bayesian networks), can be regarded as special cases of graphical models.14)on the other hand, the forms and structures of graphical models are generally inadequate to expressmuch detail, which might well be necessary for mechanistic models. in general, qualitative models do notaccount for mechanisms, but they can sometimes be developed or analyzed in an automated manner.some attempts have been made to develop formal schemes for annotating graphical models (box 5.2).15qualitative models can be logical or statistical as well. for example, statistical properties of a graphof proteinprotein interaction have been used to infer the stability of a networkõs function against mostòdeletionsó in the graph.16 logical models can be used when data regarding mechanism are unavailable and have been developed as boolean, fuzzy logical, or rulebased systems that model complexnetworks17 or genetic and developmental systems.in some cases, greater availability of data (specifically, perturbation response or timeseries data)enables the use of statistical influence models. linear,18 neural networklike,19 and bayesian20 modelshave all been used to deduce both the topology of gene expression networks and their dynamics. on the13section 5.3 is adapted from a.p. arkin, òsynthetic cell biology,ó current opinion in biotechnology 12(6):638644, 2001.14see, for example, y. moreau, p. antal, g. fannes, and b. de moor, òprobabilistic graphical models for computationalbiomedicine, methods of information in medicine 42(2):161168, 2003.15k.w. kohn, òmolecular interaction map of the mammalian cell cycle: control and dna repair systems,ó molecular biologyof the cell 10(8):27032734, 1999; i. pirson, n. fortemaison, c. jacobs, s. dremier, j.e. dumont, and c. maenhaut, òthe visualdisplay of regulatory information and networks,ó trends in cell biology 10(10):404408, 2000. (both cited in arkin, 2001.)16h. jeong, s.p. mason, a.l. barabasi, and z.n. oltvai, òlethality and centrality in protein networks,ó nature 411(6833):4142,2001; h. jeong, b. tombor, r. albert, z.n. oltvai, and a.l. barabasi, òthe largescale organization of metabolic networks,ónature 407(6804):651654, 2000. (cited in arkin, 2001.)17d. thieffry and r. thomas, òqualitative analysis of gene networks,ó pp. 7788 in pacific symposium on biocomputing, 1998.(cited in arkin, 2001.)18p. dõhaeseleer, x. wen, s. fuhrman, and r. somogyi, òlinear modeling of mrna expression levels during cns development and injury,ó pp. 4152 in pacific symposium on biocomputing, 1999. (cited in arkin, 2001.)19e. mjolsness, d.h. sharp, and j. reinitz, òa connectionist model of development,ó journal of theoretical biology 152(4):429453, 1999. (cited in arkin, 2001.)20n. friedman, m. linial, i. nachman, and d. peõer, òusing bayesian networks to analyze expression data,ó journal ofcomputational biology 7(34):601620, 2000. (cited in arkin, 2001.)catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.126catalyzing inquiryother hand, statistical influence models are not causal and may not lead to a better understanding ofunderlying mechanisms.quantitative models make detailed statements about biological processes and hence are easier tofalsify than more qualitative models. these models are intended to be predictive and are useful forunderstanding points of control in cellular networks and for designing new functions within them.some models are based on power law formalisms.21 in such cases, the data are shown to fit genericpower laws, and the general theory of power law scaling (for example) is used to infer some degree ofcausal structure. they do not provide detailed insight into mechanism, although power law modelsform the basis for a large class of metabolic control analyses and dynamic simulations.computational modelsñsimulationsñrepresent the other end of the modeling spectrum. simulation is often necessary to explore the implications of a model, especially its dynamical behavior, becausebox 5.2on graphical modelsa large fraction of todayõs knowledge of biochemical or genetic regulatory networks is represented either astext or as cartoonlike diagrams. however, text has the disadvantage of being inherently ambiguous, andevery reader must reinterpret the text of a journal article. diagrams are usually informal, often confusing, andthus fail to present all of the information that is available to the presenter of the research. for example, themeanings of nodes and arcs within a diagram are inconsistentñone arrow may mean activation, but anotherarrow in the same diagram may mean transition of the state or translocation of materials.to remedy this state of affairs, a system of graphical representation should be powerful enough to expresssufficient information in a clearly visible and unambiguous way and should be supported by software tools.there are several criteria for a graphical notation system, including the following:1.expressiveness.the notation system should be able to describe every possible relationship among theentities in a systemñfor example, those between genes and proteins in a biological model.2.semantical unambiguity.notation should be unambiguous. different semantics should be assigned todifferent symbols that are clearly distinguishable.3.visual unambiguity.each symbol should be identified clearly and not be mistaken with other symbols.this feature should be maintained with lowresolution displays, using only black and white.4.extension capability.the notation system should be flexible enough to add new symbols and relationshipsin a consistent manner. this may include the use of color coding to enhance expressiveness and readability,but information should not be lost even with blackandwhite displays.5.mathematical translation.the notation should be able to convert itself into mathematical formalisms, suchas differential equations, so that it can be applied directly for numerical analysis.6.software support.the notation should be supported by software for its drawing, viewing, editing, andtranslation into mathematical formalisms.no current graphical notation system satisfies all of these criteria fully, although a number of systems satisfysome of them.1source: adapted by permission from h. kitano, òa graphical notation for biochemical networks,ó biosilico 1(5):159176. copyright2003 elsevier.1see, for example, k.w. kohn, òmolecular interaction map of the mammalian cell cycle control and dna repair systems,ó molecularbiology of the cell 10(8):27032734, 1999; k. kohn, òmolecular interaction maps as information organizers and simulation guides,ó chaos11(1):8497, 2001.21e.o. voit and t. radivoyevitch, òbiochemical systems analysis of genomewide expression data,ó bioinformatics 16(11):10231037, 2000. (cited in arkin, 2001.)catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.computational modeling and simulation as enablers for biological discovery127human intuition about complex nonlinear systems is often inadequate.22 lander cites two examples.the first is that òintuitive thinking about map [mitogenactivated protein] kinase pathways led to thelongheld view that the obligatory cascade of three sequential kinases serves to provide signal amplification. in contrast, computational studies have suggested that the purpose of such a network is toachieve extreme positive cooperativity, so that the pathway behaves in a switchlike, rather than agraded, fashion.ó23 the second example is that while intuitive interpretations of experiments in thestudy of morphogen gradient formation in animal development led to the conclusion that simplediffusion is not adequate to transport most morphogens, computational analysis of the same experimental data led the opposite conclusion.24simulation, which traces functional biological processes through some period of time, generatesresults that can be checked for consistency with existing data (òretrodictionó of data) and can alsopredict new phenomena not explicitly represented in but nevertheless consistent with existing datasets.note also that when a simulation seeks to capture essential elements in some oversimplified andidealized fashion, it is unrealistic to expect the simulation to make detailed predictions about specificbiological phenomena. such simulations may instead serve to make qualitative predictions about tendencies and trends that become apparent only when averaged over a large number of simulation runs.alternatively, they may demonstrate that certain biological behaviors or responses are robust and donot depend on particular details of the parameters involved within a very wide range.simulations can also be regarded as a nontraditional form of scientific communication. traditionally, scientific communications have been carried by journal articles or conference presentations. thougharticles and presentations will continue to be important, simulationsñin the form of computer programsñcan be easily shared among members of the research community, and the explicit knowledgeembedded in them can become powerful points of departure for the work of other researchers.with the availability of cheap and powerful computers, modeling and simulation have becomenearly synonymous. yet, a number of subtle differences should be mentioned. simulation can be usedas a tool on its own or as a companion to mathematical analysis.in the case of relatively simple models meant to provide insight or reveal a concept, analyticaland mathematical methods are of primary utility. with simple strokes and penandpaper computations, the dependence of behavior on underlying parameters (such as rate constants), conditionsfor specific dynamical behavior, and approximate connections between macroscopic quantities(e.g., the velocity of a cell) and underlying microscopic quantities (such the number of actin filaments causing the membrane to protrude) can be revealed. simulations are not as easily harnessedto making such connections.simulations can be used handinhand with analysis for simple models: exploring slight changes inequations, assumptions, or rates and gaining familiarity can guide the best directions to explore withsimple models as well. for example, g. bard ermentrout at the university of pittsburgh developed xppsoftware as an evolving and publicly available experimental modeling tool for mathematical biologists.25 xpp has been the foundation of computational investigations in many challenging problems inneurophysiology, coupled oscillators, and other realms.mathematical analysis of models, at any level of complexity, is often restricted to special cases thathave simple properties: rectangular boundaries, specific symmetries, or behavior in a special class. simulations can expand the repertoire and allow the modeler to understand how analysis of the special cases22a.d. lander, òa calculus of purpose,ó plos biology 2 (6):e164, 2004.23c.y. huang and j.e. ferrell, òultrasensitivity in the mitogen activated protein kinase cascade,ó proceedings of the nationalacademy of sciences 93(19):1007810083, 1996. (cited in lander, òa calculus of purpose,ó 2004.)24a.d. lander, q. nie, and f.y. wan, òdo morphogen gradients arise by diffusion?ó developmental cell 2(6):785796, 2002.(cited in lander, 2004.)25see http://www.math.pitt.edu/~bard/xpp/xpp.html.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.128catalyzing inquiryrelates to more realistic situations. in this case, simulation takes over where analysis ends.26 some systemsare simply too large or elaborate to be understood using analytical techniques. in this case, simulation is aprimary tool. forecasts requiring heavy ònumbercrunchingó (e.g., weather prediction, prediction of climate change), as well as those involving huge systems of diverse interacting components (e.g., cellularnetworks of signal transduction cascades), are only amenable to exploration using simulation methods.more detailed models require a detailed consideration of chemical or physical mechanisms involved(i.e., these models are mechanistic27). such models require extensive details of known biology and havethe largest data requirements. they are, in principle, the most predictive. in the extreme, one can imaginea simulation of a complete cellñan òin silicoó cell or cybercellñthat provides an experimental frameworkin which to investigate many possible interventions. getting the right format, and ensuring that the insilico cell is a reasonable representation of reality, has been and continues to be an enormous challenge.no reasonable model is based entirely on a bottomup analysis. consider, for example, that solvingschrıdingerõs equation for the millions of atoms in a complex molecule in solution would be a futileexercise, even if future supercomputers could handle this task. the question to ask is how and why suchwork would be contemplated: finding the correct level of representation is one of the key steps to goodscientific work. thus, some level of abstraction is necessary to render any model both interestingscientifically and feasible computationally. done properly, abstractions can clarify the sources of control in a network and indicate where more data are necessary. at the same time, it may be necessary toconstruct models at higher degrees of biophysical realism and detail in any event, either becauseabstracted models often do not capture the essential behavior of interest or to show that indeed theaddition of detail does not affect the conclusions drawn from the abstracted model.28it is also helpful to note the difference between a computational artifact that reproduces somebiological behavior (a task) and a simulation. in the former case, the relevant question is: òhow welldoes the artifact accomplish the task?ó in the latter case, the relevant question is: òhow closely does thesimulation match the essential features of the system in question?ómost computer scientists would tend to assign higher priority to performance than to simulation.the computer scientist would be most interested in a biologically inspired approach to a computerscience problem when some biological behavior is useful in a computational or computer systemscontext and when the biologically inspired artifact can demonstrate better performance than is possiblethrough some other way of developing or inspiring the artifact. a model of a biological system thenbecomes useful to the computer scientist only to the extent that highfidelity mimicking of how natureaccomplishes a task will result in better performance of that task.by contrast, biologists would put greater emphasis on simulation. empirically tested and validatedsimulations with predictive capabilities would increase their confidence that they understood in somefundamental sense the biological phenomenon in question. however, it is important to note that because a simulation is judged on the basis of how closely it represents the essential features of a biologicalsystem, the question òwhat counts as essential?ó is central (box 5.3). more generally, one fundamentalfocus of biological research is a determination of what the òessentialó features of a biological system are,26at times, it is also desirable to employ a mix of analysis and simulation. analysis would be used to generate the basicequations underlying a complex phenomenon. solutions to these equations would then be explored and with luck, considerablysimplified. the simplified models can then be simulated. see, for example, e.a. ezrachi, r. levi, j.m. camhi, and h. parnas,òrightleft discrimination in a biologically oriented model of the cockroach escape system,ó biological cybernetics 81(2):8999,1999.27note that mechanistic models can be stochasticñthe term òmechanisticó should not be taken to mean deterministic.28tensions between these perspectives were apparent even in reviews of the draft of this report. in commenting on neuroscience topics in this report, advocates of the first point of view argued that ultrarealistic simulations accomplish little to furtherour understanding about how neurons work. advocates of the second point of view argued that simple neural models could notcapture the implications of the complex dynamics of each neuron and its synapses and that these models would have to besupplemented by more physiological ideas. from the committeeõs perspective, both points of view have merit, and the scientificchallenge is to find an appropriate simplification or abstraction that does capture the interesting behavior at reasonable fidelity.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.computational modeling and simulation as enablers for biological discovery129recognizing that what is òessentialó cannot be determined once and for all, but rather depends on theclass of questions under consideration.5.3.2hybrid modelshybrid models are models composed of objects with different mathematical representations. theseallow a model builder the flexibility to mix modeling paradigms to describe different portions of acomplex system. for example, in a hybrid model, a signal transduction pathway might be described bya set of differential equations, and this pathway could be linked to a graphical model of the geneticregulatory network that it influences. an advantage of hybrid models is that model components canevolve from highlevel abstract descriptions to lowlevel detailed descriptions as the components arebetter characterized and understood.an example of hybrid model use is offered by mcadams and shapiro,29 who point out that geneticnetworks involving large numbers of genes (more than tens) are difficult to analyze. noting the òmanyparallels in the function of these biochemically based genetic circuits and electrical circuits,ó theypropose òa hybrid modeling approach that integrates conventional biochemical kinetic modeling withinthe framework of a circuit simulation. the circuit diagram of the bacteriophage lambda lysislysogenydecision circuit represents connectivity in signal paths of the biochemical components. a key feature ofthe lambda genetic circuit is that operons function as active integrated logic components and introducesignal time delays essential for the in vivo behavior of phage lambda.óthere are good numerical methods for simulating systems that are formulated in terms of ordinarydifferential equations or algebraic equations, although good methods for analysis of such models arestill lacking. other systems, such as those that mix continuous with discrete time or markov processeswith partial differential equations, are sometimes hard to solve even by numerical methods. further, aparticular model object may change mathematical representation during the course of the analysis. forexample, at the beginning of a biosynthetic process there may be very small amounts of product so itsbox 5.3an illustration of òessentialóconsider the following modeling task. the phenomenon of interest is a monkey learning to fetch a bananafrom behind a transparent conductive screen. the first time, the monkey sees the banana, goes straight ahead,bumps into the screen, and then goes around the screen to the banana. the second time, the monkey, havingdiscovered the existence of the screen that blocks his way, goes directly around the screen to the banana.to model this phenomenon, a system is constructed, consisting of a charged ball and a metal sheet. thecharged metal ball is hung from a string above the banana and then held at an angle so the screen separatesthe ball and the banana. the first time the ball is released, the ball swings toward the screen, and then touchesit, transferring part of its charge to the screen. the similar charges on the screen and the ball now repel eachother, and the ball swings around the screen. the second time the ball is released, the ball sees a similarlycharged screen and goes around the screen directly.this model reproduces the behavior of the monkey in the first instance. however, no one would claim that itis an accurate model of the learning that takes place in the monkeyõs brain, even though the model replicatesthe most salient feature of the monkeyõs learning consistently: both the ball and the monkey dodge the screenon the second attempt. in other words, even though it demonstrates the same behavior, the model does notrepresent the essential features of the biological system in question.29see h.h. mcadams and l. shapiro, òcircuit simulation of genetic networks,ó science 269(5224):650656, 1994.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.130catalyzing inquiryconcentration would have to be modeled discretely. as more of it is synthesized, the concentrationbecomes high enough that a continuous approximation is justified and is then more efficient for simulation and analysis.the point at which this switch is made is dependent not just on copy number but also on where in thedynamical state space the system resides. if the system is near a bifurcation point, small fluctuations may besignificant. theories of how to accomplish this dynamic switching are lacking. as models grow morecomplex, different parts of the system will have to be modeled with different mathematical representations.also, as models from different sources begin to be joined, it is clear that different representations will beused. it is critical that the theory and applied mathematics of hybrid dynamical systems be developed.5.3.3multiscale modelsmultiscale models describe processes occurring at many time and length scales. depending on thebiological system of interest, the data needed to provide the basis for a greater understanding of thesystem will cut across several scales of space and time. the length dimensions of biological interestrange from small organic molecules to multiprotein complexes at 100 angstroms to cellular processes at1,000 angstroms to tissues at 110 microns, and the interaction of human populations with the environment at the kilometer scale. the temporal domain includes the femtosecond chemistry of molecularinteractions to the millions of years of evolutionary time, with protein folding in seconds and cell anddevelopmental processes in minutes, hours, and days. in turn, the scale of the process involved (e.g.,from the molecular scale to the ecosystem scale) affects both the complexity of the representation (e.g.,molecule base, concentration based, at equilibrium or fully dynamic) and the modality of the representation (e.g., biochemical, genetic, genomic, electrophysiological, etc.).consider the heart as an example. the macroscopic unit of interest is the heartbeat, which lastsabout a second and involves the whole heart of 10 cm scale. but the cardiac action potential (theelectrical signal that initiates myocellular contractions) can change significantly on time scales of milliseconds as reflected in the appropriate kinetic equations. in turn, the molecular interactions that underlie kinetic flows occur on time scales on the order of femtoseconds. across such variation in time scales,it is not feasible to model 1015 molecular interactions in order to model a complete heartbeat. fortunately, in many situations the response with the shorter time scale will converge quickly to equilibriumor quasisteadystate behavior, obviating the need for a complete lowerlevel simulation.30for most biological problems, the scale at which data could provide a central insight into theoperation of the whole system is not known, so multiple scales are of interest. thus, biological modelshave to allow for transition among different levels of resolution. a biologist might describe a protein asa simple ellipsoid and then in the next breath explain the effect of a point mutation by the atomiclevelstructural changes it causes in the active site.31identifying the appropriate ranges of parameters (e.g., rate constants that govern the pace of chemical reactions) remains one of the difficulties that every modeler faces sooner or later. as modelers knowwell, even qualitative analysis of simple models depends on knowing which òleadingorder termsó areto be kept on which time scales. when the relative rates are entirely unknownñtrue of many biochemical steps in living cellsñit is hard to know where to start and how to assemble a relevant model, a pointthat underscores the importance of close dialogue between the laboratory biologist and the mathematical or computational modeler.finally, data obtained at a particular scale must be sufficient to summarize the essential biologicalactivity at that scale in order to be evaluated in the context of interactions at greater scales of complexity.the challenge, therefore, is one of understanding not only the relationship of multiple variables operating at one scale of detail, but also the relationship of multivariable datasets collected at different scales.30a.d. mcculloch and g. huber, òintegrative biological modelling in silico,ó pp. 425 in ôin silicoõ simulation of biologicalprocesses no. 247, novartis foundation symposium, g. bock and j.a. goode, eds., john wiley & sons ltd., chichester, uk, 2002.31d. endy and r. brent, òmodeling cellular behavior,ó nature 409(6818):391395, 2001.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.computational modeling and simulation as enablers for biological discovery1315.3.4model comparison and evaluationmodels are ultimately judged by their ability to make predictions. qualitative models predict trendsor types of dynamics that can occur, as well as thresholds and bifurcations that delineate one type ofbehavior from another. quantitative models predict values that can be compared to actual experimentaldata. therefore, the selection of experiments to be performed can be determined, at least in part, by theirusefulness in constraining a model or selecting one model from a set of competing models.the first step in model evaluation is to replicate and test a computational model of biologicalsystems that has been published. however, most papers contain typographical errors and do not provide a complete specification of the biological properties that were represented in the model. oneshould be able to extract the specification from the modelõs source code, but for a whole host of reasonsit is not always possible to obtain the actual files that were used for the published work.in the neuroscience field, modeldb (http://senselab.med.yale.edu/senselab/modeldb/) is beingdeveloped to answer the need for a database of published models used in neuroscience research.32 it ispart of the senselab project (http://senselab.med.yale.edu/), which is supported through the humanbrain project by the national institute of mental health (nimh), the national institute of neurologistdisorders and stroke (ninds), and the national cancer institute (nci).modeldb is a curated database that is designed for convenient entry, search, and retrieval of modelswritten for any programming language or simulation environment. as of december 10, 2004, it contained 141 downloadable models. most of these are for neuron, but 40 of them are for matlab,genesis, snnap, or xpp, and there are also some models in c/c++ and fortran. database entriesare linked to the published literature so that users can more easily determine the òscientific contextó ofany given model.although modeldb is still in a developmental or research stage, it has already begun to have a positiveeffect on computational modeling in neuroscience. database logs indicate that it is seeing heavy usage, andfrom personal communications the committee has learned that even experienced programmers who writetheir own code in c/c++ are regularly examining models written for neuron and other domainspecificsimulators, in order to determine key parameter values and other important details. recently publishedpapers are beginning appear that cite modeldb and the models it contains as sources of code, equations, orparameters. furthermore, a leading journal has adopted a policy that requires authors to make their sourcecode available as a condition of publication and encourages them to use modeldb for this purpose.as for model comparison, it is not possible to ascertain in isolation whether a given model is correctsince contradictory data may become available later, and indeed even òincorrectó models may makecorrect predictions. suitably complex models can be made to fit to any dataset, and one must guardagainst òoverfittingó a model. thus, the predictions of a model must be viewed in the context of thenumber of degrees of freedom of the model, and one measure that one model is better than another is ajudgment about which model best explains experimental data with the least model complexity. in somecases, measures of the statistical significance of a model can be computed using a likelihood distributionover predicated state variables taking into account the number of degrees of freedom present in the model.at the same time, lessons learned over many centuries of scientific investigation regarding the use ofoccamõs razor may have limited applicability in this context. because biological phenomena are the resultof an evolutionary process that simply uses what is available, many biological phenomena are simplycobbled together and in no sense can be regarded as the òsimplestó way to accomplish something.as noted in footnote 28, there is a tension between the need to capture details faithfully in a modeland the desire to simplify those details so as to arrive at a representation that can be analyzed, understoodfully, and converted into scientific òknowledge.ó there are numerous ways of reducing models that arewell known in applied mathematics communities. these include dimensional analysis and multiple timescale analysis (i.e., dissecting a system into parts that evolve rapidly versus those that change on a slower32m.l. hines, t. morse, m. migliore, n.t. carnevale, and g.m. shepherd, òmodeldb: a database to support computationalneuroscience,ó journal of computational neuroscience 17(1):711, 2004; b.j. richmond, òeditorial commentary,ó journal of computational neuroscience 17(1):5, 2004.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.132catalyzing inquirytime scale). in some cases, leaving out some of the interacting components (e.g., those whose interactionsare weakest or least significant) may be a workable method. in other cases, lumping together families orgroups of substances to form aggregate components or compartments works best. sensitivity analysis ofalternative model structures and parameters can be performed using likelihood and significance measures. sensitivity analysis is important to inform a model builder of the essential components of the modeland to attempt to reduce model complexity without loss of explanatory power.model evaluation can be complicated by the robustness of the biological organism being represented. robustness generally means that the organism will endure and even prosper under a widerange of conditionsñwhich means that its behavior and responses are relatively insensitive to variations in detail.33 that is, such differences are unlikely to matter much for survival. (for example, themodeling of genetic regulatory networks can be complicated by the fact that although the data mayshow that a certain gene is expressed under certain circumstances, the biological function being servedmay not depend on the expression of that gene.) on the other hand, this robustness may also mean thata flawed understanding of detailed processes incorporated into a model that does explain survivalresponses and behavior will not be reflected in the modelõs output.34simulation models are essentially computer programs and hence suffer from all of the problemsthat plague software development. normal practice in software development calls for extensive testingto see that a program returns the correct results when given test data for which the appropriate resultsare known independently of the program as well as for independent code reviews. in principle, simulation models of biological systems could be subject to such practices. yet the fact that a given simulationmodel returns results that are at variance with experimental data may be attributable to an inadequacyof the underlying model or to an error in programming.35 note also that public code reviews areimpossible if the simulation models are proprietary, as they often are when they are created by firmsseeking to obtain competitive advantage in the marketplace.these points suggest a number of key questions in the development of a model.¥how much is given up by looking at simplified versions?¥how much poorer, and in what ways poorer, is a simplified model in its ability to describe the system?¥are there other, new ways of simplifying and extracting salient features?¥once the simplified representation is understood, how can the details originally left out bereincorporated into a model of higher fidelity?finally, another approach to model evaluation is based on notions of logical consistency. thisapproach uses program verification tools originally developed by computer scientists to determinewhether a given program is consistent with a given formal specification or property. in the biologicalcontext, these tools are used to check the consistency and completeness of a modelõs description of thebiological systemõs processes. these descriptions are dynamic and thus permit òrunningó a model toobserve developments in time. specifically, kam et al. have demonstrated this approach using thelanguages, methods, and tools of scenariobased reactive system design and applied it to modeling thewellcharacterized process of cell fate acquisition during caenorhabditis elegans vulval development.(box 5.4 describes the intellectual approach in more detail.36)33l.a. segel, òcomputing an organism,ó proceedings of the national academy of sciences 98(7):36393640, 2001.34on the basis of other work, segel argues that a biological model enjoys robustness only if it is òcorrectõõ in certain essential features.35note also the wellknown psychological phenomenon in programmingñbeing a captive of oneõs test data. programmingerrors that prevent the model from accounting for the data tend to be hunted down and fixed. however, if the model doesaccount for the data, there is a tendency to assume that the program is correct.36n. kam, d. harel, h. kugler, r. marelly, a. penueli, j. hubbard, et al., òformal modeling of c. elegans development: ascenariobased approach,ó pp. 420 in proceedings of the first international workshop on computational methods in systems biology(cmsb03; rovereto, italy, february 2003), vol. 2602, lecture notes in computer science, springerverlag, berlin, heidelberg,2003. this material is scheduled to appear in the following book: g. ciobanu, ed., modeling in molecular biology, natural computing series, springer, available at http://www.wisdom.weizmann.ac.il/~kam/celegansmodel/publications/mmbcelegans.pdf.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.computational modeling and simulation as enablers for biological discovery133box 5.4formal modeling of caenorhabditis elegans developmentour understanding of biology has become sufficiently complex that it is increasingly difficult to integrate all therelevant facts using abstract reasoning alone. [formal modeling presents] a novel approach to modeling biologicalphenomena. it utilizes in a direct and powerful way the mechanisms by which raw biological data are amassed, andsmoothly captures that data within tools designed by computer scientists for the design and analysis of complexreactive systems.a considerable quantity of biological data is collected and reported in a form that can be called òconditionresultódata. the gathering is usually carried out by initializing an experiment that is triggered by a certain set of circumstances (conditions), following which an observation is made and the results recorded. the condition is most oftena perturbation, such as mutating genes or exposing cells to an altered environment. . . . [and] a large proportion ofbiological data is reported as stories, or òscenarios,ó that document the results of experiments conducted underspecific conditions.the challenge of modeling these aspects of biology is to be able to translate such òconditionresultó phenomenafrom the òscenarioóbased natural language format into a meaningful and rigorous mathematical language. sucha translation process will allow these data to be integrated more comprehensively by the application of highlevelcomputerassisted analysis. in order for it to be useful, the model must be rigorous and formal, and thus amenableto verification and testing.we have found that modeling methodologies originating in computer science and software engineering, and createdfor the purpose of designing complex reactive systems, are conceptually well suited to model this type of conditionresult biological data. reactive systems are those whose complexity stems not necessarily from complicated computation but from complicated reactivity over time. they are most often highly concurrent and timeintensive, andexhibit hybrid behavior that is predominantly discrete in nature but has continuous aspects as well. the structure ofa reactive system consists of many interacting components, in which control of the behavior of the system is highlydistributed amongst the components. very often the structure itself is dynamic, with its components being repeatedlycreated and destroyed during the systemõs life span.the most widely used frameworks for developing models of such systems feature visual formalisms, which are bothgraphically intuitive and mathematically rigorous. these are supported by powerful tools that enable full modelexecutability and analysis, and are linkable to graphical user interfaces (guis) of the system. this enables realisticsimulation prior to actual implementation. at present, such languages and toolsñoften based on the objectorientedparadigmñare being strengthened by verification modules, making it possible not only to execute and simulate thesystem models (test and observe) but also to verify dynamic properties thereof (prove). . . .[m]any kinds of biological systems exhibit characteristics that are remarkably similar to those of reactive systems.the similarities apply to many different levels of biological analysis, including those dealing with molecular, cellular,organbased, whole organism, or even population biology phenomena. once viewed in this light, the dramaticconcurrency of events, the chainreactions, the timedependent patterns, and the eventdriven discrete nature oftheir behaviors, are readily apparent. consequently, we believe that biological systems can be productively modeledas reactive systems, using languages and tools developed for the construction of manmade systems. . . .source: n. kam et al., òformal modeling of c. elegans development: a scenariobased approach,ó pp. 420 in proceedings of the firstinternational workshop on computational methods in systems biology (cmsb03; rovereto, italy, february 2003), vol. 2602, lecture notesin computer science, springerverlag, berlin, heidelberg, 2003, available at http://www.wisdom.weizmann.ac.il/~kam/celegansmodel/publications/mmbcelegans.pdf. reprinted with permission from springerverlag.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.134catalyzing inquiry5.4modeling and simulation in actionthe preceding discussion has been highly abstract. this section provides some illustrations of howmodeling and simulation have value across a variety of subfields in biology. no claim is made tocomprehensiveness, but the committee wishes to illustrate the utility of modeling and simulations atlevels of organization from gene to ecosystem.5.4.1molecular and structural biology5.4.1.1predicting complex protein structuresinteractions between proteins are crucial to the functioning of all cells. while there is much experimental information being gathered regarding protein structures, many interactions are not fully understood and have to be modeled computationally. the topic of computational prediction of proteinprotein structure remains to be solved and is one of the most active areas of research in bioinformaticsand structural biology.zdock and rdock are two computer programs that address this problem, also known as proteindocking.37 zdock is an initial stage protein docking program that performs a full search of the relativeorientations of two molecules (referred to by convention as the ligand and receptor) to determine theirbest fit based on surface complementarity, electrostatics and desolvation. the efficiency of the algorithm is enhanced by discretizing the molecules onto a grid and performing a fast fourier transform(fft) to quickly explore the translational degrees of freedom.rdock takes as input the zdock predictions and improves them using two steps. the first step isto improve the energetics of the prediction and remove clashes by performing small movements of thepredicted complex, using a program known as charmm. the second step is to rescore these minimized predictions with more detailed scoring functions for electrostatics and desolvation.the combination of these two algorithms has been tested and verified with a benchmark set ofproteins collected for use in testing docking algorithms. now at version 2.0, this benchmark is publiclyavailable and contains 87 test cases. these test cases cover a breadth of interactions, such as antibodyantigen, and cases involving significant conformational changes.the zdockrdock programs have consistently performed well in the international dockingcompetition capri (figure 5.1). some notable predictions were for the rotavirus vp6/fab (50 of 52contacting residues correctly predicted), and sag1/fab complex (61 of 70 contacts correct), and thecellulosome cohesiondockerin structure (50 of 55 contacts correct). in the first two cases, the number ofcontacts in the zdockrdock predictions were the highest among all participating groups.5.4.1.2a method to discern a functional class of proteinsthe dnabinding helixturnhelix structural motif plays an essential role in a variety of cellularpathways that include transcription, dna recombination and repair, and dna replication. currentmethods for identifying the motif rely on amino acid sequence, but since members of the motif belongto different sequence families that have no sequence homology to each other, these methods have beenunable to identify all motif members.a new method based on threedimensional structure was created that involved the followingsteps:38 (1) choosing a conserved component of the motif, (2) measuring structural features relative37for more information, see http://zlab.bu.edu.38w.a. mclaughlin and h.m. berman, òstatistical models for discerning protein structures containing the dnabindinghelixturnhelix motif,ó journal of molecular biology 330(1):4355, 2003.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.computational modeling and simulation as enablers for biological discovery135to that component, and (3) creating classification models by comparing measurements of structuresknown to contain the motif to measurements of structures known not to contain the motif. in thiscase, the conserved component chosen was the recognition helix (i.e., the alpha helix that makessequencespecific contact with dna), and two types of relevant measurements were the hydrophobic area of interaction between secondary structure elements (sses) and the relative solvent accessibility of sses.with a classification model created, the entire protein data bank of experimentally measured structures was searched and new examples of the motif were found that have no detected sequence homology with previously known examples. two such examples are esa1 histone acetyltransferase andisoflavone 4omethyltransferase. the result emphasizes an important utility of the approach: sequencebased methods used to discern a functional class of proteins may be supplemented through the use of aclassification model based on threedimensional structural information.figure 5.1the zdock/rdock prediction for dockerin (in red) superposed on the crystal structure for capritarget 13, cohesin/dockerin. source: courtesy of brian pierce and zhiping weng, boston university.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.136catalyzing inquiry5.4.1.3molecular dockingusing a simple, uniform representation of molecular surfaces that requires minimal parameterization, jain39 has constructed functions that are effective for scoring proteinligand interactions, quantitatively comparing small molecules, and making comparisons of proteins in a manner that does notdepend on protein backbone. these methods rely on computational approaches that are rooted inunderstanding the physics of molecular interactions, but whose functional forms do not resemble thoseused in physicsbased approaches. that is, this problem can be treated as a pure computer scienceproblem that can be solved using combinations of scoring and search or optimization techniques parameterized with the use of domain knowledge. the approach is as follows:¥molecules are approximated as collections of spheres with fixed radii: h = 1.2; c = 1.6; n = 1.5; o =1.4; s = 1.95; p = 1.9; f = 1.35; cl = 1.8; br = 1.95; i = 2.15.¥a labeling of the features of polar atoms is superimposed on the molecular representation:polarity, charge, and directional preference (figure 5.2, subfigures a and b).¥a scoring function is derived that, given a protein and a ligand in some relative alignment, yieldsa prediction of the energy of interaction.¥the function is parameterized in terms of the pairwise distances between molecular surfaces.¥the dominant terms are a hydrophobic term that characterizes interactions between nonpolaratoms and a polar term that captures complementary polar contacts with proper directionality.¥the parameters of the function were derived from empirical binding data and 34 proteinligandcomplexes that were experimentally determined.¥the scoring function is described in figure 5.2, subfigure c. the hydrophobic term peaks atapproximately 0.1 unit with a slight surface interpenetration. the hydrophobic term for an ideal hydrogen bond peaks at 1.25 units, and a charged interaction (tertiary amine proton (+1.0) to a chargedcarboxylate (ð0.5)) peaks at about 2.3 units. note that this scoring function looks nothing like a forcefield derived from molecular mechanics.¥figure 5.2, subfigure d compares eight docking methods on screening efficiency using thymidine kinase as a docking target. for the test, 10 known ligands and 990 random ligands were used.particularly at low falsepositive rates (low database coverage), the scoring function approach showssubstantial improvements over the other methods.5.4.1.4computational analysis and recognition of functional andstructural sites in protein structures40structural genomics initiatives are producing a great increase in protein threedimensional structures determined by xray and nuclear magnetic resonance technologies as well as those predicted bycomputational methods. a critical next step is to study the relationships between protein structures andfunctions. studying structures individually entails the danger of identifying idiosyncratic rather thanconserved features and the risk of missing important relationships that would be revealed by statisti39see a.n. jain, òscoring noncovalent protein ligand interactions: a continuous differentiable function tuned to computebinding affinities,ó journal of computeraided molecular design 10(5):427440, 1996; w. welch, j. ruppert, and a.n. jain, òhammerhead: fast, fully automated docking of flexible ligands to protein binding sites,ó chemistry & biology 3(6):449462, 1996; j.ruppert, w. welch, and a.n. jain, òautomatic identification and representation of protein binding sites for molecular docking,ó protein science 6(3):524533, 1997; a.n. jain, òsurflex: fully automatic flexible molecular docking using a molecularsimilaritybased search engine,ó journal of medicinal chemistry 46(4):499511, 2003; a.n. jain, òligandbased structural hypotheses for virtual screening.ó journal of medicinal chemistry 47(4):947961, 2004.40section 5.4.1.4 is based on material provided by liping wei, nexus genomics, inc., and russ altman, stanford university,personal communication, december 4, 2003.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.computational modeling and simulation as enablers for biological discovery137cally pooling relevant data. the expected surfeit of protein structures provides an opportunity to develop computational methods for collectively examining multiple biological structures and extractingkey biophysical and biochemical features, as well as methods for automatically recognizing these features in new protein structures.wei and altman have developed an automated system known as feature that statistically studies the important functional and structural sites in protein structures such as active sites, binding sites,disulfide bonding sites, and so forth. feature collects all known examples of a type of site from theprotein data bank (pdb) as well as a number of control ònonsiteó examples. for each of them, feature computes the spatial distributions of a large set of defined biophysical and biochemical properties spanning multiple levels of details in order to capture conserved features beyond basic amino acidsequence similarity. it then uses a nonparametric statistical test, the wilcoxin rank sum test, to find thefeatures that are characteristic of the sites, in the context of control nonsites. figure 5.3 shows thestatistical features of calcium binding sites.by using a bayesian scoring function that recognizes whether a local region within a threedimensional structure is likely to be any of the sites and a scanning procedure that searches the wholestructure for the sites, feature can also provide an initial annotation of new protein structures.feature has been shown to have good sensitivity and specificity in recognizing a diverse set of sitetypes, including active sites, binding sites, and structural sites and is especially useful when the sites donot have conserved residues or residue geometry. figure 5.4 shows the result of searching for atp(adenosine triphosphate) binding sites in a protein structure.figure 5.2a computational approach to molecular docking. source: courtesy of a.n. jain, university ofcalifornia, san francisco.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.138catalyzing inquiryvolumecalcium model012345atomnameisany<>>>atomnameisc<<>>>atomnameisn<<>atomnameiso<>>>>>amide<<>amine<carbonyl<>>>>ringsystem<peptide<>>>vdwvolume<<>>charge>>>negcharge>>>chargewithhis>>>hydrophobicity<<<<mobility<>>solventaccessibility<>residuenameisasn>>>>residuenameisasp>>>>>residuenameisglu>>>>>residuenameisgly>>>residuenameisile>residuenameisleu>residuenameislys>residuenameisser>>residuenameisval<<residuenameishoh>residueclass1ishydrophobic<<residueclass1ischarged>>>>>residueclass1ispolar<>>>residueclass1isunknown>>residueclass2isnonpolar<<residueclass2ispolar<>>residueclass2isbasic<residueclass2isacidic>>>>>residueclass2isunknown>>secondarystructure1isturn>secondarystructure1isbend>>>>>secondarystructure1iscoil>>>>>secondarystructure1ishet>>secondarystructure2isbeta<>>secondarystructure2iscoil>>>>>secondarystructure2ishet>>figure 5.3statistical features of calcium binding sites determined by feature. the volumes in this case correspond to concentrate radial shells 1 † in thickness around the calcium ion or a control nonsite location. thecolumn shows properties that are statistically significantly different (at pvalue cutoff of 0.01) in at least onevolume between known examples of calcium binding sites and those of control nonsites. a ò>ó (greater than sign)indicates that the calcium binding sites have significantly higher value for that property at that volume comparedto control nonsites. a ò<ó (less than sign) indicates the opposite. an empty box indicates the lack of statisticallysignificant difference. source: courtesy of liping wei, nexus genomics, inc., and russ altman, stanford university, personal communication, december 4, 2003.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.computational modeling and simulation as enablers for biological discovery1395.4.2cell biology and physiology5.4.2.1cellular modeling and simulation effortscellular simulation requires a theoretical framework for analyzing the interactions of molecularcomponents, of modules made up of those components, and of systems in which such modules arelinked to carry out a variety of functions. the theoretical goal is to quantitatively organize, analyze, andinterpret complex data on cell biological processes, and experiments provide images, biochemical andelectrophysiological data on the initial concentrations, kinetic rates, and transport properties of themolecules and cellular structures that are presumed to be the key components of a cellular event.41 asimulation embeds the relevant rate laws and rate constants for the biochemical transformations beingmodeled. based on these laws and parameters, the model accepts as initial conditions the initial concentrations, diffusion coefficients, and locations of all molecules implicated in the transformation, andgenerates predictions for the concentration of all molecular species as a function of time and space.these predictions are compared against experiment, and the differences between prediction and experiment are used to further refine the model. if the system is perturbed by the addition of a ligand,electrical stimulus, or other experimental intervention, the model should be capable of predictingchanges as well in the relevant spatiotemporal distributions of the molecules involved.figure 5.4results of automatic scanning for atp binding sites in the structure of casein kinase (pdb id 1csn)using webfeature, a freely available, webbased server of feature. the solid red dots show the prediction offeature, they correspond correctly with the true location of the atp binding site, shown as white cloud.source: courtesy of liping wei, nexus genomics, inc., and russ altman, stanford university, personal communication, december 4, 2003.41a brief introduction to the rationale underlying cellular modeling can be found at the national resource for cell analysisand modeling (http://www.nrcam.uchc.edu/applications/applications.html).catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.140catalyzing inquirythere are many different tools for simulating and analyzing models of cellular systems (table 5.1).more general tools, such as mathematica and matlab or other systems that can be used for solvingsystems of differential or stochasticdifferential equations, can be used to develop simulations, andbecause these tools are commonly used by many researchers, their use facilitates the transfer of modelsamong different researchers. another approach is to link data gathering and biological informationsystems to software that can integrate and predict behavior of interacting components (currently, researchers are far from this goal, but see box 5.5 and box 5.6). finally, several platformindependentmodel specification languages are under development that will facilitate greater sharing andinteroperability. for example, sbml,42 gepasi,43 and cellml44 are specialized systems for biologicaland biochemical modeling. madonna45 is a generalpurpose system for solving a variety of equations(differential equations, integral equations, and so on).rice and stolovitzky describe the task of inferring signaling, metabolic, or gene regulatory pathways from experimental data as one of reverse engineering.46 they note that automated, highthroughput methods that collect species and tissuespecific datasets in large volume can help to deal with therisks in generalizing signaling pathways from one organism to another. at the same time, fully detailedkinetic models of intracellular processes are not generally feasible. thus, one step is to consider modelsthat describe network topology (i.e., that identify the interactions between nodes in the systemñgenes,proteins, metabolites, and so on). a model with more detail would describe network topology that iscausally directional (i.e., that specifies which entities serve as input to others). box 5.7 provides moredetail.table 5.1sample simulation programsnamedescriptorsaweb sitegepasi/copasifkfwhttp://gepasi.dbs.aber.ac.uk/softw/gepasi.htmlbiosimqwmuhttp://www.molgen.mpg.de/~biosim/biosim/biosimhome.htmljarnackrfbfwshttp://members.tripod.co.uk/sauro/jarnac.htmmcellrsuhttp://www.mcell.cnl.salk.edu/virtual cellksdfwmuhttp://www.nrcam.uchc.edu/ecellkwushttp://www.ecell.org/neuronksfwmushttp://neuron.duke.edu/genesisksushttp://www.bbb.caltech.edu/genesis/genesis.htmlplaskfbfwhttp://correio.cc.fc.ul.pt/~aenf/plas.htmlingeneueqkfmwushttp://www.ingeneue.org/dynafitkfwhttp://www.biokin.com/dynafit/stochsimrshttp://www.zoo.cam.ac.uk/compcell/stochsim.htmlt7 simulatorkushttp://virus.molsci.org/t7/molecularizer/stochastiratorkrushttp://opnsrcbio.molsci.org/alpha/comps/sim.htmlnote: all packages have facilities for chemical kinetic simulation of one sort or another. some are better designed for metabolicsystems, others for electrochemical systems, and still others for genetic systems.athe descriptors are as follows: b, bifurcation analyses and steadystate calculation; f, flux balance or metabolic control andrelated analyses; k, deterministic kinetic simulation; q, qualitative simulation; r, stochastic process models; s, spatial processes; d,database connectivity; f, fitting, sensitivity, and optimization code; m, runs on macintosh; s, source code available; u, runs onlinux or unix; w, runs on windows.42see http://www.cds.caltech.edu/erato/sbml/.43see http://www.gepasi.org/.44see http://www.cellml.org/.45see http://www.berkeleymadonna.com/.46 j.j. rice and g. stolovitzky, òmaking the most of it: pathway reconstruction and integrative simulation using the data athand,ó biosilico 2:7077, 2004.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.computational modeling and simulation as enablers for biological discovery141an example of a cellular simulation environment is ecell, an opensource system for modelingbiochemical and genetic processes. organizationally, ecell is an international research project aimedat developing theoretical and functioning technologies to allow precise òwhole celló simulation; it issupported by the new energy and industrial technology development organization (nedo) of japan.ecell simulations allow a user to model hypothetical virtual cells by defining functions of proteins, proteinprotein interactions, proteindna interactions, regulation of gene expression, and otherfeatures of cellular metabolism.47 based on reaction rules that are known through experiment andassumed concentrations of various molecules in various locations, ecell numerically integrates differential equations implicitly described in these reaction rules, resulting in changes over time in theconcentrations of proteins, protein complexes, and other chemical compounds in the cell.developers hope ecell will ultimately allow investigators a cheap, fast way to screen drugcandidates, study the effects of mutations or toxins, or simply probe the networks that govern cellbehavior. one application of ecell has been to construct a model of a hypothetical cell capable ofbox 5.5biospicebiospice, the biological simulation program for intracellular evaluation, is in essence a modeling frameworkthat provides users with model components, tools, databases, and infrastructure to develop predictive dynamical models of cellular function. biospice seeks to promote a synergy between experiment and model, inwhich model predictions drive experiment and experimental results identify areas in which a given modelneeds to be improved, and the intent is that researchers go from data to models to analysis and hypothesisgeneration, iteratively refining their understanding of the biological processes.an important component of biospice is a library of experimentally validated (and hence trusted) modelcomponents that can be used as starting points in largerscale simulations, as elements from this library arecomposed in new ways or adapted to investigate other biological systems. many biological parts and processes are represented as components, including phosphorylization events, chemotaxis, and conserved elementsof various pathways. also, because biospice is designed as an opensource environment, it is hoped that theuser community itself will make available a repertoire of model components that span a wide range of spatial,temporal, and functional scales, including those that simulate a single chemical reaction with high fidelity,those that simulate entire pathways, and those that simulate more abstract higherorder motifs.biospice tools are intended to enable researchers to use public databases and local resources to formulate aqualitative description of the cellular process of interest (e.g., models of networks or pathways), to annotatethe links between entities with biochemical interactions, and finally to convert this annotated qualitativedescription to a set of equations that can be analyzed and simulated. in addition, biospice provides a numberof simulation engines with the capability to simulate ordinary, stochastic, and partial differential equationsand other tools that support stability and bifurcation analysis and qualitative reasoning that combines probabilistic and temporal logic.source: sri kumar, defense advanced research projects agency, june 30, 2003.47see http://www.ecell.org/project/. for a view of the computer science challenges, see also k. takahashi, k. yugi, k.hashimoto, y. yamada, c.j.f. pickett, and m. tomita, òcomputational challenges in cell simulation: a software engineeringapproach,ó ieee intelligent systems 17(5):6471, 2002.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.142catalyzing inquirytranscription, translation, energy production, and phospholipid synthesis with only 127 genes. most ofthese genes were taken from mycoplasma genitalium, the organism with the smallest known chromosome (the complete genome sequence is 580 kilobases).48 ecell has also been used to construct acomputer model of the human erythrocyte,49 to estimate a gene regulatory network and signalingbox 5.6cytoscapea variety of computeraided models has been developed to simulate biological networks, typically focusingon specific cellular processes or single pathways.1 cytoscape is a modeling environment particularly suitedto the analysis of global data on network interactions (from highthroughput screens for proteinprotein, proteindna, and gene interactions) and on network states (including data on gene expression, protein abundance, and metabolite concentrations.) the javabased, opensource software uses plugins to incorporateanalyses of individual processes and pathways.2a model in cytoscape is organized as a network graph, with molecular species represented as nodes andinteractions represented as edges between nodes. nodes and edges are mapped to specific data values calledattributes that can be text strings, discrete or continuous numbers, urls, or lists, either loaded from a datarepository or generated dynamically. layered onto attributes are annotations, which represent a hierarchicalclassification of progressively more specific descriptions (such as functions) of groups of nodes and edges. it ispossible to have many levels of annotation active simultaneously, each displayed as a different attribute of anode or edge. to visualize the network, cytoscape supports several layout algorithms that fix the relativelocations of specific nodes and edges in the graphical window. an attributetovisual mapping facility allowsattributes to determine the appearance (color, shape, size) of their associated nodes and edges. graph selection and filtering reduces the complexity of the network by selectively displaying subsets of nodes and edgesaccording to a variety of criteria.cytoscapeõs plugin extensibility addresses the challenge of bridging highlevel information (relationships amongnetwork components) with lowerlevel information (reaction rates, binding constants) of specific processes. aplugin that organizes the network layout according to putative functional attributes of genes was used to studyenergy transduction pathways in halobacterium.3 another plugin allows cytoscape to simulate stochasticsbmlbiochemical models.4 the authors hope a community will further develop and enhance cytoscape.1a. gilman and a.p. arkin, ògenetic ôcodeõ: representations and dynamical models of genetic components and networks,ó annualreview of genomics and human genetics 3:341369, 20022p. shannon, a. markiel, o. ozier, n.s. baliga, j.t. wang, d. ramage, n. amin, et al., òintegrated models of biomolecular interactionnetworks,ó genome research 13:24982504, 2003.3n.s. baliga, m. pan, y.a. goo, e.c. yi, d.r. goodlett, k. dimitrov, p. shannon, et al., òcoordinate regulation of eenergy transductionmodules in halobacterium species analyzed by a global systems approach,ó proceedings of the national academy of sciences99(23):1491314918, 2002.4m. hucka, a. finney, h.m. sauro, h. bolouri, j. doyle, and h. kitano, òthe erato systems biology workbench: enabling interactionand exchange between software tools for computational biology,ó pacific symposium in biocomputing, 450461, 2002.source: adapted from p. shannon, a. markiel, o. ozier, n.s. baliga, j.t. wang, d. ramage, n. amin et al., òcytoscape: a softwareenvironment for integrated models of biomolecular interaction networks,ó genome research 13(11):24982504, 2003.48m. tomita, k. hashimoto, k. takahashi, y. matsuzaki, r. matsushima, k. saito, k. yugi, et al., òecell project overview:towards integrative simulation of cellular processes,ó genome informatics 9:242243, 1998, available at http://giw.ims.utokyo.ac.jp/giw98/cdrom/posterpdf/poster02.pdf.49m. tomita et al., òin silico analysis of human erythrocyte using ecell system,ó poster session, the future of biology in the21st century: 2nd international conference on systems biology, california institute of technology, pasadena, november 47,2001, available at http://www.icsb2001.org/posters/032kinoshita.pdf.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.computational modeling and simulation as enablers for biological discovery143pathway involved in the circadian rhythm of synechococcus sp. pcc 7942,50 and to model mitochondrialenergy metabolism and metabolic pathways in rice.51another cellular simulation environment is the virtual cell, developed at the university of connecticut health center.52 the virtual cell is a tool for experimentalists and theoreticians for computationallytesting hypotheses and models. to address a particular question, these mechanisms (chemical kinetics,membrane fluxes and reactions, ionic currents, and diffusion) are combined with a specific set of experimental conditions (geometry, spatial scale, time scale, stimuli) and applicable conservation laws to specifybox 5.7pathway reconstruction: a systems approachon topology.in this level, we are only concerned with identifying the interaction between nodes (genes, proteins, metabolites,etc.) in the system. the goal is the generation of a diagram of nondirectional connections between all interactingnodes. for example, many have sought to develop largescale maps of proteinðprotein interactions derived fromvarious sources. twohybrid studies have produced genomewide interaction maps for e. coli bacteriophage t7,yeast, drosophila, and c. elegans. although this approach can be comprehensive in regard to being genome wide,many interactions are not reproducible (a potential source of false negatives) and putative interactions occur between unlikely protein combinations (a potential source of false positives). . . . another approach to constructinglargescale connection maps is by mining databases. specific databases of protein interactions are being developed,the largest of which are dip and bind. these databases combine data from many highthroughput experimentsalong with data from other sources, such as published literature. . . . along other lines, investigators have attemptedto identify topological links by analyzing the dynamic behavior of networks. pioneering work in this area shows thatmetabolic network topologies can be derived from correlation of timeseries measurements of species concentrations. the method is further refined to better identify connections in nonlinear systems using mutual informationinstead of correlation. in another method, pairwise correlation of gene expression data is used to predict functionalconnections that could then be combined into òrelevance networksó of linked genes. other methods may seek to usesome combination of data sources, although this may not be completely straightforward.on inferring qualitative connections.in this level, we include not only associations between cellular entities but also the causal relations of such associations, such as which entities serve as input to others. . . . researchers have proposed methods that infer connectivities from the estimations of the jacobian matrix for metabolic, signaling, and genetic networks. ross and coworkershave proposed a method based on propagated perturbations of chemical species that can reconstruct causal sequences of reactions from synthetic and experimental data. to reconstruct gene regulatory systems, methods includefuzzy logic analysis of facilitator/repressor groups in the yeast cell cycle and reconstruction of binary networks.however, the wide application of such methods is often limited because the continuous nature of many biologicalsystems prevents easy abstractions into coarser signals. recently, there has been considerable work using bayesiannetwork inference. examples include inferring gene regulation using gene expression data from the yeast cell cycleor using data from synthetic gene networks.source: reprinted by permission from j.j. rice and g. stolovitzky, òmaking the most of it: pathway reconstruction and integrativesimulation using the data at hand,ó biosilico 2(2):7077. copyright 2004 elsevier. (references omitted.)50f. miyoshi et al., òestimation of genetic networks of the circadian rhythm in cyanobacterium using the ecell system,óposter session, presented at usjapan joint workshop on systems biology of useful microorganisms, september 618, 2002, keiouniversity, yamagata, japan, available at http://nedodoe.jtbcom.co.jp/abstracts/35.pdf.51e. wang et al., òerice project: reconstructing plant cell metabolism using ecell system,ó poster session presented atsystems biology: the logic of lifeñ3rd international conference on systems biology, december 1315, 2002, karolinskainstitutet, stockholm, available at http://www.ki.se/icsb2002/pdf/icsb222.pdf.52l.m. loew and j.c. schaff, òthe virtual cell: a software environment for computational cell biology,ó trends in biotechnology 19(10):401406, 2001.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.144catalyzing inquirya concrete system of differential and algebraic equations. this experimental geometry may assume wellmixed compartments or a one, two, or threedimensional spatial representation (e.g., experimental images from a microscope). models are constructed from biochemical and electrophysiological data mappedto appropriate subcellular locations in images obtained from a microscope. a variety of modeling approximations are available including pseudosteady state in time (infinite kinetic rates) or space (infinite diffusion or conductivity). in the case of spatial simulations, the results are mapped back to experimentalimages and can be analyzed by applying the arsenal of imageprocessing tools that is familiar to a cellbiologist. section 5.4.2.4 describes a study undertaken within the virtual cell framework.simulation models can be useful for many purposes. one important use is to facilitate an understanding of what design properties of an intracellular network are necessary for its function. for example, vondassow et al.53 used a simulation model of the gap and pairrule gene network in drosophila melanogasterto show that the structure of the network is sufficient to explain a great deal of the observed cellularpatterning. in addition, they showed that the network behavior was robust to parameter variation uponthe addition of hypothetical (but reasonable) elements to the known network. thus, simulations can alsobe used to formally propose and justify new hypothetical mechanisms and predict new network elements.another use of simulation models is in exploring the nature of control in networks. an example ofexploring network control with simulation is the work of chen et al..54 in elucidating the control of differentphases of mitosis and explaining the impact of 50 different mutants on cellular decisions related to mitosis.simulations have also been used to model metabolic pathways. for example, edwards and palssondeveloped a constraintbased genomescale simulation of escherichia coli metabolism (box 5.8). by applying successive constraints (stoichiometric, thermodynamic, and enzyme capacity constraints) to themetabolic network, it is possible to impose limits on cellular, biochemical, and systemic functions,thereby identifying all allowable solutions (i.e., those that do not violate the applicable constraints).compared to the detailed theorybased models, such an approach has the major advantage that it doesnot require knowledge of the kinetics involved (since it is concerned only with steadystate function).(on the other hand, it is impossible to implement without genomescale knowledge, because onlygenomescale knowledge can bound the system in question.) within the space of allowable solutions, aparticular solution corresponds to the maximization of some selected function, such as cellular growthor a response to some environmental change. a more robust model accounting for a larger number ofpathways is also described in box 5.8.the edwards and palsson model has been used to predict the evolution of e. coli metabolism undera variety of environmental conditions. in the words of ibarra et al., òwhen placed under growthselection pressure, the growth rate of e. coli on glycerol reproducibly evolved over 40 days, or about 700generations, from a suboptimal value to the optimal growth rate predicted from a wholecell in silicomodel. these results open the possibility of using adaptive evolution of entire metabolic networks torealize metabolic states that have been determined a priori based on in silico analysis.ó55simulation models can also be used to test design ideas for engineering networks in cells. forexample, very simple models have been used to provide insight into a genetic oscillator and a switch ine. coli.56 models have also been used to test designs for the control of cellular networks, as illustrated by53g. von dassow, e. meir, e.m. munro, and g.m. odell, òthe segment polarity network is a robust developmental module,ónature 406(6792):188192, 2000.54k.c. chen, a. csikasznagy, b. gyorffy, j. val, b. novak, and j.j. tyson, òkinetic analysis of a molecular model of thebudding yeast cell cycle,ó molecular biology of the cell 11(1):369391, 2000.55r.u. ibarra, j.s. edwards, and b.o. palsson, òescherichia coli k12 undergoes adaptive evolution to achieve in silico predicted optimal growth,ó nature 420(6912):186189, 2002.56m.b. elowitz and s. leibler, òa synthetic oscillatory network of transcriptional regulators,ó nature 403(6767):335338,2000; t.s. gardner, c.r. cantor, and j.j. collins, òconstruction of a genetic toggle switch in escherichia coli,ó nature403(6767):339342, 2000.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.computational modeling and simulation as enablers for biological discovery145endy and yin in using their t7 model to propose a pharmaceutical strategy for preventing both t7propagation and the development of drug resistance through mutation.57given observed cell behavior, simulation models can be used to suggest the necessity of a givenregulatory motif or the sufficiency of known interactions to produce the phenomenon. for example, qiet al. demonstrate the sufficiency of membrane energetics, protein diffusion, and receptorbindingkinetics to generate a particular dynamic pattern of protein location at the synapse between two immune cells.58the following sections describe several simulation studies in more detail.box 5.8escherichia coli constraintbased modelsa. in silico model1the escherichia coli mg1655 genome has been completely sequenced. the annotated sequence, biochemical information, and other information were used to reconstruct the e. coli metabolic map. the stoichiometric coefficients foreach metabolic enzyme in the e. coli metabolic map were assembled to construct a genomespecific stoichiometricmatrix. the e. coli stoichiometric matrix was used to define the systemõs characteristics and the capabilities of e. colimetabolism. the effects of gene deletions in the central metabolic pathways on the ability of the in silico metabolicnetwork to support growth were assessed, and the in silico predictions were compared with experimental observations. it was shown that based on stoichiometric and capacity constraints the insilico analysis was able to qualitatively predict the growth potential of mutant strains in 86% of the cases examined. herein, it is demonstrated that thesynthesis of in silico metabolic genotypes based on genomic, biochemical, and strainspecific information is possible, and that systems analysis methods are available to analyze and interpret the metabolic phenotype.b. genomescale model2an expanded genomescale metabolic model of e. coli (ijr904 gsm/gpr) has been reconstructed which includes904 genes and 931 unique biochemical reactions. the reactions in the expanded model are both elementally andcharge balanced. network gap analysis led to putative assignments for 55 open reading frames (orfs). gene toprotein to reaction associations (gpr) are now directly included in the model. comparisons between predictionsmade by ijr904 and ije660a models show that they are generally similar but differ under certain circumstances.analysis of genomescale proton balancing shows how the flux of protons into and out of the medium is importantfor maximizing cellular growth. . . . e. coli ijr904 has improved capabilities over ije660a [a model that accounted for660 genes and 627 unique biochemical reactions and was itself a slight modification of the original model describedin the above paragraph]. ijr904 is a more complete and chemically accurate description of e. coli metabolism thanije660a. perhaps most importantly, ijr904 can be used for analyzing and integrating the diverse datasets. ijr904 willhelp to outline the genotypephenotype relationship for e. coli k12, as it can account for genomic, transcriptomic,proteomic and fluxomic data simultaneously.1reprinted from j.s. edwards and b.o. palsson, òthe escherichia coli mg1655 in silico metabolic genotype: its definition, characteristics, andcapabilities,ó proceedings of the national academy of sciences 97(10): 55285533, 2000. copyright 2000 national academy of sciences.2j.l. reed, t.d. vo, c.h. schilling, and b.o. palsson, òan expanded genomescale model of escherichia coli k12 (ijr904 gsm/gpr),ógenome biology 4(9): article r54, 2003, available at http://genomebiology.com/2003/4/9/r54. reprinted by permission of the authors.57d. endy and j. yin, òtoward antiviral strategies that resist viral escape,ó antimicrobial agents and chemotherapy 44(4):10971099, 2000.58s.y. qi, j.t. groves, and a.k. chakraborty, òsynaptic pattern formation during cellular recognition,ó proceedings of thenational academy of sciences 98(12):65486553, 2001.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.146catalyzing inquiry5.4.2.2cell cycle regulationbiological growth and reproduction depend ultimately on the cycle of dna synthesis and physicalseparation of the replicate dna molecules within individual cells. in eukaryotes, these processes aretriggered by cyclindependent protein kinases (cdks). in fission yeast, cdk activity (cdc2 = kinasesubunit, cdc13 = cyclin subunit) is regulated by a network of protein interactions (figure 5.5), includingcyclin synthesis and degradation, phosphorylation of cdc2, and binding to an inhibitor.a network of such complexity, with multiple feedback loops, cannot be understood thoroughly bycasual intuition. instead, the network is converted into a set of nonlinear differential equations, and thephysiological implications of these equations are studied.59 numerical simulation of the equations(figure 5.6) provides complete time courses of every component and can be interpreted in terms ofobservable events in the cell cycle. simulations can be run, not only of wildtype cells but also of dozensof mutants constructed by deleting or overexpressing each component singly or in multiple combinations. from the observed phenotypes of these mutants it is possible to reverseengineer the regulatorynetwork and the set of kinetic constants associated with the component reactions.figure 5.5the cellcycle control system in fission yeast. this system can be divided into three modules, whichregulate the transitions from g1 into s phase, from g2 into m phase, and exit from mitosis. source: j.j. tyson, k.chen, and b. novak, ònetwork dynamics and cell physiology,ó nature reviews of molecular cell biology 2(12):908916, 2001. figure and caption reproduced with permission from nature reviews of molecular cell biology. copyright2001 macmillan magazines ltd.59j.j. tyson, k. chen, and b. novak, ònetwork dynamics and cell physiology,ó nature reviews: molecular cell biology 2(12):908916, 2001; j.j. tyson, a. csikasznagy, and b. novak, òthe dynamics of cell cycle regulation,ó bioessays 24(12):10951109, 2002.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.computational modeling and simulation as enablers for biological discovery147for understanding the dynamics of molecular regulatory systems, bifurcation theory is a powerfulcomplement to numerical simulation. the bifurcation diagram in figure 5.7 presents recurrent solutions(steady states and limit cycle oscillations) of the differential equations as functions of cell size. thecontrol system has three characteristic steady states: low cdc2 activity (g1 = prereplication), mediumcdc2 activity (s/g2 = replication and postreplication), and high cdc2 activity (m = separation of replicated dna molecules). g1 and s/g2 are stable steady states; m is unstable because of a negativefeedback loop as shown in figure 5.5 (cdc2cdc13 activates slp1, which degrades cdc13).when the time courses of size and cdc2 activity from figure 5.6 are superimposed on the bifurcationdiagram (curve labeled òsizeó), one sees how progress through the cell cycle is governed by the bifurcations that turn stable steady states into unstable steady states and/or stable oscillations. a mutationchanges a specific rate constant, which changes the locations of the bifurcation points in figure 5.7,which changes how cells progress through (or halt in) the cell cycle. by this route one can trace thedynamical consequences of genetic information all the way to observable cell behavior.figure 5.6simulated time courses of cdc2 and related proteins during the cell cycle of fission yeast. numericalintegration of the full set of differential equations that describe the wiring diagram in figure 5.5 yields these timecourses. time is expressed in minutes; all other variables are given in arbitrary units. òsizeó refers to the number ofribosomes per nucleus. notice the brief g1 phase, when ste9 is active and rum1 is abundant. after a long s/g2 phase,during which cdc2 is tyrosine phosphorylated, the cell enters m phase, when cdc25 removes the inhibitory phosphategroup. after some delay, slp1 activates and degrades cdc13. as cdc2ðcdc13 activity falls, the cell exits mitosis. sizedecreases twofold at nuclear division. source: j.j. tyson, k. chen, and b. novak, ònetwork dynamics and cellphysiology,ó nature reviews of molecular cell biology 2(12):908916, 2001. figure and caption reproduced with permission from nature reviews of molecular cell biology. copyright 2001 macmillan magazines ltd.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.148catalyzing inquiry5.4.2.3a computational model to determine the effects of snps inhuman pathophysiology of red blood cellsthe completion of the human genome project has led to the construction of single nucleotidepolymorphism (snp) maps. single nucleotide polymorphisms are common dna sequence variationsamong individuals. a result of the construction of snp maps is to determine the effects of snps on thedevelopment of disease(s) since sequence variations can lead to altered biological function or disease.currently, it is difficult to determine the causal relationship between the variations in sequence,snps, and the physiological function. one way to analyze this relationship is to create computationalmodels or simulations of biological processes. since erythrocyte (red blood cell) metabolism has beenstudied extensively over the years and many snps have been characterized, jamshidi et al. used thisinformation to build their computational models.60two important metabolic enzymes, glucose6phosphate dehydrogenase (g6pd) and pyruvate kinase (pk), were studied for alterations in their kinetic properties in an in silico model to calculate theoverall effect of snps on red blood cell function. defects in these enzymes cause hemolytic anemia.figure 5.7bifurcation diagram for the full cellcycle control network. . . . [t]he full diagram is not a simple sum of thebifurcation diagrams of its modules. in particular, oscillations around the m state are greatly modified in the compositecontrol system. superimposed on the bifurcation diagram is a òcellcycle orbitó (line on the right with arrows): fromthe time courses in figure 5.6, we plot size on the abscissa and cdc2ðcdc13 activity on the ordinate for representativetimes between birth and division. notice that, at small cell size, all three modules support stable steady states. noticehow the cellcycle orbit follows the attractors of the control system. source: j.j. tyson, k. chen and b. novak,ònetwork dynamics and cell physiology,ó nature reviews molecular cell biology 2(12):908916, 2001. figure and caption,reproduced with permission from nature reviews molecular cell biology. copyright 2001 macmillan magazines ltd.60n. jamshidi, s.j. wiback, and b.o. palsson, òin silico modeldriven assessment of the effects of single nucleotide polymorphisms (snps) on human red blood cell metabolism,ó genome research 12(11):16871692, 2002.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.computational modeling and simulation as enablers for biological discovery149clinical data taken from the published literature were used for the measured values of the kineticparameters. these values were then used in model simulations to determine whether a direct link couldbe established between the snp and the disease (anemia).the computational modeling revealed two results. for the g6pd and pk variants analyzed, thereappeared to be no clear relationship between their kinetic properties as a function of sequence variationor snp. however, upon assessment of overall biological function, a correlation was found between thesequence variation of g6pd and the severity of the clinical disease. thus, in silico modeling of biologicalprocesses may aid in analysis and prediction of snps and pathophysiological conditions.5.4.2.4spatial inhomogeneities in cellular developmentsimulation models can be used to provide insight into the significance of spatial inhomogeneities.for example, the interior of living cells does not resemble at all a uniform aqueous solution of dissolvedchemicals, and yet this is the implicit assumption underlying many views of the cell. this assumptionserves traditional biochemistry and molecular biology reasonably well, but research increasingly demonstrates that the physical locations of specific molecules are crucial. multiprotein complexes act asmachines for internal movements or as integrated circuits in signaling. messenger rna molecules aretransported in a highly directed fashion to specific regions of the cell (in nerve axons, for example). cellsadopt highly complex shapes and undergo complex movements thanks to the matrix of protein filaments and associated proteins within their cytoplasm.5.4.2.4.1unraveling the physical basis of microtubule structure and stabilitymicrotubules are cylindrical polymers found in every eukaryotic cell. microtubles play a role in cellular architecture and asmolecular train tracks used to transport everything from chromosomes to drug molecules. an understanding of microtubule structure and function is key not just to unraveling fundamental mechanismsof the cell, but also to opening the way to the discovery of new antiparasitic and anticancer drugs.until now, researchers have known that the microtubules, constructed of units called protofilamentsin a hollow, helical arrangement, are rigid but not static, and undergo periods of growth and suddencollapse. yet the mechanism for this constructiondestruction had eluded researchers.over the past several years, mccammon and his colleagues have pioneered the use of a combination of an atomically detailed model for a microtubule and largescale computations using the adaptivepoissonboltzmann solver to create a highresolution, 1.25millionatom map of the electrostatic interactions within the microtubule.61more recently, david sept and nathan baker of washington university and mccammon used thesame technique to successfully predict the helicity of the tubule with a striking correspondence toexperimental observation.62 based on the lateral interactions between protofilaments, they determinedthat the microtubule prefers to be in a configuration in which the protofilaments assemble with a seamat each turn, rather than spiraling smoothly upward with alpha and beta monomers wrapping themicrotubule as if it were a barberõs pole. at the end of each turn, a chain of alphas is trailed by a chainof betas, then after that turn, a chain of alphas, and so on. it is as if the red and white stripes on thebarberõs pole traded places with every twist (figure 5.8).61n.a. baker, d. sept, s. joseph, m.j. holst, and j.a. mccammon, òelectrostatics of nanosystems: application to microtubulesand the ribosome,ó proceedings of the national academy of sciences 98(18):1003710041, 2001.62d. sept, n.a. baker, and j.a. mccammon, òthe physical basis of microtubule structure and stability,ó protein science12(10):22572261, 2003.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.150catalyzing inquiry5.4.2.4.2the movement of listeria bacteriaalberts and odell have developed a computational modelof listera monocytogenes based on an explicit simulation of a large number of monomerscale biochemical and mechanical interactions,63 representing all proteinprotein binding interactions with onrate andoffrate kinetic equations. these equations characterize individual actin filaments: the bulk properties ofthe actin ògeló arise from the contributions of the many individual filaments of the actin network; andthe growth of any particular filament depends on that filamentõs precise location, orientation, andbiochemical state, all of which change through time. mechanical interactions, which resolve collisionsand accommodate the stretching of proteinprotein linkages, follow newtonõs laws.the model is based on a large set of differential equations that determine how the relevant statevariables change with time. these equations are solved numerically, taking into account the fact thatdiscontinuities in time occur frequently as objects suddenly collide and as objects suddenly spring intoexistence or disappear (due to new filament nucleation and depolymerization). the model accommofigure 5.8the binding free energy between two protofilaments as a function of the subunit rise between adjacent dimmers. sept et al. used electrostatic calculations to determine the binding energy between two protofilaments as a function of the subunit rise between adjacent dimers. viewed from the growing (+) end of the tubule,the graph demonstrates the most favorable configuration at various points during assembly. source: reprintedby permission from d. sept, n.a. baker, and j.a. mccammon, òthe physical basis of microtubule structure andstability,ó protein science 12:22572261, 2003. copyright 2003 by the protein society.63j.b. alberts and g.m. odell, òin silico reconstitution of listeria propulsion exhibits nanosaltation,ó plos biology 2(12):e412,2004, available at http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=532387.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.computational modeling and simulation as enablers for biological discovery151dates arbitrary geometries, explicit stochastic input, and specific smallscale events. because the modelis built from the ground up, it can predict emergent behavior that would not be apparent from intuitionor qualitative description of the behavior of individual parts. on the other hand, the simulation requiresmultiple runs of its stochastic, individual moleculebased model, and parametric relationships emergenot from closedform equations that demonstrate qualitative functional dependencies but from ensembles of many repeated simulations.the trajectories generated by this model of l. monocytogenes motility display repeated runs andpauses that closely resemble the actual nanoscale measurements of bacterial motion.64 further analysisof the simulation state at the beginning and ends of simulated pauses indicate that there is no characteristic stepsize or pause duration in these simulated trajectories and that pauses can be caused both bycorrelated brownian motion and by synchronously strained sets of actaactin filament mechanicallinks.5.4.2.4.3morphological control of spatiotemporal patterns of intracellular signalingfink andslepchenko studied calcium waves evoked by activation of the bradykinin receptor in the plasmamembrane of a neuronal cell.65 the neuromodulator bradykinin applied to the cells produced a calciumwave that starts in the neurite and spreads to the soma and growth cones. the calcium wave wasmonitored with digital microscope imaging of a fluorescent calcium indicator. the hypothesis was thatinteraction of bradykinin with its receptor on the plasma membrane activated production of inositol1,4,5trisphosphate (insp3) that diffused to its receptor on the endoplasmic reticulum, leading to calciumrelease.using the virtual cell software environment, they assembled a simulation model of this phenomenon.66 the model contained details of the relevant receptor distributions (via immunofluorescence)within the cell geometry, the kinetics of insp3 production (via biochemical analysis of insp3 in cellpopulations and photorelease of caged insp3 in individual cells), the transport of calcium through theinsp3 receptor calcium channel and the sarcoplasmic/endoplasmic reticulum calcium atpase (serca)pump (from literature studies of singlechannel kinetics and radioligand flux), and calcium buffering byboth endogenous proteins and the fluorescent indicator (from confocal measurements of indicatorconcentrations).the mathematical equations generated by this combination of molecular distributions and reactionand membrane transport kinetics were then solved to produce a simulation of the spatiotemporal patternof calcium that could be directly compared to the experiment. the characteristic calcium dynamics requires rapid, highamplitude production of [insp3]cyt in the neurite. this requisite insp3 spatiotemporalprofile is provided, in turn, as an intrinsic consequence of the cellõs morphology, demonstrating howgeometry can locally and dramatically intensify cytosolic signals that originate at the plasma membrane.in addition, the model predicts and experiments confirm that stimulation of just the neurite, but not thesoma or growth cone, is sufficient to generate a calcium response throughout the cell.64s.c. kuo and j.l. mcgrath, òsteps and fluctuations of listeria monocytogenes during actinbased motility,ó nature407(6807):10261029, 2000; j. mcgrath, n. eungdamrong, c. fisher, f. peng, l. mahadevan, t.j. mitchison, and s.c. kuo, òtheforcevelocity relationship for the actinbased motility of listeria moncytogenes,ó current biology 13(4):329332, 2003. (bothcited in alberts and odell, 2004.)65c.c. fink, b. slepchenko, i.i. moraru, j. schaff, j. watras, and l.m. loew, òmorphological control of inositol1,4,5trisphosphatedependent signals.ó journal of cell biology 147(5):929935, 1999; c.c. fink, b. slepchenko, i.i. moraru, j. watras,j.c. schaff, and l.m. loew, òan imagebased model of calcium waves in differentiated neuroblastoma cells,ó biophysicaljournal 79(1):163183, 2000.66b.m. slepchenko, j.c. schaff, i. macara, and l.m. loew, òquantitative cell biology with the virtual cell,ó trends in cellbiology 13(11):570576, 2003.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.152catalyzing inquiry67for purposes of the discussion in this subsection (section 5.4.3.1), regulation refers to cisregulation.68c.h. yuh, h. bolouri, and e.h. davidson, ògenomic cisregulatory logic: experimental and computational analysis of asea urchin gene,ó science 128(5):617629, 1998. some of this discussion is also adapted from commentary on this article: g.a.wray, òpromoter logic,ó science 279(5358):18711872, 1998.69in this context, a multifunctional organization of the regulatory system means that the protein associated with the endo16gene is differentially expressed in various cells in the sea urchin.5.4.3genetic regulationthe problem of genetic regulationñhow and under what circumstances and the extent to whichgenes are expressed as proteinsñis a central problem of modern biology. the issue originates in anapparent paradoxñevery cell in a complex organism contains the same dna sequences, and yet thereare many cell types in such organisms (blood cells, skin cells, and so on). in particular, the proteins thatcomprise any given cell type are different from those of other cell types, even though the genomicinformation is the same in both. nor is genomic information the whole story in developmentñcells alsorespond to their environment, and external signals coming into a cell from neighboring cells influencewhich proteins the cell makes.genetic regulation is an extraordinarily complex problem. molecular biologists distinguish between cisregulation and transregulation. cisregulatory elements for a given gene are segments of thegenome that are located in the vicinity of the structural portion of a gene and regulate the expression ofthe gene. transregulatory elements for a given gene refer to proteins not structurally associated with agene that nevertheless regulate its expression. the sections below provide examples of several constructs that help shed some light on both kinds of regulation.5.4.3.1cisregulation of transcription activity as process control computingit has been known for some time that the genome contains both genes and cisregulatory elements.67 the presence or absence of particular combinations of these regulatory elements determinesthe extent to which specific genes are expressed (i.e., transcribed into specific proteins). in pioneeringwork undertaken by davidson et al.,68 it was shown that cisregulation couldñin the case of a specificgeneñbe viewed as a logical process analogous to a computer program that connected various inputs toa single output determining the precise level of transcription for that gene.in particular, davidson and his colleagues developed a highlevel computer simulation of the cisregulatory system governing the expression of the endo16 gene in the sea urchin (endo16 is a gutspecificgene of the sea urchin embryo). in this context, the term òhighleveló means a highly abstracted representation, consisting at its core of 18 lines of code. this simulation enabled them to make predictionsabout the effect of specific manipulations of the various regulatory factors on endo16 transcription levelsthat could be tested against experiment.some of the inputs to the simulation were binary values. the value 1 indicated that a binding sitewas both present and productively occupied by the appropriate cisregulatory factor. a 0 indicated thatthe site was mutationally destroyed or inactive because its factor was not present or was inactive. theother inputs to the simulation were continuous and varied with time, and represented outputs (proteinconcentrations) in other parts of the system. the output of this process in some cases was a continuoustimevarying variable that regulated the extent to which the specific gene in question was transcribed.davidson et al. were able to confirm the predictions made by their computational model, concluding that all of the regulatory functions in question (and the resulting system properties) were encoded inthe dna sequence, and that the regulatory system described is capable of processing complex informational inputs and hence indicates the presence of a multifunctional organization of the endo16 cisregulatory system.69catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.computational modeling and simulation as enablers for biological discovery153the computational model undoubtedly provides a compact representation of the relationshipsbetween different inputs and different outputs.70 perhaps a more interesting question, however, is theextent to which it is meaningful to ascribe a computational function to the biochemical substrate underlying the regulatory system. davidson et al. argue that the dna sequence in this case specifies òwhat isessentially a hardwired, analog computational device,ó resulting in system properties that are òallexplicitly specified in the genomic dna sequence.ó71it is highly unlikely that the precise computational structure of endo16õs regulatory system willgeneralize to the regulatory systems of other genes. from the perspective of the biologist, the reason isclearñorganisms are not designed as generalpurpose devices. indeed, the evolutionary process virtually guarantees that individualized solutions and architectures will be abundant, because specific adaptations are the rule of the day. nevertheless, insight into the computational behavior of the endo16 cisregulatory system provides a new way of looking at biological behavior.can the regulatory systems of some other genes be cast in similar computational terms? if and whenfuture work demonstrates that such casting is possible, it will become increasingly meaningful to viewthe genome as thousands of simple computational devices operating in tandem. davidsonõs worksuggests the possibility that a class of regulatory mechanisms, complex though they might be withrespect to their behavior, may be governed by what are in essence hardwired devices whose essentialfunctionality can be understood in computational terms through a logic of operation that is in factrelatively simple at its core. prior to davidsonõs work and despite extensive research, the literature hadnot revealed any apparent regularity in the organization of regulatory elements or in the ways in whichthey interact to regulate gene expression.indeed, while many promoters appear either to have a simpler organization or to operate lesslogically than that of endo16, few promoters have been examined with the many precise quantitativeassays that were carried out by davidson et al., and nonquantitative assays would have completelymissed most of the functions that the majority of the regulatory systemõs elements encode.72 so, it is atthis point an open question whether this computational view has applicability beyond the specific caseof endo16. 5.4.3.2genetic regulatory networks as finitestate automatatransregulation (as contrasted to cisregulation) is based on the notion that some genes can haveregulatory effects on others.73 in reality, the network of connections between genes that regulate andgenes that are regulated is highly complex. in an attempt to gain insight into genetic regulatory networks from a gross oversimplification, kaufmann proposed that actual genetic regulatory networksmight be modeled as randomly connected boolean networks.74kaufmannõs model made several simplifying assumptions:70e.f. keller, making sense of life: explaining biological development with models, metaphors, and machines, harvard universitypress, cambridge, ma, 2002, p. 241.71this is not to argue that dna sequence alone is responsible for the specification of system properties. epigenetic controlmechanisms also influence system properties as do environmental conditions and cell state that are not specified in dna. ananalogy might be that although a memory dump of a computer specifies the state of the computer, many contingent activitiesmay affect the actual execution path. for example, the behavior (and timing) of specific inputoutput activities are likely to berelevant.72g.a. wray, òpromoter logic,ó science 279(5358):18711872, 1998.73for purposes of the discussion in this subsection (section 5.4.3.2), regulation refers to transregulation.74much of this work is due to the pioneering work of stuart kauffman. see for example, s.a. kauffman, the origins of order:selforganization and selection in evolution, oxford university press, new york, 1993. an alternative discussion of this materialcan be found at http://www.smi.stanford.edu/projects/helix/bmi214/ (may 13); lecture notes of russell altman.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.154catalyzing inquiry¥the total number of genes involved is n, a number of order 30,000.¥the number of genes that regulate a given target is a constant (call it k) for all regulated genes; kis a small integer.¥the regulatory signal associated with a connection or the expression of a gene is either on or off.(in fact, almost certainly it is not just the fact of a connection between genes that influences regulation,but rather the nature of that connection as a continuous timevarying value such as a molecular concentration over time.)¥every gene is governed by the same transition rule (i.e., a boolean function) that specifies its state(on or off) as a function of the activities of its k inputs at the immediately earlier time.¥the regulatory network operates synchronously (and, by implication, kinetics are unimportant).¥secondary effects on genetic regulation arising from the nondigital characteristics of dna (suchas methylation) can be neglected.¥the genes that regulate and genes that are regulated (which may overlap) are connected atrandom.box 5.9 provides more details about this model. because the model treats all genes as identical (i.e.,all obey the same transition rule) and assigns connections between genes at random, it obviously lacksfidelity to any specific genome and cannot predict the biological phenomenology of any specific organism. yet, it may provide insight into biological order that emerges from the structure of the geneticregulatory network itself.simulations of the operation of this model yielded interesting behavior, which depends on thevalues of n and k. for k = 1 or k > 5, the behavior of the network exhibits little interesting order, whereorder is defined in terms of fixed cycles known as attractors. if k = 1, the networks are static, with thenumber of attractors exponential in the size of the network and the cycle length approaching unity. ifk > 5, there are few attractors, and it is the cycle length that is exponential in the size of the network.however, for k = 2, the network does exhibit order that has potential biological significanceñboth thenumber of attractors and the cycle length are proportional to n1/2.75what might be the biological significance of these results?¥the trajectory of an attractor through its successive states would reflect the fact that, over time,different genes are expressed in a biological organism.¥the fact that there are multiple attractors within the same genome suggests that multiple biological structures might exist, even within the same organism, corresponding to the genome being in one ofthese attractor states. an obvious candidate for such structures would be multiple cell types. that is,this analysis suggests that a cell type corresponds to a given state cycle attractor, and the differentattractors to the different cell types of the organism. another possibility is that different but similarattractors correspond to cells in different states (e.g., disease state, resting state, perturbed state).¥the fact that an attractor is cyclic suggests that it may be related to cyclic behavior in a biologicalorganism. if cell types can be identified with attractors, the cyclic trajectory in phase space of anattractor may correspond to the cell cycle in which a cell divides.¥states that can be moved from one trajectory (for one attractor) to another trajectory (and anotherattractor) by changing a single state variable are not robust and may represent the phenomenon thatsmall, apparently minor perturbations to a cellõs environment may kick it into a different state.¥the square root of the number of genes in the human genome (around 30,000) is 173. under theassumption of k = 2 scaling, this would correspond to the number of cyclic attractors and thus to thenumber of cell types in the human body. this is not far from the number of cell types actually observed75a. bhattacharjya and s. liang, òpowerlaw distributions in some random boolean networks,ó physical review letters77(8):1644, 1996.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.computational modeling and simulation as enablers for biological discovery155(about 200). such a result may be numerological coincidence or rooted in the fact that nearly all cells ina given organism (even across eukaryotes) share the same basic housekeeping mechanisms (metabolism, cellcycle control, cytoskeletal construction and deconstruction, and so on), or it may reflect phenotypic structure driven by the largescale connectivity in the overall genetic regulatory network. morework will be needed to investigate these possibilities.76 box 5.10 provides one view on experimentalwork that might be relevant.to illustrate the potential value of boolean networks as a model for genetic regulatory networks, consider their application to understanding the etiology of cancer.77 specifically, cancer isbox 5.9finitestate automata and a comparison of genetic networks and boolean networksin kaufmannõs boolean representation of a genetic regulatory network, there are n genes, each with two statesof activity (expressed or inhibited), and hence 2n possible states (i.e., sets of activities) in the network. thenumber of possible connections is combinatorial in n and k. starting at time t, each gene makes a transitionto a new state at time t + 1 in accord with the transition rule and the k inputs that it receives. thus, the stateof the network at a time t + 1 is uniquely determined from its state at time t. the trajectory of the network ast changes (i.e., the sequence of states that the network assumes) is analogous to the process by which genesare expressed.this network is an instantiation of a finitestate automaton. since there are a finite number of states (2n), thesystem must eventually find itself in a state previously encountered. since the system is deterministic, thenetwork then cycles repeatedly through a fixed cycle, called an attractor. every possible system state eitherleads to some attractor or is part of an attractor.different initial conditions may or may not lead to different attractors. all of the initial conditions that lead tothe same attractor constitute what is known as a òbasinó for that attractor. any state within a basin can beexchanged with any other state in the same basin without changing the behavior of the network in the longrun. in addition, given a set of attractors, no attractor can intersect with another (i.e., pass through even onestate that is contained in another attractor). thus, attractors are intrinsically stable and are analogous to thegenetic expression pattern in a mature cell.an attractor may be static or dynamic. a static attractor involves a cycle length of one (i.e., the automatonnever changes state). a dynamic attractor has a cycle length greater than one (i.e., a sequence of states repeatsafter some finite number of time increments). attractors that have extremely long cycle lengths are regarded aschaotic (i.e., they do not repeat in any amount of time that would be biologically interesting).two system states differing in only a small number of state variables (i.e., having only a few bits that differ outof the entire set of n variables) often lie on dynamical trajectories that converge closer to one another in statespace. in other words, their attractors are robust under small perturbations. however, there can be stateswithin a basin of attraction that differ in only one state variable from a trajectory that can lead to a differentattractor.76this point is discussed further in section 5.4.2.2 and the references therein.77z. szallasi and s. liang, òmodeling the normal and neoplastic cell cycle with ôrealistic boolean genetic networksõ: theirapplication for understanding carcinogenesis and assessing therapeutic strategies,ó pacific symposium on biocomputing, pp. 6676, 1998.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.156catalyzing inquirywidely believed to be a pathology of the hereditary apparatus. however, it has been clear for sometime that singlecause, singleeffect etiologies cannot account for all or nearly all occurrences ofcancer.78box 5.10testing the potential relevance of the boolean network modelbecause of the extreme simplifications embedded in the boolean network model, detailed predictions (e.g.,genes a and b turn on gene c) are unlikely to be possible. instead, the utility of this approach as a way oflooking at genetic regulation will depend on its ability to make qualitative predictions about largescalestructure and trends. put differently, can boolean networks behave in biologically plausible ways?under certain circumstances, boolean networks do exhibit certain regularities. thus, the operative question iswhether these features have reasonable biological interpretations that afford insight into the integrated behavior of the genomic system. consider the following:1.a large fraction of the genes in boolean networks converge to fixed states of activity, on or off, that containthe same genes on all celltype attractors. the existence of this òstable coreó predicts that most genes will bein the same state of activity on all cell types of an organism. direct experimental testing of this prediction ispossible using dna chip technology today.2.nearby states in the state space of the system typically lie on trajectories that converge on each other instate space. this might be tested by cloning exogenous promoters upstream of a modest number of randomlychosen genes to transiently activate them, or by using inhibitory rna to transiently inactivate a geneõs rnaproducts, and following the trajectory of gene activities in unperturbed cells over time and perturbed cellswhere the geneõs activity is transiently altered, using dna chips to assess whether the states of activity become more similar.3.the boolean model predicts that if randomly chosen genes are transiently reversed in their activity, adownstream avalanche of gene activities will ensue. the size distribution of these avalanches is predicted tobe a power law, with many small avalanches and few large ones. there is a rough maximum size avalanchethat scales as about three times the square root of the number of genes, hence about 500 for human cells. thisis testable, again by cloning upstream controllable promoters to transiently activate random genes, or inhibitory rna to transiently inactivate random genes, and following the resulting avalanche of changes in geneactivities over time using dna chips.4.the boolean model assumes cell types are attractors. as such, celltype attractors are stable to about 95percent of the single gene perturbationsñthe system returns to the attractor from which it was perturbed.similarly, it is possible to test whether cell types are stable in the same homeostatic way by perturbing theactivity of many choices of single genes, one at a time.5.the stable core leaves behind òtwinkling islandsó of genes that are functionally isolated from one another.these are the subcircuits that determine differentiation, since each island has its own attractors, and theattractors of the network as a whole are unique choices of attractor from each of the twinkling islands in a kindof combinatorial epigenetic code. current techniques can test for such islands by starting avalanches fromdifferent single genes. two genes in the same island should have overlapping downstream members of theavalanches they set off. genes in different islands should not. the caveat here is that there may be genesdownstream from more than one island, affected by avalanches started in each.source: stuart kauffman, santa fe institute, personal communication, september 20, 2002.78see, for example, t. hunter, òoncoprotein networks,ó cell 88(3):333, 1997; b. vogelstein and k.w. kinzler, òthe multistepnature of cancer,ó trends in genetics 9(4):138, 1993. (cited in szallasi and liang, 1998.)catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.computational modeling and simulation as enablers for biological discovery157if the correspondence between attractor and cell is assumed, malignancy can be viewed as anattractor similar in most ways to that associated with a normal cell,79 and the transition from normal tomalignant is represented by a òphase transitionó from one attractor to another. such a transition mightbe induced by an external event (radiation, chemical exposure, lack of nutrients, and so on).as one illustration, szallasi and liang argue that changes in largescale gene expression patternsassociated with conversion to malignancy depend on the nature of attractor transition in the underlyinggenetic network in three ways:1.a specific oncogene can induce changes in the state of downstream genes (i.e., genes for whichthe oncogene is part of their regulatory network) and transition rules for those genes without drivingthe system from one attractor to another one. if this is true, inhibition of the oncogene will result areversion of those downstream changes and a consequent normal phenotype. in some cases, just suchphenomenology has been suggested,80 although whether or not this mechanism is the basis of someforms of human cancer is unknown as yet.2.a specific oncogene could force the system to leave one attractor and flow into another one. thenew attractor might have a much shorter cycle time (implying rapid cell division and reproduction)and/or be more resistant to outside perturbations (implying difficulty in killing those cells). in this case,inhibition of the oncogene would not result in reversion to a normal cellular state.3.a set of òpartialó oncogenes may force the system into a new attractor. in this case, no individualpartial oncogene would induce a phenotypical change by itselfñhowever, the phenomenology associated with a new attractor would be similar.these different scenarios have implications for both research and therapy. from a research perspective, the operation of the second and third mechanisms implies that the networkõs trajectory throughstate space is entirely different, a fact that would impede the effectiveness of traditional methodologiesthat focus on one or a few regulatory pathways or oncogenes. from a therapeutic standpoint, theoperation of the latter two mechanisms implies that a focus on òknocking out the causal oncogeneó isnot likely to be very effective.5.4.3.3genetic regulation as circuitsgenetic networks can also be modeled as electrical circuits.81 in some ways, the electrical circuitanalogy is almost irresistible, as can be seen from a glance at any of the known regulatory pathways: thetangle of links and nodes could easily pass for a circuit diagram of intelõs latest pentium chip. forexample, mcadams and shapiro described the regulatory network that governs the course of a phageinfection in e. coli as a circuit, and included factors such as time delays, which are critical in biologicalnetworks (gene transcription and translation are not instantaneous, for example) and indeed, in electrical networks, as well.more generally, natureõs designs for the cellular circuitry seems to draw on any number of techniques that are very familiar from engineering: òthe biochemical logic in genetic regulatory circuitsprovides realtime regulatory control [via positive and negative feedback loops], implements a branch79s.a. kauffman, òdifferentiation of malignant to benign cells,ó journal of theoretical biology 31:429, 1971. (cited in szallasi andliang, 1998.)80s. baasner, h. von melchner, t. klenner, p. hilgard, and t. beckers, òreversible tumorigenesis in mice by conditionalexpression of the her2/cerbb2 receptor tyrosine kinase,ó oncogene 13(5):901, 1996. (cited in szallasi and liang, 1998.)81h.h. mcadams and l. shapiro, òcircuit simulation of genetic networks,ó science 269(5224):650656, 1995.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.158catalyzing inquirying decision logic, and executes stored programs [in the dna] that guide cellular differentiation extending over many cell generations.ó82 table 5.2 describes some of the similarities.of course, taking an engineering view of biological circuits does not make understanding themtrivial. for example, consider that cellular regulatory circuits implement a complex adaptive controlsystem. understanding this system is greatly complicated by the fact that at the biochemical implementation level, the distinction between the controlling mechanisms and the controlled processes is not asclear as it is when such control is engineered into a humandesigned artifact. in a biochemical environment, control reactions and controlled functions are composed of intermingled molecules interacting inways that make identification of roles much more complex.nor does the analogy to electrical circuits always carry over perfectly. because critical molecules areoften present in the cell in extremely small quantities, to take the most notable example, certain criticalreactions are subject to large statistical fluctuations, meaning that they proceed in fits and starts, muchmore erratically than their electrical counterparts.5.4.3.4combinatorial synthesis of genetic networks83guet et al. have demonstrated the feasibility of creating synthetic networks, composed of wellcharacterized genetic elements, that provide a framework for understanding how diverse phenotypitable 5.2points of similarity between genetic logic and electronic digital logic in computerchipscharacteristicelectronic logicgenetic logicsignalselectron concentrationsprotein concentrationsdistributionpointtopoint (by wiresdistributed volumetrically byor by electrically encodeddiffusion or compartmenttoaddresses)compartment by activetransport mechanismsorganizationhierarchicalhierarchicallogic typedigital, clocked, sequentialanalog, unclocked (canlogicapproximate asynchronoussequential logic; dependent onrelative timing of signals)noiseinherent noise due to discreteinherent noise due to discreteelectron events andchemical events and environmental effectsenvironmental effectssignaltonoise ratiosignaltonoise ratio high insignaltonoise ratio low inmost circuits most circuitsswitching speedfast (>10ð9 sð1)slow (<10ð2 sð1)source: excerpted with permission from h. mcadams and a. arkin, òsimulation of prokaryotic genetic circuits,ó annualreview of biophysics and biomolecular structure 27:199224, 1998, available at http://caulo.stanford.edu/usr/hm/pdf/1998mcadamssimulationgeneticcircuits.pdf. originally published by annual review of biophysics and biomolecular structure.82h.h. mcadams and a. arkin, òsimulation of prokaryotic genetic circuits,ó annual reviews of biophysical and biomolecularstructure 27:199224, 1998.83section 5.4.3.4 is based on c.c. guet, m.b. elowitz, w. hsing, and s. leibler, òcombinatorial synthesis of genetic networks,ó science 296(5572):14661470, 2002.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.computational modeling and simulation as enablers for biological discovery159cal functionality can (but does not always) arise from changes in network topology rather thanchanges in the elements themselves. this functionality includes networks that exhibit the behaviorassociated with negative and positive feedback loops, oscillators, and toggle switches. by showingthat functionality can change dramatically due to changes in topology, guet et al. argue that once asimple set of genes and regulatory elements is in place, it is possible to jump discontinuously fromone functional phenotype to another using the same òtoolkitó of genes simply by modifying theregulatory connections. such discontinuous changes are different from the more gradual effects drivenby successive point mutations.such discontinuities reflect the nonlinear nature of genetic networks. furthermore, the topology ofconnectivity of a network does not necessarily determine its behavior uniquely, and the behavior ofeven simple networks built out of a few wellcharacterized components cannot always be inferred fromconnectivity diagrams alone. because genetic networks are nonlinear (and stochastic as well), the unknown details of interactions between components might be of crucial importance to understandingtheir functions. combinatorially developed libraries of simple networks may thus be useful in uncovering the existence of additional regulatory mechanisms and exploring the limits of quantitative modelingof cellular systems.the system of guet et al. uses a small number of elements restricted to a single type of interaction(transcriptional regulation), but the range of biochemical interactions can be extended by includingother modular genetic elements. for example, the approach can be extended to include linking inputand output through cellcell signaling molecules, such as those involved in quorum sensing. also, thiscombinatorial strategy can be used to search for other dynamic behaviors such as switches, sensors,oscillators, and amplifiers, as well as for highlevel structural properties such as robustness or noiseresistance.5.4.3.5identifying systems responses by combining experimentaldata with biological network informationmawuenyega et al. have developed a method to identify specific subnetworks in large biologicalnetworks.84 a biological network is constructed by identifying components (genes, proteins, transcription factors, chemicals) and interactions between components (proteinprotein, proteindna, signaltransduction, gene expression, catalysis) from genome context information as well as from externalsources (databases, literature, and direct interaction with experimentalists). by superimposing experimental data such as expression values or identified proteins, it is possible to identify a bestscoredsubnetwork in the large biological network. this subnetwork is known as the response network, identifying a systemõs response with respect to the experimental scenario and data used.proteomic mass spectroscopy (ms) analysis was used to identify and characterize 1,044 mycobacterium tuberculosis (tb) proteins and their corresponding cellular locations. from these 1,044 identified, 70proteins were selected that are known to function in lipid biosynthesis (20) and fatty acid degradation(50). it is striking that the identified proteins involved in fatty acid degradation were distributed between the different cellular compartments in an almost exclusive fashion (e.g., in the subnetworkcentered on fadb2 and fadb3) (figure 5.9).in addition, forst and colleagues performed a response network analysis of mycobacterium tuberculosis to isoniazid (inh) drug treatment.85 the entirety of the fasii fatty acid synthase group (except84k.g. mawuenyega, c.v. forst, k.m. dobos, j.t. belisle, j. chen, m.e. bradbury, a.r. bradbury, and x. chen, òmycobacterium tuberculosis functional network analysis by global subcellular protein profiling,ó molecular biology of the cell 16:396404,2005.85l. cabusora, e. sutton, a. fulmer, and c.v. forst, òdifferential network expression during drug and stress response,óbioinformatics 21:28982905, 2005, available at http://bioinformatics.oupjournals.org/cgi/content/abstract/bti440v1.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.160catalyzing inquiryacpm, which was not included in the interaction data used to construct the original, whole network)showed up in the inh response network, all with significant upregulation (figure 5.10). furthermore,the specific removal of these genes (kasa, kasb, fabd, accd6) from the initial set of genes did not affecttheir presence in the inh response subnetwork: the newly calculated network continued to containeach of them. forst concluded that inh does directly interfere with the fasii fatty acid productionpathway, in confirmation of earlier results.fatty acid degradation networkfatty acid degradationlipid biosynthesisneithercytoplasmcell wallmembranemembrane and cell wallmembrane and cytoplasmfigure 5.9fatty acid degradation network. source: courtesy of christian forst, los alamos national laboratories, december 8, 2004.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.computational modeling and simulation as enablers for biological discovery1615.4.4organ physiologysydney brenner has noted that ògenes can only specify the properties of the proteins they code for,and any integrative properties of the system must be ôcomputedõ by their interactions.ó86 in this context,subcellular behavior and function represents a first level of òcomputedó interaction; cellular behavior andfunction, a second level. organization of cells into organs provides a context for cellular behavior, and inthe words of denis noble, òsuccessful physiological analysis requires an understanding of the functionalinteractions between the key components of cells, organs, and systems, as well as how these interactionschange in disease states. this information resides neither in the genome nor even in the individualproteins that genes code for. it lies at the level of protein interactions within the context of subcellular,cellular, tissue, organ, and system structures. there is therefore no alternative to copying nature andcomputing these interactions to determine the logic of healthy and diseased states. the rapid growth inbiological databases; models of cells, tissues, and organs; and the development of powerful computinghardware and algorithms have made it possible to explore functionality in a quantitative manner all theway from the level of genes to the physiological function of whole organs and regulatory systems.ó875.4.4.1multiscale physiological modeling88physiological modeling is the modeling of biological units at a level of aggregation larger than thatof an individual cell. biological units can be successively decomposed into subunits (e.g., an organismmay consist of subsystems for circulatory, pulmonary, digestive, and cognitive function; a digestivefigure 5.10the isoniazid (inh) response network. red nodes indicate upregulated genes. blue nodes indicatedownregulated genes. source: courtesy of christian forst, los alamos national laboratories, december 8,2004.86s. brenner, òbiological computation,ó the limits of reductionism in biology. wiley, chichester, uk, 1998, pp. 106116.87d. noble, òmodeling the heartñfrom genes to cells to the whole organ,ó science 295(5560):16781682, 2002.88much of the material in section 5.4.4.1 is based on excerpts from a.d. mcculloch and g. huber, òintegrative biologicalmodelling in silico,ó novartis foundation symposium 247:419, 2002.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.162catalyzing inquirysystem may consist of esophagus, stomach, and intestines; and so on down to the level of organelleswithin cells and molecular functions within organelles), and every unit depends on the coordinatedinteraction of its subunits.given the complexity of physiological modeling, it makes sense to replicate this natural organization. thus, models of tissue, organs, and even entire organisms are relevant subjects of physiologicalmodeling. functional behavior in each of these entities depends on activity at all spatial and temporalscales associated with structure from protein to cell to tissue to organ to whole organism (box 5.11) andrequires the integration of interacting physiological processes such as regulation, growth, signaling,metabolism, excitation, contraction, and transport processes. one term sometimes used for work thatinvolves such integration is òphysiomeó (or by analogy to genomics,òphysiomicsó).89integration of such models presents many intellectual challenges. following mcculloch andhuber,90 it is helpful to consider two different types of integration. structural integration implies integration across physical scales of biological organization from protein to whole organism, while functionalintegration refers to the integrated representation of interacting physiological processes. structurallyintegrative models (e.g., models of molecular dynamics and other strategies that predict protein function from structure) are driven by first principles and hence tend to be computationintensive. becausethey are based on first principles, they impose constraints on the space of possible organismic models.functionally integrative models are strongly datadriven and therefore dataintensive, and are neededto bridge the multiple time and space scales of substructures within an organism without leaving theproblem computationally intractable. box 5.12 provides a number of examples of intersection betweenstructurally and functionally integrated models.predictive simulations of subcomponents at various levels of the hierarchy of complexity are generally based on physicochemical first principles. integrating such simulations, of which micromechanicaltissue models and molecular dynamics models are examples, with each other across scales of biologicalorganization is highly computationally intensive (and requires a computational infrastructure thatenables distributed and heterogeneous computational resources to participate in the integration andfacilitates the modular addition of new models and levels of organization).5.4.4.2hematology (leukemia)childhood acute lymphoblastic leukemia (all) is a lethal but highly treatable disease. however,successful treatment depends on the ability to deliver the correct intensity of therapy. improper intensity can result in an excess of deaths caused by toxicity, decreased mental function over the long term,and undertreatment for highrisk cases.the appropriate intensity is determined today through an extensiveñand expensiveñrange ofprocedures including morphology, immunophenotyping, cytogenetics, and molecular diagnostics.however, limsoon wong has developed a relatively inexpensive singleplatform microarray test thatuses gene expression profiling to identify each of the known clinically important subgroups of childhood all (figure 5.11) and hence the appropriate intensity of treatment.91 this is confirmed usingcomputerassisted supervised learning algorithms, in which an overall diagnostic accuracy of 96 percent was achieved in a blinded test sample. to determine whether expression profiling at diagnosis89j.b. bassingthwaighte, òtoward modeling the human physionome,ó pp. 331339 in molecular and subcellular cardiology:effects on structure and function, s. sideman and r. beyar, eds., plenum press, new york, volume 382 in advanced experimentsin medical biology, 1995; http://www.physiome.org/.90a.d. mcculloch and g. huber, òintegrative biological modelling in silico,ó pp. 425 in ôin silicoõ simulation of biologicalprocesses no. 247, novartis foundation symposium, g. bock and j.a. goode, eds., john wiley & sons ltd., chichester, uk, 2002.91l. wong, òdiagnosis of childhood acute lymphoblastic leukemia and optimization of riskbenefit ratio of therapy,ópowerpoint presentation presented at the institute for infocomm research, 2003, singapore, available at http://sdmc.lit.org.sg:8080/~limsoon/psz/wlsaasbi03.ppt.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.computational modeling and simulation as enablers for biological discovery163might further help identify those patients who are likely to relapse up to 4 years later, the expressionprofiles of four groups of leukemic samples with different outcomes were compared. distinct geneexpression profiles for each of these groups were identified.5.4.4.3immunologythe immune system provides protection for human beings from pathogens. (for purposes of thisdiscussion, the immune system of interest here refers to the adaptive immune system. the human bodyalso has an innate immune system that provides a first response to pathogens that is essentially independent of the specific pathogenñin essence, its role is to give the adaptive immune system time tobuild a more specific response.) to do so, the immune system must first identify an entity within thebody as a harmful pathogen that it should attack or eliminate and then mount a response that does so.in principle, the identification of harmful pathogens might be based on a list of known pathogens.if an entity is found within the human body that is sufficiently similar to a known pathogen, it could bemarked for later attack and destruction. however, a listbased approach to pathogen identificationsuffers from two major weaknesses. first, any such list would have to be large enough to include mostof the possible pathogens that an organism might encounter in its lifetime; some estimates of thenumber of different foreign molecules that the human immune system is capable of recognizing are ashigh as 1016.92 second, because pathogens evolve (and, thus, new pathogens are created), an a priori listcould never be complete.accordingly, nature has developed an alternative mechanism for pathogen identification based onthe notion of òselfó versus ònonself.ó in this paradigm, entities or substances that are recognized as selfare deemed harmless, while those that are nonself are regarded as potentially dangerous. thus, theimmune system has developed a variety of mechanisms to differentiate between these two categories.note that this distinction is highly simplistic, as not all nonself entities are bad for the human body (e.g.,transplanted organs that replace original organs damaged beyond repair). nevertheless, the selfnonself distinction is not a bad point of departure for understanding the human immune system.the immune system relies on a process that generates detectors for a subset of possible pathogensand constantly turns over those detectors for new detectors capable of identifying a different set ofpathogens. when the immune system identifies a pathogen, it selects one of several immunologicalmechanisms (e.g., those associated with the different immunoglobulin [ig] groups) to eliminate it.furthermore, the immune system retains memory of the pathogen, in the form of detectors that arespecifically configured for high affinity to that pathogen. such memory enables the immune system toconfer longlasting resistance (immunity) to pathogens that may be encountered in the future and tomount a stronger response to such future encounters.many of the broad outlines of the immune system are believed to be understood, and computational modeling of the immune system has shed important light on its detailed workings, as describedin box 5.13. a medical application of simulation models in immunology has been to evaluate the effectsof revaccinating someone yearly for influenza. because of the phenomenon of immune memory, avaccine that is too similar to a prior yearõs vaccine will be eliminated rapidly by the immune response (anegative interference effect). a simulation model by smith et al. has examined this effect and suggestssome circumstances under which individuals who are vaccinated annually will have greater or lessprotection than those with a firsttime vaccination.93 the smith et al. results also suggested that in theproduction of flu vaccine, a choice among otherwise equivalent strains (i.e., strains thought to be92j. inman, òthe antibody combining regionñspeculations on the hypothesis of general multispecificity,ó theoretical immunology, g. bell, ed., dekker, new york, 1978.93d.j. smith, s. forrest, d.h. ackley, and a.s. perelson, òvariable efficacy of repeated annual influenza vaccination,ó proceedings of the national academy of sciences 96(24):1400114006, 1999.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.164catalyzing inquirybox 5.11levels of biological organizationone helpful approach is to consider a set of different, but interrelated, levels of biological organization:¥organ system, in which the entire organ can be represented by a lumpedparameter systems model thatcan be used to predict the gross behavior of the organ. in the case of the heart, one model can be based on thenotion of arterial impedance, which can be used to generate the dynamic pressure boundary conditions actingon the cardiac chambers.¥whole organ continuum, in which the physical behavior and dynamical responses of the organ can becalculated from finite element methods that solve the continuum equations for the mechanics of the organ. inthe case of the heart, boundary conditions such as ventricular cavity pressures are computed from the lumpedparameter model in the top level. detailed parametric models of threedimensional cardiac geometry andmuscle fiber orientations have been used to represent the detailed structure of the whole organ with submillimeter resolution.1¥tissue, in which constitutive laws for the continuum models are evaluated at each point in the whole organcontinuum model and obtained by homogenizing the results of multicellular network models. that is, homogenization theory can be used to reparameterize the results of a micromechanical analysis into a form suitablefor continuumscale stress analysis. in the case of tissue mechanics for the heart, the basic functional units oftissue are represented, such as laminar myocardial sheets as ensembles of cell and matrix micromechanicsmodels and, in some cases, the microvascular blood vessels as well.2 a variety of approaches for these modelshave been used, including stochastic models based on measured statistical distributions of myofiber orientations.3 in cardiac electrophysiology, the tissue level is typically modeled as resistively coupled networks ofdiscrete cellular models interconnected in three dimensions.4¥single cell, in which different types of cells are represented. as a rule, singlecell models bridge to stochastic statetransition models of macromolecular function through subcellular compartment models of representative tissue structures (e.g., the sarcomere in the case of the heart). heart cells of different types to be modeledare representative cells from different regions of the heart, such as epicardial cells, midventricular mcells, andendocardial cells. for mechanical models, individual myofibrils and cytoskeletal structures are modeled bylattices and networks of rods, springs, and dashpots in one, two, or three dimensions.¥macromolecular complex, in which representative populations of crossbridges or ion channels are modeled. such complexes are typically described by markov models of stochastic transitions between discretestates of, for example, channel gating, actinmyosin binding, or nucleotide bound to myosin.¥molecular model, in which single crossbridges and ion channels are represented. crossbridges moveaccording to brownian dynamics, and it is necessary to use weightedensemble dynamics to allow the simulation to clear the energy barriers. (for example, a weightedensemble brownian dynamics simulation of iontransport through a single channel can be used to compute channel gating properties from the results of ahierarchical collective motion (hcm) simulation of the channel complex.) the flexibility of the crossbridgesthemselves can be derived from the hcm method, and the interactions with other molecules can be computed using continuum solvent approximations.¥atomic model, in which molecules are represented in terms of the positions of their constituent atoms incrystallographic structures. (such data can be found in public repositories such as the protein data bank.)such data feed molecular dynamics simulations in order to build the hcm model.the approach described aboveñof integrating models across structural and functional linesñis generallyadaptable to other tissues and organs, especially those with physical functions, such as lung and cartilage.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.computational modeling and simulation as enablers for biological discovery165box 5.12examples of intersection between structurally and functionally integrated modelsthere are a number of examples of intersection between structurally and functionally integrated models,including the following:¥linkage of biochemical networks and spatially coupled processes, such as calcium diffusion in structurallybased models of cell biophysics;1¥use of physicochemical constraints to optimize genomic systems models of cell metabolism;2¥integration of genomic or cellular system models into multicellular network models of memory and learning,3 developmental pattern formation,4 or action potential propagation;5¥integration of structurebased predictions of protein function into systems models of molecular networks;¥development of kinetic models of cell signaling and coupling them to physiological targets such as energymetabolism, ionic currents or cell motility;6¥use of empirical constraints to optimize protein folding predictions;7 and¥integration of systems models of cell dynamics into continuum models of tissue and organ physiology.81l.m. loew, òthe virtual cell project,ó novartis foundation symposium 247:151161, 2002; l.m. loew and j.c. schaff, òthe virtual cell:a software environment for computational cell biology,ó trends in biotechnology 19(10):401406, 2001.2b.o. palsson, òwhat lies beyond bioinformatics?ó nature biotechnology 15:34, 1997; c.h. schilling, j.s. edwards, d. letscher, andb.o. palsson, òcombining pathway analysis with flux balance analysis for the comprehensive study of metabolic systems,ó biotechnologyand bioengineering 71(4):286306, 20002001.3d. durstewitz, j.k. seamans, and t.j. sejnowski, òneurocomputational models of working memory,ó nature neuroscience3(supplement):s1184s1191, 2000; p.h. tiesinga, j.m. fellous, j.v. jose, and t.j. sejnowski, òinformation transfer in entrained corticalneurons,ó network: computation in neural systems 13(1):4166, 2002.4e.h. davison, j.p. rast, p. oliveri, a. ransick, c. calestani, c.h. yuh, t. minokawa, et al., òa genomic regulatory network fordevelopment,ó science 295(5560):16691678, 2002.5r.m. shaw and y. rudy, òelectrophysiologic effects of acute myocardial ischemia: a mechanistic investigation of action potentialconduction and conduction failure,ó circulation research 80(1):124138, 1997.6j.m. levin, r.c. penland, a.t. stamps, and c.r. cho, òusing in silico biology to facilitate drug development.,ó in novartis foundationsymposium 247: 222238, 2002.7l. salwinski and d. eisenberg, òmotifbased fold assignment,ó protein science 10(12):24602469, 2001.8r.l. winslow, d.f. scollan, a. holmes, c.k. yung, j. zhang, m.s. jafri, òelectrophysiological modeling of cardiac ventricular function:from cell to organ,ó annual reviews of biomedical engineering 2: 119155, 2002; n.p. smith, p.j. mulquiney, m.p. nash, c.p. bradley,d.p. nickerson, and p.j. hunter, òmathematical modelling of the heart: cell to organ,ó chaos, solitons and fractals 13:16131621, 2002.source: a.d. mcculloch and g. huber, òintegrative biological modelling in silico,ó pp. 419 in ôin silicoõ simulation of biologicalprocesses no. 247, novartis foundation symposium, g. bock and j.a. goode, eds., john wiley & sons ltd., chichester, uk, 2002.reproduced with permission from john wiley & sons ltd.1f.j. vetterand a.d. mcculloch, òthreedimensional analysis of regional cardiac function: a model of rabbit ventricular anatomy,óprogress in biophysics and molecular biology 69(23):157183, 1998.2k. maynewman and a.d. mcculloch, òhomogenization modelling for the mechanics of perfused myocardium,ó progress in biophysicsand molecular biology 69(23):463481, 1998.3t.p. usyk, j.h. omens, and a.d. mcculloch, òregional septal dysfunction in a threedimensional computational model of focalmyofiber disarray,ó american journal of physiology 281(2):h506h514, 2001.4l.j. leon and f.a. roberge, òdirectional characteristics of action potential propagation in cardiac muscle: a model study,ó circulationresearch 69: 378395, 1991.source: adapted from a.d. mcculloch and g. huber, òintegrative biological modelling in silico,ó pp. 425 in ôin silicoõ simulation ofbiological processes no. 247, novartis foundation symposium, g. bock and j.a. goode, eds., john wiley & sons ltd., chichester, uk,2002.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.166catalyzing inquiry94a.d. mcculloch and g. huber, òintegrative biological modelling in silico,ó pp. 425 in ôin silicoõ simulation of biologicalprocesses no. 247, novartis foundation symposium, g. bock and j.a. goode, eds., john wiley & sons ltd., chichester, uk, 2002.equally good guesses of the upcoming epidemic strain and equally appropriate for manufacture) shouldbe resolved in favor of the strain that is most different from the one used in the previous year, becausethis choice would reduce the effects of negative interference and thus potentially increase vaccineefficacy in recipients of repeat vaccines.5.4.4.4the heartthe heart is an organ of primary importance in vertebrates, and heart disease is one of the primarycauses of death in the western world. at the same time, the heart is an organ of high complexity.although it is in essence an impulsive pump, it is a pump that must operate continuously and repairitself if necessary while in operation. its output must be regulated according to various physiologicalconditions in the body, and its performance is affected by the characteristics of the arterial and veinnetworks to which it is connected.the heart brings together many subsystems that interact mutually through fundamental physiological processes. as a general rule, physiological processes have both functional and structural dimensions. for example, cells are functionally specializedñblood cells and myocytes (heart cells) dodifferent things. furthermore, blood cells and heart cells are themselves part of a collective of otherblood cells and heart cells; thus, the structure within which an individual cell is embedded is relevant.an integrated computational model of the heart would bring together all of the relevant physiological processes (box 5.14).94 were such a model available, it would be possible to investigate commonfigure 5.11microarray expression groupings indicating known clinically important subgroups of childhoodacute lymphoblastic leukemia (all). note in particular the second column from the right, labeled ònovel.ó in thisinstance, the hierarchical clustering of gene expression reveals a novel subtype of childhood all. source:courtesy of l. wong, institute for infocomm research, singapore, 2003.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.computational modeling and simulation as enablers for biological discovery167heart diseases and to probe cardiac structure and function in different places in the heartña point ofsome significance in light of the fact that heart failure is usually regional and nonhomogeneous. thegraphic in box 5.14 emphasizes functional integration in the heart, and the majority of functionalinteractions take place at the scale of the single cell. however, an organismõs behavior depends oninteractions that span many orders of magnitude of space and time (from molecular structures andevents to wholeorgan anatomy and physiology). thus, highfidelity modeling of an organism or organsystem within an organism demands the integration of information across similar scales.an example of a functional model of a single cell is the work of winslow et al. in modeling thecardiac ventricular cell, and specifically the relationship between various current flows in the cell andbox 5.13modeling in immunologyin basic immunology, issues related to mutation also have been the focus of mathematical modeling and intenseexperimentation. . . . [for example,] during the course of an immune response, b lymphocytes within germinalcenters can rapidly mutate the genes that code for antibody variable regions. the immune system thus provides anenvironment in which evolution occurs on a time scale of weeks. among the large number of mutant b cells that aregenerated, selection chooses for survival those b cells that have increased binding affinity for the antigen thatinitiated the response. after 2 to 3 weeks, antibodies can have improved their equilibrium binding constant forantigen by one to two orders of magnitude, and may have sustained as many as 10 point mutations. how can theimmune system generate and select variants with higher fitness this rapidly and this effectively? an optimal controlmodel has suggested that mutation should be turned on and off episodically in order to allow new variants time toexpand without being subjected to the generally deleterious effects of mutation. timevarying mutation could beimplemented by having cells recycle through one region of the germinal center, mutating while there, and proliferating in a different region of the germinal center. this suggestion has generated new experimental investigations ofevents that occur within germinal centers. opportunities exist for a range of models that address basic questionsabout in vivo cell population dynamics and evolution, as well as more detailed questions involving the immunological mechanisms underlying affinity maturation.control of the immune response is another area ripe for modeling. what determines the intensity of a response? howis the response shut off when the antigen is eliminated? feedback mechanisms may exist to control the responseintensity, response length, and type of response (cellular or antibody). some models of a basic feedback mechanisminvolving two types of helper t cells, th1 and th2, have been developed; others are needed. regulatory mechanismsinvolve interactions among many cell populations that communicate by direct cellcell contact and through thesecretion of cytokines. diagrams representing the elements of regulatory schemes commonly have scores of elements. because of the complexities involved, theorists have an opportunity to lead experimentation by providingsuggestions as to what needs to be measured and how such measurements can be used to provide an insightful viewof possible control mechanisms.a fundamental feature of the immune system is its diversity. successful recognition of antigens appears to require arepertoire of at least 105 different lymphocyte clones. the diversity of the immune system has challenged experimentalists, and many recent advances have come from developing experimental models with limited immune diversity.however, models based on ecological concepts may provide insights into the control of clonal diversity, and modern computational methods now make it practical to consider models with tens of thousands of clones. thus, it ispossible to develop models that start to approach the size of small immune systems. simulations have suggested thatfrom simple rules of cell response, emergent phenomena arise that may have immunological significance. thechallenge in using computation is to develop models that address important questions, are realistic enough tocapture the relevant immunology, and yet are simple enough to be revealing.source: reprinted by permission from s.a. levin, b. grenfell, a. hastings, and a.s. perelson, òmathematical and computationalchallenges in population biology and ecosystems science,ó science 275(5298):334343. copyright 1997 aaas. (references omitted.)catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.168catalyzing inquirybox 5.14computational modeling of the heart. . . [integrative cardiac modelling has sought] to integrate data and theories on the anatomy and structure, hemodynamics and metabolism, mechanics and electrophysiology, regulation and control of the normal and diseased heart.the challenge of integrating models of many aspects of such an organ system, including its structure and anatomy,biochemistry, control systems, hemodynamics, mechanics and electrophysiology, has been the theme of severalworkshops over the past decade or so.some of the major components of an integrative cardiac model that have been developed include [models of]ventricular anatomy and fiber structure, coronary network topology and hemodynamics, oxygen transport and substrate delivery, myocyte metabolism, ionic currents, impulse propagation, excitationcontraction coupling, neuralcontrol of heart rate and blood pressure, crossbridge cycling, tissue mechanics, cardiac fluid dynamics and valvemechanics, and ventricular growth and remodelling. . . .. . . . [t]hese models can be extended and integrated with others [by considering the role in] several major functionalmodules . . . as shown in the figure below. . . . they include:¥coronary artery anatomy and regional myocardial flows for substrate and oxygen delivery.¥metabolism of the substrate for energy metabolism, fatty acid and glucose, the tricarboxylic acid (tca) cycle, andoxidative phosphorylation.¥purine nucleoside and purine nucleotide metabolism, describing the formation of atp and the regulation of itsdegradation to adenosine in endothelial cells and myocytes, and its effects on coronary vascular resistance.¥the transmembrane ionic currents and their propagation across the myocardium.¥excitationcontraction coupling: calcium release and reuptake, and the relationships between these and thestrength and extent of sarcomere shortening.¥sarcomere dynamics of myofilament activation and crossbridge cycling, and the threedimensional mechanicsof the ventricular myocardium during the cardiac cycle.¥cell signalling and the autonomic control of cardiac excitation and contraction.. . . while [figure 5.14.1] does show different scales in the structural hierarchy, it emphasizes functional integration, andthus it is not surprising that the majority of functional interactions take place at the scale of the single cell. . . . [afunctionally integrated] model of functionally interacting networks in the cell can be viewed as a foundation for structurally coupled models that extend to multicellular networks, tissue, organ and organ system. but it can also be viewed as afocal point into which feed structurally based models of protein function and subcellular anatomy and physiology.. . . predictive computational models of various processes at almost every individual level of the hierarchy have beenbased on physicochemical first principles. although important insight has been gained from empirical models ofliving systems, models become more predictive if the number of adjustable parameters is reduced by making use ofdetailed structural data and the laws of physics to constrain the solution. these models, such as molecular dynamicssimulations, spatially coupled cell biophysical simulations, tissue micromechanical models and anatomically basedcontinuum models are usually computationally intensive in their own right. . . . this will require a computationalinfrastructure that will allow us to integrate physically based biological models that span the hierarchy from thedynamics of individual protein molecules up to the regional physiological function of the beating heart. . . .investigators have developed largescale numerical methods for ab initio simulation of biophysical processes at thefollowing levels of organization: molecular dynamics simulations based on the atomic structure of biomolecules;hierarchical models of the collective motions of large assemblages of monomers in macromolecular structures;biophysical models of the dynamics of crossbridge interactions at the level of the cardiac contractile filaments;wholecell biophysical models of the regulation of muscle contraction; microstructural constitutive models of themechanics of multicellular tissue units; continuum models of myocardial tissue mechanics and electrical impulsepropagation; and anatomically detailed whole organ models.they have also investigated methods to bridge some of the boundaries between the different levels of organization.we [mcculloch and huber] and others have developed finiteelement models of the whole heart, incorporatingmicrostructural constitutive laws and the cellular biophysics of thin filament activation. recently, these mechanicscatalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.computational modeling and simulation as enablers for biological discovery169models have been coupled with a nonlinear reactiondiffusion equation model of electrical propagation incorporating an ionic cellular model of the cardiac action potential and its regulation by stretch. at the other end of thehierarchy, huber has recently developed a method, the hierarchical collective motions method, for integratingmolecular dynamics simulation results from small sections of a large molecule into a quasicontinuum model of theentire molecule.figure 5.14.1some major functional subsystems of an integrated heart model and their hierarchical relationships fromcell to tissue to organ and cardiovascular system.source: a.d. mcculloch and g. huber, òintegrative biological modelling in silico,ó pp. 419 in ôin silicoõ simulation ofbiological processes no. 247, novartis foundation symposium, g. bock and j.a. goode, eds., john wiley & sons ltd.,chichester, uk, 2002. text and figure reproduced with permission from john wiley & sons ltd. (references omitted.)catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.170catalyzing inquiry95r.l. winslow, d.f. scollan, a. holmes, c.k. yung, j. zhang, and m.s. jafri, òelectrophysiological modeling of cardiacventricular function: from cell to organ,ó annual reviews of biomedical engineering 2:119155, 2002.96p.j. hunter, òthe iups physiome project: a framework for computational physiology,ó progress in biophysics and molecularbiology 85(23):551569, 2004.box 5.15illustrations of functional models of cellular behaviorexample 1: results from singlecell modelingwinslow et al. have developed and applied a model of the normal and failing canine ventricular myocyte toanalysis of the functional significance of changes in gene expression during tachycardia pacinginduced heartfailure. using the data on mrna and protein expression levels cited above, these investigators defined a minimalmodel of endstage heart failure as (1) 33 percent reduction of ik1; (2) 66 percent reduction of ito1; (3) 68 percentreduction of the sr [sacroplasmic reticulum] ca2+atpase; and (4) 75 percent upregulation of the na+ca2+exchanger. they incorporated these changes sequentially into the computational model and used the model topredict the functional consequences of each alteration of gene expression in this disease. results show that theminimal hf [heart failure] model can reproduce the increased apd [action potential duration] observed infailing myocytes relative to normal myocytes. the minimal model can also account for the reduced amplitudeand slowed relaxation of the ca2+ transients observed in failing versus normal myocytes. most importantly,model simulations reveal that reduced expression of the outward potassium currents ito1 and ik1 have relativelylittle impact on apd, whereas altered expression of the ca2+ handling proteins has a profound impact on apd.these results suggested a strong interplay between apd and the properties of ca2+ handling in canine myocytes. the nature of this interplay was examined in the model. the model indicated that reductions in expression level of the sr ca2+atpase and increased expression of the na+ca2+ exchanger both contribute to areduction of jsr ca2+ load. this reduction in the junctional sr (jsr) ca2+ load in turn produces a smaller ca2+release from sr, reduced subspace ca2+ levels, and therefore reduced ca2+mediated inactivation of the ca2+current. the enhanced ca2+ current then contributes to prolongation of apd. this is an important insight,because identifies the heart failureinduced reduction in jsr ca2+ load as a critical factor in apd prolongationand in the accompanying increased risk of arrhythmias related to repolarization abnormalities.analyses of the type described above are likely to become increasingly important in determining the functional role of altered gene and protein expression in various disease states as more comprehensive largescale dataon genome and protein expression in disease become available.its contractile behavior.95 in particular, winslow has used this model to show that the reduced contractility (i.e., reduction in the strength with which a ventricular muscle contracts, which is associated withheart failure) is caused largely by changes in the calcium ion currents in those cells, rather than changesin potassium ion currents as was widely speculated before this work (example 1 in box 5.15). such aninsight suggests that the development of drugs to cope with heart failure would thus be better focusedon those that can regulate calcium flow. examples 2 and 3 in box 5.15 illustrate some of the scientificinsights that can be gained with a computational model integrated across functional and structurallines.integrating these various perspectives on the heart (and other organs as well) is the mission of thephysiome project, which seeks to construct models that incorporate the detailed anatomy and tissuestructure of an organ in a way that allows the inclusion of cellbased models and spatial structure anddistribution of proteins. the physiome project has developed a computational framework for integrating the electrical, mechanical, and biochemical functions of the heart:96catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.computational modeling and simulation as enablers for biological discovery171¥the underlying anatomical descriptions are based on finite element techniques, and orthotropicconstitutive laws based on the measured fibersheet structure of myocardial tissue drive the dynamicsof the large deformation softtissue mechanics involved.¥patterns of electrical current flow in the heart are computed using reactiondiffusion equationson a grid of deforming material points which access systems of ordinary differential equations representing the cellular processes underlying the cardiac action potential; these result in representations ofthe activation wavefronts that spread around the heart and initiate contraction.¥coronary blood flow is computed based on the navierstokes equations in a system of branchingblood vessels embedded in the deforming myocardium and the delivery of oxygen and metabolites iscoupled to the energydependent cellular processes.these models of different cardiac phenomena have been been implemented with òhorizontalóintegration of mechanics, electrical activation and metabolism, together with òverticaló integration fromcell to tissue to organ. thus, these models can be said to deconstruct an organ into a set of (submodelsfor) constituent functions, with explicit feedback and connection between them represented in theoverall model of the whole organ.results from integrated modeling (examples 2 and 3)in the clinical arrhythmogenic disorder longqt syndrome, a mutation in a gene coding for a cardiomyocytesodium or potassiumselective ion channel alters its gating kinetics. this small change at the molecular levelaffects the dynamics and fluxes of ions across the cell membrane and thus affects the morphology of therecorded electrocardiogram (prolonging the qt interval and increasing the vulnerability to lifethreateningcardiac arrhythmia). such an understanding could not be derived by considering only the single gene, channel, or cell; it is an integrated response across scales of organization. a hierarchical integrative simulationcould be used to analyze the mechanism by which this genetic defect can lead to sudden cardiac death, forexample, by exploring the effects of altered repolarization on the inducibility and stability of reentrant activation patterns in the whole heart. a recent study made excellent progress in spanning some of these scales byincorporating a markov model of altered channel gating, based on the structural consequences of the geneticdefect in the cardiac sodium channel, into a wholecell kinetic model of the cardiac action potential thatincluded all the major ionic currents.. . . [it] is becoming clearer that mutations in specific proteins of the cardiac muscle contractile filament systemlead to structural and developmental abnormalities of muscle cells, impairment of tissue contractile function,and the eventual pathological growth (hypertrophy) of the whole heart as a compensatory response. in thiscase, the precise physical mechanisms at each level remain speculative, although much detail has beenelucidated recently, so an integrative model will be useful for testing various hypotheses regarding the mechanisms. the modeling approach could be based on the same integrative paradigm commonly used by experimental biologists, in which the integrated effect of a specific molecular defect or structure can be analysedusing techniques such as in vivo gene targeting.source: r.l. winslow, d.f. scollan, a. holmes, c.k. yung, j. zhang, and m.s. jafri, òelectrophysiological modeling of cardiacventricular function: from cell to organ,ó annual review of biomedical engineering 2:119156, 2000. adapted by permission from annualreview of biomedical engineering. (references and citations omitted.)catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.172catalyzing inquiry5.4.5neurosciencein recent years, neuroscience has expanded its horizons beyond the microstructure of the brainñneurons, synapses, neurotransmitters, and the likeñto focus on the brainõs largescale cognitive architecture. drawing on dramatic advances in mapping techniques, such as functional magnetic resonanceimaging (mri) and magnetoencephalography, neuroscientists hope to give a computational account ofprecisely what each specialized region of the brain is doing and how it interacts with all the other activeregions to produce highlevel thought and behavior.5.4.5.1the broad landscape of computational neuroscienceneuroscience seeks to probe the details of the brain and the mechanisms by which the nervoussystems develops, is organized, processes information, and establishes mental abilities. research inneuroscience spans many levels of organization, from atomic and molecular events on the order of onetenth to one nanometer, up to the entire nervous system on the order of a meter or more. in addition,there are on the order of 1011 neurons and thousands to tens of thousands of synapses per neuron.information processing in the brain occurs through the interactions and spread of chemical andelectrical signals both within and among neurons. acting within the extensive but intricate architectureof the neurons and their interconnections, the mechanisms are nonlinear and span a wide range ofspatial and temporal scales.97 understanding how the nervous system and brain work thus requires aninterdisciplinary approach to the challenging multiscale integration of experimental data, computational data, and theory.it is helpful to describe the nervous systemõs functional processes and their mechanisms at severaldifferent levels of detail, depending on the goal of a given effort. table 5.3 and figure 5.12 describe thenumerous spatial and temporal scales relevant to neuroscience research, and provide some indicationof the complexity of such research.to illustrate, a low level of analysis might involve consideration of individual neurons. in thisanalysis, functional properties of neurons such as electronic structure, nerve cell connections (synapses), and voltagegated ion channels are important. at a higher level, it is recognized that individualneurons connect in networksñan analysis at this level examines how individual neurons interact toform functioning circuits. the mathematics of dynamic systems and visual neuroscience are notablyrelevant at this level. at a still higher level, individual networksñeach with its own specific architectureand informationprocessing capabilitiesñinteract to form neural nets and carry out cognition, speechtable 5.3scales of neuroscience researchspatial scalecomponent1 metercentral nervous system10 centimeterssystems1 centimetermaps1 millimeternetworks100 micronsneurons1 micronsynapses10 angstromsmolecules97n.t. carnevale and s. rosenthal, òkinetics of diffusion in a spherical cell: i. no solute buffering,ó journal of neurosciencemethods 41(3):205216, 1992.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.computational modeling and simulation as enablers for biological discovery173figure 5.12temporal and spatial scales of neuroscience research. source: courtesy of christof koch, caltech.perception, and imaging. at this level, computational analysis of nervous system networks and(connectionist) modeling of psychological processes is the primary focus.computational neuroscience provides the basis for testing models of the nervous systemõs functional processes and their mechanisms, and computational modeling at several levels of detail is important, depending on the purposes of a given effort. box 5.16 describes simulators that operate at differentlevels of detail for different purposes.5.4.5.2largescale neural modeling98to better understand a system as complex as the human brain, it is necessary to develop techniquesand tools for supporting largescale, similarly complex simulations. recent advances in understandinghow single neurons represent the world,99 how large populations of neurons cooperate to build morecomplex representations,100 and how neurobiological systems compute functions over their representations make largescale neural modeling a highly anticipated next step.98section 5.4.5.2 is based largely on material supplied by chris eliasmith, university of waterloo, september 7, 2004.99f. rieke, d. warland, r. de ruyter van steveninick, and w. bialek, spikes: exploring the neural code, mit press, cambridge,ma, 1997; d. warland, m. landolfa, j. miller, and w. bialek, òreading between the spikes in the cercal filiform hair receptorsof the cricket,ó analysis and modeling of neural systems, f. eeckman, ed., kluwer academic publishers, boston, ma, 1992.100l. abbott and t. sejnowski, neural codes and distributed representations: foundations of neural computation, mit press,cambridge, ma, 1999; r.s. zemel, p. dayan, and a. pouget, òprobabilistic interpretation of population codes,ó neural computation 10, 1998.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.174catalyzing inquirybox 5.16 simulators for computational neurosciencethe nervous system is extraordinarily complex. a single cubic centimeter in the brainõs cerebral cortex contains on the order of 5 billion synapses, and these differ in size and shape. the transmission of chemicalsignals is very complex, with many molecules involved, and is an area of intense study. with the introductionof more powerful computer hardware and advances in algorithms, quantitative modeling and realistic simulation in threedimensions of the interplay of biological ultrastructure and neuron physiology have becomepossible and have provided insight into the variability in signaling and plasticity of the system.to deal with the complexity, multiscale range of space and time, and nonlinearity of neural phenomena, anumber of specialized computational tools have been developed.mcell (a monte carlo simulator of cellular microphysiology) simulates individual connections or synapsesbetween neurons and groups of synapses. mcell simulations provide insights into the behavior and variabilityof real systems comprising finite numbers of molecules interacting in spatially complex environments. mcellincorporates highresolution physical structure into models of ligand diffusion and signaling and thus can takeinto account the large complexity and diversity of neural tissue at the subcellular level. monte carlo algorithms are used to simulate ligand diffusion using threedimensional random walk movements for individualmolecules. effector sites and surface positions are mapped spatially, and the encounters during ligand diffusion are detected. bulk solution rate constants are converted into monte carlo probabilities so that the diffusing ligands can undergo stochastic chemical interactions with individual binding sites such as receptor proteins, enzymes, and transporters.genesis (the general neural simulation system) is a tool for building structurally realistic simulations ofbiological neural systems that quantitatively embed what is known about the anatomical structure and physrecent theoretical work has suggested that it is possible to generally characterize the dynamics,representation, and computational properties of any neural population (figure 5.13).101 applications ofthese methods have been used successfully to generate models of working memory, rodent navigational tasks (path integration; see figure 5.14), eye position control, representation of selfmotion, lamprey and fish motor control, and deductive reasoning (figure 5.15).box 5.17 illustrates the use of computational modeling to understand how dopamine functions inthe prefrontal cortex. the box also illustrates the oftenpresent tension between those who believe thatsimple models (in this case, advocates of a connectionist model) can provide useful insight and thosewho believe that simple models cannot capture the implications of the complex dynamics of individualneurons and their synapses and that the addition of considerable biophysical and physiological detail isneeded for real understanding. many of these models require large numbers of individual, spikingneurons to be simulated concurrently, which results in significant computational demands. in addition,calculating the necessary connection weights requires the inversion of extremely large matrices. thus,highperformance computing resources are essential for expanding these simulations to include moreneural tissue, and hence more complex neural function.101c. eliasmith and c.h. anderson, neural engineering: computation, representation and dynamics in neurobiological systems,mit press, cambridge, ma, 2003.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.computational modeling and simulation as enablers for biological discovery175iological characteristics of the neural system of interest. genesis reflects the modeling perspective that spatialorganization and structure are important for understanding neural function. genesis is organized aroundneurons constructed out of components such as compartments (short sections of cellular membrane) andvariable conductance ion channels that receive inputs, perform calculations on them, and then generateoutputs. neurons in turn can be linked to form neural circuits. genesis originally was used largely for realisticsimulations of cortical networks and of the cerebellar purkinje cell and, more recently, to interconnect celland network properties to biochemical signaling pathways.neuron is similar to genesis in many ways, but contains optimizations that enable it to run very fast onnetworks in which cable properties play a crucial role, that involve system sizes ranging from parts of singlecells to small numbers of cells, and that involve complex branched connections. furthermore, the performance of neuron degrades very slowly with increasing complexity of morphology and membrane mechanisms, and it has been applied to very large network models (104 cells with six compartments each and a totalof 106 synapses in the network. using a highlevel language known as nmodl, neuron has also beenextended to investigate new kinds of membrane channels. the morphology and membrane properties ofneurons are defined with an objectoriented interpreter, allowing for voltage control, manipulation of currentstimuli, and other biological parameters.sources: for more information, see http://www.mcell.cnl.salk.edu; j.r. stiles and t.m. bartol, jr., òmonte carlo methods for simulatingrealistic synaptic microphysiology using mcell,ó pp. 87127 in computational neuroscience: realistic modeling for experimentalists, e. deshutter, ed., boca raton, fl, crc press, 2000; j.r. stiles, t.m. bartol, jr., e.e. salpeter, m.m. salpeter, t.j. sejnowski, òsynaptic variability:new insights from reconstructions and monte carlo simulations with mcell,ó pp. 681731 in synapses, w.m. cowan, t.c. sudhof, c.f.sudhof, eds., johns hopkins university press, baltimore, 2001; j.m. bower, d. beeman, and m. hucka, òthe genesis simulation system,óthe handbook of brain theory and neural networks, second edition, m.a. arbib, ed., mit press, cambridge, ma, 2003, pp. 475478,available at http://www.genesissim.org/genesis/hbtn2eboweretal/hbtn2eboweretal.html; m.l. hines and n.t. carnevale, òthe neuron simulation environment,ó neural computation 9(6):11791209, 1997, available at www.neuron.yale.edu/neuron/papers/nc97/nsimenv.pdf.5.4.5.3muscular controlmuscles are controlled by action potentialsñbrief, rapid depolarizations of membranes in nervesand muscles. the timing of action potentials transmitted from motor neurons coordinates the contraction of the muscles they innervate. rhythmic activity of the nervous system often takes the form ofcomplex bursting oscillations in which intervals of action potential firing and quiescent intervals ofmembrane activity alternate. the relative timing of action potentials generated by different neurons is akey ingredient in the function of the nervous system.changes in the electrical potential of membranes are mediated by ion channels that selectivelypermit the flow of ions such as sodium, calcium, and potassium across the membrane. individualchannels are protein complexes containing membranespanning pores that open and close randomly atrates that depend on many factors. cellular and network models of membrane potential represent thesesystems as electrical circuits in which voltage gated channels function as ònonlinearó resistors whoseconductance depends on membrane potential. information is transmitted from one neuron to anotherthrough synapses where action potentials trigger the release of neurotransmitters that bind to channelsof adjacent cells, stimulating changes in the ionic currents of these cells. (the action potential is the basicneuronal signaling òpacketó of ionic flow through a cell membrane.)the most basic model of this mechanism is the hodgkinhuxley model, which refers to a set ofdifferential equations that describe the action potential.102 specifically, the hodgkinhuxley equations102a.l. hodgkin and a.f. huxley, òa quantitative description of membrane current and its application to conduction andexcitation in nerve,ó journal of physiology 117(4):500544, 1952.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.176catalyzing inquiryexpress quantitatively the feedback entailed in the relationship between changing ionic flows andchanging membrane potential. originally based on data collected from experiments on the giant axon ofthe squid, the physical model used is that of a membrane separating two infinite regions, each of whichis homogeneous on its side of the membrane.in the nervous system, different kinds of ions pass through the membrane, and the flow of ionsthrough these channels is voltage dependent. in the model, a circuit is used to represent the ion flowsand potential differences that drive ion flow. the semipermeable cell membrane separating the interiorof the cell from the extracellular liquid is modeled as a capacitor, and each ion channel is modeled as aseparately variable resistor. in series with each variable resistor is a battery representing the nernstpotential arising from the difference in ion concentration on each side of the membrane. all of thesecomponents are connected in parallel and are driven by a timevarying current source to ground. if atimevarying input current is injected into the cell, it may add further charge on the capacitor, or theadded charge may leak through the channels in the cell membrane. because of active ion transportthrough the cell membrane, the ion concentration inside the cell is different from that in the extracellularliquid. the potential generated by the difference in ion concentration is represented by a battery.elementary circuit theory allows the construction of a set of differential equations relating thedifferent ion currents to the potential difference across the membrane. using this set of differentialequations, certain essential features of neural behavior can be modeled. for example, assuming approsynaptic weights pscs decoding recurrent connections neuron soma dendrites input spikes h syn ( s ) h syn ( s ) recurrent matrix output spikes encoding dynamics matrices figure 5.13a generic neural subsystem. the diagram depicts the mathematical analysis of a neural subsystemand its mapping onto the biological systemña population of neurons. labels outside the gray boxes indicate therelevant biological structures and processes. neural action potentials (spikes) coming from a previous neuralpopulation generate weighted postsynaptic currents (pscs) in the dendrites of the neurons to which they areconnected. the subsequent voltage changes travel to the neural somata, where action potentials are generated,resulting in output spikes. because the input and output are neural spikes, this kind of subsystem can be linked toothers like it, permitting the construction of larger, more complex neural circuits (see figure 5.15 for an example).note that labels inside the gray boxes are generated based on understanding of the purpose of the neural systembeing modeled and on current understanding of neural representation (encoding), computation (decoding), anddynamics (dynamics matrices and hsyn). building simulations using these methods leads to a better understandingof how neural systems perform the complex functions they do. source: courtesy of chris eliasmith, universityof waterloo.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.computational modeling and simulation as enablers for biological discovery177priate parameter values, a constant input current larger than a certain critical value and turned on at agiven instant of time results in the potential difference across the membrane taking the form of a regularspike trainñwhich is reminiscent of how a real neuron fires. more realistic current inputs (e.g., stochastic ones) result in a much more realisticlooking output.despite lack of information about much of the cellular and molecular basis of neuronal excitation atthe time, hodgkin and huxley were able to provide a relatively accurate quantitative description ofhow an action potential was generated by voltagedependent ionic conductivities. the hodgkinhuxleymodel provided the basis for research for more than five decades, spinning off a new field of neurophysiology: in large part, this field rests on the foundation created by their model. recent research onmembrane ion channels can be related directly to the seminal ideas and (more importantly) precisemechanism that their model described.the òplain vanillaó hodgkinhuxley model is still interesting today. for example, a recent studydemonstrated previously unobserved dynamics in the hodgkinhuxley model, namely, the existence ofchaotic solutions in the model with its original parameters.103 the significance of chaos in this context isthat the excitability of a neural membrane with respect to firing is likely to be more complex than can beexplained by a simple sub or superthreshold potential.simulation and mathematical analysis of models have become essential tools in investigations ofthe complicated processes underlying rhythm generation in the nervous system. there are many typesof channels and synapses. the number of channels and synapses and their locations distinguish different types of neurons from one another. simulation of networks consisting of model neurons withfigure 5.14rodent navigation. these figures depict the behavior of a neurally realistic simulation of the pathintegrator in a rat. the simulation was generated by using a single (recurrent) generic neural subsystem. (a) whenthe simulation is given random noise, it spontaneously generates a stable, localized bump of neural activity over theneural sheet, which represents the ratõs current location. this demonstrates that a stable attractor (a widely acceptedmodel of how the ratõs path integrator is organized) has been implemented. (b) this model also implements control(i.e., updating of the current location based on the ratõs motion) of the path integrator in a neurally plausible way.here, straightline motion in a rightward direction is shown. (c) the model correctly integrates the circular path ofthe rat, demonstrating that it can path integrate in any direction that the rat might move. this simulation has verylittle error compared to the simulations of past models. source: chris eliasmith, university of waterloo, personalcommunication, september 7, 2004, and a. samsonovich and b.l. mcnaughton, òpath integration and cognitivemapping in a continuous attractor model,ó journal of neuroscience 17(15):59005920, 1997.abc103j. guckenheimer and r.a. oliva, òchaos in the hodgkinhuxley model,ó siam journal on applied dynamical systems 1(1):105114, 2002.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.178catalyzing inquiryspecified conductances and synapses enables researchers to test their intuitions regarding how thesenetworks function. simulations also lead to predictions of the effects of neuromodulators and disordersthat affect the electrical excitability of the systems. nonetheless, simulation alone is not sufficient todetermine the information we would like to extract from these models. the models have large numbersof parameters, many of which are difficult or impossible to measure, and the goal is to determine howthe system behavior depends on the values of all of these parameters.dynamical systems theory provides a conceptual framework for characterizing rhythms. this theoryexplains why there are only a small number of dynamical mechanisms that initiate or terminate burstsof action potentials, and it provides the foundations for algorithms that compute parameter space mapsdelineating regions with different dynamical behaviors. the presence of multiple time scales is animportant ingredient of this analysis because the rates at which different families of channels respond tochanges in membrane potential or ligand concentration vary over several orders of magnitude.figure 5.16 illustrates this type of analysis using a model for bursting in the prebıtzinger complex,a neural network in the brain stem that controls respiration. the first panel shows voltage recordingsfrom intracellular recordings of a medullar slice from neonatal rats. butera and colleagues measuredconductances in this preparation and constructed a model for this system.104 simulations of the bursta b c d f eg k x t r a=tr i h a* v j l m n r«a* a¥a *=<v,a¥a*> figure 5.15system for learning and performing deductive reasoning. the graphic describes the proposed system used during solution of the wason card selection task; see p.c. wason and p.n. johnsonlaird, psychology ofreasoning: structure and content, harvard university press, cambridge, ma, 1972. this task requires determiningwhen a logical rule is valid or invalid, and so is a form of deductive reasoning. humans perform notoriously badlyon many versions of this task, but well on other versions. this kind of context/content sensitivity is captured bythis model; see c. eliasmith, òlearning context sensitive logical inference in a neurobiolobical simulation,ó pp.1719 in compositional connectionism in cognitive science: papers from the aaai fall symposium, s.d. levy and r.gayler, program cochairs, october 2124, 2004, the aaai press, arlington, va, technical report fs0403, 2004.the depicted largescale circuit consists of 14 neural subsystems, distributed across frontal and ventral areas of thebrain. this is a good example of the degree of complexity that can be built into a neurally realistic simulation usingthese new techniques. populations ad learn and apply the appropriate context for interpretation of the rule (r)encoded by population e. populations f and g apply the relevant transformation (t) to the rule, giving the currentanswer (a). populations h, k, and l determine the degree of correctness or incorrectness of the suggested answer(either given the correct answer, or given a reward or punishment signal), resulting in an error signal e. populations m and n provide a guess at the best possible transformation. this guess and the error signal are integratedinto the learning algorithm. source: courtesy of chris eliasmith, university of waterloo.104r.j. butera, jr., j. rinzel, and j.c. smith, òmodels of respiratory rhythm generation in the prebıtzinger complex. i.bursting pacemaker neurons,ó journal of neurophysiology 82(1):382397, 1999.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.computational modeling and simulation as enablers for biological discovery179box 5.17computational perspectives on dopamine function in the prefrontal cortexconnectionist models of dopamine neuromodulationa longheld hypothesis suggests that catecholamine neurotransmitters, including dopamine (da), modulate targetneuron responses, by increasing their signaltonoise (snr) ratio (i.e. by increasing the differentiation between background or baseline firing rates and those that are evoked by afferent stimulation). for example, studies in the striatumshowed that da potentiated the response of target neurons to the effect of both excitatory and inhibitory signals.however, the precise biophysical mechanisms underlying these effects were not well understood. moreover, theview that da acts as a modulator in the prefrontal cortex (pfc) has been controversial, because, for many years, daapplication or stimulation of da neurons reliably inhibited spontaneous pfc activity. thus, many investigatorsargued that da served as an inhibitory transmitter in pfc.the first explicit computational models of the neuromodulatory function of catecholamines, and da in particular,were developed within the connectionist framework, and focused on their effects on information processing. although such models do not typically incorporate biophysical detail, by virtue of their simplicity they have theadvantage of simulating system level function and performance in a wide variety of cognitive tasks. within thisframework, da effects were simulated as a change in the slope (or gain) of the sigmoidally shaped inputoutputactivation function of processing units. thus, in the presence of da, both the excitatory and inhibitory influences ofafferent inputs are potentiated. computational analyses showed that this modulatory function would not improve thesnr characteristics of single neurons, but could do so at the network level. models implementing these ideas proveduseful for accounting for a wide range of phenomena, including the pharmacological effects of da on performancein tasks thought to rely on pfc and the effects of disturbances of da in schizophrenia.biophysically detailed modelsin recent work, computational studies have focused on more biophysically detailed accounts of da action withinpfc. models by durstewitz et al. and brunel and wang, all include data on the different biophysical effects of da onspecific cellular processes. these models have been used to simulate the dynamics of activity in networks thatclosely parallel the patterns observed in vivo within pfc. . . .these models synthesize the rapidly growing, but often confusing literature on the neurophysiology of da withinpfc. for example, the biophysical effects of da are shown to produce a suppressive influence on spontaneousactivity, explaining its apparent inhibitory actions, while at the same time causing an enhanced excitability inresponse to afferent drive. furthermore, the selective enhancement of inputs from recurrent versus external afferentsprovides a mechanism for stabilizing sustained activity patterns within pfc that are resistant to interference fromexternal inputs. these computational analyses support the characterization of da as a modulatory neurotransmitter,rather than a classical excitatory or inhibitory one, and explain its role in support sustained activity within pfc.strikingly, these models are remarkably consistent with the original hypothesis that da increases snr within thepfc, and the expression of this idea in earlier connectionist models. the underlying assumption in both types ofmodels is that shortterm storage of information in pfc occurs through recirculating activity within local recurrentnetworks, which can be described as fixedpoint attractor systems. da activity helps to stabilize attractor states, bothby making high activity states more stable (active maintenance), and low activity states (spontaneous backgroundactivity) less likely to spuriously transition to high activity states in the absence of strong afferent input. this isaccomplished by the concurrent potentiation of excitatory and inhibitory transmission, implemented as changes inion channel properties in biophysically detailed models and òsummarizedó as a change in the gain of the sigmoidalactivation function in connectionist models.these mechanisms can be used to simulate the effects of da on performance in cognitive tasks that rely on pfcfunction. for example, in a task emphasizing the role of pfc in working memory, increased da activation in thedurstewitz et al. model enhanced the stability of pfc working memory representations by making them less susceptible to interference from the intervening distractors. within connectionist models, similar effects have been demonstrated by changing the gain of the activation function, and simulating human performance in tasks known to rely onpfc, tasks similar to those simulated by durstewitz et al. and brunel and wang.source: reprinted by permission from j.d. cohen, t.s. braver, and j.w. brown, òcomputational perspectives on dopamine function inprefrontal cortex,ó current opinion in neurobiology 12(2):223229. copyright 2002 elsevier. (references omitted.)catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.180catalyzing inquiryfigure 5.16bursting in the prebotzinger complex.panel 1: example of voltagedependent properties of prebıtzinger complex (prebıtc) inspiratory bursting neurons. traces show wholecell patchclamp recordings from a single candidate pacemaker neuron in the prebıtc ofa 400µmthick neonatal rat transverse medullary slice with rhythmically active respiratory network. recordingsin a and b were obtained respectively before and after block of synaptic transmission by low ca2+ conditionsidentical to those described in johnson et al. (1994) (i.e., 0.2 mm ca2+, 4 mm mg2+, 9 mm k+ in slice bathingsolution). patch pipette solution and procedure for wholecell recording were as described previously (smith et al.1991, 1992). before block of synaptic transmission, the neuron bursts in synchrony with the inspiratory phase ofnetwork activity as monitored by the inspiratory discharge recorded on the hypoglossal (xii) nerve (smith et al.1991). after block of synaptic activity (30 minutes under lowca2+ conditions), the cell exhibits intrinsic voltagedependent oscillatory behavior. as the cell is depolarized by constant applied current, it undergoes a transitionfrom silence (baseline potential below 65 mv, left) to oscillatory bursting to beating (baseline potential above 45mv, right). in the bursting regime, the burst period and duration decreases (see expanded timebase traces in b) asthe baseline membrane potential is depolarized. source: reprinted by permission from r.j. butera, jr., j. rinzel,and j.c. smith, òmodels of respiratory rhythm generation in the prebıtzinger complex. i. bursting pacemakerneurons,ó journal of neurophysiology 82(1):382397, 1999. copyright 1999 american physiological society.continuedpanel 1catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.computational modeling and simulation as enablers for biological discovery181panel 2figure 5.16continuedpanel 2: gating and iv characteristics of components of models 1 and 2. (a) spikegenerating kinetics: m3(v) andh(v) of ina and n(v) and n(v) of ik; note that h=1− n. (b1) gating characteristics of inap: m(v), h(v), and h(v)(bold); left: yaxis scale for steadystate gating functions; right: yaxis scale for h(v). (b2) iv plots of inap forh=h(v) and h=1. first case results in a small window current at subthreshold potentials; second case corresponds to inaph with complete removal of inactivation. (c1) gating characteristics of iks: k(v) and k(v) (bold);left: yaxis scale for activation function; right: yaxis scale for k(v). (c2) iv plots of inap+iks for k=k(v) andk=0. first case results in a small current at subthreshold potentials; second case corresponds to inap with complete removal of the opposing iks.source: reprinted by permission from r.j. butera, jr., j. rinzel, and j.c. smith, òmodels of respiratory rhythmgeneration in the prebıtzinger complex. i. bursting pacemaker neurons,ó journal of neurophysiology 82(1):382397, 1999. copyright 1999 american physiological society.ing rhythms displayed by this model are shown in the second panel. the third panel shows a map of thesimulated trajectory that illustrates the relationship of the bursting to slow and fast variables in thesystem.5.4.5.4synaptic transmissionthe intercellular signaling process of synaptic transmission is a muchstudied problem. much hasbeen learned about synaptic structure and function through the classical techniques of neuropharmacology, electron microscopy (em) neuroanatomy, and electrophysiology, and correlation of the observations made through these various techniques has led to the development of computational models ofsynaptic microphysiology. however, the scope of previous modeling attempts has been limited byavailable computing power, modeling framework, and lack of highresolution threedimensional ultrastructural data in an appropriate machine representation.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.182catalyzing inquirypanel 3figure 5.16continuedpanel 3: projection of trajectory onto fixed points of fast subsystem. the axes are v: membrane potential; h: inactivation of the hh sodium channel (there is also a persistent sodium channel in the model); and n: activation of thehh òdelayed rectifieró potassium channel. the voltage traces show the changes of voltage as a function of time.the values of h and n also change with time. think of v, n, h as the three coordinates of a point moving throughspace. this plot depicts the path taken by this point in a bursting oscillation of the model. the curves are states atwhich the motion through this space is particularly slow, becoming zero in the limit so that the slower currents inthe model are not allowed to change at all. source: derived from figure 4, panel a3, in r.j. butera jr., j. rinzel,and j.c. smith, òmodels of respiratory rhythm generation in the prebıtzinger complex. i. bursting pacemakerneurons,ó journal of neurophysiology 82(1):382397, 1999. copyright 1999 american physiological society. used bypermission.what has been missing is an appropriate set of tools for acquiring, building, simulating, and analyzing biophysically realistic models of subcellular microdomains. coggan et al. have developed and useda suite of such computational tools to build a realistic computational model of nicotinic synaptic transmission based on serial electron tomograms of a chick ciliary ganglion somatic spine mat.105the chick ciliary ganglion somatic spine mat is a complex system with more than one type ofneurotransmitter receptor, possible alternative locations for transmitter release, and a tortuous synapticgeometry that includes a spine mat and calyxtype nerve terminal. highly accurate models of thesynaptic ultrastructure are obtained through largescale, highresolution electron tomography; com105j.s. coggan, t.m. bartol, e. esquenazi, j.r. stiles, s. lamont, m.e. martone, d.k. berg, m.h. ellisman, and t.j. sejnowski,òevidence for ectopic neurotransmission at a neuronal synapse,ó science 309(5733):446451, 2005.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.computational modeling and simulation as enablers for biological discovery183puteraided methods for extracting accurate surfaces and defining insilico representations of theirmolecular properties; and physiological underpinnings from a variety of studies conducted by theinvolved laboratories and from the literature.these data are then used as the framework for advanced simulations using mcell running on highperformance supercomputers as well as distributed or gridbased computational resources. this projectpushes development of tools for acquisition of improved largescale tomographic reconstructions ofcellular interfaces down to supramolecular scales. it also drives improvements in the software toolsboth for the distribution of molecular components within the surface models extracted from the tomographic reconstructions and for the deposition and retrieval of relevant information for the mcellsimulator (box 5.18) in the tomography and cellcentered database (ccdb) environment.realistic modeling of synaptic microphysiology (as illustrated in figure 5.17) requires the following:1.acquisition of highresolution, threedimensional synaptic ultrastructureñthis is accomplishedwith serial em tomography.2.segmentation of pre and postsynaptic membrane from the tomographic volumeñthis is accomplished using the tracing tool in xvoxtrace.3.threedimensional reconstruction of the membrane surface topology to form a triangle meshñthis is accomplished using the marching cubes isosurface extraction tool in xvoxtrace.4.subdivision of the membrane surface meshes into physiologically relevant regions (e.g., spineversus nonspine membrane and psd [phosphorylation site domain] versus nonpsd regions)ñthis isaccomplished using the mesh tagging tool in dreamm.5.placement of effector molecules (e.g., receptors, enzymes, reuptake transporters) onto membranesurfaces with the desired distribution and densityñthis is accomplished using the mcell model description language (mdl). effector distribution and density may be determined by labeling and imaging studies.6.specification of the diffusion constant, quantity, and location of neurotransmitter releaseñthis isaccomplished using mcell mdl.7.specification of the reaction mechanisms and kinetic rate constants governing the mass actionkinetics interaction of neurotransmitter and effector moleculesñthis is accomplished using mcell mdl.8.specification of what quantitative measures should be made during the simulationñthis is accomplished using mcell mdl.9.simulation of the defined systemñthis is accomplished using the mcell compute kernel.10.analysis of the results at various points in the parameter space defined by the systemñthis isaccomplished using analysis tools of the investigatorõs discretion.analysis of miniature excitatory postsynaptic currents (mepscs) recorded in electrophysiologicalexperiments shows that mepscs in the cg somatic spine mat occur in a broad spectrum of amplitudes,rise times, and fall times. the differential kinetics and complementary distributions of 3 and 7nachrs are expected to lead to mepscs whose characteristics are highly dependent on the location ofneurotransmitter release within the spine mat. realistic simulation makes it possible to explore andquantify the degree to which this hypothesis is true and to make quantitative comparisons of thesimulation and electrophysiological results. figure 5.18 summarizes the results of simulations designedto explore the limits of mepsc behavior by virtue of the choice of neurotransmitter release locations.the results not only confirm the qualitative expectations at each site but also predict their quantitativebehavior, allowing fine discriminations to be made.the process briefly outlined above represents a significant advance in the ability to create realisticcomputational models of subcellular microdomains from actual cellular ultrastructure. the preliminaryresults presented are just the beginning of exciting computational experiments that can now be performed on the cg model in an effort to illuminate and inform further bench experiments. among all ofthe things learned, perhaps the most important is which of the physical characteristics of the cg are thecatalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.184catalyzing inquirybox 5.18the mcell simulatormcell is a general monte carlo simulator of cellular microphysiology. mcell simulations provide insights intothe behavior and variability of real systems comprising finite numbers of molecules interacting in spatiallycomplex environments. mcell incorporates highresolution physical structure into models of ligand diffusionand signaling, and thus can take into account the large complexity and diversity of neural tissue at the subcellular level.mcell is based on the use of rigorously validated monte carlo algorithms to track the evolution of biochemicalevents in time and threedimensional space for individual ligand and effector molecules. that is, the montecarlo approach is based on the use of random numbers and probabilities to effect the simulation of individualcases of the systemõs behavior.in the mcell models used in neural signaling employing a brownian dynamics random walk algorithm, individual ligand molecules move according to a threedimensional brownian dynamics random walk and encounter membrane boundaries and effector molecules as they diffuse. bulk solution rate constants are converted into monte carlo probabilities so that the diffusing ligands can undergo stochastic chemical interactionswith individual binding sites such as receptor proteins, enzymes, and transporters. these interactions aregoverned by userspecified reaction mechanisms.the diffusion algorithms are gridfree, and the reaction algorithms are at the level of interactions betweenindividual molecules and thus do not involve solving systems of differential equations. membrane boundariesare represented as triangle meshes and may be of arbitrary complexity.the monte carlo approach has certain important advantages over the finite element (fe) approach often usedto include spatial information in kinetic modeling. the fe approach divides threedimensional space into aregular grid of contiguous subcompartments, or voxels. it assumes wellmixed conditions within each voxeland uses differential equations to compute fluxes between, and reactions within, each voxel. mass actionequations are based on continuum processes and predict average concentrations. in large, simple volumeswith great numbers of a few types of molecules (e.g., reactions in a test tube), fluctuations are relatively small,and knowledge of average concentrations accounts most of the interesting phenomena. however, synapticsignaling is inherently discrete and stochastic because the number of molecules involved is small; hence, thefe method will fail to describe accurately the biochemistry of synaptic signaling because these methodsprovide only averaged data. furthermore, complex cellular structuresñsuch as the structures that characterizethe synapseñrequire that the voxel grid be very fine and irregular in shape, making an fe approach bothcomputationally expensive and difficult to implement.mcell is very general because it includes a highlevel model description language (mdl), which allows theuser to build subcellular structures and signaling pathways of virtually any configuration. mcellõs algorithmsscale smoothly from typical workstations to sharedmemory multiprocessor machines to massively parallelsupercomputers.source: for more information, see http://www.mcell.cnl.salk.edu; j.r. stiles and t.m. bartol, jr., òmonte carlo methods for simulatingrealistic synaptic microphysiology using mcell,ó pp. 87127 in computational neuroscience: realistic modeling for experimentalists, e. deschutter, ed., crc press, boca raton, fl, 2000; j.r. stiles, t.m. bartol, jr., e.e. salpeter, m.m. salpeter, and t.j. sejnowski, òsynapticvariability: new insights from reconstructions and monte carlo simulations with mcell,ó pp. 681731 in synapses, w. cowan, t.c. sudhof,and c.f. stevens, eds., johns hopkins university press, baltimore, md, 2001. discussion of the pros and cons of fe versus mc is from k.m.franks and t.j. sejnowski, òcomplexity of calcium signaling in synaptic spines,ó bioessays 24(12):11301144, 2002.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.computational modeling and simulation as enablers for biological discovery185figure 5.17constructing the geometry of a chick ciliary ganglion (cg) somatic spine mat model. a serial emtomogram of a cg spine mat was obtained at ~4 nm per voxel resolution. the serial tomogram encompassed avolume of ~27 mm3 (~3 µm × 3 µm × 3 µm).(a) a typical slice through the tomographic volume together with handtraced contours of the pre and postsynaptic membranes. tracing and segmentation of presynaptic (cyan) and postsynaptic (red) membrane contours generated using xvoxtrace.(b) threedimensional reconstruction of pre and postsynaptic membrane surfaces as triangle meshesñview looking down onto intracellular face of presynaptic membrane (visualized using dreamm). the presynaptic mesh iscomposed of 100,000 triangles and the postsynaptic mesh is composed of 300,000 triangles.(c) postsynaptic membrane surfaceñview of extracellular face of membrane (presynaptic membrane invisible).(d) completed model including postsynaptic membrane subdivided into distinct spines, psd areas (black regionswith yellow borders), receptor molecules (tiny blue particles on membrane surface), and several neurotransmitterrelease sites (red spheres). the membrane was subsequently populated with the desired distributions and densities of nicotinic acetylcholine receptor (nachr) types and acetylcholine esterase (ache) enzyme. also visible in(d) are several acetylcholine (ach) vesicular release sites whose locations are most clearly illustrated in figure5.18a.(e) magnified view of the state of a simulation of synaptic transmission model as simulated by mcell. state ofsystem 300 ms after release of 5,000 molecules of acetylcholine (small green ellipsoids) is shown. 7 nachr typesare shown in blue, and 3* nachr types are shown in yellow (inactive receptors are semitransparent, and openreceptors are opaque).source: courtesy of tom bartol, salk institute, san diego, california.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.186catalyzing inquiryfigure 5.18summary of synaptic transmission simulations.(a) location of selected transmitter release sites and their associated simulated mepsc traces, each decomposedinto their 3 and 7 nachr components. each trace is the average of 100 simulations using mcell. site 1 is locatedat a psd on nonspine membrane. this site is expected to have a large 3 response and a very small 7 response. atthe other extreme of behavior, sites 3 and 5 are placed over nonpsd spine membrane. rich in 7 receptors andpoor in 3 receptors, these sites are expected to have large 7 responses and minimal 3 responses. the other sitesare placed at locations expected to give rise to mepscs of mixed nachr origin.(b) mepsc amplitudes (decomposed into their 3 and 7 nachr components) at each of 550 distinct vesicularrelease sites. the mepsc amplitudes are indicated by the diameter of the yellow spherical glyph and demonstratea strong dependence on location and underlying geometry.source: courtesy of tom bartol, salk institute, san diego, ca.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.computational modeling and simulation as enablers for biological discovery187least constrained, are least understood, and have the greatest impact on synaptic function. specifically,the results clearly demonstrate that synaptic geometry, receptor distribution, and vesicle release location each have a profound quantitative impact on the efficacy of the postsynaptic response. this meansthat attention to accuracy in the modelbuilding process must be a prime concern.5.4.5.5neuropsychiatry106the field of computational neuropsychiatry has been exploding with applications of largedeformation brain mapping technology that provide mechanisms for discovering neuropsychiatric disorders ofmany types. the hippocampus is a region of the brain (depicted in green in figure 5.19) that has beenimplicated in schizophrenia and other neurodegenerative diseases such as alzheimerõs. using largedeformation brain mapping tools in computational anatomy, researchers can define, visualize, andmeasure the volume and shape of the hippocampus. these methods allow for precise assessment ofchanges in hippocampal formation.researchers at the center for imaging science (cis) used mapping tools to compare the left andright hippocampi (figure 5.20) in 15 pairs of schizophrenic and control subjects. in the schizophrenicfigure 5.19the hippocampus in situ. source: courtesy of michael miller, johns hopkins university.106section 5.4.5.5 is based on l. wang, s.c. joshi, m.i. miller, and j.g. csernansky, òstatistical analysis of hippocampalasymmetry in schizophrenia,ó neuroimage 14(3):531545, 2001; j.g. csernansky, l. wang, s. joshi, j.p. miller, m. gado, d. kido,d. mckeel, et al., òearly dat is distinguished from aging by highdimensional mapping of the hippocampus,ó neurology55(11):16361643, 2000.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.188catalyzing inquirysubjects, deformations were localized to hippocampal subregions that send projections to the prefrontalcortex. the deformations strongly distinguish schizophrenic subjects from control subjects. the picturesindicate inward deformations by cooler colors, outward deformations by warmer colors, and littledeformation by a neutral green color. these results support the current hypothesis that schizophreniainvolves a disturbance of hippocampalprefrontal connections.in a separate study, cis researchers also compared asymmetry between the left and right hippocampi. the left and the right side of normal brains develop at different rates. structures on both sides ofthe brain are similar, but not identical. this is normal brain asymmetry. if a different asymmetry patternexists in schizophrenic subjects, it may indicate a disturbance of the leftright balance during earlystages of brain development. researchers found that the left hippocampus was narrower along theoutside edge than the right hippocampus. this asymmetry was similar in schizophrenic and normalsubjects (figure 5.21, left image). however, further comparison revealed a significant difference inasymmetry patterns of the hippocampal area called the subiculum (figure 5.21, right image). peoplewith schizophrenia tend to have a more pronounced depression and a downward bend in the surface ofthat structure.as part of washington universityõs healthy aging and senile dementia (hasd) program, cisresearchers have also applied brain mapping tools to assess the structure of the hippocampus in olderhuman subjects (depicted in figure 5.22). they compared measurements of hippocampal volume andshape in 18 subjects with early dementia of the alzheimer type (dat) with 18 healthy elderly and 15younger control subjects. hippocampal volume loss and shape deformities observed in subjects withdat distinguished them from both elderly and younger control subjects. the pattern of hippocampalfigure 5.20left and right hippocampuses. source: courtesy of michael miller, johns hopkins university.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.computational modeling and simulation as enablers for biological discovery189deformities in subjects with dat was largely symmetric and suggested damage to the ca1 hippocampal subfield.hippocampal shape changes were also observed in healthy elderly subjects, which distinguishedthem from healthy younger subjects. these shape changes occurred in a pattern distinct from thepattern seen in dat and were not associated with substantial volume loss. these assessments indicatethat hippocampal volume and shape derived from computational anatomy large deformation brainmapping tools may be useful in distinguishing early dat from healthy aging.5.4.6virologymathematical and computational methods are increasingly important to virology. for example, aprimary and surprising phenomenological aspect of hiv infection is that progression to aids usuallyfigure 5.21asymmetry in schizophrenia. source: michael miller, johns hopkins university.figure 5.22hippocampal structure in normal aging (left) versus in alzheimerõs disease patients (right). source:courtesy of michael miller, johns hopkins university.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.190catalyzing inquirybox 5.19modeling the in vivo dynamics of hiv1 infectionmathematical models of hiv infection and treatment have provided quantitative insights into the major biological processes that underlie hiv pathogenesis and helped establish the treatment of patients with combination therapy. this in turn has changed hiv from a fatal disease to a treatable one. the models successfullydescribe the changes in viral load in patients under therapy and have yielded estimates of how rapidly hiv isproduced and cleared in vivo, how long hivinfected cells survive while producing hiv, and how fast hivmutates and evolves drug resistance. they have also provided clues into the process of tcell depletion thatcharacterizes aids. the models have also provided means to rapidly screen antiviral drug candidates forpotency in vivo, thus hastening the introduction of new antiretroviral therapies.on average, hiv takes about 10 years to advance from initial infection to immune dysfunction (or aids).during this period the amount of virus measured in a personõs blood hardly changes. because of this slowprogression and the unchanging level of virus it was initially thought that this infection was slow and it wasunclear whether treating this disease early, when symptoms were not apparent, was worthwhile.recognizing that constant levels of virus meant only that the rates of viral production and clearance were inbalance, but not necessarily slow, perelson and david ho from rockefeller university used experimental drugtherapy to òperturbó the viral steady state. mathematically modeling the response to this perturbation using asystem of ordinary differential equations that kept track of the concentrations of infected cells and hiv, andfitting the experimental data to the model, revealed a plethora of new features of hiv infection.figure 5.19.1 shows that after therapy is initiated at time 0, levels of hiv rna (a surrogate for virus) fall tento a hundredfold in the first week or two of therapy. this suggested that hiv has a halflife (t1/2) of 12 days,and thus maintaining the pretherapy constant level of virus requires enormous virus productionñin fact, theamount of virus in the body must double every 12 days.detailed analysis showed that this viral decay was governed by two processes, clearance of free virus particles(t1/2 < 6 hours) and loss of productively infected cells (t1/2 < 1.6 days). from this rapid clearance of virus onecould compute that at steady state, ~1010 virions are produced daily and given the mutation rate of hiv, thateach single and most double mutations of the hiv genome are produced daily. thus, effective drug therapyfigure 5.19.1model predictions (lines) of the biphasic decay of hiv viral load compared with typical patient data(symbols). source: courtesy of a.s. perelson, los alamos national laboratory.continueddayshiv rna copies (per ml)104103105106dayshiv rna copies (per ml)104103105106102203040001012345678catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.computational modeling and simulation as enablers for biological discovery191takes a very long time, and individuals who have not progressed to fullblown aids are asymptomatic.as box 5.19 suggests, computational models have been able to shed considerable light on this phenomenology, and these insights have altered the view of aids from a static picture in which the virus isessentially dormant and does not do very much for a long time to a much more dynamic picture of a roughbalance between the virus and the immune system, both working very hard, for that period of time. thesefindings have had tangible impact, because they have affected drug treatment regimes considerably.more specifically, the average rate of hiv production in the human body is on the order of 1010copies per day as noted in box 5.19. empirical data indicate that errors in hiv replication occur at a rateon the order of 10ð4 to 10ð5 per base per generation, and since the hiv genome is 10,000 base pairs long,the likelihood that a replicated genome will contain at least one error is 10 percent to nearly unity (andthe vast majority of these errors are errors in a single base). because there are only four possible bases indna (and hence each base can change into only one of three other bases), there are only 30,000 possiblesinglebase mutations of a given genome. an error rate of 10ð4 to 10ð5 per base per generation distributed among 1010 copies each with 104 bases means that each generation produces 109 to 1010 mutations,which are distributed over the set of 30,000 possible mutations. put differently, every new day brings tolife on the order of 105 instances of every possible singlebase variant of hiv.thus, a drug known to bind to a particular sequence of amino acids at a certain location in a proteintoday will face 105 to 106 new variants tomorrow against which its effectiveness will be questionable.this fact suggests that drug treatment regimes must target multiple binding sites, and hence combination drug therapy is likely to be more effective because drugresistant variants must then be the result ofmultiple errors in the replication process (which occur much less frequently). this in fact reflects recentexperience with combination drug regimes.1075.4.7epidemiologyepidemiology is the study of the dynamics of disease in a population of individuals. of particularinterest is the epidemiology of infectious diseases, which arise from contact between an environmentalbox 5.19 continuedwould require drug combinations that can sustain at least three mutations before resistance arises, and thisengendered the idea of triple combination therapy. other analyses showed that the slope of viral decay wasproportional to the drug combinationsõ antiviral efficacy, providing a means of comparing therapies.following the rapid 12 week òfirst phaseó loss, the rate of hiv rna decline slows. models of this òsecondphaseó of decline, when fitted to the kinetic data, suggested that a small fraction of infected cells might live aperiod of weeks while infected (t1/2 ~ 14 days).following upon the success of these joint modeling and experimental efforts, many similar studies wereundertaken and revealed a fourth, much longer timescale for the decay of latently infected cells of 644months. latently infected cells, which harbor the hiv genome but do not produce virus, can hide from theimmune system and reignite infection when the cells are stimulated into proliferation. clearing latently infected cells is one of the last remaining obstacles to eradicating hiv from the body.107for further discussion, see a.g. rodrigo, òhiv evolutionary genetics,ó proceedings of the national academy of sciences96(19):1055910561, 1999; b.a. cipra, òwill viruses succumb to mathematical models?ó siam news 32(2), 1999, available athttp://www.siam.org/siamnews/0399/viruses.pdf.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.192catalyzing inquiryagent and an individual (e.g., an insect that bites an individual) or between individuals (e.g., an individual who sneezes in a room filled with people) that leads to the transmission of disease. the dynamicsof infectious diseases depend on many things, such as the likelihood of transmission between carrieragent and infected individual given that contact has been made, the geographical distribution of carrieragents and individuals, and the susceptibility of individuals to the disease.a central problem in epidemiology is how the dynamics of disease play out across geographicalspace.108 problems of spatial heterogeneity play out at many different levels of aggregation: individuals, families, work groups and firms, neighborhood, and cities. box 5.20 provides an example takenfrom the study of measles.at the same time, spatial heterogeneity is not the only inhomogeneity of interest. for example, theepidemiology of sexually transmitted diseases (stds) cannot be separated from a consideration of theirdynamics in different social groups. for example, patterns of stds in prostitutes and intravenous drugbox 5.20spatial heterogeneity in epidemiology: an exampleone of the best illustrations of [the significance of spatial heterogeneity] is provided by the highly dynamic spatiotemporal epidemic pattern of measles. an important set of analyses of simple, homogeneous models predicted thepossibility of chaotic dynamics; however, the resulting largeamplitude [predicted] epidemics generate unrealistically low persistence of infection in small communities. adding successive layers of social and geographical spaceñand moving from deterministic to stochastic modelsñimproves spatial realism and may reduce the propensity forchaos.the major computational challenge in these highly nonlinear stochastic systems is to represent hierarchical spatialcomplexity and especially its impact on vaccination strategies. depending on the problem, all scalesñfrom theindividual level to big citiesñmay be important, both in terms of social space [family and school infection dynamics]and in terms of geographic spread and coherency.. . . [a] central question is: how spatially aggregated and parsimonious a model can provide useful results in a givencontext? this is particularly important in comparisons between directly transmitted human infectionsñwhere longrange movements may bring infection dynamics comparatively close to mean field behavior (in which every individual is assumed to have equal contact with every other individual, thus experiencing the mean or average field)ñandthe equivalent infections in natural populations, where more restricted movements and host population dynamicsadd extra complexities.it is risky to model at a given level of detail without having data at the relevant spatial grain. notifiable infectiousdiseases are unusually well [documented], with large and often as yet uncomputerized spatiotemporal data sets.these data provide a huge potential testbed for developing methods for characterizing spatiotemporal dynamics innonlinear, nonstationary stochastic systems. an encouraging development is that the current, generally nonparametric, approaches to characterizing chaos and other nonlinear behaviors are increasingly incorporating lessons frommechanistic epidemiological models.source: reprinted by permission from s.a. levin, b. grenfell, a. hastings, and a.s. perelson, òmathematical and computationalchallenges in population biology and ecosystems science,ó science 275(5298):334343, 1997. copyright 1997 aaas. (references omitted.)108k. dietz, òthe estimation of the basic reproduction number for infectious diseases,ó statistical methods in medical research2(1):2341, 1993; a.d. cliff and p. haggett, atlas of disease distributions: analytic approaches to epidemiologic data, blackwell ltd,oxford, uk, 1988; d. mollison and s.a. levin, òspatial dynamics of parasitism,ó pp. 384398 in ecology of infectious diseases innatural populations, b.t. greenfell and a.p. dobson, eds., cambridge university, cambridge, uk, 1995.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.computational modeling and simulation as enablers for biological discovery193users exhibit different dynamical patterns than those in the general population because of factors suchas rates of sexual contact with others (both inside and outside the individualõs own social group) anddifferent sexual practices of individuals in each group (e.g., use of condoms). box 5.21 elaborates on thisnotion in greater detail.5.4.8evolution and ecology5.4.8.1commonalities between evolution and ecologyno two fields in biology encompass such a broad range of levels of biological organization asecology and evolutionary biology. although the two fields ask different questions, they both contendwith factors of space and time, and share common theories about relationships between individuals,populations, and communities. the two intertwined fields view these relationships in different ways.evolutionary biologists want to understand and quantify the effect of environment (e.g., natural selection) on individuals and populations; ecologists want to understanding the role of individuals andpopulations in shaping their environment (ecological inheritance, niche construction).the two fields encompass a diverse assemblage of topics with applications in resource management, epidemiology, and global change. in these fields, data have been relatively difficult to collect inways that relate directly to mathematical or computational models, although this has been changingover the past 10 years. thus, both fields have relied heavily on theory to advance their insights. in fact,ecology and evolution have been the substrate for the development of important mathematical concepts. the quantitative study of biological inheritance and evolution provided the context for statistics,probability theory, stochasticity, and dynamical systems theory.box 5.21social heterogeneity in epidemiology: an examplethe main focus for modeling social space (the space of social interactions) and disease is, of course, on aids andother sexually transmitted infections. simple models illustrated clearly that heterogeneities in contact rates cansubstantially alter the predicted course of epidemics. this area has seen an explosion of research, both in dataanalysis of contact structures and in graphtheoretic and other approaches to modeling. models and data analysis aremost productive when combined, especially in allowing the observations to limit the universe of possible networks.the major computational challenge is how to deal with the complexity of networks, where concurrency of partnerships often means that closure to a few moments of the distribution is difficult. this problem is especially acute giventhe sensitivity of obtaining data for std networks, in that the nature of the network is generally only partially andimperfectly known. the use of mathematical models for human immunodeficiency virus (hiv) transmission will beespecially important in assessing the impact of potential vaccines. another major computational challengeñwhichdeveloped with the aids epidemic and is currently being applied to another pathogen, the bovine spongiformencephalopathy agentñis to estimate the parameters of transmission models from disease incidence and otherdemographic data.one hope for the future for both of these areas is network information embedded in viral genomes. a body of recentwork indicates exciting possibilities for estimating epidemiological parameters from the birth and death processes ofpathogen evolutionary trees. more generally, new mathematical and computational techniques will be needed tounderstand the epidemiological implications of the rapidly accumulating data on pathogen sequences, especially inthe context of parasite genetic diversity and the host immunological response to it.source: reprinted by permission from s.a. levin, b. grenfell, a. hastings, and a.s. perelson, òmathematical and computationalchallenges in population biology and ecosystems science,ó science 275(5298):334343,1997. copyright 1997 aaas. (references omitted.)catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.194catalyzing inquiryamong the fundamental questions in the study of evolution are those that seek to know the relativestrengths of natural selection, genetic drift, dispersal processes, and genetic recombination in shapingthe genome of a populationñessentially the forces that provide genetic variability in a species. bothecologists and evolutionary biologists want to know how these forces lead to morphological changes,speciation, and ultimately, survival over time. the fields seek theory, models, and data that can accountfor genetic changes over time in large heterogeneous populations in which genetic information isexchanged routinely in an environment that also exerts its influence and changes over time.in addition to interest in genetic variability and fitness within a single species, the two fields areinterested in relationships between multiple species. in ecology, this manifests itself in questions of howthe individual forces of variability within and between species affect their relative ability to compete forresources and space that leads to their survival or extinctionñin other words, forces that determines thebiodiversity of an ecosystem (i.e., a set of biological organisms interacting among themselves and theirenvironment). ecologists want to understand what determines the minimum viable population size fora given population, the role of keystone species in determining the diversity of the ecosystem, and therole of diversity in preservation of the ecosystem.for evolutionary biologists, questions regarding relationships between species focus on trying tounderstand the flow of genetic information over long periods of time as a measure of the relatedness ofdifferent species and the effects of selection on the genetic contribution to phenotypes. among the greatmysteries for evolutionary biologists is whether and how evolution relates to organismal development,an interaction for which no descriptive language currently exists.how will ecologists and evolutionary biologists answer these questions? these fields have had fewtools to monitor interactions in real time. but new opportunities have emerged in areas from genomicsto satellite imaging and in new capabilities for the computer simulation of complex models.5.4.8.2examples from evolutiona plethora of genomic data is beginning to help untangle the relationship between traits, genes,developmental processes, and environments. the data will serve as the substrate from which newstatistical conclusions can be drawn, for example, new methods for identifying inherited gene sequences such as those related to disease. to answer question about the process of genome rearrangement, the possibility of comparing gene sequences from multiple organisms provides the basis fortesting tools that discern repeatable patterns and elucidate linkages.as more detailed dna and protein sequence information is compiled for more genes in moreorganisms, computational algorithms for estimating parameters of evolution have become extremelycomplex. new techniques will be needed to handle the likelihood functions and produce satisfactorystatistics in a reasonable amount of time. studies of the role of environmental and genetic plasticity intrait development will involve largescale simulations of networks of linked genes and their interactingproducts. such simulations may well suggest new approaches to such old problems as the naturenurture dichotomy for human behaviors.new techniques and the availability of more powerful computers have also led to the developmentof highly detailed models in which a wide variety of components and mechanisms can be incorporated.among these are individual unit models that attempt to follow every individual in a population overtime, thereby providing insight into dynamical behavior (box 5.22).levin argues that such models are òimitation[s] of reality that represent at best individual realization of complex processes in which stochasticity, contingency, and nonlinearity underlie a diversity ofpossible outcomes.ó109 from the collective behaviors of individual units arise the observable dynamics109s.a. levin, b. grenfell, a. hastings, and a.s. perelson, òmathematical and computational challenges in population biologyand ecosystems science,ó science 275(5298):334343, 1997.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.computational modeling and simulation as enablers for biological discovery195of the system. òthe challenge, then, is to develop mechanistic models that begin from what is understood about the interactions of the individual units, and to use computation and analysis to explainemergent behavior in terms of the statistical mechanics of ensembles of such units.ó such models mustextrapolate from the effects of change on individual plants and animals to changes in the distribution ofindividuals over longer time scales and broader space scales and hence in communitylevel patternsand the fluxes of nutrients.5.4.8.2.1reconstruction of the saccharomyces phylogenetic treealthough the basic structure andmechanisms underlying evolution and genetics are known in principle, there are many complexitiesthat force researchers into computational approaches in order to gain insight. box 5.23 addresses complexities such as multiple loci, spatial factors, and the role of frequency dependence in evolution, anddiscusses a computational perspective on the evolution of altruism, a behavioral characteristic that iscounterintuitive in the context of individual organisms doing all that they can to gain advantage in theface of selection pressures.box 5.22the dynamics of evolutionavida is a simulation software system developed at the digital life laboratory at the california institute oftechnology.1 in it, digital organisms have genomes comprised of a sequence of instructions that operate on avirtual machine. these instructions include the ability to perform simple mathematical operations, copy values from memory location to memory location, provide input and output, and check conditions. through asequence of instructions, these organisms can copy their genome, thereby reproducing asexually. since thesoftware can simulate many hundreds of thousands of generations of evolution for thousands of organisms,their digital evolution not only can be observed in reasonable lengths of time, but also can be preciselyinspected (since there are no inconvenient gaps in the fossil record). moreover, alternate scenarios can beexplored by going back into evolutionary history and reversing the effects of mutations, for example. at aminimum, this can be seen as experiment by analogy, revealing potential avenues for investigation or hypotheses to test in actual biological evolution. a stronger argument holds that evolution is an abstract mathematical process and will operate under similar dynamics whether embodied in dna in the physical world or indigital simulations of it.avida has been used to explore how complex features can arise through mutation, competition, and selectivepressure.2 in a series of experiments, organisms were provided with a limited supply of energy units necessaryfor the execution of their genome of instructions. however, organisms that performed any of a set of complexlogical operations were rewarded with an increased allowance and thus increased opportunities to reproduce.more complicated logical operations provided proportionally greater rewards.the experiment was seeded with an ancestral form that could perform none of those operations, containingonly the instructions to reproduce. mutation arose through imperfect copying of the genome during reproduction. equ, the most complex logical operation checked for [representing the logical statement (a and b) or(~a and ~b)], arose in 23 out of 50 populations studied where the simpler operations also provided rewards.the sequence of instructions that evolved to perform the operation varied widely in length and implementation. however, in other simulations where only equ was rewarded, no lineages ever evolved it. this evidence agrees with the standard theory of biological evolutionñstated as early as darwinñthat complexstructures arise through the combination and modification of useful intermediate forms.1c. adami, introduction to artificial life, springerverlag, new york, 1998.2r.e. lenski, c. ofria, r.t. pennock, and c. adami, òthe evolutionary origin of complex features,ó nature 423:139144, 2003.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.196catalyzing inquirybox 5.23genetic complexities in evolutionary processesthe dynamics of alleles at single loci are well understood, but the dynamics of alleles at two loci are still notcompletely understood, even in the deterministic case. as a rule, twolocus models require the use of a varietyof computational approaches, from straightforward simulation to more complex analyses based on optimization or the use of computer algebra systems. threelocus models can be understood only through numericalapproaches, except for some very special cases.compare these analytical capabilities to the fact that the number of loci exhibiting genetic variation in populations of higher organisms is well into the thousands. thus, the number of possible genotypes can be muchlarger than the population. in such a situation, the detailed population simulation (i.e., a detailed consideration of events at each locus) leads to problems of substantial computational difficulty.an alternative is to represent the population as phenotypesñthat is, in terms of traits that can be directlyobserved and described. for example, certain traits of individuals are quantitative in the sense that theyrepresent the sum of multiple small effects. efforts have been undertaken to integrate statistical models of thedynamics of quantitative traits with more mechanistic genetic approaches, though even under simplifyingassumptions concerning the relation between genotype and phenotype, further approximations are requiredto obtain a closed system of equations.frequency dependence in evolution refers to the phenomenon in which the fitness of an individualdepends both on its own traits and on the traits of other individuals in the populationñthat is, selection isdependent on the frequency with which certain traits appear in the population, not just on pressures fromthe environment.this point arises most strongly in understanding how cooperation (altruism) can evolve through individualselection. the simplest model is the game of prisonerõs dilemma, in which the gametheoretic solution for asingle encounter between parties is unconditional noncooperation. however, in the iterated prisonerõs dilemma, the game theoretic solution is a strategy known as òtitfortat,ó which begins with cooperation and thenuses the strategy employed by the other player in the previous interaction. (in other words, the iterated prisonerõs dilemma stipulates repeated interactions over time between players.)although the iterated prisonerõs dilemma yields some insight into how cooperative behavior might emergeunder some circumstances, it is a highly and perhaps oversimplified model. most importantly, it does notaccount for possible spatial localizations of individualsña point that is important in light of the fact thatindividuals who are spatially separated have low probabilities of interacting. because the evolution of traitsdependent on population frequency requires knowledge of which individuals are interacting, more realisticmodels introduce some explicit spatial distribution of individualsñand, for these, simulations are required todynamical understanding. these more realistic models suggest that spatial localization affects the evolution ofboth cooperative and antagonistic behaviors.source: adapted from s.a. levin, b. grenfell, a. hastings, and a.s. perelson, òmathematical and computational challenges inpopulation biology and ecosystems science,ó science 275(5298):334343, 1997. (references in the original.)catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.computational modeling and simulation as enablers for biological discovery197along these lines, a particularly interesting work on the reconstruction of phylogenies was reportedin 2003 by rokas et al.110 one of the primary goals of evolutionary research has been understanding thehistorical relationships between living organismsñreconstruction of the phylogenetic tree of life. aprimary difficulty in phylogenetic reconstruction is that different singlegene datasets often result indifferent and incongruent phylogenies. such incongruences occur in analyses at all taxonomic levels,from phylogenies of closely related species to relationships between major classes or phyla and highertaxonomic groups.many factors, both analytical and biological, may cause incongruence. to overcome the effect ofsome of these factors, analysis of concatenated datasets has been used. however, phylogenetic analysesof different sets of concatenated genes do not always converge on the same tree, and some studies haveyielded results at odds with widely accepted phylogenies.rokas et al. exploited genome sequence data for seven saccharomyces species and for the outgroupfungus candida albicans to construct a phylogenetic tree. their results suggested that datasets consisting ofa single gene or a small number of concatenated genes had a significant probability of supporting conflicting topologies, but that use of the entire dataset of concatenated genes resulted in a single, fully resolvedphylogeny with the maximum likelihood. in addition, all alternative topologies resulting from singlegeneanalyses were rejected with high probability. in other words, even though the individual genes examinedsupported alternative trees, the concatenated data exclusively supported a single tree. they concludedthat òthe maximum support for a single topology regardless of method of analysis is strongly suggestiveof the power of large data sets in overcoming the incongruence present in singlegene analyses.ó5.4.8.2.2modeling of myxomatosis evolution in australiaevolution also provides a superb and easytounderstand example of time scales in biological phenomena. around 1860, a nonindigenous rabbitwas introduced into australia as part of british colonization of that continent. since this rabbit had noindigenous foe, it proliferated wildly in a short amount of time (about 20 years). early in the 1950saustralian authorities introduced a particular strain of virus that was deadly to the rabbit.the data indicated that in the short term (say, on a time scale of a few months), the mostvirulent strains of the virus were dominant (i.e., the virus had a lethality of 99.8 percent). this is notsurprising, in the sense that one might expect virulence to be a measure of viral fitness. however, inthe longer term (on a scale of decades), similar measurements indicate that these more virulentstrains were no longer dominant, and the dominant niche was occupied by less virulent strains(lethality of 90 percent or less). the evolutionary explanation for this latter phenomenon is that anexcessively virulent virus would run the risk of killing off its hosts at too rapid a rate, therebyjeopardizing its own survival. the underlying mechanism responsible for this counterintuitivephenomenon is that transmission of the virus depended on mosquitoes feeding from live rabbits.rabbits that were infected with the more virulent variant died quickly, and thus, fewer were available as sources of that variant.the above system was modeled in closed form based on a set of coupled differential equations; thismodel was successful in reproducing the essential qualitative features described above.111 in 1990, thismodel was extended by dwyer et al. to incorporate more biologically plausible features.112 for example, the evolution of rabbit and virus reacting to each other was modeled explicitly. a multiplicity of110a. rokas, b.l. williams, n. king, and s.b. carroll, ògenomescale approaches to resolving incongruence in molecularphylogenies,ó nature 425(6960):798804, 2003.111s. levin and d. pimentel, òselection of intermediate rates of increase in parasitehost systems,ó the american naturalist117(3), 1981.112g. dwyer, s.a. levin, and l.a. buttel, òa simulation model of the population dynamics and evolution of myxomatosis,óecological monographs 60(4):423447, 1990.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.198catalyzing inquiryvirus vectors was modeled, each with different transmission efficiencies, rather than assuming a singlevector. the inclusion of such features, coupled with exploitation of a wealth of data available on thissystem, allowed dwyer et al. to investigate questions that could not be addressed in the earlier model.these questions included whether the system will continue to evolve antagonistically and whether thevirus will be able to control the rabbit population in the future.more broadly, this example illustrates the important lesson that both time scales are equally significant from an evolutionary perspective, and one is not more òfundamentaló than the other when it comesto understanding the dynamical behavior of the system. furthermore, it demonstrates that pressures fornatural selection can operate at many different levels of complexity.5.4.8.2.3the evolution of proteins by making use of simple physical models of proteins, it is possibleto model evolution under different evolutionary, structural, and functional scenarios. for example,cubic lattice models of proteins can be used to model enzyme evolution involving binding to twohydrophobic substrates. gene duplication coupled to subfunctionalization can be used to predict enzyme gene duplicate retention patterns and compare with genomic data.113 this type of physical modeling can be expanded to other evolutionary models, including those that incorporate positive selectivepressures or that vary population genetic parameters. at a structural level, they can be used to addressissues of protein surfaceareatovolume ratios or the evolvability of different folds. ultimately, suchmodels can be extended to real protein shapes and can be correlated to the evolution of different foldsin real genomes.114the role of structure in evolution during potentially adaptive periods can also be analyzed. asubset of positive selection will be dictated by structural parameters and intramolecular coevolution.common interactions, like rkde ionic interactions can be detected in this manner. similarly, lesscommon interactions, like cationp interactions, can also be detected and the interconversion betweendifferent modes of interactions can be assessed statistically.one important tool underlying these efforts is the adaptive evolution database (taed), a phylogenetically organized database that gathers information related to coding sequence evolution.115 thisdatabase is designed to both provide highquality gene families with multiple sequence alignments andphylogenetic trees for chordates and embryophytes and to enable answers to the question, òwhatmakes each species unique at the molecular genomic level?óstarting with genbank, genes have been grouped into families, and multiple sequence alignmentsand phylogenetic trees have been calculated. in addition to multiple sequence alignments and phylogenetic trees for all families of chordate and embryophyte sequences, taed includes the ratio ofnonsynonymous to synonymous nucleotide substitution rates (ka/ks) for each branch of every phylogenetic tree. this ratio, when significantly greater than 1, is an indicator of positive selection and potentially a change of function of the encoded protein in closely related species, and has been useful in theconstruction of phylogenetic trees with probabilistic reconstructed ancestral sequences calculated usingboth parsimony and maximum likelihood approaches. with a mapping of gene tree to species tree, thebranches whose ratio is significantly greater than 1 are collated together in a phylogenetic context.113f.n. braun and d.a. liberles, òretention of enzyme gene duplicates by subfunctionalization;ó international journal ofbiological macromolecules 33(13):1922, 2003.114h. hegyi, j. lin, d. greenbaum, and m. gerstein, òstructural genomics analysis: characteristics of atypical, common, andhorizontally transferred folds,ó proteins 47(2):126141, 2002.115d.a. liberles, òevaluation of methods for determination of a reconstructed history of gene sequence evolution.ó molecular biology and evolution 18(11):20402047, 2001; d.a. liberles, d.r. schreiber, s. govindarajan, s.g. chamberlin, and s.a. benner,òthe adaptive evolution database (taed),ó genome biology 2(8):research0028.10028.6, 2001; c. roth, m.j. betts, p. steffansson,g. s¾lensminde, and d.a. liberles, òthe adaptive evolution database (taed): a phylogenybased tool for comparativegenomics,ó nucleic acids research 33(database issue):d495d497, 2005.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.computational modeling and simulation as enablers for biological discovery199116e.v. koonin, n.d. fedorova, j.d. jackson, a.r. jacobs, d.m. krylov, k.s. makarova, r. mazumder, et al., òa comprehensive evolutionary classification of proteins encoded in complete eukaryotic genomes,ó genome biology 5(2):r7, 2004. (cited inroth et al., òthe adaptive evolution database,ó 2005.)117r. rossnes, òphylogenetic reconstruction of ancestral character states for gene expression and mrna splicing data,óm.sc. thesis, universtiy of bergen, norway, 2004. (cited in roth et al., 2005.)118see, for example, g.f. joyce, òthe antiquity of rnabased evolution,ó nature 418(6894):214221, 2002.119m. eigen, òselforganization of matter and the evolution of biological macromolecules,ó naturwissenschaften 58(10):465523,1971.120p. szabš, i scheuring, t. czaran, and e. szathmary, òin silico simulations reveal that replicators with limited dispersalevolve towards higher efficiency and fidelity,ó nature 420(6913):340343, 2002. a very helpful commentary on this article canbe found in g.f. joyce, òmolecular evolution: booting up life,ó nature 420(6894):278ð279, 2002. the discussion in section5.4.8.2.4 is based largely on this article.121w.k. johnston, p.j. unrau, m.s. lawrence, m.e. glasner, and d.p. bartel, òrnacatalyzed rna polymerization: accurateand general rnatemplated primer extension,ó science 292(5520):13191325, 2001.the taed framework is expandable to incorporate other genomicscale information in a phylogenetic context. this is important because coding sequence evolution (e.g., as reflected in the ka/ks ratio)is only one part of the molecular evolution of genomes driving phenotypic divergence. changes in genecontent116and phylogenetic reconstructions of changes in gene expression and alternative splicingdata117 can indicate where other significant lineagespecific changes have occurred. altogether, phylogenetic indexing of genomic data presents a powerful approach to understanding the evolution offunction in genomes.5.4.8.2.4the emergence of complex genomeshow did life get started on earth? today, life is based ondna genomes and protein enzymes. however, biological evidence exists to suggest that in a previousera, life was based on rna, in the sense that genetic information was contained in rna sequences andphenotypes were expressed as catalytic properties of rna.118an interesting and profound issue is therefore to understand the transition from the rna to thedna world, one element of which is the fact that dna genomes are complex structures. in 1971, eigenfound an explicit relationship between the size of a stable genome and the error rate inherent in itsreplication, specifically that the size of the genome was inversely proportional to the pernucleotidereplication error rate.119 thus, for a genome of length l to be reasonably stable over successive generations, the maximum tolerable error rate in replication could be no more than 1/l per nucleotide.however, more precise replication mechanisms tend to be more complex. given that the replicationmechanism must itself be represented in the genome, the puzzle is that a precise replication mechanism is needed to maintain a complex genome, but a complex genome is required to encode such amechanism.the only possible answer to this puzzle is that complex genomes evolved from simpler ones. szabšet al. investigated this possibility through computer simulations.120 they constructed a population ofdigital genomes subject to evolutionary forces and found that under a certain set of circumstances, bothgenome size and replication fidelity increased with the run time of the simulation. however, suchbehavior was dependent on the existence of a sufficient amount of spatial isolation of the evolvingpopulation. in the absence of separation (i.e., in the limit of very rapid diffusion of genomes across thetwodimensional surface to which they were confined), genome complexity and replication fidelitywere both limited. however, if diffusion is slow (i.e., the characteristic time constant of diffusion is lessthan the time scale of replication), both complexity and fidelity increase.in addition, johnston et al. have synthesized in the laboratory a catalytic rna molecule that contains about 200 nucleotides and synthesizes rna molecules of up to 14 nucleotides, with an error rateof about 3 percent per residue.121 this laboratory demonstration, coupled with the computationalfinding described above, suggest that a small rna genome that operates as an rna replicase withcatalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.200catalyzing inquirymodest efficiency and fidelity could evolve a succession of everlarger genomes and everhigher replication efficiencies.5.4.8.3examples from ecology122simulationbased study of an ecosystem considers the dynamic behavior of systems of individualorganisms as they respond to each other and to environmental stimuli and pressures (e.g., climate) andexamines the behavior of the ecosystem in aggregate terms. however, no individual run of such asimulation can be expected to predict the detailed behavior of each individual organism within anecosystem. rather, the appropriate test of a simulationõs fidelity is the extent to which it can, through aprocess of judicious averaging of many runs, predict features that are associated with aggregation atmany levels of spatial and/or temporal detail. these more qualitative features provide the basis fordescriptions of ecosystem dynamics that are robust across a variety of dynamical scenarios that aredifferent at a detailed level and also provide highlevel descriptions that can be more readily interpretedby researchers.because of the general applicability of the approach described above, simulations of dynamicalbehavior can be developed for aggregations of any organisms as long as they can be informed byadequate understandings of individuallevel behavior and the implications of such behavior for interactions with other individuals and with the environment.note also the key role played by ecosystem heterogeneity. spatial heterogeneity is one obvious wayin which nonuniform distributions play a role. but in biodiversity, functional heterogeneity is alsoimportant. in particular, essential ecosystem functions such as the maintenance of fluxes of certainnutrients and pollutants, the mediation of climate and weather, and the stabilization of coastlines maydepend not on the behavior of all species within the ecosystem but rather on a limited subset of thesespecies. if biodiversity is to be maintained, the most fragile and functionally critical subsets species mustbe identified and understood.the mathematical and computational challenges range from techniques for representing and accessing datasets, to algorithms for simulation of largescale spatially stochastic, multivariate systems, tothe development and analysis of simplified description. novel data acquisition tools (e.g., a satellitebased geographic information system that records changes for insertion in the simulations) would bewelcome in a field that is relatively data poor.5.4.8.3.1impact of spatial distribution in ecosystemsan important dimension of ecological environments is how organisms interact with each other. one oftenmade computationally simple assumption isthat an organism is equally likely to interact with every other organism in the environment. although thisis a pragmatic assumption, actual ecosystems are physical and organisms interact only with a very smallnumber of other organismsñnamely, the ones that are nearby in a spatial sense. moreover, localizedselectionñin which a fitness evaluation is undertaken only under nearest neighborsñis also operative.introducing these notions increases the speciation rate tremendously, and the speculation is that ina nonlocalized environment, the pressures on the population tend toward population uniformityñeverything looks similar, because each entity faces selection pressure from every other entity. whenlocalization occurs, different species emerge in different spatial areas. further, the individuals that areevolving will start to look quite different from each other, even though they have (comparably) high122section 5.4.8.3 is based largely on material taken from s.a. levin, b. grenfell, a. hastings, and a.s. perelson, òmathematicaland computational challenges in population biology and ecosystems science,ó science 275(5298):334343, 1997.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.computational modeling and simulation as enablers for biological discovery201fitness ratings. (this phenomenon is known as convergent evolution, in which a given environmentmight evolve several different species that are in some sense equally well adapted to that environment.)as an example of spatial localization, kerr et al. developed a computational model to examine thebehavior of a community consisting of three strains of e. coli,123 based on a modification of the latticebased simulation of durrett and levin.124 one of the strains carried a gene that created an antibioticcalled colicin. (the colicinproducing strain, c, was immune to the colicin it produced.) a second strainwas sensitive to colicin (s), while a third strain was resistant to colicin (r). furthermore, the factors thatmake the s strain sensitive also facilitate its consumption of certain nutrients, and the r strain is less ableto consume these nutrients. however, because the r strain does not have to produce colicin, it avoids ametabolic cost incurred by the c strain. the result is that c bacteria kill s bacteria, s bacteria thrivewhere r bacteria do not, and r bacteria thrive where c bacteria do not. the community thus satisfies aòrockpaperscissorsó relationship.the intent of the simulation was to explore the spatial scale of ecological processes in a communityof these three strains. it was found found (and confirmed experimentally) that when dispersal andinteraction were local, patches of different strains formed, and these patches chased one another overthe latticeñtype c patches encroached on s patches, s patches displaced r patches and r patcheinvaded c patches. within this mosaic of patches, the local gains made by any one type were soonenjoyed by another type; hence the diversity of the system was maintained. however, dispersal andinteraction were no longer exclusively local (i.e., in the òwellmixedó case in which all three strains areallowed to interact freely with each other): continual redistribution of c rapidly drove s extinct, and rthen came to dominate the entire community5.4.8.3.2forest dynamics125to simulate the growth of northeastern forests, a stochastic and mechanistic model known as sortie has been developed to follow the fates of individual trees and theiroffspring. based on speciesspecific information on growth rates, fecundity, mortality, and seed dispersal distances, as well as detailed, spatially explicit information about local light regimes, sortiefollows tens of thousands of trees to generate dynamic maps of distributions of nine dominant orsubdominant species of tree that look like real forests and match data observed in real forests atappropriate levels of spatial resolution. sortie predicts realistic forest responses to disturbances (e.g.,small circles within the forest boundaries within which all trees are destroyed), clearcuts (i.e., largedisturbances), and increased tree mortality.sortie consists of two units that account for local light availability and species life history for eachof nine tree species. local light availability refers to the availability of light at each individual tree. thisis a function of all of the neighboring trees that shade the tree in question. information on the spatialrelations among these neighboring tree crowns is combined with the movement of the sun throughoutthe growing season to determine the total, seasonally averaged light expressed as a percentage of fullsun. in other words, the growth of any given tree depends on the growth of all neighboring trees.the species life history (available for each of nine tree species) provides the relationship betweenradial growth rates as a function of its local light environment and is based on empirically estimatedlifehistory information. radial growth predicts height growth, canopy width, and canopy depth inaccordance with estimated allometric relations. fecundity is estimated as an increasing power functionof tree size, and seeds are dispersed stochastically according to a relation whereby the probability of123b. kerr, m.a. riley, m.w. feldman, and b.j. bohannan, òlocal dispersal promotes biodiversity in a reallife game of rockpaperscissors,ó nature 418(6894):171174, 2002.124r. durrett and s. levin, òallelopathy in spatially distributed populations,ó journal of theoretical biology 185(2):165171, 1997.125section 5.4.8.3.2 is based largely on d.h. deutschman, s.a. levin, c. devine, and l.a. buttel, òscaling from trees to forests:analysis of a complex simulation model,ó science online supplement to science 277(5332), 1997, available at http://www.sciencemag.org/content/vol277/issue5332. science online article available at http://www.sciencemag.org/feature/data/deutschman/home.htm.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.202catalyzing inquirydispersal declines with distance. mortality risk is also stochastic and has two elements: random mortality and mortality associated with suppressed growth.because sortie is intended to aggregate statistical properties of forests, an ensemble of simulation runsis necessary, in which different degrees of smoothing and aggregation are used to determine how muchinformation is lost by averaging and to find out where error is compressed and where it is enlarged in thecourse of this process. sortie is a computationintensive simulation even for individual simulations, because multiple runs are needed to generate the necessary ensembles for statistical analysis. in addition,simulations carried out for heterogeneous environments require an interface between large dynamic simulations and geographic information systems, providing realtime feedbacks between the two.5.5technical challenges related to modelinga number of obstacles and difficulties must be overcome if modeling is to be made useful to lifescientists more broadly than is the case today. the development of a sophisticated computational modelrequires both a conceptual foundation and implementation. challenges related to conceptual foundations can be regarded as mathematical and analytical; challenges related to implementation can beregarded as computational or, more precisely, as related to computer science (box 5.24).todayõs mathematical tools for modeling are limited. nonlinear dynamics and bifurcation theoryprovide some of the most welldeveloped applied mathematical techniques and offer great successes inilluminating simple nonlinear systems of differential equations. but they are inadequate in many situations, as illustrated by the fact that understanding global stability in systems larger than four equationsis prohibitively hard, if not unrealistic. visualization of highdimensional dynamics is still problematicin computational as well as analytical frameworks; the question remains as to how to represent suchcomplex dynamics in the best, most easily understood ways. moreover, many highdimensional systems have effectively lowdimensional dynamics. a challenge is to extract the dynamical behavior fromthe equations without first knowing what the lowdimensional subspace is. box 5.25 describes one newand promising approach to dealing with highdimensional multiscale problems.other mathematical methods and new theory will be needed to find solutions that apply not only tobiological problems, but to other scientific and engineering applications as well. these include methodsfor global optimization and for reverse engineering of structure (of any òblack box,ó be it a network ofgenes, a signal transduction pathway, or a neuronal system) based on data elicited in response tostimuli and perturbations.identification of model structure and parameters in nonlinear systems is also nontrivial. this isespecially true in biological systems due to incomplete knowledge and essentially limitless types ofinteractions. decomposition of complex systems into simpler subsystems (òmodulesó) is an importantchallenge to our ability to analyze and understand such systems (a point discussed in chapter 6).development of frameworks to incorporate moving boundaries and changing geometries or shapes isessential to describing biological systems. this is traditionally a difficult area. ideally, it would bedesirable to be able to synthesize and analyze models that have nonlinear deterministic as well asstochastic elements, and continuous as well as discrete, algebraic constraints, with other more traditional nonlinear dynamics. (see section 5.3.2 for greater detail.) all of these can be viewed as challengesin nonlinear dynamics aspects of modeling.further developing both computational (numerical simulation) methods and analytical methods(bifurcation, perturbation methods, asymptotic analysis) for large nonlinear systems will invariablymean great progress in the ability to build more elaborate and detailed models. however, with theselarge models come large challenges. one is how to find methodical ways of organizing parameter spaceexploration for systems that have numerous parameters. another is the development of ways to codifyand track assumptions that have gone into the construction of a model. understanding these assumptions (or simplifications) is essential to understanding the limitations of a model and when its predictions are no longer biologically relevant.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.computational modeling and simulation as enablers for biological discovery203box 5.24modeling challenges for computer scienceintegration methods¥methods for integrating dissimilar mathematical models into complex and integrated overall models¥tools for semantic interoperabilitymodels¥highperformance, scalable algorithms for network analyses and cell modeling¥methods to propagate measures of confidence from diverse data sources to complex modelsvalidation¥robust model and simulationvalidation techniques (e.g., sensitivity analyses of systems with huge numbers of parameters, integration of model scales)¥methods for assessing the accuracy of genomeannotation systemssource: u.s. department of energy, report on the computer science workshop for the genomes to life program, gaithersburg, md,march 67, 2002, available at http://doegenomestolife.org/compbio/.box 5.25equationfree multiscale computation:enabling microscopic simulators to perform systemlevel tasksyannis kevrikides of princeton university and his colleagues have developed a framework for computeraidedmultiscale analysis. this framework enables models at a òfineó (microscopic, stochastic) level of description toperform modeling tasks at a òcoarseó (macroscopic, systems) level. these macroscopic modeling tasks, yieldinginformation over long time and large space scales, are accomplished through appropriately initialized calls to themicroscopic simulator for only short times and small spatial domains: òpatchesó in macroscopic spacetime.in general, traditional modeling approaches require the derivation of macroscopic equations that govern thetime evolution of a system. with these equations in hand (usually partial differential equations (pdes)), a varietyof analytical and numerical techniques for their solution is available. the framework of kevrikides and colleagues, known as the equationfree (ef) approach can, when successful, bypass the derivation of the macroscopic evolution equations when these equations conceptually exist but are not available in closed form.the advantage of this approach is that the longterm behavior of the system bypasses the computationallyintensive calculations needed to solve the pdes that describe the system. that is, the ef approach enables analternative description of the physics underlying the system at the microscopic scale (i.e., its behavior onrelatively short time and space scales) provide information about the behavior of the system over relativelylarge time and space scales directly without expensive computations. in effect, the ef approach constitutes asystems identificationbased, òclosure on demandó computational toolkit, bridging microscopicstochasticsimulation with traditional continuum scientific computation and numerical analysis.source: the ef approach was first introduced by yannis kevrikides and colleagues in k. theodoropoulos et al., òcoarse stability andbifurcation analysis using timesteppers: a reaction diffusion example,ó proceedings of the national academy of sciences 97:9840, 2000,available at http://www.pnas.org/cgi/reprint/97/18/9840.pdf. the text of this box is based on excerpts from an abstract describing a presentation by kevrikides on april 16, 2003, to the singaporemit alliance program on high performance computation for engineered systems(hpces); abstract available at http://web.mit.edu/sma/events/seminar/kevrekidis.htm.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.204catalyzing inquiryin the second category, issues related to implementing the model arise. often such issues involvethe actual code used to implement the model. computational models are, in essence, large computerprograms; issues of software development come to the fore. as the desire for and utility of computational modeling increase, the needs for software are growing rather than diminishing as hardwarebecomes more capable. on the other hand, progress in software development and engineering over thelast several decades has not been nearly as dramatic as progress in hardware capability, and thereappears to be no magic bullets on the horizon that will revolutionize the software development process.this is not to say that good software engineering does not or should not play a role in the developmentof computational models. indeed, the biomedical information science and technology initiative (bisti)planning workshop of january 1516, 2003, explicitly recommended that nih require grant applications,proposing research in bioinformatics or computational biology to adopt as appropriate, accepted practicesof software engineering.126 section 4.5 describes some of the elements of good software engineering in thecontext of tool development, and the same considerations apply to model development.a second important challenge as large simulation models become more prevalent is a standardspecification language to unambiguously specify the model, its parameters, annotations, and even themeans by which it is to be scored against data. the challenge will be to provide a language flexibleenough to capture all interesting biological processes and incorporate models at different levels ofabstraction and in different mathematical paradigms, including stochastic differential, partial differential, algebraic, and discrete equations. it may prove necessary to develop a set of nested languagesñforexample, a language that specifies the biological process at a very high level and a linked language thatspecifies the mathematical representation of each process. there are some current attempts at theselanguages based on the xml framework. sbml and cellml are attempts in this direction.finally, many biological modeling applications involve a problem space that is not well understoodand may even be intended to explore queries that are not well formulated. thus, there is a highpremium on reducing the labor and time involved to produce an application that does somethinguseful. in this context, technologies for òrapid prototypingó of biological models have considerableinterest.127126see http://www.bisti.nih.gov/2003meeting/report.cfm.127note, however, that in the rapid prototyping process often used to create commercial applications, there is a dialoguebetween developer and user that reveals what the user would find valuable: once the developer knows what the user reallywants, the software development effort is straightforward. by contrast, in biological applications, it is nature that determines theappropriate structuring and formulation of a problem, and a problem cannot be structured in a certain way simply because it isconvenient to do so. thus, technologies for the rapid prototyping of biological models must afford the ability to rearrange modelcomponents and connections between components with ease.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.a computational and engineering view of biology2052056a computational and engineering view of biologybecause 21st century biology is very concerned with function, it is helpful to have abstractionsavailable that characterize the functionality of interest. by doing so, insights derived from study of thoseabstractions in other contexts become available for biological use. in addition, because biological systems are the products of eons of evolutionary history and decision making, viewing them through thelens of engineering yields insights that are not otherwise available from an analysis that might be basedon first principles.6.1 biological information processing1as noted in chapter 2, biological systems are extraordinarily complexñand partly as a consequence, poorly understood. yet it is clear that biological systems demonstrate and exemplify functionality at different levels.artifacts such as computer hardware and software also exhibit functionality and multiple levels. tofacilitate the understanding and construction of such artifacts, computer science has developed information abstractions that seek to capture and encapsulate certain kinds of functional behavior in manipulating and managing information; such abstractions are a primary focus of study of the computerscientist (box 6.1).one key connection to 21st century biology is that many biological problems now require thesimultaneous consideration of phenomena at different scales. for example, biologists can think ofgenetics at the level of individual nucleotides, at the level of chromosomes, at the level of genomes, andat the level of populations. from nucleotide to population is a span of many orders of magnitude, andit is difficult to conceptualize such a range without moving seamlessly between different levels ofabstraction.section 6.1 describes several such abstractions and their specific biological applications already inuse, but the description is not intended to be exhaustive, and there are likely many more such abstractions capable of providing biological insight, including new or as yet undiscovered techniques orconcepts. as such, this area represents opportunities for both biologists and computer scientists.1much of the discussion in section 6.1 about cells as informationprocessing devices is adapted from r. aviv and e. shapiro,òcellular abstractions: cells as computation,ó nature 419:343, 2002.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.206catalyzing inquiryconsider that biological processes, such as catalysis, protein synthesis, and other metabolic systems, are consumers, processors, or creators of information. as loewenstein puts it, in biological systems, òin addition to flows of matter and energy, there is also flow of information. biological systemsare informationprocessing systems and this must be an essential part of any theory we may construct.ó2 sydney brenner goes farther, arguing that ò. . . this information flow, not energy per se, is theprime mover of lifeñthat molecular information flowing in circles brings forth the organization we callôorganismõ and maintains it against the everpresent disorganizing pressures in the physics universe.so viewed, the information circle becomes the unit of life.ó3the current state of intellectual affairs with respect to biological information and complexity mayhave some historical analogy with the concept of energy at the beginning of the 19th century. althoughthe concept was intuitively obvious, it was not formally defined or measured at that time. carnotõsanalysis of the performance of steam engines formalized the meaning of energy, creating the basis forbox 6.1on the abstractions of the computer scientist and engineerabstraction is a generic technique that allows the scientist or engineer to focus only on certain features of asystem while hiding others. scientists in all disciplines typically use abstractions as a way to simplify calculations for purposes of analysis, but computer scientists also use abstractions for purposes of design: to buildworking computer systems. because building systems is the central focus of much work in computer science,the use of abstractions to cope with complexity over a wide range of scale, size, and levels of detail is centralto a computer scientistõs way of thinking.the focus of the computer scientist in creating an abstraction is to hide the complexity of operation òunderneaththe abstractionó while offering a simple and useful set of services òon top of it.ó using such abstractions is theprincipal technique for organizing and constructing very sophisticated computer systems, and they enable computer scientists to deal with large differences of scale. for example, one particularly useful abstraction useshardware, system software, and application software as successive layers on which useful computer systems canbe built. this illustrates one very important use of abstraction in computer systems: each layer provides thecapability to specify that a certain task be carried out without specifying how it should be carried out. in general,computing artifacts embody many different abstractions that capture many different levels of detail.a good abstraction is one that captures the important features of an artifact and allows the user to ignore theirrelevant ones. (the features decided to be important collectively constitute the interface of the artifact to theoutside world.) by hiding details, an abstraction can make working with an artifact easier and less subject toerror. but hiding details is not costfreeñin a particular programming problem, access to a hidden detail mightin fact be quite helpful to the person who will use that abstraction. thus, deciding how to construct an abstraction (i.e., deciding what is important or irrelevant) is one of the most challenging intellectual issues in computerscience. a second challenging issue is how to manage all of the details that are hidden. the fact that they arehidden beneath the interface does not mean that they are irrelevant, only that the computer scientist must designand implement approaches to handle these details òautomaticallyó (i.e., without external specification).source: adapted from computer science and telecommunications board, national research council, computing the future: a broaderagenda for computer science and engineering, national academy press, washington, d.c., 1991.2w. loewenstein, the touchstone of life: molecular information, cell communication, and the foundations of life, oxford universitypress, new york, 1998, p. xiv.3s. brenner, òtheoretical biology in the third millennium,ó philosophical transactions of the royal society b 354(1392):19631965,1999.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.a computational and engineering view of biology207the science of thermodynamics. only after energy had been identified and studied in the artificial realmof steam engines was it recognized as a prime aspect of natural systems as well.similarly, the existing state of the theory of biological information (or, indeed, information of anysort) is based on the work of claude shannon, who studied the processing of information in humantechnological channels of communication, and the field of computational complexity, which was created to analyze the performance characteristics of algorithms running on humanbuilt computers. however, just as thermodynamics successfully widened its scope to the natural world from steam engines,information and computation theory may become a powerful lens for describing, measuring, andunderstanding processes in the natural world.biological information is likely to have a close relationship to information in the shannon sense ofthe term, if only because biological entities depend on information to coordinate their internal activity.cells coordinate their internal activity because they have harnessed intracellular shannon informationchannels. multicellular organisms coordinate their internal activity because they have harnessed intercellular shannon information channels. these channels are the conduits through which genes transfertheir information content to proteins, proteins serve as signaling agents, and nervous systems work.also, shannonõs insight about the nature of information transmission allows us to understand howsignals can reliably be sent through a noisy unpredictable environment (whether cell telephone signals,internet packets, or hormone signaling proteins) and received accurately at the other end.on the other hand, shannon information applies in the strict sense only when it is possible toidentify a sender and receiver connected by a channel. there are some places in which this applies, suchas the projection of the retina to the brain. yet in the context of information feedback and loops ratherthan channels, it is not clear that shannon information continues to have a welldefined meaning.there have been a number of attempts to generalize shannon information to problems at thecellular and subcellular levels, of which the conceptualization by manfred eigen of hypercycles, quasispecies, and sequence space is one of the most notable.4 but whether these concepts are the right onesis not as important as the recognition that new concepts are needed.a more specific connection between biology and computation can be seen in the biological use ofinformation to enhance the survival and reproductive functions of an organism. that is, biologicalorganisms use information about the environment to stimulate or drive responses that boost the likelihood of survival and successful reproduction. this process is effectively a computation that transformsthe inputs (which describe environmental conditions) into the appropriate outputs (the organismõsbehavior).5 for example, hartwell et al. note that signals from the environment entrain circadian biological clocks to produce responses to predicted fluctuations in light intensity and temperature.6embedded within cells are complex signaling mechanisms that transfer information from one partof a cell to another and intercellular mechanisms that transfer information from one part of a multicellular organism to another. indeed, signal transduction pathwaysñand the proteins associated withthemñappear to serve the functions of information processing and transfer,7 rather than those of moreòtraditionaló biology (e.g., chemical transformation of metabolic intermediates or the building of cellular structures).4m. eigen, òthe origin of biological information,ó presented at the seventh international conference on intelligent systemsfor molecular biology, august 610, 1999; heidelberg, germany, available at http://bioinf.mpisb.mpg.de/conferences/ismb99/www/abstracts/abseigen.html.5indeed, it has been asserted that the history of life can be described as the evolution of systems that manipulate one set ofsymbols representing inputs into another set of symbols that represent outputs. j.j. hopfield, òphysics, computation, and whybiology looks so different,ó journal of theoretical biology 171:5360, 1994.6l.h. hartwell, j.j. hopfield, s. leibler, and a.w. murray, òfrom molecular to modular cell biology,ó nature 402(6761suppl):c47c52, 1999.7d. bray, òprotein molecules as computational elements in living cells,ó nature 376(6538):307312, 1995. the examples in thenext paragraph are also brayõs.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.208catalyzing inquiryfor example, a simple enzyme protein could be viewed as a computational element that takes aninputñthe concentration of its òsubstrate,ó the molecule with which it interactsñand produces anoutput: a concentration of the catalyzed reaction product. an enzyme that becomes active only when itbinds to two separate regulator molecules will function something like a boolean and gate, and so on.circuits formed from these elements can be as simple as a switch or an oscillator, or as complex as todrive a bacteriumõs chemotaxis response. indeed, the cell even possesses a kind of shortterm, òrandomaccessó memory, in the sense that events in its environment have profoundly shaped the concentrationand activity of many thousands of molecules in the cell. in short, these proteinbased circuits constitutea kind of nervous system for the cell, providing it with much of what it needs to control its behavior.box 6.2 provides some additional perspective on this subject.additional insights can be gained from the notion that both computational processes and biologicalpathways can be viewed as processes that affect the state of a system according to welldefined (thoughpossibly probabilistic) rules. thus, it is possible to describe regulatory, metabolic, and signaling pathways, as well as multicellular processes such as immune responses, as systems of interacting computations operating in parallel. in particular, languages such as petrinets, statecharts (discussed in section4.3.1), and the picalculus, originally developed for the specification and study of systems of interactingcomputations, can be used to represent such systems.8 such representations enable researchers tosimulate their behavior, and to support qualitative and quantitative reasoning on the properties of thesesystems.to cite two prominent researchers in this area:processes, the basic interacting computational entities of these languages, have an internal state andinteraction capabilities. process behavior is governed by reaction rules specifying the response to an inputmessage based on its content and the state of the process. the response can include state change, a changein interaction capabilities, and/or sending messages. complex entities are described hierarchicallyñforexample, if a and b are abstractions of two molecular domains of a single molecule, then (a parallel b) isan abstraction of the corresponding twodomain molecule. similarly, if a and b are abstractions of the twopossible behaviors of a molecule in one of two conformational states, depending on the ligand it binds,then (a choice b) is an abstraction of the molecule, with the choice between a and b determined by itsinteraction with a ligand process.9abstractions of the cell as a computing or informationprocessing device allow one to distinguishbetween two conceptual levels: a òlowleveló view that focuses on implementation (i.e., how the systemis builtñwhere the wires go or the detailed molecular processes involved) and a òhighleveló view thatfocuses on functionality (what the system doesñanalogous to a logic gate or a computational device).10for example, one might distinguish between the pathways involved in regulating the circadian rhythmof an organism and its functional behavior as an oscillator.the difference between these levels of abstraction enables biologically significant comparisons to bemade. for example, it would be instructive if two different organisms implemented the same functionin different ways. in other words, functional equivalence between related implementations in differentorganisms could be regarded as a measure of the behavioral similarity of entire systems. (in the literature of evolutionary biology, the implementation of the same function in different ways is called òanalogousó implementation.) perhaps more importantly, a functional perspective is an enabler for the integration of knowledge about the function, activity, and interaction of cellular molecular systems.8r. aviv and e. shapiro, òcellular abstractions: cells as computation,ó nature 419:343, 2002.9r. aviv and e. shapiro, òcellular abstractions,ó 2002.10in many circumstances, different parts of a biological system may play different roles at different times or even differentroles at different time scales at the same time. this is especially true in splicing variants, where the expression of a gene mayproduce proteins with quite different functions according to the behavior of the splicing mechanism. indeed, in some cases,different splicings have opposite functions. nevertheless, in understanding a given role at a given time and time scale, the highlevel abstraction focused on functionality is meaningful and scientifically significant.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.a computational and engineering view of biology209this perspective on cells as computational devices should not be taken as an argument that cellsprocess information the way a digital computer does. the organizations are radically different. to namejust a few differences, in a cell there is no clean separation between the data store and the centralprocessing unit: the cellõs memory is the same protein reaction network that does its processing. realproteins rarely respond or act in a completely binary fashionñthe levels of concentration matter. apartfrom dna, few portions of a cellõs internal machinery are explicitly digital in natureñwith the resultthat signaling in a cell must take place in a highly noisy environment.box 6.2role of computation in complex regulatory networkscomputation . . . [is] a crucial ingredient when dealing with the description of biocomplexity and its evolution,because it turns out to be much more relevant than the underlying physics. its dynamics is governed mainly by thetransmission, storage and manipulation of information, a process which is highly nonlinear. this nonlinearity is wellillustrated by the nature of signaling in cells: local events involving a few molecules can produce a propagatingcascade of signals through the whole system to yield a global response. . . . if we try to make predictions about theoutcomes of these signaling events in general, we are faced with the inherent unpredictability of computationalsystems. it is at this level where computation becomes central and where idealized models of regulatory networksseem appropriate enough to capture the essential features at the global scale.cells are probably the most complete example of this traffic of signals at all levels. . . . the cellular network can bedivided into three major selfregulated subwebs:¥the genome, in which genes can affect each otherõs level of expression;¥the proteome, defined by the set of proteins and their interactions by physical contact; and¥the metabolic network (or the metabolome), integrated by all metabolites and the pathways that link each other.all these subnetworks are very much intertwined since, for instance, genes can only affect other genes throughspecial proteins, and some metabolic pathways, regulated by proteins themselves, may be the very ones to catalyzethe formation of nucleotides, in turn affecting the process of translation. . . . it is not difficult to appreciate theenormous complexity that these networks can achieve in multicellular organisms, where large genomes have structural genes associated with at least one regulatory element and each regulatory element integrates the activity of atleast two other genes. . . .luckily, all this extraordinary complexity can be abstracted, at least at some levels, to simplified models which canhelp in the study of the innerworkings of cellular networks. overall, irrespective of the particular details, biologicalsystems show a common pattern: some lowlevel units produce complex, highlevel dynamics coordinating theiractivity through local interactions. thus, despite the many forms of interaction found at the cellular level, all comedown to a single fact: the state of the elements in the system is a function of the state of the other elements it interactswith. what models of network functioning try, therefore, is to understand the basic properties of general systemscomposed of units whose interactions are governed by nonlinear functions. these models, being simplifications, donot allow one to make predictions at the level of the precise state of particular units. their average overall behavior,however, can shed light into the way real cells behave as a system. . . .. . . [m]any entities in cellular networks can be identified as the basic units of regulation, mainly distinguished bytheir unique roles with respect to interaction with other units. these basic units are genes, each of the proteins thatthe genes can produce, each of the forms of a protein, protein complexes, and all related metabolites. these unitshave associated values that either represent concentrations or levels of activation. their values depend on the valuesof the units that affect them due to the mechanisms discussed, plus some parameters that govern each special formof interaction. . . . computer modeling of [the] network [the segment polarity network of drosophila melanogaster]has provided insight into various questions. a very important result is the fact that this network seems to be aconserved module. evidence for this has been obtained by simulations demonstrating its robustness against thechange of parameters. . . .source: reprinted from p. fernandez and r.v. sole, òthe role of computation in complex regulatory networks,ó santa fe instituteworking paper, 2003, available at http://www.santafe.edu/sfi/publications/workingpapers/0310055.pdf; to appear in a chapter inpower laws, scalefree networks and genome biology, landes bioscience. reprinted with permission.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.210catalyzing inquiryit is also interesting that biological function often relies on what might be called exploration withselectionñthe production of many intermediate products resulting from stochastic subprocesses thatare then refined to unique and appropriate solutions.11 taken across the entire population, exploration with selection exploits the difference between creating a solution and testing a solution forcorrectnessñthe first being in general a much more difficult computational task than the second.12random processes are used to explore the space of possible solutions,13 and other machinery cullsthese possible solutions. as hartwell et al. argue, òsimilar messy and probabilistic intermediatesappear in engineering systems based on artificial neural networksñmathematical characterizationsof information processing that are directly inspired by biology. a neural network can usefully describe complicated deterministic inputoutput relationships, even though the intermediate calculations through which it proceeds lack any obvious meaning and their choice depends on random noisein a training process.ó146.2 an engineering perspective on biological organisms6.2.1 biological organisms as engineered entitiesengineering insights can be useful in understanding biological organisms as engineered entities,and the rationale for seeking insights from engineering is based on three notions. first, although thephysical scales may differ in some cases, human technology and natural systems operate in the sameworld and must obey the same physical rules. knowledge that engineering fields have accumulatedabout what techniques work and the limits of those techniques can serve as a potentially valuable guidein investigating the physical basis of the operations of natural systems. this is especially true forbiomechanical feats, such as structural support, locomotion, circulation, and so on.the second rationale is that because evolution and a long history of environmental accidents havedriven processes of natural selection, biological systems are more properly regarded as engineeredartifacts than as objects whose existence might be predicted on the basis of the first principles of physics,although the evolutionary context means that an artifact is never òfinishedó and is rather evaluated ona continuous basis.15 both engineered artifacts and biological organisms demonstrate function, embody11for example, the immune system relies on the random generation of pathogen detectors, which are then eliminated whenthey match some definition of òself.ó in single molecules, kinetic funnels direct different molecules of the same protein throughmultiple, different paths from the denatured state to a unique folded structure (k.a. dill and h.s. chan, òfrom levinthal topathways to funnels,ó nature structural biology 4:1019, 1997). within cells, the shape of the mitotic spindle is due partly toselective stabilization of randomly generated microtubules whose ends happen to be close to a chromosome (r. heald, r.tournebize, t. blank, r. sandaltzopoulos, p. becker, a. hyman, and e. karsenti, òselforganization of microtubules into bipolarspindles around artificial chromosomes in xenopus egg extracts,ó nature 382(6590):420425, 1996). within the brain, the patterning of the nervous system is refined by the death of nerve cells and the decay of synapses that fail to connect to an appropriate target.12this point can be formalized in the language of theoretical computer science. see j. hartmanis, òcomputational complexityand mathematical proofs,ó pp. 251256 in informatics: 10 years back, 10 years ahead, 2000, lecture notes in computer science,springerverlag, berlin, heidelberg, 2001.13for example, random processes are at the heart of stochastic optimization methods that can be used for protein structureprediction and receptor ligand docking, including simulated annealing, basin hopping, and parallel tempering. (an interestingintroduction to stochastic optimization methods can be found at w. wenzel, òstochastic optimization methods,ó available athttp://iwrwww1.fzk.de/biostruct/opti/opti.htm.) also, the systematic exploration of ecological models discussed in section5.4.8 is also based on the use of random processes.14the quote is taken from l.h. hartwell, j.j. hopfield, s. leibler, and a.w. murray, òfrom molecular to modular cell biology,ó nature 402(6761 suppl.):c47c52, 1999. hartwell et al. credit sejnowski and rosenberg with the neural network example(t.j. sejnowski and c.r. rosenberg, òparallel networks that learn to pronounce english text,ó complex systems 1:145168, 1987).15a classic paper on this subject is f. jacob, òevolution and tinkering,ó science 196(4295):11611166, 1977.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.a computational and engineering view of biology211behavior, and manifest an evolutionary history.16 engineered artifacts serve the purposes of theirhuman designers, and biological organisms serve the purposes of natureñthat is, to survive and reproduce.17 thus, the concepts needed to understand biological function may have some resemblance tosome of the concepts already developed for òsyntheticó disciplines, of which engineering and computerscience are prime examples.a third rationale is that the engineering disciplines have already had a long history of systemslevelthinking and, indeed, have produced artifacts that are arguably approaching biological levels of complexity. for example, a boeing 777 jetliner contains about 150,000 subsystem modules, including 1,000computers, a number of the same order of magnitude as the estimated 300,000 different proteins in atypical human cell. just as in the cell, moreover, these aeronautical subsystems are linked into animmensely complex ònetwork of networksóña control system that just happens to fly.18a related point, and a key lesson from engineering, is that large systems are built out of smallersystems that are stable. decomposition of a complex structure into an assembly of simpler structureswhose operation is coordinated tends to be a much more successful strategy that building the complexstructure from scratch, and this approach can be seen in the structure of the cell. consider that a humancell has many physical structures within itñnucleus, mitochondria, and so on; each of these can beregarded as a device, many of which compose the cell. further, many and perhaps even most cellularfunctions (e.g., genetic regulatory networks, metabolic pathways, signaling cascades) are implementedin a manner that is highly robust against singlepoint failure (i.e., the function will continue to operateproperly even when one element is missing). section 6.2.3 addresses this point in more detail.a second view of biological organisms as engineered entitiesñas novel entities to be constructed byhuman beings rather than as existing organisms to be understood by human beingsñis discussed insection 8.4.2 on synthetic biology.6.2.2 biology as reverse engineeringbiological organisms are generally presented to scientists as completed entities, so the challenge ofachieving an engineering understanding of them is in fact a challenge of reverse engineering. one definition of reverse engineering is òthe process of analyzing a subject system with two goals in mind: (1) to16while it is generally recognized that biology and evolution are intimately linked, the analogous connection between engineering and evolution is less well understood. nevertheless, most humanengineered objects have a lot of historicity in them aswell. most human objects are designs based as improvements on previous designs, not de novo, and this can complicate theunderstanding of the relationship between functionality and design of a human artifact. one reason is a desire for backwardcompatibilityñconsider the fact that twoprong electric plugs and sockets are much more hazardous than some alternativedesigns and yet they are ubiquitous in appliances today. the same is true for operating systemsñlater versions of an operatingsystem often incorporate large amounts of code from previous versions to facilitate backward compatibility. a second reason isthat previous designs may have solved a design problem in a particularly effective way, and these solutions from the past areignored today at the designerõs peril. for example, consider the evolution of the rotary phone into todayõs pushbutton phones.donald norman observes that the cradle of the phone handset and the buttonswitch in it had two distinct functions: the cradleprovided a place for the user to put the phone and the buttonswitch turned the phone on and off. norman notes that whetherdeliberately or by accident, the particular design of the rotary phone that placed the onoff switch in a protected spot in thecradle also protected the onoff switch from the user accidentally hanging up the phone. however, the designers of newer pushbutton phones did not pick up on that feature; many pushbutton phones are designed so that the onoff switch and the hangupcradle are separateñthus making the onoff switch much easier to bump and thereby to accidentally disconnect a phone call. seed. norman, the design of everyday things, basic books, new york, 1998.17see for example l.h. hartwell, j.j. hopfield, s. leibler, and a.w. muray, òfrom molecular to modular cell biology,ó nature402(6761 suppl):c4752, 1999, available at http://cgr.harvard.edu/publications/modular.pdf. hartwell et al. further argue thatit is notions of function and purpose that differentiate biology from other natural sciences such as chemistry or physics, andhence that reductionist biologyñinquiry that seeks to explain biological phenomena only in chemical or physical termsñisinherently incomplete.18m.e. csete and j.c. doyle, òreverse engineering of biological complexity,ó science 295(5560):16641669, 2002, available athttp://www.sciencemag.org/cgi/content/abstract/295/5560/1664.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.212catalyzing inquiryidentify the systemõs components and their interrelationships and (2) to create representations of thesystem in another form or at a higher level of abstraction.ó19a better description could not be developed for the goal of systems biology, even without having tochange any words in this definition. and yet reverse engineering, despite being a fairly standardengineering topic, is not taught to biologists.20 one drawback is that the metaphor itself is foreign tobiologists; if they wanted to do engineering of any kind, they would have been engineers. second,reverse engineering is generally a more difficult task than forward engineering (i.e., the fabrication of adevice to implement some specific functionality), and reverse engineering of a biological organism is aparticularly difficult endeavor.one important reason is that reverse engineering is often underdetermined, in the sense that multiple solutions can be developed to account for a given behavior. in such cases, choosing among themthus requires either more data or a priori assumptions about the true nature of the system being reverseengineered. for example, in dealing with the reverseengineering task of building detailed kineticmodels of intracellular processes from timeseries data, rice and stolovitzky note that assumptionssuch as linearity or sparseness or the use of predetermined model structures (e.g., reactions limited inthe number of possible reactants and substrates) can help to reduce the nonuniqueness.21a second and even more important reason for the difficulty of reverse engineering is that because oftheir evolutionary history, the organisms of interest are constructed in a highly nonoptimal manner.when engineers seek to understand how an artifact has been constructed, the basic question they ask is,why? why is this structure here? why was that material used? by asking such questions of a humanengineered artifact, the engineer can often divine a reason that answers them. the reason is that engineers can be expected to design artifacts using principles such as modularity and separation of function(i.e., to minimize unnecessary links between subsystems with different purposes). these principlesguard human designs against unforeseen side effects that would arise if components were not deliberately assembled in such a way as to minimize undesired or unanticipated interactions.however, the same is not true of biological organisms. in many cases, the only answer for biologicalsystems is, òthatõs the way it was built.ó nature builds from accidents that happen to work and createsnew mechanisms on top of old ones. while some evolved systems are quite elegant (e.g., the sensory andthe motor components of the escherichia coli chemotaxis mechanism), many if not most such systems atleast appear to a human as inelegant, redundant, òkludgy,ó and inefficientñsome of them extremely so.systems engineered by humans, even very poorly engineered ones and even though they too often showtheir historical origins, are seldom if ever as arcane and kludgy as evolved biological organisms.finally, it is helpful to distinguish between two different approaches to reverse engineering. oneapproach to reverse engineering of biological systemsña òtopdownó approachñbegins with its observable behavior and characteristics, and seeks to decompose the system into components or subsystems that collectively exhibit the macroscopic behavior in question. that is, the topdown approachis based on a successive decomposition down to the systemõs most elemental components.a second approach is based on a òbottomupó approach, which begins with an understanding of theconstituent parts at the lowest level, e.g., the macromolecules and the genetic regulatory networks of the19e.j. chikofsky and j.h. cross, òreverse engineering and design recovery: a taxonomy,ó ieee software 1317, 1990.20indeed, the bio2010 report on undergraduate education in biology (national research council, bio 2010: undergraduateeducation to prepare biomedical research scientists, national academies press, washington, dc, 2003) noted that òone approach tothe study of biology is as a problem in reverse engineering. manufactured systems are easier to understand than biologicalsystems, because they have no unknown components, and their design principles can be explicitly stated. it is easiest to learnhow to analyze systems through investigating how manufactured systems achieve their designed purpose, how their functiondepends on properties of their components, and how function can be reliable even with imperfect components.ó also, underscoring the point that engineering is not a part of biology education today, the report emphasized the importance of exposingbiology students to engineering principles and analysis in the course of their undergraduate educations. chapter 10 has morediscussion of this point.21j.j. rice and g. stolovitzky, òmaking the most of it: pathway reconstruction and integrative simulation using the data athand,ó biosilico 2(2):7077, 2004.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.a computational and engineering view of biology213cells that make up the system. the philosophical notion embedded in the bottomup approach is that acomponent is likely to be easier to understand than the system in which it is embedded. by successiveassembly of component parts, one is able to create everlarger assemblies whose operation is understood.both approaches seek as their underlying ultimate goal an understanding of how a biologicalsystem works in all of its complexity. but they require different strategies for acquiring data at differentlevels of scale (topdown entails data acquisition at eversmaller scales, while bottomup entails dataacquisition at everlarger scales). and also, it should be expected that they will generate differentintermediate outputs and products along the way to this ultimate goal.6.2.3 modularity in biological entities22a functional perspective on biology is centrally based on the notion that biological function isseparable, into what might be called modules. the essence of a moduleñwell known in engineeringdisciplines as well as computer scienceñis that of an entity whose function is separable from othermodules. in the computer science context, a module might be a subroutine upon which various programs can build. these various programs would interact with the subroutine only through the programming interfaceñthe set of arguments to the subroutine that parameterize its behavior. box 6.3describes how the search for functional modules plays into systems biology.box 6.3functional modules in biologyan important theme in systems biology has been to look for functional modules that have been conserved andreused. the idea of breaking biological systems into small functional blocks has obvious appeal; the parts can bedivided and conquered so that the most complex of machines become readily understood in terms of block diagramsor sets of subroutines. clearly, some conserved modules exist such as the ribosome and the tricarboxylic acid cycle.one method to search for modules involves looking for higherorder structures or recurring subnetworks (oftentermed òmotifsó) in metabolic or gene regulatory networks. another approach mentioned earlier is clustering expression profiles to produce groups of genes that appear to be coregulated that should ideally reveal the functionalmodules. however, this assumption does not appear to generalize to all functional groups under all conditions, assome functional groups show wellcorrelated expression profiles whereas others do not. the low correlation of genesobserved within some functional groups has been attributed to the fact that some of these genes belong to multiplefunctional classes. in another analysis in e. coli, 99 cases were found where one reaction existed in multiple pathways in ecocyc. these observations suggest potential pitfalls with anticipating too much functional modularity interms of biology being neatly partitioned into nonoverlapping modules. moreover, the tissue or speciesspecificdifferences mentioned earlier may prevent simplistic transfer of modules from one biological system to another. itremains to be seen if biology is as modular as the system biologist might like it to be.biological modules may turn out be more interconnected and overlapping than independent in many systems. inaddition, the experiences with pathway reconstruction suggest that the combinations of data source produce a moreaccurate if not more complete characterization of the system under study. these observations point to an eventualneed to develop largescale, predictive models based on a multitude of data sources. for example, metabolic modelsmay combine data from many sources into a quantitative set of equations that can make predictions amenable toexperimental verification. in another system, cardiac models can bridge data at multiple levels (i.e. molecular,cellular, organ, etc.) and their corresponding characteristic timescales. in this system, modeling efforts at the singlecell level in the heart suggested a mechanism of increased contraction force that was later confirmed in experimentalstudies of whole heart.source: reprinted by permission from j.j. rice and g. stolovitzky, òmaking the most of it: pathway reconstruction and integrativesimulation using the data at hand,ó biosilico 2(2):7077. copyright 2004 elsevier.22section 6.2.3 is based largely on l.h. hartwell, j.j. hopfield, s. leibler, and a.w. murray, òfrom molecular to modular cellbiology,ó nature 402(6761 suppl.):c47c52, 1999.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.214catalyzing inquiryimportant insights into biological organisms can be gained by seeking to identify general principles thatgovern the structure and function of modules (box 6.4). in a biological context, a module might be an entitythat performs some biochemical function apart from other modules, isolated from those other modules byspatial localization (i.e., it is physically separated from those other modules) or by chemical specificity (i.e., itsbiochemical processes are sensitive only to the specific chemical signals of that module and not to others thatmay be present). furthermore, modules must be able to interact with each other selectively. specific connectivity enables module a to influence the functional behavior of module b, but not to affect the operation ofmodules c through z. also, the particular pattern of connectivity can account for some emergent propertiesof these modules, such as an ability to integrate information from multiple sources.as noted by hartwell et al., òhigherlevel functions can be built by connecting modules together.for example, the supermodule whose function is the accurate distribution of chromosomes to daughtercells at mitosis contains modules that assemble the mitotic spindle, a module that monitors chromosome alignment on the spindle, and a cellcycle oscillator that regulates transitions between interphaseand mitosis.ó when a function of a protein is restricted to one module, and the connections of thatmodule to other modules are through such proteins, it becomes much easier to alter connections toother modules without global consequences for the entire organism.modular structures have many advantages. for example, the imposition of modular design on anentity allows a module to be used repeatedly by different parts of the entity. furthermore, changesinternal to the module do not have global impact if those changes do not affect its functional behavior.modules can be combined and recombined in ways that alter the functionality of the complete systemñbox 6.4some mechanisms underlying the structure and function of modules1.positive feedback loops can drive rapid transitions between two different stable states of a system. for example,positive feedback drives cells rapidly into mitosis, and another makes the exit from mitosis a rapid and irreversibleevent.12.negative feedback loops can maintain an output parameter within a narrow range, despite widely fluctuatinginput. for example, negative feedback in bacterial chemotaxis2 allows the sensory system to detect subtle variationsin an input signal whose absolute size can vary by several orders of magnitude.3 (this topicñrobustness againstnoiseñis described in more detail in section 6.2.5.)3.coincidence detection systems require two or more events to occur simultaneously in order to activate an output.for example, coincidence detection is central in eukaryotic gene transcription, in which several different transcription factors must be present simultaneously at a promoter site before transcription can occur. (note the similarity toa multiinput and gate.)4.parallel circuits allow devices to survive failures in one of the circuits. for example, dna replication involvesproofreading by the dna polymerase backed up by a mismatch repair process that removes incorrect bases after thepolymerase has moved on. both of these must fail before a cell cannot produce viable progeny, and these twomechanisms, combined with a system for killing potentially cancerous cells, reduce the frequency at which individual cells give rise to cancer to about 1 in 1015.5.quality control systems monitor the output of many biological processes to ensure that the processes haveexecuted correctly. such systems can be seen in cellcycle checkpoints, dna replication and repair, choices between cell survival and death after insults to cells, or quality control in protein folding and/or sorting events.1d.o. morgan, òcyclindependent kinases: engines, clocks, and microprocessors,ó annual review of cell and developmental biology13:261291, 1997.2chemotaxis is the propensity of certain bacteria, such as e. coli, to swim toward higher concentrations of nutrients.3h.c. berg, òa physicist looks at bacterial chemotaxis,ó cold spring harbor symposium on quantitative biology 53(1):19, 1988.source: items 14 adapted from l. hartwell, j.j. hopfield, s. leibler, and a.w. murray, òfrom molecular to modular cell biology,ónature 402(suppl.):c47c52, 1999.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.a computational and engineering view of biology215the building blocks remain more or less stable, while the connectivity among them determines thecharacter of the system.if biological modules really do exist, one might expect to find them reused in different cellularcontexts, performing the same function but to different ends. understanding the function and behaviorof a cellular pathway would entail the discovery and characterization of such modular building blocks,tasks that should be simpler than trying to understand biological networks of different organisms as anirreducible whole.several independent pieces of evidence have emerged supporting the modularity hypothesis. forexample, evidence is accruing that certain regions of dna are òconservedó from one species to another.these regions may be associated with genes coding for proteins or with regulatory and structuralfunctionality. caenepeel et al. found that the human and mouse kinomes (i.e., the collection of proteinkinases in an organism) are 99 percent identical, although the percentage of identity between orthologues(i.e., genes or proteins from different organisms that have the same function) ranges from 70 percent to99 percent (with single nucleotide insertions or deletions in many cases).23 dermitzakis et al. found thatperhaps a third of the highly conserved dna regions between mouse and human code for proteins,while much of the rest probably codes for regulatory and structural functionality.24genetic expression networks may also display regular patterns of interconnections (motifs) recurring in many different parts of a network at frequencies much higher than those found in randomizednetworks.25 such motifs might be regarded as building blocks that can be used to assemble entities ofmore complex functionality.26 for example, shenorr et al. discovered a series of simple, recurringnetwork motifs in the gene interaction map of the bacterium e. coli.27 shortly afterwards, richardyoung and colleagues found the same motifs to recur at statistically surprising frequencies in yeast.28milo et al. found that these motifs were also overrepresented in a neuronal connectivity network ofcaenorhabditis elegans as well as the connectivity networks in the iscas89 benchmark set of sequentiallogic electronic circuits, but not in ecosystem food webs.29 milo et al. speculate that these motifs reflectthe underlying processes that generated each type of network, in this case one set of motifs for those thatprocess information (the genetic regulation, neuronal connectivity, and electronic logic networks) andanother set of motifs for those that process and carry energy.finally, a collaborative project led by eric davidson and his group at the california institute oftechnology, and involving bolouri and hood at the institute for systems biology, also suggests simpledesign principles and building blocks in genetic networks. figure 6.1 is a map of the interactions among23s. caenepeel, g. charydezak, s. sudarsanam, t. hunter, and g. manning, òthe mouse kinome: discovery and comparativegenomics of all mouse protein kinases,ó proceedings of the national academy of sciences 101(32):1170711712, 2004.24e.t. dermitzakis, a. reymond, r. lyle, n. scamuffa, c. ucla, s. deutsch, b.j. stevenson, et al., ònumerous potentiallyfunctional but nongenic conserved sequences on human chromosome 21,ó nature 420(6915):578582, 2002.25r. milo, s. shenor, s. itzkovitz, n. kashtan, d. chklovskii, and u. alon, ònetwork motifs: simple building blocks ofcomplex networks,ó science 298(5594):824827, 2002.26alon refines the notion of module as building block to suggest that modules and motifs are related but separate concepts. inalonõs view, a module in a network is a set of nodes that have strong interactions and a common function. some nodes areinternal and do not interact significantly with nodes outside the module. other nodes accept inputs and produce outputs thatcontrol the moduleõs interactions with the rest of the network. alon argues that one reason modules evolve in biology is that newdevices or entities can be constructed out of existing, welltested modules; thus, adaptation to new conditions (and new forces ofnatural selection) is more easily accomplished. if modules are to be swapped in and out, they must possess the property thattheir inputoutput response is approximately independent of what is connected to themñthat is, that the module is functionallyencapsulated. by contrast, a motif is an overrepresented patterns of interconnections in a network that is likely to perform someuseful behavior. however, it may not be functionally encapsulated, in which case it is not a module. for more discussion, see u.alon, òbiological networks: the tinkerer as an engineer,ó science 301(5641):18661867, 2003.27s.s. shenorr, r. milo, s. mangan, and u. alon, ònetwork motifs in the transcriptional regulation network of escherichiacoli,ó nature genetics 31(1):6468, 2002.28t.i. lee, h.j. yang, s.y. lin, m.t. lee, h.d. lin, l.e. braverman, and k.t. tang, òtranscriptional regulatory networks insaccharomyces cerevisiae,ó science 298(5594):799804, 2002.29r. milo et al., ònetwork motifs,ó 2002.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.216catalyzing inquiryfigure 6.1the endomesoderm specification network in the sea urchin species strongylocentrotus purpuratus.the period of activity represented spans embryonic growth from single cell to gastrulation (approximately 600cells). the different background colors denote different cell types, as indicated on the cartoon of an early blastulastage embryo on the top right. the short, thick horizontal lines represent regulatory dna of a particular gene inthe network, to which transcription factors bind to activate or repress transcription. the bent arrow emanatingfrom each regulatory domain represents the basal transcription apparatus of the gene, and the line(s) emergingfrom it represent the interactions of the product of the gene with other proteins (via the white and black interactionboxes) or cisregulatory dna.the architecture of the network is based on perturbation and expression data, on data from cisregulatoryanalyses for several genes, and on other experiments discussed in the references below. for quantitative results ofperturbation experiments and temporal details and the latest view of the network, see http://sugp.caltech.edu/endomes/.the repression cascade motif referred to in the text is indicated by the thick black (upstream gene) and gray(downstream genes) arrows. this work is described in the following:1.e.h. davidson, j.p. rast, p. oliveri, a. ransick, c. calestani, c.h. yuh, t. minokawa, et al., òa genomicregulatory network for development,ó science 295(5560):16691678, 2002.2.h. bolouri and e.h. davidson, òmodeling dna sequencebased cisregulatory gene networks,ó developmental biology 246(1):213, 2002.3.c.t. brown, a.g. rust, p.j.c. clarke, z. pan, m.j. schilstra, t. de buysscher, g. griffin, et al., ònew computational approaches for analysis of cisregulatory networks,ó developmental biology 246(1):86102, 2002.4.a. ransick, j.p. rast, t. minokawa, c. calestani, and e.h. davidson, ònew early zygotic regulators ofendomesoderm specification in sea urchin embryos discovered by differential array hybridization,ó developmental biology 246(1):132147, 2002.5.c.h. yuh, c.t. brown, c.b. livi, l. rowen, p.j.c. clarke, and e.h. davidson, òpatchy interspecific sequencesimilarities efficiently identify positive cisregulatory elements in the sea urchin,ó developmental biology246(1):148161, 2002.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.a computational and engineering view of biology217approximately 50 genes underlying an early celltype specification event in sea urchin embryos thatincludes several recurring interaction motifs. for example, there are several cases in which a gene (thickblack arrow), instead of activating another gene directly, represses a repressor of the target gene (thickgray arrows). such an arrangement can provide a number of possible advantages, including a sharperactivation profile for the target gene, important in defining spatial boundaries between cell types.modularity and conservation suggest a potential for comparative studies across species (e.g.,pufferfish, mice, humans) to contribute to an understanding of biological function. that is, understanding the role of a certain protein in mice, for example, may suggest a similar role for that same protein ifit is found in humans.these comments should not be taken to mean that functional modules in biological entities arenecessarily simple or static. biological systems are often made up of elements with multiple functionsinteracting in ways that are complex and difficult to separate, and nature exploits multiple linkages thata human engineer would not tolerate in the design of an artifact.30 for example, a component of onemodule may (or may not) play a role in a different module at a different time. a moduleõs functionalbehavior may be quantitatively regulated or switched between qualitatively different functions bychemical signals from other modules. despite these important differences between biological modulesand the modules that constitute humanly engineered artifacts, the notion of a collection of parts that canbe counted on to perform a given functionñthat is, a moduleñis meaningful from an analytical perspective and our understanding of that function.6.2.4 robustness in biological entitiesrobustness is one of the characteristics of biological systems that is most admired and most desiredfor engineered systems. especially as compared to software and information systems, which are notoriously brittle, biological systems maintain functionality in the face of a range of perturbations. moretraditional hardware engineering, however, has studied the questions of robustness (under variousnames including faulttolerance and control systems). applying the analytical techniques developed inengineering to studying the mechanics of robustness in biology, the logic goes, might reveal newinsights not only about biology, but about robust system design.in biology, the term robustness is used in many different ways in different subfields, including thepreservation of species diversity, a measure of healing, comprehensibility in the face of incompleteinformation, continuity of evolutionary lineages, phenotypic stability in development, cell metabolicstability in the face of stochastic events, or resistance to point mutations.31 its most general usage,6.e.h. davidson, j.p. rast, p. oliveri, a. ransick, c. calestani, c.h. yuh, t. minokawa, et al., òa provisionalregulatory gene network for specification of endomesoderm in the sea urchin embryo,ó developmental biology246(1):162190, 2002.7.j.p. rast, r.a. cameron, a.j. poustka, and e.h. davidson, òbrachyury target genes in the early sea urchinembryo isolated by differential macroarray screening,ó developmental biology 246(1):191208, 2002.8.p. oliveri, d.m. carrick, and e.h. davidson, òa regulatory gene network that directs micromere specification in the sea urchin embryo,ó developmental biology 246(1):209228, 2002.source: figure from m. levine and e.h. davidson, ògene regulatory networks for development,ó proceedingsof the national academy of sciences 102(14):49364942, 2005, available at http://www.pnas.org/cgi/content/full/102/14/4936. copyright 2005 national academy of sciences.30this is not to say that humanengineered artifacts are not affected by their origins. òcapture by historyó characterizes manyhuman artifacts as well, but likely not as strongly. for more discussion of these points, see d. norman, 1998, cited in footnote 16.31d.c. krakauer, òrobustness in biological systemsña provisional taxonomy,ó complex systems science in biomedicine, t.s.deisboeck, j.y. kresh, and t.b. kepler, eds., kluwer, new york, 2003.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.218catalyzing inquiryhowever, refers to the ability of a structure or process to persist in the face of perturbations of internalcomponents or the environment. those perturbations might include outright component failure, unexpected behavior from components or other cooperating systems, stochastic changes in chemical concentrations or reaction rates, mutations, or the motion of external biochemical parameters. these sorts ofperturbations, such as stochastic changes of molecular concentrations, are intrinsic to the nature ofbiology, from the molecular scale to the ecological.a robust response to these perturbations generally consists of one of three types: (1) parameterinsensitivity, meaning that a robust process does not depend on a single ideal value of an input; (2)graceful degradation, in which the level of functionality of the system is indeed lessened by componentfailures, but it continues to function; and (3) adaptation, in which internal components reconfigure toreact to a change to maintain the same level of functionality.32kitano notes that robustness is attained in biological systems by using mechanisms well known tohuman engineers. he describes four mechanisms or approaches to biological robustness:331.system control mechanisms such as negativefeedback and feedforward control;2.redundancy, whereby multiple components with equivalent functions are introduced for backup;3.structural stability, where intrinsic mechanisms are built to promote stability; and4.modularity, where subsystems are physically or functionally insulated so that failure in onemodule does not spread to other parts and lead to systemwide catastrophe.kitano then notes that these approaches used in engineering systems are also found in biologicalsystems, pointing out that òredundancy is seen at the gene level, where it functions in control of the cellcycle and circadian rhythms, and at the circuit level, where it operates in alternative metabolic pathways in e. coli.ó furthermore, engineering approaches have proven to be a useful lens when investigating biological robustness.for example, barkai and leibler34 established a model (later confirmed experimentally) to explainperfect robust adaptation in bacterial chemotaxis, or the ability of bacteria to move toward increasedconcentrations of certain ligands. it had long been known that the mechanism responsible for this abilityhad several key attributes, among them a high sensitivity to changes in chemical concentration, togetherwith an ability to adapt to the absolute level of that concentration. working with the known molecularmakeup of these cells (e.g., the receptors, kinases, and diffusible messenger proteins), barkai and leiblershowed that when varied separately, many of the rate constants (such as molecular concentrations ofelements of the signaling network or reaction rates) could be varied by orders of magnitude withoutaffecting the magnitude of the response.35later work by yi et al. used the mathematics of control systems to show how the barkaileiblermodel was a special case of integral feedback control, a wellstudied approach of control theory.36 inaddition to control theory (including feedback and feedforward control), many other engineeringapproaches are found in biological systems, including redundancy, modularity, purging (quickly eliminating failing components), and spatial compartmentalization.3732h. kitano, òsystems biology: a brief overview,ó science 295(5560):16621664, 2002. available at http://www.sciencemag.org/cgi/content/abstract/295/5560/1662.33h. kitano, òsystems biology,ó 2002.34n. barkai and s. leibler, òrobustness in simple biochemical networks,ó nature 387(6636):913917, 1997.35 however, the mechanism does not account for the full dynamic range of the sensor patches at a molecular level. (it may bethat some sort of emergent property of the sensor patch as a whole, as opposed to some property of the individual sensorcomplexes, is necessary to obtain the full dynamic range. see, for example, t.s. shimizu, s.v. aksenov, and d. bray, òa spatiallyextended stochastic model of the bacterial chemotaxis signaling pathway,ó journal of molecular biology 329(2):291309, 2003.)36t.m. yi, y. huang, m.i. simon, and j. doyle, òrobust perfect adaptation in bacterial chemotaxis through integral feedbackcontrol,ó proceedings of the national academy of sciences 97(9):46494653, 2000.37d.c. krakauer, òrobustness in biological systems,ó 2003.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.a computational and engineering view of biology219kitano makes the point that robustness is a property of an entire system;38 it may be that noindividual component or process within a system would be robust, but the systemwide architecturestill provides robust behavior. this presents a challenge for analysis, since elucidating such behaviorscan be counterintuitive and computationally demanding.39 in one such example, von dassow andcolleagues investigated the development of striped patterns in drosophila.40 they computationally modeled a network of interactions between genes and regulatory proteins active during embryogenesis andexplored the parameter space to see which sets of parameters produced stable striping. in their firstattempt, they were unable to reproduce such behavior computationally. however, once they added twomore molecular events and their interactions to the network, a surprisingly high proportion of therandomly chosen parameters produced the desired results. this strongly implies that such a network,taken as a whole, is a robust developmental module, able to produce a particular effect despite widevariation in reaction parameters.in a refinement to that work, ingolia investigated the architecture of that network to attempt todetermine the structural sources of such robust behavior.41 he determined that the source of the robustness at the network level was a pair of positive feedback loops of gene expression, which led to cellsbeing forced to one of two stable states (bistability). that is, small perturbations or changes in certainparameters would necessarily result in individual cells reaching one of two states. ingolia showed thatsuch bistability, at both an individual cell level and a network level, is an important architecturalproperty leading to robust behavior and that the latter is in fact a consequence of the former. moreover,it is this bistability that is responsible for the ability of the network to maintain a fixed pattern of geneexpression even in the face of cell division and growth.42robustness comes at a cost of increased complexity. the simplest bacteria can survive only withinnarrow ranges of environmental parameters, while more complex bacteria, such as e. coli (with agenome an order of magnitude larger than mycoplasma), can withstand more severe environmentalfluctuations.43 this increased complexity can in turn be the root of cascading failures, if the elements ofthe network responsible for the adaptive response fail. this implies that increased robustness of acertain aspect or element of a system with respect to a certain perturbation may come at the cost ofincreased vulnerability in a different aspect or element or to a different attack.robustness can also serve as a signpost for discovering the details of biological function. althoughthere may be a prohibitively large number of ways that a genetic network could produce a given result,for example, only a few of those ways are likely to do so robustly. knowledge of the robust qualities ofa biological system, coupled with theoretical or simulated analysis of networks, could aid in reverseengineering the system to determine its actual configuration.44an open and intriguing question is the relationship between robustness and evolution. becauserobustness is the quality of maintaining stability, in some sense it stands as a potential inhibitor toevolution, for example, by masking the effects of point mutations. and yet robust modules or organisms are more likely to survive, and thus pass on into succeeding generations. how does robustnessevolve? how do robust systems evolve? one engineering approach to this problem is to considerbiological systems as sets of components interacting through protocols,45 with one critical measure of a38h. kitano, òsystems biology,ó 2002. available at http://www.sciencemag.org/cgi/content/abstract/295/5560/1662.39a.d. lander, òa calculus of purpose,ó plos biology2(6):e164, 2004.40g. von dassow, e. meir, e.m. munro, and g.m. odell, òthe segment polarity network is a robust developmental module,ónature 406(6792):188192, 2000.41n.t. ingolia, òtopology and robustness in the drosophila segment polarity network,ó plos biology2(6):e123, 2004.42a.d. lander, òa calculus of purpose,ó 2004.43j.m. carlson and j. doyle, òcomplexity and robustness,ó proceedings of the national academy of sciences 99(suppl. 1):25382545, 2002.44u. alon, òbiological networks: the tinkerer as an engineer,ó science 301:18661867, 2003.45m.e. csete and j.c. doyle, òreverse engineering of biological complexity,ó science 295:16641669, 2002.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.220catalyzing inquirygood protocol being its ability to support both robustness and evolvability, a key consideration intechnical protocols of human engineering such as tcp/ip.6.2.5 noise in biological phenomena46as one illustration of how engineering disciplines might shed light on biological mechanism, consider the opposition of robustness and noise in biological phenomena. biological organisms exhibit highdegrees of robustness in the face of changing environments. engineered artifacts designed by humanbeings have used mechanisms such as negative feedback to provide stability, redundancy to providebackup, and modularity for the isolation of failures to enhance robustness. as the discussion belowindicates, these mechanisms are used for these purposes in biological organisms, as well.47in a biological context, noise can take the form of fluctuations in quantities such as reaction rates,concentrations, spatial distributions, and fluxes. in addition, fluctuations may also occur at the molecular level. however, despite the noise inherent in the internal environment of a cell, cells operateñoftenrobustly and quite stablyñwithin strict parameters, and robustness has been hypothesized as an intrinsic property of intracellular networks. (for instance, the chemotaxis pathway in e. coli functions over awide range of enzymatic activities and protein concentrations.48 robustness is also illustrated in somedevelopmental processes49 and phage lambda regulation.50) this robustness suggests that cells use andreject noise in a systematic manner.for the analysis of biological noise, much of the analysis originally derived from signal processingand control theory is applicable.51 indeed, pathways can be regarded as analog filters and classified interms of frequency response, where the differences between filtering electronic noise and filteringbiological noise are reflected only in the details of the underlying mechanisms rather than in highlevelabstractions of filtering theory.cascades and relays such as twocomponent systems and the mitogenactivated protein kinasepathway function as lowpass filters (i.e., filters that attenuate highfrequency noise).52 as a generalrule, longer cascades are more effective at reducing noise. however, because noise arises in the pathwayitself, the amount of internally generated noise increases with cascade lengthñsuggesting that there isan optimal cascade length for attenuating noise.53it is not surprising that lowpass filters are components of biological systems. as noted above,biological systems operate homeostatically,54 and the essential principle underlying homeostasis is thatof negative feedback. from the standpoint of signal processing, a negative feedback loop functions as alowpass filter.46section 6.2.5 is based on and incorporates several excerpts from c.v. rao, d.m. wolf, and a.p. arkin, òcontrol, exploitationand tolerance of intracellular noise,ó nature 420(6912):231237, 2002.47h. kitano, òsystems biology: a brief overview,ó science 295(5560):16621664, 2002. available at http://www.sciencemag.org/cgi/content/abstract/295/5560/1662.48n. barkai and s. leibler, òrobustness in simple biochemical networks,ó nature 387:913917, 1997; u. alon, m.g. surette, n.barkai and s. leibler, òrobustness in bacterial chemotaxis,ó nature 397:168171, 1999. (cited in rao et al., 2002.)49g. von dassow, e. meir, e.m. munro, and g.m. odell, òthe segment polarity network is a robust developmental module,ónature 406:188192, 2000; e. meir, g. von dassow, e. munro, and g.m. odell, òrobustness, flexibility, and the role of lateralinhibition in the neurogenic network,ó current biology 12:778786, 2002. (cited in rao et al., 2002.)50j.w. little, d.p. shepley, and d.w. wert, òrobustness of a gene regulatory circuit,ó embo journal 18:42994307, 1999.51a.p. arkin, òsignal processing by biochemical reaction networks,ó pp. 112144, selforganized biological dynamics and nonlinear control, j. walleczek, ed., cambridge university press, london, 2000; m. samoilov, a. arkin, and j. ross, òsignal processing by simple chemical systems,ó journal of physical chemistry 106:1020510221, 2002. (cited in rao et al., 2002.)52p.b. detwiler, s.a. ramanathan, a. sengupta, and b.i. shraiman, òengineering aspects of enzymatic signal transduction:photoreceptors in the retina,ó biophysical journal 79(6):28012817, 2000. (cited in rao et al., 2002.)53m. thattai and a.van oudenaarden, òattenuation of noise in ultrasensitive signaling cascades,ó biophysical journal82(6):29432950, 2002. (cited in rao et al., 2002.)54homeostasis is the property of a system that enables it to respond to changes in its environment in such a way that it tends tomaintain its original state.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.a computational and engineering view of biology221a second useful construct from signal processing is the bandpass filter, which is based on thecontrol theory notion of integral feedback. integral feedback is a kind of negative feedback that amplifies intermediate frequencies and attenuates low and high frequencies. a biological instantiation ofintegral feedback is contained in bacterial chemotaxis.55in addition to the filters described above, other mechanisms attenuate noise in systems. theseinclude the following:¥redundancy. noise in a single channel might be misinterpreted as a genuine signal. however,redundancyñin the form of multiple channels serving the same functionñcan help to minimize thelikelihood of such an occurrence. in a biological context, redundancy has been demonstrated in mechanisms such as gene dosage and parallel cascades,56 which attenuate the effects of noise by increasing thelikelihood of gene expression or establishing a consensus from multiple signals.¥checkpointing. noise can interfere with the successful completion of various biological operationsthat are essential in a pathway. however, a checkpoint can ensure that each step in a pathway iscompleted successfully before proceeding with the next step. such checkpoints have been characterizedin the cell cycle and flagellar biosynthesis.57¥proofreading. noise can introduce errors into a process. but errorcorrecting mechanisms canreduce this effect of noise, as is the case of kinetic proofreading in protein translation.58a final, and surprising, mechanism is that complexity itself in some cases can be implicated in therobustness of an organism against noise. in 1942, waddington noted the stability of phenotypes (fromthe same species) against a backdrop of considerable genetic variation, a phenomenon known as canalization.59 in principle, such stability could result from explicit genetic control of phenotype features,such as the number of fingers on a hand or the placement of wings on an insectõs body. however, siegaland bergman modeled the developmental process responsible for the emergence of such features as anetwork of interacting transcriptional regulators and found that the network constrains the geneticsystem to produce canalization.60 furthermore, the extent of canalization, measured as the insensitivityof a phenotype to changes in the genotype (i.e., to mutations), depends on the complexity of thenetwork, such that more highly connected (i.e., more complex) networks evolve to be more canalized.(box 6.5 provides more details.)consider that noise can also make positive contributions to biological systems. for example, it iswell known from the agricultural context that monocultures are less robust than ecosystems thatinvolve multiple speciesñthe first can be wiped out by a disease that targets the specific crop inquestion, whereas the second cannot. thus, some degree of variation in a populating species isdesirable, and noise is one mechanism for introducing variation that results in population heteroge55the size of a single bacterium is so small that the bacterium is unable to sense a spatial gradient across the length of its body.thus, to sense a spatial gradient, the bacterium moves around and senses chemical concentrations in different locations atdifferent times; the result is a motion bias toward attractants. see t.m. yi, y. huang, m.i. simon, and j. doyle, òrobust perfectadaptation in bacterial chemotaxis through integral feedback control,ó proceedings of the national academy of sciences 97(9):46494653, 2000. (cited in rao et al., 2002.)56h.h. mcadams and a. arkin, òitõs a noisy business! genetic regulation at the nanomolar scale,ó trends in genetics 15(2):6569, 1999; d.l. cook, a.n. gerber, and s.j. tapscott, òmodeling stochastic gene expression: implications for haploinsufficiency,óproceedings of the national academy of sciences 95(26):1564115646, 1998. (cited in rao et al., 2002.)57l.h. hartwell and t.a. weinert, òcheckpoints: controls that ensure the order of cell cycle events,ó science 246(4930):629634, 1989. (cited in rao et al., 2002.)58m.v. rodnina and w. wintermeyer, òribosome fidelity: trna discrimination, proofreading and enduced fit,ó trends inbiochemical science 26(2):124130, 2001. (cited in rao et al., 2002.)59c.h. waddington, òcanalization of development and the inheritance of acquired characters,ó nature 150:563565, 1942.60m.l. siegal and a. bergman, òwaddingtonõs canalization revisited: developmental stability and evolution,ó proceedings ofthe national academy of sciences 99(16):1052810532, 2002.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.222catalyzing inquiryneity and diversity. for example, noise (in the form of molecular fluctuations) introduced into thegenetic circuit governing development in phage lambda can cause an initially homogeneous population to separate into lytic and lysogenic populations.61 (in this case, the basic mechanism involvesbox 6.5canalization and the connectivity of transcriptional regulatory networksto explore the possibility that genetic canalization may be a byproduct of other selective forces, . . . [we start with]the model of a. wagner, who treats development as the interaction of a network of transcriptional regulatory genes,phenotype as the equilibrium state of this network, and fitness as a function of the distance between an individualõsequilibrium state and the optimum state. . . . evolution in the model [a generalized version of wagnerõs] consists ofthree phases: mating, development, and selection. mating and selection are modeled in accord with traditionalpopulationgenetic approaches. . . . [to handle development] one can represent a network of transcriptional regulators by a state vector containing the concentration of each gene product and a matrix, the entries of which representthe effects of each gene product on the expression of each gene. entries may be either positive (activating) ornegative (repressing) and may differ in magnitude. zero elements in the matrix represent the absence of interactionbetween the given gene product and gene. the developmental process is then fully described by a set of nonlinearcoupled difference equations. . . . wagner draws an analogy between the rows of the interaction matrix and theenhancer regions of the genes in the network and further justifies the biological realism of this type of model byreference to data from actual genetic networks. an important assumption in the model, also justified by a. wagner,is that functional genetic networks will reach a stable equilibrium geneexpression state, and that unstable networksreflect, in a sense, the failure of development. thus, in his model and ours, development itself enforces a kind ofselection, because we require that the network of regulatory interactions produce a stable equilibrium geneexpression state (its òphenotypeó), whose distance to an optimum state can then be measured during the selection phase.. . . we report here the results of numerical simulations of our model of an evolving developmentalgenetic system.we demonstrate an important, perhaps primary, role for the developmental process itself in creating canalization, inthat insensitivity to mutation evolves even when stabilizing selection is absent. we go on to demonstrate that thecomplexity of the network is a key factor in this evolutionary process, in that networks with a greater proportion ofconnections evolve greater insensitivity to mutation.. . . one is led to wonder whether the evolution of canalization under no stabilizing selection on the geneexpressionpattern is an artifact of the modeling framework or whether it represents a finding of real biological significance. weargue that the latter is true on a number of counts. to begin, we acknowledge that it is difficult to envision a scenarioin nature in which the stability of a developmental module is required, but the phenotype produced by that moduleis not subject to selection. one situation in which this condition may hold is when a species colonizes a new territorywith virtually unlimited resources, so selection is only for those that develop to reproduce. furthermore, even if sucha scenario does not pertain, the conceptual decomposition of stabilizing selection into selection for an optimum andselection for developmental stability is important. thus, even in scenarios in which members of a population aresubject to selection for an optimum, the evolution of canalization may proceed because of the underlying selectionfor stability of the developmental outcome. our results suggest that this underlying selection can occur very fast.because others have argued that the evolution of canalization under stabilizing selection may be slow, developmental stability may therefore be the dominant force in the evolution of canalization.source: reprinted by permission from m.l. siegal and a. bergman, òwaddingtonõs canalization revisited: developmental stability andevolution,ó proceedings of the national academy of sciences 99(16):1052810532, 2002. copyright 2002 national academy of sciences.(references and figures are omitted above and can be found in the original article.)61a. arkin, j. ross, and h.h. mcadams, òstochastic kinetic analysis of developmental pathway bifurcation in phage lambdainfected escherichia coli cells,ó genetics 149(4):16331648, 1998. (cited in rao et al., 2002.)catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.a computational and engineering view of biology223two antagonistic feedback loops that create a switch and molecular fluctuations that partition theinitial population stochastically.)noise can be used to enhance a signal when certain nonlinear effects are present, as demonstratedby the phenomenon of stochastic resonance.62 stochastic resonance is found in many biological systems, including the electroreceptors of paddlefish,63 mechanoreceptors in the tail fins of crayfish,64 andhair cells in crickets.65 a similar phenomenon can potentially increase sensitivity in certain signalingcascades.66finally, noise can be useful for introducing stability. the network that controls circadian rhythmsconsists of multiple, complex, interlocking feedback loops. both deterministic and stochastic mechanisms for noise resistance in circadian rhythms have been explored,67 and it turns out that stochasticmodels are able to produce regular oscillations when the deterministic models do not,68 suggesting thatthe regulatory networks may utilize molecular fluctuations to their advantage.the discussion above suggests that biological robustness is in some ways a problem of controllingthe effects of noise and in other ways one of exploiting those effects. considerations of noise androbustness thus offer insight into the design and function of intracellular networks.69 that is, thefunction of an intracellular network may require specific regulatory and information structures, andcertain design features are necessary for a stable network phenotype.finally, note that mechanisms of the sorts described above do not generally function in isolation,but rather interact in complex networks involving multiple feedback loops, and the resulting networkscan produce diverse phenomena, including switches, memory, and oscillators.70 such coupling also hasan important analytical consequenceñnamely, that the composite behavior of multiple coupled mechanisms is much more difficult to predict than the behavior of individual components. to analyze multiple coupled systems, computational models are highly useful.6.3 a computational metaphor for biologyin addition to the abstractions described above, computing and computer science can also providelife scientists with a rich source of language, metaphors, and analogies with which to describe biologicalphenomena and insights from a computational perspective. these linguistic and cognitive aspects maywell make it easier for insights originating in computing to be made relevant to biology, and thus62l. gammaitoni, p. hanggi, p. jung, and f. marchesoni, òstochastic resonance,ó reviews of modern physics 70:223287, 1998.(cited in rao et al., 2002.)63d.f. russell, l.a. wilkens, and f. moss, òuse of behavioural stochastic resonance by paddle fish for feeding,ó nature402(6759):291294, 1999. (cited in rao et al., 2002.)64j.k. douglass, l. wilkens, e. pantazelou, and f. moss, ònoise enhancement of information transfer in crayfish mechanoreceptors by stochastic resonance,ó nature 365(6444):337340, 1993. (cited in rao et al., 2002.)65j.e. levin and j.p. miller, òbroadband neural encoding in the cricket cercal sensory system enhanced by stochasticrresonance,ó nature 380(6570):165168, 1996. (cited in rao et al., 2002.)66j. paulsson, o.g. berg, and m. ehrenberg, òstochastic focusing: fluctuationenhanced sensitivity of intracellular regulation,ó proceedings of the national academy of sciences 97(13):71487153, 2000. (cited in rao et al., 2002.)67n. barkai and s. leibler, òcircadian clocks limited by noise,ó nature 403(6767):267268, 2000; d. gonze, j. halloy, and a.goldbeter, òrobustness of circadian rhythms with respect to molecular noise,ó proceedings of the national academy of sciences99(2):673678, 2002; p. smolen, d.a. baxter, and j.h. byrne, òmodeling circadian oscillations with interlocking positive andnegative feedback loops,ó journal of neuroscience 21(17):66446656, 2001. (cited in rao et al., 2002.)68j.m. vilar, h.y. kueh, n. barkai, and s. leibler, òmechanisms of noise resistance in genetic oscillators,ó proceedings of thenational academy of sciences 99(9):59885992, 2002. (cited in rao et al., 2002.)69m.e. csete and j.c. doyle, òreverse engineering of biological complexity,ó science 295(5560):16641669, 2002; m. morohashi,et al., òrobustness as a measure of plausibility in models of biochemical networks,ó journal of theoretical biology 216(1):1930,2002; l.h. hartwell, j.j. hopfield, s. leibler, and a.w. murray, òfrom molecular to modular cell biology,ó nature 402(6761suppl):c47c52, 1999. (cited in rao et al., 2002.)70m.b. elowitz and s. leibler, òa synthetic oscillatory network of transcriptional regulators,ó nature 403(6767):335338,2000; t.s. gardner, c.r. cantor, and j.j. collins, òconstruction of a genetic toggle switch in escherichia coli,ó nature 403(6767):339342, 2000. (cited in rao et al., 2002.)catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.224catalyzing inquiryinformation abstractions can be used to communicate about or to explain biological processes andconcepts. consider, for example, the jacob and monod description of the genome as a ògenetic program,ó capable of controlling its own execution.71 (conversely, biological metaphors and languagemight offer analogous benefits to computing, which is the subject of chapter 8.) at the same time,poorly chosen metaphors can limit understanding by carrying over misleading or irrelevant details. forexample, the ògenetic programó metaphor described above might lead one to think of protein synthesisas being executed one instruction at a time (as most computer programs would be), obscuring theparallel and interconnected nature of the genetic protein synthesis network.72the use of a metaphor (to look at a problem in field a through the lens of field b) invites one toapply insights from field b to the problem in field a. metaphors are often (indeed, almost always)imprecise and somewhat vague, because they are not specific about which insights from field b arerelevant to field a. they can nevertheless be useful, because they constitute an additional source ofinsight and new ways of thinking to be brought to bear on field a that might not otherwise be availablein the absence of those metaphors. moreover, field bñas a disciplineñconstitutes an existence proofthat the insights in question can in fact be part of an intellectually coherent whole.consider, for example, extending the notion of the ògenetic program.ó in some sense, the dna sequencecan be analogized to the binary code of a program. however, in many real computer programs, a programstructure or architecture or individual components may be apparent from representing the program in itssource code form, where things such as variable declarations and subroutines make manifestly obvious whatis obscured in the binary representation. calling sequences between program and subprogram define program interfaces and protocols for how different components of a program may communicateñdata definitions, formats, and semantics, for instance. thus, it may be meaningful to inquire about the analogous thingsin biology, and indeed, a gene contained in dna might well be one analogue of a subprogram or the actionpotential in neuroscience one analogue of a communications protocol.another analogy can be drawn between the evolution of computing and the biological transitionfrom singlecell organisms to multicell organisms. multicellular life exploits four broad strategies: collaboration between highly specialized cells; communication by polymorphic messages; self, defined bya stigmergic structure; and self, protected by programmed cell death. these strategies are rare in singlecell organisms but nearly universal in multicellular organisms, and evolved before or coincident withthe emergence of multicellular life. as described in table 6.1, each of these strategies may be analogousto trends seen in computing today.to illustrate how the use of a computational metaphor can provide insight and lead to deeper exploration, note that cellular processes are concurrent (i.e., changes in the surrounding environment can trigger theexecution of many parallel processes); operate at many levels including the submolecular, molecular, subcellular, and cellular; and involve relationships among many subcellular and molecular objects. computerscientists have devised a number of formalisms that are capable of representing such processes, and kam etal.73 modeled aspects of tcell activation using the formalism of statecharts,74 as they have been adapted tothe framework of objectoriented modeling.75 because the objectoriented statechart approach supports71f. jacob and j. monod, ògenetic regulatory mechanisms in the synthesis of proteins,ó journal of molecular biology 3:318356,1961.72e.f. keller, making sense of lifeñexplaining biological developments with models, metaphors, and machines, harvard universitypress, cambridge, ma, 2003.73n. kam, i.r. cohen, and d. harel, òthe immune system as a reactive system: modeling t cell activation with statecharts,óproceedings of a symposium on visual languages and formal methods (vlfmõ01), part of ieee symposium on humancentriccomputing (hccõ01), 2001, pp. 1522.74d. harel, òstatecharts: a visual formalism for complex systems,ó science of computer programming 8:231274, 1987. (cited inkam et al., ò the immune system as a reactive system,ó 2001.)75g. booch, objectoriented analysis and design, with applications, addisonwesley, menlo park, ca, 1994; d. harel and e.gery, òexecutable object modeling with statecharts,ó computer, 3142, 1997; j. rumbaugh, m. blaha, w. premerlani, f. eddy, andw. lorensen, objectoriented modeling and design, prentice hall, englewood cliffs, nj, 1991. (cited in kam et al., 2001.)catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.a computational and engineering view of biology225concurrency, multilevel description, and object orientation, kam et al. constructed a tcell simulation thatpresents its results by displaying animated versions of the modelõs statecharts.a second example is provided by the work of searls. it is a common, if not inescapable, metaphorthat dna represents the language of life. in the late 1980s and early 1990s, david b. searls and collaborators made the metaphor much more concrete, applying formal language theory to the analysis ofnucleic acid sequences.76 linguistics theory considers four levels of interpretation of text: lexical (thetable 6.1principles of operation for multicellular organisms and networked computingprinciplemulticellular organismsnetworked computingcollaborationcells in biofilms specializetoday most computers retain abetween highlytemporarily according to òquorumólarge repertoire of unused generalspecialized cellscues from neighbors. cells inbehavior susceptible to viral oròtrueó multicellular organismsworm attack. biology suggestspermanently specializethat more specialization and less(differentiate) during development.monoculture would beloss of differentiation is an earlyadvantageous (although marketsign of cancer.forces may oppose this).communicationcells in multicelled organismsexecutable code is the analogue ofby polymorphiccommunicate with each other viadna. most pcs permit easy, andmessagesmessenger molecules, never dna.hidden, download of executablethe òmeaningó of celltocellcode (activex or even exe).messages is determined by thehowever, importing executablereceiving cell, not the sender.code is well known to createsecurity risks, and secure systemsminimize or eliminate thiscapability.òselfó defined bymulticelled organisms and biofilmsdetermination of self is largely ada stigmergicbuild extracellular stigmergichoc in todayõs systems. however,structurestructures (bone, shell, or justan organizationõs intranet is aslime) that define the persistentstigmergic structure, as are itsself. òselfnessó resides as much inpersistent databases.the extracellular matrix as in thecells.òselfó protectedevery healthy cell in a multicelleda familiar example in computingby programmedorganism is prepared to commitis the blue screen of death, whichcell death (pcd)suicide. pcd evolved to deal withis a programmed response to andna replication errors, viralunrecoverable error. an analogousinfection, and rogue undifferentiatedcomputer should sense its owncells. pcd reflects a multicellularrogue behavior (e.g., download ofperspectiveñsacrificing theuncertified code) and disconnectindividual cell for the good of theitself from the network or rebootmulticellular organism.itself periodically to give itself aclean initial state.source: steve burbeck, ibm, personal communication, october 11, 2004.76d.b. searls, òthe linguistics of dna,ó american scientist 80:579591, 1992. formal language theory is a major subfield ofcomputer science theory; it is based on noam chomskyõs work on linguistics in the 1950s and 1960s, especially the chomskyhierarchy, a categorization of languages by their inherent complexity. formal languages are at the heart of parsers and compilers,and there exists a wide range of both theoretic analysis and practical software tools for the production, transformation, andanalysis of text. the main algorithmic tool of language theory is the generative grammar, a series of rules that transforms higherlevel abstract units of meaning (such as òsentenceó or ònoun phraseó) into more concrete potential statements in a given language. grammars can be categorized into regular, contextfree, contextsensitive, and recursively enumerable, each of whichrequires more algorithmic complexity to recognize than the level before it.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.226catalyzing inquiryidentification of specific words), syntactic (the grouping of words into grammatically correct phrases),semantic (the assignment of meaning to words and phrases), and pragmatic (the role of a piece of text inthe larger context). these match entirely well to genomic analysis: grouping bases into codons, genes,the function of the resulting protein, and the role of that protein in the larger molecular system.77linguistic analyses can reveal or explain relationships between bases that are far apart in a sequence. for example, an rna structure called a stemloop has a palindromelike sequence, with watsoncrick pairs at equal distances away from the center. traditional probabilistic or patternsearchingapproaches would have some difficulty recognizing this structure, but it is quite simple with a grammarthat produces palindromes. some sequences of nucleic acids result in ambiguous linguistic interpretations; while this is a difficulty for computer languages, it represents a strength of biological linguisticanalysis, because these ambiguities correctly represent alternative secondary structures.78this approach has been fruitful for analyzing genetic sequences and characterizing the complexityand structure of genes. genlang, a software system that employs linguistic approaches, has successfully identified trna genes, group i introns, proteinencoding genes, and the specification of generegulatory elements.79 other important findings include placing rna in the chomsky hierarchy as atleast beyond contextfree languages. finally, the approach provides a powerful tool for understandingthe evolution of nucleic acid sequences; since the first sequences were most likely random (and thusregular languages), there must be a mechanism that somehow promoted sequence language into morepowerful linguistic categories. this can be seen as an algebraic problem of operational closure, and thequestion is, for which string operations are regular languages and contextfree languages not closed?8077d.b. searls, òreading the book of life,ó bioinformatics 17(7):579580, 2001.78d.b. searls, òthe language of genes,ó nature 420(6912):211217, 2002.79d.b. searls, and s. dong, òa syntactic pattern recognition system for dna sequencesó in proceedings of the second international conference on bioinformatics, supercomputing, and complex genome analysis, h.a. lim, j. fickett, c.r. cantor, and r.j. robbins,eds., world scientific publishing co., pp. 89101, 1993.80d.b. searls, òformal language theory and biological macromolecules,ó series in discrete mathematics and theoretical computer science 47:117140, 1999.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.cyberinfrastructure and data acquisition2272277cyberinfrastructure and data acquisition7.1 cyberinfrastructure for 21st century biologytwentyfirst century biology seeks to integrate scientific understanding at multiple levels of biological abstraction, and it is holistic in the sense that it seeks an integrated understanding of biologicalsystems through studying the set of interactions between components. because such an enormous,dataintensive effort is necessarily and inherently distributed over multiple laboratories and investigators, an infrastructure is necessary that facilitates the integration of experimental data, enables collaboration, and promotes communication among the various actors involved.7.1.1 what is cyberinfrastructure?cyberinfrastructure for science and engineering is a term coined by the national science foundation (nsf) to refer to distributed computer, information, and communication technologies and theassociated organizational facilities to support modern scientific and engineering research conducted ona global scale. as articulated by the atkins panel,1 the technology substrate of cyberinfrastructureinvolves the following:¥highend generalpurpose computing centers that provide supercomputing capabilities to the community at large. in the biological context, such capabilities might be used to undertake, for example,calculations to determine the threedimensional structure of proteins given their genetic sequence. insome cases, these computing capabilities could be provided by local clusters of computers; in othercases, specialpurpose hardware; and in still others, computing capabilities on demand from a computing grid environment.¥data repositories that are well curated and that store and make available to all researchers largevolumes and many types of biological data, both in raw form and as associated derived products. suchrepositories must store data, of course, but they must also organize, manage, and document these1órevolutionizing science and engineering through cyberinfrastructure: report of the nsf blueribbon advisory panel oncyberinfrastructure,ó 2003, available at http://www.communitytechnology.org/nsfcireport/report.pdf.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.228catalyzing inquirydatasets dynamically. they must provide robust search capabilities so that researchers can find thedatasets they need easily. also, they are likely to have a major role in ensuring the data interoperabilitynecessary when data collected in one context are made available for use in another.¥digital libraries that contain the intellectual legacy of biological researchers and provide mechanisms for sharing, annotating, reviewing, and disseminating knowledge in a collaborative context. whereprint journals were once the standard mechanism through which scientific knowledge was validated,modern information technologies allow the circumvention of many of the weaknesses of print. knowledge can be shared much more broadly, with much shorter lag time between publication and availability.different forms of information can be conveyed more easily (e.g., multimedia presentations rich in biological imagery). one researcherõs annotations to an article can be disseminated to a broader audience.¥highspeed networks that connect largescale, geographically distributed computing resources, datarepositories, and digital libraries. because of the large volumes of data involved in biological datasets,todayõs commodity internet is inadequate for highend scientific applications, especially where there is arealtime element (e.g., remote instrumentation and collaboration). network connections ten to a hundredtimes faster than those generally available today are a lower bound on what will be necessary.in addition to these components, cyberinfrastructure must provide software and services to thebiological community. for example, cyberinfrastructure will involve many software tools, system software components (e.g., for grid computing, compilers and runtime systems, visualization, programdevelopment environments, distributed scalable and parallel file systems, human computer interfaces,highly scalable operating systems, system management software, parallelizing compilers for a varietyof machine architectures, sophisticated schedulers), and other software building blocks that researcherscan use to build their own cyberinfrastructureenabled applications. services, such as those needed tomaintain software on multiple platforms and provide for authentication and access control, must besupported through the equivalent of helpdesk facilities.from the committeeõs perspective, the primary value of cyberinfrastructure resides in what it enables with respect to data management and analysis. thus, in a biological context, machinereadableterminologies, vocabularies, ontologies, and structured grammars for constructing biological sentencesare all necessary higherlevel components of cyberinfrastructure as tools to help manage and analyzedata (discussed in section 4.2). highend computing is useful in specialized applications but, by comparison to tools for data management and analysis, lacks broad applicability across multiple fields ofbiology.7.1.2 why is cyberinfrastructure relevant?the atkins panel noted that the lack of a ubiquitous cyberinfrastructure for science and engineeringresearch carries with it some major risks and costs. for example, when coordination is difficult, researchers in different fields and at different sites tend to adopt different formats and representations ofkey information. as a result, their reconciliation or combination becomes difficult to achieveñandhence disciplinary (or subdisciplinary) boundaries become more difficult to break down. without systematic archiving and curation of intermediate research results (as well as the polished and reducedpublications), useful data and information are often lost. without common building blocks, researchgroups build their own application and middleware software, leading to wasted effort and time.as a field, biology faces all of these costs and risks. indeed, for much of its history, the organizationof biological research could reasonably be regarded as a group of more or less autonomous fiefdoms.unifying biological research into larger units of aggregation is not a plausible strategy today, and so thefederation and loose coordination enabled by cyberinfrastructure seem well suited to provide the majoradvantages of integration while maintaining a reasonably stable largescale organizational structure.furthermore, wellorganized, integrated, synthesized information is increasingly valuable to biological research (box 7.1). in an era characterized by dataintensive research observations, collecting,catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.cyberinfrastructure and data acquisition229box 7.1a cyberinfrastructure view: envisioning and empowering successes for21st century biological sciencescreating and sustaining a comprehensive cyberinfrastructure (ci; the pervasive applications of all domains ofscientific computing and information technology) are as relevant and as required for biology as for any science or intellectual endeavor; in the advances that led to todayõs opportunity, the national science foundationõs directorate for biological sciences (nsf bio) made numerous, ad hoc contributions, and now canintegrate its efforts to build the complete platforms needed for 21st century biology. doing so will accelerateprogress in extraordinary ways.the time has arrived for creating a ci for all of the sciences, for research and education, and nsf will lead theway. nsf bio must codefine the extent and fine details of the nsf structure for ci, which will involve majorinternal nsf partnerships and external partnerships with other agencies, and will be fully international inscope.only the biological sciences have seen advances as remarkable, sustained, and revolutionary as those incomputer and information sciences. only in the past few years has the world of computing and informationtechnology reached the level of being fully applicable to the wide range of cuttingedge themes characteristicof biological research. multiplying the exponentials (of continuing advances in computing and bioscience)through deep partnerships will inevitably be exciting beyond any anticipation.the stretch goals for the biological sciences community include both communitylevel involvement andrealization of the complete spectrum of ci, namely, people and training, instrumentation, collaborations,advanced computing and networking, databases and knowledge management; and analytical methods (modeling and simulation).nsf bio must:¥invest in people;¥ensure science pull, technology push;¥stay the course;¥prepare for the data deluge;¥enable science targets of opportunity;¥select and direct the technology contributions; and¥establish national and international partnerships.the biology community must decide how it can best interact with the quantitative science community, whereand when to intersect with computational sciences and technologies, how to cooperate on and contribute toinfrastructure projects, and how nsf bio should partner administratively. an implementation meeting, aswell as briefings to the community through professional societies, will be essential.for nsf bio to underestimate the importance of cyberinfrastructure for biology, or fail to provide fuel over theentire journey, would severely retard progress and be very damaging for the entire national and internationalbiological sciences community.source: adapted from subcommittee on 21st century biology, nsf directorate for biological sciences advisory committee, building acyberinfrastructure for the biological sciences 2005 and beyond: a roadmap for consolidation and exponentiation, a workshop report, july1415, 2003.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.230catalyzing inquirymanaging, and connecting data from various modalities and on multiple scales of biological systems,from molecules to ecosystems, are essential to turn that data into information. each biological subdiscipline also now requires the tools of information technology to probe that information, to interconnectexperimental observations and modeling, and to contribute to an enriched understanding or knowledge. the expansion of biology into discovery and synthetic analysis, that is, genomeenabled biologyand systems biology as well as the hardening of many biological research tools into highthroughputpipelines, serves also to drive the need for cyberinfrastructure in biology.box 7.2 illustrates existing efforts in the development of cyberinfrastructure for biology that arerelevant. note that the examples span a wide range of subfields within biology, including proteomics(pdb), ecology (neon and lter), neuroscience (birn), and biomedicine (nbcr).data repositories and digital libraries are discussed in chapter 3. the discussion below focusesprimarily on computing and networking.box 7.2examples of possible elements of a cyberinfrastructure for biologypacific rim application and grid middleware assemblythe pacific rim application and grid middleware assembly (pragma) is a collaborative effort of 15 institutions around the pacific rim. pragmaõs mission is to establish sustained collaborations and advance the useof grid technologies among a community of investigators working with leading institutions around the pacificrim. to fulfill this mission, pragma hosts a series of workshop for members to focus on developing applications and on developing a testbed for these applications. current applications include workflows in biology(protein annotation); linking via web services climate data (working with some longterm ecological research [lter] network sites in the united states and east asia pacific region [ilter]); running solvationmodels; and extending telescience application to more institutions.the protein data bankthe protein data bank (pdb) was established in 1971 as a computerbased archival resource for macromolecular structures. the purpose of the pdb was to collect, standardize, and distribute atomic coordinates andother data from crystallographic studies. in 1977 the pdb listed atomic coordinates for 47 macromolecules. in1987, the number began to increase rapidly at a rate of about 10 percent per year due to the development ofarea detectors and widespread use of synchrotron radiation; by april 1990, atomic coordinate entries existedfor 535 macromolecules. commenting on the state of the art in 1990, holbrook and colleagues [citationomitted] noted that crystal determination could require one or more manyears. as of 1999, the biologicalmacromolecule crystallization database (bmcd) of the pdb contain[ed] entries for 2,526 biological macromolecules for which diffraction quality crystals had been obtained. these include proteins, proteinproteincomplexes, nucleic acids, nucleic acidnucleic acid complexes, proteinnucleic acid complexes, and viruses.in july 2004, the pdb held information on 26,144 structures (23,676 proteins, peptides, and viruses; 1,338nucleic acids; 1,112 protein/nucleic acid complexes; and 18 carbohydrates).the national center for biotechnology informationthe national center for biotechnology information (ncbi), part of nihõs national library of medicine, has beencharged with creating automated systems for storing, analyzing, and facilitating the use of knowledge aboutmolecular biology, biochemistry, and genetics. in addition to genbank, ncbi curates the online mendelianinheritance in man (omim), the molecular modeling database (mmdb) of threedimensional protein structures,the unique human gene sequence collection (unigene), the taxonomy browser, and the cancer genomeanatomy project (cgap), in collaboration with the national cancer institute. ncbiõs retrieval system, entrez,permits linked searches of the databases, while a variety of tools have been developed for data mining, sequenceanalysis, and threedimensional structure display and similarity searching. ncbiõs senior investigators and extended staff collaborate with the external research community to develop novel algorithms and research approaches that have transformed computational biology and will enable further genomic discoveries.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.cyberinfrastructure and data acquisition2317.1.3 the role of highperformance computingloosely speaking, processing capability refers to the speed with which a computational solution toa problem can be delivered. high processing capability is generally delivered by computing unitsoperating in parallel and is generally dependent on two factorsñthe speed with which individual unitscompute (usually measured in operations per second) and the communications bandwidth betweenindividual units. if a problem can be partitioned so that each subcomponent can be processed independently, then no communication at all is needed between individual computing units. on the other hand,as the dependence of one subcomponent on others increases, so does the amount of communicationsrequired between computing units.eurogridõs bio gridfunded by the european commission, bio grid is intended to help biologists and chemists who are notfamiliar with highperformance computing (hpc) execution systems by developing intuitive user interfaces forselected biomolecular modeling packages and creating compatibility interfaces between the packages andtheir databases through bio gridõs unicore platform. the unicore system will allow investigators tostreamline their work processes, connect to internetaccessible databases, and run a number of quantumchemistry and molecular dynamics software programs developed as plugins by bio gridõs staff.the nsf national ecological observatory network (neon)neon is a continentalscale research instrument consisting of geographically distributed networked infrastructure, with lab and field instrumentation; sitebased experimental infrastructure; natural history archivefacilities; and computational, analytical, and modeling capabilities. neon is intended to transform ecologicalresearch by enabling studies on major environmental challenges at regional to continental scales. scientistsand engineers use neon to conduct realtime ecological studies spanning all levels of biological organizationand many temporal and geographical scales. neonõs synthesis, computation, and visualization infrastructureconstitutes a virtual laboratory that enables the development of a predictive understanding of the direct effectsand feedbacks between environmental change and biological processes.the nsf longterm ecological research network (lter)since 1980, nsf has supported the longterm ecological research (lter) network. the lter program ischaracterized by long temporal and broad spatial scales of research and fosters ecological comparisons among26 u.s. sites that illustrate the importance of comprehensive analyses of ecosystems and of distinguishingsystem features across multiple scales of time and space. data collected at each site are accessible to otherscientists and the general public, and the lter network works with other research institutions to standardizeinformation management practices to achieve network and communitywide data integration, facilitatingdata exchange and advancing data analysis and synthesis. ltersupported work has included efforts in climate variability and ecosystem response, standardization of protocols for measuring soil properties for longterm ecological research, synthesis of global data on winter ice duration on lakes and rivers, and comparisonsof ecosystem productivity, among others.sources: pragma: material adapted from http://www.pragmagrid.net.pdb: material pre2004 excerpted from t. lenoir, òshaping biomedicine as an information science,ó pp. 2745 in proceedings of the1998 conference on the history and heritage of science information systems, m. bowden, t. hahn, and r. williams, eds., asis monographseries, information today, inc., medford, nj, 1999, available at http://www.stanford.edu/dept/hpst/timlenoir/publications/lenoirbioasinfoscience.pdf. information for 2004 taken from protein data bank annual report 2004, available at http://www.rcsb.org/pdb/annualreport04.pdf.ncbi: material adapted from http://www.ncbi.nlm.nih.gov.bio grid: material adapted from http://www.eurogrid.org.neon: material adapted from http://www.nsf.gov/bio/neon/.lter: material adapted from the lter brochure, available at http://intranet.lternet.edu/archives/documents/publications/brochures/lterbrochure.pdf.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.232catalyzing inquirymany biological applications must access large amounts of data. furthermore, because of thecombinatorial nature of the exploration required in these applications (i.e., the relationships betweendifferent pieces of data is not known in advance and thus all possible combinations are a prioripossible), assumptions of locality that can be used to partition problems with relative ease (e.g., incomputational fluid dynamics problems) do not apply, and thus the amount of data exchange increases. one estimate of the magnitude of the dataintensive nature of a biological problem is that acomparison of two of the smallest human chromosomes using the best available dynamic programming algorithm allowing for substitutions and gaps would require hundreds of petabytes of memoryand hundredpetaflop processors.2thus, in supercomputers intended for biological applications, speed in computation and in communication are both necessaryñand many of todayõs supercomputing architectures are thus inadequatefor these applications.3 note that communications issues deal both with interprocessor communications (e.g., comparing sequences between processors, dividing long sequences among multiple processors) and traditional inputoutput (e.g., searching large sequence libraries on disk, receiving manyrequests at a time from the outside world). when problems involve large amounts of data exchange,communications become increasingly important.greater processing capability would enable the attack of many biologically significant problems.today, processing capability is adequate to sequence and assemble data from a known organism. tosome extent, it is possible to find genes computationally (as discussed in chapter 4), but the accuracy oftodayõs computationally limited techniques is modest. simulations of interesting biomolecular systemscan be carried out routinely for about hundreds of thousands of atoms for tens of nanoseconds. orderofmagnitude increases (perhaps even two or three orders of magnitude) in processing capability wouldenable great progress in problem domains such as protein folding (ab initio prediction of threedimensional structure from onedimensional sequence information), simulation methods based on quantummechanics that can provide more accurate predictions of the detailed behavior of interestingbiomolecules in solution,4 simulations of large numbers of interacting macromolecules for times ofbiological interest (i.e., for microseconds and involving millions of atoms), comparative genomics (i.e.,finding similar genetic sequences across the genomes of different organismsñthe multiple sequencealignment problem), proteomics (i.e., understanding the combinatorially large number of interactionsbetween gene products), predictive and realistic simulations of biological systems ranging from cells toecosystems), and phylogenetics (the reconstruction of historical relationships between species or individuals). box 7.3 provides some illustrative applications of highperformance computing in life sciencesresearch.any such estimate of the computing power needed to solve a given problem depends on assumptions about how a solution to that problem might be structured. different ways of structuring a problem2shankar subramanian, university of california, san diego, personal communication, september 24, 2003.3this discussion of communications issues is based on g.s. heffelfinger, òsupercomputing and the new biology,ó powerpointpresentation at the aaas annual meeting, denver, co, february 1318, 2003.4a typical problem might be the question of enzymes that exhibit high selectivity and high catalytic efficiency, and a detailedsimulation might well provide insight into the related problem of designing an enzyme with novel catalytic activity. simulationsbased on classical mechanics treat molecules essentially as charged masses on springs. these simulations (socalled moleculardynamics simulations) have had some degree of success, but lead to seriously inaccurate results where ions must interact inwater or when the breaking or forming of bonds must be taken into account. simulations based on quantum mechanics modelmolecules as collections of nuclei and electrons and entail solving of quantum mechanical equations governing the motion ofsuch particles; these simulations offer the promise of much more accurate simulations of these processes, although at a muchhigher computational cost. these comments are based on excerpts from a white paper by m. colvin, òquantum mechanicalsimulations of biochemical processes,ó presented at the national research councilõs workshop on the future of supercomputing, lawrence livermore national laboratory, santa fe, nm, september 2628, 2003. see also òbiophysical simulations enabledby the ultrasimulation facility,ó available at http://www.ultrasim.info/doedocs/biophysicsultrasimulationwhitepaper4103.pdf.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.cyberinfrastructure and data acquisition233box 7.3grand challenges in computational structural and systems biologythe onset of cancerit is well known that cancer develops when cells receive inappropriate signals to multiply, but the details ofcell signaling are not well understood. for example, activation of the epidermal growth factor signaling pathway is under the control of growth factors that bind to a receptor site on the exterior of a cell. binding of thereceptor initiates a cascade of protein conformational changes through the cell membrane, involving a complex rearrangement of many different proteins, including the ras enzyme. the ras enzyme is a molecularswitch that can initiate a cascade of protein kinases that in turn transfer the external signal to the cell nucleuswhere it controls cell proliferation and differentiation. disruption of this signaling pathway can have direconsequences as illustrated by the finding that mutations of the ras enzyme have been found in 30 percent ofhuman tumors. because computer simulations can provide atomiclevel detail that is difficult or impossible toobtain from experimental studies, computational studies are essential. however, this requires the modeling ofan extremely large complex of biomolecules, including bilayer lipid membranes, transmembrane proteins,and a complex of many intercellular kinases, and thousands of molecules of waters of solvation.environmental remediationmicrobes may be able to contribute to the cleanup of polluted sites by concentrating waste materials ordegrading them into nontoxic form. understanding the role of gramnegative bacteria in moderating subsurface reductionoxidation chemistry and the role of such systems in bioremediation technologies requires thestudy of how cell walls, including many transmembrane protein substituents, interact with extracellular mineral surfaces and solvated atomic and molecular species in the environment. simulations of these processesrequires that many millions of atoms be included.degradation of toxic chemical weaponscomputational approaches can be used for the rational redesign of enzymes to degrade chemical agents. anexample is the enzyme phosphotriesterase (pte), which could be used to degrade nerve gases. combinedexperimental and computational efforts can be used to develop a series of highly specific pte analogues,redesigned for optimum activity at specific temperatures, or for optimum stability and activity in nonaqueous,lowhumidity environments or in foams, for improved degradation of warfare neurotoxins. advanced computations can also facilitate the design of better reactivators of the enzyme acetylcholinesterase (ache) that canbe used as more efficient therapeutic agents against highly toxic phosphoester compounds such as the nervewarfare agents dfp (diisopropyl fluorophosphate), sarin, and soman and insecticides such as paraoxon. acheis a key protein in the hydrolysis of acetylcholine, and inhibition of ache through a phosphorylation reactionwith such phosphoesters can rapidly lead to severe intoxication and death.multiscale physiological modeling of the heartthe heart has a characteristic volume of around 60 cm3. at a resolution of 0.1 mm, a grid of some 6 × 107 cellsis required. if 100 variables are associated with each cell, 10 floating point operations are needed for eachtime step in a simulation, and the time resolution is around 1 ms (a single heartbeat has a duration around 1second), a computing throughput of 6 × 1013 floating point operations per second (60 teraflops) is necessary.in addition, a flexible and composable simulation infrastructure is required. for example, for a spatiallydistributed system, only a representative and relatively small subset of substructures can be represented in themodel explicitly, because it is not feasible to model all of them. contributions of the substructures missingfrom the model are inferred by an interpolative process. for practical purposes, it will not be known inadvance how much and what kinds of detail will be necessary for a useful simulation; the same a prioriignorance also characterizes the nature and extent of the communications required between different levels ofthe simulation. thus, the infrastructure must support easy experimentation in which different amounts ofdetail and different degrees of communication can be explored.source: the first three examples are adapted with minimal change from d.a. dixon, t.p. straatsma, and t. headgordon, ògrandchallenges in computational structural and systems biology,ó available at http://www.ultrasim.info/doedocs/escresponse.bio.dad.pdf.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.234catalyzing inquirysolution often result in different estimates for the required computing power, and for any complexproblem, the òbestó structuring may well not be known. (different ways of structuring a problem mayinvolve different algorithms for its solution, or different assumptions about the nature of the biologically relevant information.) furthermore, the advantage gained through algorithm advances, conceptual reformulations of the problem, or different notions about the answers being sought is often comparable to advantages from hardware advances, and sometimes greater. on the other hand, for decadescomputational scientists have been able to count on regular advances in computing power that accruedòfor free,ó and whether or not scientists are able to develop new ways of looking at a given problem,hardwarebased advances in computing are likely to continue.three types of computational problem in biology must be distinguished.5 problems such as proteinfolding and the simulation of biological systems are similar to other simulation problems that involvesubstantial amounts of ònumber crunching.ó a second type of problem entails largescale comparisonsor searches in which a very large corpus of datañfor example, a genomic sequence or a protein databaseñis compared against another corpus, such as another genome or a large set of unclassified proteinsequences. in this kind of problem, the difficult technical issues involve the lack of good software forbroadcast and parallel access disk storage subsystems. the third type of problem involves single instances of large combinatorial problems, for example, finding a particular path in a very large graph. inthese problems, computing time is often not an issue if the object can be modeled in the memory of themachine. when memory is too small, the user must write code that allows for efficient random access toa very large objectña task that significantly increases development time and even under the best ofcircumstances can degrade performance by an order of magnitude.the latter two types of problem often entail the consideration of large numbers of biological objects(cells, organs, organisms) characterized by high degrees of individuality, contingency, and historicity.such problems are typically found in investigations involving comparative and functional genomicsand proteomics, which generally involve issues such as discrete combinatorial optimization (e.g., themultiple sequence alignment problem) or pattern inference (e.g., finding clusters or other patterns inhighdimensional datasets). algorithms for discrete optimization and pattern inference are often nphard, meaning that the time to find an optimal solution is far too long (e.g., longer than the age of theuniverse) for a problem of meaningful size, regardless of the computer that might be used or that can beforeseen. since optimal solutions are not in general possible, heuristic approaches are needed that cancome reasonably close to being optimalñand a considerable degree of creativity is involved in developing these approaches.historically, another important point has been that the character of biological data is different fromthat of data in fields such as climate modeling. many simulations of nonbiological systems can becomposed of multiple repeating volume elements (i.e., a mesh that is well suited for finding floatingpoint solutions of partial differential equations that govern the temporal and spatial evolution of various field quantities). by contrast, some important biological data (e.g., genomic sequence data) arecharacterized by quantities that are better suited to integer representations, and biological simulationsare generally composed of heterogeneous objects. however, today, the difference in speed betweeninteger operations and floating point operations is relatively small, and thus the difference betweenfloating point and integer representations is not particularly significant from the standpoint of supercomputer design.finally, it is important to realize that many problems in computational biology will never be solvedby increased computational capability alone. for example, some problems in systems biology are combinatorial in nature, in the sense that they seek to compare òeverything against everythingó in a searchfor previously unknown correlations. search spaces that are combinatorially large are so large that even5the description of problem types in this paragraph draws heavily from g. myers, òsupercomputing and computationalmolecular biology,ó presented at the nrc workshop on the future of supercomputing, santa fe, nm, september 2628, 2003.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.cyberinfrastructure and data acquisition235with exponential improvements in computational speed, methods other than exhaustive search must beemployed as well to yield useful results in reasonable times.6the preceding discussion for the life sciences focuses on the largescale computing needs of thefield. yet these are hardly the only important applications of computing, and rapid innovation is likelyto require information technology on many scales. for example, researchers need to be able to exploreideas on local computers, albeit for scaleddown problems. only after smallerscale explorations areconducted do researchers have the savvy, the motivation, and the insight needed for meaningful use ofhighend cyberinfrastructure. researchers also need tools that can facilitate quick and dirty tasks, andworking knowledge of spreadsheets or perl programming can be quite helpful. for this reason, biologists working at all scales of problem size will be able to benefit from advances in and knowledge ofinformation technology.7.1.4 the role of networking as noted in chapter 3, biological data come in large quantities. highspeed networking (e.g., one ortwo orders of magnitude faster than that available today) would greatly facilitate the exchange ofcertain types of biological data such as highresolution imaging as well as enable realtime remoteoperation of expensive instrumentation. highspeed networking is critical for life science applicationsin which large volumes of data change or are created rapidly, such as those involving imaging or remoteoperation of instrumentation.7the internet2 effort also includes the middleware initiative (i2mi), intended to facilitate the creation of interoperable middleware infrastructures among the membership of internet2 and relatedcommunities.8 middleware generally consists of sets of tools and data that help applications use networked resources and services. the availability of middleware contributes greatly to the interoperabilityof applications and reduces the expense involved in developing those applications. i2mi developsmiddleware to provide services such as identifiers (labels that connect a realworld subject to a set ofcomputerized data); authentication of identity; directories that index elements that applications mustaccess; authorization of services for users; secure multicasting; bandwidth brokering and quality ofservice; and coscheduling of resources, coupling data, networking, and computing together.7.1.5 an example of using cyberinfrastructure for neuroscience researchthe biomedical informatics research network (birn) project is a nationwide effort by nationalinstitutes of health (nih)supported research sites to merge data grid and computer gridcyberinfrastructure into the workflows of biomedical research. the brain morphometry birn, one ofthe testbeds driving the development of birn, has undertaken a project that uses the new technologyby integrating data and analysis methodology drawn from the participating sites. the multisite imaging research in the analysis of depression (miriad) project (figure 7.1) applies sophisticated imageprocessing of a dataset of magnetic resonance imaging (mri) scans of a longitudinal study of elderlysubjects. the subjects include patients who enroll in the study with symptoms of clinical depressions6consider the following example. the human genome is estimated to have around 30,000 genes. if the exploration of interest isassumed to be 5 genes operating together, there are approximately 3 × 1020 possible combinations of 30,000 genes in sets of 5. ifthe assumption is that 6 genes may operate together, there are on the order of 1026 possible combinations (the number of possiblecombinations of n genes in groups of k is given by n!/(k!(n ð k)!), which for large n and small k reduces to nk/k!).7in the opposite extreme case, in which enormous volumes of data never change, it is convenient rather than essential to useelectronic or fiber links to transmit the informationñfor a small fraction of the cost of highspeed networks, media (or even entireservers!) can be sent by federal express more quickly than a highspeed network could transmit the comparable volume ofinformation. see, for example, jim gray et al., terascale sneakernet: using inexpensive disks for backup, archiving, and data exchange, microsoft technical report, mstr0254, may 2002, available at ftp://ftp.research.microsoft.com/pub/tr/tr200254.pdf.8see http://middleware.internet2.edu/overview/.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.236catalyzing inquiryand agematched controls. some of the depression patients go on to develop alzheimerõs disease (ad)and the goal of the miriad project is to measure the changes in brain images, specifically volumechanges in cortical and subcortical gray matter, that correlate with clinical outcome.of particular significance from the standpoint of cyberinfrastructure, the miriad project is distributed among four separate sites: duke university neuropsychiatric imaging research laboratory,brigham and womenõs hospital surgical planning laboratory, university of california, los angeleslaboratory of neuro imaging, and university of california, san diego birn. each of these sites hasresponsibility for some substantive part of the work, and the work would not be possible without thebirn infrastructure to coordinate it.dukearchivesuclaair registrationand lobar analysisbwhintensity normalizationand em segmentationdukeclinical analysis1234bwh probabilistic atlas(onetime transfer)ucsdsupercomputingmiriad data flow1. uploading of  retrospective date from  duke study2. lobar analysis and  registration of atlas  to subjects3. anatomical segmentation4. comparison to clinical  historyfigure 7.1steps in data processing in the birn miriad project.1. t2weighted and proton density (pd) mri scans from the duke university longitudinal study are loaded intothe birn data archive (data grid), accessible by members of the miriad group for analysis using the computerresources at the university of california, san diego (ucsd) and the san diego supercomputer center (computegrid).2. the laboratory of neuro imaging at the university of california, los angeles (ucla) performs a nonlinearregistration to define the threedimensional geometric mapping between each subject and a standard brain atlasthat encodes the probabilities of each tissue class at each location in the brain.3. the surgical planning laboratory at brigham and womenõs hospital (bwh) then applies an intensity normalization and expectationmaximization algorithm to combine the original image pixel intensities (t2 and pd)and the tissue probabilities to label each point in the images and to calculate the overall volumes of tissue classes.4. duke performs statistical tests on the imageprocessing results to assess the predictive value of the brainmorphometry measurements with respect to clinical outcome.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.cyberinfrastructure and data acquisition2377.2 data acquisition and laboratory automationas noted in chapter 3, the biology of the 21st century will be dataintensive across a wide range ofspatial and temporal scales. todayõs highthroughput data acquisition technologies depend onparallelization rather than on reducing the time needed to take individual data points. these technologies are capable of carrying out global (or nearly global) analyses, and as such they are well suited forthe rapid and comprehensive assessment of biological system properties and dynamics. indeed, in 21stcentury biology, many questions are asked because relevant data can be obtained to answer them.whereas earlier researchers automated existing manual techniques, todayõs approach is more orientedtoward techniques that match existing automation.7.2.1 todayõs technologies for data acquisition9some of todayõs data acquisition technologies include the following:10¥dna microarrays. microarray technology enables the simultaneous interrogations of a humangenomic sample for complete human transcriptomes, provided that the arrays do not contain onlyputative protein coding regions. the oligonucleotide microarray can identify singlenucleotide differences and distinguish mrnas from individual members of multigene families, characterize alternatively spliced genes, and identify and type alternative forms of singlenucleotide polymorphisms.microarrays are also used to observe in vitro proteindna binding events and to do comparativegenome hybridization (cgh) studies. box 7.4 provides a closeup of microarrays.¥automated dna sequencers. prior to automated sequencing, the sequencing of dna was performed manually, at many tens (up to a few hundred) of bases per day.11 in the 1970s, the developmentof restriction enzymes, recombinant dna techniques, gene cloning techniques, and polymerase chainreaction (pcr) contributed to increasing amounts of data on dna, rna, and protein sequences. morethan 140,000 genes were cloned and sequenced in the 20 years from 1974 to 1994, many of which werehuman genes. in 1986, an automated dna sequencer was first demonstrated that sequenced 250 basesper day.12 by the late 1980s, the nih genbank database (release 70) contained more than 74,000 sequences, while the swiss protein database (swissprot) included nearly 23,000 sequences. in addition,protein databases were doubling in size every 12 months. since 1999, more advanced models of automated dna sequencer have come into widespread use.13 today, a stateoftheart automated sequencercan produce on the order of a million base pairs of raw dna sequence data per day. (in addition,technologies are available that allow the parallel processing of 16 to 20 residues at a time.14 these enablethe determination of complete transcriptomes in individual cell types from organisms whose genome isknown.)¥mass spectroscopy. mass spectroscopy (ms) enables the inquantity identification and quantification of large numbers of proteins.15 used in conjunction with genomic information, ms information canbe used to identify and type singlenucleotide polymorphisms. some implementations of mass spec9section 7.2.1 is adapted from t. ideker, t. galitski, and l. hood, òa new approach to decoding life: systems biology,óannual review of genomics and human genetics 2:343, 2001.10adapted from t. ideker et al., òa new approach to decoding life,ó 2001.11l. hood and d.j. galas, òthe digital code of dna,ó nature 421(6921):444448, 2003.12l.m. smith, j.z. sanders, r.j. kaiser, p. hughes, c. dodd, c.r. connell, c. heiner, et al., òfluorescence detection in automated dna sequence analysis,ó nature 321(6071):674679, 1986. (cited in ideker et al., 2001.)13l. rowen, s. lasky, and l. hood, òdeciphering genomes through automated large scale sequencing,ó methods in microbiology, a.g. craig and j.d. hoheisel, eds., academic press, san diego, ca, 1999, pp. 155191. (cited in ideker et al., 2001.)14s. brenner, m. johnson, j. bridgham, g. golda, d.h. lloyd, d. johnson, s. luo, et al., ògene expression analysis by massively parallel signature sequencing (mpss) on microbead arrays,ó nature biotechnology 18(6):630634, 2000. (cited in ideker etal., 2001.)15j.k. eng, a.l. mccormack, and j.r.i. yates, òan approach to correlate tandem mass spectral data of peptides with amino acidsequences in a protein database,ó journal of the american society for mass spectrometry 5:976989, 1994. (cited in ideker et al., 2001.)catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.238catalyzing inquirytroscopy today allow 1,000 proteins per day to be analyzed in an automated fashion, and there is hopethat a nextgeneration facility will be able to analyze up to 1 million proteins per day.16¥cell sorters. cell sorters separate different cell types at high speed on the basis of multiple parameters. while microarray experiments provide information on average levels of mrna or protein withina cell population, the reality is that these levels vary from cell to cell. knowing the distribution ofexpression levels across cell types provides important information about the underlying control mechanisms and regulatory network structure. a stateoftheart cell sorter can separate 30,000 elements persecond according to 32 different parameters.17box 7.4microarrays: a closeupa òclassicaló microarray typically consists of singlestranded pieces of dna from virtually an entire genomeplaced physically in tiny dots on a flat surface and labeled with a fluorescent dye. (lithographic techniquesused to develop semiconductor chips are now used to deposit the dna on a silicon chip that can later be readoptically.) in a microarray experiment, messenger rna (mrna) from a cell of interest is extracted and placedin contact with the prepared surface. if the sample contains mrna corresponding to the dna on one or moreof the dots on the surface, the molecules will bind and the dye will fluoresce. because the mrna representsthe fraction of genes from the sample that have been transcribed from dna into mrna, the resulting fluorescent dots on the surface are a visual indicator of gene expression (or transcription) in the cellõs genome.different intensities of the dots reflect greater or lesser levels of transcription of particular genes.obtaining the maximum value from a microarray experiment depends on the ability to correlate the data froma microarray experiment per se with extensive data that identify or classify the genes by other characteristics.in the absence of such data, any given microarray experiment merely points out the fact that some genes areexpressed to a greater extent than others in a particular experimental situation.protein microarrays can identify proteinprotein (and proteindrug) interactions among some 10,000 proteinsat once.1 as described by templin,2[protein] microarray technology allows the simultaneous analysis of thousands of parameters within a single experiment. microspots of capture molecules are immobilized in rows and columns onto a solid support and exposed tosamples containing the corresponding binding molecules. readout systems based on fluorescence, chemiluminescence, mass spectrometry, radioactivity or electrochemistry can be used to detect complex formation within eachmicrospot. such miniaturized and parallelized binding assays can be highly sensitive, and the extraordinary powerof the method is exemplified by arraybased gene expression analysis. in these systems, arrays containing immobilized dna probes are exposed to complementary targets and the degree of hybridization is measured. recentdevelopments in the field of protein microarrays show applications for enzymesubstrate, dnaprotein and differenttypes of proteinprotein interactions. here, we discuss theoretical advantages and limitations of any miniaturizedcapturemoleculeligand assay system and discuss how the use of protein microarrays will change diagnostic methods and genome and proteome research.1see g. macbeath and s.l. schreiber, òprinting proteins as microarrays for highthroughput function determination,ó science 289(5485):17601763, 2000.2reprinted by permission from m.f. templin, d. stoll, m. schrenk, p.c. traub, c.f. vohringer, and t.o. joos, òprotein microarraytechnology,ó trends in biotechnology 20(4):160166, 2002. copyright 2002 elsevier.note: an overview of microarray technology is available on a private web site created by leming shi: http://www.genechips.com/. seealso http://www.genome.gov/10000533 and p. gwynne and g. page, òmicroarray analysis: the next revolution in molecular biology,óspecial advertising supplement, science 285, august 6, 1999, available at http://www.sciencemag.org/feature/emarket/benchtop/micro.shl.16s.p. gygi, b. rist, s.a. gerber, f. turecek, m.h. gelb, and r. aebersold, òquantitative analysis of complex protein mixturesusing isotopecoded affinity tags,ó nature biotechnology 17(10):994999, 1999. (cited in ideker et al., 2001.)17see, for example, http://www.systemsbiology.org/default.aspx?pagename=cellsorting.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.cyberinfrastructure and data acquisition239¥microfluidic systems. microfluidic systems, also known as microtas (total analysis system), allow the rapid and precise measurement of sample volumes of picoliter size. these systems put onto asingle integrated circuit all stages of chemical analysis, including sample preparation, analyte purification, microliquid handling, analyte detection, and data analysis.18 these òlabonachipó systems provide portability, higherquality and higherquantity data, faster kinetics, automation, and reduction ofsample and reagent volumes.¥embedded networked sensor (ens) systems. ens systems are largescale, distributed systems, composed of smart sensors embedded in the physical world, that can provide data about the physical worldat unprecedented granularity. these systems can monitor and collect large volumes of information atlow cost on such diverse subjects as plankton colonies, endangered species, and soil and air contaminants. across a wide range of largescale biological applications broadly cast, these systems promise toreveal previously unobservable phenomena. box 7.5 describes some applications of ens systems.finally, a specialized type of data acquisition technology is the hybrid measurement device thatinteracts directly with a biological sample to record data from it or to interact with it. as one illustration, contemporary tools for studying neuronal signaling and information processing include implantable probe arrays that record extracellularly or intracellularly from multiple neurons simultaneously.18see, for example, http://www.eurobiochips.com/euro2002/html/agenda.asp. to illustrate the difficulty, consider the handling of liquids. dilution ratios required for a process may vary by three or four orders of magnitude, and so an early challenge(now largely resolved successfully) is the difficulty of engineering an automated system that can dispense both 0.1microliter and1milliliter volumes with high accuracy and in reasonable time periods.box 7.5applications of embedded network sensor systemsmarine microorganisms1marine microorganisms such as viruses, bacteria, microalgae, and protozoa have a major impact on theecology of the coastal ocean; present public health issues for coastal human populations as a consequence ofthe introduction of pathogenic microorganisms into these waters from land runoff, storm drains, and sewageoutflow; and have the potential to contaminate drinking water supplies with harmful, pathogenic, or nuisancemicrobial species.today, the environmental factors that stimulate the growth of such microorganisms are still poorly understood. to understand these factors, scientists need to correlate environmental conditions with microorganismal abundances at the small spatial and temporal scales that are relevant to these organisms. for a variety oftechnological and methodological reasons, sampling the environment at the necessary high resolution andidentifying microorganisms in situ in nearreal time has not been possible in the past.habitat sensing2understanding in detail the environmental, organismal, and cultural conditions, and the interactions betweenthem, in natural and managed habitats is a problem of considerable biological complexity. data must becaptured and integrated across a wide range of spatial and temporal scales for chemical, physiological, ecological, and environmental purposes. for example, data of interest might include microclimate data; a videoof avian behavioral activities related to climate, nesting, and reproduction; and data on soil moisture, nitrate,co2, temperature, and rootfungi activities in response to weather.1adapted from http://www.cens.ucla.edu/portal/aquaticmicrobialobservingsyst.html.2adapted from http://deerhound.ats.ucla.edu:7777/portal/page?pageid=54,42365,5442372&dad=portal&schema=portal.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.240catalyzing inquirysuch arrays have been used in moths (manduca sexta) and sea slugs (tritonia diomeda) and, when linkeddirectly to the electronic signals of a computer, essentially record and simulate the neural signalingactivity occurring in the organism. box 7.6 describes the dynamic clamp, a hybrid measurement devicethat has been invaluable in probing the behavior of neurons. research on this interface will serve bothto reveal more about the biological system and to represent that system in a format that can becomputed.box 7.6the dynamic clampthe dynamic clamp is a device that mimics the presence of a membrane or synapse proximate to a neuron.that is, the clamp essentially simulates the electrical conductances in the network to which a neuron isostensibly connected. during clamp operation, the membrane potential of a neuron is continuously measuredand fed into a computer. the dynamic clamp program contains a mathematical model of the conductance tobe simulated and computes the current that would flow through the conductance as a function of time. thiscurrent is injected into the neuron, and the cycle of membrane potential measurement, current computation,and current injection continues. this cycle enables researchers to study the effects of a membrane current orsynaptic input in a biological cell (the neuron) by generating a hybrid system in which the artificial conductance interacts with the natural dynamic properties of the neuron.the dynamic clamp can be used to mimic any voltagedependent conductance that can be expressed in amathematical model. depending on the type of conductance, most applications can be grouped in one of thefollowing categories:1.generating artificial membrane conductances. these may be voltage dependent or independent.2.simulating natural stimuli. the dynamic clamp can mimic natural conditions such as barrages of synapticinputs to neurons in silent brain slices. here, an artificial synaptic conductance trace is used to compute anartificial synaptic current from the momentary membrane potential of the postsynaptic neuron. that current iscontinuously injected into the neuron, and the effect of the artificial input on the activity of the neuron isassessed.3.generating artificial synapses. in a configuration where the dynamic clamp computer monitors the membrane potential of several neurons and computes and injects current through several output channels, thedynamic clamp can be used to create artificial chemical or electrotonic synaptic connections between neurons that are not connected in vivo or to modify the strength or dynamics of existing synaptic connections.4.coupling of biological and model neurons. the dynamic clamp can also be used to create hybrid circuitsby coupling model and biological neurons through artificial synapses. in this application, the dynamic clampcomputer continuously solves the differential equations that describe the model neuron and the synapses thatconnect it to the biological neuron.the first application of the dynamic clamp involved the stimulation of a gammaaminobutyric acid (gaba)response in a cultured stomatogastric ganglion neuron. this application illustrated that the dynamic clampeffectively introduces a conductance into the target neuron. demonstration of an artificial voltagedependentconductance resulted from simulation of the action of a voltagedependent proctolin response on a neuron inthe intact stomatogastric ganglion, which showed that shifts in the activation curve and the maximal conductance of the response produced different effects on the target neuron. lastly, the dynamic clamp was used toconstruct reciprocal inhibitory synapses between two stomatogastric ganglion neurons that were not couplednaturally, illustrating that the dynamic clamp could be used to simulate new networks at will.source: the description of a dynamic clamp is based heavily on a.a. prinz, òthe dynamic clamp a decade after its invention,ó axoninstruments newsletter 40, february 2004, available at http://www.axon.com/axobits/axobits40.pdf. the description of the first applicationof the dynamic clamp is nearly verbatim from a.a. sharp, m.b. oõneil, l.f. abbott, and e. marder, òdynamic clamp: computergeneratedconductances in real neurons,ó journal of neurophysiology 69(3):992995, 1993.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.cyberinfrastructure and data acquisition2417.2.2 examples of future technologiesas powerful as these technologies are, new instrumentation and methodology will be needed in thefuture. these technical advances will have to reduce the cost of data acquisition by several orders ofmagnitude.consider, for example, the promise of genomically individualized medical care, which is based onthe notion that treatment and/or prevention strategies for disease can be customized to groups ofindividuals smaller than the entire population, and perhaps ultimately groups as small as one. becausethese groups will be identified in part by particular sets of genomic characteristics, it will be necessaryto undertake the genomic sequencing of these individuals. the first complete sequencing of the humangenome took 13 years and $2.7 billion. for broad use in the population at large, the cost of assemblingand sequencing a human genome must drop to hundreds or thousands of dollarsña reduction in costof 105 or 106 that would enable the completion of a human genome at such cost in a matter of days.19computation per se is expected to continue to drop in cost in accordance with mooreõs law at leastover the next decade. but automation of data acquisition will also play an enormous role in facilitatingsuch cost reductions. for example, the laboratory of richard mathies at the university of california,berkeley, has developed a 96lane microfabricated dna sequencer capable of sequencing at a rate of1,700 bases per minute.20 using this technology, the complete sequencing of an individual 3billion basegenome would take 1,000 sequencerdays. future versions will incorporate higher degrees of parallelism.similar advances in technology will help to reduce the cost of other kinds of biological research aswell. a number of biological signatures useful for functional genomics have been susceptible to significantly greater degrees of automation, miniaturization, and multiplexing; these signatures are associatedwith electrophoresis, molecular microarrays, mass spectrometry, and microscopy.21 electrophoresis,molecular microarrays, and mass spectrometry provide more opportunities for multiplexed measurement (i.e., the simultaneous measurement of signatures from many molecules from the same source).such multiplexing can reduce errors due to misalignment of unmultiplexed measures in space and/ortime.in general, the biggest payoffs in laboratory automation are those efforts that can address processesthat involve physical material. much work in biology involves multiple laboratory procedures that eachcall for multiple fluid transfers, heating and cooling cycles, and mechanical operations such as centrifuging, waiting, and imaging. when these procedures can be undertaken òonchip,ó they reduce theamount of human interaction involved and thus the associated time and cost.in addition, the feasibility of lab automation is closely tied to the extent to which human craft can betaken out of lab work. that is, because so much lab work must be performed by humans, the skills of theparticular individuals involved matter a great deal to the outcomes of the work. a particular individualmay be the only one in a laboratory with a òknackó for performing some essential laboratory procedure(e.g., interpretation of certain types of image, preparation or certain types of sample) with high reliability, accuracy, and repeatability.19l.m. smith, j.z. sanders, r.j. kaiser, p. hughes, c. dodd, c.r. connell, c. heiner, et al., òfluorescence detection in automated dna sequence analysis,ó nature 321(6071):674679, 1986; l. hood and d. galas, òthe digital code of dna,ó nature421(6921):444448, 2003. note that done properly, the second complete sequencing of a human being would be considerably lessdifficult. the reason is that every member of a biological species has a dna that is almost identical to that of every othermember. in humans, the difference between dna sequences of different individuals is about one base pair per thousand. (seespecial issues on the human genome: science 291(5507) february 16, 2001; nature 409(6822), february 15, 2001.) so, assuming it isknown where to check for every difference, a reduction in effort of at least a factor of 103 is obtainable in principle.20b.m. paegel, r.g. blazej, and r.a. mathies, òmicrofluidic devices for dna sequencing: sample preparation and electrophoretic analysis,ó current opinion in biotechnology 14(1):4250, 2003, available at http://www.wtec.org/robotics/usworkshop/june22/papermathiesmicrofluidicssampleprep2003.pdf.21g. church, òhunger for new technologies, metrics, and spatiotemporal models in functional genomics,ó available athttp://recomb2001.gmd.de/abstracts/church.html.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.242catalyzing inquirywhile reliance on individuals with specialized technical skills is often a workable strategy for anacademic laboratory, it makes much less sense for any organization interested in largescale production.for largescale, costeffective production, process automation is a sine qua non. when a process can beautomated, it is generally faster to perform, more free from errors, more accurate, and less expensive.22some of the clearest success stories involve genomic technologies. for example, dna sequencingwas a craft at the start of the 1990sñtoday, automated dna sequencing is common, with instrumentsto undertake such sequencing in high volume (a million or more base pairs per day) and even acommercial infrastructure to which sequencing tasks can be outsourced. nevertheless, a variety ofadvanced sequencing technologies are being developed, primarily with the intent of lowering the costof sequencing by another several orders of magnitude.23an example of such a technology is pyrosequencing, which has also been called òsequencing bysynthesis.ó24 with pyrosequencing, the dna to be sequenced is denatured to form a single strand andthen placed in solution with a set of selected enzymes. in a cycle of individual steps, the dnaenzymesolution is mixed with deoxynucleotide triphosphate molecules containing each of the four bases. whenthe base that is the complement to the next base on the target strand is added, the added base joins aforming complement strand and releases a pyrophosphate molecule. that molecule starts a reactionthat ends with luciferin emitting a detectable amount of light. thus, by monitoring the light output ofthe reaction (for example, with a ccd camera), it is possible to observe in real time which of the fourbases has successfully matched.454 life sciences has applied pyrosequencing to wholegenome analyses by taking advantage of itshigh parallelizability. using a picotiter plate, a microfluidic system performs pyrosequencing on hundreds of thousands of dna fragments simultaneously. custom software analyzes the light emitted andstitches together the complete sequence. this approach has been used successfully to sequence thegenome of an adenovirus,25 and the company is expected to produce commercial hardware to performwholegenome analysis in 2005.a second success story is microarray technology, which historically has relied heavily on electrophoretic techniques.26 more recently, techniques have been developed that do away entirely withelectrophoresis. one approach relies instead on microbeads with different messenger rnas on theirsurfaces (serving as probes to which targets bind selectively) and a novel sequencing procedure to22the same can be said for many other aspects of lab work. in 1991, walter gilbert noted, òthe march of science devises evernewer and more powerful techniques. widely used techniques begin as breakthroughs in a single laboratory, move to being usedby many researchers, then by technicians, then to being taught in undergraduate courses and then to being supplied as purchased servicesñor, in their turn, superseded. . . . fifteen years ago, nobody could work out dna sequences, today everymolecular scientists does so and, five years from now, it will all be purchased from an outside supplier. just this happened withrestriction enzymes. in 1970, each of my graduate students had to make restriction enzymes in order to work with dnamolecules; by 1976 the enzymes were all purchased and today no graduate student knows how to make them. once one had tosynthesize triphosphates to do experiments; still earlier, of course, one blew oneõs own glassware.ó see w. gilbert, òtowards aparadigm shift in biology,ó nature 349(6305):99, 1991.23a review by shendure et al. classifies emerging ultralowcost sequencing technologies into one of five groups: microelectrophoretic methods (which extend and incrementally improve todayõs mainstream sequencing technologies first developed byfrederick sanger); sequencing by hybridization; cyclic array sequencing on amplified molecules; cyclic array sequencing onsingle molecules; and noncyclical, singlemolecule, realtime methods. the article notes that most of these technologies are still inthe relatively early stages of development, but that they each have great potential. see j. shendure, r.d. mitra, c. varma, andg.m. church, òadvanced sequencing technologies: methods and goals,ó nature reviews: genetics 5(5):335344, 2004, available athttp://arep.med.harvard.edu/pdf/shendure04.pdf. pyrosequencing, provided as an example of one new sequencing technology, is an example of cyclic array sequencing on amplified molecules.24m. ronaghi, òpyrosequencing sheds light on dna sequencing,ó genome research 11(1):311, 2001.25a. strattner, òfrom sanger to ôsequenatorõ,ó bioit world, october 10, 2003.26genes are expressed as proteins, and these proteins have different weights. electrophoresis is a technique that can be used todetermine the extent to which proteins of different weights are present in a sample.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.cyberinfrastructure and data acquisition243identify specific microbeads.27 each bead can be interrogated in parallel, and the abundance of a givenmessenger rna is determined by counting the number of beads with that mrna on their surfaces. inaddition to greatly simplifying the samplehandling procedure, this technique has two other importantadvantages: a direct digital readout of relative abundances (i.e., the bead counts) and throughputincreases by more than a factor of 10 compared to other techniques.a second approach to the elimination of electrophoresis is known as optical mapping or sequencing(box 7.7). optical mapping eliminates dependence on ensemblebased methods, focusing on the statistics of individual dna molecules. although this technique is fragile and, to date, not replicable inmultiple laboratories,28 it may eventually be capable of sequencing entire genomes much more rapidlythan is possible today.a different approach based on magnetic detection of dna hybridization seeks to lower the cost ofperforming microarray analysis. chen et al. have suggested that instead of tagging targets with fluorescent molecules, targets are tagged with microscopic magnetic beads.29 probes are implanted on amagnetically sensitive surface, such as a floppy disk, after removing the magnetic coating at the probe27s. brenner, ògene expression analysis by massively parallel signature sequencing (mpss) on microbead arrays,ó naturebiotechnology 18(6):630634, 2002. the elimination of electrophoresis (a common laboratory technique for separating biologicalsamples by molecular weight) has many practical benefits. conceptually, electrophoresis is a straightforward process. a taggedbiological sample is inserted into a viscous gel and then subjected to an external electric field for some period of time. the sampledifferentiates in the electric field because the lighter components move farther under the influence of the electric field than theheavier ones. the tag on the biological sample is, for example, a compound that fluoresces when exposed to ultraviolet light.measuring the intensity of the fluorescence provides an indication of the relative abundances of components of different molecular weight. however, in practice there are difficulties. the gel must undergo appropriate preparationñno small task. for example, the gel must be homogeneous, with no bubbles to interfere with the natural movement of the sample components. thetemperature of the gelsample combination may be important, because the viscosity of the gel may be temperaturesensitive.while the gel is drying (a process that takes a few hours), it must not be physically disturbed in a way that introduces defects intothe gel preparation.28bud mishra, new york university, personal communication, december 2003.29c.h.w. chen, v. golovlev, and s. allman, òinnovative dna microarray hybridization detection technology,ó poster abstract presented at human genome meeting 2002, april 1417, 2004, shanghai, china; also, òdetection of polynucleotides onsurface of magnetic media, available at http://www.scientec.com/news1.htm.box 7.7on optical mappingoptical mapping is a single molecule based physical mapping technology, which creates an ordered restriction mapby enumerating the locations of occurrences of a specific òrestriction patternó along a genome. thus, by locating thesame patterns in the sequence reads or contigs, optical maps can detect errors in sequence assembly, and determinethe phases (i.e., chromosomal location and orientation) of any set of sequence contigs. since the input genomic datathat can be collected from a single dna molecule by the best chemical and optical methods (such as those used inoptical mapping) are badly corrupted by many poorly understood noise processes, this type of technology derivesits utility through powerful probabilistic modeling used in experiment design and bayesian algorithms that canrecover from errors by using redundant data. in this way, optical mapping with gentig, a powerful statistical mapassembly algorithm invented and implemented by the authors, has proven instrumental in completing many microbial genomic maps (escherichia coli, yersinia pestis, plasmodium falciparum, deinococcus radiodurans, rhodobacter sphaeroides, etc.) as well as clone maps (daz locus of y chromosome).source: t. anantharaman and b. mishra, genomics via optical mapping (i): 01 laws for single molecules, s. yancopoulos, ed., oxforduniversity press, oxford, england, 2005, in press.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.244catalyzing inquirylocation, and different probes are attached to different locations. readout of hybridized probetargetpairs is accomplished through the detection of a magnetic signal at given locations; locations withoutsuch pairs provide no signal because the magnetic coating of the floppy disk has been removed fromthose locations. also, the location of any given probetarget pair is treated simply as a physical addresson the floppy disk. preliminary data suggest that with the spatial resolution currently achieved, a singlefloppy diskette can carry up to 45,000 probes, a figure that compares favorably to that of most glassmicroarrays (of order 10,000 probes or less). chen et al. argue that this approach has two advantages:greater sensitivity and significantly lower cost. the increased sensitivity is due to the fact that signalstrength is controlled by the strength of the beads rather than the amount of hybridizing dna per se;and so, in principle, this approach could detect even a single hybridization event. lower costs arguablyresult from the fact that the most of the components for magnetic detection are massproduced inquantity for the personal computer industry today.laboratory robotics is another area that offers promise of reduced labor costs. for example, theminimization of human intervention is illustrated by the introduction of compact, userprogrammablerobot arms in the early 1980s.30 one version, patented by the zymark corporation, equipped a robotarm with interchangeable hands. this arm was the foundation of robotic laboratory workstations thatcould be programmed to carry out multistep sample manipulations, thus allowing them to be adaptedfor different assays and samplehandling approaches.building on the promise offered by such robot arms, a testbed laboratory formed in the 1980s by dr.masahide sasaki at the kochi medical school in japan demonstrated the feasibility of a high degree oflaboratory automation: robots carried test tube racks, and conveyor belts transported patient samples tovarious analytical workstations. automated pipettors drew serum from samples for the required assays. onearmed stationary robots performed pipetting and dispensing steps to accomplish preanalyticalprocessing of higher complexity. the laboratory was able to perform all clinical laboratory testing for a600bed hospital with a staff of 19 employees. by comparison, hospitals in the united states of similarsize required up to 10 times as many skilled clinical laboratory technologists.adoption of the òtotal laboratory automationó approach was mixed. many clinical laboratories inparticular found that it provided excess capacity whose costs could not be recovered easily. midsizedhospital laboratories had a hard time justifying the purchase of multimilliondollar systems. by contrast, pharmaceutical firms invested heavily in robotic laboratory automation, and automated facilitiesto synthesize candidate drugs and to screen their biological effects provided three to fivefold increasesin the number of new compounds screened per unit time.in recent years, manufacturers have marketed òmodularó laboratory automation products, including modules for specimen centrifugation and aliquoting, specimen analysis, and postanalytical storageand retrieval. while such modules can be assembled like building blocks into a system that providesvery high degrees of automation, they also enable a laboratory to select the module or modules that bestaddress its needs.even mundane but humanintensive tasks are susceptible to some degree of automation. considerthat much of biological experimentation depends on the availability of mice as test subjects. mice needto be housed and fed, and thus require considerable human labor. the stowers institute for medicalresearch in kansas city has approached this problem with the installation of an automated mouse carefacility involving two robots, one of which dumps used mouse bedding and feeds it to a conveyorwashing machine and the other of which fills the clean cages with bedding and places them on a rack.31these robots can process 350 cages per hour and reduce the labor needs of cleaning cages by a factor ofthree (from six technicians to two). at a cost of $860,000, the institute expects to recoup its investment in30j. boyd, òrobotic laboratory automation,ó science 295(5554):517518, 2002. much of the discussion of laboratory automationis based on this article.31c. holden, ed., òhightech mousekeeping,ó science 300(5618):421, 2003.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.cyberinfrastructure and data acquisition2456 years, with much of the savings coming from reduced repetitive motion injuries and fewer healthproblems caused by allergen exposure.in the future, modularization is likely to continue. in addition, fewer standalone robot arms arebeing used because the robotics necessary for sampling from conveyor belts are often integrated directly into clinical analyzers. attention is turning from the development of hardware to the design ofprocess control software to control and integrate the various automation components; to manage thetransport, storage, and retrieval of specimens; and to support automatic repeat and followup testingstrategies.7.2.3 future challengesfrom a conceptual standpoint, automation for speed depends on two thingsñspeeding up anindividual process and processing many samples in parallel. individual processes can be speeded up tosome extent, but because they are limited by physical time constants (e.g., the time needed to mix asolution uniformly, the time needed to dry, the time needed to incubate), the speedups possible arelimitedñperhaps factors of a few or even ten can be possible. by contrast, parallel processing is a muchbigger winner, and it is easy to imagine processing hundreds or even thousands of samples simultaneously.in addition to quantitative speedups, qualitatively new data acquisition techniques are needed aswell. the difficulty of collecting meaningful data from biological systems has often constrained the levelof complexity at which to collect data. biologists often must use indirect or surrogate measures thatimply activity. for example, oxygen consumption can be used as a surrogate for breathing.there is a need to develop new mechanisms to collect data, particularly mechanisms that can forma bridge from the living system to a computer system, in other words, tools that detect and monitorbiological events and directly collect and store information about those events for later analysis. challenges in this area include the connection of cellular material, cells, tissues, and humans to computersfor rapid diagnostics and data download, bioaided computation, laboratory study, or humancomputer interactivity, and how to perform òsmartó experiments that use models of the biological systemsto probe the biology dynamically so that measurements of the spatiotemporal dynamics of living cells atmany scales become possible.a good example of future data acquisition challenges is provided by singlecell assays and singlemolecule detection. traditional assays can involve thousands or tens of thousands of cells and producedatasets that reflect the aggregate behavior of the entire sample. while for many types of experimentsthis is an appropriate approach, there are current and future biological research issues for which thisdoes not provide sufficient resolution. for example, cells within a population may be in different stagesof their life cycle, may be experiencing local variations of environmental conditions, or may be ofentirely different types. alternatively, a probe might not touch the cell type of interest, due to inadequate purification of a sample drawn from a subject that contains many cell types.32 for some biological questions, there is simply not a sufficient supply of cells of interest; for example, certain humannervous system tissue is highly specialized, and a biological inquiry may concern only a few cells.similarly, in attempts to isolate some diseases, there may be only a few, or even only one, affected cellñfor example, in attempts to detect cancerous cells before they develop into a tumor.many technologies offer approaches to analyzing and characterizing the behavior of single cells,including the use of mass spectrometry, microdissection, laserinduced fluorescence, and electrophoresis. ideally, it would be possible to monitor the behavior of a living cell over time with sufficientresolution to determine the functioning of subcellular components at different stages of the life cycleand in response to differing environmental stimuli.32today, this issue is addressed by the very laborintensive process of òpluckingó individual cells from a sample and aggregating themña process that typically requires 104 to 105 cells when todayõs assays are used.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.246catalyzing inquirya further challenge in ultrasensitive data acquisition in living cells is that the substances of interest,particularly proteins, occur at a wide range of concentrations (varying by many orders of magnitude).for many important proteins, this may be as few as hundreds of individual molecules. detection andanalysis at such low levels must work even in the face of wide statistical fluctuation, transient modifications, and a wide range of physical and chemical properties.33at the finest grain, detection and analysis of single molecules could provide further understandingof cellular mechanisms. again, although there are current techniques to analyze molecular structure(such as nuclear magnetic resonance and xray crystallography), these work on large, static samples. toachieve more precise understanding of cellular mechanisms, it is necessary to detect the presence andactivity of very small concentrations, even single molecules, dynamically within living cells. makingprogress in this field will require advances in chemistry, instrumentation, sensors, and image analysisalgorithms.34embedded networked sensor (ens) systems will ride the cost reduction curve that characterizesmuch of modern electronic systems. based on microsensors, onboard processing, and wireless communications, ens systems can monitor phenomena òup close.ó nevertheless, taken as a whole, ens systems present challenges with respect to longevity, autonomy, scalability, performance, and resilience.for example, offtheshelf sensors embedded in heterogeneous soil for monitoring soil moisture andnitrate levels raise issues related to calibration when embedded in a previously unknown environment.in addition, the uncertainty in the data they provide must be characterized. interesting theoreticalissues arise with respect to the statistical and informationtheoretic foundations for adaptive samplingand data fusion. also, of course, programming abstractions, common services, and tools for programming the network must be developed.to illustrate a specific application, consider some of the computing challenges in deploying enssystems for marine microorganisms. the ultimate goal is to deploy large groups of autonomous, mobilemicrorobots capable of identifying and tracking microorganisms in real time in the marine environment, while measuring the relevant environmental conditions at the required temporal and spatialscales. sensors must be mobile to track microorganisms and assess their abundance with a reasonablenumber of sensors. they must be small, so that they are able to gather information at a spatial scalecomparable to the size of the microorganisms and to avoid disturbing them. they must operate in aliquid environmentñcombined with small sensor size, operation in such an environment raises manydifficult issues of mobility, communications, and power, which in turn strongly impact network algorithms and strategies. also, sensors must be capable of in situ, realtime identification of microorganisms, which requires the development of new sensors with considerable onboard processing capability.progress in this applicationñmonitoring marine environments and singlecell identificationñis expected to be applicable to other liquid environments, such as the circulatory system of higher organisms, including humans.33r.d. smith et al., òapplication of new technologies for comprehensive, quantitative and high throughput microbialproteomics,ó abstracts of the department of energyõs (doe) genomes to life systemsbiology projects on microbes sequencedby the u.s. doeõs microbial genome program, available at http://doegenomestolife.org/pubs/2004abstracts/html/techdev.shtml#vpid289.34see, for example, the text of the nih program announcement pa01049, òsingle molecule detection and manipulation,óreleased february 12, 2001, available at http://grants.nih.gov/grants/guide/pafiles/pa01049.html.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.biological inspiration for computing2472478biological inspiration for computingchapters 47 address ways in which computer science and engineering can assist in the pursuit of abroadly defined research agenda in biology. this chapter suggests how insights from the biologicalsciences may have a positive impact on certain research areas in computing, although the impact of thisreversed direction is at present much more speculative.18.1the impact of biology on computing8.1.1biology and computing: promise and skepticismtodayõs computer systems are highly complex and often fragile. although they provide high degrees of functionality to their users, many of todayõs systems are also subject to catastrophic failure,difficult to maintain, and full of vulnerabilities to outside attack. an important goal of computing is tobe able to build systems that can function with high degrees of autonomy, robustly handle data withlarge amounts of noise, configure themselves automatically into networks (and reconfigure themselveswhen parts are damaged or destroyed), rapidly process large amounts of data in a massively parallelfashion, learn from their environment with minimal human intervention, and òevolveó to become betteradapted to what they are supposed to do.there is little doubt that such computer systems with these properties would be highly desirable.although the development of such systems is an active area of computer science research today (indeed, the internet itself is an example of a system that is capable of operating without centralizedauthority and reconfiguring itself when parts are damaged), computer science researchers are workingto develop new such systems, and the prospect of looking outside the existing computer science toolboxfor new types of hardware, software, algorithms, or something entirely different (and unknown) isincreasingly attractive.one possible area of research focuses on a set of techniques inspired by the biological sciences,because biological organisms often exhibit properties that would be desirable in computer systems.1a popularized account of biological inspiration for computing is n. forbes, imitation of life: how biology is inspiring computing, mit press, cambridge, ma, 2004.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.248catalyzing inquirythey function with high degrees of autonomy. some biological entitiesñsuch as neurons in a brainñcan configure themselves automatically into networks (and reconfigure themselves to some degreewhen parts are damaged or destroyed). sensory systems rapidly pick out salient features buried in largeamounts of data. many animals learn from their environment and become better adapted to what theyare supposed to do. all biological organisms have mechanisms for selfrepair, and all multicellularorganisms grow from an initial state that is much less phenotypically complex than their final states.carver mead once noted that òengineers would be foolish to ignore the lessons of a billion years ofevolution.ó the solutions that nature has evolved to difficult engineering problems are, in many cases,far beyond presentday engineering capability. for example, the human brain is not fast enough toprocess all of the raw sensory data detected by the optic or auditory nerves into meaningful information. to reduce processing load, the brain uses a strategy we know as òattentionó that focuses on certainparts of the available information and discards other parts. such a strategy might well be useful for anartificial machine processing a large visual field. studies of the way in which humans limit their attention has led to computational models of the strategy of shifting attention. such models of biologicalsystems are worth studying even if they appear intuitively less capable than computation, if only for thefact that no machine systems exist that can function as autonomously as a housefly or an ant.on the other hand, biological organisms operate within a set of constraints that may limit theirsuitability as sources of inspiration for computing. perhaps the most important constraint is the fact thatbiological organisms emerge from natural selection and the evolutionary process. because selectionpressures are multidimensional, biological systems must be multifunctional. for example, a biologicalsystem may be able to move, but it has also evolved to be able to feed itself, to reproduce, and to defenditself. the list of desirable functions in a biological system is long, and successfully mimicking biologyfor one particular function requires the ability to separate nonrelevant parts of the system that do notcontribute to the desired function. furthermore, because biological systems are multifunctional, theycannot be optimized for any one function. that is, their design always represents a compromise between competing goals. organisms must be adequately (rather than optimally) adapted to their environments. (the notion of òoptimal designó is also somewhat problematic in the context of stochasticrealworld environments.) also, optimal adaptation to any one environment is likely to disadvantagean organism in a significantly different environment, and so adequately adapted organisms tend to bemore robust across a range of environments.the evolutionary process constrains biological solutions as well. for example, biological systemsinevitably include vestiges of genetic products and organs that are irrelevant to the organism in itscurrent existence. thus, biological adaptation to a given environment depends not only on the circumstances of the environment but also on its entire evolutionary historyña fact that may well obscure thefundamental mechanisms and principles in play that are relevant to the specific environment of interest.(this point is a specific instantiation of a more general phenomenon, which is that our understanding ofbiological phenomena will often be inadequate to provide detailed guidance in engineering a computational device or artifact.)a corollary notion is that nature may evolve different biological mechanisms to solve a givenproblem. all of these mechanisms may enable the organism to survive and even to prosper in itsenvironment, but it is far from clear how well these mechanisms work relative to one another.2 thus,which one of many biological instantiations is the most appropriate model to mimic remains an important question.finally, there are only a few examples of successful biologically inspired computing innovations.thus, the jury is still out on the ultimate value of biology for computing. rather than biology beinghelpful across the board to all of computing, the committee believes that biologyõs primary relevance(at least in the short term) is likely to be to specific problem areas within computing that are poorly2for example, fish and squid use different mechanisms to propel themselves through the water. which mechanism is betterunder what circumstances and for what engineered artifacts is a question for research to answer.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.biological inspiration for computing249understood, or for which the relevant underlying technologies are too complex or unwieldy, and inproviding approaches that will address parts of a solution (as described in section 8.1.2). nevertheless, the potential benefits that biology might offer to certain problem areas in computing are large,and it is worth exploring different approaches to exploit these benefits; this is the focus of sections 8.2to 8.4.8.1.2the meaning of biological inspirationwhat does it mean for something to be biologically inspired? it is helpful to consider severalpossible interpretations. one interpretation is that significant progress in computing can occur onlythrough the application of principles derived from the study of biology. this interpretation, offeredlargely as a strawman, is absurdñthere are many ways in which computing can progress without theapplication of biologically derived principles.a second, somewhat less grandiose and more reasonable interpretation is that significant progressin computing can occur through the application of principles derived from the study of biology. that is,a biological system may operate according to principles that have applicability to nonbiological computing problems. by studying the biological system, one may be able to derive or understand therelevant principles and use them to help solve a nonbiological problem. it is this interpretationñthatbiology is relevant to computing only when principles emerge directly from a study of biologicalphenomenañthat underlies many claims of biological relevance or irrelevance to computing.a third interpretation is that certain aspects of biology are analogous to aspects of computing,which means that insights from biology are relevant to aspects of computing. this is the case, forinstance, when a set of principles or paradigms turns out to have strong applicability both to a biologicalsystem or systems and to interesting problems in computing. these principles or paradigms may havehad their intellectual origin in the study of a biological or a nonbiological system.when their origin is in a biological system, this interpretation reduces to the second interpretationabove. what makes the case of an origin in a nonbiological system interesting is that the principles inquestion may be more manifestly obvious in a biological context than in a nonbiological context. that is,the principles and their application may most easily be seen and appreciated in a biological context,even if they did not initially originate in a biological context. moreover, the biological context may alsoprovide a source of language, concepts, and metaphors that are useful in talking about a nonbiologicalproblem or phenomenon.for this report, the term òinspirationó will be used in its broadest sense, that is, the third interpretation above, but there are three other points to keep in mind:¥biological inspiration does not mean that the weaknesses of biology must be adopted along withthe strengths. in some cases, it may be possible to overcome problems found in the actual biologicalsystem when the principles underlying them are implemented in engineered artifacts.¥as noted in chapter 1, even when biology cannot provide insight into potential computingsolutions, the drive to solve biological problems can still inspire interesting, relevant, and intellectuallychallenging research in computingñso biology can serve as a useful and challenging problem domainfor computing.33for example, ibm used the problem of protein folding to motivate the development of the bluegene/l supercomputer.specifically, the problem was formulated in terms of obtaining a microscopic view of the thermodynamics and kinetics of thedynamic proteinfolding process over longer time scales than have previously been possible. because this project involved bothcomputer architecture and the exploration of algorithmic alternatives, the applications architecture was structured in such a waythat subject experts in molecular simulation could work on their applications without having to deal with the complexity of theparallel communications environment required by the underlying machine architecture (see bluegene/l team, òan overviewcatalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.250catalyzing inquiry¥incomplete (and sometimes even incorrect) biological understandings help to inspire differentand useful approaches to computing problems. important and valuable insights into possible ways tosolve a current problem have been derived from biological models that were incomplete (as in the caseof evolutionary programming) or even inaccurate (as in the case of immunologically based computersecurity).on the other hand, it must be understood that the use of a biological metaphor to inspire newapproaches to computing does not necessarily imply that the biological side is well understood, whetheror not the metaphor leads to progress in computing. that is, even if a biological metaphor is applicableand relevant to a computing problem, this does not mean that the corresponding biological phenomenacan necessarily be understood in computational terms.for example, although researchers use the term ògenetic algorithmsó to describe a class of algorithms using operators that have a similar flavor to evolutionary genetic operators such as mutation orrecombination to search a solution space stochastically, the definition and implementation of thesegenetic operators does not imply a fundamental understanding of biological evolutionary processes.similarly, although the field of òartificial neural networksó is an informationprocessing paradigminspired by the parallel processing capabilities and structure of nerve tissue, and it attempts to mimiclearning in biology by learning to adjust òsynapticó connections between artificial processing elements,the extent to which an artificial neural network reflects real neural systems may be tenuous.8.1.3 multiple roles: biology for computing insightbiological inspiration can play many different roles in computing, and confusion about this multiplicity of meanings accounts for a wide spectrum of belief about the value of biology for developingbetter computer systems and improved performance of computational tasks. one point of view is thatonly a detailed ògroundupó understanding of a biological system can result in such advances, andbecause such understanding is available for only a very small number of biological systems (and òverysmalló is arguably zero), the potential relevance of biology for computing is small, at least in the nearterm.a more expansive view of biologyõs value for computing acknowledges that detailed understanding is the key for a maximal application of biology to computing, but also holds that biological metaphors, analogies, examples, and phenomenological insights may suggest new and interesting ways ofthinking about computational problems that might not have been imagined without the involvement ofbiology.4 from this perspective, what matters is performance of a task rather than simulation of what abiological system actually does, though one would not necessarily expect initial performance modelsof the bluegene/l supercomputer,ó presented at supercomputing conference, november 2002, available at http://sc2002.org/paperpdfs/pap.pap207.pdf). other obvious problems inspired by biology include computer vision and artificial intelligence. it isalso interesting to note this historical precedent of biological problems being the domain in which major suites of statistical toolswere developed. for instance, galton invented regression analysis (correlation tests) to study the relation of phenotypes betweenparents and progeny (see f. galton, natural inheritance, 5th edition, macmillan and company, new york, 1894). pearson invented the chisquare and other discrete tests to study the distribution of different morphs in natural populations (see k.pearson, òmathematical contributions to the theory of evolution, viii. on the inheritance of characters not capable of exactquantitative measurement,ó philosophical transactions of the royal society of london, series a 195:79150, 1900). r.a. fisher invented analysis of variance to study the partitioning of different effects in inheritance (see r. fisher, òthe correlation betweenrelatives on the supposition of mendelian inheritance,ó transactions of the royal society of edinburgh 52:399433, 1918).4an analogy might be drawn to the history of superconducting materials. a mix of quantum principles, phenomenology, andtrained experience has led to superconducting materials with everhigher transition temperatures. (indeed, the discovery ofsuperconducting materials preceded quantum mechanics by more than a decade.)catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.biological inspiration for computing251based on biological systems to function more effectively than models constructed using more traditional techniques.one of biologyõs most important roles is that it can serve as an existence proof of performanceñthatsome desirable behavior is possible. the reasoning is that if a biological system can do somethinginteresting, why canõt an artificial system to the same thing? birds fly, so why shouldnõt people orconstructed artifacts be able to fly? many biological behaviors and functions would be desirable in acomputing context, and biological systems that exhibit such behavior demonstrate that this behavior ispossible.5existence proofs are important in engineering. for example, in the view of many nuclear scientistsassociated with the manhattan project, the information that was most critical to the soviet development effort was not a secret gained through espionage, but rather the fact that a nuclear explosion waspossible at allñand that fact was reported in every major newspaper in the world.6 in other words, itis one thing to work toward a goal that may well be impossible to achieve and an entirely differentpsychological matter to work toward a goal whose achievement is knownñwith certaintyñto bepossible.an example of using a biological metaphor for understanding some dimension of computing relates to computer security. from many centuries of observation, it is well known that an ecology basedon a monoculture is highly vulnerable to threats that are introduced from the outside. with this insightin mind, many expert observers have used the term òmonocultureó to describe the presentday securityenvironment for desktop computers in which one vendor dominates the operating system market. thisreport does not take a position on whether such a characterization is necessarily accurate,7 but the pointis that the metaphor, used in this manner, can determine the terms of discussion and thus provide auseful way of looking at the issue.despite its conceptual value, an existence proof does not speak directly to how to build the artifactso that it does the same thing. that is, existence proofs do not necessarily provide insight about construction or creation. diversity as a strategy for survival does not necessarily indicate how much orwhat kinds of diversity would be helpful in any given instance. similarly, aerodynamics is a body oftheory that explains the flight of birds, and also enables human beings to design airplanes, but a studyof birds did not lead to the airplane. for construction or creations, a deeper understanding of biology isrequired. knowing what kind of deeper understanding is possible potentially leads to at least threeadditional roles for biology:¥biology as source of principles. nature builds systems out of the same atoms that are available tohuman engineers. if a biological system can demonstrate a particular functionality, it is because thatsystem is built according to principles that enable such functionality. the hope is that upon closeexamination, the physical, mathematical, and informationprocessing principles underlying the interesting biological functionality can be applied through human engineering to realize a better artificialsystem. note also that in some cases, the actual principles underlying some biological functionality maybe difficult to discern. however, plausibility counts for a great deal here, and biology may well provideinspiration for engineered artifacts if human beings propose a set of plausible principles that govern thebehavior of interest in an actual organism, even if those principles, as articulated, turn out not to have abiological instantiation in that organism. (note that in this domain the division between òapplying5an accessible and more extended discussion of these ideas can be found in j. benyus, biomimicry: innovation inspired by nature,william morrow, new york, 1997.6d. holloway, stalin and the bomb: the soviet union and atomic energy, 19391956, yale university press, new haven, 1994.7for example, it may be that even though the number of operating system platforms is small compared to the number ofdesktop computers in use, different computer configurations and different operational practices might introduce sufficientdiversity to mitigate any systemwide instabilities. furthermore, replication has many other advantages in the computer context,such as easier interoperability.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.252catalyzing inquirybiological principles to information processingó and òunderstanding biological information processingó is least meaningful.)¥biology as implementer of mechanism. nature also implements mechanisms to effect certain functions. for example, a biological organism may implement an algorithm that could be the basis of asolution to a computing problem of interest to people. or, it may implement an architecture or a way toorganize and design the structural and dynamic relationships between elements in a complex system,knowledge of which might greatly improve the design of an engineered artifact. in this category are theneural network architecture as inspired by the activation model of dendrites and axons in the brain,evolutionary computation as driven by genomic changes and selection pressures, and the use ofelectroactive polymers as actuator mechanisms for robots, inspired by the operation of animal muscles(rather than, for example, gears). (note that implementations of biological mechanisms tend to be easierto identify and extract for later use when they involve physical observablesñand so mechanismsunderlying sensors and locomotion have had some nontrivial successes in their application to engineered artifacts.)¥biology as physical substrate for computing. computation can be regarded as an abstract or a physically instantiated form. in the abstract, it is divorced from anything tangible. but all realworld computation requires hardwareña device of some kind, whether artificial or biologicalñand given that biological organisms are functional physical devices, it makes sense to consider how engineered artifactsmight have biological components. for example, biology may provide parts that can be integrated intoengineered devices. thus, a sensitive chemical detection system might use a silk moth as the sensor forchemicals in the air and thus instrument the moth to appropriate readouts. or a small animal might beused as the locomotive platform for carrying a useful payload (e.g., a camera), and its movements mightbe teleoperated through electrodes implanted in the animal by a human being viewing the images sentback by a camera.these three different roles are closely connected to the level(s) of abstraction appropriate for thinking about biological systems. for some systems and phenomena of interest, a very òbottomupó perspective is warranted. in the same way that one needs to know how to use transistors to build a logicgate for a siliconbased computer, one needs to know how neurons in the brain encode information inorder to understand how a neural implant or prosthetic device might be constructed. for other systemsand phenomena, architecture provides the appropriate level of abstraction. in this case, understandinghow parts of a system are interconnected, the nature of the information that is passed between them,and the responses of those parts to such information flows may be sufficient.another way of viewing these three roles is to focus on the differences between computationalcontent, computational representation, and computational hardware. consider, for example, a catenarycurveñthe shape that a cable suspended at both ends takes when subjected to gravity.¥the computational content is specified by a differential equation and the appropriate boundaryconditions. although the solution is not directly apparent from the differential equation, the differentialequation implies a specific curve that represents the answer.¥the computational representation refers to how the computation is actually representedñindigital form (as bits in a computer), in analog form (as voltages in an analog computer), in neural form(as how a calculus student would solve the problem), or in physical form (as the string or cable beingrepresented).¥the computational hardware refers to the physical device used to solve the equationñthe digitalcomputer, the analog computer, the human being, or the cable itself.these three categories correspond roughly and loosely to the three categories described above: contentas source of principles, representation as implementer of mechanism, and hardware as physical substrate.the remaining sections of this chapter describe some biological inspirations for work in computing.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.biological inspiration for computing2538.2 examples of biology as a source of principles for computing8.2.1 swarm intelligence and particle swarm optimizationswarm intelligence is a property of systems of nonintelligent, independently acting robots thatexhibit collectively intelligent behavior in an environment that the robots do sense and can alter.8 oneform of swarm intelligence is particle swarm optimization, based on the flocking of birds.9the canonical example of flocking behavior is a flight of birds wheeling through the sky, or a schoolof fish darting through a coral reef. somehow, myriad notverybright individuals manage to move,turn, and respond to their surroundings as if they were as a single, fluid organism. moreover, they seemto do so collectively, without a leader: biologists armed with highspeed video cameras have shown thatthe natural assumptionñthat each flock or school has a single, dominant individual that always initiates each turn just a fraction of a second before the others followñis simply not true.the first known explanation of the leaderless, collective quality of flocking or schooling behavioremerged in 1986. this explanation used swarms of simulated creaturesñòboidsóñthat could formsurprisingly realistic flocks if each one simply sought to maintain an optimum distance from its neighbors. the steering rules of the socalled reynolds simulation were simple:10¥separation: steer to avoid crowding local flock mates.¥alignment: steer toward the average heading of local flock mates.¥cohesion: steer toward the average position of local flock mates.these rules were entirely local, referring only to what an individual boid could see and do in itsimmediate vicinity;11 none of them said, òform a flock.ó yet the flocks formed every time, regardless ofthe starting positions of the boids. these flocks were able to fly around obstacles in a very fluid andnatural manner. sometimes the flock would even break into subflocks that flowed around both sides ofan obstacle, rejoining on the other side as if the boids had planned it all along. in one run, a boidaccidentally hit a pole, fluttered around for a moment, and then darted forward to rejoin the flock as itmoved on.today, the reynolds simulation is regarded as one of the best and most evocative demonstrations ofemergent behavior, in which complex global behavior arises from the interaction of simple local rules. theapproach embodied in the simplerule/complexbehavior approach has become a widely used technique in computer animationñwhich was reynoldsõ primary interest in the first place.128t. white, òswarm intelligence: a gentle introduction with applications,ó powerpoint presentation, available at http://www.sce.carleton.ca/netmanage/tony/swarmpresentation/tsld001.htm.9bird flocks are an example of complex, adaptive systems. among the many other examples that scientists have studied are theworld economy, brains, rain forests, traffic jams, corporations, and the prehistoric anasazi civilization of the four corners area.complex adaptive systems are similar in structure and behavior even if they differ in their superficial manifestations. forexample, complex adaptive systems are massively parallel and involve many quasiindependent òagentsó interacting at once.(an agent might be a single firm in an economy, a single driver on a crowded freeway, and so on.) each of them is adaptive,meaning that the agents that constitute them are constantly responding and adapting to each other. and each of them isdecentralized, meaning that no one agent is in charge. instead, a complex systemõs overall behavior tends to emerge spontaneously from myriad lowlevel interactions.10c.w. reynolds, òflocks, herds, and schools: a distributed behavioral model,ó computer graphics 21(4):2534, 1987, availableat http://www.cs.toronto.edu/~dt/siggraph97course/cwr87/ and http://www.red3d.com/cwr/papers/1987/siggraph87.pdf. an updated discussion, with many pictures and references to modern applications, can be found in c.w. reynolds, òboids:background and update,ó 2001, available at http://www.red3d.com/cwr/boids/.11more precisely, each boid had global information about the physical layout of its environment, including any obstacles, but ithad no information about its flock mates, except for those that happened to come within a certain distance that defined its localneighborhood.12the first hollywood film to use a version of reynoldsõ boids software was tim burtonõs batman returns (1992), whichfeatured swarms of animated bats and flocks of animated penguins. since then it has been used in films such as the lion king(1994) and many others (see http://www.red3d.com/cwr/boids/).catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.254catalyzing inquirya second simulation of flocking behavior, developed in 1990, employed the reynoldsõ rules (thoughthey were independently developed) and also incorporated the influence of òdynamic forcesó on thebehavior of the simulated creatures.13 these dynamic forces would allow the creatures to be attractedtoward a convenient roosting point, say, or a particularly rich cornfield. as a result, the flock wouldturn and head in the direction of a cornfield as soon as it was placed into view, with various subgroupsswinging out and in again until finally the whole group had landed right on target.these two models are direct ancestors of the particle swarm optimization (pso) algorithm, firstpublished in 1995.14 the algorithm substitutes a mathematical function for the original roosts andcornfields, and employs a conceptual swarm of birdlike particles that swoop down on the functionõsmaximum value, even when the function has many local maxima that might confound more standardoptimization algorithms.the essential innovation of the pso algorithm is to scatter particles at random locations throughouta multidimensional phase space that represents all the arguments to the function to be maximized. thenthe algorithm sets the particles in motion. each particle evaluates the function as it flies through phasespace and keeps trying to turn back toward the best value that it has found so far. however, it isattracted even more toward the best value that any of its neighboring particles have found. so itinexorably begins to move in that directionñalbeit with a little builtin randomness that allows it toexplore other values of the function along the way. the upshot is that the particles quickly form a flockthat flows toward a point that is one of the highest function values available, if not the highest.the pso algorithm is appealing for both its simplicityñthe key steps can be written in just a fewlines of computer codeñand its effectiveness. in the original publication of the pso algorithm, thealgorithm was applied to a variety of neural network problems, and it was found to be a very efficientway to choose the optimum set of connection weights for the network.15 since then, the basic techniquehas been refined and extended to systems that have discrete variables, say, or that change with time. italso has been applied to a wide variety of engineering problems,16 such as the automatic adjustment ofpower systems.17the pso algorithm is biologically inspired in the sense that it is a plausible account of bird flockingbehavior. however, it is not known whether birds, in fact, use the pso algorithm to fly in formation.swarm algorithms have the virtues of simplicity and robustness, not to mention an ability to function without the need for centralized control. for this reason, they may find their most importantapplications in, say, selfhealing and selforganizing communications networks or in electrical powernetworks that could protect themselves from line faults and reroute current around a broken link òonthe fly.ó18on the other hand, simple rules are not automatically good. witness army ants, which are suchobsessive selforganizers that the members of an isolated group will often form a òcircular mill,ó follow13f.h. heppner and u. grenander, òa stochastic nonlinear model for coordinated bird flocks,ó the ubiquity of chaos, s.krasner, ed., aaas publications, washington, dc, 1990.14j. kennedy and r.c. eberhart, òparticle swarm optimization,ó pp. 19421948 in proceedings of the ieee international conferenceon neural networks, ieee service center, piscataway, nj, 1995; r. eberhart, y. shi, and j. kennedy, swarm intelligence, morgankaufman, san francisco, ca, 2001.15see section 8.3.3.2 for further discussion.16a good sense of current activity in the field can be gleaned from the programs and talks at the 2003 ieee swarm intelligencesymposium, april 2426, 2003, available at http://www.computelligence.org/sis/index.html. extensive references to pso canbe found at òwelcome to particle swarm central,ó 2003, available at http://www.particleswarm.info. this site also contains anumber of links to online tutorials and downloadable pso code.17k.y. lee and m.a. elsharkawi, eds., modern heuristic optimization techniques with applications to power systems, john wileyand ieee press, new york, march 2003.18e. bonabeau, òswarm intelligence,ó presented at the oõreilly emerging technology conference, april 2225, 2005, santaclara, ca. powerpoint presentation available at http://conferences.oreillynet.com/presentations/et2003/bonabeaueric.ppt.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.biological inspiration for computing255ing one another around and around and around until they die from starvation.19 such blindleadingtheblind behaviors are an everpresent possibility in swarm intelligence; the trick is to find simple rulesthat minimize the chances of that happening.a closely related challenge is to find ways of designing emergent behavior, so that the swarm willproduce predictable and desirable results. today, swarm algorithms are based on the loose and imprecise specification of a relatively small number of parametersñbut it is almost certainly true that engineered artifacts that exhibit complex designed behavior will require the tight specification of manyparameters.this point is perhaps most obvious in the cooperative construction problem, where the rule sets thatproduce interesting, complex structures are actually very rare; most selforganized structures look morelike random blobs.20 the same problem is common to all collective behaviors; finding the right rules isstill largely a matter of trial and errorñnot least because it is in the very nature of emergence for asimpleseeming change in the rules to produce a huge change in the outcome. thus, in their efforts tofind the right rules, researchers may well seek to develop procedures that will find in the right rulesrather than trying to find them directly themselves. this point is discussed further in section 8.3.1.8.2.2 robotics 1: the subsumption architectureone approach to robotic design is based on the notion that complex and highly capable systems areinherently expensive, and hence fewer can be built. instead, this approach asserts the superiority ofusing large numbers of individually smaller, less capable, and inexpensive systems.21 in 1989, brooksand flynn suggested that ògnat robotsó might be fabricated using silicon micromachining to fabricatefreely movable structures onto silicon wafers. such an approach potentially allows sensors, actuators,and electronics to be embedded on the same silicon substrate. this arrangement is the basis for brooksõsubsumption architecture, in which lowlevel functionality can be used as building blocks for higherlevel functionality.robots fabricated in this manner could be produced by the thousands, just as integrated circuits areproduced todayñand thus become an inexpensive, disposable system that does its work and need notbe retrieved. for applications such as exploration in hostile environments, the elimination of a retrievalrequirement is a significant cost savings.to the best of the committeeõs knowledge, no selfpropelled robots or other operational systemshave been built using this approach. indeed, experience suggests that the actual result of applying theswarm principle is that one highly capable robot is not replaced by many robots of lesser capability, butrather one such robot. this suggests that realworld applications are likely to depend on the ability tofabricate many small robots inexpensively.a key challenge is thus to develop ways of assembling microrobots that are analogous to chipfabrication production lines. one step toward meeting this challenge has been instantiated in a conceptknown as òsmart dust,ó for which actual prototypes have been developed. smart dust is a concept for a19b. hılldobler and e.o. wilson, the ants, belknap press of harvard university press, cambridge, ma, 1990, pp. 585586. in afamous account published in 1921, the entomologist william beebe described a mill he saw in the amazonian rain forest thatmeasured some 360 meters across, with each ant taking about 21/2 hours to complete a circuit. they kept at it for at least 2 days,stumbling along through an everaccumulating litter of dead bodies, until a few workers finally straggled far enough from thetrail to break the cycle. and from there, recalled beebe, the group resolutely marched off into the forest. see w. beebe, edge of theforest, henry holt and company, new york, 1921.20but then, so do most insect nests. honeycombs, waspsõ nests, and other famous examples are the exception rather than therule.21r.a. brooks and a.m. flynn, òfast, cheap and out of control: a robot invasion of the solar system,ó journal of the britishinterplanetary society 42:478485, 1989.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.256catalyzing inquiryhighly distributed sensor system.22 each dust mote has sensors, processors, and wireless communications capabilities and is light enough to be carried by air currents. sensors could monitor the immediateenvironment for light, sound, temperature, magnetic or electric fields, acceleration, pressure, humidity,selected chemicals, and other kinds of information, and the motes, when interrogated, would send thedata over kilometerscale ranges to a central base station, as well as communicate with local neighbors.this architecture was the basis of an experiment that sought to track vehicles with an unmanned aerialvehicle (uav)delivered sensor network.23 the prototype sensors were approximately a cubic inch involume and contained magnetic sensors for detecting vehicles (at ranges of about 10 meters), a microprocessor, radiofrequency communications, and a battery or solar cell for power. with six to eight airdelivered sensor motes landed diagonally across a road at about 5meter intervals, the sensor network wasable to detect and track vehicles passing through the network, store the information, and then transfervehicle track information from the ground network to the interrogating uav and then to the base camp.the subsumption architecture also asserts that this robust behavior can emerge from the bottomup.24 for example, in considering the problem of an autonomously functioning vehicle (i.e., one thatdrives itself), a series of layers can be defined that¥avoid contact with objects (whether the objects move or are stationary),¥wander aimlessly around without hitting things, and¥explore the world by seeing places in the distance that look reachable and heading for them.any given level contains as a subset (subsumes) the lower levels of competence, and each level canbe built as a completely separate component and added to existing layers to achieve higher levels ofcompetence. in particular, a level 0 machine would be built that simply avoided contact with objects. alevel 1 machine could be built by adding another control layer that monitors data paths in the level 0layer and inserts data onto the level 0 data paths, thereby subsuming the normal data flow of level 0.more complex behavior is thus built on top of simpler behaviors.brooks claims that the subsumption architecture is capable of accounting for the behavior of insects,such as a house fly, using a combination of simple machines with no central control, no shared representation, slow switching rates, and lowbandwidth communication. this results in robust and reliable behaviordespite its limited sensing capability and an unpredictable environment, because individual behaviors cancompensate for each othersõ failures, resulting in coherent and emergent behavior despite the limitations ofthe component behaviors. a number of robots have been built using subsumption architectures. of particular note is hannibal,25 a hexapod with more than 100 physical sensors and 1,500 augmented finitestatemachines grouped into several dozen behaviors split over eight onboard computers.268.2.3 robotics 2: bacteriuminspired chemotaxis in robots27the problem of locating gradient sources and tracking them over time is an important problem inmany realworld contexts. for example, fires cause temperature gradients in their immediate vicinity;22see, for example, http://robotics.eecs.berkeley.edu/~pister/smartdust/.23see http://robotics.eecs.berkeley.edu/~pister/29palms0103/.24r.a. brooks and a.m. flynn, òfast, cheap and out of control,ó 1989.25c. ferrell, òrobust agent control of an autonomous robot with many sensors and actuators,ó ph.d. thesis, department ofelectrical engineering and computer science, massachusetts institute of technology, cambridge, ma, 1993.26a finitestate machine is a machine with a finite number of internal states that transitions from one state to another on thebasis of a specified function. that is, the argument of the function is the machineõs previous state, and the functionõs output is itsnew state. an augmented finitestate machine is a finitestate machine augmented with a timer that forces a transition after acertain time.27material in section 8.2.3 is based on excerpts from a. dhariwal, g.s. sukhatme, and a.a.g. requicha, òbacteriuminspiredrobots for environmental monitoring,ó international conference on robotics and automation, new orleans, la, april 2004.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.biological inspiration for computing257chemical spills lead to chemical concentration gradients in the soil and/or water; ecosystems hostgradients of light, salinity, and ph. in many cases, the source intensity of these gradients varies withtime (e.g., because of movement of the source), and there may be multiple sources for any givencharacteristic (e.g., two fires causing a complex temperature gradient).autonomous detection, location, and tracking of gradient sources would be very helpful for thosetrying to study or respond to the environment. using robots, an environmental scientist might need tofind the source(s) of a given toxic chemical, whereas a firefighter might need to locate the source(s) of afire in order to extinguish it.noting that other approaches for locating and tracking gradient sources were primarily useful instatic or quasistatic environments, and inspired by biological studies of how bacteria are attracted togradient sources of nutrition, dhariwal et al.28 sought to develop a strategy for finding gradient sourcesthat worked well with sources that are small, weak, mobile, or timevarying in intensity. specifically,their algorithm is based on the repetition of a straightline run for a certain time, followed by a randomchange in direction that sets up the direction for a new run. if the bacterium senses a higher concentration in its immediate environment, the run length is longer. thus, although the bacterium still undergoes a random walk, it is a random walk biased in the direction of the gradient source.this algorithm is also well suited for implementation in a simple robot. that is, only the last sensorreading must be stored, and so memory requirements are lower. because only one computation has tobe done (a comparison between the present and the previous sensor reading), processing requirementsare minimal.dhariwal et al. compared the performance of this algorithm with a simple gradient descent algorithm. they found that for single, weak sources, the simple gradient algorithm displayed better performance. however, the bacteriuminspired algorithm displayed better performance in locating and tracking multiple and/or dissipative sources and in covering the entire area in which the gradient can befound.8.2.4 selfhealing systemsin the past few years, the term òselfhealingó has become a fashionable object of study and interestin the academic and research computer science communities29 and in the marketing materials of information technology (it) companies such as ibm,30 microsoft,31 sun,32 and hp.33 despite (or becauseof?) this level of interest, there is no commonly accepted definition of òselfhealingó or agreement ofwhat functionality it encompasses or requires.28a. dhariwal, g.s. sukhatme, and a.a.g. requicha, òbacteriuminspired robots for environmental monitoring,ó ieee international conference on robotics and automation, new orleans, la, april 2530, 2004, available at http://wwwlmr.usc.edu/~lmr/publications/icra04bact.pdf.29workshop on selfhealing, adaptive and selfmanaged systems (shaman), june 23, 2002, available at http://www.cse.psu.edu/~yyzhang/shaman/proc.html; icse 2003 workshop on software architectures for dependable systems, may 2003 (formore information, see http://www.cs.kent.ac.uk/events/conf/2003/wads/); david garlan, selfhealing systems course, #17811, carnegie mellon university seminar, spring 2003 (for more information see http://www2.cs.cmu.edu/~garlan/17811/);d. garlan, j. kramer, and a. wolf, eds., proceedings of the first workshop on selfhealing systems, acm press, new york, 2002.30m. hamblen, òibm to boost selfhealing capabilities in tivoli line,ó computerworld, april 4, 2003, available at http://www.computerworld.com/softwaretopics/software/story/0,10801,80050,00.html.31"windows 2000 professional: most reliable windows ever,ó december 5, 2000, available at http://www.microsoft.com/windows2000/professional/evaluation/business/overview/reliable/default.asp.32"sun and raytheon create open, adaptive, selfhealing architecture for dd 21,ó available at http://wwws.sun.com/software/jini/news/jiniraytheon.pdf.33"hp delivers selfhealing and virtual server software to advance the adaptive enterprise,ó press release, may 6, 2003,available at http://www.hp.com/hpinfo/newsroom/press/2003/030506c.html.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.258catalyzing inquiryin fact, many of the techniques described as selfhealing are familiar to the decadesold hardwarefield of reliable systems, also known as fault tolerance or high availability. these techniques, such asfault detection, fault masking, and fault tolerance, are in common use when designing hardware toimprove the reliability and availability of large systems. this is most likely because hardware designers,unlike software programmers, long ago accepted the unavoidable reality that components of theirdesigns will fail at some point. (it also helps immeasurably that hardware failures are often easier tocharacterize than software failures.) in areas with extremely high demands for reliability, such asaerospace or power plants, these faulttolerance techniques have become quite sophisticated, as havemechanisms for testing system operation. the oldest and most accepted use of the term selfhealing isfound in networking;34 networks from the original arpanet (and even the public switched telecommunications network) to modern peertopeer embedded networks are selfhealing in the sense thattraffic is routed around unresponsive nodes.in contrast, until quite recently, software quality has focused on producing bugfree products, by anintensive effort of careful design, code review, and extensive prerelease testing. however, when bugsdo occur, software typically has no ability to detect or react to them, or to continue to operate. this wasa workable strategy for much of the history of modern software, but the continuing rise of the complexity of software applications has made formal review or correctness proofs inadequate to provide minimum levels of reliability.35this rise in complexity and the resulting rise in human cost of configuration and maintenance ofsoftware applications has spurred interest in selfhealing, hoping to shift much of the burden of thisconfiguration and maintenance back to the software. the idea is that, like its biologically analogousnamesake, a selfhealing system would detect the presence of nonfunctioning (or, more challengingly,malfunctioning) components and initiate some response to continue proper overall functionality, preferably without any centralized or external force (such as a system administrator) required. the mostcommon implementation today seems to be one of reconfiguration: if a fault is detected, a spare hardware component is brought into play. this is òhealingó only in the loosest sense, although it certainly isa valid fault tolerance technique. however, it doesnõt translate well to softwareonly failures.none of the systems that describe themselves as selfhealing (such as microsoft windows 2000, ibmdb/2, or sunõs jini) seem to actually employ biological principles, other than in the grossest sense ofhaving redundancy. however, one research project that is inspired very explicitly by biology is swarmat the university of virginia.36 the swarm programming model defines units as individual cells, whichcan both reproduce through cellular division and die. additionally, they can emit signals at variousstrengths and respond to the aggregate strength of signals in the environment. for example, a systemset to grow to a certain size would start with a single cell that emitted a small amount of signal and witha program set to reproduce if the aggregate signal was at a certain threshold. until the total amount ofsignal exceeded that threshold, the cells would continue to divide, but they would stop once thethreshold was exceeded. if cells were to fail or otherwise be deleted, other cells would respond bydividing again to bring the signal back to the threshold. this is indeed a primitive form of selfhealing.however, this programming model is unlikely to catch on for complex tasks without significant higherlevel abstractions available.34w.d. grover, òthe selfhealing network: a fast distributed restoration technique for networks using digital crossconnect machines,ó proceedings of the ieee global telecommunications conference, tokyo, 1987, pp. 10901095.35in his lecture on receiving the acm turing award in 1980, c.a.r. hoare said, òthere are two ways of constructing asoftware design: one way is to make it so simple that there are obviously no deficiencies, and the other way is to make it socomplicated that there are no obvious deficiencies.ó lecture available at http://www.braithwaitelee.com/opinions/p75hoare.pdf.36g. selvin, d. evans, and l. davidson, òa biologically inspired programming model for selfhealing systems,ó proceedings ofthe first workshop on selfhealing systems, november 2002, available at http://www2.cs.cmu.edu/~garlan/woss02/.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.biological inspiration for computing2598.2.5 immunology and computer security37the mammalian immune system is an information processorñthis is clear from its ability to distinguish between self and nonself. (section 5.4.4.3 provides a brief introduction to the immune system.)some have thus been drawn to the architecture of the immune system as a paradigm of informationprocessing that might be useful in solving a variety of different computational problems. immunological approaches have been proposed for solving problems in computer security, semantic classificationand query, document and email classification, collaborative filtering problem, and optimization.38 thissection concentrates on computer security applications.8.2.5.1 why immunology might be relevantcomputer and network security is intended to keep external threats at bay, and this remains anintellectually challenging problem of the highest order. it is useful to describe two general approachesto such security problems. the first, widely in use today, is based on the notion of what might be calledenvironmental controlñthe idea that by adequately controlling the environment in which a computeror network functions, better security can be obtained. the computer or network environment is definedbroadly, to include security policy (who should have what rights and privileges), resources (e.g., programs that provide users with computing or communications capability), and system configuration. insupport of this approach, a number of reports39 cite security problems that arise from flaws in securitypolicy, bugs in programs, and configuration errors and argue that correcting these flaws, bugs, anderrors will result in greater security.a complementary approach is to take as a given the inability to control the computing or networkenvironment.40 this approach is based on the idea that computer security can result from the use ofsystem design principles that are more appropriate for the imperfect, uncontrolled, and open environments in which most computers and networks currently exist. note that there is nothing mutuallyexclusive about the two approachesñboth could be used in the design of an effective overall approachto system or network security.for inspiration in addressing problems in computer security, some researchers have considered theimmune system and the unpredictable and largely hostile environment in which it functions.41 that is,the unpredictable pathogens to which the immune system must respond are analogous to some of thethreats that computer systems face, and the principles underlying the operation of the immune systemmay provide new approaches to computer security.8.2.5.2 some possible applications of immunologybased computer securitya variety of loose analogies between computer security and immunology are intuitively obvious,and there is clearly at least a superficial conceptual connection between the protection afforded to37the discussion in section 8.2.5 owes much to stephanie forrest of the university of new mexico.38for a view of the immune system as information processor, see s. forrest and s. hofmeyr, òimmunology as informationprocessing,ó design principles for immune systems and other distributed autonomous systems, l.a. segal and i.r. cohen, eds.,oxford university press, 2000. for an overview of various applications of an immunological computing paradigm, seewww.hpl.hp.com/personal/ stevecayzer/downloads/030213ais.ppt and references therein.39national research council, cybersecurity today and tomorrow: pay now or pay later, national academy press, washington,dc, 2002.40this discussion is based on a. somayaji, s. hofmeyr, and s. forrest, òprinciples of a computer immune system,ó proceedingsof the 1997 workshop on new security paradigms, acm press, langdale, uk, 1998, pp. 7582.41one of the first papers to suggest that selfnonself discrimination, as used by the immune system might be useful in computersecurity was by s. forrest, a.s. perelson, l. allen, and r. cherukuri, òselfnonself discrimination in a computer,ó proceedings of the1994 ieee symposium on research in security and privacy, ieee computer society press, los alamitos, ca, 1994, pp. 202212. thispaper focused mainly on the issue of protection against computer viruses but set the stage for a great deal of subsequent work.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.260catalyzing inquiryhuman beings by the immune system and computer security. the following examples are adapted fromsomayaji et al.:42¥protecting active processes on a single host. for this application, a computer running multiple processes might be conceptualized as a multicellular organism (in which each process is analogous to acell). an adaptive immune system could be a detector process that queried other processes to seewhether they were functioning normally. if not (i.e., if the detector process found ònonselfó in itsprobes), the adaptive system could slow, suspend, kill, or restart the misbehaving process. one approach to detection (positive detection) is based on the establishment of a profile of observed normalbehaviors and using that profile to notice when a program behaves abnormally.43¥protecting a network of computers. for this application, each computer in a network might beconceptualized as a cell in an individual. each process would still be considered as a cell, but now anindividual is a network of computers. (another possible analogy for the network of computers is thateach computer represents a single organism and populationlevel protections are achieved by the collective group through independence, diversity, and sharing of information.) an adaptive detector process could be implemented as described above, with the added feature that these detectors couldmigrate between computers, thereby enabling all computers on the network to benefit from the detection of a problem on one of them.¥protecting a network of disposable computers. this application is similar to that described above,with the addition that when an anomaly is detected, the problematic machine can be isolated, rebooted,or shut down. if the true source of the anomaly were outside the network, a detector process or systemcould stand in for the victimized machine, doing battle with the malicious host and potentially sacrificing itself for the good of the network. note that this application requires that hosts be more or lessinterchangeableñotherwise the network could not afford the loss of a single host.8.2.5.3 immunological design principles for computer securitythe immune system exhibits a number of characteristicsñone might call them design principlesñthat could reasonably describe how effective computer security mechanisms might operate in a computer system or network. (as in section 5.4.4.3, òimmune systemó is understood to mean the adaptiveimmune system.) for example, the immune system is:44¥distributed, in the sense that it has no central point of control. instead, the components of theimmune system interact locally to mount responses to foreign pathogens (e.g., pathogen detectors[lymphocytes] operate locally to flag the presence of pathogens). by contrast, a computer system basedon centralized control is vulnerable to òdecapitationóña successful attack on the point(s) of centralizedcontrol renders the system entirely useless.45¥diverse, in the sense that because of the ways in which pathogen detectors are produced, eachindividual human being can detect a somewhat different set of pathogensña diversity that protects42a. somayaji, s. hofmeyr, and s. forrest, òprinciples of a computer immune system,ó proceedings of the 1997 workshop on newsecurity paradigms, acm press, langdale, uk, 1998, pp. 7582.43an alternative approach is to use a randomly generated detector or set of detectors, living for a limited amount of time, afterwhich it would be replaced by another detector. detectors that proved particularly useful during their lifetimes (e.g., by detecting new anomalies) could be given a longer life span or allowed to spawn related processes. this approach has been used byforrest et al. in the development of a network intrusion detection system known as lisys.44this discussion of the immune system is based on s. forrest and s. hofmeyr, òimmunology as information processing,ódesign principles for immune systems and other distributed autonomous systems, l.a. segal and i.r. cohen, eds., oxford universitypress, new york, 2001.45a distributed, mobile agent architecture for security was also proposed in m. crosbie and g. spafford, òactive defense of acomputer system using autonomous agents,ó technical report 95008, department of computer science, purdue university,1995.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.biological inspiration for computing261the species as a whole. by contrast, computer system monoculture (i.e., lack of diversity) implies thatsystems share vulnerabilities, and a successful attack on one system is likely to succeed on othersystems as well.46¥autonomous, in the sense that it classifies and eliminates pathogens and repairs itself by replacingdamaged cells without the benefit of any centralized control mechanism. given the growing securityburden placed on todayõs computer systems and networks, it will be increasingly desirable for thesesystem and networks to manage security problems with minimal human intervention.¥tolerant of error, in the sense that some mistakes in identification of pathogens (false positives orfalse negatives) are not generally fatal and do not cause immune system collapse, although they cancause lingering autoimmune disease. such tolerance is in part the result of a multilayered design of theimmune system, in which multiple, independently architected layers of defense (òdefense in depthó)operate to provide levels of protection that are not achievable by any single mechanism.47 computersystems are often not so tolerant, and small errors or problems in some part of a system can lead tosignificant malfunctions.¥dynamic, in the sense that pathogen detectors are continually being produced to replace thosethat are (routinely) destroyed. these detectors, circulated through the body, provide wholebody protection and may be somewhat different in each new generation (in that they respond to differentpathogens). because these detectors turn over, the immune system has a greater potential coverage. bycontrast, protection against computer viruses, for example, is based on the notion that all threat virusesare knownñand most antiviral systems are unable to cope with a new virus for which no signature isknown.¥capable of remembering (adaptable), in the sense that the immune system can learn about newpathogens and òrememberó how it coped with one pathogen in order to respond more effectively to afuture encounter with the same or a similar pathogen. it can also òforgetó about nonself entities that areincorporated into the body (e.g., food gets turned into body parts). computer systems must also adaptto new environments, as for example, when new software is added legitimately, as well as identify newthreats.¥imperfect, in the sense that individual pathogen detectors do not identify pathogens perfectly, butrather respond to a variety of pathogens. greater specificity is obtained through redundant detection ofa pathogen using different detector types. by contrast, computer security systems that look for precisesignatures of intruders (e.g., viruses) are easily circumvented.¥redundant, in the sense that multiple and different immune system detectors can recognize apathogen. pathogens generally contain many parts, called epitopes, that are recognized by immunesystem detectors; thus, failure to recognize one epitope is not fatal because many others are available forrecognition.¥homeostatic, in the sense that the immune system can be regarded as one mechanism throughwhich the human body seeks to maintain a stable internal state despite a changing environment. acomputer system can be designed to autonomously monitor its own activities, routinely making smallcorrections to maintain itself in a ònormaló state, even in the face of wide variations in inputs, such asthose caused by intruders.48at a deeper level, it is instructive to ask whether the particular methods by which the immunesystem achieves these characteristics (implements these design principles) have potential relevance tocomputer security. to address this issue, deeper and more detailed immunological knowledge is necessary, but some work has been done in this area and is described below.46for more discussion of this point, see computer science and telecommunications board, national research council, computers at risk: safe computing in the information age, national academy press, washington, dc, 1991.47this point suggests that detection mechanisms are biased to be more tolerant of false negatives than false positives, becausethreats that are unaffected by one layer (i.e., false negatives) might well be intercepted by another.48a. somayaji and s. forrest, òautomated response using system call delays,ó journal of computer security 6:151180, 1998.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.262catalyzing inquiry8.2.5.4 an example: immunology and intruder detectionto detect pathogens, the immune system generates detectors that can bind to pathogens, and onlyto pathogens (i.e., do not bind to self). (a detector binding to a pathogen is the marker of a detectionevent.) to vastly simplify a complex process, the immune system first generates detectors at random.through a process known as tolerization, detectors that bind to self are destroyed, leaving only detectors that bind to nonself at the end; these detectors are called mature. mature detectors are releasedthroughout the body; if they do not bind to a nonself entity in some period of time (several days?), theyare destroyed (selfdestruct?). those that do bind to nonself entities are regarded as activated detectors.however, an activated detector must receive a second, independent signal (created by the binding ofanother type of detector to the same pathogen costimulation) to become capable of surviving for a longperiod of time. these longterm survivors are memory detectors that enable subsequent immune responses to be generated much more rapidly and are the basis for longterm immunity. (memory detectors have lifetimes that range from days to the lifetime of an organism, and the underlying mechanismsgoverning their lifetimes are not well understood.)in the context of computer security, forrest and hofmeyr have described models for networkintrusion detection and virus detection.49 in the network intruder detection example, self is definedthrough a set of ònormaló connections in a local area network. each connection is defined by a tripletconsisting of the addresses of the two parties in communication with each other and the port over whichthey communicate (a total of 49 bits), and the set of all triplets (normal triplets) generated during atraining period represents, by definition, normal operation of the network.when the network operates outside the training period, the intrusion detection system generatesrandom detector strings that are 49 bits in length. matches are declared according to an òrcontiguousbitó ruleña match is deemed to exist if a random detector string matches some normal triplet in at leastr contiguous bit positions. in this phase (the maturation phase), detector strings that match some normaltriplet are eliminated, leaving only mature detectors that have not matched any normal triplet.mature detectorsñwhich might match an abnormal triplet that arises as the result of a networkintrusionñare then exposed to the nontraining network operation. if a mature detector matches sometriplet found in the nontraining network operation, such a match is potentially a sign of networkintrusion (which would be indicated by an unusual pair of systems communicating over an unusualport). if a mature detector does not match any such triplet in a given period of time, it too is eliminated.50 the remaining detectorsñactivated detectorsñare now fully capable of signaling the presenceof abnormal triplets.however, as a further guard against false positives, the system invoked a mechanism inspired byimmunological costimulation. costimulation reduces the likelihood that a pathogen will be indicatedwhen there is no pathogen. after negative selection of lymphocytes occurs, the remaining nowmaturelymphocytes are likely to bind to nonself entities encountered. however, before the lymphocytes areòpromotedó to memory cells, they must be activated by a costimulatory signal indicating that thesubstances to which they bind are in fact pathogens. this costimulatory signal is generated independently and reduces the incidence of pathogen detectors that are overly sensitive (and hence the likelihood of autoimmune reactions).the intrusion detection system implements a costimulatory mechanism as the requirement of ahuman confirmation of behavior flagged as potentially anomalousñthat is, it presents matches signaled by an activated detector to a human operator for confirmation. if the system receives humanconfirmation within a fixed amount of time, the activated detector responsible for the warning is made49s. forrest and s. hofmeyr, òimmunology as information processing,ó design principles for immune systems and other distributed autonomous systems, l.a. segal and i.r. cohen, eds., oxford university press, new york, 2001.50in fact, the mature detector is eliminated if it does not exceed some parametrically set threshold (the activation threshold) forthe number of matches to abnormal triplets.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.biological inspiration for computing263into a memory detector (with an indefinite lifetime and a subsequent activation threshold of 1). however, if human confirmation is not forthcoming, the detector responsible is eliminated.an intrusion detection product based on this approach was introduced in early 2003.51 the realworld success of this product remains to be seen.8.2.5.5 interesting questions and challenges8.2.5.5.1 definition of selfany paradigm for computer security that is based on the differentiation ofself from nonself must imply some operational definition of self that represents normal and benignoperation. it is clear that a good definition is matched to the signature of the threat being defendedagainst, and hence the designer must be able to answer the question, òhow would i know my systemwere under attack?ó thus, self might be definable in terms of memory access patterns on a single host,tcp/ip packets entering and leaving a single host, the collective behavior of a local network of computers, network traffic through a router, instruction sequences in an executing or stored program, sequences of system calls, user behavior patterns, or keyboard typing patterns.52at the same time, computer security must account for the fact that òselfó on a computer system,even one that has not been subject to threat or intrusion, changes over time. new users are added, newsoftware is added, and files are created, deleted, and modified in the course of normal activity, eventhough all such activities may also occur in the course of an attack. that is, the notion of self must bedynamically modifiable.these points suggest that better insights into characterizing threat signatures dynamically would behelpful if immunological approaches are to be used to enhance computer security.8.2.5.5.2 more immunological mechanismsanother intellectual challenge is to incorporate more ofwhat is known about immunology into computer security. thus, it is interesting to consider how anumber of immunological mechanisms known today might be useful in making the analogy closer,using the functions and design principles of these specific mechanisms within the general context of animmunologically based approach to computer security. one such mechanism is antigen processing andthe major histocompatibility complex (mhc). some pathogens have the ability to òhideó within cellsgenerally recognized as self. because lymphocytes can detect antigens only by binding to them, they areunable to detect pathogens inside friendly cells. molecules from the mhc have the ability to bring keyparts of such pathogens to the surface of those cells, thereby enabling the lymphocytes to detect them.moreover, each individual has a different set of mhc molecules; hence the kinds of hidden pathogensthat can be brought to a cellõs surface are different for different individuals, providing an importantimmunological diversity in the population as a whole.an analogous mechanism was implemented in the intrusion detection system described above. justas certain pathogens are able to hide within cell interiors to avoid detection, the use of detectors that canmatch a number of subsets of nonself patterns (so that fewer detectors are needed) implies that thereexist some nonself patterns for which no detectors can be generated. in other words, a detector capableof matching such nonself patterns would also match some patterns found in self. furthermore, as thenumber of nonself patterns that can be recognized by a single detector increases, the number of problematic nonself patterns also increases. because they result from the structure of the set of self patterns,dynamic change in the detectors cannot find them.a solution that proved to be effective at reducing the overall number of holes (i.e., gaps in coverage)is multirepresentationñdifferent representations are used for different detectors. one way of achieving51see http://www.sanasecurity.com.52s. forrest, s.a. hofmeyr, and a. somayaji, òcomputer immunology,ó communications of the acm 40(10):8896, 1997.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.264catalyzing inquirythis is for each detector to have a randomly generated permutation rule, according to which all datapath triples are permuted before being matched against the detector. this effectively changes the structure of the self set for each detector, with the result that different detectors will be subject to differentholes. consequently, where one detector fails to detect a nonself triple, another may succeed.multirepresentation was particularly effective at reducing the number of holes when the nonself patterns were similar to self patterns. to deal with this problem, the bits in a given triplet of connectiontriplets were randomly permuted before presentation to detectors, just as the specific mhc moleculesthat are operating to bring pathogens to the surface are probabilistically determined (with respect to anaveraging over the population).8.2.5.6 some possible difficulties with an immunological approachalthough these analogies have appeal, it remains to be seen how far they can be pushed. given thatthe immune system is a very complex entity whose operation is not fully understood, a bottomupdevelopment of a computer security system based on the immune system is not possible today. thehuman immune system has evolved to its present state due to many evolutionary accidents as well as theconstraints imposed by biology and chemistryñmuch of which is likely to be artifactual and mostlyirrelevant to the underlying principles that the system embodies and also to the design of a computersecurity system. further, the immune system is oriented toward problems of survival. by contrast, computer security is traditionally concerned with confidentiality, accountability, and trustworthinessñandthe relevance of immunological processes to confidentiality and accountability is entirely unclear today.8.2.6 amorphous computingan area of research known as amorphous computing seeks to understand how to obtain òcoherentbehavior from the cooperation of large numbers of unreliable parts that are interconnected in unknown,irregular, and timevarying ways.ó53 this work, inspired by observations of cooperative and selforganizing biological phenomena, seeks to identify the engineering principles that can be used toobserve, control, organize, and exploit the behavior of cooperating multitudes for human purposes suchas the design of engineered artifacts.an individual entity in a collection of cooperating multitudes has the following characteristics:¥it is inexpensive, in the sense that it is easy to create large numbers of them. for all practicalpurposes, each entity is identical to every other one.¥it is locally guided or programmed. that is, the guidance or programming is carried by the entityòonboardó rather than being resident elsewhere in the overall system. as a consequence of fabrication,the guidance or programming aboard any given entity is identical to that aboard every other entity.¥it communicates with nearby entities, but in a stochastic manner without the need for preciseinterconnections and testing. note also that the ability to function in a stochastically connective environment implies that the overall macrosystem is robust in the face of damaged or nonoperational components. furthermore, by eliminating the need for precision interconnections, these entities can reduce theenormous costs usually associated with interconnection in traditional forms of assembly, costs that aregenerally higher than those associated with individual elements.¥it interacts with its environment locally, so that the entity is directly knowledgeable about someaspect of its immediate environment but not about anything more global. to the extent that an individual entity gains global knowledge about the environment, it is as the result of a selforganizingprocess that develops such information and transmits it to all entities in the system. similarly, any onboard effectors affect only the immediate environment.53see http://www.swiss.ai.mit.edu/projects/amorphous/.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.biological inspiration for computing265these characteristics are easily obtained by biology and are increasingly true for certain artifactsthat result from todayõs chip fabrication technologies. a metaphor with some resonance is that ofòpaintableó computersña paint that can be applied to a surface, in which are suspended millions ofcomputing and memslike entities that communicate with each other and interact with the surface onwhich they are painted. (mems is an acronym for microelectromechanical systems.)the vision presented by abelson et al.54 is that smart materials may reduce the need for strengthand precision in mechanical and electrical apparatus, through the application of computation. forexample, coating a building or a bridge with òsmart paintõõ may enable it to report on traffic loads andwind loads, to monitor the integrity of the structure, to resist buckling, or to heal small cracks by shiftingmaterial around. a different kind of smart paint may make it possible to build clean rooms with òactiveskins,ó lined with cilia that can push dirt and dust into a corner for removal. still other paints mayenable walls that sense vibration or actively cancel noise. òsmart dust,ó with light sensors in eachparticle, could be spread over a wide area to recognize shadows or other traffic passing overhead.in short, the hope is to create systems with unprecedented responsiveness to their environment.abelson et al. further argue that the study of amorphous computing has implications for softwaredesign in a more general sense. specifically, a software problem has long been recognizedñthe dependence of greater functionality of software on increasingly complex software packages and systems.today, software is mostly developed òby hand,ó and each line is individually coded. one obtains a highdegree of detailed control in this manner, but reliably abstracting the higherlevel behavior of a softwaresystem so developed is highly problematic. principles of amorphous computing may enable a more topdown specification of systems that more closely tracks how humans define the functionality they wishto obtain from software.amorphous computing may be applicable to fabrication as well. for example, consider amorphouscomputing entities that are capable of some mechanical interaction with the substrate on which they arepainted (e.g., they might expand or contract in certain directions). nagpal has demonstrated the feasibility of an amorphous computing substrate that is capable of pattern formation (box 8.1); if the entitiesmaking up this formation have the mechanical property described, it is conceivable that they might beable to warp a sheet onto which they were painted into a threedimensional structure.it is also conceivable that the vision described in amorphous computing and other approaches tothat area could be extended so that appropriately configured microentities could be programmed toselfassemble into useful physical structures on the nanoscale. these structures might be useful to endusers in and of themselves, or might serve as nanofabrication machinery that could construct otherstructures useful to the end user. in particular, the large macromolecules involved in the biochemistryof lifeñspecifically protein moleculesñdemonstrate the ability to configure themselves into structures,and some research seeks to coopt biochemical machinery to assemble structures designed for entirelyhuman purposes (as described in section 8.4.3).8.3 biology as implementer of mechanisms for computing8.3.1 evolutionary computation558.3.1.1 what is evolutionary computation?evolutionary computation is inspired by genetics and evolutionary events.56 given a particularproblem for which a solution is desired, evolutionary computation requires three components:54h. abelson, t.f. knight, g.j. sussman, et al., òamorphous computing,ó available at http://www.swiss.ai.mit.edu/projects/amorphous/whitepaper/amorphnew/amorphnew.html.55the discussion in section 8.3.1 owes much to melanie mitchell, now at portland state university in oregon.56evolutionary computation is a generic name for techniques that are based loosely on evolutionary principles. there are anumber of variants, including evolutionary programming, evolution strategies, genetic programming, and genetic algorithms,which have somewhat different emphases but share the generic approach.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.266catalyzing inquiry¥a population of candidate solutions to the problem. for example, these candidate solutions maybe a sequence of amino acids that can fold into some protein, a computer program, some encoding of thedesign for something, or some set of rules in a production system.¥a òfitnessó metric by which the ògoodnessó of a candidate solution can be evaluated. for example, if the program was intended to model the output of some designer circuit, the fitness metricmight be based on the performance of a candidate program acting on a test case. that is, given the testcase, the fitness metric would be the deviation of the output of the program from a known, appropriateanswer. programs that minimized this deviation would be more fit.box 8.1pattern formation using identical autonomous agentsin a 2001 ph.d. thesis, nagpal describes a language for instructing a sheet of identically programmed, flexible, and randomly but densely distributed autonomous agents (òcellsõõ) to assemble themselves into a predetermined global shape, using only local interactions. a wide variety of global shapes and patterns can besynthesized (patterns including flat layered shapes, all plane euclidean constructions, and a variety of tessellation patterns) using only local interactions between identically programmed deformable cells. that is, theglobal shape results from a coordinated set of local shape changes in individual cells. despite being programmed identically, each cell deforms in its own individualized manner, depending on the behavior andstate of its neighbors. (the governing structural metaphor is that of epithelial cells, which generate a widevariety of structures: skin, capillaries, and many embryonic structures (gut, neural tube) through the coordinated effect of many cells changing their individual shape.)the global shape is specified as a folding construction on a continuous sheet, using a small set of axioms, simpleinitial conditions (edges and corners of the sheet), and two types of folds. from an engineering standpoint, thesignificance of global shape description is that a process that is inherently local can be harnessed to produce ashape of known configuration. this differs significantly from approaches based on cellular automata, in whichthe localtoglobal relationship is not well understood and there is no framework for constructing local rules toobtain any desired pattern (and patterns òemergeó in a nonobvious way from the local interactions).in this formalism, the specific global shape desired uniquely determines the program executed by all cells. thecellular program is based on several (biologically inspired) primitives for interacting with the cellõs localenvironment. a cell can change the local environment in ways that create the equivalent of chemical gradients, query its local neighborhood and collect information about the state of local companions (e.g., collectneighboring values of a gradient), broadcast messages to all the cells in its local neighborhood, invert itspolarity, connect with neighbors in physical contact to establish communication (thus allowing multiplelayers of the sheet to act as a single fused layer), and fold itself along a particular orientation by calling thelocal fold within its program with two arguments: a pair of neighbors and a cell surface.each cell has limited resources and reliability. all cells execute the same program and differ only in a smallamount of local dynamic state. the cell program does not rely on regular cell placement, global coordinates,or synchronous operation. robustness against a small amount of random cell death is achieved by dependingon large and dense cell populations, using average behavior rather than individual behavior, trading offprecision for reliability, and avoiding any centralized control. further, global coordinates are not required,because cells are able to òdiscoveró positional information. an average cell neighborhood of 15 is sufficientto reliably selfassemble complex shapes and geometric patterns on randomly distributed cells.source: r. nagpal, òprogrammable selfassembly: constructing global shape using biologicallyinspired local interactions andorigami mathematics,ó ph.d. thesis, mit department of electrical engineering and computer science, june 2001.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.biological inspiration for computing267¥a mechanism (or mechanisms) by which changes to the candidate solutions can be introducedñportions of different candidate solutions are exchanged, for example, or modified in some small random way.57with these components in place, an evolutionary process takes place. the set of new solutions isevaluated for fitnessñthose with lower fitness scores are thrown out and those with higher scores areretained. this mutation process is iterated many times, and the result at the end is (supposed to be) asolution that is much better than anything in the starting set.initially demonstrated on the solving of what might be called òtoyó problems, evolutionary techniques have been used in a variety of business applications, including scheduling and productionoptimization, image processing, engine design, and drug design. evolutionary computation has alsoachieved results that are in some sense competitive with humandeveloped solutions to quite substantive problems. competitiveness has a number of possible measures, among them results that are comparable to those produced by a succession of human researchers working on a welldefined problemover a period of years, a result that is equivalent to a previously patented or patentable invention, aresult that is publishable in its own right (i.e., independent of its origins), or a result that wins or rankshighly in a judged competition involving human contestants.58evolutionary computation has demonstrated successes according to all of these measures. forexample, there are at least 21 instances in which evolutionary techniques have led to artifacts related topreviously patented inventions.59 eleven of these infringe on previously issued patents, and ten duplicate the functionality of previously patented inventions in a noninfringing way. also, while some ofthe relevant patents were issued many years ago (as early as 1917), others were issued as recently as2001. some of the inventions created by evolutionary processes include the ladder filter, the crossoverfilter, a secondderivative controller, a nand circuit, a pid (proportional, integrative, and derivative)controller, a mixed analogdigital variable capacitor circuit, a voltagecurrent conversion circuit, and acubic function generator. they have also created a soccerplaying program that won its first two gamesin the robo cup 1997 competition and another that ranked in the middle of the field of 34 humanwritten programs in the robo cup 1998 competition, four different algorithms for the transmembranesegment identification problem for proteins, and a variety of quantum computing algorithms, and haverediscovered the campbell ladder topology for lowpass and highpass filters.evolutionary computation also poses intellectual challenges, as described in the next severalsections.8.3.1.2 suitability of problems for evolutionary computation60whether or not an evolutionary approach will be successful in solving a given problem is not yetfully understood. although many components of a full theory of evolutionary algorithms have beenworked out, there are critical gaps that remain open questions.it is known that the relationship between the representation of a problem, genetic operators, and theobjective function is the primary determinant of the performance of an evolutionary algorithm. for anyoptimization problem, there is always a representation or a genetic operator that makes the optima easyto find with an evolutionary algorithm.61 in addition, evolutionary algorithms are no better or worse57in biology, òcrossoveró refers to the process in which chromosomal material is exchanged between chromosomes during cellduplication. the exchanged chromosomal material is analogous to portions of the different candidate solutions. òmutationsó aregenetic changes induced as the result of random environmental events.58see http://www.geneticprogramming.org.59see http://www.geneticprogramming.com/humancompetitive.html. more information on these accomplishments can befound in j.r. koza, m.a. keane, m.j. streeter, w. mydlowec, j. yu, and g. lanza, genetic programming iv: routine humancompetitive machine intelligence, series in genetic programming, volume 5, springer, new york, 2005.60lee altenberg of the university of hawaii was a major contributor to section 8.3.1.2.61g.e. liepins and m.d. vose, òrepresentational issues in genetic optimization,ó journal of experimental and theoretical artificial intelligence 2(2):101115, 1990.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.268catalyzing inquirythan any other search algorithm over the space of all problems.62 therefore, problemspecific knowledge must be incorporated either implicitly or explicitly in an evolutionary algorithm for it to performwell. finally, evolutionary algorithms are dynamical systems, and the systems properties necessary tomake them good search algorithms are well characterized.63the primary question that remains to tie together the above is the following: howñand whenñcanknowledge about the problem be translated into representations and genetic operators that produce anevolutionary algorithm with good performance?in the absence of this critical link in the theory of evolutionary algorithms, the approach taken bydesigners resorts to the empirical: try it out and see if it works. evolutionary approaches provide thegreatest advantage over other methods in cases where it is not understood how to construct answersfrom òfirst principlesó (i.e., logicodeductive procedures), but where approximate solutions can berefined by variation and testing. such problems can be characterized as òdifficult inverse problems,ówhere the inverse refers to finding inputs that produce desired outputs of the system in question.moreover, evolutionary techniques tend to work best on problems involving relatively large searchspaces and large numbers of variables that are not well understood. evolutionary algorithms have beenable to construct and adapt complex neural networks that are intractable analytically or for whichderivativebased backpropagation is inapplicable. genetic programming has produced complex circuits that infringe on patented inventions. by contrast, problems involving small search spaces canusually be searched systematically, and search spaces being well understood generally means thatspecialpurpose heuristics are available. (for example, the traveling salesman problem is reasonablywell understood, and there are very good special heuristics for solving that problem.)for problems in which evolutionary techniques are unable to find global optima, they may nevertheless find very good approximations that are robust to wideranging initial conditions. thus, thesolutions generated may be adequate to the task at hand. for this reason, evolutionary techniques mayalso be better when data are very noisy or in the presence of a varying fitness function: the algorithmmay rapidly produce approximate solutions that track the changing environment, just as evolvingspecies can track environmental changes. (an example of a problem calling for a varying fitness function might be a robot that must learn, online, in a dynamic environment, where the task facing the robotchanges over time.)8.3.1.3 correctness of a solutionone of the most challenging aspects of evolutionary computation is evaluating the correctness of asolution derived through evolutionary means. because evolutionary solutions are cumulative, in thesense that they build on previous solutions, the design process does not have an opportunity to developsolutions that are clean and elegantly designed from first principles. human inspection of a solution soderived is unlikely to yield much insight. thus, essentially the only way known today to assess thecorrectness of such a solution is to subject it to extensive testing. rather than a human being understanding how the solution achieves its goals, the proposed solution convinces a human being that it willdo so.note, however, that ascertaining the correctness of any large computational artifact (e.g., a complexsoftware system or a vlsi chip) depends to a large degree on testing. of course, because the thoughtand decisionmaking processes of human beings are not available to public inspection, it is only byobserving a human being in action that one develops confidence in the designerõs ability to perform62d.h. wolpert and w.g. macready, òno free lunch theorems for optimization,ó ieee transactions on evolutionary computation 1(1):6782, 1997, available at http://citeseer.ist.psu.edu/wolpert96no.html.63l. altenberg, òopen problems in the spectral analysis of evolutionary dynamics,ó pp. 73102 in frontiers of evolutionarycomputation, a. menon, ed., genetic algorithms and evolutionary computation series, volume 11, kluwer academic publishers, boston, ma, 2004.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.biological inspiration for computing269appropriately under certain circumstances. thus, in the limit of increasing complexity, testing an evolutionary solution may resemble the turing test. (in the turing test, an outside observer is asked todistinguish between a human beingõs answers to a set of questions and a computerõs answers. thecomputer is said to have passed the turing test if the outside observer is unable to distinguish betweenthe two.) 8.3.1.4 solution representationin biological organisms, the genetic code of dna is subject to changes (e.g., mutation), and theimpact of these changes becomes manifest as the new mutated code is involved in the reproductiveprocess. that is, the particular dna sequence of an organism can be said to be biologyõs representationof a òsolutionó to the problem of adapting the organism to a particular set of evolutionary selectivepressures.from the standpoint of someone solving a problem with techniques from evolutionary computation, the question arises as to the analogue of dna. more formally, how is a solution to a computationalproblem to be represented?in general, the solution to a computational problem is an algorithm. however, an algorithm can berepresented in many different ways. just as data can be represented as lists of numbers or in graphicalform, computer programs (which embed algorithms) can be represented as òsource codeó that is readable by human beings or as òobject codeóñthe raw ones and zeros of binary computation.if candidate solutions are to be computer programs, one might imagine that their machine languagerepresentation is an obvious possible representation. however, changing a machine language programone bit at a time, at random, is highly likely to prevent the (modified) program from running at all(because previously valid opcodes will be turned into invalid ones), and a nonrunning program isuseless. the same comments apply to the source code of a program. by randomly changing charactersin the source code file, the most likely result is a program that will not compile and therefore cannot beevaluated in any meaningful way. thus, attempting to evolve a binary program or the source code of aprogram would likely result in an extraordinarily slow rate of evolution.a more robust way to conduct this process is to impose the constraint that the program must beexecutable. thus, one might insist that the source code of a program be syntactically correct but not placeany limits whatsoever on its semantics (on what it does). for example, statements in a program can berepresented as combinations of functions with various numbers of arguments, and the only requirement for syntactic correctness is that a function have the right number of arguments.64 changes to theprogram can be effected by changing the functions and the specific arguments to the functions. theresult, by definition, is a program that is still syntactically correct, still runs, but does not necessarily dowhat is desirable. a typical initial program is then created by randomly generating a parse tree. apopulation of such parse trees is then subject to crossovers that exchange different parts of the variousparse trees, or mutations that replace one argument or function with a new argument or function.8.3.1.5 selection of primitivesclosely related to the issue of representations is the question of the appropriate semantic primitives(i.e., the smallest meaningful unit that can be changed). for example, in the representation of programsas parse trees, the relevant primitives are functions with arguments, and the efficacy of a geneticalgorithm is strongly dependent on the particular set of functions that the evolutionary process canmanipulate.64this approach is based on parse trees, a way of representing statements in computer programs. see j.r. koza, geneticprogramming: on the programming of computers by the means of natural selection, mit press, cambridge, ma, 1992.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.270catalyzing inquiryto illustrate, any computable function can in principle be built from the appropriate combination ofboolean operators (and, or, and not). but these functions operate at too low a level to build the kindof hierarchical structures needed to do anything complicated. it is for this reason that highlevel programming languages have emerged that are not based on these operators directly. such languagesallow the creation of many other kinds of structure. for example, a program intended to undertakefinancial analysis might benefit from an operator or function that would allow finding the average stockprice for the previous month. if its task were to evolve a program for financial analysis, such functionsmight be included in the set of primitives from which an evolutionary process might draw.one important aspect of the evolutionary approach is the ability to evolve new operators or newfunctions that can be used subsequently. in some instances, new structures can emerge spontaneouslythat are more or less stable; more frequently, it is possible to insert rules that will prevent such structures from changing. alternatively, functions can be defined automaticallyñthe environment providesslots for function and the ability to call on those function (even if they are noops), and the subsequentevolutionary process fills in those spaces with functions.658.3.1.6 more evolutionary mechanismsthe model described above is a very crude model of evolution, incorporating only a few bareessential features. however, biologists have characterized other features of evolution. two of the mostimportant with possible application to computing are coevolution and development; these are discussed below. other aspects of evolution, such as diploid behavior and sexual selection, do not at thisstage provide obvious new approaches to computing.8.3.1.6.1 coevolutioncoevolution refers to the biological phenomenon in which two or more speciesinteract as they evolve. for example, a host may be susceptible to infection by a parasite. the hostevolves some defenses against the parasite, which in turn stimulates the parasite to evolve ways inwhich to penetrate or circumvent those defenses. in coevolution, other speciesñwhich are also evolvingñconstitute part of the environment in which a given species is embedded.one application of coevolution to evolutionary programming is to allow the evolution of testingdata simultaneously with the solution. doing so enables the program to account for a wider range ofinput. in this case, one fitness function is required for the program to evaluate how well it performsagainst a given set of test data, while a different fitness function is needed for the test data to evaluatehow well it breaks the program.668.3.1.6.2 developmentdevelopment refers to the phenomenon in which biological complexity is shapedby growth within the organism (what might be called maturation) and the action of environmentalforces on the organism. it is very difficult to create significant complexity using genetic mechanismsalone. thus, one intellectual thrust in evolutionary computation focuses on the creation of developmental mechanisms that can be evolved to better create their own complexity. for example, evolutionarytechniques can be used to evolve neural networks (see section 8.3.3.2). in designing neural networks,the problems involve various issues related to the topology and configuration of the network. however,a grammar can be used to generate structures of interest. (a grammar is a formal system of rules thatcan be used to generate far larger structures.) grammars can evolve as well, with the fitness functionbeing the complexity of the structures it can generate.65j.r. koza, genetic programming, 11: automatic discovery of reusable programs, mit press, cambridge, ma, 1994.66d. hillis, òcoevolving parasites improve simulated evolution as an optimization procedure,ó physica d 42(13):228234,1990.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.biological inspiration for computing271in this case, the goal is to evolve a neural network that has the potential to learn things, rather thanevolving the things themselves that are the object of learning. in the case of a robotic brain, it is toodifficult to anticipate all of the possibilities that might face the robot, and thus it is impossible to developa fitness function that fully reflects this diversity. by giving the brain the ability to learn and reason, onecan circumvent this difficulty, and as long as one can develop a fitness function for how well the brainhas learned over some period, evolutionary techniques can be used to evolve a robotic brain. (note thatthe indirect nature of this approach makes it doubly difficult to understand what is going on.)an example of such work is that of sims (box 8.2).8.3.1.7 behavior of evolutionary processestoday, those working in evolutionary computation are not able to predict, in general, how long itwill take to evolve some desired solution or determine a priori how large an initial population sizeshould be, how rapidly mutations should occur, or how often genetic crossovers should take place.obviously, all of these parameters have some potential impact on the rate of evolution and how effective a solution might be. yet how they should be set and their possible relationship to the nature of agiven problem are, in general, not known, although some intuitions exist in this area.box 8.2genetic programming in animationin the world of computer graphics and animation, it can be difficult to build virtual creatures that behave in arealistic manner and simultaneously remain under the userõs direct control. for example, directly controllingthe positions and angles of moving objects such as limbs can result in detailed behavioral control, but likelyat the expense of achieving physically plausible motions. on the other hand, providing a realistic, physicsbased environment in which the relevant dynamics are simulated can result in a higher degree of realism, butwill likely make it difficult to achieve the desired behavior, especially as the entities involved become morecomplex.one way to manage the complexity of control is to optimize the behavior of the creature against some fitnessfunction. using evolutionary techniques, it is possible to fabricate creatures that behave realistically withoutunderstanding the procedures or parameters used to generate them. different fitness functions can representdifferent modes of movement (e.g., swimming, walking, jumping, following a source). this approach forcesthe user to sacrifice some detailed control, but there is also considerable gain in automating the creation ofcomplexityñand the user still influences the outcome by specifying the fitness function.for purposes of animation, a creature is determined by its physical morphology (e.g., size, shape, number oflegs) and the neural system for controlling the relevant muscle forces (the neural system involves sensors thattell the creature about its immediate environment, effectors that cause motion [analogous to muscles], andneurons that retain some memory of its previous states). both morphology and neural system can be evolved,resulting in a succession of increasingly òfitó creatures that move realistically in a given mode.in simsõ work, a developmental process was used to generate the creatures and their control systems. the useof such a process allowed similar components, including their local neural circuitry, to be defined once andthen replicated, instead of requiring each to be separately specified. thus, a coded representationña genotypeñof a creature was established that uniquely defined the phenotype of that creatureñits morphology andneural system. by evolving the genotype, different phenotypes emerged.source: adapted from k. sims, òevolving virtual creatures,ó computer graphics, annual conference series (siggraph ô94 proceedings), july 1994, pp. 1522.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.272catalyzing inquiryfor example, variation in a species results from mutations (involving random changes to a genome)and crossovers (involving exchanges of different parts of existing genomes). one hypothesis is thatcrossovers result in changes that are much more rapid than those driven by mutation. the argument infavor of this is that genomic exchange is in some sense enabling an organism to build on stable substructures. on the other hand, it may be that evolutionary solutions cannot make good use of existingsubstructures or that crossover is incapable of integrating existing substructures.if it is true that evolutionary change is more rapid with crossovers than with mutations, thissuggests that programs designed to evolve genetic programs may wish to emphasize crossover in theirprocesses for introducing variation.8.3.2 robotics 3: energy and compliance managementbiological systems provide an existence proof that selfeffected motion is possible. furthermore,compared to the locomotion made possible by human engineering, biological mechanisms capable oflocomotion appear to be energetically efficient, possible in a wide variety of physical environments, andoften small in size.given these characteristics, it is not unreasonable to ask what lessons biology might hold for thedesign of engineered systems for locomotion. for example, one reason that biological systems areenergetically efficient is that they are not rigid, but rather compliant, and often have mechanisms forenergy recovery. that is, these mechanisms store kinetic energy that might otherwise be dissipated,much as a braking electric car can store in batteries the kinetic energy associated with slowing down. akangaroo employs such a mechanism in its tail, which acts as a spring that compresses as the kangaroolands from one jump and then assists the kangaroo in pushing off for the next jump. full has argued thatleg locomotion can be described as a point mass attached to a spring and finds that the ratio of relativeleg stiffness67 to body mass is more or less constant across legged animals spanning a wide range ofsize.68 in this context, leg musculature functions not just as a source of power but also as an actuator, aspringy òstrutó that participates in energy absorption, storage, and return.a second example is that manylegged animals demonstrate an inherent dynamic stability. contrary to expectations that locomotion would require complex neural control feedback mechanisms, thestructure of the leg itself and its inherent multifunctionality provide a key aspect of the control of thesystem and the combination of stability and forward momentum needed for locomotion. indeed, analysis of manylegged animals reveals that this inherent stability arises from the production of large lateraland opposing leg forces when the legs are moving. modeling these forces as a spring between opposinglegs reveals that the system is highly stable against perturbationsñand the leg assembly is capable ofstabilizing itself without any equivalent of neural reflexes at all. thus, the animal does not need todevote expensive neurological processing to the supervision of locomotive tasks.raibert was one of the pioneers of robotics engineering based on physicsinspired control lawsñone for height, one for pitch, and one for speed. a fundamental insight was that running animals makeuse of dynamic stabilityña running animal moving forward is out of balance, but legs move forward inrhythm to break its fall. to model this phenomenon, a onelegged òanimaló (the òplanar oneleggedhopperó) was created. it consisted of a mechanized pogo stick with a threepart control systemñonecontrolling forward running speed, one controlling body attitude, and one controlling hopping height.stepping motion was not programmed explicitly, but rather emerged under the constraints of balance67relative leg stiffness is the weightnormalized, sizenormalized spring constant of the leg.68r. blickhan and r.j. full, òsimilarity in multilegged locomotion: bouncing like a monopode,ó journal of comparative physiology 173:509517, 1993; t.m. kubow and r.j. full, òthe role of the mechanical system in control: a hypothesis of selfstabilization in hexapedal runners,ó philosophical transactions of the royal society of london b 354:849862, 1999; a. altendorfer et al.,òrhex: a biologically inspired hexapod runner,ó journal of autonomous robots 11:207213, 2002.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.biological inspiration for computing273and controlled travel.69 with this basic unit, a twolegged running animal (the planar biped) could bemodeled as a body with two pogo sticks working 180° out of phase.70 a fourlegged animal couldconsist of two twolegged pairs working in opposition (left front and right rear, for example).71since raibertõs pioneering work, these insights have been applicable to the design of other artificiallegged locomotion devices. for example, an autonomous hexapod named òrhexó has a motor associated with each leg, each of which is springy and is able to turn on its central axis. this design enablesrhex to have selfcorrecting reflexes that enable it to respond to obstacles without computationalcontrol. another family of sixlegged robots, called the sprawl family, is cockroaches. each leg,driven by a piston, acts as a spring that enables sprawl robots to bounce over objects in their pathwithout feedback from the environment. analysis of the force pattern exerted by the legs closely matchesthat exerted by a running cockroach.other robots are intended to manipulate objects into precise orientations. the traditional way tobuild such robots is to build them rigidly, with limb motion effected through motors and gear assemblies to increase torque. however, gear assemblies are inherently imprecise, because their very motionrequires some degree of play where the gears meet (i.e., some nonzero compliance). in practice, theeffect of compliance in the gears introduces a noise function that greatly complicates the prediction ofhow a limb will move given a certain motor input, and puts limits on the precision with which the finalorientation can be known.one solution to this problem is to use òdirectdriveó motors placed at every joint, thus eliminatingthe gears entirely.72 another solution is based on the deliberate introduction of compliance into a gearassembly. this solution is based on the observation that humans can effect precise positioning withoutprecision in their joints. in particular, natural joints are often based on ballandsocket mechanisms evenwhen they are intended to exhibit 1 degree of freedom. soft tissue around and in the ball joint introduces compressive compliance in the joint, allowing it to absorb impact and automatically maintain adegree of tightness in the joint.in the robot context, pratt et al. inserted a spring mechanism into a limb joint so that the responselags the input.73 this spring adds a large but known compliance in series into the joint (socalled serieselasticity) that is much larger than the unknown compliance of the gears; thus, the gear compliance cansafely be ignored in the prediction of final position. entirely apart from the increased ease of prediction,the introduction of series elasticity enables a local response to any sudden changes in loadingñduringwhich time the motors involved can build up torque to handle that load. other benefits include shocktolerance, lower reflected inertia, more accurate and stable force control, less damage during inadvertent contact, and energy storage.8.3.3 neuroscience and computingnatural brains demonstrate an alternative to the traditional von neumann computing architecture(i.e., a fully serial information processor); thus, it is natural to consider possible lessons of neurosciencefor computer design. these lessons occur at varying levels of detail.69see http://www.ai.mit.edu/projects/leglab/robots/2dhopper/2dhopper.html; see also m.h. raibert and h.b. brown,jr., òexperiments in balance with a 2d onelegged hopping machine,ó asme journal of dynamic systems, measurement, andcontrol 106:7581, 1984.70see http://www.ai.mit.edu/projects/leglab/robots/2dbiped/2dbiped.html; see also j. hodgins and m.h. raibert, òplanar biped goes head over heels,ó proceedings asme winter annual meeting, boston, december 1987.71see http://www.ai.mit.edu/projects/leglab/robots/quadruped/quadruped.html; see also m.h. raibert, òfourlegged running with onelegged algorithms,ó pp. 311315 in second international symposium on robotics research, h. hanafusa and h. inoue,eds., mit press, cambridge, ma, 1985.72h. asada and t. kanade, òdesign of a directdrive mechanical arm,ó asme journal of vibration, stress, and reliability indesign 105(3):312316, 1983.73g.a. pratt, m.m. williamson, p. dillworth, j. pratt, k. ulland, and a. wright, òstiffness isnõt everything,ó preprints of thefourth international symposium on experimental robotics, iser õ95, stanford, ca, june 30july 2, 1995.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.274catalyzing inquiry8.3.3.1 neuroscience and architecture in broad strokesthe most general lesson is that much of human cognition depends on the ability to ignore most ofthe information made available by the senses.74 that is, a very high fraction of the raw information thatis accessible through sight, sound, and so on does not participate directly in the humanõs cognitiveprocesses. human and mammalian cognition is based on an architecture that involves a flexible, butlowcapacity, working memory and attentional selection mechanisms that place events and objects intoworking memory where they become available for cognitive processing.75this approach of selective attention stands in sharp contrast to traditional algorithms that aredesigned with the goal of seeking optimal solutions and based on the use of as much information aboutthe problem domain as possible. the architecture of biological computation has generally evolved witha different purposeñthe adequate management of a complex, changing, and potentially dangerousenvironment in real time (where òadequateó means òprovides for survivaló).this architecture is based on a twotrack processing arrangementña very flexible, albeit slowsystem that implements consciousness, awareness, and cognition but attends to only few things, and alarge number of online, fastacting, sensorymotor systems that bypass attention and awareness (e.g.,eye movements, head and hand movements, posture adjustments, and other reflex and reflexlikeresponses).koch et al. have investigated the utility of such a strategy in multiple contexts: (1) a saliencybasedvisual attention mechanism that selects highly òsalientó location in natural images for further processing;76 (2) a competitive, twoperson video game in which an algorithm that focuses on a restrictedportion of the playing field outperforms an òoptimaló player when a temporal limitation is imposed onthe duration of each move;77 and (3) an algorithm that rapidly solves the npcomplete binpackingproblem under most conditions.788.3.3.2 neural networksbiology affords an alternative computing model that (1) appears well suited for many illposedproblems constrained by uncertainty, which is the problem set for which digital machines to date havebeen reasonably ineffective; and (2) provides an existence proof that slow and noisy circuits can undertake very rapid computations of a certain class. furthermore, it provides huge numbers of workingexamples. although the mechanisms underlying nerve tissue computation are not well understooddespite many decades of study, the fact remains that biology has found incredibly good solutions tomany engineering problems, and these approaches may well serve to inform practical solutions forengineering problems posed by human beings. indeed, although biological tissue is not naturally suitedfor information processing as understood in traditional terms, the fact that biological tissue can doinformation processing suggests that the underlying architectural principles must be powerful indeed.neural networks are among the most successful of biologyinspired computational systems and aremodeled on the massively parallel architecture of the brainñand on the brainõs inherent ability to learn74c. koch, òwhat can neurobiology teach computer engineers?ó january 31, 2001, unpublished paper, available at http://www7.nationalacademies.org/compbiowrkshps/christofkochpositionpaper.doc.75f. crick and c. koch, òconsciousness and neuroscience,ó cerebral cortex 8(2):97107, 1998.76f. crick and c. koch, òconsciousness and neuroscience,ó cerebral cortex 8(2):97107, 1998; l. itti and c. koch, òa saliencybased search mechanism for overt and covert shifts of visual attention,ó vision research 40(1012):14891506, 2000; l. itti and c.koch, òtarget detection using saliencybased attention,ó search and target acquisition, rto meeting proceedings 45, nato,rtomp45, 2000; l. itti, c. koch, and e. niebur, òa model of saliencybased visual attention for rapid scene analysis,ó ieeetransactions on pattern analysis and machine intelligence (pami) 20:12541259, 1998.77j.g. billock, òattentional control of complex systems,ó ph.d. thesis, 2001, available at http://sunoptics.caltech.edu/~billgr/thesis/thesiscolor.pdf.78j.g. billock, d. psaltis, and c. koch, òthe match fit algorithm: a testbed for the computational motivation of attention,óinternational conference on computational science 2: 208216, 2001.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.biological inspiration for computing275from experience.79 a neural network is a network of nodes and links.80 the nodes, or units, are verysimple processors that correspond to neuronsñthe brainõs electrically active cellsñand are usuallyorganized in layers, while the links, or connections, are nodetonode data channels that correspond tosynapsesñthe junctions that convey nerve impulses from one neuron to the next. each node has anactivation level that corresponds to a neuronõs rate of firing off nerve impulses, while each link has anumeric weight that corresponds to the strength or efficiency of a synapse.digital òactivation energyó patterns are presented to the network via the òinput layer.ó81 from theinput layer, the activation surges through the various intermediate layers automatically, with the flowbeing shaped and channeled by the connection strengths in much the same way that the flow of nerveimpulses in the brain is shaped by synapses. once everything has settled down, the answer can be readout from the pattern of activation on a set of designated output nodes in the final layer.this computationbynetwork architecture is where parallelism is relevant:82 all of the nodes areactive at once, and the activation can travel on any number of paths simultaneously. it is also the basisof the systemõs ability to learn: since the flow of activation (and, thus, the computation) is shaped by theconnection weights, it can be reshaped by changing the weights according to some form of learning rule.how the connection weights are modified in response to the input patterns is the content of the learningrule. this seems similar in some ways to what happens in the cerebral cortex, where knowledge andexperience are encoded as subtle changes in the synaptic strengths. likewise in a neural network: withvery few exceptions, it will always contain some sort of builtin mechanism that can adjust the weightsto improve its performance.these brainlike characteristics give neural networks some decided advantages over traditionalalgorithms in certain contexts and problem types. because they can learn, for example, the networks canbe trained to recognize patterns and compute functions for which no rigorous algorithms are known,simply by being shown examples. (òthis is a letter b: b. so is this: b.ó) often, in fact, they can generalizefrom the training examples well enough to recognize patterns theyõve never seen before. and theirparallel architecture helps them keep on doing so even in the face of noisy or incomplete data or, for thatmatter, faulty components. the multiple data streams can do a lot to compensate for whatever ismissing.training a neural network generally involves the use of a large number of individual runs todetermine the best solution (i.e., a specific set of connection weights that enables the network to do itsjob).83 most learning rules have a parameter that controls the rate of convergence between the currentsolution and the global minimum and another that controls the degree to which the network will ignorelocal minima. once the network is trained to demonstrate satisfactory performance, it can be presentedwith other data.84 with new data, the network no longer invokes the learning rule, and the connectionweights remain constant.79note that neural networks are only one approach to the general problem of machine learning. a second general approachinvolves what is called statistical learning techniques, so called because they are techniques for the estimation of unknownprobabilistic distributions based on data. these techniques have not, as a rule, been derived from the consideration of biologicalsystems.80useful online tutorials can be found at http://neuralnetworks.aidepot.com/3minutes/ and http://www.colinfahey.com/2003apr20neuron/2003apr20neuron.htm.81some of this discussion is adapted from http://www.cs.wisc.edu/~bolo/shipyard/neural/neural.html.82note, however, that this does not represent parallelism on the scale of the brain, where the neurons are numbered in thehundreds of billions, if not trillions. the number of units in a neural network is more likely to be measured in the dozens. inpractice, moreover, these networks are usually simulated on ordinary, serial computersñalthough for specific applications theycan also be implemented as specialized microchips. (see the online tutorial at http://www.particle.kth.se/~lindsey/hardwarennwcourse/home.html.) still, the parallelism is there in principle.83some of this is adapted from http://www.cs.wisc.edu/~bolo/shipyard/neural/neural.html.84note that it is possible to òovertrainó a neural network, which means that the network cannot respond properly to anythingbut the training data. (this might correspond to rote memorization.) obviously, such a network is not particularly useful.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.276catalyzing inquiryneural networks are most useful for problems that are not amenable to computational approachesand are constrained by strict assumptions of normality, linearity, variable independence, and so on.85that is, they work well in classifying objects, capturing associations, and discovering regularities withina set of patterns where the volume, number of variables, or diversity of the data is very great; when therelationships between variables are vaguely understood or the relationships are difficult to describeadequately with conventional approaches; or when the problems in question are illposed and involvehigh degrees of uncertainty.86 in addition, they are well suited for problems that are subject to distortions in the input data.neural networks have been applied to a large number of realworld problems of high complexity,including the following.87¥optical character recognition. commercial ocr (optical character recognition) software packageshave incorporated neural network technology since the mid1980s, when it significantly increased theirability to recognize unfamiliar fonts and noisy, degraded documents such as faxes.88 today, ocrsystems typically use a mix of neural network and rulebased approaches.¥finance and marketing. neural networksõ ability to detect unanticipated patterns has made them afavored tool for analyzing market trends, predicting risky loans, detecting credit card fraud, managingrisk, and many other such tasks in the financial sector.89¥security and law enforcement. neural networksõ patterndetection ability has likewise made thema useful tool for fingerprint matching, face identification, and surveillance applications.90¥robot navigation. neural networksõ ability to extract relevant features from noisy sensor data canhelp autonomous robots do a better job of avoiding obstacles.91¥detection of medical phenomena. a variety of healthrelated indices (e.g., a combination of heartrate, levels of various substances in the blood, respiration rate) can be monitored. the onset of aparticular medical condition could be associated with a very complex (e.g., nonlinear and interactive)combination of changes on a subset of the variables being monitored. neural networks have been usedto recognize this predictive pattern so that the appropriate treatment can be prescribed.¥stock market prediction. fluctuation of stock prices and stock indices is another example of acomplex, multidimensional, but in some circumstances at least partially deterministic phenomenon.neural networks are being used by many technical analysts to make predictions about stock pricesbased on a large number of factors such as past performance of other stocks and various economicindicators.¥credit assignment. a variety of pieces of information are usually known about an applicant for aloan. for instance, the applicantõs age, education, occupation, and many other facts may be available.after training a neural network on historical data, neural network analysis can identify the most relevant characteristics and use them to classify applicants as good or bad credit risks.85this material adapted from http://cfei.geomatics.ucalgary.ca/matlab/ann.html.86see http://www.cs.wisc.edu/~bolo/shipyard/neural/neural.html.87see http://www.emsl.pnl.gov:2080/proj/neuron/neural/what.html; see also http://neuralnetworks.aidepot.com/applications.html. examples in the list below for the topics òdetection of medical phenomenaó through òengine managementó are taken from http://www.statsoftinc.com/textbook/stneunet.html#apps.88see http://www.scansoft.com/omnipage/ocr/. at the time, the state of the art in commercial ocr software was the rulebased approach, in which a system broke each character image into simple features and then identified the letters by reasoningabout curves, lines, and such. this approach worked wellñbut only if the fonts were known and the text was very clean.89see http://neuralnetworks.aidepot.com/applications.html; see also http://www.nd.com/ and http://www.walkrich.com/valueinvesting/howdo.htm.90see http://www.neurodynamics.com/.91see http://aidepot.com/botnavigation/obstacleintroduction.html.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.biological inspiration for computing277¥monitoring the condition of machinery. neural networks can be instrumental in cutting costs bybringing additional expertise to scheduling the preventive maintenance of machines. a neural networkcan be trained to distinguish between the sounds a machine makes when it is running normally (òfalsealarmsó) versus those it makes when it is on the verge of a problem. after this training period, theexpertise of the network can be used to warn a technician of an upcoming breakdown, before it occursand causes costly unforeseen òdowntime.ó¥engine management. neural networks have been used to analyze the input of sensors from anengine. the neural network controls the various parameters within which the engine functions, in orderto achieve a particular goal, such as minimizing fuel consumption.8.3.3.3 neurally inspired sensorsone of the first attempts to draw on the principles underlying biological sensors occurred in themid1980s, when researchers such as carver mead and his coworkers at caltech made their first attempts to create artificial retinas using vlsi technology,92 with hopedfor applications that ranged fromartificial eyes for the blind to better sensors for robots. a second, more recent example of a neurallyinspired sensor is the computational sensor of brajovic and kanade.93 many approaches toward improving machine vision have been based on better cameras with higher resolution and sensitivity, newsensors such as uncooled infrared cameras, and new recognition algorithms. but standard vision systems typically have high latency (a long time between registration of the image on the vision systemõssensors and image recognition), induced by the requirements of transferring large amounts of data fromthe sensor to the processor and processing those large amounts of data quickly. in addition, latencyincreases more or less linearly with image size. standard vision systems can also be very sensitive tosmall details in the appearance of an object in sensor images. a number of processorbased algorithmshave been developed that adjust for such variations, but they are often complex and ad hoc, and henceunreliable.the computational sensor approach borrows biological architectural principles to use lowlatencyprocessing and topdown sensory adaptation as techniques for speeding up vision processes. computational sensors are (usually) vlsi circuits that include onchip processing elements tightly coupled withonchip sensors, exploit unique optical design or geometrical arrangement of elements, and use thephysics of the underlying material for computation. the integration of sensor and processor elementson a vlsi chip enables latency to be reduced by a considerable factor and provides opportunities forfast processorsensor feedback in service of topdown adaptationñand computational sensors haveproduced an orderofmagnitude improvement in sensing and information processing itself, such asrange sensing, sorting, highdynamic range imaging, and display.8.3.4 ant algorithmsant colonies depend on workers that can collectively build nests, find food, and carry out a multitude of other complex tasks while having little or no intelligence of their own. further, they must do sowithout the benefit of a leader to organize their efforts. they also continue to do so even in the face ofoutside disruptions, or the failure and death of individual members, thereby exhibiting a high degree offlexibility and robustness.92m.a. sivilotti, m.a. mahowald, and c. mead, òrealtime visual computations using analog cmos processing arrays,ó pp.295312 in advanced research in vlsi: proceedings of the 1987 stanford conference, p. losleben, ed., mit press, cambridge, ma,1987.93v. brajovic, òcomputational sensor for global operations in vision,ó ph.d. thesis, carnegie mellon university, pittsburgh,pa, 1996.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.278catalyzing inquiry8.3.4.1 ant colony optimizationentomologists have devoted a great deal of research to figuring out how the social insects achievethese feats.94 their answers, in turn, have led computer scientists to devise a variety of òant algorithms,ó all of which attempt to capture some of those same qualities of bottomup selforganization,flexibility, and robustness.95 ant algorithms are an example of agentbased modelsña broad class ofsimulations that began to emerge in the early 1990s as researchers tried to model complex adaptivesystems on a computer. the idea was to represent different agents with variables that werenõt justnumbers, as they would be in conventional econometric models, but complex data structures that couldrespond and adapt to one anotherñrather like agents in the real world. (in practice, each agent could bemodeled as an expert system, a neural network, or any number of other ways.)the first antbased optimizationñthe ant colony optimization algorithmñwas created in theearly 1990s.96 the algorithm is based on observations of ant foraging, something that ants do with highefficiency. imagine that worker ants wandering far from the nest come across a rich food source. eachant carrying food back to the nest marks her trail by laying pheromone on the ground. when anotherrandomly moving ant encounters this previously marked trail, it will follow it with high probability andreinforce the trail with its own pheromone. this behavior is thus characterized by a positive feedbackloop in which the probability with which an ant chooses a given trail increases with the number of antsthat previously chose the same trail.because the first ant to reach the nest will be the one whose path just happens to be the shortest,there will be a period of time during which the shortest path is the only path to the nest. this factprovides a òseedó around which further pheromone depositions can occur and collectively converge ona path that is one of the shortest possible.the paradigmatic application of this algorithm is the traveling salesman problem. a salesman isassigned to visit a specified list of cities, going through each of them once and only once before returning to his starting point. in what sequence should he visit them so as to minimize his total distance?what makes the traveling salesman problem difficult is that there seems to be no guaranteed wayof finding the absolute shortest path other than to check every possible sequence, and the number ofsuch sequences grows explosively as the number of cities increases, quickly outstripping the computational ability of any computer imaginable.97 as a result, practical programmers have had to give up on94see, for example, e.o. wilson and b. hılldobler, the ants, belknap press of harvard university press, cambridge, ma, 1990.95overviews can be found in e. bonabeau, m. dorigo, and g. theraulaz, swarm intelligence: from natural to artificial systems,oxford university press, new york, 1999; e. bonabeau, òswarm intelligence,ó presented at the oõreilly emerging technologyconference, available at http://conferences.oreillynet.com/presentations/et2003/bonabeaueric.ppt; and e. bonabeau and g.theraulez, òswarm smarts,ó scientific american 282(3):7279, 2000.96m. dorigo, òoptimization, learning, and natural algorithms,ó ph.d. dissertation, politecnico di milano, italy, 1992; m.dorigo, v. maniezzo, and a. colorni, òthe ant system: an autocatalytic optimizing process,ó technical report no. 91016revised, politecnico di milano, italy, 1991; m. dorigo, v. maniezzo, and a. colorni, òpositive feedback as a search strategy,ótechnical report no. 91016, politecnico di milano, italy, 1991 (later published as m. dorigo et al., òthe ant system: optimization by a colony of cooperating agents,ó ieee transactions on systems, man, and cyberneticspart b 26(1):2941, 1996, available atfuture generationcomputer systems (special issues on ant algorithms) 16(8), 2000. dorigo maintains a web page on ant colony optimization,including an extensive bibliography (with many papers downloadable), plus links to tutorials and software, available at http://iridia.ulb.ac.be/~mdorigo/aco/about.html.97if there are n cities in the list, then the number of possible routes is on the order of n!ñthat is, n × (n ð 1) × (n ð 2). . . . × 2 ×1. (there are n choices of a place to start, n ð 1 choices of a city to visit next, n ð 2 choices to visit after that, and so on.) this isnothing much to worry about for small numbers: 10 cities yield only 10! = 3.628 million paths, which a personal computer couldexamine fairly quickly, but 20 cities would yield about 2.4 × 1018 pathsña (very fast) computer that examined one path pernanosecond would take more than 77 years to get through all of them; and 30 cities (30! = 2.65 × 1032) would keep that samecomputer busy for 8 quadrillion years. in computer science, this is a classic example of an npcomplete problem. an npcomplete problem is both np (i.e., verifiable in nondeterministic polynomial time) and nphard (any other np problem can betranslated into this problem). in an npcomplete problem, the number of computations required to solve it grows faster than anypower of its size. (òverifiable in nondeterministic polynomial timeó means that a proposed solution to this problem can beverified in polynomial time on a computer that can execute different instructions depending on its input. polynomial time meansa time that is proportional to some power of the problemõs size.)catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.biological inspiration for computing279finding the best solution to the traveling salesman problem and its relatives, and instead look foralgorithms that find an acceptable solution in an acceptable amount of time. many such algorithms havebeen developed over the years, and the ant colony optimization algorithm has proved to rank amongthe bestñespecially after dorigo and his colleagues introduced several refinements during the 1990s toimprove its scaling behavior.98variations of the algorithm have also been developed for practical applications such as vehiclerouting, scheduling, routing of traffic through a data network, or the design of connections betweencomponents on a microchip, and the scheduling of special orders in a factory.99 the technique isparticularly useful in such cases because it allows for very rapid rerouting in the face of unexpecteddisruptions in the network. among the successful commercial applications are plant scheduling for theconsumer products giant unilever; truck routing for the italian oil company pina petroli; supply chainoptimization and control for the french industrial gas supplier air liquide; and network routing forbritish telecom, france telecom, and mci.1008.3.4.2 other ant algorithmsant algorithms are based on two essential principles: (1) selforganization, in which global behaviorarises from a myriad of lowlevel interactions, and (2) stigmergy, in which the individuals interact withone another indirectly using the environment as an intermediary.101 that is, one individual changes itssurroundings (e.g., by laying a pheromone trail), and other individuals then react to those changes at alater time. as researchers have looked to other ant colony behaviors for inspiration, moreover, thosesame two principles turn up again and again.102 for example:¥sorting behavior. certain species of ants apparently have an instinct to keep their surroundingsclean; if dead ants are scattered through the nest at random, the workers will immediately begin movingall the corpses into neat little piles (albeit piles in random locations). these ants likewise seem to have aninstinct for keeping the brood chambers well organized; if workers are presented with a random jumbleof antstobe, they will quickly see to it that the eggs and micropupae are in the center, while the largerand more developed pupae and larvae are toward the outside where they have more room. simulatedants can produce much the same results by following a simple local rule: pick up any item that isisolatedñthat is, any item that has no others like it in the neighborhoodñand drop it whenever manyof those items are encountered. picking things up and then dropping them modifies the environment,while the constant shifting causes the piles and/or broods to selforganize fairly rapidly.98the algorithm and its refinements are discussed at length in chapter 2 of e. bonabeau, m. dorigo, and g. theraulaz, swarmintelligence: from natural to artificial systems, oxford university press, new york, 1999.99many of their key papers are available for downloading at m. dorigo, òant colony optimization,ó 2003, available at http://iridia.ulb.ac.be/~mdorigo/aco/about.html.100e. bonabeau, òswarm intelligence,ó presented at the oõreilly emerging technology conference, 2003, april 2225, 2003,santa clara, ca, available at http://conferences.oreillynet.com/presentations/et2003/bonabeaueric.ppt.101e. bonabeau, m. dorigo, and g. theraulaz, swarm intelligence: from natural to artificial systems, oxford university press,new york, 1999.102among the most notable of these investigators have been entomologist guy theraulaz of the french national center forscientific research (cnrs) and telecommunications engineer eric bonabeau, formally of france telecom. bonabeau, in particular, has been among the most active in the promotion and commercialization of ant algorithms, first as head of europeanoperations for the santa febased biosgroup and since 2000 as head of his own company, icosystem, inc., of cambridge,massachusetts. details of the various ant behaviors under study, and the algorithms drawn from them, can be found in e.bonabeau, m. dorigo, and g. theraulaz, swarm intelligence: from natural to artificial systems, oxford university press, new york,1999; e. bonabeau and g. theraulez, òswarm smarts,ó scientific american 282(3):7279, 2000; and e. bonabeau, òswarm intelligence,ó presented at the oõreilly emerging technology conference, 2003, available at http://conferences.oreillynet.com/presentations/et2003/bonabeaueric.ppt.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.280catalyzing inquiry¥division of labor. in order to gather food, maintain the nest, defend against predators, and so on,a colony has to allocate many different tasks among many different ants simultaneouslyñagain, without the benefit of central planning or individual intelligence. in many cases this is done by a physicalcaste system, so that workers do certain jobs, soldiers do others, and so on. yet ants will often allocatetasks even within a single caste. a simple mechanism that reproduces this behavior is to give eachindividual a response threshold for each task: once the stimuli associated with that task pass thethresholdñimagine the smell of accumulating garbageñthe individual gets to work. the result is thatindividuals with higher and higher thresholds keep pitching in until the stimuli are under control,leaving everyone else free to engage in tasks for which they have low thresholds.¥cooperative transport. if a single ant encounters a food item thatõs too big for her to carry alone(e.g., a dead cockroach), she will recruit nest mates via pheromones to help. now, however, without aleader or brains, they somehow have to start pulling in the same direction. a simple, twopart rule thatreproduces the observed behavior is (1) if the object is already moving in the direction youõre pulling,keep pulling, and (2) itõs not moving at all, or is moving in a different direction, reorient yourself atrandom and start pulling that way. the result is a sequence in which the ants start out pulling theirburden from every direction at once, to no effectñuntil suddenly, when enough ants just happen to lineup by accident, a kind of phase transition sets in and the load begins to move.¥cooperative construction. many species of social insects can build structures of astonishing complexity: witness the vast, hexagonal combs of the honeybee or the multilayered, intricately swirlingnests of the paper wasp. and yet again, they manage to do so without the benefit of central planning orindividual intelligence. one way to account for such behavior in simulated insects is to equip eachindividual with a collection of local rules: in situation 1, take action a; in situation 2, take action b; andso on. for a wasp carrying a load of wood pulp, say, such a rule might be, òif youõre surrounded bythree walls, then deposit the pulp.ó in general, each insect will modify the environment encountered bythe others, and the structure will organize itself in much the same way that the proteins comprising avirus particle assemble themselves inside an infected cell.ant algorithms are conceptually similar to the particle swarm optimization algorithm described insection 8.2.1. however, at least in the case of the ant colony optimization algorithm, it is known thatants really use the algorithm described. for this reason, this algorithm was placed in the category ofbiologically inspired mechanisms (rather than principles).8.4 biology as physical substrate for computing8.4.1 biomolecular computingthe idea of constructing computer components from single molecules or atoms is the logical, ifdistant, end point of the seemingly inexorable miniaturization of chips and has been foreseen at leastsince richard feynmanõs lecture òthereõs plenty of room at the bottomó in 1959.103 molecular computing would have significant advantages, most obviously minuscule size of the resulting component, butalso a potentially low marginal cost per component and extreme energy efficiency. however, the technology for the precision placing of single atoms or molecules on a large scale is still in its infancy.however, there is a significant shortcut available: to use biological molecules, including dna,rna, and various enzymes, as instruments to perform computational tasks. the sophisticated functions of dna and related molecules, coupled with the existing technological infrastructure for synthesizing, manipulating, and analyzing them found in molecular biology laboratories, make it feasible toemploy them as a universal set of computing components. also, because the code of dna is essentially103r.p. feynman, òthereõs plenty of room at the bottom,ó american physical society, december 29, 1959; available at http://www.zyvex.com/nanotech/feynman.html.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.biological inspiration for computing281a digital code, particular strands of dna can be used to code information, and in particular, joiningsand other recombinations of these strands can be used to represent putative solutions to certain computational problems.this idea is known variously as dna computation, molecular computation, and biomolecularcomputation (bmc). the use of dna as a computational system had been discussed theoretically by t.head in 1987,104 but the idea leapt into prominence with len adlemanõs publication in 1994 of aworking experiment (box 8.3) that solved a sevennode instance of the hamiltonian path problem, annpcomplete problem that is a special case of the traveling salesman problem.1058.4.1.1 descriptionearly attention has focused on dna because its properties are extremely attractive as a basis for acomputational system. first, it offers a digital abstraction: the value of a piece of dna can be preciselyand only a, g, t, or c. this abstraction is of course quite familiar to the digital abstractions of 0 and 1.second, the watsoncrick complementarity of the bases (a with t, g with c) allows matching operations, conceptually similar to òifó clauses in programming. third, dnaõs construction as a string allowsa number of useful operations such as insertion, concatenation, deletion, and appending. next, billionsof years of evolution have provided a large set of enzymes and other molecules that perform thoseoperations, some in very specific circumstances. finally, the last few decades of progress in molecularbiology have created a laboratory and instrument infrastructure for the manipulation and analysis ofdna, such as the custom synthesis of sequences of dna, chips that can detect the presence of individual sequences, and techniques such as polymerase chain reaction (pcr) that can amplify existingsequences. without such an infrastructure (importantly including the existence of a body of trainedlaboratory technicians), the use of dna for computation would be entirely theoretical.biomolecular computing provides a number of advantages that make it quite attractive as a potential base for computation. most obvious are its information density, about 1021 bits per gram (billions oftimes more dense than magnetic tape), and its massive parallelism, 1015 or 1016 operations per second.106 less immediately apparent, but of equal potential importance, is its energy efficiency: it usesapproximately 10ð19 joules per operation, close to the information theoretic limit (compared to 10ð9joules per operation for silicon).one class of biomolecular computing generates witness molecules for all possible solutions to aproblem and then uses molecular selection to sift out molecules that represent solutions to the problemat hand. this was the basic architecture developed by adleman (described in box 8.3), and with anexponential amount of witness material, this approach can theoretically solve npcomplete problems.short sequences of dna (or rna) are used to represent data, and these are combined to form longerstrands, each of which represents a potential solution. obtaining the particular dna strand that represents the solution is thus based on laboratory processes that extract the proper dna strand, and theselaboratory processes are based on the existence of an algorithm that can distinguish between correct andincorrect solutions.a further important step was taken in 2001 by benenson et al., who developed a programmablefinite automaton comprising dna and dnamanipulating enzymes that solves certain computationalproblems autonomously.107 in particular, the automatonõs òhardwareó consisted of a restriction nu104t. head, òformal language theory and dna: an analysis of the generative capacity of specific recombinant behaviors,óbulletin of mathematical biology 49(6):737759, 1987.105l.m. adleman, òmolecular computation of solutions to combinatorial problems,ó science 266(5187):10211024, 1994.106it is only the fact of massive parallelism that makes biological computing at all feasible, because biological switching speedsare diffusionlimited and quite slow.107y. benenson, t. pazelizur, r. adar, e. keinan, z. livneh, and e. shapiro, òprogrammable and autonomous computingmachine made of biomolecules,ó nature 414(6862):430434, 2001.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.282catalyzing inquirybox 8.3adleman and dna computingadleman used the tools of molecular biology to solve an instance of the directed hamiltonian path problem.a small graph was encoded in molecules of dna, and the òoperationsó of the computation were performedwith standard protocols and enzymes. this experiment demonstrates the feasibility of carrying out computations at the molecular level.the hamiltonian path problem is based on finding a special path through an arbitrarily connected set of nodes(i.e., an arbitrary directed graph). (the adjective òdirectedó means that the connections between nodes areunidirectional, so that a path from a to b does not mean necessarily that another connection from b to aexists.) this path (the hamiltonian path) is special in the sense that beginning with a specified entering nodeand ending with a specified exiting node, a continuous path exists that enters and exits every other node onceand only once. hamiltonian paths do not necessarily exist for a given directed graph, and their existence maydepend on an appropriate specific choice of entering and exiting nodes.all known algorithms for determining whether an arbitrary directed graph with designated vertices has a hamiltonian path exhibit worstcase exponential complexity, which means that there are some directed graphs with asmall number of nodes for which this determination would take an impractical amount of computing time.one method for determining if a hamiltonian path exists is illustrated in the first column of the table below.stepalgorithmic stepbiological equivalent0establish directed graph notationencode each node and directed nodetonode path asas problem representation.a specific dna sequence.1generate all possible pathscombine large amounts of these dna sequences,through the graph.and with a sufficiently large quantity, the probabilitythat all possible paths will be generated is essentiallyunity. (in general, these various combinations willbe in length several multiples of a single sequence.)2keep only those paths that beginuse polymerase chain reaction (pcr) that amplifieswith a specified starting andonly those molecules encoding paths that begin andending node.end with the specified nodes.3if the graph has n nodes,separate only those sequences from step 2 that havethen keep only those paths that enterthe correct length (corresponding to the number ofexactly n nodes.nodes in the graph).4keep only those paths that enterseparate the sequences from step 3 that have aall of the nodes of the graph atsubsequence corresponding to each and every node.least once.5if any paths remain, say, òyes, ause pcr amplification on the output of step 4, whathamiltonian path existsó;remains after step 5 represents the solution to theotherwise, say òno.óproblem.source: adapted from l.m. adleman, òmolecular computation of solutions to combinatorial problems,ó science 266(5187):10211024, 1994.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.biological inspiration for computing283clease and ligase, while the software and input were encoded by doublestranded dna. programmingwas implemented by choosing appropriate software molecules. the automaton processed the inputmolecule through a cascade of restriction, hybridization, and ligation cycles, producing a detectableoutput molecule encoding the automatonõs computational result. however, a finitestate automaton isnot turingcomplete, and the actual demonstration of a turingcomplete biomolecular machine with aset of primitives sufficient for universal computation has yet to be shown experimentally.108since adlemanõs initial publication, researchers have explored many variants of the basic biologicalapproach. one such variant is the use of rna, which simplifies the process of removing invalid sequences. in this variant, rna is used for the solution sequences and dna is used to represent anelement of an invalid solution. thus, any potential solution that was invalid would be represented by adnarna hybridized double strand. a single enzyme, ribonuclease h, destroys all dnarna hybridized pairs, leaving only valid solutions. this is significantly simpler than the use of many, potentially noncompatible enzymes necessary to mark and destroy the appropriate dnadna hybrids in thetraditional method. (in developing an algorithm based on rna computing for solving a certain chessproblem, cukras et al.109 found that although the algorithm was able to recover many more correctsolutions than would be expected at random, the persistence of errors continued to present the mostsignificant challenge.)other variants of the process seek to automate or simplify the management of stages of the reactions. in the original experiments, the dna reactions took place in solution in test tubes or othercontainers, with stages of the process controlled by humansñfor example, by introducing new enzymes, changing the temperature (perhaps to break chemical bonds), or mixing dna solutions. some ofthese steps can be automated through the use of laboratory robotics. in some variants, dna strands arechemically anchored to various types of beads; these beads can be designed with different properties,such as being magnetic or electrically charged, allowing the manipulation of the dna strands throughthe application of electromagnetic fields. another solution is to use microfluidic technologies, whichconsist of mems devices that operate as valves and pumps; a properly designed system of pipettes andmicrofluidic devices offers significant advantages by automating tasks and reducing the total volume ofmaterials required.110still another variant is to restrict the chemical operations to a surface, rather than to a threedimensional volume.111 in this approach, dna sequences, perhaps representing all of the solutionspace of an np problem, would be chemically attached to a surface. challenges in this approach includethe attachment chemistry, addressing particular strands on the surface, and determining whether chemical attachment interferes with dna hybridization and enzymatic reactions.a second class of biomolecular computing begins with an input and a program represented in amolecular form and evolves the program in a number of steps to process the input to produce an output.in this approach, the complexity of the problem does not manifest itself in the number of startingmolecules, but rather in the form of the rules provided and the amount of time or number of stepsneeded to fully evaluate a particular problem and input. for example, in the programmed mutagenesismethod, dna molecules that represent rewrite rules are combined with dna molecules that encodeinput data and program. when the combined mixture of these dna molecules is thermally cycled in the108however, rothemund has provided a highly detailed description of a turingcomplete dna computer. see p.w.k.rothemund, òa dna and restriction enzyme implementation of turing machines,ó pp. 75119 in dna based computers: proceedings of a dimacs workshop, vol. 27, r.j. lipton and e.b. baum eds., dimacs series in discrete mathematics and theoreticalcomputer science, american mathematical society, princeton, nj, 1996.109a.r. cukras, d. faulhammer, r.j. lipton, and l.f. landweber, òchess games: a model for rna based computation,óbiosystems 52(13):3545, 1999.110a. gehani and j.h. reif, òmicroflow biomolecular computation,ó biosystems 52(13):197216, october 1999.111l.m. smith, r.m. corn, a.e. condon, m.g. lagally, a.g. frutos, q. liu, and a.j. thiel, òa surfacebased approach to dnacomputation,ó journal of computational biology 5(2):255267, 1998.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.284catalyzing inquirypresence of dna polymerase and dna ligase, the rewrite rules cause new dna molecules to beproduced that represent intermediate states in a computation. these new dna molecules can be a verygeneral function of the beginning mixture of dna molecules, and a dna encoding has been discoveredthat permits such a system to theoretically implement arbitrary computation.8.4.1.2 potential application domainsthe field of biomolecular computing is still composed of theory and tentative laboratory steps; weare years away from commercial activity. the results of laboratory experiments are proofs of concept; asyet, no biomolecular computer has outperformed an electronic computer.biomolecular computing is, in principle, well suited for problems that involve òbrute forceó solutions, in which candidate solutions can be tested individually to see if they are correct. as noted above,the main application pursued for the first decade of biomolecular computing work is the exhaustivesolution of npcomplete problems. while this has been successful for small numbers of nodes (up to 20),the fact that it requires exponential volumes of dna most likely limits the further development of npsolving systems (see below for further discussion).biomolecular computation also has potential value in the field of cryptography. for example, dna,with its incredible information density, could serve as an ideal onetime pad, as a tiny sample couldprovide petabytes of data suitable for use for encryption (as long as it was suitably random). moregenerally, biomolecules could serve as components of a larger computational system, possibly servingalongside traditional siliconbased semiconductors. for this, and indeed any biomolecular computingsystem, a challenge is the transformation of information from digital representation into biomoleculesand back again. traditional molecular biological engineering has provided a number of tools for synthesizing dna sequences and reading them out; however, these tend to be fairly lengthy processes. recentadvances in dna chips show the potential for more efficient biodigital interfaces. for example, photosensitive chips will synthesize given sequences of dna based on optical inputs and, similarly, willproduce optical signals in the presence of certain sequences. these optical signals are twodimensionalarrays of intensities that can be read by digital imageprocessing hardware and software. other approaches for output include the inclusion of fluorescent materials in the dna molecules or otheradditives that can be detected with the use of microscopy.a potential component role for biomolecules is as memory. whereas biomolecular computationmust compete against rapidly improving and increasingly parallel optoelectronic technologies for computation, biomolecular memory is many orders of magnitude superior to conventional magnetic implementations in terms of density. although dna memory is unlikely to be used as the rapidaccess readwrite memory of modern computers, its density makes it useful for òblackboxó applications that writea great deal of data, but read only on rare occasions (a fact that would usually tend to increase theacceptable retrieval time).one such implementation would use dna as the storage medium of an associative database. adna strand would encode the information of a specific record, with sequences on that strand representing attributes of the record and a unique index. query strings would be composed of the complement of the desired attribute. although individual lookups would be slow (limited by the speed ofdna chemistry), the total amount of information stored would be enormous and the queries wouldexecute in parallel over the entire database. in contrast, conventional electronic computer implementations of associative memory require linear time with the size of the database.such a dna database might be most useful as a set of tools to manipulate, retrieve, or analyzeexisting biological or chemical substances. for example, specialpurpose dna computers might searchthrough databases of genetic material. in this model, a large library of genetic material (perhaps representing dna sequences of various biological lineages, or of criminals) would be stored in its originaldna form, rather than as an electronic digital representation. biomolecular computers would generateappropriate strands representing a query (matching a sequence found in a new organism, or at a crimecatalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.biological inspiration for computing285scene) and, in massively parallel fashion, identify potential matches. this idea could even be extendedto queries of proteins or chemicals, if the appropriate query strand of dna can be generated.a separate approach to biomolecular memory uses changes in the sequence of individual strands torepresent bits. certain enzymes known as sitespecific recombinases (ssrs) can (among a set of otherpotential modifications) reverse the sequence of the bases between two marker sequences; repeatedapplication of such an enzyme would flip the sequence back and forth, representing 0 and 1. in thisimplementation, a single bit requires a long series of bases; research aims at attaining the far more denseuse of single bases as bits (in fact, as two bits, since each base can have four values).8.4.1.3 challengesbiomolecular computing faces some significant challenges to adoption beyond the laboratory. themost cited barrier is the exponential doubling of the volume of dna required to perform exhaustivesearch of npcomplete problems, such as done by adleman (section 8.4.1.1). that is, while the numberof different dna sequences required grows linearly with the number of directed paths in a graph, thevolume of those dna sequences needed to solve a given problem is exponential in the problemõs size(in this case, the number of nodes in the graph). put differently, for the problems to which dnacomputing is applicable, a problem that can be solved in exponential time on siliconbased vonneumann computers is replaced by one that can be solved with exponential increases of mass. it is thusan open question today about what kinds of problems can be solved practically using dna computing.for example, hartmanis reports that the amount of dna necessary to replicate adlemanõs experimentfor a 200node problem would exceed the weight of the earth.112while this is a valid concern, standard computers have been widely accepted despite their inabilityto solve npcomplete problems in a timely fashion. to the best understanding of computer sciencetoday, npcomplete problems are fundamentally challenging, and so it ought to be no surprise thateven new models of computation struggle with them. nevertheless some breakthrough may providesubexponential scaling for biomolecularbased exhaustive search.a second concern involves the timeconsuming and expensive laboratory techniques necessary toset up and read out the answer from an experimentñin essence, the inputoutput problem for biomolecular computing. while dna reactions themselves offer staggering parallelism (although in factthey take about an hour), the bottleneck may be the time it takes for trained humans to undertake theexperiment. adlemanõs experiment required about 7 days of laboratory work. and although dnasynthesis itself is cheap, some of the enzymes used in adlemanõs experiments cost 10,000 times as muchas gold,113 suggesting that scaling up significantly may not be feasible on economic grounds.related to this is the fact that dna computation is not errorfree. synthesis of sequences canintroduce errors; strands of dna that are close to being complementsñbut not quiteñmay still hybridize; point mutations may occur; sheer chance may allow strands of dna to escape enzymatic destruction; and so forth. although comparatively high error rates can be acceptable in laboratory environments, this is far more problematic for computation. the problem can be ameliorated partly by the useof techniques familiar to communications protocols, including errorcorrecting codes and careful designof the code words used in computation, so as to maximize the information distance between any pair.this last example is a good case of computer science and biological cooperation: the distance between apair of code words composed of a series of bases is a product of both its information content and itsbiochemical properties. word design is currently an active area of dna computation research.112j. hartmanis, òon the weight of computations,ó bulletin of the european association for theoretical computer science 55:136138, 1995.113a.l. delcher, l. hood, and r.m. karp, òreport on the dna/biomolecular computing workshop (june 67, 1996),ó national science foundation, nsf 97168, 1998, available at http://www.nsf.gov/pubs/1998/nsf97168/nsf97168.htm.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.286catalyzing inquirya related problem is the lack of programmability of current models. even if experimental verification of turingcomplete biomolecular computing can be achieved, individual runs must still be carefully tuned to a specific instance of a specific problem, much like the hardwiring of the first generationof electronic computers. worse yet, the sequences of biomolecules synthesized for a particularbiomolecular computation are usually consumed or destroyed during the computation. for a replication of the experiment, even with the same dataset, much of the entire process of setup must berepeated. if a different dataset or a different òprogramó is run, then other steps must be included, suchas designing the set of sequences to be used as òwordsó of the computation and determining the set ofenzymes and concentration levels necessary to correctly identify, mark, destroy, and read out theappropriate strands of nucleic acids. the ability to formulate a problem of any generality in terms thatmap onto a set of chemical processing lab procedures is likely an essential aspect of dna computing,but it is not at all clear today how such formulations can occur in general.finally, the most significant challenge is the high bar that dna computation will have to surpass togain wide acceptance. mooreõs law is expected to continue unabated for at least a decade, resulting inpetaflop machines by 2015. additionally, biomolecular computation is not the only radical technique intown; quantum computation, various other applications of nanotechnology, analog computing, andother contenders may turn out to offer more favorable performance, programmability, or convenience.these challenges are quite significant and possibly decisive. len adleman himself was pessimisticabout the prospect of general computation in a 2002 paper: òdespite our successes, and those of others,in the absence of technical breakthroughs, optimism regarding the creation of a molecular computercapable of competing with electronic computers on classical computational problems in not warranted.ó114 of course, such breakthroughs may yet occur, and this possibility warrants some level ofcontinued research.8.4.1.4 future directionswhile it was dnaõs resemblance to the tape of a turing machine that inspired adleman to investigate the possibility, this model has not yet been pursued experimentally. nor is it likely that it wouldhave practical computing utilityña turing machine is extraordinarily slow even executing simplealgorithms.a very different approach would involve single molecules of dna (or rna or another biomolecule)acting as the memory of a single process, while enzymes performed the computation by splicing andcopying sequences of bases. although this has been discussed theoretically, it has not yet been shown inan experiment. this model would be best used for massively parallel applications, since the individualoperations on dna are still quite slow compared to electronic components, but it would offer massiveimprovements of density and energy efficiency over traditional computers.in a slightly different approach, enzymes that operate on dna sequences are used as logic gates,such as xor, and, or not. dna strands are data, and the enzymes, by reacting to the presence ofcertain sequences, modify the dna or generate new strands. thus, using fairly traditional digital logicdesign techniques, assemblies of logic gates can be constructed. the resulting circuits will operate inexactly the same manner as traditional silicon electronicbased circuits, but at the energy efficiency andsize of molecules.115even if it turns out that biomolecular computation is a dead end, the research that went into it willnot be for naught: the laboratory techniques, enabling technologies, and deeper understanding of114r.s. braich, c. johnson, p.w.k. rothemund, d. hwang, n. chelyapov, and l.m. adleman, òsolution of a 20variable 3satproblem on a dna computer,ó science 296(5567):499502, 2002.115m.n. stojanovic, t.e. mitchell, and d. stefanovic, òdeoxyribozymebased logic gates,ó journal of the american chemicalsociety 124(14):35553561, 2002.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.biological inspiration for computing287biomolecular processes will be valuable. already, commercial spinoff technologies are available: basedon adlemanõs research, a company in japan developed a way to synthesize 10,000 dna sequences torapidly search for the presence of genes related to cancer.116 also, biologist laura landweberõs researchinto biomolecular computation at princeton has provided insights for her research on dna and rnamechanisms in living organisms. for example, her and lila kariõs analysis of the dna manipulationsthat occur in some protozoa is based on techniques of formal languages from computer science, showing that the cellular operations performed by these protozoa are actually turingcomplete. the use offormal computer science theory, in other words, has proven a useful tool for the analysis of naturalgenetic processes.8.4.2 synthetic biologyas a field of inquiry, the goal of biologyñreductionist or otherwiseñhas been to catalog the diversity of life and to understand how it came about and how it works. these goals emphasize the importance of observation and understanding. synthetic biology, in contrast, is a new subfield of biology withdifferent intent: based on biological understanding, synthetic biology seeks to modify living systemsand create new ones.synthetic biology encompasses a wide variety of projects, definitions, and goals and thus is difficultto define precisely. it usually involves the creation of novel biological functions, such as custom metabolic or genetic networks, novel amino acids and proteins, and even entire cells. for example, a synthetic biology project may seek to modify escherichia coli to fluoresce in the presence of tnt, creating ineffect a new organism that can be used for human purposes.117 in one sense, this is a mirror image ofnatural selection: adding new features to lineages not through mutation and blind adaptation to anenvironment, but through planned design and forethought. synthetic biology shares some similaritieswith recombinant genetic engineering, a common approach that involves transplanting a gene from oneorganism into the genome of another. however, synthetic biology does not restrict itself to using actualgenes found in organisms; it considers the set of all possible genes. in effect, synthetic biology involveswriting dna, not merely reading it.one basic motivation of this field is that creating artificial cells, or introducing novel biologicalfunctions, challenges our understanding of biology and requires significant new insight. in this view,only by reproducing life can we demonstrate that we fully understand it; this is the ultimate acid test forour theories of biology. it is precisely analogous to early synthetic chemistry: only by the successfulsynthesis of a substance would a theory of its composition be verified.118more broadly, some synthetic biology researchers see created life as an opportunity to explorewider conceptions of life beyond the examples provided by nature. for example, what are the physicallimitations of biological systems?119 are other selfreplicating molecular information systems possible?are there general principles of biochemical organization? these inquires may help researchers to understand how life began on earth, as well as the possibility of life in extraterrestrial environments.120finally, synthetic biology has the potential to contribute significantly to technology, offering inmany ways a new industrial revolution. in this view, chemical synthesis, detection, and modificationcould all be done by creating a microbe with the desired characteristics. this holds the promise of newmethods for energy production, environmental cleanup, pharmaceutical synthesis, pathogen detection116business week, òlen adleman: tapping dna power for computers,ó january 4, 2002.117l.l. looger, m.w. dwyer, j.j. smith, and h.w. hellinga, òcomputational design of receptor and sensor proteins withnovel functions,ó nature 423(6936):185190, 2003.118s.a. benner, òact natural,ó nature 421:118, 2003.119d. endy, quoted in l. clark, òwriting dna: first synthetic biology conference held at mit,ó available at http://web.mit.edu/be/news/synthbio.htm.120j.w. szostak, d.p. bartel, and p.l. luisi, òsynthesizing life,ó nature 409(6818):387390, 2001.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.288catalyzing inquiryand neutralization, biomaterials synthesis, or any task that can be done by biochemistry. this is essentially a form of nanotechnology, in which the already existing mechanisms of biology are employed tooperate on structures at the molecular scale.however, all of these goals will require a different set of approaches and techniques than traditionalbiology or any natural science provides. while synthetic biology employs many of the same techniquesand tools as systems biologyñsimulation, computer models of genetic networks, gene sequencing andidentification, massively parallel experimentsñit is more of an engineering discipline than a purelynatural science.8.4.2.1 an engineering approach to building living systemsalthough as a viewpoint it is not shared by all synthetic biology researchers, a common desire is toinvent an engineering discipline wherein biological systems are both the raw materials and the desiredend products. engineeringñparticularly, electronics designñis an appropriate discipline to draw on,because no other design field has experience with constructing systems composed of millions or evenbillions of components. the engineering design approaches of abstraction, modularity, protocols, andstandards are necessary to manage the complexity of the biomolecular reality.one important piece of establishing an engineering discipline of building living systems is to createa library of welldefined, wellunderstood parts that can serve as components in larger designs. a teamled by tom knight and drew endy at the massachusetts institute of technology (mit) have created themit registry of standard biological parts, also known as biobricks, to meet this need.121 an entry in theregistry is a sequence of dna that will code for a piece of genetic or metabolic mechanism. each entryhas a set of inputs (given concentrations or transcription rates of certain molecules) and a similar set ofoutputs.the goal of such a library is to provide a set of components for wouldbe synthetic biology designers, where the parts are interchangeable, components can be composed into larger assemblies and easilybe shared between separate researchers, and work can build on previous success by incorporatingexisting components. taken together, these attributes allow the designers to design in ignorance of theunderlying biological complexity.these biobricks contain dna sequences at either end that are recognized by specific restrictionenzymes (i.e., enzymes that will cut dna at a target sequence); thus, by adding the appropriate enzymes, a selected dna section can be spliced. when two or more biobricks sequences are ligatedtogether, the same restriction sequences will flank the ends of the dna sequence, allowing the researcher to treat the composite as a single component. biobricks are in the early stages of research still,and the final product will likely be substantially different in construction.8.4.2.2 cellular logic gatesof particular interest to synthetic biologists are modifications to cellular machinery that simulatethe operations of classical electronic logic gates, such as and, not, xor, and so forth. these arevaluable for many reasons, including the fact that that their availability in biological systems wouldmean that researchers could draw on a wide range of existing design experience from electronic circuits.such logic gates are especially powerful because they increase the ability of designers to build moresophisticated control and reactivity into engineered biological systems. finally, it is the hope of someresearchers that, just as modern electronic computers are composed of many millions of logical gates, anew generation of biological computers could be composed of logic gates embedded in cells.121t. knight, òidempotent vector design for standardassembly of biobricks,ó available at http://docs.syntheticbiology.org/biobricks.pdf.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.biological inspiration for computing289researchers have begun to construct cellular logic gates in which signals are represented by proteinconcentrations rather than electrical voltages, with the intent of developing primitives for digital computing on a biological substrate and control of biological metabolic and genetic networks. in otherwords, the logic gate is an abstraction of an underlying technology (based on silicon or on cellularbiology): once the abstraction is available, the designer can more or less forget about the underlyingtechnology.a biological logic gate uses intracellular chemical mechanisms, such as the genetic regulatorynetwork, metabolic networks, or signaling systems to organize and control biological processes, just aselectronic mechanisms are used to control electronic processes.any logic gate is fundamentally nonlinear, in the sense that it must be able to produce two levels ofoutput (zero and one), depending on the input(s), in a manner that is highly insensitive to noise (hence,subsequent computations based on the output of that gate are not sensitive to noise at the input). thatis, variations in the input levels that are smaller than the difference between 1 and 0 must not besignificant to the output of the gate.once a logic gate is created, all of the digital logic design principles and tools developed for use inthe electronic domain are in principle applicable to the construction of systems involving cellular logic.a basic construct in digital logic is the inverting gate. knight et al.122 describe a cellular inverterconsisting of an òoutputó protein z and an òinputó protein a that serves as a repressor for z. thus,when a is present, the cellular inverter does not produce z, and when a is not present, the inverter doesproduce z. one implementation of this inverter is a genetic unit with a binding site for a (an operator),a site on the dna at which rna polymerase binds to start transcription of z (a promoter), and astructural gene that codes for the production of z.protein z is produced when rna polymerase binds to the promoter site. however, if a binds to theoperator site, it prevents (represses) the binding of rna polymerase to the promoter site. thus, ifproteins have a finite lifetime, the concentration of z varies inversely with the concentration of a. toturn this behavior into digital form, it is necessary for the cellular inverter to provide low gain forconcentrations of a that are very high and very low, and high gain for intermediate concentrations of a.overall gain can be increased by providing multiple copies of the structural gene to be controlled bya single operator binding site. where high and low concentrations call for low gain, a combination ofmultiple steps or associations into a single pathway (e.g., the mitogenactivated protein [map]kinasepathway, which consists of many switches that turn on successively) can be used to generate a muchsharper nonlinear response for the system as a whole than can be obtained from a single step.once this inverter is available, any logic gate can be constructed from combinations of inverters.123for example, a nand gate can be constructed from two inverters that have different input repressors(e.g., a1 and a2) but the same output protein z, which will be produced unless both a1 and a2 arepresent. on the other hand, cellular logic and electronic logic differ in that cellular logic circuits aremore inherently asynchronous because signal propagation in cellular logic circuits is based on diffusionof proteins, which makes both synchronization and high speed very hard to achieve. in addition,because these diffusion processes are, by definition, not channeled in the same way that electricalsignals are confined to wires, a different protein must be used for each unique signal. therefore, thenumber of proteins required to implement a circuit is proportional to the complexity of the circuit.using different proteins means that their physical and chemical properties are different, thus complicating the design and requiring that explicit steps be taken to ensure that the signal ranges for coupledgates are appropriately matched.122t.f. knight and g.j. sussman, òcellular gate technology,ó unconventional models of computation, c. calude, j. casti, andm.j. dinneen, eds., springer, auckland, new zealand, 1998.123in general, the availability of an inverter is not sufficient to compute all boolean functionsñan and or an or function isalso needed. in this particular case, however, the implementing technology permits inverters to be placed side by side to formnotand (nand) gates.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.290catalyzing inquirycellular circuits capable of logic operations have been demonstrated. for example, elowitz andleibler designed and implemented a threegene network that produced oscillations in protein concentration.124 the implemented network worked in only a fraction of the cells but did, in fact, oscillate.gardner et al. built a genetic latch that acted as a toggle between two different stable states of geneexpression.125 they demonstrated that different implementations of the general designs yielded moreor less stable switches with differing variances of concentration in the stable states. while both of theseapplications demonstrate the ability to design a simple behavior into a cell, they also demonstrate thedifficulty in implementing these circuits experimentally and meeting design specifications.in a step toward clinical application of this type of work,126 benenson et al. developed a molecularcomputer that could sense its immediate environment for the presence of several mrna species ofdiseaserelated genes associated with models of lung and prostate cancer and, upon detecting all ofthese mrna species, release a short dna molecule modeled on an anticancer drug.127 benenson et al.suggest that this approach might be applied in vivo to biochemical sensing, genetic engineering, andmedical diagnosis and treatment.8.4.2.3 broader views of synthetic biologywhile cellular logic emphasizes the biological network as a substrate for digital computing, synthetic biology can also use analog computing. to support analog computing, the biomolecular networksinvolved would be sensitive to small changes in concentrations of substances of interest. for example, amicrobe altered by synthetic biology research might fluoresce with an intensity proportional to theconcentration of a pollutant. such analog computing is in one sense closer to the actual functionality ofexisting biomolecular networks (although of course there are many digital elements in such networks aswell), but is more alien to the existing engineering approaches borrowed from electronic systems.for purposes of understanding existing biology, one approach inspired by synthetic biology is tostrip down and clean up genomes for maximal clarity and comprehensibility. for example, drewendyõs group at mit is cleaning the genome of the t7 bacteriophage, removing all unnecessary sequences, editing it so that genes are contiguous, and so on.128 such an organism would be easier tounderstand than the wild genotype, although such editing would obscure the evolutionary history ofthe genome.while synthetic biology stresses the power of handdesigning biological functions, evolution andselection may have their place. ron weissõs group at princeton university has experimented with usingartificial selection as a way to achieve desired behavior.129 this approach can be combined with engineering approaches, using evolution as a final stage to eliminate unstable or faulty designs.the most extreme goal of synthetic biology is to generate entirely synthetic living cells. in principle,these cells need have no chemical or structural similarity to natural cells. indeed, achieving an understanding of the range of potential structures that can be considered living cells will represent a profoundstep forward in biology. this goal is discussed further in section 9.3.124m.b. elowitz and s. leibler, òa synthetic oscillatory network of transcriptional regulators,ó nature 403(6767):335338,2000.125t.s. gardner, c.r. cantor, and j.j. collins, òconstruction of a genetic toggle switch in escherichia coli,ó nature 403(6767):339342, 2000.126y. benenson, b. gil, u. bendor, r. adar, and e. shapiro, òan autonomous molecular computer for logical control ofgene expression,ó nature 429(6990):423429, 2004.127in fact, the molecular computerñanalogous to a process control computerñis designed to release a suppressor moleculethat inhibits action of the druglike molecule.128w.w. gibbs, òsynthetic life,ó scientific american 290(5):7481, 2004.129y. yokobayashi, c.h. collins, j.r. leadbetter, r. weiss, and f.h. arnold, òevolutionary design of genetic circuits and cellcell communications,ó advances in complex systems, world scientific, 2003.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.biological inspiration for computing2918.4.2.4 applicationswhile significant from a research view, synthetic biology also has practical applications. a strongdriver of this is the rapidly falling cost of custom dna synthesis. for a few dollars per base pair in 2004,laboratories can synthesize an arbitrary sequence of dna;130 these prices are expected to fall by ordersof magnitude over the next decade. this not only has enabled research into constructing new genes, butalso offers the promise of costeffective use of synthetic biology for commercial or industrial applications. once a new lineage is created, of course, organisms can selfreplicate in the appropriate environment, implying extremely low marginal cost.cells can be abstracted as chemical factories controlled by a host of process control computers. if theprogramming of these process control computers can be manipulated, or new processes introduced, itisñin principleñpossible to coopt the functional behavior of cells to perform tasks of engineering orindustrial interest. natural biology creates cells that are capable of sensing and actuating functions: cellscan generate motion and light, for example, and respond to light or to the presence of chemicals in theenvironment. natural cells also produce a variety of enzymes and proteins with a variety of catalyticand structural functions. if logic functions can be realized through cellular engineering, cellular computing offers the promise of a seamlessly integrated approach to process control computing.synthetic or modified cells could lead to more rational biosynthesis of a variety of useful organiccompounds, including proteins, small molecules, or any substance that is too costly or difficult tosynthesize by ordinary bench chemistry. some of this is already being done by cloning and genetransfection (e.g., in yeast, plants, and many organisms), but synthetic biology would allow finer control, increased accuracy, and the ability to customize such processes in terms of quantity, precise molecular characteristics, and chemical pathways, even when the desired characteristics are not availablein nature.8.4.2.5 challengessynthetic biology brings the techniques and metaphor of electronic design to modify biomolecularnetworks. however, in many ways, these networks do not behave like electronic networks, and thenature of biological systems provides a number of challenges for synthetic biology researchers in attempting to build reliable and predictable systems.a key challenge is the stochastic and noisy nature of biological systems, especially at the molecularscale. this noise can lead to random variation in the concentration of molecular species; systems thatrequire a precise concentration will likely work only intermittently. additionally, as the mechanisms ofsynthetic biology are embedded in the genome of living creatures, mutation or imperfect replication canalter the inserted gene sequences, possibly disabling them or causing them to operate in unforeseenways.unlike actual electronic systems, the components of biomolecular networks are not connected byphysical wires that direct a signal to a precise location; the many molecules that are the inputs andoutputs of these processes share a physical space and can commingle throughout the cell. it is thereforedifficult to isolate signals and prevent crosstalk, in which signals intended for one recipient are received by another. this physical location sharing also means that it is more difficult to control the timingof the propagation of signals; again, unlike electronics, which typically rely on a clock to preciselysynchronize signals, these biomolecular signals are asynchronous and may arrive at varying speeds.finally, the signals may not arrive, or may arrive in an attenuated fashion.131130one firm claims to be able to provide dna sequences as long as 40,000 base pairs. see http://www.blueheronbio.com/genemaker/synthesis.html. others suggest that sequences in the 100 base pair range are the longest that can be synthesizedtoday without significant error in most of the resulting strands.131r. weiss, s. basu, s. hooshangi, a. kalmbach, d. karig, r. mehreja, and i. netravali, ògenetic circuit building blocks forcellular computation, communications, and signal processing,ó natural computing 2:4784, 2003.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.292catalyzing inquiryaside from the technical challenges of achieving the desired results of synthetic biology projects,there are significant concerns about the misuse or unintended consequences of even successful work. ofmajor concern is the potential negative effect on the environment or the human population if modifiedor created organisms became unmanaged, through escape from a laboratory, mutation, or any othervector. this is especially a concern for organisms, such as those intended to detect or treat pollutants,that are designed to work in the open environment. such a release could occur as a result of an accident,in which case the organism would have been intended to be safe but may enter an environment inwhich it could pose a threat. more worrisome, an organism could be engineered using the techniques ofsynthetic biology, but with malicious intent, and then released into the environment. the answer tosuch concerns must include elements of government regulation, public health policy, public safety, andsecurity. some researchers have suggested that synthetic biology needs an òasilomaró conference, byanalogy to the conference in 1975 that established the ground rules for genetic engineering.132some technical approaches to answer these concerns are possible, however. these include òbarcodingó engineered organisms, that is, including a defined marker sequence of dna in their genome(or in every inserted sequence) that uniquely identifies the modification or organism. more ambitiously,modified organisms could be designed to use molecules incompatible with natural metabolic pathways,such as righthanded amino acids or lefthanded sugars.1338.4.3 nanofabrication and dna selfassembly134nanofabrication draws from many fields, including computer science, biology, materials science,mathematics, chemistry, bioengineering, biochemistry, and biophysics. nanofabrication seeks to applymodern biotechnological methodologies to produce new materials, analytic devices, selfassemblingstructures, and computational components from both naturally occurring and artificially synthesizedbiological molecules such as dna, rna, peptide nucleic acids (pnas), proteins, and enzymes. examples include the creation of sensors from dnabinding proteins for the detection of trace amounts ofarsenic and lead in ground waters, the development of nonsocial dna cascade switches that can beused to identify single molecular events, and the fabrication of novel materials with unique optical,electronic, rheological, and selective transport properties.8.4.3.1 rationalescientists and engineers wish to be able to controllably generate complex two and threedimensional structures at scales from 10ð6 to 10ð9 meters; the resulting structures could have applications inextremely highdensity electronic circuit components, information storage, biomedical devices, ornanoscale machines. although some techniques exist today for constructing structures at such tinyscales, such as optical lithography or individual atomic placement, in general they have drawbacks ofcost, time, or limited feature size.biotechnology offers many advantages over such techniques; in particular, the molecular precisionand specificity of the enzymatic biochemical pathways employed in biotechnology can often surpasswhat can be accomplished by other chemical or physical methods. this is especially true in the area ofnanoscale selfassembly. consider the following quote from m.j. frechet, a chemistry professor at the132d. ferber, òsynthetic biology: microbes made to order,ó science 303(5655):158161, 2004.133o. morton, òlife, reinvented,ó wired 13.01, 2005.134section 8.4.3 draws heavily from t.h. labean, òintroduction to selfassembling dna nanostructures for computation andnanofabrication,ó world scientific, cbgi, 2001; e. winfree, òalgorithmic selfassembly of dna: theoretical motivations and 2dassembly experiments,ó journal of biomolecular structure and dynamics 11(2):263270, 2000; j.h. reif, t.h. labean, and n.c.seeman, òchallenges and applications for selfassembled dna nanostructures,ó pp. 173198 in proceedings of the sixth international workshop on dnabased computers, a. condon and g. rozenberg, eds., dimacs series in discrete mathematics andtheoretical computer science, springerverlag, berlin, 2001.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.biological inspiration for computing293university of california, berkeley, who is a leader in the area of the synthesis and control of moleculararchitectures on the nanometer scale:135while most common organic moleculesñòsmall moleculesóñhave sizes well below one nanometer, macromolecules such as proteins or synthetic polymers have sizes in the nanometer range. within this sizerange, it is generally very difficult to control the 3d structure of the molecules. nature has learned howto achieve this with proteins and dna, but most other large synthetic macromolecules have little shapepersistence and precise functional group placement is difficult.it is this fine control of nanoscale architecture exhibited in proteins, membranes, and nucleic acidsthat researchers hope to harness with these applied biotechnologies, and the goal of research into òselfassemblyó is to develop techniques that can create structures at a molecular scale with a minimum ofmanual intervention.selfassembly, also known as bottomup construction, is a method of fabrication that relies onchemicals forming larger structures without centralized or external control.136 because of its ability torun in parallel and at molecular scales, selfassembly is considered to be a potentially important technique for constructing submicron devices such as future electronic circuit components.since the role of dna and related molecules in biology is to generate complicated threedimensional macromolecules such as proteins, dna is a natural candidate for a system of selfassembly.researchers have investigated the potential of using dna as a medium for selfassembling structures atthe nanometer scale. dna has many characteristics that make it an excellent candidate for creatingarbitrary components: its threedimensional shape is well understood (in contrast to most proteins,which have poorly understood folding behavior); it is a digital, informationencoding molecule, allowing for arbitrary customization of sequence; and it, with a set of easily accessible enzymes, is designedfor selfreplication. box 8.4 describes some key enabling technologies for dna selfassembly.one important focus of dna selfassembly research draws on the theory of wang tiles, a mathematical theory of tiling first laid out in 1961.137 wang tiles are polygons with colored edges, and theymust be laid out in a pattern such that the edges of any two neighbors are the same color. later, bergerestablished three important properties of tiling: the question of whether a given set of tiles could coveran area was undecidable; aperiodic sets of tiles could cover an area; and tiling could simulate a universal turing machine,138 and thus was a full computational system.139the core of dna selfassembly is based on constructing special forms of dna in which strandscross over between multiple double helices, creating strong twodimensional structures known as dnatiles. these tiles can be composed of a variety of combinations of spacing and interconnecting patterns;the most common, called dx and tx tiles, contain two or three double helices (i.e., four or six strands),although other structures are being investigated as well. ends of the single strands, sequences ofunhybridized bases, stick out from the edges of the tile, and are known as òsticky endsó (or òpadsó)because of their ability to hybridizeñstick toñother pads. pads can be designed to attach to the stickyends of other tiles. by careful design of the base sequence of these pads, tiles can be designed to connectonly with specific other tiles that complement their base sequence.the congruence between wang tiles and dna tiles with sticky ends is straightforward: the stickyends are designed so that they will bond only to complementary sticky ends on other tiles, just as wangtiles must be aligned by color of edge. the exciting result of combining wang tiles with dna tiles is thatdna tiles have also been shown to be turingcomplete and thus a potential mechanism for computing.135see http://www.cchem.berkeley.edu.136see, for example, g.m. whitesides et al., òmolecular selfassembly and nanochemistryña chemical strategy for thesynthesis of nanostructures,ó science 254(5036):13121319, 1991.137h. wang, òproving theorems by pattern recognition,ó bell system technical journal 40:141, 1961.138a universal turing machine is an abstract model of computer execution and storage with the ability to perform anycomputation that any computer can perform.139r. berger, òthe undecidability of the domino problem,ó memoirs of the american mathematical society 66:172, 1966.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.294catalyzing inquirybox 8.4enabling technologies for dna selfreplicationdna surface arrayscurrent dna array technologies based on spotting techniques or photolithography extend down to pixel sizeson the order of 1 micron.1 examples of these arrays are those produced by affymetrix and nanogen.2 thecreation of dna arrays on the nanometer scale require new types of nonphotolithographic fabrication technologies, and a number of methods utilizing scanning probe microscopic techniques and selfassembledsystems have been reported.dna microchannelsthe separation and analysis of dna by electrophoresis is one of the driving technologies of the entire genomics area. the miniaturization of these analysis technologies with micronsized fluidic channels has beenvigorously pursued with the end goal of creating òlab on a chipó devices. examples are the products of calipertechnologies and aclara biosciences.3 the next generation of these devices will target the manipulation ofsingle dna molecules through nanometersized channels. attempts to make such channels both lithographically and with carbon nanotubes have been reported.dna attachment and enzyme chemistryrobust attachment of dna, rna, and pna onto surfaces and nanostructures is an absolute necessity for theconstruction of nanoscale objectsñboth to planar surfaces and to nanoparticles. the primary strategy is to usemodified oligonucleotides (e.g., thiol, aminecontaining derivatives) that can be reacted either chemically orenzymatically. the manipulation of dna sequences by enzymatic activity has the potential to be a verysequencespecific methodology for the fabrication of dna nanostructures.4dnamodified nanoparticlesnanoscale objects that incorporate dna molecules have been used successfully to create biosensor materials. in one example, the dna is attached to a nanometersized gold particle, and then the nucleic acid is usedto provide biological functionality,while the optical properties of the gold nanoparticles are used to reportparticleparticle interactions.5 semiconductor particles can also be used, and recently the attachment of dnato dendrimers or polypeptide nanoscale particles has been exploited for both sensing and drug delivery.6dna code designto successfully selfassemble nucleic acid nanostructures by hybridization, the dna sequences (often referred to as dna words) must be òwell behavedó (i.e., they must not interact with incorrect sequences). thecreation of large sets of well behaved dna molecules is important not only for dna materials research byalso for largescale dna array analysis. an example of the work in this area is the dna word design byprofessor anne condon at the university of british columbia.7dna and rna secondary structurethe secondary structure of nucleic acid objects beyond simple dna watsoncrick duplex formation, whetherthey are simple single strands of rna or the complex multiple junctions of ned seeman, have to be understood by a combination of experimental methods and computer modeling. the incorporation of nucleic acidstructures that include mismatches (e.g., bulges, hairpins) will most likely be an important piece of the selfassembly process of dna nanoscale objects.8catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.biological inspiration for computing295given a set of tiles with the appropriate pads, any arbitrary pattern of tiles can be created. simple,periodic patterns have been successfully fabricated and formed from a variety of different dna tiles,140and large superstructures involving these systems and containing tens of thousands of tiles have beenobserved. however, nonperiodic structures are more generally useful (e.g., for circuit layouts), andlarger tile sets with more complicated association rules are currently being developed for the assemblyof such patterns.the design of the pads is a critical element of dna selfassembly. since the sticky ends are composed of a sequence of bases, the set of different possible sticky ends is very large. however, there arephysical constraints that restrict the sequences chosen; pads and their complements should be sufficiently different from other matched pairs, as to avoid unintended hybridization; they should avoidpalindromes, and so on.141 most importantly, the entire set of pads must be designed so as to producethe desired overall assembly.the process of dna selfassembly requires two steps: the first is the creation of the tiles, by mixinginput strands of dna together; then, the tiles are placed in solution and the temperature is loweredslowly until the tilesõ pads connect and the overall structure takes form. this process of annealing cantake from several seconds to hours.multistrand dna nanostructures and arraysthe creation of threedimensional objects with multistrand dna structures has been pursued for many yearsby researchers such as ned seeman at new york university. computer scientists such as erik winfree at thecalifornia institute of technology and john reif at duke university have been using the assembly of thesenanostructures to create mosaics and tile arrays on surfaces. the application of computer science concepts toòprogramó the selfassembly of materials is the eventual goal. since singlestranded rna forms many biologically functional structures, researchers are also pursuing the use of rna as well as dna for these selfassembling systems.91a.c. pease, d. solas, e.j. sullivan, m.t. cronin, c.p. holmes, and s.p.a. fodor, òlightgenerated oligonucleotide arrays for rapid dnasequence analysis,ó proceedings of the national academy of sciences 91(11):50225026, 1994.2see http://www.affymetrix.com and http://www.nanogen.com.3see http://www.caliper.com; and http://www.alcara.com.4a.g. frutos, a.e. condon, l.m. smith, and r.m. corn, òenzymatic ligation reactions of dna ôwordsõ on surfaces for dna computing,ójournal of the american chemical society 120 (40):1027710282, 1998. also, q. liu, l. wang. a.g. frutos, a.e. condon, r.m. corn, andl.m. smith, òdna computing on surfaces,ó nature 403:175179, 2000.5c.a. mirkin, r.l. letsinger, r.c. mucic, and j.j. storhoff, òa dnabased method for rationally assembling nanoparticles into macroscopic materials,ó nature 382(6592):607609, 1996; t.a. taton, c.a. mirkin, and r.l. letsinger, òscanometric dna array detection withnanoparticle probes,ó science 289(5485):17571760, 2000.6f. zeng and s.c. zimmerman, òdendrimers in supramolecular chemistry: from molecular recognition to selfassembly,ó chemicalreview 97(5):16811713, 1997; m.s. shchepinov, k.u. mir, j.k. elder, m.d. frankkamenetskii, and e.m. southern, òoligonucleotidedendrimers: stable nanostructures,ó nucleic acids research 27(15):30353041, 1999.7a. maranthe, a.e. condon, and r.m. corn, òon combinatorial word design,ó dimacs series in discrete mathematics and theoreticalcomputer science 54:7590, 2000.8c. mao, t. labean, j.h. reif, and n.c. seeman, òlogical computation using algorithmic selfassembly of dna triple crossovermolecules,ó nature 407(6803):493496, 2000.9e. winfree, f. liu, l.a. wenzler, and n.c. seeman, òdesign and selfassembly of twodimensional dna crystals,ó nature394(6693):539544, 1998.140c. mao, òthe emergence of complexity: lessons from dna,ó plos biology 2(12):e431, 2004, available at http://www.plosbiology.org/archive/15457885/2/12/pdf/10.1371journal.pbio.0020431s.pdf.141t.h. labean, òintroduction to selfassembling dna nanostructures for computation and nanofabrication,ó computational biology and genome informatics, j.t.l. wang et al., eds., world scientific, singapore, 2003.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.296catalyzing inquiryonce the structure is completed, a number of methods can be used to obtain the output if necessary.the first is to image the resulting structure, for example, with an atomic force microscope or transmission electron microscope. in some cases, the structure by itself is visible; in others, tiles can be madedistinguishable by reflectivity or the presence of extra atoms such as gold or fluorescents possiblyadded to a turn of the strand that extends out of the plane. second, with the use of certain tiles, aòreporteró strand of dna can be included in such a way that when all the tiles are connected, the singlereporter strand winds through all of them. once the tiling structure completes assembly, that strand canthen be isolated and sequenced by pcr or another technique to determine the ordering of the tiles.8.4.3.2 applicationsdna selfassembly has a wide range of potential applications, drawing on its ability to createarbitrary, programmable structures. selfassembled structures can encode data (especially array datasuch as images); act as a layout foundation for nanoscale structures such as circuits; work as part of amolecular machine; and perform computations.since a tiled assembly can be programmed to form in an arbitrary pattern, it is potentially a usefulway to store data or designs. in one dimension, this can be accomplished by synthesizing a sequence ofdna bases that encode the data; then, in the selfassembly step, tiles join to the input strand, extendingthe encoding into the second dimension. this twodimensional striped assembly can be inspectedvisually using microscopy, enabling a useful way to read out data. to store twodimensional data, theinput strand is designed with a number of hairpin turns so that the strand weaves across every otherline of the assembly; the tiles then attach between adjacent turns of the input strand. the resultingassembly can encode any twodimensional pattern, and in principle this approach could be extended tothree dimensions.this approach can also be used to create a foundation for nanometerscale electronic circuits. forthis application, the dna tiles would contain some extra materials, such as tiny gold beads, possibly ina strand fragment that extended above the plain of the tile. after the tiles have formed the desiredconfiguration, chemical deposition would be used to coat the gold beads, increasing their size, untilthey merge and form a wire. box 8.5 describes a fantasy regarding a potential application to circuitfabrication.dna has been used as a scaffold for the fabrication of nanoscale devices.142 in crystalline form,dna has enabled the precise and closely spaced placement of gold nanoparticles (at distances of 1020angstroms). gold nanoparticles might function as a singleelectron storage device for one bit, and othernanoparticles might be able to hold information as well (e.g., in the form of electric charge or spin). atone bit per nanoparticle, the information density would be on the order of 1013 to 1014 bits per squarecentimeter.computation through selfassembly is an attractive alternative to traditional exhaustive search dnacomputation. although traditional dna computation, such as performed by adleman, required a linearnumber of steps with the input size, in algorithmic selfassembly, the computation occurs in a single step.in current experiments with selfassembly, a series of tiles are provided as input, and computation tilesand output tiles form into position around the input. for example, in an experiment that used dna tilesto calculate cumulative xor, input tiles represented the boolean values of four inputs, while output tiles,designed such that a tile representing the value 0 would connect to two identical inputs, and a tilerepresenting the value of 1 would connect to two dissimilar inputs, formed alongside the input tiles. then,the reporter strand is ligated, extracted, and amplified to read out the answer.143142s. xiao, f. liu, a.e. rosen, j.f. hainfeld, n.c. seeman, k. musierforsyth, and r.a. kiehl, òassembly of nanoparticlearrays by dna scaffolding,ó journal of nanoparticle research 4:313317, 2002.143c. mao, t.h. labean, j.h. reif, and n.c. seeman, òlogical computation using algorithmic selfassembly of dna triplecrossover molecules,ó nature 407:493496, 2000.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.biological inspiration for computing297this approach has two main drawbacks: the speed of individual assemblies, and the error rate.first, the dna reactions can take minutes or hours, and so any individual computation by selfassembly will likely be substantially slower than using a traditional computer. the potential for selfassemblyis that, like exhaustive dna computation, it can occur in parallel, with a parallelism factor as high as1018. in the xor experiment, researchers observed an error rate of 2 to 5 percent. certainly, this rate maybe lowered as experience is gained in designing laboratory procedures and assembly methods; however, the error rate is likely to remain higher than that for electronic computers. for certain classes ofproblems, an ultraparallel though unreliable approach may be an effective way to compute a solution.8.4.3.3 prospectsso far, dna selfassembly has been demonstrated successfully in the laboratory, constructing relatively simple patterns (e.g., alternating bands, or the encoding of a binary string) that are visible throughmicroscopy. it has also been used successfully for simple computations such as counting, xor, andaddition.moving forward, laboratory techniques must improve in sophistication to handle the more complexassemblies and reactions that will accompany largescale computations or designs. along with progressin the lab, further theoretical developments are possible in developing algorithms for constructingarbitrary aperiodic patterns.although so far dna selfassembly has used only naturally occurring variants of dna, a possibleimprovement is to employ alternative chemistries, such as peptide nucleic acid, an artificial form ofdna in which the backbone has peptide links in place of the phosphate that occurs in natural dna.box 8.5a fantasy of circuit fabricationconsider:. . . a fantasy of nanoscale circuit fabrication in a future technology. imagine a family of primitive molecularelectronic components, such as conductors, diodes, and switches, is available from generic parts suppliers.perhaps we have bottles of these common components in the freezer. . . . suppose we have a circuit to implement. the first stage of the construction begins with the circuit and builds a layout incorporating the sizes of thecomponents and the ways they might interact. next, the layout is analyzed to determine how to construct ascaffold. each branch is compiled into a collagen strut that links only to its selected targets. the struts are labeledso that they bind only to the appropriate electrical component molecules. for each strut, the dna sequence tomake that kind of strut is assembled, and a protocol is produced to insert the dna into an appropriate cell. thesevarious custom parts are then synthesized by the transformed cells.finally, we create an appropriate mixture of these custom scaffold parts and generic electrical parts. speciallyprogrammed worker cells are added to the mixture to implement the circuit edifice we want. the worker cellshave complex programs, developed through amorphous computing technology. the programs control how theworkers perform their particular task of assembling the appropriate components in the appropriate patterns. witha bit of sugar (to pay for their labor), the workers construct copies of our circuit we then collect, test, and packagefor use.source: h. abelson, r. weiss, d. allen, d. coore, c. hanson, g. homsy, t.f. knight, jr., et al., òamorphous computing,ó communications of the acm 43(5):7482, 2000.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.298catalyzing inquiryalso, a wide variety of potential geometries exists for crossover tiles. there have been experiments witha socalled 4 × 4 tile, where the sticky ends extend at right angles.dna also has the property that its length scale can bridge the gap between molecular systems andmicroelectronics components. if the issues of surface attachment chemistry, secondary structure, andselfassembly can be worked out, hybrid dnasilicon nanostructures may be feasible, and a dnacontrolled field effect transistor is one possible choice for a first structure to fabricate. some otherspecific nearterm objectives for research in dna selfassembly include the creation of highly regulardna nanoparticles and the creation of programmable dna selfassembling systems. for the cell regulatory systems and enzymatic pathways, some specific nearterm objectives include the creation of setsof coupled proteindna interactions or genes, the simulation and emulation of kinase phosphorrelaysystems, and the creation of networks of interconnecting nanostructures with unique enzyme communication paths.to be adopted successfully as an industrial technology, however, dna selfassembly faces challenges similar to solutionbased exhaustive search dna computing: a high error rate, the need to runnew laboratory procedures for each computation, and the increasing capability of nondna technologies to operate at nanoscales. for example, while it is likely true that current lithography technology haslimits, various improvements already demonstrated in laboratories such as extreme ultraviolet lithography, halo implants, and laserassisted direct imprint techniques can achieve feature sizes of 10 nm,comparable to a single dna tile. some other targets might be the ability to fabricate biopolymers suchas oligonucleotides and polypeptides as long as 10,000 bases for the creation of molecular controlsystems and the creation of biochemical and hybrid biomolecularinorganic systems that can be selfassembled into larger nanoscale objects in a programmable fashion.8.4.3.4 hybrid systemsa hybrid system is one that is assembled from both biological and nonbiological parts. hybridsystems have many applications, including biosensors, measurement devices, mechanisms, and prosthetic devices.biological sensors, or biosensors, probe the environment for specific molecules or targets throughchemical, biochemical, or biological assays. such devices consist of a biological detection element attuned to the target and a transduction mechanism to translate a detection event into a quantifiableelectronic or optical signal for analysis. for example, antennae from a living silkworm moth have beenused as an olfactory sensor connected to a robot.144 such antennae are much more sensitive thanartificial gas sensors, in this case to moth pheromones. a mobile robot, so equipped, has been shown tobe able to follow a pheromone plume much as a male silkworm moth does. when a silkworm mothõsantennae are stimulated by the presence of pheromones, the mothõs nervous system activities alternatebetween active and inactive states in a pattern consistent with the activity pattern of neck motor neurons that guide the mothõs direction of motion. in the robot, the silkworm mothõs antennae are connected to an electrical interface, and a signal generated by the right (left) antenna results in a òturnrightó (òturn leftó) command. this suggests that such signals may play an important role in controllingthe pheromoneoriented zigzag walking of a silkworm moth.144y. kuwana et al., òsynthesis of the pheromoneoriented behaviour of silkworm moths by a mobile robot with mothantennae as pheromone sensors,ó biosensors and bioelectronics 14:195202, 1999.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.illustrative problem domains at the interface of computing and biology2992999illustrative problem domains at theinterface of computing and biology9.1 why problemfocused research?problems offered by nature do not respect disciplinary boundaries. that is, nature does not packagea problem as a òbiologyó problem, a òcomputingó problem, or a òphysicsó problem. many disciplinesmay have helpful insights to offer or useful techniques to apply to a given problem, and to the extentthat problemfocused research can bring together practitioners of different disciplines to work on sharedproblems, this can only be a good thing.this chapter describes problem domains in which the expenditure of serious intellectual effort canreasonably be expected to generate (or to require!) significant new knowledge in biology and/or computing. biological insight could take different formsñthe ability to make new predictions, the understanding of some biological mechanism, the construction of a new biological organism. the same is truefor computingñinsight might take the form of a new biologically inspired approach to some computingproblem, different hardware, or novel architecture. it is important to note that these domains containvery difficult problemsñand it is unrealistic to expect major progress in a short time.challenge problems can often be found in interesting problem domains. a òchallenge problemó isa scientific challenge focused on a particular intellectual goal or application (box 9.1). such problemshave a long history of stimulating important research efforts, and a list of ògrand challengesó in computational biology originating with david searls, senior vice president of worldwide bioinformatics forglaxosmithkline, includes protein structure prediction, homology search, multiple alignment and phylogeny construction, genomic sequence analysis, and gene finding.1 appendix b provides a sampling ofgrand challenge problems found in other reports and from other life scientists.the remainder of this chapter illustrates problem domains that display the intertwined themes ofunderstanding biological complexity and enabling a novel generation of computing and informationscience. it incorporates many of the dimensions of the basic knowledge sought by each field anddiscusses some of the technical and biological hurdles that must be overcome to make progress. however, no claim whatsoever is made that these problems exhaust the possible interesting or legitimatedomains at the biocomp interface.1d.b. searls, ògrand challenges in computational biology,ó computational methods in molecular biology, s.l. salzberg, d.searls, and s. kasif, eds., elsevier science, 1999.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.300catalyzing inquiry9.2 cellular and organismal modeling2a living cell is a remarkable package of biological molecules engaged in an elaborate and robustchoreography of biological functions. currently, however, we have very incomplete knowledge aboutall of the components that make up cells and how these components interact to perform those functions.understanding how cells work is one of biologyõs grand challenges. if it were possible to understandmore completely how at least some of the machinery of cells works, it might be possible to anticipate theonset and effects of disease and create therapies to ameliorate those effects. if it were possible toinfluence precisely the metabolic operations of cells, they might be usable as highly controllable factories for the production of a variety of useful organic compounds.however, cell biology is awash in data on cellular components and their interactions. althoughsuch data are necessary starting points for an understanding of cellular behavior that is sufficient forprediction, control, and redesign, making sense out of the data is difficult. for example, diagramstracing all of the interactions, activities, locations, and expression times of the proteins, metabolites, andnucleic acids involved have become so dense with lines and annotations that reasoning about theirfunctions has become almost impossible.box 9.1on challenge problemschallenge problems have a history of stimulating scientific progress. for example:¥the u.s. high performance computing and communications program focused on problems in appliedfluid dynamics, meso to macroscale environmental modeling, ecosystem simulations, biomedical imagingand biomechanics, molecular biology, molecular design and process optimization, and cognition.1 theseproblem domains were selected because they drove applications needs for very highperformance computing.¥a second example is the text retrieval conference (trec), sponsored by the national institute of standardsand technology, in cooperation with the national security agency and the defense advanced research projectsagency. the purpose of this conference is to òsupport research within the information retrieval community byproviding the infrastructure necessary for largescale evaluation of text retrieval methodologies. . . . the trecworkshop series has the following goals: to encourage research in information retrieval based on large testcollections; to increase communication among industry, academia, and government by creating an open forumfor the exchange of research ideas; to speed the transfer of technology from research labs into commercialproducts by demonstrating substantial improvements in retrieval methodologies on realworld problems; and toincrease the availability of appropriate evaluation techniques for use by industry and academia, including development of new evaluation techniques more applicable to current systems.ó2 trec operates by presenting aproblem in text retrieval clearly and opening it up to all takers. it makes available to the community at large allbasic tools, and its structure and organization have attracted a large number of research sites.¥still another approach to challenge problems is to offer prizes for the accomplishment of certain wellspecified tasks. for example, in aeronautics, the kremer prize was established in 1959 for the first humanpowered flight over a specific course; this prize was awarded to paul macready for the flight of the gossamercondor in 1977. the kremer prize is widely regarded as having stimulated a good deal of innovative researchin humanpowered flight. a similar approach was taken in cryptanalysis, in which nominal prizes were offered for the first parties to successfully decrypt certain coded messages. these prizes served to motivate thecryptanalytic community by providing considerable notoriety for the winners. on the other hand, pressures tobe the first to achieve a certain result often strongly inhibit cooperation, because sharing oneõs own work mayeliminate the competitive advantage that one has over others.1see http://www.ccic.gov/pubs/blue96/index.html.2see http://trec.nist.gov/overview.html.2section 9.2 is based largely on a.p. arkin, òsynthetic cell biology,ó current opinion in biotechnology 12(6):638644, 2001.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.illustrative problem domains at the interface of computing and biology301as noted in section 5.4.2, cellular simulation efforts have for the most part addressed selectedaspects of cellular functionality. the grand challenge of cellular modeling and simulation is a highfidelity model of a cell that captures the interactions between the many different aspects of functionality, where òhigh fidelityó means the ability to make reasonably accurate and detailed predictions aboutall interesting cellular behavior under the various environmental circumstances encountered in its lifecycle. of course, a model perforce is an abstraction that omits certain aspects of the phenomenon it isrepresenting. but the key term in this description is òinterestingó behaviorñbehavior that is interestingto researchers. in this context, the model is intended to integrateñas a real cell wouldñdifferent aspectsof its functionality. although the grand challenge may well be unachievable, almost by definition, thegoal of increasing degrees of integration of what is known and understood about various aspects ofcellular function remains something for which researchers strive.the development of a highfidelity simulation of a cellñeven the simplest cellñis an enormousintellectual challenge. indeed, even computational models that are very well developed, such as modelsof neural and cardiac electrophysiology, often fail miserably when they are exercised beyond the datathat have been used to construct them. yet if a truly highfidelity simulation could be developed, theability to predict cellular response across a wide range of environmental conditions using a single modelwould imply an understanding of cellular function far beyond what is available today, or even in theimmediate future, and would be a tangible and crowning achievement in science. and, of course, thescientific journey to such an achievement would have many intermediate payoffs, in terms of tools andinsights relevant to various aspects of cellular function. from a practical standpoint, such a simulationwould be an invaluable aid to medicine and would provide a testbed for biological scientists andengineers to explore techniques of cellular control that might be exploited for human purposes.an intermediate step toward the highfidelity simulation of a real cell would be a model of a simplehypothetical cell endowed with specific properties of real cells. this model would necessarily includerepresentations of several key elements (box 9.2). the hundreds of molecules and hundreds of thousands of interactions required do not appear computationally daunting, until it is realized that the timescale of molecular interaction is on the order of femtoseconds, and interesting time scales of cellularresponse may well be hours or days.the challenges fall into three general categories:¥mechanistic understanding. highfidelity simulations will require a much more profound physical understanding of basic biological entities at multiple levels of detail than is available today. (forexample, it is not known how rna polymerase actually moves along a dna strand or what rates ofbinding or unbinding occur in vivo.) an understanding of how these entities interact inside the cell isequally critical. mechanistic understanding would be greatly facilitated by the development of newmathematical formalisms that would enable the logical parsing of large networks into small modulesbox 9.2elements of a hypothetical cell¥an outside and inside separated by some coat or membrane (e.g., lipid)¥one or more internal compartments inside the cell¥genes and an internal code for regulation of function¥an energy supply to keep the cell òaliveó or òworkingó¥reproductive capability¥at least hundreds of biologically significant molecules, with potentially hundreds of thousands of interactions between them¥responsiveness to environmental conditions that affect the internal operation and behavior of the cell (e.g.,changes in temperature, acidity, salinity)catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.302catalyzing inquirywhose behavior can be analyzed. such modules would be building blocks that researchers could use tobuild functionality, understand controllable aspects, and identify points of failure.¥data acquisition. simulation models are dataintensive, and today there are relatively few systems with enough quality data to create highly detailed models of cellular function. it will be importantto develop ways of measuring many more aspects of internal cellular state, and in particular, newtechniques for measuring rates of processes and biochemical reactions in situ in living cells will benecessary. besides additional reporter molecules, selective fluorescent dyes, and so on, a particular needis to develop good ways of tracking cellular state at different points in time, so that cellular dynamicscan be better understood. large volumes of data on reaction rates will also be necessary to model manycellular processes.¥integrative tools. because cellular function is so complex, researchers have used a variety of datacollection techniques. data from multiple sourcesñmicroarrays, protein mass spectroscopy, capillaryand highpressure chromatographies, highend fluorescence microscopy, and so onñwill have to beintegratedñand are indeed requiredñif validated, highfidelity cellular models are to be built. moreover, because existing models and simulations relevant to a given cell span multiple levels of organizational hierarchy (temporal, spatial, etc.), tools are necessary to facilitate their integration. with suchtools at the researcherõs disposal, it will be possible to develop complex models rapidly, assemblingmolecular components into modules, linking modules, computing dynamic interactions, and comparing predictions to data.finally, despite the power of cellular modeling and simulation to advance understanding, modelsshould not be regarded as an end product in and of themselves. because all models are unfaithful to thephenomena they represent in some way, models should be regarded as tools to gain insight and to beused in continual refinement of our understanding, rather than as accurate representations of realsystems, and model predictions should be taken as promising hypotheses that will require experimentalvalidation if they are to be accepted as reliable.the discussion above suggests that many researchers will have to collaborate in the search for anintegrated understanding. such coordinated marshaling of researchers and resources toward a sharedgoal is a common model for industry, but this multiinvestigator approach is new for the academicenvironment. large governmentfunded projects such as the alliance for cellular signaling (discussedin chapter 4) or private organizations like the institute for systems biology3 are the new great experiments in bringing a cooperative approach to academic biology.still more ambitiousñprobably by an order of magnitude or moreñis the notion of simulating thebehavior of a multicelled organism. for example, harel proposes to develop a model of the caenorhabditiselegans nematode, an organism that is well characterized with respect to its anatomy and genetics.4harel describes the challenge as one of constructing òa full, truetoallknownfacts, 4dimensional,fully animated model of the development and behavior of this worm. . . , which is easily extendable asnew biological facts are discovered.óin harelõs view, the feasibility of such a model is based on the notion that the complexity ofbiological systems stems from their high reactivity (i.e., they are highly concurrent and timeintensive,exhibit hybrid behavior that is predominantly discrete in nature but with continuous aspects as well,and consist of many interacting, often distributed, components). the structure of a reactive system mayitself be dynamic, with its components being repeatedly created and destroyed during the systemõs lifespan. harel notes:3see http://www.systemsbiology.org/home.html.4d. harel, òa grand challenge for computing: towards full reactive modeling of a multicellular animal,ó european association for theoretical computer science (eatcs) bulletin, 2003, available at http://www.wisdom.weizmann.ac.il/~dharel/papers/grandchallenge.doc.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.illustrative problem domains at the interface of computing and biology303[b]iological systems exhibit the characteristics of reactive systems remarkably, and on many levels; fromthe molecular, via the cellular, and all the way up to organs, full organisms, and even entire populations.it doesnõt take much to observe within such systems the heavy concurrency, the eventdriven discretenature of the behavior, the chainreactions and causeeffect phenomena, the timedependent patterns, etc.harel concludes that biological systems can be modeled as reactive systems, using languages andtools developed by computer science for the construction of manmade reactive systems (briefly discussed in section 5.3.4 and at greater length in the reference in footnote 4 of this chapter).if the harel effort is successful, a model of c. elegans would result that is fully executable, flexible,interactive, comprehensive, and comprehensible. by realistically simulating the wormõs developmentand behavior, it would help researchers to uncover gaps, correct errors, suggest new experiments,predict unobserved phenomena, and answer questions that cannot be addressed by standard laboratorytechniques alone. in addition, it would enable users to switch rapidly between levels of detail (from theentire macroscopic behavior of the worm to the cellular and perhaps molecular levels). most importantly, the model would be extensible, allowing biologists to enter new data themselves as they arediscovered and to test various hypotheses about aspects of behavior that are not yet known.9.3 a synthetic cell with physical formthe most ambitious goal of synthetic biology (section 8.4.2) is the biochemical instantiation of arealñif syntheticñcell with the capability to grow and reproduce. such an achievement wouldnecessarily be accompanied by new insights into the molecular dynamics of cells, the origins of lifeon earth, and the limits of biological life. in practical terms, such cells could be engineered toperform specific functions, and thus could serve as a platform for innovative industrial and biomedical applications.cellular modification has a long history ranging from the development of plasmids carrying biosynthetic genes, or serving as òengineering blanksó for production of new materials, to the creation ofsmall genetic circuits for the control of gene expression. however, the synthetic cells being imaginedtoday would differ from the original cell much more substantially than those that have resulted frommodifications to date. in principle, these cells need have no chemical or structural similarity to naturalcells. since they will be designed, not evolved, they may contain functions or structures unachievablethrough natural selection.synthetic cells are a potentially powerful therapeutic tool that may be able to deliver drugs todamaged tissue to seek and destroy foreign cells (in infections), destroy malignant cells (in cancer),remove obstructions (in cardiovascular disease), rebuild or correct defects (e.g., reattach severed nerves),or replace parts of tissue that was injuredñand doing so without affecting nonproblematic tissues,while reducing the side effects of current conventional treatments.the applications of synthetic cells undertaking celllevel process control computing are not limitedto those of medicine and chemical sensing. there are also potential applications to the nanofabricationof new and useful materials and structures. indeed, natural biology exhibits propulsive rotors andlimbs at the microscale, and synthetic cells may be an enabling technology for nanofabricationñthebuilding of structures at the microscopic level. there may be other techniques to accomplish this, butsynthetic cells offer a promise of high efficiency through massively parallel reproduction. the generegulatory networks incorporated into synthetic cells allow for the simultaneous creation of multipleoligonucleotide sequences in a programmable fashion. conversely, selfassembled dna nanostructurescan potentially be used as control structures that interact with intracellular components and molecules.such control could enable the engineering construction of complex extracellular structures and precisecontrol of fabrication at the subnanometer level, which might in turn lead to the construction of complexmolecularscale electronic structures (section 8.4.3.2) and the creation of new biological materials, muchas natural biological materials result from natural biological processes.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.304catalyzing inquiryconstructing these structures will require the ability to fabricate individual devices and theability to assemble these devices into a working system, since it is likely to be very difficult toassemble a system directly from scratch. one approach to an assembly facility is to use a mostlypassive scaffold, consisting of selectively selfassembling molecules that can be used to support thefabrication of molecular devices that are appropriately interconnected. indeed, dna molecules andtheir attendant enzymes are capable of selfassembly. by exploiting that capability, it has been possible to create a number of designed nanostructures, such as tiles and latticed sheets. although thecharacteristics of these biomaterials need further exploration, postulated uses of them include asscaffolds (for example, for the crystallization of macromolecules); as photonic materials with novelproperties; as designable zeolitelike materials for use as catalysts or as molecular sieves; and asplatforms for the assembly of molecular electronic components or biochips.5 uses of dna as amolecular òlegoó kit with which to design nanomachines, such as molecular tweezers and motors onrunways, are also under investigation.the relevance of synthetic cell engineering to nanofabrication is driven by the convergence ofdevelopments in several areas, including the miniaturization of biosensors and biochips into the nanometerscale regime, the fabrication of nanoscale objects that can be placed in intracellular locationsfor monitoring and modifying cell function, the replacement of silicon devices with nanoscale, molecularbased computational systems, and the application of biopolymers in the formation of novelnanostructured materials with unique optical and selective transport properties. the highly predictablehybridization chemistry of dna, the ability to completely control the length and content of oligonucleotides, and the wealth of enzymes available for modification of dna make nucleic acids an attractivecandidate for all of these applications.furthermore, by designing and implementing synthetic cells, a much better understanding will begained of how real cells work, how they are regulated, and what limitations are inherent in theirmachinery. here, the discovery process is iterative, in that our understanding and observations of livingcells serve as òtruthingó mechanisms to inform and validate or refute the experimental constructs ofsynthetic cells. in turn, the mechanisms underlying synthetic cells are likely to be more easily understood than comparable ones in natural cells. using this combined information, the behavior of biological processes in living cells can slowly be unraveled. for such reasons, the process of creating syntheticcells will spin off benefits to biology and science, just as the human genome project led to dramaticimprovements in the technology of molecular biology.to proceed with the creation of synthetic cells, three separate but interrelated problems must beaddressed:¥the theoretical and quantitative problem of formulating, understanding, and perhaps even optimizing the design of a synthetic cell;¥the biological problem of applying lessons learned from real cells to such designs and usingsynthetic cells to inform our understanding of more complicated natural cells; and¥the engineering and chemistry problem of assembling the parts into a physical system (or todesign selfassembling pieces).one approach to building such a cell de novo is to start with a set of parts and assemble them intoa functional biomolecular machine. conceiving a cell de novo means that cellular components and theirassembly are predetermined, and that the cell engineer has a quantifiable understanding of events andoutcomes that can be used to predict the behavior of the components and their interactions at leastprobabilistically. a key aspect of de novo construction is that a de novo cellular design is not constrained by evolutionary history and hence is much more transparent than cells found in nature. be5e. winfree, f. liu, l.a. wenzler, and n.c. seeman, òdesign and selfassembly of twodimensional dna crystals,ó nature394(6693):539544, 1998.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.illustrative problem domains at the interface of computing and biology305cause an engineered cell would be designed by human beings, the functions of its various elementswould be much better known. this fact implies that it would be easier to identify critical control pointsin the system and to understand the rules by which the system operates.a second approach is to modify an existing living cell to give it new behaviors or to removeunwanted behaviors; classical metabolic engineering and natural product synthesis would be relevantto this approach. one starting point would be to use the membrane of an existing cell, but modificationof these lipid bilayers to incorporate chemically inducible channels, integrated inorganic structures forsensing and catalysis, and other biopolymer structures for the identification and modification of biological substrates will provide a greater degree of freedom in the manipulation of the chemical state ofthe synthetic cell.a third approach is to abandon dnabased cells. szostak et al.6 argue that the òstrippingdownó ofa presentday bacterium to its minimum essential components still leaves hundreds of genes andthousands of different proteins and other molecules. they suggest that synthetic cells could use rna asthe repository of ògeneticó information and as enzymes that catalyze metabolism. in their view, themost important requirements of a synthetic cell from a scientific standpoint are that it replicates autonomously and that it is subject to evolutionary forces. in this context, autonomous replication meanscontinued growth and division that depends only on the input of small molecules and energy, not onthe products of preexisting living systems such as protein enzymes. evolution in this context means thatthe structure is capable of producing different phenotypes that are subject to forces of natural selection,although being subject to evolutionary forces has definite disadvantages from an engineering perspective seeking practical application of synthetic cells.the elements of a synthetic cell are likely to mirror those of simulations (see box 9.2), except ofcourse that they will take physical representation. inputs to the synthetic cell would take the form ofenvironmental sensitivities of various kinds that direct cellular function. (another perspective on òartificial cellsó similar to this reportõs notion of synthetic cells is offered by pohorille.7 in general, syntheticcells share much with artificial cells, and the dividing line between them is both blurry and somewhatarbitrary. the modal use of the term òartificial celló appears to refer to an entity with a liposomemembrane, whose physical dimensions are comparable to those of natural cells, that serves a functionsuch as enzyme delivery, drug delivery for cell therapy, and red blood cell substitutes.8) however, ifsynthetic cells are to be useful or controllable, it will be necessary to insert control points that can supplyexternal instructions or òreprogramó the cell for specialized tasks (e.g., a virus that injects dna into thecell to insert new pieces of code or instructions).researchers are interested in expanding the size and complexity of pathways for synthetic cells thatwill do more interesting things. but there is little lowhanging fruit in this area, and todayõs computational and mathematical ability to predict cellular behavior quantitatively is inadequate to do so, letalone to select for the desired behavior. to bring about the development of synthetic cells from conceptto practical reality, numerous difficulties and obstacles must be overcome. following is a list of majorchallenges that have to be addressed:¥a framework for cellular simulation that can specify and model cellular function at different levels ofabstraction (as described in section 9.2). simulations will enable researchers to test their proposed designs, minimizing (though not eliminating) the need for in vivo construction and experimentation. notethat the availability of such a framework implies that the data used to support it are also available toassist in the engineering development of synthetic cells.6j.w. szostak, d.p. bartel, and p.l. luisi, òsynthesizing life,ó nature 409(6818):387390, 2001.7a.pohorille, òartificial cells: prospects for biotechnology,ó trends in biotechnology20(3):123128, 2002.8see, for example, t.m.s. chang, òartificial cell biotechnology for medical applications,ó blood purification 18:9196, 2000,available at http://www.medicine.mcgill.ca/artcell/isbp.pdf.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.306catalyzing inquiry¥stability and robustness in the face of varying environmental conditions and noise. for example, it iswell known that nature provides a variety of redundant pathways for biological function, so that (forexample) the incapacitation of one gene is often not unduly disruptive to the cell.¥improvement in the libraries of dnabinding proteins and their matching repressor patterns. these areat present inadequate, and good data about their kinetic constants are unavailable (hence signal transfercharacteristics cannot be predicted). any specific combination of proteins might well interact outsidethe genetic regulatory mechanisms involved, thus creating potentially undesirable side effects.¥control point design and insertion.¥data measurement and acquisition. to facilitate the monitoring of a synthetic cellõs behavior, it isdesirable to incorporate into the structure of the cell itself methods for measuring internal state parameters. such measurements would be used to parameterize the functionality of cellular elements andcompare performance to specifications.¥deeper understanding of biomolecular design rules. engineering of proteins for the modification ofbiointeractions will be required in all aspects of cell design, because it is relevant to membranebasedreceptors, protein effectors, and transcriptional cofactors. today, metabolic engineers are frequentlyfrustrated in attempts to reengineer metabolic pathways for new functions because, at this point, theòdesign principlesó of natural cells are largely unknown. to design, fabricate, and prototype cellularmodules, it must be possible to engineer proteins that will bind to dna and regulate gene expression.current examples of dna binding proteins are zinc fingers, response regulators, and homeodomains.the goal is to create flexible protein systems that can be modified to vary binding location and strengthand, ultimately, to insert these modules into living cells to change their function.¥a òdevicepackingó design framework that allows the rapid design and synthesis of new networks insidecells. this framework would facilitate designs that allow the reuse of parts and the rapid modificationof said parts for creating various òmodulesó (switches, ramps, filters, oscillators, etc.). the understanding available today regarding how cells reproduce and metabolize is not sufficient to enable the insertion of new mechanisms that interact with these functions in predictable and reliable ways.¥tool suites to support the design, analysis, and construction of biologic circuits. such suites are as yetunavailable (but see box 9.3).9.4 neural information processing and neural prostheticsbrain research is a grand challenge area for the coming decades. in essence, the goal of neuroscienceresearch is to understand how the interplay of structural dynamics, biochemical processes, and electribox 9.3tool suitesone tool suite is a simulator and verifier for genetic digital circuits, called biospice. the input to biospice isthe specification of a network of gene expression systems (including the relevant protein products) and a smalllayout of cells on some medium. the simulator computes the timedomain behavior of concentration ofintracellular proteins and intercellular messagepassing chemicals. (for more information, see http://www.biospice.org.)a second tool would be a òplasmid compileró that takes a logic diagram and constructs plasmids to implementthe required logic in a way compatible with the metabolism of the target organism. both the simulator and thecompiler must incorporate a database of biochemical mechanisms, their reaction kinetics, their diffusionrates, and their interactions with other biological mechanisms.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.illustrative problem domains at the interface of computing and biology307cal signals in nervous tissue gives rise to higherorder functions such as normal or abnormal thoughts,actions, memories, and behaviors. experimental advances of the past decades have given the brainresearcher an increasingly powerful arsenal of tools to obtain datañfrom the level of molecules tonervous systemsñand to compare differences between individuals.today, neuroscientists have begun the arduous process of adapting and assembling neurosciencedata at all scales of resolution and across disciplines into electronically accessible, distributed databases.these information repositories will complement the vast structural and sequence databases created tocatalog, organize, and analyze gene sequences and protein products. such databases have provenenormously useful in bioinformatics research; whether equal rewards will accrue from similar effortsfor tissuelevel data, wholebrain imaging, physiological data, and so forth remains to be seen, butbased on the successes of the molecular informatics activities and the challenge questions of the neuroscientist, big payoffs can be anticipated.at the very least, multiscale informatics efforts for brain research will provide organizing frameworks and computational tools to manage neuroscience data, from the lab notebook to published data.an ideal and expected outcome will be the provisioning for new opportunities to integrate large amountsof biological data into unified theories of function and aid in the discovery process.to provide some perspective on the problem, consider that animal brains are the informationprocessing systems of nature. a honeybeeõs brain contains roughly 100 million synapses; a contemporary computer contains roughly 100 million transistors. given a history of inputs, both systems choosefrom among a set of possible outputs. yet although it is understood how a digital computer adds andsubtracts numbers and stores errorfree data, it is not understood how a honeybee learns to find nectarrich flowers or to communicate with other honeybees.we do not expect a honeybee to perform numerical computations; likewise, we do not expect adigital computer to learn autonomously, at least not today. however, an interesting question is theextent to which the structure of an informationprocessing system and the information representationsthat it uses predispose the system to certain types of computation. put another way, in what ways andunder what circumstances, if any, are neuronal circuits and neural informationprocessing systemsinherently superior to von neumann architectures and shannon information representations for adaptation and learning? given the desirability of computers that can learn and adapt, an ability to answerthis question might provide some guidance in the engineering of such systems.some things are known about neural information processing:¥animal brains find good solutions to realtime problems in image and speech processing, motorcontrol, and learning. to perform these tasks, nervous systems must represent, store, and processinformation. however, it is highly unlikely that neural information is represented in digital form.¥it is likely that neurons are the nervous systemõs primary computing elements. a typical neuronis markedly unlike a typical logic gate; it possesses on average 10,000 synaptic inputs and a similarnumber of outputs.¥the stored memory of a neural informationprocessing system is contained in the pattern andstrength of the analog synapses that connect it to other neurons. nervous systems use vast numbers ofsynapses to effect their computations: in neocortical tissue, the synapse density is roughly 3 × 108synapses per cubic millimeter.9 specific memories are also known not to be localized to particularneurons or sets of neurons in the brain.109r. douglas, òrules of thumb for neuronal circuits in the neocortex,ó notes for the neuromorphic vlsi workshop, telluride,co, 1994.10the essential reason is that specific memories are generally richly and densely connected to other memories, and hence canbe reconstructed through that web of connections.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.308catalyzing inquiry¥the disparity between the information processing that can be done by digital computers and thatdone by nervous systems is likely to be a consequence of the different way in which nerve tissuerepresents and processes information, although this representation is not understood.¥at the device level, nervous tissue operates on physical principles that are similar to those thatunderlie semiconductor electronics.11 thus, differences between neural and silicon computation mustbe the result of differences in computational architecture and representation. it is thus the higherlevelorganization underlying neural computation that is of interest and relevance. note also that for thepurposes of understanding neural signaling or computation, a neuronbyneuron simulation of nervoustissue per se cannot be expected to reveal very much about the principles of organization, though it maybe necessary for the development of useful artifacts (e.g., neural prostheses).some of the principles underlying neural computation are understood. for example, neurobiologyuses continuous adaptation rather than absolute precision in responding to analog inputs. the dynamicrange of the human visual system is roughly 10 decades in input light intensityñabout 32 bits. butbiology doesnõt process visual signals with 32bit precision; rather, it uses a 7 or 8bit instantaneousdynamic range and adapts the visual pathwayõs operating point based on the background light intensity. although this approach is similar to the automatic gain control used in electronic amplifiers,biology takes the paradigm much farther: adaptation pervades every level of the visual system, ratherthan being concentrated just at the front end.12there are essentially two complementary approaches toward gaining a greater understanding ofneural information processing. one approach is to reproduce physiological phenomena to increase ourunderstanding of the nervous system.13 a second approach is based on using a manageable subset ofneural properties to investigate emergent behavior in networks of neuronlike elements.14 those favoring the first approach believe that these details are crucial to understanding the collective behavior ofthe network and are developing probes that are increasingly able to include the relevant physiology.those favoring the second approach make the implicit assumption that reproducing many neurophysiological details is secondary to understanding the collective behavior of nervous tissue, even whileacknowledging that only a detailed physiological investigation can reveal definitively whether thedetails are in fact relevant.what can be accomplished by building silicon circuits modeled after biology? first, once the neuronal primitives are known, it will be possible to map them onto silicon. once it is understood howbiological systems compute with these primitives, biologically based silicon computing will be possible.second, we can investigate how physical and technological limits, such as wire density and signaldelays and noise, constrain neuronal computation. third, we can learn about alternative models ofcomputation. biology demonstrates nondigital computing machines that are incredibly space andenergyefficient and that find adequate solutions to illposed problems naturally.11in both integrated circuits and nervous tissue, information is manipulated principally on the basis of charge conservation. inthe former, electrons are in thermal equilibrium with their surroundings and their energies are boltzmann distributed. in thelatter, ions are in thermal equilibrium with their surroundings and their energies also are boltzmann distributed. in semiconductor electronics, energy barriers are used to contain the electronic charge, by using the work function difference between siliconand silicon dioxide or the energy barrier in a pn junction. in nervous tissue, energy barriers are also erected to contain the ioniccharge, by using lipid membranes in an aqueous solution. in both systems, when the height of the energy barrier is modulated,the resulting current flow is an exponential function of the applied voltage, thus allowing devices that exhibit signal gain.transistors use populations of electrons to change their channel conductance, in much the same way that neurons use populations of ionic channels to change their membrane conductance.12adaptation helps to explain why some biological neural systems never settle downñthey can be built so that when facedwith unchanging inputs, the inputs are adapted away. this phenomenon helps to explain many visual aftereffects. a stabilizedimage on the retina disappears after a minute or so, and the whole visual field appears gray.13m.a. mahowald and r.j. douglas, òa silicon neuron,ó nature 354(6354):515518, 1991.14j. hertz, a. krogh, and r.g. palmer, introduction to the theory of neural computation, addisonwesley, reading, ma, 1991.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.illustrative problem domains at the interface of computing and biology309the challenges of neural information processing fall into two primary categories: the semantics ofneural signaling and the development of neural prostheses. signaling is the first challenge. it is knownthat the spike trains of neurons carry information in some wayñneurons that cannot òfireó are essentially dead.15 also, the physical phenomena that constitute òfiringó are knownñelectrical spikes ofvarying amplitude and timing. however, the connections among these patterns of signaling in multipleneurons to memories of specific events, motor control of muscles, sensory perception, or mental computation are entirely unknown. how do neurons integrate data from large numbers of multimodal sensors? how do they deal with data overload? how do they decide a behavioral response from multiplealternatives under severe and illposed constraints?todayõs neural instrumentation (e.g., positron emission tomography [pet] scans, functional magneticresonance imaging [fmri]) can identify areas of the brain that are active under various circumstances, butsince the spatial resolution of these probes is wholly inadequate to resolve individual neuronal activity,16such instrumentation can provide only the roughest guidance about where researchers need to look formore information about neuronal signaling, rather than anything specific about that information itself.the primary challenge in this domain is the development of a formalism for neuronal signaling (mostlikely a timedependent one that takes kinetics into account), much like the boolean algebra that providesa computational formalism based on binary logic levels in the digital domain.a step toward a complete molecular model of neurotransmission for an entire cell is provided bymcell, briefly mentioned in chapter 5. mcell is a simulation program that can model single synapsesand groups of synapses. to date, it been used to understand one aspect of biological signal transduction, namely the microphysiology of synaptic transmission. mcell simulations provide insights into thebehavior and variability of real systems comprising finite numbers of molecules interacting in spatiallycomplex environments. mcell incorporates highresolution physical structure into models of liganddiffusion and signaling, and thus can take into account the large complexity and diversity of neuraltissue at the subcellular level. it models the diffusion of individual ligand molecules used in neuralsignaling using a brownian dynamics random walk algorithm, and bulk solution rate constants areconverted into monte carlo probabilities so that the diffusing ligands can undergo stochastic chemicalinteractions with individual binding sites, such as receptor proteins, enzymes, and transporters.17the second challenge is that of neural prosthetics. a neural prosthesis is a device that interfacesdirectly with neurons, receiving and transmitting signals that affect the function and activity of thoseneurons, and that behaves in predictable and useful ways. perhaps the òsimplestó neural prosthesis isan artificial implant that can seamlessly replace nonfunctioning nerve tissue.today, some measure of cognitive control of artificial limbs can be achieved through bionic brainmachine or peripheralmachine interfaces. william craelius et al.18 have designed a prosthetic handthat offers amputees control of finger flexion using natural motor pathways, enabling them to undertake slow typing and piano playing. the prosthetic hand is based on the use of natural tendon movements in the forearm to actuate virtual finger movement. a volitional tendon movement within theresidual limb causes a slight displacement of air in foam sensors attached to the skin in that location,and the resulting pressure differential is used to control a multifinger hand.15it is also known that not all neural signaling is carried by spikes. a phenomenon known as graded synaptic transmission alsocarries neural information and is based on a release of neurotransmitter at synaptic junctions whose volume is voltage dependent and continuous. graded synaptic transmission appears to be much more common in invertebrates and sometimes existsalongside spikemediated signaling (as in the case of lobsters). the bandwidth of this analog channel is as much as five times thehighest rates measured in spiking neurons (see, for example, r.r. de ruyter van steveninck and s.b. laughlin, òthe rate ofinformation transfer at gradedpotential synapses,ó nature 379:642645, 1996), but the analog channel is likely to suffer a muchhigher susceptibility to noise than do spikemediated communications.16the spatial resolution of neural instrumentation is on the order of 1 to 10 mm. see d. purves et al., neuroscience, sinauerassociates inc., sunderland, ma, 1997. given about 3 × 108 synapses per cubic millimeter, not much localization is possible.17see http://www.mcell.cnl.salk.edu/.18w. craelius, r.l. abboudi, and n.a. newby, òcontrol of a multifinger prosthetic hand,ó icorr õ99: international conferenceon rehabilitation robotics, stanford, ca, 1999.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.310catalyzing inquirya second example of a neural prosthesis is a retinal prosthesis intended to provide functionality whenthe retina of the eye is nonfunctional. in one variant, a lightsensitive microchip is implanted into the backof the eye. light striking the microchip (which has thousands of individual sensors) generates electricalsignals that travel through the optic nerve to the brain and are interpreted as an image.19 in anothervariant, the retina is bypassed entirely through the use of a camera mounted on a pair of eyeglasses tocapture and transmit a light image via a radio signal to a chip implanted near the ganglion cells, whichsend nerve impulses to the brain.20 in a third variant, an implanted microfluidic chip that controls the flowof neurotransmitters translates digital images into neurochemical signals that provide meaningful visualinformation to the brain. the microfluidic chip has a twodimensional array of small controllable pores,corresponding to pixels in an image. an image is created by the selective drip of neurotransmitters ontospecific bipolar cells, which are the cells that carry retinal information to the brain.21a third example of work in this area is that of musallam et al., who have demonstrated the feasibility of a neural interface that enables a monkey to control the movement of a cursor on a computer screenby thinking about a goal the monkey would like to achieve and assigning a value to that goal.22 theinteresting twist to this work is the reliance of signals from parts of the brain related to higherorder(òcognitiveó) brain functions for movement planning for the control of a prosthetic device. (previousstudies have relied on lowerlevel signals from the motor cortex.23)the advantage of using higherlevel cognitive signals is that they capture information about themonkeyõs goal (moving the cursor) and preferences (the destination on the screen the monkey wants).musallam et al. point out that once the signals associated with the subjectõs goals are decoded, a smartexternal device can perform the lowerlevel computations necessary to achieve the goals. for example,a smart robotic arm would be able to understand what the intended goal of an arm movement is andthen computeñon its ownñthe trajectory needed to move the arm to that position. furthermore, theabstract nature of a cognitive command would allow it to be used for the control and operation of anumber of different devices. if higherlevel signals associated with speech or emotion could be decoded,it would become possible to record thoughts from speech areas (reducing the need for the use ofcumbersome letter boards and timeconsuming spelling programs) or to provide online indications of apatientõs emotional state.a fourth example is provided by theodore berger of the university of southern california, who isattempting to develop an artificial hippocampusña silicon implant that will behave neuronally in amanner identical to the brain tissue that it replaces.24 the hippocampus is the part of the brain responsible for encoding experiences so that they can be stored as longterm memories elsewhere in the brain;without the hippocampus, a person is unable to store new memories but can recall ones stored prior toits loss. because the manner in which the hippocampus stores information is unknown, bergerõs approach is based on designing a chip that can provide the identical inputoutput response. the input19n.s. peachey and a.y. chow, òsubretinal implantation of semiconductorbased photodiodes: progress and challenges,ójournal of rehabilitation research and development 36(4):371376, 1999.20w. liu, e. mcgucken, m. clements, s.c. demarco, k. vichienchom, c. hughes, et al., òmultipleunit artificial retinachipset system to benefit the visually impaired,ó to be published in ieee transactions on rehabilitation engineering. available athttp://www.icat.ncsu.edu/projects/retina/files/marcsystempaper.pdf.21b. vastag, òfuture eye implants focus on neurotransmitters,ó journal of the american medical association 288(15):18331834,2002.22s. musallam, b.d. corneil, b. greger, h. scherberger, and r.a. andersen, òcognitive control signals for neural prosthetics,óscience 305(5681):258262, 2004. a caltech press release of july 8, 2004, available at http://pr.caltech.edu/media/pressreleases/pr12553.html, describes this work in more popular terms.23j. wessberg, c.r. stambaugh, j.d. kralik, p.d. beck, m. laubach, j.k. chapin, j. kim, s.j. biggs, m.a. srinivasan, and m.a.l.nicolelis, òrealtime prediction of hand trajectory by ensembles of cortical neurons in primates,ó nature 408(6810):361365,2000. similar work on rats is described in j.k. chapin, k.a. moxon, r.s. markowitz, and m.a.l. nicolelis, òrealtime control ofa robot arm using simultaneously recorded neurons in the motor cortex,ó nature neuroscience 2(7):664670, 1999.24r. merritt, ònerves of silicon: neural chips eyed for brain repair,ó ee times, march 17, 2003 (10:37 a.m. est), available athttp://www.eetimes.com/story/oeg20030317s0013.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.illustrative problem domains at the interface of computing and biology311output response of a hippocampal slice was determined by stimulating it with a randomsignal generator, and a mathematical model was developed to account for its response to these different stimuli. thismodel is then the basis for the chip circuitry.by december 2003, berger and his colleagues had completed the first test of using a microchipmodel to replace a portion of the hippocampal circuitry contained in a specific hippocampal brain slice.in that slice is the major intrinsic circuitry of the hippocampus that consists of three major cell fields,designated a, b, and c. field a projects to and excites field b, which projects to and excites field c.berger et al. developed a predictive mathematical model of the signal transformations that field bperforms on the input signals that come from field a, and that field b then projects onto field c, andimplemented the model in a fieldprogrammable gate array (fpga) for field b. when field b wassurgically removed and the fpga model of b was substituted, the result was that the output from areac of the hippocampal slice remained unchanged in all meaningful respects. next steps beyond thiswork (e.g., developing circuitry that is less sensitive to the details of slice preparation, understandingthe hardware in terms of meaningful abstractions) remain to be realized.one result of such work may be the creation of building blocks that can be used to calculateuniversal mathematical functions and ultimately be the basis of families of devices for neural patternmatching. such building blocks may also serve as a point of departure for understanding neural functions at a higher level of abstraction than is possible today.an analogy might be drawn to finding a mathematical representation of a particular dataset. theapproach of mapping an exhaustive inputoutput response is similar to a curvefitting process thatgenerates a function capable of reproducing the dataset perfectly. knowledge of such a function doesnot necessarily entail any understanding of the casual mechanisms underlying that dataset; thus, afunction resulting from a curvefitting process is highly unlikely to be able to account for new data. still,developing such a function may be the first step toward such understanding.as suggested above, building a successful neural prosthetic implies some understanding of thesemantics of neural information processing: how the relevant nerve tissue stores and replicates andprocesses information. however, it also requires a wellunderstood interface between a biological organism (e.g., a person) and the engineered device.one of the primary challenges in the area of neural interface design is the physical connection ofneurons to a chipñthe right neurons must make connection with the right electrodes. the bodyõsnatural response to an electrode implanted in living tissue is to wall it off with glial cells that preventneuron and electrode from making contact. one approach to solving this problem is to coat the electrode with a substance that does not trigger the glial reaction. another is to rely on the neural tissue toreconfigure itself. based on the knowledge that auditory nerves can reconfigure themselves to accommodate the signals emitted by cochlear implants, it may be possible to send out a signal that attracts theright nerves to the right contacts.prosthetic devices that restore or augment human physical abilities are increasingly sophisticated,and followon work will focus on enabling control of more complex actions by robotic arms and otherdevices. on the other hand, although some early work on prostheses that help to replace cognitiveabilities has been successful, prostheses that improve cognitive abilities, by enhancing perception (superhuman sense) and decisionmaking (superhuman computation or knowledge) capabilities, must atpresent be regarded as being on the distant horizon.9.5 evolutionary biology25although the basic principles of evolution (natural selection and mutation) are understood in thelarge, both population genetics and phylogenetics have been radically transformed by the recent avail25section 9.5 is adapted largely from the web page of john huelsenbeck, university of california, san diego, http://biology.ucsd.edu/faculty/huelsenbeck.html.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.312catalyzing inquiryability of large quantities of molecular data. for example, in population genetics (the study of mutationsin populations), more molecular variability was found in the 1960s than had been expected, and thisfinding stimulated kimuraõs neutral theory of molecular evolution.26 phylogenetics (the study of theevolutionary history of life) makes use of a variety of different kinds of data, of which dna sequencesare the most important, as well as wholegenome, metabolic, morphological, geographical, and geological data.27evolutionary biology is founded on the concept that organisms share a common origin and havediverged through time. the details and timing of these divergencesñthat is, the estimation or reconstruction of an evolutionary historyñare important for both intellectual and practical reasons, andphylogenies are central to virtually all comparisons among species. from a practical standpoint,phylogenetics has helped to trace routes of infectious disease transmission (e.g., dental transmission ofaids/hiv) and to identify new pathogens such as the new mexico hantavirus. moret (footnote 27)notes that phylogenetic analysis is useful in elucidating functional relationships within living cells,making functional predictions from sequence data banks of gene families, predicting ligands, developing vaccines, antimicrobials, and herbicides, and inferring secondary structure of rnas. a clear pictureof how life evolved from its humble origins to its present diversity would answer the ageold question,where do we come from?there are many interesting phylogenetic problems. for example, consider the problem of estimating large phylogenies, which is a central challenge in evolutionary biology. given three species, thereare only three possible trees that could represent their phylogenetic history: (a,(b,c)); (b,(a,c)); and(c,(a,b)). (the notation (a,(b,c)) means that b and c share a common ancestor, who itself shares adifferent common ancestor with a. thus, even if one picks a tree at random, there is a one in threechance that the tree chosen will be correct. but the number of possible trees grows very rapidly with thenumber of species involved. for a òsmalló phylogenetic problem involving 10 species, there are34,459,425 possible trees. for a problem involving 22 species, the number of trees exceeds 1023. today,most phylogenetic problems involve more than 80 species and some data sets contain more than 500species. (for 500 species, there are approximately 1.0085 × 101280 possible trees, only one of which can becorrect.) of course, the grandest of all challenges in this area is the construction of the entire phylogenyof all organisms on the planetñthe complete òtree of lifeó involving some 107 to 108 species.given the existence of such large state spaces, it is clear that exhaustive search for the single correctphylogenetic tree is not a feasible strategy, regardless of how fast computers become in the foreseeablefuture. researchers have developed a number of methods for coping with the size of these problems,but many of these methods have serious deficiencies. for example, the optimality criteria used by thesemethods often have dubious statistical justifications. in addition, many of these methods are simplystepwise addition algorithms and make no effort to explore the space of trees. methods with the beststatistical justification, such as maximum likelihood and bayesian inference, are also the most difficultto implement for large problems.thus, the algorithmics of evolutionary biology are a fertile area for research. moret (footnote 27)notes that reconstruction of the tree of life will require either the scalingup of existing reconstructionmethods or the development of entirely new ones. he notes that sequencebased reconstruction methodologies are available that are likely to scale effectively from 15,000 to 100,000 taxa, but that thesemethodologies are not likely to scale to millions of taxa. moret also points out that the use of geneorderdata (i.e., lists of genes in the order in which they occur along one or more chromosomes) can circumvent many of the difficulties associated with using sequence data. on the other hand, there are relatively26m. kimura, òevolutionary rate at the molecular level,ó nature 217(129):624626, 1968; motoo kimura, the neutral theory ofmolecular evolution, cambridge university press, cambridge, ma, 1983.27b.m.e. moret, òcomputational challenges from the tree of life,ó proceedings of the 7th workshop on algorithm engineering andexperiments, alenex õ05, vancouver, siam press, philadelphia, pa, 2005. this paper presents a number of computationalchallenges in evolutionary biology, of which only a few are mentioned in the subsequent discussion in this section.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.illustrative problem domains at the interface of computing and biology313few wholegenome data today, few models for the evolution of gene content and gene order, and a fargreater complexity of the mathematics for gene orders compared to that for dna sequences.a related problem is that of comparing one or more features across species. the comparativemethod has provided much of the evidence for natural selection and is probably the most widely usedstatistical method in evolutionary biology. but comparative analyses must account for phylogenetichistory, since the similarity in features common to multiple species that originate in a common evolutionary history can inappropriately and seriously bias the analyses. a number of methods have beendeveloped to accommodate phylogenies in comparative analyses, but most of these methods assumethat the phylogeny is known without error. however, this is patently unrealistic, because almost allphylogenies have a large degree of uncertainty. an important question is therefore to understand howcomparative analyses can be performed that accommodate phylogenetic history without depending onany single phylogeny being correct.still another interesting problem concerns the genetics of adaptationñthe genomic changes thatoccur when an organism adapts to a new set of selection pressures in a new environment. because theprocess of adaptive change is difficult to study directly, there are many important and unansweredquestions regarding the genetics of adaptation. for example, how many mutations are involved in agiven adaptive change? does this figure change when different organisms or different environments areinvolved? what is the distribution of fitness effects implied by these genetic changes during a bout ofadaptation? how and to what extent are adaptations constrained by phylogenetic history? to whatextent are specific genetic changes inevitable given a change of selection pressures?9.6 computational ecology28the longterm scientific goal of computational ecology is the development of methods to predict theresponse of ecosystems to changes in their physical, biological, and chemical components. computational ecology seeks to combine realistic models of ecological systems with the often large datasetsavailable to aid in analyzing these systems, utilizing techniques of modern computational science tomanage the data, visualize model behavior, and statistically examine the complex dynamics that arise.29questions raised immediately by computational ecology have a direct bearing on issues of importantpolicy significance todayñpotential losses of biodiversity, achievement of sustainable futures, andimpact of global change on local communities.30the scientific questions to be addressed by computational ecology have both theoretical and applied significance. these questions include the following:31¥how are communities organized in space and time?¥what factors maintain or reduce biodiversity?¥what are the implications for ecosystem function?¥how should biodiversity be measured?¥how is ecological robustness maintained?consider, for example, ecological robustness. in ecological communities, many of the salient features remain unchanged, despite the fact that the identities of the relevant actors are continually in flux.28much of the discussion in this section is based on j. helly, t. case, f. davis, s. levin, and w. michener, eds., the state ofcomputational ecology, national center for ecological analysis and synthesis, santa barbara, ca, 1995, available at http://www.sdsc.edu/compecoworkshop/report/report.html.29j. helly et al., eds., the state of computational ecology, national center for ecological analysis and synthesis, santa barbara,ca, 1995, available at http://www.sdsc.edu/compecoworkshop/report/report.html.30j. lubchenco et al., òthe sustainable biosphere initiative: an ecological research agenda,ó ecology 72(2):371412, 1991.31much of this list is taken from helly et al., the state of computational ecology, 1995.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.314catalyzing inquiryspecies richness, species abundance relations, and biogeochemical cycles exhibit remarkable regularity,despite changes at lower levels of organization. in marine systems, the redfield ratios,32 which characterize the mean stoichiometry of plankton and of the water column, summarize the great constancy seenin the concentration ratios of carbon, nitrogen, and phosphorus relative to each other, although absolutelevels vary considerably across the oceans. similarly, sheldon et al.33 observed that the size spectrum,from the smallest particles to large fish, follows a power law with a characteristic exponent, valid acrossa range of trophic levels.ecosystems and the biosphere are complex adaptive systems,34 in which macroscopic patternsemerge from interactions at lower levels of organization and feed back to influence dynamics on thosescales. although macroscopic investigations, such as those of carlson and doyle,35 can shed considerable light on designed or managed systems, or on organ systems that have been the direct products ofevolution, they provide at best a benchmark for comparisons for complex adaptive systems in whichselection acts well below the level of the whole system.the robustness of complex adaptive systems is dependent upon the same suite of characteristicsthat govern the robustness of any systemñheterogeneity and diversity, redundancy and degeneracy,modularity, and the tightness of feedback loops. heterogeneity, for example, provides the adaptivecapacity that allows a system to persist in a changing environment; indeed, the robustness of themacroscopic features of such systems may arise despite, in fact even because of, the lack of robustnessof their components. yet these systems are neither designed nor selected for their macroscopic features.how different then are such systems from those in which the level of selection is the whole system?should robustness be expected to emerge from the bottom up, and how does this selforganized robustness differ from what would be optimal for the robustness of systems as a whole?given that selection is most effective at much lower levels of organization, it is unclear whatsustains ecological robustness at the macroscopic level. a key problem is to understand the propertiesof such selforganized, complex adaptive systemsñto develop theories that facilitate scaling from individuals to whole systems and relating structure to function in order to identify signals warning ofcollapse. what are the consequences of the erosion of biodiversity, the homogenization of systems, andthe breakdown of ecological barriers? how, indeed, will such changes affect the spread of disturbances,from forest fires to novel infectious diseases? addressing these questions will require iterative integration of computational approaches with explorations into largescale stochastic and distributed dynamical systems, with the goal of developing more parsimonious descriptors of essential aspects.general theory concerning the robustness of complex systems focuses on a few key features: heterogeneity and diversity, redundancy and degeneracy, modularity, and the tightness of feedback loops.36robustness is a design objective for most engineering applications, and investigations such as those ofcarlson and doyle have demonstrated how one might select on complex systems as a whole to achievetolerance to particular classes of perturbations. one general principle that emerges from such studies isthat there are tradeoffs between robustness on diverse scales. systems in general may be characterizedas òrobust, yet fragile.ó that is, their robustness to one class of perturbations, or on one scale, may32a.c. redfield, òon the proportions of organic derivatives in sea water and their relation to the composition of plankton,ópp. 176192 in james johnstone memorial volume, r.j. daniel, ed., university press of liverpool, liverpool, uk, 1934.33r.w. sheldon and t.r. parsons, òa continuous size spectrum for particulate matter in the sea,ó journal of the fisheriesresearch board of canada 24:909915, 1967; r.w. sheldon, a. prakash, and w.h. sutcliffe, jr., òthe size distribution of particles inthe ocean,ó limnological oceanography 17:327340, 1972.34s.a. levin, fragile dominion: complexity and the commons, perseus books, reading, ma, 1999; s.a. levin, òcomplex adaptivesystem: exploring the known, the unknown and the unknowable,ó bulletin of the american mathematical society 40:319, 2003.35j.m. carlson and j. doyle, òhighly optimized tolerance: robustness and design in complex systems,ó physical reviewletters 84(11):25292532, 2000.36s.a. levin, fragile dominion: complexity and the commons, perseus books, reading, ma, 1999; s.a. levin, òcomplex adaptivesystems; exploring the known, the unknown and the unknowable,ó bulletin of the american mathematical society 40:319, 2003.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.illustrative problem domains at the interface of computing and biology315necessarily lead to fragility to other classes of perturbations, or on other scales. understanding suchtradeoffs is one dimension of considerable intellectual challenge and problem richness.these general points are instantiated in many different problem areas. two illustrative areasñeachimportant in its own rightñinclude the dynamics of infectious diseases and the dynamics of marinemicrobial systems. in the first case, increased computational resources have fostered the development ofmodels that relate individual behaviors to the spread of novel diseases, including smallpox and newstrains and subtypes of influenza. such models have been given added stimulus by concerns about theintroduction and spread of infectious agents as weapons of bioterror, but the potential for new pandemicsof influenza and other infectious diseases is probably a greater motivation for their development.marine microbial systems represent a vast and important storehouse of biodiversity, about whichmuch too little is known. recent efforts, stimulated by the success of genomics, have directed attentionto characterizing the massive genetic diversity found in these systems. the computational challengesare substantial, even to catalog the vast array of data being collected. yet just as sequencing efforts ingenomics have highlighted the importance of knowing what the catalog of genetic detail reveals abouthow systems function in their ecological environments, the mass of accumulating information aboutmarine microbial diversity spurs efforts at understanding how those marine ecosystems are organizedand what maintains the robustness of features such as microbial diversity.to address the scientific questions described above, researchers need techniques for dealing withsystems across scales of space, time, and organizational complexity. ultimately, an essential enablingtool will be a statistical mechanics of heterogeneous and nonindependent entities, in which the components of a system of interest are continually changing through processes of mutation and other forms ofchange.37 such a system differs dramatically from systems that have traditionally been analyzedthrough the machinery of traditional statistical mechanics (e.g., systems composed of identical, independently moving particles), and analytical methods for dealing with heterogeneous, nonindependententities are generally very sophisticated. in general, such methods rely on the ability to capture theheterogeneity of the distribution (e.g., of traits) in terms of a small number of moments or other descriptors or rely on òequationfreeó approaches38 that finesse the need for explicit closures. in the absence ofsuch an analytical characterization, computation is generally the only alternative to gaining insightsabout ensemble behavior, although computation may often provide analytical insights (and vice versa).today, computational ecology makes use of continuum and individual descriptions. continuummodeling focuses on the impact on local ecological communities of largescale (global) influences suchas climate and fluxes of key elements such as carbon and nitrogen. these models are typically characterized by parameterized partial differential equations that represent appropriately averaged continuumquantities of ecological significance (e.g., density of a species). a central intellectual challenge of the topdown approach is reconciling the hundredkilometer resolution of models that predict global climatechange and elemental fluxes with the meter and centimeter scales of interest in natural and managedecosystems.the ab initio formulation of realistic continuum models is difficult, because the details of theunderlying populations and entities matter a great deal. for example, nałve assumptions of independence, random motion, zero mixing time, or infinite propagation speed, which are often used in the abinitio formulation of continuum models, simply do not hold at the underlying individual level.39accordingly, great care must be taken to derive a continuum description from knowledge of the individual elements in play.37s. levin, mathematics and biology: the interface, lawrence berkeley laboratory pub701, berkeley, ca, 1992, available athttp://www.bio.vu.nl/nvtb/interface.html.38c. theodoropolous, y. quan, and i.g. kevrekidis, òcoarse stability and bifurcation analysis using timesteppers: a reactiondiffusion example,ó proceedings of the national academy of sciences 97(18):98409843, 2000.39s.a. levin, òcomplex adaptive systems: exploring the known, the unknown and the unknowable,ó bulletin (new series) ofthe american mathematical society 40(1):319, 2002.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.316catalyzing inquiryindividualbased modeling seeks to extrapolate from the level of effects on individual plants andanimals to changes in communitylevel patterns, which are necessarily characterized by longer timescales and broader space scales than those of individuals. individualbased models, an ecological formof agentbased models, are rulebased approaches that can track the growth, movement, and reproduction of many thousands of individuals across the landscape40 and, in looking at the global consequencesof local interactions of individuals, are particularly well suited to address questions that relate to spatialheterogeneities (e.g., ecological sanctuaries).in individualbased models, the inherent parallelism of ecological systemsñthat organisms interactconcurrently across spaceñis manifest.41 (by contrast, the parallelism in many computational modelsof other biological systems such as genomes and proteins is primarily a speedup mechanism for computationintensive problems.) individualbased models have been used to represent populations of predators, trees, and endangered species, and they are very useful in understanding the detailed response ofthe population of interest to alternative environmental circumstances.in general, individualbased models are powerful tools for investigating systems that are analytically intractable, and they provide opportunities for the consideration of various scenarios and forexploring ecosystem management protocols that would not otherwise be possible. nevertheless, suchsimulations often contain too many degrees of freedom to allow robust prediction. thus, efforts todevelop macroscopic representations that reduce dimensionality and that suppress irrelevant detail areessentialña point that reinforces the desirability of developing an appropriate statistical mechanics asdescribed above.individualbased modeling is generally computationintensive, for two reasons. the first is that amultitude of individuals must be represented, the behavior of each must be computed, and the entireecosystem being modeled must be timestepped at appropriately fine intervals. the second is that realismdemands a certain amount of stochasticity; thus, an ensemble of simulations must be run in order tounderstand how changes in environmental and other parameters affect predicted outcomes. grid implementations, taking advantage of the inherent parallelism of ecosystems, are one recent effort to advanceindividualbased modeling. the development of algorithms implementing parallelization for individualbased ecological models has enabled a number of simulations, including simulations for fish populationsin the everglades42 and for more general models aimed ultimately at resource management.43data issues in computational ecology are also critical. information technology has been a key enablerfor a great deal of ecological data. for example, highresolution multispectral images captured by satellitesprovide a wealth of information about ecosystems, resulting in maps that can depict how ecologicallysignificant quantities can vary across large areas. while such images cannot yield significant informationon the behavior of individuals, modern telemetry can be used to follow the movements of many individual organisms, a method applied routinely for certain endangered and threatened species.at the same time, much remains to be done. groundbased sensors take data only in their immediate locality. thus, the spatial resolution provided by such sensors is a direct function of their arealdensity. therefore, the advent of inexpensive networked sensors, described in chapter 7, is potentiallythe harbinger of a new explosion of ecological data. for example, a survey of thirty papers chosenrandomly from the journal ecology illustrates that most ecological sampling is conducted with measurements being taken in small areas or at low frequency (often including onetime sampling).44 wireless40see d.l. deangelis and l.j. gross, eds., individualbased models and approaches in ecology, routledge, chapman and hall,new york, 1992.41j. haefner, òparallel computers and individualbased models: an overview,ó pp. 126164 in d. deangelis and l. gross,eds., individualbased models and approaches in ecology, chapman and hall, new york, 1992.42d. wang, m.w. berry, e.a. carr, and l.j. gross, òa parallel landscape model for fish as part of a multiscale ecologicalsystem,ó available at http://www.tiem.utk.edu/gem/papers/dalipaper.pdf.43d. wang, e.a. carr, m.r. palmer, m.w. berry, and l.j. gross, òa grid service module for naturalresource managers,óieee internet computing 9(1):3541, 2005, available at http://www.tiem.utk.edu/gem/papers/gridservice.pdf.44j. porter et al., òwireless sensor networks for ecology,ó biosciences, 2005, in press.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.illustrative problem domains at the interface of computing and biology317sensor networks can fill a gap in our current capabilities by enabling researchers to sample at finerspatial scales or faster rates not currently possible. it is this range of spacetime (widely distributedspatial sensing with high temporal frequency) that will be critical to address the grand challenges of theenvironmental sciences (biogeochemical cycles, biological diversity and ecosystem functioning, climatevariability, hydrologic forecasting, infectious disease and the environment, institutions and resourceuse, landuse dynamics, reinventing the use of materials) proposed by the national research council.45similarly, an explosion of data and of information will arise from sensors carried by individual animals.the extent of information potentially provided by continuous monitoring of position and physiologicaldata, compared to tags and radio collars, is obvious.note also an important synergy between modeling and the use of sensor networks. the effectiveuse of sensor networks relies on modeling and analytical work to guide the placement of sensors. inturn, sensor data provide data to models that allow for prediction and interpretation of models, tounderstand the underlying processes. in this sense, models are the basis for an adaptive samplingscheme for sensor use.another data issue is progress in capturing specimen data in electronic form. over the years,hundreds of millions of specimens have been recorded in museum records. while the information inextant collections could provide numerous opportunities for modeling and increased understanding,very few records are in electronic form and even fewer have been geocoded. museum records carry awealth of image and text data, and digitizing these records in a meaningful and useful way remains aserious challenge, in terms of both appropriate technical methods and the practical effort and resourcesrequired.9.7 genomeenabled individualized medicineby many accounts, knowledge of the sequence of the human genome has enormous potential forchanging the practice of medicine and the delivery of health care services. as more is understood abouthuman biology, it is increasingly feasible for medicine to be predictiveñto have advance knowledge ofhow a personõs health status will respond (positively or negatively) to various exposures to differentfoods and environmental events, and to prevent disease and sustain lifelong health and wellbeing.both these goals depend on a personalized medicine that begins with deep knowledge of the implications of the genetic makeup of any given individual, as well as his or her health and medical life history.indeed, one of the most important implications of knowledge of the genome is the possibility thatmedical treatment and interventions might be more customized to the genetic profile of individuals orgroups in ways that maximize the likelihood of successful outcomes.46one necessary precondition for genomebased individualized medicine is technology for the inexpensive acquisition of sequence informationñperhaps a few hundred dollars for an individualõs complete genome, for example.47 on the other hand, from a costeffectiveness standpoint, it is better tostratify individuals into subcategories that are relevant to various treatment or intervention regimes bylooking at a limited number of genetic markers, rather than to acquire the complete genetic sequence ofall individuals involved. this vision has led major pharmaceutical companies to proclaim that genomic45national research council, grand challenges in environmental sciences, national academy press, washington, dc, 2001.46one of the most ambitious efforts to exploit the potential of genomeenabled individualized medicine is being undertaken bymexico, whose population is composed of more than 65 native indian groups and spaniards. because the overall genetic makeupof this population is associated with a characteristic set of disease susceptibilities, mexico has undertaken this initiative to reducethe social and financial burden of health problems, since new strategies for prevention, early diagnosis, and more effectivetreatment are essential to meet the mid and longterm health care goals in mexico. see gerardo jimenezsanchez, òdeveloping aplatform for genomic medicine in mexico,ó science 300:295296, 2003.47note that this is 105 times less expensive than the sequencing of the first genome. whether the least expensive approachturns out to be sequencing individual genomes from scratch, or sequencing only those portions specific to individuals andintegrating those portions into the genome of the generic human, remains to be seen.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.318catalyzing inquirymedicine and related technologies will allow physicians to provide the right drug to the right patient atthe right time. thus, the term òindividualized medicineó should be regarded as one that ranges fromsingle individuals (likely in the fartherterm future) to genetically differentiated subpopulations (morelikely to happen in the near term).the fundamental challenge is to correlate genetic variation to susceptibility for specific diseases,specific drug reactions, and specific responses to environmental insult. but even with these correlationsin hand, it is a very long way from examination of individual druggene interactions to individualizedmedicineñwhat might be called translational medicineñthat affects the wellbeing of the citizenry atlarge. traversing this distance will require considerable advances on multiple fronts: in the laboratory,on the computer, and in how scientists conceptualize the relationships between all of the individualcomponents involved.9.7.1 disease susceptibility48it has been known for many years that many medical conditions have a genetic basis. indeed, formany illnesses, the strongest predictor of risk is an individualõs family history. the association ofspecific genomic differences with the likelihood of disease will provide physicians and patients withmore specific and more certain information. such knowledge will allow individuals to takes steps thatreduce the likelihood and/or severity of such disease in the future. these steps might include greatermedical surveillance or screening, environmental changes, diet, exercise, or preventive drug therapy(e.g., more frequent colonoscopies starting earlier in life for individuals with genetic profiles that implya high degree of risk for colon cancer).it is useful to distinguish between genetic signatures that are highly penetrant and those that arehighly prevalent. a highly penetrant genetic signature associated with a disease is one whose presenceimplies a high likelihood that the disease will develop in an individual with that signature: examplesprovided by guttmacher and collins (footnote 48) include mutations in the brca1 and brca2 genesthat increase the risk of breast and ovarian cancer, in the hnpcc gene set that increases the risk ofhereditary nonpolyposis colorectal cancer, and in the gene for synuclein that causes parkinsonõs disease. a highly prevalent genetic signature is one that occurs frequently in the population, but itspresence may or may not be associated with a large increase in the likelihood that a disease will developin an individual with that signature: as examples, guttmacher and collins (footnote 48) include amutation in the factor v leiden gene that increases the risk of thrombosis, in the apc (adenomatosispolyposis coli) gene that increases the risk of colorectal cancer, and in the apolipoprotein gene thatincreases the risk of alzheimerõs disease.from the standpoint of the individual, identification of a highly penetrant genetic signature associatedwith disease will have important clinical ramifications. however, from a public health standpoint, it is theidentification of highly prevalent genetic signatures associated with disease that is most significant.the bestunderstood genetic disorders leading to disease are those associated with the inheritanceof a single gene. such disease conditions have been cataloged in the online mendelian inheritance inman (omim) catalog.49 examples of singlegene conditions cited by guttmacher and collins includehereditary hemochromatosis, cystic fibrosis, alpha1antitrypsin deficiency, and neurofibromatosis. these48the discussion in this section on monogenic and highly penetrant signatures is based on excerpts from a.e. guttmacher andf.s. collins, ògenomic medicineña primer,ó new england journal of medicine 347(19):15121520, 2002. the discussion in thissection on polygenic and highly prevalent signatures is based on excerpts from p.d. pharoah, a. antoniou, m. bobrow, r.l.zimmern, d.f. easton, and b.a. ponder, òpolygenic susceptibility to breast cancer and implications for prevention,ó naturegenetics 31(1):3336, 2002. a highly positive and optimistic view of the impact of the genome on medicine can be found in f.s.collins and v.a. mckusick, òimplications of the human genome project for medical science,ó journal of the american medicalassociation 285(5):540544, 2001. a somewhat contrary view can be found in n.a. holtzman and t.m. marteau, òwill geneticsrevolutionize medicine?ó new england journal of medicine 343(2):141144, 2000.49see http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=omim.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.illustrative problem domains at the interface of computing and biology319disorders are highly penetrant, but occur relatively rarely in the population (with approximate incidences of one in several hundred or less).on the other hand, multifactor genetic causality for disease is almost certainly much more common than monogenic causality. in principle, knowledge of the range of genetic variations across manyloci in the population will allow researchers to estimate risks arising from the combined effect of suchvariations.using breast cancer as a case study, pharoah et al. (footnote 48) compared the potential for prediction of risk based on common genetic variations with the predictions that could be made using knownand established risk factors. they concluded that a typical polygenic approach for analysis wouldsuggest that the half of the population at highest risk would account for 88 percent of all affectedindividuals, if all of the susceptibility genes could be identified. however, using currently knownfactors for breast cancer to stratify the population, they estimated that the half of the population athighest risk would account for only 62 percent of all cases. pharoah et al. thus suggest that geneticprofiles may provide significant improvement in the ability to differentiate atrisk individuals fromindividuals not at risk.nevertheless, for a variety of reasons, identifying the relevant genetic signatures over multiplegenes that account for disease susceptibility will pose significant intellectual challenges. probably themost important point is that the contribution of any given gene involved is likely to be weak; hencedetecting its clinical significance may be problematic. nongenomic effects, such as posttranslationalmodifications, may also be relevant. zimmern50 notes that even monogenic conditions can result invariable expressivity and incomplete penetrance, and that similar disease phenotypes may result fromgenetic heterogeneity, whether in the form of allelic heterogeneity (different mutations at the samelocus) or locus heterogeneity (where mutations occur at different loci). different mutations of the samegene may also give rise to separate clinical effects. environmental factors may be difficult to disentanglefrom genetic ones. as a consequence of such issues, definitive conclusions about the relationship of agiven polygenic genotype to a specific disease condition may well be difficult to draw.an extension of the genomic approach to disease susceptibility applies to understanding the impactof an individualõs genomic composition on that individualõs response to various environmental insultsto the body, such as those caused by exposure to chemicals (e.g., from drinking water or air pollution)or electromagnetic fields (e.g., from cell phones or ambient radiation). furthermore, in dealing withcertain environmental insults, stochasticity is likely to play an important role. for example, in considering the effects of radiation on the genome, macroscopic parameters that characterize radiation such asduration and intensity are insufficient to determine its effect, simply because what part of a genome isaffected is mostly a matter of chance. thus, a given dose of a certain kind of radiation will not affectindividuals in equal measure and, more to the point, could not be expected to affect even an ensembleof identical twins similarly.overall, there is wide variability in individual responses to environmental influences. while existing diseases, differences in gender, or differences in nutritional status affect such variability, geneticinfluences are also important. genes that affect the human response to environmental exposure (calledenvironmentally responsive genes by the environmental genome project [egp] of the national institute of environmental health sciences [niehs] tend to fall into several categories.51 that is, they affectthe cell cycle, dna repair, cell division, cell signaling, cell structure, gene expression, apoptosis, andmetabolism. the initial phases of the egp are focused on identifying single nucleotide polymorphisms(snps) associated with 554 genes identified by the scientific community as environmentally responsive.identification of the snps associated with environmentally responsive genes would make it possible toconduct epidemiological studies that classify subjects by snps, thus increasing the utility of these50r.l. zimmern, òthe human genome project: a false dawn?ó british medical journal 319(7220):1282, 1999.51see http://www.niehs.nih.gov/envgenom/egp.htm.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.320catalyzing inquirystudies in detecting genetic contributions to the likelihood of various diseases with at least partialenvironmental causation.the challenges of polygenic data analysis are formidable. an example of methodological researchin this area is that of nelson et al.,52 who developed the combinatorial partitioning method (cpm) forexamining multiple genes, each containing multiple variable loci, to identify partitions of multilocusgenotypes that predict interindividual variation in quantitative trait levels. the cpm offers a strategyfor exploring the highdimensional genotype state space so as to predict the quantitative trait variationin the population at large that does not require the conditioning of the analysis on a prespecified geneticmodel, such as a model that assumes that interacting loci can each be identified through their independent, marginal contribution to trait variability. on the other hand, a bruteforce approach to this correlation problem explodes combinatorially. therefore, it is likely that finding significant correlations willdepend on the ability to prune the search space before specific combinations are testedñand the abilityto prune will depend on the availability of insight into biological mechanisms.9.7.2 drug response and pharmacogenomics53as with disease susceptibility, it has been known for many years that different individuals responddifferently to the same drug at the same dosages and that the relevant differences in individuals are atleast partly genetic in origin. however, characterization of the first human gene containing dna sequence variations that influence drug metabolism did not take place until the late 1980s.54 today,pharmacogenomicsñthe impact of an individualõs genomic composition on his or her response tovarious drugsñis an active area of investigation that many believe holds significant promise for changing the practice of medicine by enabling individualbased prescriptions for compound and dosage.55an individualõs genetic profile may well suggest which of several drugs is most appropriate for a givendisease condition. because genetics influence drug metabolism, an individualõs weight will no longer bethe determining factor in setting the optimal dosage for that individual.similarly, many drugs are known to be effective in treating specific disease conditions. however,because of their side effects in certain subpopulations, they are not available for general use. detailedòomicó knowledge about individuals may help to identify the set of people who might benefit fromcertain drugs without incurring undesirable side effects, although some degree of empirical testing willbe needed if such individuals can be identified.56 in addition, some individuals may be more sensitivethan others to specific drugs, requiring differential dosages for optimal effect.as in the case of disease susceptibility, the bestunderstood genetic polymorphisms that affect drugresponses in individuals are those that involve single genes. as an example, evans and relling note that52m.r. nelson, s.l.r. kardia, r.e. ferrell, and c.f. sing, òa combinatorial partitioning method to identify multilocus genotypic partitions that predict quantitative trait variation,ó genome research 11(3):458470, 2001.53much of the discussion in section 9.7.2 is based on excerpts from w.e. evans and m.v. relling, òmoving towards individualized medicine with pharmacogenomics,ó nature 429(6990):464468, 2004.54f.j. gonzalez, r.c. dkoda, s. kimura, m. umeno, u.m. zanger, d.w. nebert, h.v. gelboin, et al., òcharacterization of thecommon genetic defect in humans deficient in debrisoquine metabolism,ó nature 331(6155):442446, 1988. cited in evans andrelling, 2004.55if the promise of pharmacogenomics is realized, a number of important collateral benefits follow as well. drug compoundsthat have previously been rejected by regulatory authorities because of their side effects on some part of the general populationat large may become available to those individuals genomically identified as not being subject to those side effects. thus, theseindividuals would have options for treatment that would not otherwise exist. furthermore, clinical trials for drug testing couldbe much more targeted to appropriate subpopulations with a higher likelihood of ultimate success, thus reducing expensesassociated with failed trials. also, in the longer term, pharmacogenomics may enable the customized creation of more powerfulmedicines based on the specific proteins and enzymes associated with genes and diseases.56another application often discussed in this context is the notion of drugs customized to specific individuals based on òomicódata. however, the business model of pharmaceutical companies today is based on large markets for their products. until itbecomes possible to synthesize and manufacture different drug compounds economically in small quantity, customsynthesizeddrugs for small groups of individuals will not be feasible.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.illustrative problem domains at the interface of computing and biology321individuals that are deficient in thiopurine smethyltransferase (tpmt) can be treated with much lowerdoses of the thiopurine drugs mercaptopurine and azathiopurine used as immunosuppressants and totreat neoplasias. there is a clinical diagnostic test available for the genomic detection of the tpmtdeficiency, but routine use of tpmt genotyping to make treatment decisions is limited. a secondexample also discussed by evans and relling is that polymorphisms in a gene known as cyp2d6 havea strong effect on individualsõ responses to the antihypertensive drug debrisoquine and in the metabolism of the oxytocic drug sparteine.a second example is found in the area of certain drugs for the treatment of cardiovascular disease.numerous examples of differences among individuals have been seen as potential candidate pharmacodynamic loci (e.g., those for angiotensinogen, angiotensinconverting enzyme, and the angiotensin iireceptor). polymorphisms at these loci predict responses to specific treatments such as the inhibition ofangiotensinconverting enzyme. here, researchers hope to establish and utilize antihypertensive drugsthat are matched to the genetic variations among individuals, and thus to optimize blood pressurecontrol and reduce side effects.57a number of monogenic polymorphisms have been found, encoding drugmetabolizing enzymes,drug transporters, and drug targets, as well as diseasemodifying genes, that have been linked to drugeffects in humans. however, these are the òlowhanging fruitó of pharmacogenetics, and for most drugeffects and treatment outcomes, monogenic polymorphisms with clearly recognizable drugresponsephenotypes do not characterize the situation. for example, as in the case of disease susceptibility,nongenomic effects (e.g., posttranslational modifications) on protein function may be relevant. or,multiple genes may act together in networks to create a single drugresponse phenotype.as evans and relling note, genomewide approaches, such as gene expression arrays, genomewide scans, or proteomic assays, can contribute to the identification of asyetunrecognized candidategenes that may have an influence on a drug response phenotype. for example, it may be possible todetect genes whose expression differentiates drug responders from nonresponders (or those for whomcertain drugs are toxic from those for whom they are not), genomic regions with a paucity of heterozygosity in responders compared with nonresponders, or proteins whose abundance differentiates drugresponders from nonresponders.in expressionarray and proteomic approaches, the level of the signal may directly reflect functionalvariationña distinct advantage from an experimental point of view. yet there can be many otherreasons for differences in signal level, such as the choice of tissue from which the samples are drawn(which may not be the tissue of interest where toxicity or response is concerned) or changes in functionnot reflected by levels of mrna or protein. thus, when such studies suggest that a given gene or geneproduct is relevant to drug response, evans and redding point out that largescale molecular epidemiological association studies (in vivo or in vitro with human tissues), biochemical functional studies, andstudies on preclinical animal models of candidate gene polymorphisms become necessary to furtherestablish the link between genetic polymorphism and drug response.a second challenge in pharmacogenomics relates to integrating pharmacogenomics with the everyday practice of medicine. although there are cultural and historical sources of resistance to such integration, it is also true that definitive clinical pharmacogenomic studies have not been conducted thatdemonstrate unambiguously the benefits of integration on clinical outcomes. indeed, there are manydifficulties in conducting such studies, including the multigenic nature of most drug effects and thedifficulty in controlling for nongenetic confounders such as diet or exercise. until such difficulties areovercome, it is unlikely that a significant change will occur in clinical practice.one of the most important databases for the study of pharmacogenomics is a database known as thestanford pharmgkb, described in box 3.4. supported by the national institute of general medical57p. cadman and d. oõconnor, òpharmacogenomics of hypertension,ó current opinion in nephrology and hypertension 12(1):6170, 2003.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.322catalyzing inquiryscience (nigms), pharmgkb is a publicly available, internetaccessible database for pharmacogeneticsand pharmacogenomics. its overall aim is to aid researchers in understanding how genetic variationamong individuals contributes to differences in reactions to drugs.58 the database integrates pharmacodynamics (drug actions), pharmacokinetics (drug metabolism), toxicity, sequence and other molecular data, pathway information, and patient data.9.7.3 nutritional genomicstraditional nutrition research has had among its goals the establishment of overarching dietaryrecommendations for everyoneñin principle, for the worldõs entire population. today, and more so inthe future, the implications of individual genetic makeup for optimal diet have changed that perspective. to understand and exploit the interplay of diet and genetics, nutritional genomics is a relativelynew specialization within the life sciences with two separate but related foci. one focus relates anindividualõs genetic makeup to dietary regimes that are more or less healthy for him or her. for example, it is well known that some individuals are more likely to suffer from high blood pressure if theyconsume salt in relatively large quantities, while others are not. poch et al.59 found a possible geneticbasis on which to differentiate saltsensitive individuals and saltinsensitive ones. if it is possible todevelop genetic tests for salt sensitivity, saltsensitive individuals could be advised specifically to limittheir salt intake, and saltinsensitive individuals could continue to indulge at will their taste for saltysnacks.the traditional focus of nutrition research is not in any way rendered irrelevant by nutritionalgenomics. still, beyond general good advice and informed common sense, in the most ambitious scenarios, recommended dietary profiles could be customized for individuals based on their specific genomic composition. ordovas and corella write:60nutritional genomics has tremendous potential to change the future of dietary guidelines and personalrecommendations. nutrigenetics will provide the basis for personalized dietary recommendations basedon the individualõs genetic makeup. this approach has been used for decades for certain monogenicdiseases; however, the challenge is to implement a similar concept for common multifactorial disordersand to develop tools to detect genetic predisposition and to prevent common disorders decades beforetheir manifestation. . . . [p]reliminary evidence strongly suggests that the concept should work and thatwe will be able to harness the information contained in our genomes to achieve successful aging usingbehavioral changes; nutrition will be the cornerstone of this endeavor.a second focus of nutritional genomics is on exploiting the potential for modifying foodstuffs to bemore healthy, and so dietary advice and discipline might be supplanted in part by such modifications.for example, it may be possible to redesign the lipid composition of oil seed crops using geneticmodification techniques (through either selective breeding or genetic engineering). however, whetherthis is desirable depends on how consumption of a different mix of lipids affects human health. watkinset al.61 argue for an understanding of the overall metabolomic expression of lipid metabolism to ensurethat a particular metabolite composition truly improves overall health, so that a change in lipid composition that is deemed healthy when viewed as lowering the risk of one disease does not simultaneouslyincrease the risk of developing another.58t.e. klein and r.b. altman, òpharmgkb: the pharmacogenetics and pharmacogenomics knowledge base,ó pharmacogenomicsjournal 4(1):1, february 2004.59e. poch, d. gonzalez, v. giner, e. bragulat, a. coca, and a. de la sierra, òmolecular basis of salt sensitivity in humanhypertension: evaluation of reninangiotensinaldosterone system gene polymorphisms,ó hypertension 38(5):12041209, 2001.60j.m. ordovas and d. corella, ònutritional genomics,ó annual review of genomics and human genetics 5:71118, 2004.61s.m. watkins, b.d. hammock, j.w. newman, and j.b. german, òindividual metabolism should guide agriculture towardfoods for improved health and nutrition,ó american journal of clinical nutrition 74(3):283286, 2001.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.illustrative problem domains at the interface of computing and biology323more generally, watkins et al. point out that the goal of nutritional improvement of agricultureñtoproduce changes in crops and foods that provide health benefits to allñis difficult to achieve becausemodifications of existing foodstuffs are likely to advantage some people while disadvantaging others.watkins et al. cite the example of recent attempts to increase the carotenoid content of the food supplyña move that was thought to have protective value against certain cancers, especially lung cancer. in themidst of this effort, it was found that high intakes of §carotene as a supplement actually increased theincidence of lung cancer in smokersñand the move was abandoned.the intellectual underpinning of this effort is thus metabolomics, the quantitative characterizationof the set of metabolitesñgenerally small, nonprotein moleculesñinvolved in the metabolism or a cell,tissue, or organism over its lifetime. in the context of nutritional genomics, metabolomic studies attemptto characterize the levels, activities, regulation, and interactions of all metabolites in an individual anddetermine how this characterization changes in response to various foods that are consumed. genomicsis important because genetic makeup is an important influence on the specific nature of the metabolomicchanges that result as a function of food consumption.9.8 a digital human on which a surgeon can operate virtuallya surgical act on a human being is by definition an invasive process, one that inflicts many insultson the body. prior to the advent of medical imaging techniques, surgeons relied on their generalknowledge of anatomy to know where and what to cut. todayõs imaging technologies provide thesurgeon with some idea of what to expect when he or she opens the patient.at the same time, a surgeon in the operating room has no opportunity to practice the operation onthis particular patient. experience with other patients with similar conditions helps immeasurably, ofcourse, but it is still not uncommon even in routine surgical operations to find some unexpectedproblem or complication that the surgeon must manage. fortunately, most such problems are minorand handled easily. surgeonsintraining operate first on cadavers and move to live patients only aftermuch practice and under close supervision.consider then the advantages that a surgeon might have if he or she were to be able to practice adifficult operation before doing it on a live patient. that is, a surgeon (or surgeonintraining) wouldpractice or train on a digital model of a human patient that incorporates static and dynamic physicalproperties of the body in an operating room environment (e.g., under anesthesia, in real gravity) whenit is subject to surgical instruments.in this environment, the surgeon would likely wear glasses that projected an appropriate image tohis or her retina and use implements that represented real instruments (e.g., a scalpel). kinetic parameters of the instrument (e.g., speed, velocity, orientation) would be monitored and registered onto theimage that the surgeon sees. when òtouchedó by the instrument, the image would respond appropriately with a change in shape and connectivity (e.g., when a scalpel touches a piece of skin, it mightseparate into two parts and a cut would appear). blood would emerge at realistic rates, and tissue underthe skin would appear.even in this very simple example, many challenges can be seen. to name just a few:¥realistic modeling of body subsystems. from the perspective of a surgeonõs scalpel, the body issimply a heterogeneous and spatially organized mass of tissue. of course, this mass of tissue is functionally a collection of subsystems (e.g., organs, muscle tissue, bone) that have different properties.these subsystems must be separated so that the physiological responses of surgery are appropriatelypropagated through them when surgery occurs.¥integration of personspecific information with a generic model of a human being. because of the laborinvolved in constructing a digital model of a human being, it makes sense to consider an approach inwhich a model of a generic human being is developed and then adjusted according to personspecificinformation of any given patient.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.324catalyzing inquiry¥spatial registration and alignment of instruments, the surgeonõs hands, and the digital body being operated on. the surgeon must see an instrument move to the position in the body to which the surgeon hasmoved it. when a cutting motion is made, the appropriate tissue should split in the appropriate placeand amount.¥the different feel and texture of tissue depending on whether the instrument is a scalpel or a finger. adigital human for surgical use must provide appropriate force feedback (òhaptic capabilityó) to thesurgeon so that, for example, cutting into soft tissue feels different than cutting into bone.¥incorporation of gravity in the model. many organs consist of soft tissue that is deformed easilyunder pressure from instruments and touch. as importantly, tissues are subject to gravitational forcesthat will change their shape as their orientation is changed (the breast of a woman lying on her back hasan entirely different shape than when she is lying on her side).some first steps have been taken in many of these areas. for example, a project at the ohiosupercomputer center (osc) in 1996 sought to develop a virtual realitybased simulation of regionalanesthesia that employed haptic techniques to simulate the resistance felt when an injection is given ina certain area (box 9.4).a second example is work in computational anatomy, one application of which has sought tocharacterize the structure of human brains in a formal manner. structure is interesting to neuroscientistsbecause of a presumed link between physical brain structure and neurological function. through mathematical transformations that can deform one structure into another, it is possible to develop metricsthat can characterize how structurally different two brains are. these metrics can then be correlatedwith understanding of the neurological functions of which each brain is capable (box 9.5). such metricscan also be used to identify normal versus diseased states that are reflected anatomically.box 9.4a virtual reality simulation of regional anesthesiaa collaborative effort between researchers at the ohio state university hospitals, immersion corporation, andthe ohio supercomputer center has led to the creation of a virtual reality simulator that enables anesthesiologistsintraining to practice in a realistic environment the injection of a local anesthetic into the epiduralspace of the spinal column. the system includes a workstation capable of stereo display, a realtime spatialvolume renderer, a voiceactivated interface, and most importantly, a onedimensional haptic probe capableof simulating the resistive forces of penetrated tissues.although this procedure appears simple, it is in fact a delicate manual operation that requires the placementof a catheter into a small epidural space using only haptic cues (i.e., cues based on tactile sensations ofpressure) to guide the needle. by feeling the resistive forces of the needle passing through various tissues, theanesthesiologist must maneuver the tip of the needle into the correct space without perforating or damagingthe spinal cord in the process.the system is designed to enable the trainee to practice the procedure on a variety of datasets representativeof what he or she might experience with real patients. that is, the pressure profile as a function of needlepenetration would vary from patient to patient. by training in this environment, the trainee can gain proficiency in the use of this technique in a nonharmful manner.source: l. hiemenz, j.s. mcdonald, d. stredney, and d. sessanna, òa physiologically valid simulator for training residents to performan epidural block,ó proceedings of the 15th southern biomedical engineering conference, march 2931, 1996, dayton, oh. see alsohttp://www.osc.edu/research/biomed/pastprojects/anesthesia/index.shtml.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.illustrative problem domains at the interface of computing and biology3259.9 computational theories of selfassembly and selfmodification62selfassembly is any process in which a set of components joins together to form a larger, morecomplex structure without centralized or manual control. for example, it includes biologically significant processes ranging from the joining of amino acids to form a protein and embryonic development tononbiological chemical processes such as crystallization. more recently, the term has become widelyused as researchers attempt to create artificial selfassembling systems as a way to fabricate structuresefficiently at nanometer scale.one kind of structureñthat can be described as a simple repeating pattern in which molecules forminto a regular structure or latticeñis the basis for creating artifacts such as crystals or batteries that canbe extended to potentially macroscopic scale; this process is known as periodic selfassembly. however,for applications such as electronic circuits, which cannot be described as a simple repeating pattern, abox 9.5computational anatomycomputational anatomy seeks to make more precise the commonsense notion that samples of a given organ froma particular species are both all the same and all different. they are the same in the sense that all human brains, forexample, exhibit similar anatomical characteristics and can be associated with the canonical brain of homosapiens, rather than the canonical brain of a dog. they are all different in the sense that each individual has a slightlydifferent brain, whose precise anatomical characteristics differ somewhat from those of other individuals.computational anatomy is based on a mathematical formalism that allows one structure (e.g., a brain) to bedeformed reversibly into another. (reversibility is important because irreversible processes destroy information about the original structure.) in particular, the starting structure is considered to be a deformable template.the template anatomy is morphed into the target structure via transformations applied to subvolumes, contours, and surfaces. these computationally intensive transformations are governed by generalizations of theeuler equations of fluid mechanics and are required only to preserve topological relationships (i.e., to transform smoothly from one to the other).key to computational anatomy is the ability to calculate a measure of difference between similar structures.that is, a distance parameter should represent in a formalized manner the extent to which two structuresdifferñand a distance of zero should indicate that they are identical. in the approach to computationalanatomy pioneered by grenander and miller,1 the distance parameter is the square root of the energy required to transform the first structure onto the metric of the second with the assumption that normal transformations follow the leastenergy path.one instance in which computational anatomy has been used is in understanding the growth of brains asjuveniles mature into adults. thompson et al.2 have applied these deformation techniques to the youngestbrains, with results that accord well with what was seen in older subjects. in particular, they are able to predictthe most rapid growth in the isthmus, which carries fibers to areas of the cerebral cortex that support languagefunction. a second application has sought to compare monkey brains to human brains.1u. grenander and m.i. miller, òcomputational anatomy: an emerging discipline,ó quarterly journal of applied mathematics 56:617694, 1998.2p.m. thompson, j.n. giedd, r.p. woods, d. macdonald, a.c. evans, and a.w. toga, ògrowth patterns in the developing braindetected by using continuum mechanical tensor maps,ó nature 404:190193, march 9, 2000; doi:10.1038/35004593.source: much of this material is adapted from òcomputational anatomy: an emerging discipline,ó envision 18(3), 2002, available athttp://www.npaci.edu/envision/v18.3/anatomy.html#establishing.62section 9.9 is based largely on material from l. adleman, q. cheng, a. goel, m.d. huang, d. kempe, p. moisset de espan”s,p. wilhelm, and k. rothemund, òcombinatorial optimization problems in selfassembly,ó stoc õ02, available at http://www.usc.edu/dept/molecularscience/optimizeselfassembly.pdf.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.326catalyzing inquirymore expressive form of selfassembly is required. ideally, a designer could select a set of componentsand a set of rules by which they connect, and the system would form itself into the desired final shape.this kind of selfassembly, called nonperiodic or programmable selfassembly, would allow thecreation of arbitrary arrangements of components. nonperiodic selfassembly would be useful for theefficient execution of tasks such as electronic circuit design, material synthesis, micro and nanomachineconstruction, and many other technological feats. for the purposes of artificial selfassembly technology, the pinnacle result of a theory would be to be able to select or design an appropriate set ofcomponents and assembling rules to produce an arbitrary desired result.selfassembly, both as a biological process and as a potential technology, is poorly understood. arange of significant (and possibly insuperable) engineering and technological challenges stands in theway of effectively programming matter to form itself into arbitrary arrangements. a less prominent butno less important challenge is the lack of a theoretical foundation for selfassembly.a theory of selfassembly would serve to guide researchers to determine which structures are achievable, select appropriate sets of components and assembling rules to produce desired results, and estimate thelikely time and environmental conditions necessary to do so. such a theory will almost certainly be basedheavily on the theory of computation and will more likely be a large collection of theoretical results andproofs about the behavior of selfassembling systems, rather than a single unified theory such as gravity.the grandest form of such a theory would encompass and perhaps unify a number of disparateconcepts from biology, computer science, mathematics, and chemistryñsuch as thermodynamics, catalysis and replication, computational complexity, and tiling theory63and would require increases inour understanding of molecular shape, the interplay between enthalpy and entropy, and the nature ofnoncovalent binding forces.64 a central caveat is that selfassembly occurs with a huge variety of mechanisms, and there is no a priori reason to believe that one theory can encompass all or most of selfassembly and also have enough detail to be helpful to researchers. in more limited contexts, however,useful theories may be easier to achieve, and more limited theories could serve in guiding researchers todetermine which structures are achievable or stable, to identify and classify failure modes and malformation, or to understand the time and environmental conditions in which various selfassemblies canoccur. furthermore, theories in these limited contexts may or may not have anything to do with howreal biological systems are designed.for example, progress so far on a theory of selfassembly has drawn heavily from the theory oftilings and patterns,65 a broad field of mathematics that ties together geometry, topology, combinatorics, and elements of group theory such as transitivity. a tiling is a way for a set of shapes to cover aplane, such as m.c. escherõs famous tesselation patterns. selfassembly researchers have focused onnonperiodic tilings, those in which no regular pattern of tiles can occur. most important among aperiodic patterns are wang tiles, a set of tiles for which the act of tiling a plane was shown to be equivalentto the operation of a universal turing machine.66 (because of the grounding in the theory of wang tilesin particular, the components of selfassembled systems are often referred to as òtilesó and collections oftiles and rules for attaching them as òtiling systems.ó)with a fundamental link between nonperiodic tilings and computation being established, it becomespossible to consider the possibility of programming matter to form desired shapes, just as turing machines canbe programmed to perform certain computations. additionally, based on this relationship, computationallyinspired descriptions might be sufficiently powerful to describe biological selfassembly processes.today, one of the most important approaches to a theory of selfassembly focuses on this abstractmodel of tiles, which are considered to behave in an idealized, stochastic way. tiles of different typesare present in the environment in various concentrations, and the probability of a tile of a given type63l.m. adleman, òtoward a mathematical theory of selfassembly,ó usc tech report, 2000, available at http://www.usc.edu/dept/molecularscience/papers/fp000125satechreportnote.pdf.64g. whitesides, òselfassembly and nanotechnology,ó fourth foresight conference on molecular nanotechnology, 1995.65b. grunbaum and g.c. shephard, tilings and patterns, w. h. freeman and co., new york, 1987.66h. wang, ònotes on a class of tiling problems,ó fundamenta mathematicae 82:295305, 1975.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.illustrative problem domains at the interface of computing and biology327attempting to connect to an established shape is proportional to its share of the total concentration oftiles. then, the ògluesó of touching sides of the adjacent tiles have a possibility of attaching. simulatingsuch selfassembly is actually relatively simple. given a set of tiles and glues, a simulation can predictwith arbitrary accuracy the end result. however, this is complicated by the fact that a given tilingsystem might not have a unique end result; situations could arise in which two different tiles join theassembly in the same location. while this may seem an undesirable situation, such ambiguous systemsmay be necessary to perform universal computation.67the more challenging question is the converse of simulation: given a desired result, how do we getthere? research into the theory of selfassembly has focused on two more specific framings of thisquestion. first, what is the minimum number of tile types necessary to create a desired shape (theòminimum tile set problemó) and, given a specific tiling system, what concentrations produce the endresult the fastest (the òtile concentrations problemó)? the former has been shown to be npcomplete,but has polynomial solutions given certain restrictions on shape and temperature.the current state of the art in the theory of selfassembly abstracts away much of the details ofchemistry. first, the theory considers only the assembly of twodimensional patterns. for artificial dnatiles, designed to be flat, rigid, and square, this may be a reasonable approximation. for a more generaltheory that includes the selfassembly in three dimensions of proteins or other nonrigid and highlyirregularly shaped macromolecules, it is less clear that such a theory is sufficient. extending the currenttheory to irregular shapes in three dimensions is a key element of this challenge problem.the history of the motivation of research into the theory of selfassembly provides a lesson forresearch at the biocomp interface. originally, researchers pursued the link between selfassembly andcomputation because they envisioned selfassembled systems constructed from dna as potential competitors to electronic digital computing hardware, that is, using biochemistry in the service of computation. however, as it became less obvious that this research would produce a competitive technology,interest has shifted to using the computational theory of selfassembly to increase the sophistication ofthe types of molecular constructs being created. in other words, todayõs goal is to use computationaltheory in the service of chemistry. this ebb and flow of both source theory and application betweencomputation and biochemistry is a hallmark of a successful model of research at the interface.another area related to theories of selfassembly is what might be called adaptive programming.today, most programs are static; although variables change their values, the structure of the code doesnot. because computer hardware does not fundamentally differentiate between òcodeó and òdataó (atthe machine level, both are represented by 1õs and 0õs), there is no reason in principle that code cannotmodify itself in the course of execution. selfmodifying code can be very useful in certain contexts, butits actual execution path can be difficult to predict and, thus, the results that might be obtained fromprogram execution are uncertain.however, biological organisms are known to learn and adapt to their environmentsñthat is, theyselfmodify under certain circumstances. such selfmodification occurs at the genomic level, where thedna responsible for the creation of cellular proteins contains both genetic coding and regions thatregulate the extent to which, and the circumstances under which, genes are activated. it also occurs atthe neural level, where cognitive changes (e.g., a memory or a physical skill) are reflected in reorganizedneural patterns. thus, a deep understanding of how biology organizes selfmodification in using dnaor in a neural brain may lead to insights about how one might approach human problems that call forselfmodifying computer programs.9.10 a theory of biological information and complexitymuch of this report is premised on the notion of biology as an information science and has arguedthat information technology is essential for acquiring, managing, and analyzing the many types of67p.w. rothemund, òusing lateral capillary forces to compute by selfassembly,ó proceedings of the national academy ofsciences 97(3):984989, 2000.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.328catalyzing inquirybiological data. this factñthat an understanding of biological systems depends on so many differentkinds of biological data, operating at so many different scales, and in such volumeñsuggests thepossibility that biological information and/or biological complexity might be notions with some formalquantitative meaning.how much information does a given biological system have? how should biological complexity beconceptualized? can we quantify or measure the amount of information or the degree of complexityresident in, say, a cell, or perhaps even more challengingly, in an organelle, an ecosystem, or a species?in what sense is an organism more complex than a cell or an ecosystem more complex than an individual organism? establishing an intellectually rigorous methodology through which such informationcould be measured, capturing not only the raw scale of information needed to describe the constituentelements of a system but also its complexity, could be a powerful tool for answering questions about thenature of evolution, for quantifying the effects of aging and disease, and for evaluating the health ofecologies or other complex systems.developing such a theory of biological information and complexity will be extraordinarily challenging, however. first, complexity and information exist at a vast range of orders of magnitude in size andtime, as well as in the vast range of organisms on earth, and it is not at all clear that a single measure orapproach could be appropriate for all scales or creatures. second, progress toward such a theory hasbeen made in fields traditionally separate from biology, including physics and computer science. transferring knowledge and collaboration between biology and these fields is difficult at the best of times,and doubly challenging when the research is at an early stage. finally, such a theory may prove to be thebasis of a new organizing principle for biology, which may require a significant reorientation forpracticing biologists and biological theory.some building blocks for such a theory may already be available. these include information theory,formulated by claude shannon in the mid20th century for analyzing the performance of noisy communication channels; an extension of information theory, developed over the last few decades by theoretical physicists, that defines information in thermodynamic terms of energy and entropy; the body ofcomputational complexity theory, starting from turingõs model of computation and extending it toinclude classes of complexity based on the relative difficulty of families of algorithms; and complexitytheory (once called òchaos theoryó), an interdisciplinary effort by physicists, mathematicians, and biologists to describe how apparently complex behavior can arise from the interaction of large numbers ofvery simple components.measuring or even defining the complexity of a biological systemñindeed, of any complex, dynamic systemñhas proven to be a difficult problem. traditional measures of complexity that have beendeveloped to analyze and describe the products of human technological engineering are difficult toapply or inappropriate for describing biological systems. for example, although both biological systemsand engineered systems often have degrees of redundancy (i.e., multiple instances of the same òcomponentó that serve the same function for purposes of reliability), biological systems also show many othersystemslevel design behaviors that are rarely if ever found in engineered systems. indeed, many suchbehaviors would be considered poor design. for example, òdegeneracyó in biological systems refers tothe property of having different systems produce the same activity. similarly, in most biological systems, many different components contribute to global properties, a design that if included in a humanengineered system would make it very difficult to understand.other attempts at measuring biological complexity include enumerating various macroscopic properties of an organism, such as the number of distinct parts, number of distinct cell types, number ofbiological functions performed, and so forth. in practice this can be difficult (what is considered aòdistinctó part?) or inconclusive (is an organism with more cell types necessarily more complex?).more conveniently, the entire dna sequence of an organismõs genome can be analyzed. since dnaplays a major role in determining the structure and functions of an organism, one approach is toconsider the information content of the dna string. of course, biological knowledge is nowhere closeto actually being able to infer the totality of an organism merely from a dna sequence, but the argucatalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.illustrative problem domains at the interface of computing and biology329ment is that sequence complexity will be highly correlated with organismal complexity. (some advantages of dealing with strings of letters as an abstraction are discussed in section 4.4.1.)because information theory treats all bits as alike and of equal significance, a purely informationtheoretic view would suggest that a gene of a thousand base pairs that encode a crucial protein requiredfor the development of a human characteristic has the same information content (about 2,000 bits) as arandom sequence of the same length with no biological function. this view strains plausibility or,rather, would have limited applicability to biology. thus, the example suggests that something more isneeded.generally, the types of complexity measures applied to dna sequences are defined by their relationship to the process of computation. for example, a string might be considered to be a program, aninput to a program, or the output of a program, and the resulting complexity measure might include thesize of the turing machine that produced it, its running time, or the number of states. each measurecaptures a different sense of complexity of the dna string and will consider different strings to berelatively more or less complex.one such approach is the notion of kolmogorov (or more formally, kolmogorovchaitinsolomonoff) complexity. kolmogorov complexity is a measure of the extent to which it is possible toeliminate redundancies from a bit string without loss of information. specifically, a program is writtento generate the bit string in question. for a truly random string, the program is at least as long as thestring itself. but if there are information redundancies in the string, the string can be compressed, withthe compressed representation being the program needed to reproduce it. a string with highkolmogorov complexity is one in which the difference in length between the string and its program issmall; a string with low kolmogorov complexity is one that contains many redundancies and thus forwhich the generating program is shorter than the string.however, for the purpose of analyzing overall complexity, a purely random string will have amaximal kolmogorov score, which is not what seems appropriate intuitively for estimating biologicalcomplexity. in general, a desired attribute of measures of biological complexity is the socalled onehump criterion. a measure that incorporated this criterion would indicate a very low complexity forboth very ordered sequences (e.g., a purely repeating sequence) and very random sequences and thehighest complexity for sequences in the middle of a notional continuum, neither periodic nor random.68feldman and crutchfield further suggest that biological complexity must also be defined in a settingthat gives a clear interpretation to what structures are quantified.69other measures that have been proposed include thermodynamic depth, which relates a systemõsentropy to the number of possible histories that produced its current state; logical depth, which considers the minimal running time of a program that produced a given sequence; statistical complexitymeasures, which indicate the correlation among different elements of an entityõs components and the68a related phenomenon, highly investigated but poorly understood, is the ubiquity of socalled 1/f spectra in many interesting phenomena, including biological systems. the term ò1/f spectraó refers to a type of signal whose power distribution as afunction of frequency obeys an inverse power law in which the exponent is a small number. a 1/f signal is not random noise(random noise would result in an exponent of zero; i.e., the power spectrum of a random noise source is flat). on the other hand,there is some stochastic component to 1/f spectra as well as some correlation between signals at different nonadjacent times (i.e.,1/f noise exhibits some degree of longrange correlation). similar statistical analyses have been applied to spatial structures, suchas dna, although power and frequency are replaced by frequency of basepair occurrence and spatial interval, respectively (see,for example, a.m. selvam, òquantumlike chaos in the frequency distributions of the bases a, c, g, t in drosophila dna,óapeiron 9(4):103148, 2002; w. li, t.g. marr, and k. kaneko, òunderstanding longrange correlations in dna sequences,óphysica d 75(13):392416, 1994 [erratum:82, 217,1995]). 1/f spectra have been found in the temporal fluctuations of many biological processes, including ion channel kinetics, auditory nerve firings, lung inflation, fetal breathing, human cognition, walking,blood pressure, and heart rate. (see j.m. hausdorff and c.k. peng, òmultiscaled randomness: a possible source of 1/f noise inbiology,ó physical review e 54(2):21542157, 1996, and references therein. hausdorff and peng suggest that if the time scales of theinputs affecting a biological system are òstructuredó and there are a large number of inputs, it is very likely that the output willexhibit 1/f spectra, even if individual input amplitudes and time scales are loosely correlated.)69d.p. feldman and j.p. crutchfield, òmeasures of statistical complexity: why?,ó physics letters a 238:244252, 1997.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.330catalyzing inquirydegree of structure or pattern in that entity; and physical complexity, which interprets the sharedkolmogorov complexity of an ensemble of sequences as information stored in the genome about theenvironment. this last makes the interesting point that one cannot know anything about the meaning ofa dna sequence without considering the environment in which the corresponding organism is expected to live.all of these capture some aspect of the way in which complexity might arise over time through anundirected evolutionary process and be stored in the genome of a species. however, in their physicsinspired search for minimal descriptions, they may be missing the fact that evolution does not produceoptimal or minimal descriptions. that is, because biological organisms are the result of their evolutionary histories, they contain many remnants that are likely to be irrelevant to their current environmentalniches, yet contribute to their complexity. put differently, any given biological organism is almostcertainly not optimized to perform the functions of which it is capable.another difficulty with many of these measuresõ application to biology is that, regardless of theirtheoretical soundness, they will almost certainly be hard to determine empirically. more prosaically,they often involve a fair amount of mathematics or theoretical computational reasoning (e.g., to whatlevel of the chomskian hierarchy of formal languages does this sequence belong?) completely outsidethe experience of the majority of biologists. regardless, this is an area of active research, and furtherintegration with actual biological investigation is likely to produce further progress in identifyingaccurate and useful measures of complexity.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.culture and research infrastructure33133110culture and research infrastructureearlier chapters of this report have focused on what might be achieved experimentally and on thescientific and technical hurdles that must be overcome at the interface of biology and computing. thischapter focuses on the infrastructural underpinnings needed to support research at this interface. notethat because the influence of computing on biology has been much more significant than the influenceof biology on computing, the discussion in this chapter is focused mostly on the former.10.1 setting the contextin 1991, walter gilbert sketched a vision of 21st century biology (described in chapter 1) and notedthe changes in intellectual orientation and culture that would be needed to realize that vision. he wrote:to use [the coming] flood of [biological] knowledge [i.e., sequence information], which will pour across thecomputer networks of the world, biologists not only must become computerliterate, but also change theirapproach to the problem of understanding life. . . . the next tenfold increase in the amount of information inthe databases will divide the world into haves and havenots, unless each of us connects to that informationand learns how to sift through it for the parts we need. this is not more difficult than knowing how toaccess the scientific literature as it is at present, for even that skill involves more than a traditional reading ofthe printed page, but today involves a search by computer. . . . we must hook our individual computers intothe worldwide network that gives us access to daily changes in the database and also makes immediate ourcommunications with each other. the programs that display and analyze the material for us must beimprovedñand we [italics added] must learn how to use them more effectively.1in short, gilbert pointed out the need for institutional change (in the sense of individual life scientists learning to cooperate with each other) and for biologists to learn how to use the new tools ofinformation technology.because the biocomp interface encompasses a variety of intellectual paradigms and disparateinstitutions, section 10.2 describes the organizational and institutional infrastructure supporting workat this interface, illustrating a variety of programs and training approaches. section 10.3 addresses some1w. gilbert, òtoward a paradigm shift in biology,ó nature 349(6305):99, 1991.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.332catalyzing inquiryof the barriers that affect research at the biocomp interface. chapter 11 is devoted to proposing possibleways of helping to reduce the negative impact of these barriers.10.2 organizations and institutionsefforts to pursue research at the biocomp interface, as well as the parallel goal of attracting andtraining a sufficient workforce, are supported by a number of institutions and organizations in thepublic and private sectors. a prime mover is the u.s. government, both by pursuing research in its ownlaboratories, and by providing funding to other, largely academic, organizations. however, the government is only a part of a larger web of collaborating (and competing) academic departments, privateresearch institutions, corporations, and charitable foundations.10.2.1 the nature of the communitythe members of an established scientific community can usually be identified by a variety ofcommonalitiesñfields in which their degrees were received, journals in which they publish, and so on.2the fact that important work at the biocomp interface has been undertaken by individuals who do notnecessarily share such commonalities indicates that the field in question has not jelled into a singlecommunity, but in fact is composed of many subcommunities. members of this community may comefrom any of a number of specialized fields, including (but not restricted to) biology, computer science,engineering, chemistry, mathematics, and physics. (indeed, as the various epistemological and ontological discussions of previous chapters suggest, even philosophers and historians of science may havea useful role to play.)because the intellectual contours of work at the intersection have not been well established, thedefinition of the community must be broad and is necessarily somewhat vague. any definition mustencompass a multitude of cultures and types, leaving room for approaches that are not yet known.furthermore, the field is sufficiently new that people may enter it at many different stages of theircareers.for perspective, it is useful to consider some possible historical parallels with the establishment ofbiochemistry, biophysics, and bioengineering as autonomous disciplines. in each case, the phenomenaassociated with life have been sufficiently complex and interesting to warrant the bringing to bear ofspecialized expertise and intellectual styles originating in chemistry, physics, and engineering.nonbiologists, including chemists, physicists, and engineers, have made progress on some biologicallysignificant problems precisely because their approaches to problems differed from those of biologistsand thus have advanced biological understanding because they were not limited by what biologists feltcould not be understood. on the other hand, chemists, physicists, and engineers have also pursuedmany false or unproductive lines of inquiry because they have not appreciated the complexity thatcharacterizes many biological phenomena or because they addressed problems that biologists alreadyregarded as solved. eventually, biochemistry, biophysics, and bioengineering became established intheir own right as education and cultural inculcation from both parent disciplines came to be required.it is also to be expected that the increasing integration of computing and information into biologywill raise difficult questions about the nature of biological research and science. if an algorithm toexamine the phylogenetic tree of life is too slow to run on existing hardware, clearly a new algorithmmust be developed. does developing such an algorithm constitute biological research? indeed, modernbiology is sufficiently complex that many of the most important biological problems are not easilytamed by existing mathematical theory, computational models, or computing technologies. ultimately,success in understanding biological phenomena will depend on the development and application ofnew tools throughout the research process.2t.s. kuhn, the structure of scientific revolutions, third edition, university of chicago press, chicago, il, 1996.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.culture and research infrastructure33310.2.2 education and trainingeducation, either formal or informal, is essential for practitioners of one discipline to learn aboutanother, and there are many different venues in which training for the biocomp interface may occur.(contrast this to a standard program in physics, for example, in which a very typical career pathinvolves an undergraduate major in physics, graduate education in physics culminating in a doctorate,and a postdoctoral appointment in physics.)reflecting this diversity, it is difficult to generalize about approaches toward academic training at thebiocomp interface, since different departments and institutions approach it with varied strategies. onemain difference in approaches is whether the initiative for creating an educational program and theoversight and administration of the program come from the computer science department or the biologydepartment. other differences include whether it is a standalone program or department, or a concentration or interdisciplinary program that requires a student or researcher to have a òhomeó department aswell, and whether the program was established primarily as a research program for postdoctoral fellowsand professors (and is slowly trickling down to undergraduate and graduate education), or as an undergraduate curriculum that is slowly building its way up to a research program. those differences in originresult in varying emphases on what constitutes core subject matter, whether interdisciplinary work isencouraged and how it is handled, and how research is supported and evaluated.what is clear is that this is an active area of development and investment, and many major collegesand universities have a formal educational program of some sort at the biocomp interface (generally inbioinformatics or computational biology) or are in the process of developing one. of course, there is notyet widespread agreement on what the curriculum for this new course of study should be3 or indeed ifthere should be a single, standard, curriculum.10.2.2.1 general considerationsas a general rule, serious work at the biocomp interface requires knowledge of both biology andcomputing. for example, many models and simulations of biological phenomena are constrained bylack of quantitative data. the paucity of measurements of in vivo rates or parameters associated withdynamics means that it is difficult to understand systems from a dynamic, rather than a static, point ofview. for example, to further the use of biological modeling and simulation, kinetics should be animportant part of early biological courses, including biochemistry and molecular biology, to instill anappreciation in experimental biologists that kinetics is important. the requisite background in quantitative methods is likely to include some nontrivial exposure to continuous mathematics, nonlinear dynamics, linear algebra, probability and statistics, as well as computer programming and algorithmdesign.from the engineering side, few nonbiologists get any exposure to biological laboratory research ordevelop an understanding of the collection and analysis of biological data. this also leads to unrealisticexpectations of what can be done practically, how repeatable (or unrepeatable) a set of experiments canbe, and how difficult it can be to understand the system in detail. computer scientists also requireexposure to probability, statistics, laboratory technique, and experimental design in order to understand the biologistõs empirical methodology. more fundamentally, nonbiologists working at thebiocomp interface must have an understanding of the basic principles relevant to the biological problem domains of interest, such as physiology, phylogeny, or proteomics. (a broad perspective on biology, including some exposure to evolution, ecosystems, and metabolism, is certainly desirable, but islikely not absolutely necessary.)finally, it must be noted that many students choose to study biology because it is a science whosestudy has traditionally not involved mathematics to any significant extent. similarly, w. daniel hillis3r. altman, òa curriculum for bioinformatics: the time is ripe,ó bioinformatics 14(7):549550, 1998.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.334catalyzing inquiryhas noted that òbiologists are biologists because they love living things. a computation is not alive.ó4indeed, this has been true for several generations of students, so that many of these same students arenow incumbent instructors of biology. managing this particular problem will pose many challenges.10.2.2.2 undergraduate programsthe primary rationale for undergraduate programs at the biocomp interface is that the undergraduate years of university education in the sciences carry the greatest burden in teaching a studentthe professional language of a science and the intellectual paradigms underlying the practice of thatscience. the term òparadigmó is used here in the original sense first expounded by kuhn, whichincludes the following:5¥symbolic generalizations, which the community uses without question,¥beliefs in particular models, which help to determine what will be accepted as an explanation ora solution,¥values concerning prediction (e.g., predictions must be accurate, quantitative) and theories (e.g.,theories must be simple, selfconsistent, plausible, compatible with other theories in current use), and¥exemplars, which are the concrete problem solutions that students encounter from the start oftheir scientific education.the description in section 10.3.1 suggests that the disciplinary paradigm of biology is significantlydifferent from that of computer science. because the de novo learning of one paradigm is easier thansubsequently learning a second paradigm that may (apparently) be contradictory or incommensuratewith one that has already been internalized, the argument for undergraduate exposure is based on thepremise that simultaneous exposure to the paradigms of two disciplines will be more effective thansequential exposure (as would be the case for someone receiving an undergraduate degree in one fieldand then pursuing graduate work in another).undergraduate programs in most scientific courses of study are generally designed to preparestudents for future academic work in the field. thus, the goal of undergraduate curricula at the biocompinterface is to expose students to a wide range of biological knowledge and issues and to the intellectualtools and constructs of computing such as programming, statistics, algorithm design, and databases.today, most such programs focus on bioinformatics or computational biology, and in the most typicalcases, the integration of biology and computing occurs later rather than earlier in these programs (e.g.,as senioryear capstone courses).individual programs vary enormously in the number of computer science classes required. forexample, the george washington university department of computer science offers a concentration inbioinformatics leading to a b.s. degree; the curriculum includes 17 computer science courses and 4biology courses, plus a single course on bioinformatics. the university of california, los angeles(ucla) program in cybernetics offers a concentration in bioinformatics, in contrast, in which the students can take as few as seven computer science courses, including four programming classes and twobiologythemed classes. in other cases, a university may have an explicit undergraduate major inbioinformatics associated with a bioinformatics department. such programs are traditionally structuredin the sense of having a set of specific courses required for matriculation.in addition to concentrations at the interface, a number of other approaches have been used toprepare undergraduates:¥an explicitly interdisciplinary b.s. science program can expose students to the interrelationshipsof the basic sciences. sometimes these are cotaught as single units: students in their first year may take4w.d. hillis, òwhy physicists like models, and biologists should,ó current biology 3(2):7981, 1993.5t.s. kuhn, the structure of scientific revolutions, third edition, university of chicago press, chicago, 1996.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.culture and research infrastructure335mathematics, physics, chemistry, and biology as a block, taught by a team of dedicated professors.modules in which ideas from one discipline are used to solve problems in another are developed andused as case studies for motivating the connections between the topics. other coordinated scienceprograms intersperse traditional courses in the disciplines with cotaught interdisciplinary courses(examples: applications of physical ideas in biological systems; dimensional analysis in the sciences;mathematical biology).¥a broad and unrestricted science program can allow students to count basic courses in anydepartment toward their degree or to design and propose their personal degree program. such a systemgives graduates an edge in the ability to transcend boundaries between disciplines. a system of coadvising to help students balance needs with interests would be vital to ensure that such open programsfunction well.¥courses in quantitative science with explicit ties to biology may be more motivating to biologystudents. some anecdotal evidence indicates that biology students can do better in math and physicswhen the examples are drawn from biology; at the university of washington, the average biologystudentõs grade in calculus rose from c to b+ when òcalculus for biologistsó was introduced.6 (note thatsuch an approach requires that the instructor have the knowledge to use plausible biological examplesña point suggesting that simply handing off modules of instruction will not be successful.)¥summer programs for undergraduates offer undergraduates an opportunity to get involved inactual research projects while being exposed to workshops and tutorials in a range of issues at thebiocomp interface. many such programs are funded by a national science foundation or nationalinstitutes of health program.7when none of these options are available, a student can still create a program informally (either onhis or her initiative or with the advice and support of a sympathetic faculty member). such a programwould necessarily include courses sufficient to impart a thorough quantitative background (mathematics, physics, computer science) as well as a solid understanding of biology. as a rule, quantitativetraining should come first, because it is often difficult to develop expertise in quantitative approacheslater in the undergraduate years. exposure to intriguing ideas in biology (e.g., in popular lecture series)would also help to encourage interest in these directions.finally, an important issue at some universities is the fact that computer science departments andbiology departments are located in different schools (school of engineering versus school of arts andsciences). as a result, biology majors may well face impediments to enrolling in courses intended forcomputer science majors, and vice versa. such a structural impediment underlines both the need andthe challenges for establishing a biological computing curriculum.10.2.2.3 the bio2010 reportin july 2003, the national research council (nrc) released bio 2010: undergraduate education toprepare biomedical research scientists (national academies press, washington, dc). this report concluded that undergraduate biology education had not kept pace with computationally driven changesin life sciences research, among other changes, and recommended that mathematics, physics, chemistry,computer science, and engineering be incorporated into the biology curriculum to the point that interdisciplinary thinking and work become second nature for biology students. in particular, the reportnoted òthe importance of building a strong foundation in mathematics, physical and information sciences to prepare students for research that is increasingly interdisciplinary in character.óthe report elaborated on this point in three other recommendationsñthat undergraduate life sci6mary lidstrom, university of washington, personal communication, august 1, 2003.7see http://www.nsf.gov/pubs/2002/nsf02109/nsf02109.htm.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.336catalyzing inquiryence majors should be exposed to engineering principles and analysis, should receive quantitativetraining in a manner integrated with biological content, and should develop enough familiarity withcomputer science that they can use information technology effectively in all aspects of their research.10.2.2.3.1 engineeringin arguing for exposure to engineering, the report noted that the notion of function (of a device or organism) is common to both engineering and biology, but not to mathematics,physics, or chemistry. echoing the ideas described in chapter 6 of this report, bio2010 concluded:understanding function at the systems level requires a way of thinking that is common to many engineers. an engineer takes building blocks to build a system with desired features (bottomup). creating(or recreating) function by building a complex system, and getting it to work, is the ultimate proof thatall essential building blocks and how they work in synchrony are truly understood. getting a system towork typically requires (a) an understanding of the fundamental building blocks, (b) knowledge of therelation between the building blocks, (c) the systemõs design, or how its components fit together in aproductive way, (d) system modeling, (e) construction of the system, and (f) testing the system and itsfunction(s). . . . organisms can be analyzed in terms of subsystems having particular functions. to understand system function in biology in a predictive and quantitative fashion, it is necessary to describe andmodel how the system function results from the properties of its constituent elements.the pedagogical conclusion was clear in the report:understanding cells, organs, and finally animals and plants at the systems level will require that thebiologist borrow approaches from engineering, and that engineering principles are introduced early inthe education of biologists. . . . students should be frequently confronted throughout their biology curriculum with questions and tasks such as how they would design ôxxx,õ and how they would test to seewhether their conceptual design actually works. [for example,] they should be asked to simulate theirsystem, determine its rate constants, determine regimes of stability and instability, investigate regulatoryfeedback mechanisms, and other challenges.a second dimension in which engineering skills can be useful is in logistical planning. there aremany areas in biology now where it is relatively easy to conceive of an important experiment, butdrawing out the implications of the experiment involves a combinatorial explosion of analytical effortand thus is not practical to carry out. it is entirely plausible that many important biological discoverieswill depend on both the ability to conceive an experiment and the ability to reconceive and restructureit logistically so that it is, in fact, doable. engineers learn to apply their fundamental scientific knowledge in an environment constrained by nonscientific concerns, such as cost or logistics, and this abilitywill be critically important for the biologist who must undertake the restructuring described above.box 10.1 provides a number of examples of engineering for life science majors.10.2.2.3.2 quantitative training in its call for greater quantitative training, the bio2010 report echoedthat of other commentators.8 recognizing that quantitative analysis, modeling, and prediction playimportant roles in todayõs biomedical research (and will do so increasingly in the future), the reportnoted the importance to biology students of understanding concepts such as rate of change, modeling,equilibrium, and stability, structure of a system, and interactions among components, and argued thatevery student should acquire the ability to analyze issues arising in these contexts in some depth, usinganalytical methods (including paperandpencil techniques) and appropriate computational tools. aspart of a necessary background, the report suggested that an appropriate course of study would includeaspects of probability, statistics, discrete models, linear algebra, calculus and differential equations,modeling, and programming (box 10.2).8see for example, a. hastings and m.a. palmer, òa bright future for biologists and mathematicians,ó science 299(5615):20032004, 2003, available at http://www.biosino.org/bioinformatics/a%20bright%20future.pdf.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.culture and research infrastructure33710.2.2.3.3 computer science finally, the bio2010 report noted the importance of information technologybased tools for biologists. it recommended that all biology majors be able to develop simulations ofphysiological, ecological, and evolutionary processes; to modify existing applications as appropriate; touse computers to acquire and process data; to carry out statistical characterization of the data andperform statistical tests; to graphically display data in a variety of representation; and to use information technology (it) to carry out literature searches, locate published articles, and access major databox 10.1engineering for life science majorsone example of an engineering topic suitable for inclusion in a biology curriculum is the subject of longrangeneuron signals. introducing such a topic might begin with the electrical conductivity of salt water and of the lipidcell membrane, and the electrical capacitance of the cell membrane. it would next develop the simple equationsfor the attenuation of a voltage applied across the membrane at one end of an axon òcylinderó with distancedown the axon, and the effect of membrane capacitance on signal dynamics for timevarying signals.after substituting numbers, it becomes clear that amplifiers will be essential. on the other hand, real systemsare always noisy and imperfect; amplifiers have limited dynamical range; and the combination of these factsmakes sending an analog voltage signal through a large number of amplifiers essentially impossible.the pulse coding of information overcomes the limitations of analog communication. how are òpulsesógenerated by a cell? this would lead to the power supply needed by an amplifierñion pumps and the nernstpotential. how are action potentials generated? a first example of the transduction of an analog quantity intopulses might be stickslip fraction, in which a block resting on a table, and pulled by a weak spring whose endis steadily moved, moves in òjumpsó whose distance is always the same. this introduction to nonlineardynamics contains the essence of how an action potential is generated.the ònegative resistanceó of the sodium channels in a neuron membrane provides the same kind of òbreakdownó phenomenon. stability and instabilities (static and dynamic) of nonlinear dynamical systems can beanalyzed, and finally the hodgkinhuxley equations illustrated.the material is an excellent source of imaginative laboratories involving electrical measurements, circuits,dynamical systems, batteries and the nernst potential, information and noise, and classical mechanics. it hasgreat potential for simulations of systems a little too complicated for complete mathematical analysis, and thusis ideal for teaching simulation as a tool for understanding.other biological phenomena that can be analyzed using an engineering approach and that are suitable forinclusion in a biology curriculum include the following:¥the blood circulatory system and its control; fluid dynamics; pressure and force balance;¥swimming, flying, walking, dynamical description, energy requirements, actuators, control; material properties of biological systems and how their structure relates to their function (e.g., wood, hair, cell membranecartilage);¥shapes of cells: force balance, hydrostatic pressure, elasticity of membrane and effects of the spatial dependence of elasticity; effects of cytoskeletal force on shape; and¥chemical networks for cell signaling; these involve the concepts of negative feedback, gain, signaltonoise, bandwidth, and crosstalk. these concepts are simple to experience in the context of how an electricalamplifier can be built from components.source: adapted from national research council, bio2010: transforming undergraduate education for future research biologists, thenational academies press, washington, dc, 2003.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.338catalyzing inquirybox 10.2essential concepts of mathematics and computer science for life scientistscalculus¥complex numbers¥functions¥limits¥continuity¥the integral¥the derivative and linearization¥elementary functions¥fourier series¥multidimensional calculus: linear approximations, integration over multiple variableslinear algebra¥scalars, vectors, matrices¥linear transformations¥eigenvalues and eigenvectors¥invariant subspacesdynamical systems¥continuous time dynamicsñequations of motion and their trajectories¥test points, limit cycles, and stability around them¥phase plane analysis¥cooperativity, positive feedback, and negative feedback¥multistability¥discrete time dynamicsñmappings, stable points, and stable cycles¥sensitivity to initial conditions and chaosprobability and statistics¥probability distributions¥random numbers and stochastic processes¥covariation, correlation, and independence¥error likelihoodinformation and computation¥algorithms (with examples)¥computability¥optimization in mathematics and computation¥òbitsó: information and mutual informationdata structures¥metrics: generalized òdistanceó and sequence comparisons¥clustering¥tree relationships¥graphics: visualizing and displaying data and models for conceptual understandingsource: reprinted from national research council, bio2010: transforming undergraduate education for future research biologists,the national academies press, washington, dc, 2003.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.culture and research infrastructure339bases. from the perspective of this report, box 10.3 describes some of the essential intellectual aspects ofcomputer science that biologists must understand.recognizing that students might require competence at multiple levels depending on their needs,the bio2010 report identified three levels of competence as described in box 10.4.box 10.3essential concepts of computer science for the biologist key for the computer scientist is the notion of a field that focuses on information, on understanding ofcomputing activities through mathematical and engineering models and based on theory and abstraction, onthe ways of representing and processing information, and on the application of scientific principles andmethodologies to the development and maintenance of computer systemsñwhether they are composed ofhardware, software, or both.there are many views of understanding the essential concepts of computer science. one view, developed in1991 in the nrc report computing the future, is that the key intellectual themes in computing are algorithmicthinking, the representation of information, and computer programs.1¥an algorithm is an unambiguous sequence of steps for processing information. of particular relevance ishow the speed of the algorithm varies as a function of problem sizeñthe topic of algorithmic complexity.typically, a result from algorithmic complexity will indicate the scaling relationships between how long ittakes to solve a problem and the size of the problem when the solution of the problem is based on a specificalgorithm. thus, algorithm a might solve a problem in a time of order n2, which means that a problem that is100 times as large would take 1002 = 10,000 times as long to solve, whereas a faster algorithm b might solvethe same problem in time of order n ln n, which means a problem 100 times as large would take 100 ln 100= 460.5 times as long to solve. such results are important because all computer programs embed algorithmswithin them. depending on the functional relationship between run time and problem size, a given programthat works well on a small set of test data mayñor may notñwork well (run in a reasonable time) for a largerset of real data. theoretical computer science thus imposes constraints on real programs that software developers ignore at their own peril.¥the representation of information or a problem in an appropriate manner is often the first step in designing analgorithm, and the choice of one representation or another can make a problem easy or difficult, and its solutionslow or fast. two issues arise: (1) how should the abstraction be represented, and (2) how should the representation be structured properly to allow efficient access for common operations? for example, a circle of radius 2can be represented by an equation of the form x2 + y2 = 4 or as a set of points on the circle ((0.00, 2.00), (0.25,1.98), (0.50, 1.94), (0.75, 1.85), (1.00, 1.73), (1.25, 1.56), (1.50, 1.32), (1.75, 0.97), (2.00, 0.00)), and so on.depending on the purpose, one or the other of these representations may be more useful. if the circle of radius2 is just a special case of a problem in which circles of many different radii are involved, representation as anequation may be more appropriate. if many circles of radius 2 have to be drawn on a screen and speed isimportant, a listing of the points on the circle may provide a faster basis for drawing such circles.¥a computer program expresses algorithms and structure information using a òprogramming language.ósuch languages provide a way to represent an algorithm precisely enough that a òhighleveló description (i.e.,one that is easily understood by humans) can be translated mechanically (òcompiledó) into a òlowlevelóversion that the computer can carry out (òexecuteó); the execution of a program by a computer is what allowsthe algorithm to be realized tangibly, instructing the computer to perform the tasks the person has requested.computer programs are thus the essential link between intellectual constructs such as algorithms and information representations and the computers that perform useful tasks.continued1the discussion below is adapted from computer science and telecommunications board, national research council, computing thefuture: a broader agenda for computer science and engineering, national academy press, washington, dc, 1992.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.340catalyzing inquirybox 10.3 continuedthis last point is often misunderstood. for many outsiders, computer science is the same as computer programmingña view reinforced by many introductory òcomputer scienceó courses that emphasize the writingof computer programs. but it is better to understand computer programs as the specialized medium in whichthe ideas and abstractions of computer science are tangibly manifested. focusing on the writing of the computer program without giving careful consideration to the abstractions embodied in the program is not unlikeunderstanding the writing of a novel as no more than the rules of grammar and spelling.algorithmic thinking, information representation, and computer programs are themes central to all subfieldsof computer science and engineering research. they also provide material for intellectual study in and ofthemselves, often with important practical results. the study of algorithms is as challenging as any area ofmathematics, and one of practical importance as well, since improperly chosen or designed algorithms maysolve problems in a highly inefficient manner. the study of programs is a broad area, ranging from the highlyformal study of mathematically proving programs correct to very practical considerations regarding tools withwhich to specify, write, debug, maintain, and modify very large software systems (otherwise called softwareengineering). information representation is the central theme underlying the study of data structures (howinformation can best be represented for computer processing) and much of humancomputer interaction (howinformation can best be represented to maximize its utility for human beings).finally, computer science is closely tied to an underlying technological substrate that evolves rapidly. thissubstrate is the òstuffó out of which computational hardware is made, and the exponential growth that characterizes its evolution makes it possible to construct everlarger, evermorecomplex systemsñsystems thatare not predictable based on an understanding of their individual components. (as one example, the properties of the internet prove a rich and surprisingly complex area of study even though its componentsñcomputers, routers, fiberoptic cablesñare themselves well understood.)a second report of the national research council described fluency with information technology as requiringthree kinds of knowledge: skills in using contemporary it, foundational concepts about it and computing, andintellectual capabilities needed to think about and use it for purposeful work.2 the listing below is theperspective of this report on essential concepts of it for everyone:¥computers (e.g., programs as a sequence of steps, memory as a repository for program and data, overallorganization, including relationship to peripheral devices).¥information systems (e.g., hardware and software components, people and processes, interfaces (bothtechnology interfaces and humancomputer interfaces), databases, transactions, consistency, availability, persistent storage, archiving, audit trails, security and privacy and their technological underpinnings).¥networks: physical structure (messages, packets, switching, routing, addressing, congestion, local areanetworks, wide area networks, bandwidth, latency, pointtopoint communication, multicast, broadcast, ethernet, mobility), and logical structure (client/server, interfaces, layered protocols, standards, network services).¥digital representation of information: concept of information encoding in binary form; different information encodings such as ascii, digital sound, images, and video/movies; precision, conversion and interoperability (e.g., of file formats), resolution, fidelity, transformation, compression, and encryption; standardizationof representations to support communication.¥information organization (including forms, structure, classification and indexing, searching and retrieving,assessing information quality, authoring and presentation, and citation; search engines for text, images, video,audio).¥modeling and abstraction: methods and techniques for representing realworld phenomena as computermodels, first in appropriate forms such as systems of equations, graphs, and relationships, and then in appropriate programming objects such as arrays or lists or procedures. topics include continuous and discrete2computer science and telecommunications board, national research council, being fluent with information technology, nationalacademy press, washington, dc, 1999.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.culture and research infrastructure34110.2.2.4 graduate programsgraduate programs at the biocomp interface are often intended to provide b.s. graduates in onediscipline with the complementary expertise of the other. for example, individuals with bachelorõsdegrees in biology may acquire computational or analytical skills during early graduate school, withcondensed òretrainingó programs that expose then to nonlinear dynamics, algorithms, and so on. alternatively, individuals with bachelorõs degrees in computer science might take a number of courses toexpose them to essential biological concepts and techniques.graduate education at the interface is much more diverse than at the undergraduate level. although there is general agreement that an undergraduate degree should expose the student to thecomponent sciences and prepare him or her for future work, the graduate degree involves a far widerarray of goals, focuses, fields, and approaches. like undergraduate programs, graduate programs canbe standalone departments, independent interdisciplinary programs, or certificate programs that require students to have a òhomeó department.a bioinformatics program oriented toward genomics is very common. virginia techõs program, forexample, has been renamed the program in ògenetics, bioinformatics, and computational biology,óindicating its strong focus on genetic analysis. in contrast, the keck graduate institute at claremontstresses the interdisciplinary skill set necessary for the effective management of companies that straddlethe biologyquantitative science boundary. it awards a masterõs of bioscience, a professional degreemodels, discrete time events, randomization, and convergence, as well as the use of abstraction to hideirrelevant detail.¥algorithmic thinking and programming: concepts of algorithmic thinking, including functional decomposition, repetition (iteration and/or recursion), basic data organization (record, array, list), generalization andparameterization, algorithm vs. program, topdown design, and refinement.¥universality and computability: ability of any computer to perform any computational task.¥limitations of information technology: notions of complexity, growth rates, scale, tractability, decidability,and state explosion combine to express some of the limitations of information technology; connections toapplications, such as text search, sorting, scheduling, and debugging.¥societal impact of information and information technology: technical basis for social concerns about privacy, intellectual property, ownership, security, weak/strong encryption, inferences about personal characteristics based on electronic behavior such as monitoring web sites visited, ònetiquette,ó òspamming,ó and freespeech in the internet environment.a third perspective is provided by steven salzberg, senior director of bioinformatics at the institute for genomic research in rockville, maryland. in a tutorial paper for biologists, he lists the following areas as importantfor biologists to understand:3¥basic computational concepts (algorithms, program execution speed, computing time and space requirements as a function of input size; really expensive computations),¥machine learning concepts (learning from data, memorybased reasoning),¥where to store learned knowledge (decision trees, neural networks),¥search (defining a search space, search space size, treebased search),¥dynamic programming, and¥basic statistics and markov chains.3s.l. salzberg, òa tutorial introduction to computation for biologists,ó computational methods in molecular biology, s.l. salzberg, d.searls, and s. kasif, eds., elsevier science ltd., new york, 1998.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.342catalyzing inquirysomewhat like an m.b.a. with a science requirement. some programs, such as stanfordõs, are administered by the medical school, leading to a focus on medical informatics as well as bioinformatics. thiswould include topics such as clinical trials and image analysis, which would not show up in a moretraditional genomicsfocused bioinformatics degree.the research training program of the keck center for computational and structural biology isintended to develop one of two different kinds of expertise. emerging from this program, a traineewould be a computational expert well versed in computer science and quantitative methods whowould also be knowledgeable in at least one application area of biological significance, or an expert insome biological area (e.g., molecular biology) who would also be aware of the most advanced conceptsin computing. students entering from computational backgrounds take at least three courses in biologybiochemistrybiophysics areas, while students entering from biological backgrounds at least threecourses in computational areas. in addition, all students take an introductory course in computationalscience. dissertation research is supervised by a committee with faculty members as required bythe studentõs home department, but with representation from the computational biology facultyat other keck center institutions as well. research can be undertaken in areas including the visualization of biological complexes, the development of dna and protein sequence analysis, and advancedsimulations.box 10.4competence and expertise in computer science for biology studentsthe bio2010 report recommended that all biology students receive instruction in computer science, distinguishing among three levels of competency. from lowest to highest, these include the following:¥fluency. based on the nrc report being fluent with information technology, fluency refers to the abilityof biology students to use information technology today and to adapt to changes in it in the future. forexample, they need a basic understanding of how computers work and of programming, and a higher degreeof fluency in using networks and databases. students should also be exposed to laboratory experiences usingmedline, genbank, and other biological databases, as well as physiological and ecological simulations. forexample, students could be asked to use computer searches to track down all known information about agiven gene and the protein it encodes, including both structure and function. this would involve exploring theinternal structure of the gene (exons, introns, promoter, transcription factor binding sites); the regulatorycontrol of the gene; sequence homologues of the gene and the protein; the structure and function of theprotein; gene interaction networks and metabolic pathways involving the protein; and interactions of theprotein with other proteins and with small molecules.¥capability in program design for computational biology and genomics applications. students at this levelacquire the minimal skills required to be effective computer users within a computationally oriented biologyresearch team. for example, they would learn structured software development and selected principles ofcomputer science, with applications in computational biology and allied disciplines, and would use examplesand tutorials drawn from problems in computational biology.¥capability in developing software tools for use by the biology community. at this sophisticated level,students need a grounding in discrete mathematics, data structures, and algorithms, as well as database management systems, information systems, software engineering, computer graphics, or computer simulationtechniques. students at this level would be able to design and specify database and information systems foruse by the entire community. of special interest will be tools that require background in graph theory, combinatorics, and computational geometry as applications in highthroughput genomics research and rationaldrug design become increasingly important.source: adapted from national research council, bio2010: transforming undergraduate education for future research biologists, thenational academies press, washington, dc, 2003.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.culture and research infrastructure343a challenge for a field as interdisciplinary as this is that incoming students will arrive with possiblycompletely nonoverlapping backgrounds. most programs accept a b.s. in computer science, biology, ormath as a prerequisite; to produce a wellrounded computational biologist will require very differenttraining programs. the university of coloradoõs certificate program in computational biology requiresincoming students to take preparatory classes in òbiology for computer scientists,ó òcomputer sciencefor bioscientists,ó or òmathematics for bioscientists,ó depending on what the student missed earlier inhis or her education.an advantage of graduate programs is that when communication among faculty of different disciplines is good, graduate projects provide an ideal opportunity for students to work in an interdisciplinary environment. in some cases, work with adjunct professors from industry can lead to excitingprojects. on the other hand, if communication between faculty is poor (which may be possible forreasons described later in this chapter), a graduate student dependent on completing a project (e.g., adissertation) can get caught in the middle of a dispute with no way to graduate.10.2.2.5 postdoctoral programspostdoctoral programs at the biocomp interface are also varied. some postgraduate programs areexplicitly aimed at òconversion,ó that is, training a fully trained member of one field (usually biology) inthe basic tenets of its complement. for example, the university of pennsylvaniaõs postdoctoral programin computational biology is a masterõs degree in computer and information systems, designed for thosewith ph.d.s in biology who need the training. other programs focus on involving the participant inresearch and laboratory work, in preparation for industry or a faculty position, just as in postdoctoralprograms in other fields.some programs, such as dukeõs center for bioinformatics and computational biology, are similarto graduate programs in that they focus on genome analysis. others, like the johns hopkinsõ program incomputational biology, are firmly grounded in genomics but are pointedly reaching out to largerquestions of integrative biology and experimental biology.in promoting postdoctoral programs at the interface of computing and biology, it will be necessaryto take into account the very different traditions of the two fields. in biology, one or more postdoctoralfellowships are quite common (indeed, routine) before an individual strikes out on his or her own. bycontrast, the most typical career path for a newly graduated ph.d. in computer science calls for appointment to a junior faculty position or a position in industryñpostdoctoral fellows in computer science arerelatively rare (though not unheard of).two foundationsupported postdoctoral programs have been influential in stimulating interest atthe biocomp interface: the sloandepartment of energy (doe) program and the burroughswelcomeprogram.10.2.2.5.1 the sloandoe postdoctoral awards for computational molecular biology9 for 8 years,the alfred p. sloan foundation and the u.s. department of energy (office of biological and environmental research) have jointly sponsored postdoctoral research awards for scientists interested in computational molecular biology. the purpose of these fellowships has been to catalyze career transitionsinto computational molecular biology by those holding doctorates in mathematics, physics, computerscience, chemistry, engineering or other relevant fields who would like to bring their computationalsophistication to bear on the complex problems that increasingly face molecular biology.operationally, the program was designed to offer computationally sophisticated young scientistsan intensive postdoctoral opportunity in an appropriate molecular biology laboratory. in most cases,awardees had strong educational backgrounds in a computationally intensive field, although in a few9see http://www.sloan.org/programs/scitechpostdoct.shtml.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.344catalyzing inquiryinstances, awardees had backgrounds from more traditional biological orientations without the computational dimension. of particular interest to the sloandoe program are important problems in structural biology and genome analysis, including analysis of protein and nucleic acid sequence, protein andnucleic acid structure, genome structure and maps, crossspecies genome analysis, multigenic traits,and structurefunction relationships where the structures are from genomes, genes, or gene products.the sloandoe postdoctoral award supports up to 2 years of research in an appropriate molecularbiology department or laboratory in the united states or canada selected by the awardee. in magnitude, the award provides for a total budget of $120,000 (including indirect and overhead costs), spreadover a grant period of 2 years.10.2.2.5.2 the burroughswellcome career awards at the scientific interface10 the burroughswellcome career awards at the scientific interface are intended to foster the early career developmentof researchers with backgrounds in the physical and computational sciences whose work addressesbiological questions and who are dedicated to pursuing a career in academic research.11 prospectiveawardees are expected to have ph.d.level training in a scientific field other than biology and areencouraged to describe potential collaborations with wellestablished investigators working on interface problems of interest.the program provides $500,000 over 5 years to support up to 2 years of advanced postdoctoraltraining and the first 3 years of a faculty appointment. in general, an awardee is expected to accept afaculty position at an institution other than the one supporting the postdoc, a requirement that is likelyto spread the philosophy of interface research embodied in the program more effectively than thepublishing of papers or program descriptions.in addition, the burroughswelcome fund (bwf) requires the facultyhiring institution to make asignificant commitment to the award recipientõs career development, where òsignificant commitmentóis demonstrated by the financial and professional situation offered. tenuretrack faculty appointmentsare strongly preferred, accompanied by salary support and/or support for starting up a laboratory.awardees are required to devote at least 80 percent of their time to researchrelated activities. furthermore, the facultyhiring institution must offer the awardee to take an adjunct appointment in a seconddepartment and name at least one tenured faculty member in a discipline complementary to theawardeeõs primary discipline who is willing to serve as an active collaborator.10.2.2.5.3 keck center for computational and structural biology: the research training program thew.m. keck center for computational and structural biology is an interdisciplinary and interinstitutional organization, including baylor college of medicine, the university of houston, rice university,university of texas health science center, the m.d. anderson cancer center, and university of texasmedical branch at galveston. subareas of focus include computational methods and tools, biomolecularstructure and function, imaging and dynamics, mathematical modeling of biosystems, and medical andgenomic informatics. the faculty include some 130 members, drawn from member institutions, and a10see http://www.bwfund.org/programs/interfaces/careerawardsbackground.html.11a previous burroughswellcome fund (bwf) program, known as institutional awards at the scientific interface, has beendiscontinued. (together with the career awards program, it constituted the bwf interfaces in science effort.) the purpose of theinstitutional awards program was to support u.s. and canadian academic institutions in developing interdisciplinary graduateand postdoctoral training programs for individuals with backgrounds in the physical, computational, or mathematical sciencesto pursue biological questions. for example, pre and postdoctoral fellows at the la jolla consortium and the university ofchicagoõs institute for biophysical dynamics had to propose research projects that required the participation of two mentorsñone from the quantitative sciences and one from the biological sciencesñbefore being awarded financial support. for more onthe institutional awards program, see n.s. sung, j.i. gordon, g.d. rose, e.d. getzoff, s.j. kron, d. mumford, j.n. onuchic, et al.,òscience education: educating future scientists,ó science 301(5639):1485, 2003, available at http://www.bwfund.org/programs/interfaces/institutionalmain.html.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.culture and research infrastructure345few dozen predoctoral and postdoctoral fellows. the keck center was established in 1990 by a$5 million grant from the keck foundation and currently receives more than $20 million annually ingrants, from agencies such as the national institutes of health, the national science foundation, thedepartment of defense, and private sources.the keck centerõs training program, supported by the w.m. keck foundation and the nationallibrary of medicine, seeks to crosstrain new scientists in both computational science and a specializedarea of biology so that they can shed new light on the cellular and molecular basis of biological processes.12 fellowships are supported for research in algorithm development, advanced computationalmethods, biomedicine, crystallography, electron cryomicroscopy and computer reconstruction, genomestudies, imaging and visualization, mathematical modeling of biosystems, medical informatics, neuroscience, protein dynamics and design, robotics applications in molecular biology, and the structure andfunction of biomolecules. the fellowship provides trainees with crosstraining in computational scienceand in biological applications, dual mentorship, and access to cuttingedge facilities.10.2.2.6 faculty retraining in midcareerfaculty training or retraining can augment the above opportunities. in some cases, this meansparticipation in workshops (given release time to allow for this investment), sabbaticals spent learninga new subject, or explicitly switching from one field to another. as a rule, funded release time will benecessary to provide a break from academic constraints and to offer the time and opportunity to seebiological work up close. in some cases, a good way to develop crossdisciplinary expertise is to spenda sabbatical year in the laboratory of a colleague in another discipline.the committee was unable to find programs specifically oriented toward retraining computer scientists to do biological research. however, the national science foundation (nsf) does support the interdisciplinary grants in the mathematical sciences program through its mathematical and physical sciencesdirectorate whose objective is òto enable mathematical scientists to undertake research and study inanother discipline so as to expand their skills and knowledge in areas other than the mathematicalsciences, subsequently apply this knowledge in their research, and enrich the educational experiences andbroaden the career options of their students.ó13 recipients spend a year fulltime (in a 12month period) ina nonmathematical academic science department or in an industrial, commercial, or financial institution,and the outcome is expected to be sufficient familiarity with another discipline on the part of the supported individual òto open opportunities for effective collaboration by the mathematical scientist withresearchers in another discipline.ó applicants must have a tenured or tenuretrack academic appointment,and the proposal must include a coprincipal investigator at the level of dean (or higherlevel universityofficial) at the submitting institution as well as a commitment from the host institution or department thatthe hosted individual will be treated as a regular faculty member within the host unit and that at least onesenior person will be provided who will serve as institutional host.in addition, the national institutes of health (nihõs) national research service awards programfor senior fellows (f33) supports scientists from any field with 7 or more years of postdoctoral researchexperience who wish to make major changes in the direction of their research careers or who wish tobroaden their scientific background by acquiring new research capabilities. in most cases, these awardsare used to support sabbatical experiences for established independent scientists in which they receivetraining to increase their scientific capabilities. such training must be within the scope of biomedical,behavioral, or clinical research and must offer an opportunity for individuals to broaden their scientificbackground or extend their potential for research in healthrelated areas. the maximum annual stipendis considerably lower than senior scientists typically receive, but most awardees find supplements sothat they may obtain their full salaries while pursuing studies in a new field. the guidelines for eligibil12see http://cohesion.rice.edu/centersandinst/keckcenter/training.cfm?docid=2368.13see http://www.nsf.gov/pubs/2001/nsf01115/nsf01115.htm.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.346catalyzing inquiryity specifically do not include previous experience in biomedical research; and thus, computer scientistswould be eligible for such a program.1410.2.3 academic organizationstypically, academic research is conducted in departments or in centers that draw on faculty frommultiple departments. the descriptions of the three departments below are simply illustrative and notexhaustive (no inference should be drawn from the fact that any given department or center is notincluded below):¥cornell university maintains four distinct programs in computational biology, three hosted by aparent discipline. biological sciences, computer science, and mathematics all offer concentrations incomputational biology (the math department calls it òmathematical biologyó). the only standalonedepartment is the department of biological statistics and computational biology (bscb), a part of thecollege of agriculture and life sciences. bscb was originally the department of biometry and biostatistics, and in 2005, it has six tenuretrack faculty (plus two emeritus professors), one nontenuretracklecturer, and four òadjunctó faculty. there are 2 postdoctoral associates, 26 graduate students, and 6570undergraduate students. the department focuses mainly on biological statistics, computational biology, and statistical genomics. research interests of the faculty include statistical genomics, bayesianstatistics, population genetics, epidemiology, modeling, molecular evolution, and experiment design.¥the university of california at santa cruz has a department of biomolecular engineering, aninterdisciplinary department that contains research programs in bioinformatics and experimentalsystems biology, among others. the bioinformatics program was originally administered by thecomputer engineering department. in 2005, the program has nine core tenuretrack faculty members,and one affiliated faculty member. the bioinformatics curriculum includes a core of bioethics, bayesian statistics, molecular biology, biochemistry, computational analysis of proteins, and computational genomics. electives are drawn from biology, chemistry, computer science, and applied mathematics and statistics.¥carnegie mellon university (cmu) has offered programs in computational biology (through itscomputer science, biology, mathematics, physics, and chemistry departments) since 1989. in 2005,cmuõs department of biological sciences had 5 faculty involved in both computational biology andbioinformatics and genomics, proteomics, and systems biology. the department offers a b.s. in computational biology, which consists largely of a traditional biological curriculum augmented with math,programming, and computer science classes. in addition, students (b.s. or ph.d.) can participate in theinterdepartmental merck computational biology and chemistry program, which requires students tohave a home department in biology, computer science, statistics, math, or chemistry. this program wasestablished in 1999 with a grant from the merck company foundation.centers are often created without specific departmental affiliation because the number of departments that might plausibly contribute expertise is large. in these instances, absent a center, it is difficultto unify and coordinate research and educational activities or to convey to the outside world what theuniversity is doing in the area. centers are intended to be focal points for research at the biocompinterface (most often with a bioinformatics or computational biology flavor), and they usually workwith departments to make new faculty appointments and provide a single point for students to learnabout university programs.four universitybased centers are described below, simply as illustrative:14for more information, see http://grants.nih.gov/grants/guide/pafiles/pa00131.html.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.culture and research infrastructure347¥the university of californiaberkeleyõs center for integrative genomics was founded in december 2002, supported by the gordon and betty moore foundation.15 its mission is to bring tools frommany disciplines to bear on problems at the intersection of evolution and developmental biology. theenabling technology for new progress in this field will be acceleration of the sequencing of speciesgenomes, and it is hoped to sequence 100 genomes of various species in the next 5 years.16 the facultyincludes 20 researchers drawn from molecular cellular biology, integrative biology, statistics, plant andmicrobial biology, mathematics, computer science, bioengineering, physics, paleontology, and thelawrence berkeley national laboratory. the center also plans to serve an educational role, teaching orsupporting the teaching of genomic science to computer science students and computer topics to biology students, as well as providing a center for graduate and postgraduate work.¥the vanderbilt institute for integrative biosystem research and education (viibre) at vanderbiltuniversity (nashville, tennessee) was begun with an initial grant from vanderbiltõs academic venturecapital fund.17 viibre has also received projectspecific funding and other support from nsf, darpa,nih, and other institutions, enabling it to create centers of bioengineering education technologies andto begin research in cellular instrumentation and control, biomedical imaging, technologyguidedtherapy, biological applications of nanosystems, cellular and tissue bioengineering and biotechnology,and bioengineering education technologies. engineers, scientists, doctors, and mathematicians conductresearch for viibre; more than 20 biological physics and bioengineering faculty in vanderbiltõs collegeof arts and science and the schools of engineering and medicine participate in the program. viibre isalso developing a postdoctoral training program for physical scientists and engineers who wish todirect their careers toward the interface between biology, medicine, engineering, and the physicalsciences.¥the computational and systems biology initiative (csbi) at the massachusetts institute of technology (mit) is a campuswide education and research program that links biologists, computer scientists, and engineers in a multidisciplinary approach to the systematic analysis of complex biologicalphenomena.18 csbi places equal emphasis on computational and experimental methods and on molecular and systems views of biological function. csbi includes about 80 faculty members from morethan 10 academic units in science, engineering, and management. overall, membership in csbi is selfdetermined, based on a selfidentified interest in systems biology, and it is offered to faculty andprincipal investigators, postdoctoral fellows, graduate students, and research staff.¥the institute for biophysical dynamics at the university of chicago19 is focused on interdisciplinary study of biological entities and is supported by the bwf program of institutional awards at thescientific interface. drawing on the biological and physical science divisions of the university, theinstitute focuses on rnadna structure, function, and regulation; protein dynamics, folding, andengineering; cytoskeleton, membranes, and organelles; hormones and cell signaling; and cell growth,death, and multicellular function. physical scientists at the institute have expertise in macromolecularscale manipulation via optical tweezer and chemical means; biologically relevant model systems; measurement of dynamics of macromolecules and assemblies on scales from femtoseconds to seconds;theoretical and simulation methods; soft condensed matter theory of complex and analysis of nonlineardynamic phenomena. part of the instituteõs mission is to establish crossdisciplinary training programsfor students. the essential feature of the program is the placement, on a competitive basis, of predoctoralfellows with backgrounds in the physical sciences into biological science research groups, thereby15see http://www.moore.org/grantees/grantsummariescontent.asp?grantee=ucbcig.16g. shiffrar, ònew center for integrative genomics to study major evolutionary changes,ó college news; see http://ls.berkeley.edu/new/02/cig.html.17see http://www.vanderbilt.edu/viibre/avgoal.html and http://www.physics.vanderbilt.edu/oldpurplesite/whatshot/newsletterwinter0102.html.18for more information, see http://csbi.mit.edu/whatis.19for more information, see http://ibd.uchicago.edu/.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.348catalyzing inquiryformalizing interdisciplinary connections. fellows participate in new òtranslational core courses,ó establishing a common culture, and select an individualized program of additional coursework tailored totheir research and career goals. they also take a lead role in a weekly seminardiscussion program.finally, in some cases centers are not associated with a specific university at all. their purpose canbe to consolidate resources on a larger scale or simply to provide a congenial intellectual home for likeminded individuals. three nonuniversity centers are described below, again as illustrations only:¥cold spring harbor laboratory (cshl) is a private research institution on long island, newyork, that employs more than 800 people (300 classified as scientists) and has an annual budget of over$120 million. cshl was established in 1889 with missions in biological research and education. in 1993,it began the annual cold spring harbor symposium on quantitative biology. as of 1998, it offers aph.d. program. its prime research focus is on cancer biology, although it also has strong programs inplant genetics, genomics and bioinformatics, and neurobiology. in genomics, its researchers are investigating genome structure, sequencing, pattern recognition, gene expression, prediction of protein structure and function, and other related topics. a large portion of its funding comes from revenue, such aspublications, intellectual property licensing, and events fees.¥the institute for systems biology (isb) is a private nonprofit institution founded in 2000 inseattle, washington, by leroy hood, alan aderem, and ruedi aebersold.20 with a mission of applyingsystems biology to problems of human health such as cancer, diabetes, and diseases of the immunesystem, its 11 faculty members and 170 staff have expertise in fields such as immunity, proteomics,genomics, computer science, biotechnology, and biophysics. since its founding, isb has received itsfunding predominantly from federal grants, although also including private, corporate, and foundationsupport and industrial collaboration.21 isb has also spun out a number of companies to pursue commercialization opportunities around cell sorting and cancer therapies, in addition to cooperating in amultiventure capital firmbacked incubator for new biotechnology startups.22 of particular significance is the report that hood left the university of washington after he failed to convince it to establisha systems biology research center; he later said that he thought òthe university culture and bureaucracyjust could not have sufficient flexibilityó to respond to the opportunity that posthuman genomeproject systems biology presented.23¥the sloanswartz centers for theoretical neurobiology were created in 1994 under the auspicesof the sloan foundation.24 located at brandeis university, california institute of technology, newyork university, salk institute, and university of california, san francisco, the swartz foundation alsomade major grants to these centers in 2000. these centers place experimentalists and theoreticians fromphysics, mathematics, and computer sciences in experimental brain research laboratories, where theylearn about neuroscience and apply their vantage point and nontraditional skills to cooperative lines ofinquiry. the centers have investigated topics such as gain fields and gain control in nerve circuits,neural coding and information theory, neural population coding and response, natural field analysis,and shortterm memory.20see http://www.systemsbiology.org.21l. timmerman, òprogress, not profit: nonprofit biotech research groups grow in size, influence,ó seattle times, august 4,2003.22j. cook, òaccelerator aims to lure, nurture best ideas in biotech,ó seattle postintelligencer, may 23, 2003.23òunder biologyõs hood,ó technology review, september 2001, available at http://www.techreview.com/articles/01/09/qa0901.asp.24see http://www.swartzneuro.org/researcha.asp and http://www.sloan.org/programs/scitechsupresearch.shtml.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.culture and research infrastructure34910.2.4 industryindustrial interest in the biocomp interface is driven by the prospect of potentially very largemarkets in the life sciencesñespecially medicine. informationenabled bioscience is further expected tocreate large markets for information technologies customized and adapted to the needs of life scientistsñaccounting in substantial measure for the interest of some large it companies in this area. indeed,according to the international data corporation, life science organizations will spend an estimated $30billion on technologyrelated purchases in 2006, up from $12 billion in 2001.25life science companies (e.g., pharmaceuticals) view information technology as a (or perhaps the)key enabler for drug design and treatments that can in principle be customized to groups as small as asingle individual. consider, for example, the specific problem of finding useful organic compounds,such as drugs, to treat or reduce the effects of disease. one approach is based on the use of combinatorial methods in chemistry, genetic engineering, and highthroughput screening technology. such anapproach relies on trialanderror to sift candidate compounds on a large scale to sidestep the complexities of data in a search for compounds with sufficient potential to be worth the effort of laboratorytesting for useful outcomes; similar techniques can be used for strain improvement and natural productsynthesis.26a second approach is to use computational modeling and simulation. data mining (section 4.4.8)can be used in addition to empirical screening to identify compounds that are likely to have a desiredpharmacological effect. moreover, what the combinatorial and highthroughput empirical approachgains in expediency, it may lose in insight. for example, causality in combinatorial approaches is oftendifficult to attribute; and thus, it is difficult to generalize these results to other systems. combinatorialmethods are less likely to find solutions when the desired functionality is complex (e.g., when thebiosynthetic route to a product is complicated or when a disease treatment relies on the inhibition,without side effects, of various pathways). also, of course, from the standpoint of basic science, predictive understanding is at a premium. computational simulation is thus used as the screening tool forpromising compoundsña cellõs predicted functional response to a given compound is used as thatcompoundõs measure of promise for further (empirical) testing. thus, although granting drug approvals on the basis of simulations makes little sense, simulations may be able to predict with an adequatedegree of reliability what drugs should not advance to expensive in vivo clinical trials.27 many believethat informationenabled bioscience and biotechnology have the potential to be as revolutionary asinformation technology was a few decades ago.25e. frauenheim, òcomputers replace petri dishes in biological labs,ó cnet news.com, june 2, 2003, available at http://news.com.com/203066793998622.html?tag=fdlede2hed.26see, for example, c. khosla, and r.j. zawada, ògeneration of polyketide libraries via combinatorial biosynthesis,ó trends inbiotechnology 14(9):335341, 1996; c.r. hutchinson, òcombinatorial biosynthesis for new drug discovery,ó current opinion inmicrobiology 1(3):319329, 1998; a.t. bull, a.c. ward, and m. goodfellow, òsearch and discovery strategies for biotechnology:the paradigm shift,ó microbiology in molecular biology review 64(3):573606, 2000; y. xue and d.h. sherman, òbiosynthesis andcombinatorial biosynthesis of pikromycinrelated macrolides in streptomyces venezuelae,ó metabolic engineering 3(1):1526, 2001;and l. rohlin, m. oh, and j.c. liao, òmicrobial pathway engineering for industrial processes: evolution, combinatorial biosynthesis and rational design,ó current opinion in microbiology 4(3):330335, 2001.27for example, the tufts center for the study of drug development estimates the cost of a new prescription drug at $897million, a figure that includes expenses of project failures (e.g., as those drugs tested that fail to prove successful in clinicaltrials). since clinical trialsñoccurring later in the drug pipelineñare the most expensive parts of drug development, theability to screen out drug candidates that are likely to fail in clinical trials would have enormous financial impact and wouldalso reduce the many years associated with clinical trials. see tufts center for the study of drug development news release,òtotal cost to develop a new prescription drug, including cost of postapproval research, is $897 million,ó may 13, 2003,available at http://csdd.tufts.edu/newsevents/recentnews.asp?newsid=29. of particular interest is a finding reported bydimasi that if preclinical screening could increase success rates from the current 21.5 percent to 33 percent, the cost perapproved drug could be reduced by $230 million (j.a. dimasi, òthe value of improving the productivity of the drug development process: faster times and better decisions,ó pharmacoeconomics 20(s3):110, 2002).catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.350catalyzing inquiryas in the case of academic organizations, specific company names provided below are illustrativeand hardly exhaustive, and no inference should be drawn from the fact that any given company is notincluded.10.2.4.1 major it corporationsas a fastgrowing, (comparatively) wellfunded, and highprofile sector, life sciences research andbusiness represents an irresistible target to large it vendors. as such, companies such as hp and ibmhave both developed suites of products and services customized for the consumption of research labs aswell as the biotechnology and pharmaceutical sectors. these services are not necessarily substantiallydifferent from those that vendors provide to other sectorsña disk drive is a disk driveñbut are bundledwith useful software or interfaces designed with the life sciences in mind.ibm established its life sciences business unit in 1998, incorporating hardware, consulting services, and an aggressive alliance program that includes many major vendors of bioinformatics andrelated software. in addition, it provides discoverylink, a customized front end to ibmõs successfuldb/2 relational database product. among other features, discoverylink allows singleapplication viewsand queries into multiple backend databases, providing a convenient answer to a very common situation in bioinformatics, which often deals with many databases simultaneously.of higher profile are ibmõs research activities in computational biology. one of these is blue gene,the architectural successor to deep blue, the ibmdesigned supercomputer that beat chess championgary kasparov in 1997. blue gene, announced in 1999 as a $100 million, 5year project, is projected to be1 petaflop (1015 floating point operations per second), a thousand times more powerful than deep blue,and 30 times more powerful than the nec earthsimulator/5120. blue gene is designed in part to beable to simulate the molecular forces that occur during protein folding, in order to better understandhow a large protein shape emerges from a peptide sequence.28blue gene is only one project, albeit the best known, of ibm researchõs computational biologycenter. this is a group of approximately 35 researchers who are investigating computational techniquesin molecular dynamics, pattern discovery, genome annotation, heterogeneous database techniques, andso forth.hewlettpackard also maintains a life sciences division, and aggressively sells hardware, software,and services to genomics research organizations, pharmaceutical companies, and agribusiness.29 hphas had good success in winning highprofile clients.10.2.4.2 major life science corporationsgenomic bioinformatics, and more generally the use of information technology to support researchand development, has become one of the central pillars of the modern biotechnology industry, especially the pharmaceutical sector. a waveñsome say a boomñof investment in bioinformatics in the late1990s and early 2000s has tapered off, however, due to disappointing returns amid mounting costs.while few in the industry doubt the eventual impact of computational techniques, the more significanteffects may not be felt for years. even in 2002, however, corporate spending in bioinformatics wasestimated to be $1 billion.30the first wave of biotechnology firms, established in the 1970s, has grown into multibillion dollaroperations. these firmsñamgen, biogen, chiron, genentech, and genzymeñwere all founded with28f. allen, g. almasi, w. andreoni, d. beece, b.j. berne, a. bright, j. bruheroto, et al., òblue gene: a vision for protein scienceusing a petaflop supercomputer,ó ibm systems journal 40(2):310327, 2001, available at http://www.research.ibm.com/journal/sj/402/allen.html.29see http://www.hp.com/techservers/lifesciences/overview.html.30see http://www.redherring.com/investor/2002/0419/dealflop.html.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.culture and research infrastructure351the idea of capitalizing on progress in genetic technologies. yet because they predate the bioinformaticsboom, they were often late to the game, catching up by heavy investment or by outright purchasing ofother firms that had organically grown the bioinformatics capability. for example, in december of 2001,amgen announced that it was buying the bioinformaticsrich biotech company immunex corp for $16billion.31 genentech highlights its own bioinformatics capabilities as a key part of the research portfolio.32 however, while these firms and the pharmaceutical giants are clearly great consumers ofbioinformatics software and human resources, it is less clear to what extent they are performing originalcomputational biology research.a second wave of companies was founded in the 1990s, in the era of the human genome projectand the increase in availability of information technology. millennium pharmaceuticals, for example,was founded in 1993 with the goal of being a science and technologydriven pharmaceutical company,with a capability for target discovery based on the human genome information being published. however, most of millenniumõs drugs on the market have come from acquisitions, and the goal of realrational drug discovery remains challenging. millennium does have a highprofile leader in charge ofbioinformatics and uses it for three main functions: bioinformatic inference making, such as identifyinglikely functions of novel proteins or the existence of gene expression patterns that correlate with diseasestates; chemoinformatics, searchable databases of chemical structure and biological activity; and computational analysis to predict drug candidatesõ physiological qualities such as absorption rates, distribution, metabolism, excretion, and toxicity.of higher profile is celera, which craig venter founded in 1998 to compete with the publiclyfunded human genome project. while genomics experts still argue over his methods, he certainlyfound innovative uses for computational and analytic techniques in stitching together the results of hisòshotgunó sequencing method. regardless of its scientific success, however, celera has had little commercial success33 as it turned from sequencing to the potentially more lucrative field of drug discovery.it still makes money by offering access to its proprietary databases to other biotechnology and pharmaceutical companies, but its has given up on its efforts to commercialize its software platform, selling thecelera discovery system to sister company applied biosystems (both celera and applied biosystemsare owned by applera corporation). in addition to the celera discovery system, a subscriptionbaseddatabase, applied biosystems offers an array of software for gene sequencing, laboratory informationmanagement, and gene analysis (as well as a variety of instrumentation and reagents).10.2.4.3 startup and smaller companiesthe area still receives some attention from venture capital firms such as flagship ventures, kleinerperkins caufield byers, atlas ventures, and alloy ventures. however, the emphasis seems to be shifting from bioinformatics to a stronger emphasis on biology, including medical devices and drug discovery. even companies that once positioned themselves as bioinformatics companies now describe themselves as being in the drug discovery business,34 most notably celera but also many smaller companies.for companies that concentrate primarily or exclusively on informatics, times are very difficult, in largepart due to the same sort of bubble collapse as mainstream it faced from 2000 onward.analysts blame overinvestment in the area, leading to more companies than the space can support;companies founded by it players with insufficient biological knowledge; and increasing competitionfrom big players such as ibm and hp.midsize companies such as gene bank and incyte have a similar business model to celera, offeringaccess to proprietary databases, which often contain patented gene sequences. one model that seems to31see http://www.informationweek.com/story/iwk20011221s0038.32see http://www.genentech.com/gene/research/biotechnology/bioinformatics.jsp.33see http://www.fool.com/portfolios/rulebreaker/2002/rulebreaker020423.htm.34see http://www.bizjournals.com/washington/stories/2002/07/08/newscolumn5.html.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.352catalyzing inquirybe more successful than others is òin silicoó simulation of various biological and biomedical processes,such as offered by anvil informatics.35 beyond genomics develops proprietary algorithms that look forlargescale biological systems such as pathways in gene and protein bioinformatics and experimentaldata. a second seemingly successful model is a focus on providing information about pathways andnetworks; ingenuity, cytoscape, genego, pathart, are companies that have sought to exploit this niche.beyond bioinformatics, vendors are attempting to develop or customize for life sciences customersa number of it solutions, including applications for knowledge management, laboratory informationmanagement, and tracking clinical trials (including sophisticated statistical analysis).a leading example of real computer science research being applied to biology problems is theapplication of distributed or grid computing to extremely computationintensive tasks such as proteinfolding simulation. while many it vendors are developing and pushing their grid platform, stanfordhas been running folding@home, a screensaver that anyone can download and run on a home computer, which calculates a tiny piece of the protein folding problem.3610.2.5 funding and supportboth the federal government and private foundations support research at the biocomp interface. (thelatter can be regarded as an offshoot of the historically extensive foundation support for biology research.)10.2.5.1 general considerations10.2.5.1.1 the role of funding institutions funding institutions obviously exert a great deal of controland influence over the nature and direction of research. that is, researchers tend to gravitate towardresearch problems for which funding is available. funding agencies can also influence the developmentof new talent in the field by encouraging faculty development, as illustrated nonexhaustively below:¥release time to design new curricula and collect successful course material. however, as in peerreviewed scientific research, the fruits of these efforts should be made public, and their successes orlimitations should be openly available (e.g., as online courses or published material).¥supervision of undergraduate special projects or research at the biocomp interface. specialprojects for one or a few undergraduates (e.g., summer student projects, undergraduate theses) can beundertaken with minimal risk, and facilitating early exposure to a variety of ideas would benefit bothstudents and faculty.¥support for individuals who wish to make the transition to research at the biocomp interfaceearly in their careers. such individuals may lack the publication track record that would enable moresenior researchers to undertake such a transition. thus, support dedicated to such people may facilitateearly career transitions and all of the accompanying benefits.10.2.5.1.2 the review process a central dimension of funding institutions is the review process theyemploy to decide what research to support. different institutions have different styles, but they all facethe same types of issues.¥excellence. no institution wants to support mediocre research. but as suggested below in section10.3.1, definitions of excellence are in many ways fieldspecific. thus, an effective review process mustfind ways of managing this tension when proposals cross disciplinary lines.35see http://www.anvilinformatics.com.36see http://folding.stanford.edu. perhaps the most famous of such distributed applications is seti@home, a program thatsupports the data processing underlying the search for extraterrestrial life.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.culture and research infrastructure353¥potential impact. all else being equal, institutions would prefer to support research in which thepotential impact of success is large. however, as a rule, claims of large impact are much more speculative than other claims, simply because the longterm ramifications of any given discovery are difficult tounderscore in any convincing manner before the fact.¥technical risk. a research investigation may or may not be successful. research that presents thelowest technical risk (i.e., the lowest risk of failure or of being unsuccessful) is most often very closelytied to some existing and successful research. thus, as a rule, research that is of low technical risk tendsalso to be of lesser potential impact.¥personnel risk. research is performed by people, and any given research effort can be executedmore or less effectively depending on the people involved. established track records of success are animportant dimension of the teams proposed to undertake research but cannot be the only dimensiontaken into account if new researchers with good ideas are to be welcomed.¥budget. institutions with a fixed level of support to offer investigators can support a largernumber of inexpensive research proposals or a smaller number of more expensive ones. all else beingequal, inexpensive proposals will tend to be favored over expensive ones.proposals for research must weigh each of these factors and make tradeoffs among them. forexample, a lower budget may mean greater technical or personnel risk; a highimpact project may havegreater technical risk. funding agencies must assess the plausibility of the tradeoffs that a prospectiveresearch team has made.these notions suggest that review panels need a wide range of expertise and experience to judge themerits of new proposals effectively or to carry out peer review of scientific papers. in principle, therequisite range of expertise can be obtained through the use of a set of individual disciplinary expertswhose collective expertise is adequately broad. an alternative is to use a few individuals who themselves have interdisciplinary expertise. the disadvantage of the first model is that for practical purposesit may reproduce forums in which the difficulties of crossdisciplinary understanding are manifested.the disadvantage of the second model is that such individuals may be few in number and thus difficultto enlist.10.2.5.2 federal supporta variety of federal agencies support work at the biocomp interface, and this support has grownover time.10.2.5.2.1 the national institutes of health for computational biology (i.e., the computingtobiologyside of the biocomp interface), the main actor in the u.s. government is the national institutes ofhealth, part of the department of health and human services.a notable instance of bioinformatics work at nih is the national center for biotechnology information (ncbi), a part of the national library of medicine. established in 1988, it is ncbi that created andmaintains genbank (see chapter 3).the nihõs national institute of general medical sciences (nigms) manages the biomedical information science and technology initiative, or bisti. bisti represents an nihwide collaboration andcoordination program between its many institutes and centers, as computational biology andbioinformatics activity is spread throughout the organization. in addition, nigms also runs the centerfor bioinformatics and computational biology, which focuses on theoretical and methodological infrastructure, such as modeling, simulation, theory, and analysis tools in biological networks.37 the nihõscenter for information technology, in addition to providing it services to the rest of nih, also main37see http://www.nigms.nih.gov/news/releases/cbcb.html.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.354catalyzing inquirytains the division of computational bioscience, which includes activities in highperformance computing and molecular modeling; it is staffed mostly by computer scientists rather than biologists andappears to focus on the computer science aspects of problems.in addition, the national center for research resources (ncrr) is a center within nih whosemission is to create new research technologies and provide researchers access to resources such as highend instrumentation, animal models, and cell line repositories. in fy 2004, it had a budget of slightlyover a billion dollars, in large part dedicated to funding research centers, as well as individualpredoctoral, postdoctoral, and career awards. ncrrõs 20042008 strategic plan includes a number ofcomputational biology activities within its funding programs. this includes support for software andalgorithm development, mathematical modeling, and simulation. ncrr, through its research infrastructure division, also supports the creation of networks to promote crossinstitutional collaboration,including virtual laboratories and shared databases for a variety of specific clinical research programs.this includes the biomedical informatics research network (birn), an internet2 project first funded in2002 and slated to expand in 2004. ncrr also supports crossdiscipline training at all levels of aresearcherõs careerñfor example, supporting the entry into biology of individuals with backgrounds intechnical fields such as computer science, and retraining established researchers in appropriate fields.the national institute for biomedical imaging and bioengineering (nibib) is the newest institute atnih, and is unusual for its mission of assessing and developing technological capabilities for health andmedical research. its research goals and portfolio include support for a number of activities at thebiocomp interface, including bioinformatics, simulation and computational modeling, image processing, braincomputer interfaces, and telemedicine. more broadly, its support for interdisciplinary training and research that draw on engineering, as well as physical and life sciences, mark it as anotherinstrument for encouraging the development of researchers and scientists having experience with andexposure to computational science.in addition to these institutional entities, nih has created a set of programmatic initiatives to promotequantitative, interdisciplinary approaches to biomedical problems that involve the complex, interactivebehavior of many components.38 one initiative consists of a variety of programs to develop humancapital, including those for predoctoral training for life scientists in bioinformatics and computationalbiology,39 support for short courses on mathematical and statistical tools for the study of complex phenotypes and complex systems,40 postdoctoral fellowships in quantitative biology,41 and support for a periodof supervised study and research for professionals with quantitative scientific and engineering backgrounds outside of biology or medicine who have the potential to integrate their expertise with biomedicine and develop into productive investigators.42 the national library of medicine supported awards forpredoctoral and postdoctoral training programs in informatics research oriented toward the life sciences(originally medical informatics but moving toward biomedical informatics in its later years).43a second group of programs is targeted toward specific problems involving complex biomedicalsystems. this group includes an r01 program focused on genetic architecture, biological variation, andcomplex phenotypes (including human diseases);44 another on quantitative approaches to the analysisof complex biological systems, with a special focus on research areas in which systems approaches arelikely to result in the determination of the systemorganizing principles and/or the system dynamics;45and still another on evolutionary mechanisms in infectious diseases.4638see http://www.nigms.nih.gov/funding/complexsystems.html.39see http://grants.nih.gov/grants/guide/pafiles/par99146.html.40see http://grants.nih.gov/grants/guide/pafiles/pa98083.html.41see http://grants.nih.gov/grants/guide/pafiles/pa98082.html.42see http://grants.nih.gov/grants/guide/pafiles/pa02127.html.43see http://grants.nih.gov/grants/guide/rfafiles/rfalm01001.html.44see http://grants.nih.gov/grants/guide/pafiles/pa02110.html.45see http://grants.nih.gov/grants/guide/pafiles/pa98077.html. this program includes p01 program project awards as well.46see http://grants.nih.gov/grants/guide/pafiles/pa02113.html. this program includes p01 program project awards as well.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.culture and research infrastructure355a third group of programs is institutional in nature. one program establishes new academic centers of excellence in complex biomedical systems research47 that promote the analysis of the organization and dynamic behaviors of complex biological systems through the development of multiinvestigator teams capable of engaging biomedical complexity with a scope of activities not possible with otherfunding mechanisms, including research, training, workshops, symposia, and other forms of outreach.typical areas of interest include computationally based modeling of processes such as the cell cycle;pattern formation during embryogenesis; the flux of substrates and intermediates in metabolism; andthe application of network analysis to understanding the integrated systemic host responses to trauma,burn, or other injury. a second program on integrative and collaborative approaches to research48encourages collaborative and integrative approaches to research on multifaceted biological problemsfor individual investigators with existing support who need to attract and coordinate expertise indifferent disciplines and approaches and require access to specialized resources, such as computationalfacilities, highthroughput technologies, and equipment. a third program49 supports new quantitativeapproaches to the study of complex, fundamental biological processes by encouraging nontraditionalcollaborations across disciplinary lines through supplements to existing r01, r37, or p01 nigms grantsto support the salary and expenses of collaborating investigators such as physicists, engineers, mathematicians, and other experts with quantitative skills relevant to the analysis of complex systems.finally, a major contributor to research that includes biology and computation is the nih roadmap.the roadmap is a broad set of funding opportunities and programs dealing with research issues that,due to their complexity, scope, or interdisciplinary nature, could not be addressed adequately by asingle nih institute or center. relevant biocomp programs described by the roadmap include molecular libraries, which in part seek to develop large databases of òsmall molecules,ó and structural biology,which includes research to develop algorithmic tools for analyzing and predicting protein structure.the most significant biocomp initiative within the roadmap, however, is the bioinformatics andcomputational biology program. this program seeks to create and support a national program ofexcellence in biomedical computing (npebc), a national network of software engineering and gridresources to support cuttingedge biomedical research. the prime components of the npebc are thenational centers for biomedical computing (ncbcs), seven 5year u54 grants that total approximately$120 million, along with a larger number of r01 and r21 individual grants to support collaborationopportunities with the ncbcs.the ncbcs are intended as more than merely wellfunded research centers; their missions oftraining, tool creation and dissemination, community support, and liberal intellectual property policiesfor software and data are designed to create national networks and communities of researchers organized around biocomputational research. the structure of the grant process required the identificationof three different research thrusts (or òcoresó): a core of computational research, responsible for performing original work in algorithms and computer science; a core of biomedical research, or òdrivingbiological projects,ó and a core biocomputing engineering, responsible for both interfacing betweencomputation and biomedical research, and creating the concrete tools and software systems to actualizethe research.the recipients of the first round of ncbc funding were announced in september of 2004, coveringfour centers. the second round, expected to fund an additional three centers, will be announced in 2005.the centers funded in the first round include:¥the stanford center for physicsbased simulation of biological structures, an effort that seeks tocreate common software and algorithmic representation for modeling and simulation, addressing prob47see http://grants.nih.gov/grants/guide/rfafiles/rfagm03009.html.48see http://grants.nih.gov/grants/guide/pafiles/pa00099.html.49see http://grants.nih.gov/grants/guide/pafiles/pa98024.html.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.356catalyzing inquirylems of how to integrate models that may have widely different physical scales, have discrete orcontinuous approximations, or work at very different levels of abstraction. the driving biological problems for this center include rna folding, myosin dynamics, neuromuscular dynamics, and cardiovascular mechanics.¥the national alliance for medical image computing (namic) is a center based at brigham andwomenõs hospital in boston that includes partners from universities and research centers around thecountry. the goal of namic is to develop computational tools for analysis and visualization of imagedata, especially in integrating data from many different imaging technologies (e.g., magnetic resonanceimaging, electroencephalography, positron emission tomography, etc.) with genomic and clinical data.the initial driving biological projects for namic are various forms of neurological abnormality associated with schizophrenia.¥the center for computational biology at ucla is also investigating questions of imaging, concentrating on the production of òcomputational atlases,ó databaselike structures that allow sophisticated queries of largescale data. the computational research includes mathematics of volumes andgeometry, and the driving biological projects are language development, alzheimerõs, multiple sclerosis, and schizophrenia.¥the center for informatics for integrating biology and the bedside (i2b2), organized by a consortium of bostonarea universities, hospitals, and medical insurance providers, seeks to develop techniques to integrate and present huge sets of clinical data in ways appropriate for research into thegenetic bases of disease and, thus, helping to identify appropriate targeted therapies for individualpatients. this involves the development of statistical and algorithmic techniques for analyzing proteinstructure, as well as population dynamics. the driving biological projects include airways diseases suchas asthma, hypertension, huntingtonõs disease, and diabetes.10.2.5.2.2 the national science foundation the national science foundation provides a great deal ofsupport for research at the biocomp interface through its programs of individual and institutionalgrants. the nsfõs directorate of biological sciences (bio) formerly offered a funding program in computational biology activities. the bio directorate ended this program in 1999,50 not because the research no longer deserved funding, but because computational biology had òmainstreamedó to becomean important part of many other biological research activities, particularly environmental biology,integrative biology, and molecular and cellular biosciences. nsf does, in its biological infrastructuredivision, maintain a biological databases and informatics program that funds direct research into thecreation of tools and datasets.in its 2003 report science and engineering infrastructure for the 21st century: the role of the nationalscience foundation, nsf concludes that its support for science and engineering infrastructure(cyberinfrastructure), in which it includes nextgeneration computational tools and data analysis andinterpretation toolkits (along with a great deal of other infrastructure elements), should increase from 22percent of its total budget to 27 percent; it also recommends strengthening its support for crossdisciplinary fields of research. both of these recommendations are likely to improve the funding climate forcomputational biology and bioinformatics, although of course they will still be competing with a number of other important infrastructure programs.many existing nsf funding programs emphasize interdisciplinary research and thus are effectivevehicles for supporting biocomp research, although not exclusively. for example, the integrative graduate education and research traineeship (igert) program offers 5year, $3 million grants to universitiesto support interdisciplinary graduate student training.51 many of the existing programs funded byigert work at the biocomp interface, such as bioinformatics, computational neuroscience, computa50see http://www.nsf.gov/pubs/1999/nsf99162/nsf99162.htm.51see http://www.nsf.gov/pubs/2005/nsf05517/nsf05517.htm.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.culture and research infrastructure357tional phylogenetics, functional genomics, and so forth. within biology, the frontiers in integrativebiological research (fibr) program is designed to fund research projects using innovative approachesthat draw on many fields, including information sciences, to attack major unanswered questions inbiology.52 it funds projects for five years at $1 million per year, and the 2005 round will fund eightprojects. also, a funding program for postdoctoral training in bioinformatics is funded at $1 million.53a central and challenging application in biocomp research is an attempt to construct the entirehistoric phylogenetic tree of life. nsf is supporting this research through its assembling the tree oflife program, funded at $29 million; databases will contain molecular, morphological, and physiological evidence for placing taxa in relationship to other taxa. current algorithms and data structures do notscale well at the number of taxa and data points necessary, so both computational and biologicalresearch is necessary to achieve this grand challenge.the nsf participates with other government agencies in coordinating research agendas and programs. of particular note is the joint initiative between the nsf directorate for mathematics andphysical sciences and nigms to support research in mathematical biology.54 work supported underthis initiative is expected to impact biology and advance mathematics or statistics, and the competitionis designed to encourage new collaborations between the appropriate mathematical and biologicalscientists as well as to support existing ones. the office of science and technology policy (ostp)included research into òmolecularlevel understanding of life processesó in a list of the governmentõstop priorities for science and engineering research.55 nsf is supporting this goal through its careerfunding program, which is aimed at faculty members early in their careers.56finally, nsf sponsors a small grants exploratory research program that supports highrisk research on a small scale. according to nsf, proposals eligible for support under this program must be foròsmallscale, exploratory, highrisk research in the fields of science, engineering and education normally supported by nsf may be submitted to individual programs. such research is characterized aspreliminary work on untested and novel ideas; ventures into emerging research ideas; application ofnew expertise or new approaches to ôestablishedõ research topics; efforts having a severe urgency withregard to availability of, or access to data, facilities, or specialized equipment, including quickresponseresearch on natural disasters and similar unanticipated events; or efforts of a similar character likely tocatalyze rapid and innovative advances.ó57 typically, grants provided under this program are less than$200,000.10.2.5.2.3 department of energy the department of energy played a key role in the initiation of thehuman genome project. its scientific interest was first motivated by a need to understand the biologicaleffects of ionizing radiation, which it viewed as part of the science mission surrounding its stewardshipof the nationõs nuclear weapons program. furthermore, doe scientists have had considerable experience with advanced computation in the design and manufacturing process for nuclear weapons, a factthat doe leveraged to investigate the genome.today, the department of energy is a major supporter of 21st century biology, because it believesthat biological approaches may help it to meet its missions of energy production, global climate changemitigation, and environmental cleanup.¥for energy production, renewable energy from plants requires the design of plants with biomassthat can be transformed efficiently to fuels. however, a limiting factor in developing such plants is the52see http://www.nsf.gov/pubs/2004/nsf04596/nsf04596.htm.53see http://www.nsf.gov/pubs/2004/nsf04539/nsf04539.html.54see http://www.nsf.gov/pubs/2002/nsf02125/nsf02125.htm.55see fy 2004 interagency research and development priorities, http://www.ostp.gov/html/ombguidmemo.pdf.56see http://www.nsf.gov/pubs/2002/nsf02111/nsf02111.htm.57see http://www.nsf.gov/pubs/2004/nsf042/dcletter.htm.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.358catalyzing inquirylack of understanding about their metabolic pathways, and knowledge of these pathways may lead tomore efficient strategies for converting biomass to fuels.¥for mitigating climate change, reduction in the buildup of greenhouse gases (specifically co2)would be desirable. one approach to this problem is to alter natural biological cycles to store extracarbon in the terrestrial biomass, soils, and biomass that sinks to ocean depthsña sequestration approach. research continues on the best ways to achieve largescale carbon sequestration, and onemethod under investigation is tied to microbial metabolism and activities that may lead to new ways tostore and monitor carbon.¥for environmental cleanup, microbes may provide a means to degrade or immobilize contaminants and accelerate the development of new, less costly strategies for cleaning up a variety of doewaste sites. for example, microbes may be developed that can consume waste materials and degradethem or concentrate them in a form that is easier to clean up.to address these missions, doe supports a number of programs. perhaps the best known is thegenomestolife (gtl) program, a large research grantproviding program with four major scientificgoals: (1) identification of systems of interacting proteins at the microbial level (òprotein machinesó), (2)characterization of gene regulatory networks, (3) exploration of microbial communities and ecosystems,and (4) development of the computational capability for modeling biological systems. to pursue thesegoals, the gtl program combines large experimental datasets with advanced data management, analysis, and computational simulations to create predictive simulation models of microbial function and ofthe protein machines and pathways that embody those behaviors. the program identifies specificchallenges for computer science:58 automated gene annotation; software to support protein expressionproteomics analysis; the ability to meaningfully and automatically extract meaning from biologicaltechnical papers; simulation for cellular networks; and model and system interoperability. these willrequire advances in data representation, analysis tools, integration methods, visualization techniques,models, standards, and databases. the program has funded five major projects (three at doe labs andtwo at academic institutions) for a total of $103 million over the period from 2002 to 2007. in the projectdescriptions of the winners, four included òcomputational modelsó as part of their charge.59a second doe effort is the microbial genome program, which spun off from the human genomeproject in 1994. the microbial genome program exploits modern sequencing technologies to sequencecompletely the genomes of microbes, primarily prokaryotes, based on their relevance for energy, theglobal carbon cycle, and bioremediation. as of april 2003, the genomes of about 100 microbes had beensequenced, most of them by the joint genome institute,60 and placed in public databases. microbialgenomics presents some particularly interesting science in that for newly sequenced microbial genomes, a large fraction of the genes identified (about 40 percent) have unknown functions and biological value. in addition, most of what is known about microbes involves microbes that are easy to cultureand study or that cause serious human and animal diseases. these constitute only a small minority of allmicrobes living in natural environments. most microbes are part of communities that are very difficultto study but play critical roles in earthõs ecology, and a genomic approach to understanding thesemicrobes may be one of the only paths toward developing an understanding of them.a third component of doeõs efforts is in structural biology. the purpose of this program is tounderstand the function of proteins and protein complexes that are key to the recognition and repair ofdna damage and the bioremediation of environmental contamination by metals and radionuclides.research supported in this program focuses on determining the highresolution threedimensional58see http://www.doegenomestolife.org/pubs/computerscience10execsumm.pdf.59see http://doegenomestolife.org/research/2002awards.htm.60the joint genome institute, established in 1997, is a consortium of scientists, engineers, and support staff from doeõslawrence berkeley, lawrence livermore, and los alamos national laboratories. see http://www.jgi.doe.gov/whoweare/index.html.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.culture and research infrastructure359structures of key proteins; understanding the changes in protein structure related to interaction withmolecules such as dna, metals, and organic ligands; visualization of multiprotein complexes that areessential to understand dna repair and bioremediation; prediction of protein structure and functionfrom sequence information, and modeling of the molecular complexes formed by proteinprotein orproteinnucleic acid interactions.10.2.5.2.4 defense advanced research projects agency with a reputation for engaging in òhighrisk,highreturnó research, darpa has been a key player in the development of applications that utilizebiomolecules as information processing, sensing, or structural components in anticipation of reachingthe limits of mooreõs law. this research area, largely supported under darpaõs biocomputation program,61 was described in section 8.4. managed out of darpaõs information processing technologyoffice (ipto), the biocomputation program has also supported the biospice program, a computationalframework with analytical and modeling tools that can be used to predict and control cellular processes(described in chapter 5 (box 5.7)). finally, the biocomputation program has supported work in synthetic biology (i.e., the design and fabrication of biological components and systems that do not alreadyexist in the natural world) as well as the redesign and fabrication of existing biological systems (described in section 8.4.2.2).ipto also supports a number of programs that seek to develop information technology that embodies certain biological characteristics.62 these programs have included the following:¥software for distributed robotics, to develop and demonstrate techniques to safely control, coordinate, and manage large systems of autonomous software agents. a key problem is to determine effective strategies for achieving the benefits of agentbased systems, while ensuring that selforganizingagent systems will maintain acceptable performance and security protections.¥mobile autonomous robot software, to develop the software technologies needed for controlling theautonomous operation of singly autonomous, mobile robots in partially known, changing, and unpredictable environments. in this program, ideas from robot learning and control are extended, includingsoft computing, robot shaping, and imitation.¥taskable agent software kit, to codify agent design methodology as a suite of control and decisionmechanisms, to devise metrics that characterize the conditions and domain features that indicate appropriate design solutions, and to explain and formalize the notion of emergent behavior.¥selfregenerative systems, to develop core technologies necessary for making computational systems able to continue operation in the face of attacks, damage, or errors. specific avenues of investigation include biological metaphors of diversity, such as mechanisms to automatically generate a largenumber of different implementations of a given function that most of them will not share a given flaw;immune systems; and human cognitive models.¥biologically inspired cognitive architectures, to codify a set of theories, design principles, and architectures of human cognition that are specifically grounded in psychology and neurobiology. althoughimplementation of such models on computers is beyond the scope of the current project, it is a naturalextension once sufficiently complete models can be created.darpaõs defense sciences office (dso) supports a variety of programs that connect biology tocomputing in the broad sense in which this report uses the term. these programs have included thefollowing:61see http://www.darpa.mil/ipto/programs/biocomp/index.htm.62see http://www.darpa.mil/ipto/programs/programs.htm.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.360catalyzing inquiry¥bio:info:micro. in collaboration with dso, ipto and the microsystems technology office, thebio:info:micro program supports research in neuroprocessing and biological regulatory networks. theseresearch thrusts seek to develop devices for interrogating and manipulating living brains and brainslices (in the neuroprocessing track) and single cells or components thereof (in the regulatory networktrack), and the computational tools needed to analyze and interpret information derived from thesedevices. thus, neural decoding algorithms for neural spikes and local field potentials, and methods forrepresenting spatial components in distributed systems and using decision theoretic approaches fordecoding brain signals are of interest to the neuroprocessor track, and algorithms that can automaticallydetect patterns and networks given appropriate data and models for networks that govern cell growthand death are of interest to the regulatory track.¥biological input/output systems. focused on the design and assembly of molecular components andpathways that can be used to sense and report the presence of chemical or biological analytes, thisprogram seeks to develop technologies to enable the facile engineering and assembly of functionalbiological circuits and pathways in living organisms, thereby enabling such organisms to serve asremote sentinels for those analytes. the essential notion is that the binding of an analyte to an engineered cytoplasmic or cell surface receptor will lead to regulated and specific changes in an organism,which might then be observed by imaging, spectroscopy, or dna analysis.¥simulation of biomolecular microsystems. biological or chemical microsystems in which biomolecularsensors are integrated with electronic processing elements offer the potential for significant improvements in the speed, sensitivity, specificity, efficiency, and affordability of such systems. this programseeks to develop data, models, and algorithms for the analysis of molecular recognition processes;transduction of molecular recognition signals into measurable optical, electrical, and mechanical signals; and onchip fluidicmolecular transport phenomena. the ultimate goal is to produce advancedcomputeraided design (cad) tools for routine analysis and design of integrated biomolecularmicrosystems.¥engineered biomolecular nanodevices and systems. this program is focused on hybrid (bioticabiotic)nanoscale interface technologies that enable direct, realtime conversion of biomolecular signals intoelectrical signals. success in this area would enable engineered systems to exploit the high sensorysensitivity, selectivity, and efficiency that characterize many biological processes. the objective of thisresearch is to develop hybrid biomolecular devices and systems that use biological units (e.g., proteinion channels or nanopores, gproteincoupled receptors) for performing a sensing function but usesilicon circuitry to accomplish the signal processing. ultimately, this research is intended to lay thefoundation for advanced òbiologytodigitaló converter systems that enable direct, realtime conversionof biological signals into digital information.¥biologically inspired multifunctional dynamic robots. this program seeks to exploit biological approaches to propulsion mechanisms for multifunctional, dynamic, energyefficient, and autonomousrobotic locomotion (e.g., running over multiple terrains, climbing trees, jumping and leaping, graspingand digging); recognition and navigation mechanisms that enable biological organisms to performterrain following, grazing incidence landings, target location and tracking, plume tracing, and hive andswarm behavior; and the integration of these capabilities into demonstration robotic platforms.¥compact hybrid actuators program. this program seeks to develop electromechanical andchemomechanical actuators that perform the same functions for engineered systems that muscle performs for animals. the performance goal is that these new actuators must exceed the specific power andpower density of traditional electromagnetic and hydraulicbased actuation systems by a factor of 10.¥active biological warfare sensors. this program seeks to develop technology to place living cellswith similar behavior to human cells onto chips, so that their health and behavior can be monitored forthe presence of harmful chemical or biological agents.¥protein design processes. this program is using two specific challenge problems to motivate research into technologies for designing novel proteins for specific biological purposes. such design willrequire advances in computational models, as well as knowledge of molecular biology. the challengecatalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.culture and research infrastructure361problems include tasks of designing specific proteins in less than a day that can catalyze specificchemicals or inactivate an influenza virus.as a vehicle for pursuing its mission, darpa typically uses broad agency announcements. someare focused on achieving a specific technical capability (e.g., a program that will develop technology forsynthesizing, within 24 hours, an arbitrary 10,000oligonucleotide sequence in quantity), whereas others are more broadly cast. in many cases, these programs target private industry as well as the moreengineeringoriented academic institutions.10.3 barriersbecause work at the biocomp interface draws on different disciplines, there are barriers to effectivecooperation between practitioners from each field. (in some cases, òeach fieldó is more properly cast asthe contrast between practitioners of systems biology and practitioners of empirical or experimentalbiology.) this section describes some of these barriers.6310.3.1 differences in intellectual styleit is almost axiomatic that substantial progress in any area of intellectual inquiry depends on theexcellence of work undertaken in that area. on the other hand, differences in intellectual style will affectwhat is regarded as excellence, and computer scientists and biologists often have very different intellectual styles.the existence of shared intellectual styles tends to increase the mutual understanding of colleaguesworking within their home disciplines, a fact that leads to more efficient communication and to sharedepistemological understanding and commitments. however, when working across disciplines, lack of ashared intellectual style increases the difficulties for both parties in making meaningful progress.what are some of the differences involved? while it is risky (indeed, foolhardy) to assert hard andfast differences between the disciplines, an examination of the intellectual traditions and historiesassociated with each discipline suggests that practitioners in each are generally socialized and educated with different styles.64 over time, these differences may moderate as biology becomes a morequantitative discipline (indeed, a premise of this report is that such evolution is to be encouraged andfacilitated).10.3.1.1 historical origins and intellectual traditionsmany differences in intellectual style between the two fields originate in their histories.65 computerscience results from a marriage between mathematics and electrical engineeringñalthough it hasevolved far from these beginnings. the mathematical thread of computer science is based on formalproblem statements, formulating hypotheses (conjectures) based on those statements, and generatingformally correct proofs of those hypotheses. most importantly, a single counterexample to a conjectureinvalidates the conjecture. note also that formal proofs often entail problems that are far from reality,because many real problems are simply too complex to be represented as formal problem statementsthat are at all comprehensible. research in mathematics (specifically, applied mathematics) often con63an early perspective on some of these barriers can be found in k.a. frenkel, òthe human genome project and informatics:a monumental scientific adventure,ó communications of the acm 34:4051, 1991.64an interesting ethnographic account of life in an academic biology laboratory is provided in j. owensmith, òmanaginglaboratory work through skepticism: processes of evaluation and control,ó american sociological review 66(3):427452, 2001.65some of this discussion is inspired by g. wiederhold, òscience in two domains,ó stanford university, march 2002, updatedfebruary 2003. unpublished manuscript.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.362catalyzing inquirysists of finding solutions to abstractly formulated problems and then finding realworld problems towhich these solutions are applicable.the engineering thread of computer science is based on finding useful and realizable solutions torealworld problems. the space of possible solutions is usually vast and involves different architecturesand approaches to solving a given problem. problems are generally simplified so that only the mostimportant aspects are addressed. economic, human, and organizational factors are at least as importantas technological ones, and tradeoffs among alternatives to decide on the òbestó approach to solve a(simplified) problem often involve art as much as science.biologyñthe study of living thingsñhas an intellectual tradition grounded in observation andexperiment. because biological insight has often been found in apparently insignificant information,biologists have come to place great value on data collection and analysis. in contrast to the theoreticalcomputer scientistõs idea of formal proof, biologists and other life scientists rely on empirical work totest hypotheses.because accommodating a large number of independent variables in an experiment is expensive, acommon experimental approach (e.g., in medicine and pharmaceuticals) is to rely on randomizedobservations to eliminate or reduce the effect of variables that have not explicitly been represented inthe model underlying the experiment. subsequent experimental work then seeks to replicate the resultsof such experiments.a biological hypothesis is regarded as òprovenó or òvalidatedó when multiple experiments indicatethat the result is highly unlikely to be due to random factors. in this context, the term òprovenó issomewhat misleading, as there is always some chance that the effect found is a random event. ahypothesis òvalidatedó by experimental or empirical work is one that is regarded as sufficiently reliableas a foundation for most types of subsequent work. generalization occurs when researchers seek toextend the study to other conditions, or when investigation is undertaken in a new environment or withmore realism. under these circumstances, the researcher is investigating whether the original hypothesis (or some modification thereof) is more broadly applicable.within the biological community (indeed, for researchers in any science that relies on experiment),repetition of an experiment is usually the only way to validate or generalize a finding, and replicationplays a central role in the conduct of biological science. by contrast, reproducing the proof of a theoremis done by mathematicians and computer scientists mostly when a prior result is suspicious. althoughthere is an honored tradition of seeking alternative proofs of theorems even if the original proof is not atall suspicious, replication of results is not nearly as central to mathematics as it is to biology.finally, biology is constrained by nature, which makes rules (even if they are not known a priori tohumans), and models of biological phenomena must be consistent with the constraints that those rulesimply. by contrast, computer science is a science of the artificialñmore like a game in which one canmake up oneõs own rulesñand the only òhardó constraints are those imposed by mathematical logicand consistency (hence data for most computer scientists have a very different ontological role than forbiologists).10.3.1.2 different approaches to education and trainingthe first introduction to computer science for many individuals involves building a computerprogram. the first introduction to biology for many individuals is to watch an organism grow (remember growing seeds in dixie cups in grade school?). these differences continue in different trainingemphases for practitioners in computer science and biology in their undergraduate and graduate work.to characterize these different emphases in broad strokes, formal training in computer sciencetends to emphasize theory, abstractions, problem solving, and formalism over experimental work (indeed, computer programmingñcore to the fieldñis itself an abstraction). moreover, as with manymathematically oriented disciplines, much of the intellectual content of computer science is integratedand, in that sense, cumulative. by contrast, data and experimental technique play a much more centralcatalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.culture and research infrastructure363role in a biologistõs education. traditionally, mathematics (apart from statistics) is not particularlyimportant to biology education; indeed many biologists have entered the field because they wish topursue science that does not involve a great deal of math. although there is a common core of knowledge among most biologists, there is an enormous amount of highly specialized knowledge that is nottightly integrated.a second issue, often encountered in conversion programs, is the difficulty of expanding oneõshorizons to choose intellectual approaches or tools appropriate to the nature of the problem. disciplinary training in any field entails exposure to the tools and approaches of that field, which may not be thebest techniques for addressing problems in another field. thus, successful researchers and practitioners at the biocomp interface must be willing to approach problems with a wide array of methodologies and problemsolving techniques. computer scientists often may be specialists in some specificmethodology, but biological research often requires the coordination of multiple approaches. conversely, biological labs or groups that address a wide range of questions may be more hospitable tocomputational researchers, because they may provide more opportunities in which computationalexpertise is relevant.10.3.1.3 the role of theorytheory plays a very different role and has a very different status in the two fields. for computerscientists, theoretical computer science is essentially mathematics, with all of the associated rigor, certainty, and difficulties. of particular interest in theoretical computer science is the topic of algorithmiccomplexity. the most important practical results from algorithmic complexity indicate the scaling relationships between how long it takes to solve a problem and the size of the problem when its solution isbased on a specific algorithm. thus, algorithm a might solve a problem in a time of order n2, whichmeans that a problem that is 3 times as large would take 32 = 9 times as long to solve, whereas a fasteralgorithm b might solve the same problem in time of order n log n (that is, o(n log n)), which meansthat a problem 3 times as large would take 3 log 3 = 3.29 times as long to solve. (a specific example isthat when asked to write a program to sort a list of numbers in ascending order, one of the mostcommon programs written by novice programmers involves an o(n2) algorithm. it takes a somewhatgreater degree of algorithmic sophistication to write a program that exhibits o(n log n) behaviorñwhich can be proven to the best that is possible.)such results are important to algorithm design, and all computer programs embody algorithms.depending on the functional relationship between run time and problem size, a given program thatworks well on a small set of test data mayñor may notñwork well (i.e., run in a reasonable time) for alarger set of real data. theoretical computer science thus imposes constraints on real programs thatsoftware developers ignore at their own peril.computer scientists and mathematicians derive satisfaction and pleasure from elegance of reasoning, logic, and structure. being able to explain a phenomenon or account for a dynamical behavior witha simple model is highly valued. the reason for this penchant is clear: the simpler the model, the morelikely it is that the tools of analysis can be used to dissect and understand the model fully.this sometimes means that a tendency to oversimplify overwhelms the need for preserving realisticfeatures, to the dissatisfaction or derision of biologists. computer scientists, of course, may well perceive a biologistõs dissatisfaction as a lack of analytical or theoretical sophistication and an unwillingness to be rigorous, and often fail to recognize the complexity inherent in biological systems. in othercases, the love of elegance leads to fixation with elegant, but irrelevant, models far beyond their valueoutside the field, simply because the inherent model is clean and simple. in still other cases, the lack oftraining of computer scientists in eliciting from users the precise nature of their problems has ledcomputer scientists to develop good solutions to problems that are not interesting to most biologists orrelevant to real biological phenomena.by contrast, manyñperhaps mostñbiologists today have a deep skepticism about theory andcatalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.364catalyzing inquirybox 10.5some examples of oversimplified and/or misleading computational andmathematical models in biology¥the turing reactiondiffusion theory for pattern formation in developmental biologyñfirst suggested by turing in 1952, and largely dormant until the mid1970s, this theory, based on an activatorinhibitor system, became a focus of partial differential equations research. initially, attempts were made to show that diffusion andreaction of the activatorinhibitor type are responsible for the development of real structures in real embryos(stripes or spots, positions of limbs and digits, etc.) however, later work has shown that the biological solution tothe pattern formation problem is inelegant and òkludgyó, with many òredundantó or òinefficientó parts.1¥a senior computer scientist faced the issue of how one might infer the structure of a genetic regulatorynetwork from data on the presence or absence of transcription factors. in a cell, a set of genes interact toproduce a proteinñand the transcription factors (themselves proteins) influence the rate at which that proteinis produced. his initial model of this network was a boolean circuit, in which the presence or absence ofcertain factors led to the production of the protein. a typical experimental procedure in a biology lab to probethe nature of this circuit is to observe its behavior by inhibiting the production of some transcription factor andto observe whether or not the protein is produced. the analogous action in the boolean circuit would becutting a wire in that circuit. however, this simple analogy failed to model the actual behavior of the biological system because, in many cases, the inhibition of one transcription factor results in another set of proteinsthat do the same job. thus, the notion of simple perturbation experiments that can be viewed as analogous tojust snipping a wire in a logic circuit is obvious for computer scientistsñbut turns out to be not particularlyrelevant to this particular phenomenon.¥the problem of genome sequence assembly involves piecing together a large number of short sequences(fragments) into the correct master sequence. the initial computer scientist formulation of this problem was tofind the shortest sequence that would contain a given set of sequences as a consecutive piece. but thisformulation of the problem was completely wrong for two reasons. first, the available information on thefragments is sometime erroneousñthat is, the data might indicate that a fragment would have a certain baseat a given location, but in reality it would have a different base at that location. second, dna molecules havea great deal of repeated structure (i.e., the same sequence is typically found multiple times). thus, the shortestsequence is not biologically plausible because that repeated structure is ignored.¥amino acids are represented by codons (i.e., triplets of nucleotide bases). because there are 4 nucleotides,the number of possible codons is 43, or 64. but for a long time, only 20 amino acids were known that occurin nature. it turns out that by assuming that the codons overlapped each other and requiring that the coding beunambiguous, only 20 codons are possible. because of this match, a natural assumption was that an overlapping code was operative in dna coding. however, experimental data dispelled this notion, indicating insteadthat multiple codons can represent the same amino acid and further that the codons were not overlapping.1see, for example, g. von dassow, e. meir, e.m. munro, and g.m. odell, òthe segment polarity network is a robust developmentalmodule,ó nature 406(6792):188192, 2000. at the same time, the reactiondiffusion approach appears to have nontrivial utility in explaining other biological phenomena, such as certain aspects of microtubule organization (c. papaseit, n. pochon, and j. tabony, òmicrotubuleselforganization is gravitydependent,ó proceedings of the national academy of sciences 97(15):83648368, 2000).models, at least as represented by mathematicsbased theory and computational models. for example,theoretical biology has a very different status within biology and has often been a poor stepchild tomainstream biology. results from theoretical biology are often irrelevant to specific biological systemssuch as a particular species, and even the simplest biological organism is so complex as to rendervirtually impossible a theoretical analysis based on first principles. indeed, most biologists have a longingrained suspicion of theoretical models that they regard as vastly oversimplified (i.e., almost all ofthem) and are skeptical of any purported insights that emerge from such models. (box 10.5 providessome examples of misleading computational and mathematical models of biological phenomena.)catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.culture and research infrastructure365a related point is that computer scientists tend to assume that universal statements have no exceptions, whereas biologists have learned that there are almost always exceptions to rules. for example, ifa biologist says that all crows are black, and one asks about albino crows, the answer will be, òoh, sure,albino crows are white, but all normal crows are black.ó the biologist is describing the average caseñallstandard crows are blackñbut keeps in the back of his or her mind the exceptional cases. by contrast,the computer scientist wants to know if the crow database he or she is building needs to accommodateanything other than a black crowñand thus, when a computer scientist makes a biological generalization, the biologist will often jump immediately to the exceptional case as a way of dismissing thegeneralization.these comments should not be taken to imply that biologists do not use theory at all; in fact,biologists use theory and models in their everyday work. the theory of evolution is among the mostpowerful of all scientific theories, in the sense that it underlies the scientific understanding of all naturalbiological phenomena. but because the outcomes of evolutionary processes are driven by a myriad ofenvironmental and chance influences, it is difficult to make measurable or quantitative predictionsabout specific biological phenomena. in this context, evolution is more of an organizing principle thana predictive formalism.perhaps a fairer statement is that many biologists remain to be persuaded of the value of quantitative theory and abstraction on a global basis, although they accept their value in the context of specialized hypothesis, individual probes, or inquiries on a biological process. biological researchers are beginning to see the potential explanatory value of computational and mathematical approachesña potentialthat is less apparent than might be expected because of the very success of an empirical approach tobiology that has been grounded in experiment and observation for many decades.10.3.1.4 data and experimentationas mentioned above, computer scientists and biologists also view data quite differently. for thecomputer scientist, data usually result from measurements of some computational artifact in use (e.g.,how long it takes for a program to run, how many errors a program has). because these data are tied toartifacts that have been made by human beings, they are as ephemeral and transient as the underlyingartifact, which may indeed change in the next revision or release. because computer science is a scienceof the artificial, the intellectual process of the computer scientist does not begin with data, but ratherwith an act of artifact creation, after which measurements can be taken.66indeed, for the computer scientist, the term òexperimental computer scienceó refers to the engineering and creation of new or improved computational artifactsñhardware or softwareñas the centralobjective of intellectual efforts.67 engineering has intellectual biases toward model reduction, extractingkey elements, and understanding subsystems in isolation before assembling larger structures. theengineering approach also rests on the idea that basic units (e.g., transistors, silicon chips) have repeatable, predictable behavior; that òmodulesó with specific capability (e.g., switches, oscillators, and filters)can be made from such units; and that larger systems with arbitrary complexity are, in turn, made ofsuch modules.in contrast, biology today is a datadriven scienceñand theories and models are created to fit thedata. data, presuming they are accurate, impose òhardó constraints on the biologist in much the sameway that results from theoretical computer science impose hard constraints on the computer scientist.because of the central role that data play in biology, biologists pay a great deal of attention to experi66this is not to deny that computer scientists often work with large datasets. for example, computer scientists may work withterabytes of textual or image data. but these data are the subjects of manipulation and processing, rather than being tied directlyto the performance of the hardware and software artifacts of the computer scientist.67national research council, academic careers for experimental computer scientists and engineers, national academy press,washington, dc, 1994.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.366catalyzing inquirymental technique and laboratory procedure and instrumentationñmuch more so than most computerscientists pay to the comparable areas in computer science. thus, a computer scientist with insufficientawareness of experimental design may not be accustomed to or even aware of techniques of formalmodel or simulation validation.in addition, biology has not traditionally looked to engineering for insight or inspiration. forexample, proteins come in an endless variety with many variations and do not necessarily have straightforward analogues to engineering parts. experimental biologists often focus on discovering new piecesof cellular machinery and on how defective behavior stems from broken or missing pieces (e.g., mutations). experimental work is aimed at proving or disproving specific hypotheses, such as whether or nota particular biochemical pathway is relevant to some cellular phenomena.the training that computer scientists receive also emphasizes general solutions that give guaranteesabout events in terms of their worstcase performance. biologists are interested in specific solutions thatrelate to very particular (although voluminous) datasets. (a further complication is that biological dataare often erroneous and/or inconsistent, especially when collected in large volume.) by recognizing andexploiting special characteristics of biologically significant datasets, specialpurpose solutions can becrafted that function much more effectively than generalpurpose solutions. for example, in the problem of genomic sequence assembly, it turns out that by exploiting the information available concerningthe size of fragments, the number of choices for where a fragment might fit is sharply restricted.the central role that experimental data plays in biology is responsible for the fact that, to date,computer scientists have been able to make their most important contributions in areas in which thedetails of some biological phenomena can be neglected to some important extent. thus, the abstractionof dna as merely a string of characters derived from a fourletter alphabet is a very powerful notion,and considerable headway in genomics can be made knowing little else. to be sure, there are experimental errors to take into account, and a model of the noisiness of the data must be developed, but theunderlying problem is pretty clear to a computer scientist.on the other hand, as the discussion in section 4.4.1 makes clear, there are limits to this abstractionthat arise from just such òdetails.ó also, proteomicsñin which the threedimensional structure of a protein, rather than the linear sequence, determines its functionñpresents even greater challenges. to understand the geometry of a threedimensional structure, discrete mathematicsñthe stock in trade of thecomputer scientistñis far less useful than continuous mathematics.68 furthermore, the properties andcharacteristics of the specific amino acids in a protein matter a great deal to its structure and function,whereas the various nucleotide bases are more or less equivalent from an informational standpoint. inshort, proteomics involves a much more substantial body of domain knowledge than does genomics.one illustration related by a senior computer scientist working in biology is his original dream that,with enough data,69 it would be computationally straightforward to understand the mechanisms ofgene regulation. that is, with sufficient data on regulatory pathways, cascades, gene knockouts, expression levels, and their dependencies on environmental factors, how genetic regulatory networks workwould become reasonable clear. with the hindsight of several years, he now believes that this dreamwas hopelessly nałve in that it did not account for the myriad exceptions and apparent special casesinherent in biological data that make the biologistõs intellectual life very complicated indeed.finally, consider that many biologists are suspiciousñor at least not yet persuadedñof the value andimportance of highthroughput measurement of biological systems (section 7.2). because many biologistswere educated and worked in an era in which data were scarce, experiments in biology have historicallybeen oriented toward hypothesis testing. highthroughput data collection drives in the opposite direction,68the reason is that geometric descriptions naturally involve continuous variables such as lengths and angles, and functions ofthose variables.69richard karp, university of california, berkeley, personal communication, july 29, 2002.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.culture and research infrastructure367and relies on the ability to sift through and recognize patterns in large volumes of data whose meaning canthen be inferred. of course, predictions that emerge from the analysis of large volumes of data must stillbe verified one at a time, and science is today far from the point at which such analysis would, by itself,provide reliable biological conclusions. nevertheless, such analysis can play an important role in suggesting interesting hypotheses and thus expand the options available for biological exploration.10.3.1.5 a caricature of intellectual differencesa number of oneliners that can be used to encapsulate the differences described above, though aswith all oneliners, there is considerable oversimplification. here are four:¥the goal of computer science (cs) is to develop solutions that can be useful in solving manyproblems, while the goal of biology is to look for solutions to individual and specific problems.¥computer science is driven by the development of method and technique, while biology isdriven by experiment and data.¥computer scientists are trained to search for boundary conditions and constraints, whereasbiologists are trained to seek signal in the noise of their experimental data.¥computer scientists are trained to take categorical statements literally, whereas biologists usethem informally.10.3.2 differences in cultureanother barrier at the biocomp interface is cultural. each field has its own cultural style, and whatseems obvious to practitioners in one field may not be obvious to those in the other. consider, forexample, differences between computer science and biology. before powerpoint became ubiquitous toboth fields, computer scientists tended to use overhead transparencies in visiting lectures, while biologists tended to use 35 mm slides. computer science, as a discipline, can often be pursued while workingat home, whereas biological lab work requires being òin the officeó to a far greater extentña computerscientist who is away from the lab may well be seen by biologists as ònot being around enoughó or ònotbeing a team player.ó computer scientists are accustomed to having their own office space, whilebiologists (especially postdoctoral associates) work out of their labs and rarely have their own officesuntil they achieve an appropriate seniority.such differences are in some sense trivial, but they do suggest the reality of different cultures, and it ishelpful to explore some other differences that are not so trivial. one of the most important differences is thatof intellectual style: the discussion in section 10.3.1 would suggest that biologists (especially those untrainedin quantitative sciences) may well distrust the facile approaches and oversimplified models of computerscientists or mathematicians unfamiliar with the complexities of living things, and the computer scientistmay well regard the biologist as obsessed with details and molecular parts lists rather than the qualitative orquantitative whole. this section explores some issues that lie outside the domain of intellectual style.10.3.2.1 the nature of the research enterprisewhen practitioners from two fields collaborate, each brings to the table the values that characterizeeach field. given the importance that biologists place on the understanding of specific biological phenomena of interest, they place the highest value on answers that are specific to those phenomena.biologists want òthe answer,ó and they are interested in details of a computational model only insofaras they have an effect on the answer; for the most part, they care far less about a hypothetical biologicalphenomenon than about explaining the data obtained from experiment. computer scientists and mathematicians, in contrast, are interested in the parameters of a model or a solution and in ways to improvecatalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.368catalyzing inquiryit, characterize it, understand it better, or make it more generally applicable to other problems. thus, thebiologist will likely be interested in the results of a model run on the single dataset of interest, while thecomputer scientist will want to run hundreds or thousands of datasets to better analyze the behavior ofthe model, and mathematicians will want to explore the limits of a modelõs applicability.70an example of this cultural difference is illustrated in the history of the gene ontology (go) discussedin chapter 4. begun in 1998 as a collaboration between researchers responsible for three model organismdatabases (flybase [drosophila], the saccharomyces genome database, and the mouse genome database),go collaborators sought to develop structured, controlled vocabularies that describe gene products in termsof their associated biological processes, cellular components, and molecular functions in a speciesindependent manner. in their work, these researchers have apparently not made extensive use of the (mostlydomainindependent) theoretical contributions of computer science from the last 20 years, but rather havereinvented much of that work on their own. the reason for this reinvention, offered by one knowledgeableobserver, is that they were unable to find computer scientists with appropriately specialized experience whowere willing to sacrifice their quest for general applicability to develop a functional, usable system.71a related point is that in academia, research computer scientists have very little motivation to take asoftware implementation beyond the prototype stage. that is, they may have developed a powerfulalgorithm that is likely to be useful in many biological contexts, implemented a prototype software systembased on this algorithm, and convincingly demonstrated its utility in a few cases. but because most of theintellectual credit inheres in the prototype (e.g., papers for publication and promotions), research computer scientists have little motivation to move from the prototype system, which can generally be usedonly by those familiar with the quirks of its operation, to a more robust system that can be used by thebroader community at large. because going from prototype to broadly usable system is generally a timeintensive process, many powerful methods are not available to the biology community.similar considerations apply in the biology community with respect to data curation. intellectualcredit for academic biologists inheres in the publication of primary data, rather than in any longtermfollowup to ensure that the data are useful to the broader community. (indeed, if the data are not madeuseful to the broader community, the researcher originally responsible for the data gains the competitive advantage of being the only one, or one of a few, able to use them.) this suggests that culturalincentives for data curation (or the lack thereof) have to be altered if data curation is to become a moresignificant activity in the research community.7270these differences in perspective are also found at the interface of medical informatics and bioinformatics. for example,altman notes that òthe pursuit of bioinformatics and clinical informatics together is not without some difficulties. practitioners inclinical medicine and basic science do not instantly understand the distinction between the scientific goals of their domains andthe transferability of methodologies across the two domains. they sometimes question whether informatics investigators are reallydevoted to the solution of scientific problems or are simply enamored of computational methodologies of unclear significance [emphasisadded].ó to reduce these tensions, altman arguesñsimilarly to the argument presented in this reportñthat òinformatics investigators (and their students) be able to work collaboratively with physicians and scientists in a manner that makes it clear that thecreation of excellent, wellvalidated methods for solving problems in these domains is the paramount goal.ó see r.b. altman,òthe interactions between clinical informatics and bioinformatics: a case study,ó journal of the american medical informaticsassociation 7(5):439443, 2000.71russ b. altman, stanford university, personal communication, december 16, 2003.72one approach that has been used to support data annotation and curation activities is the data jamboree. in november 1999,the celera corporation hosted an invitationonly event (òthe jamboreeó) in which participants worked for two weeks at annotating and correcting data from the drosophila melanogaster genome. by all accounts a successful event that resulted in the publication of the complete sequence as well as appropriate annotations (see m.d. adams, s.e. celniker, r.a. holt, c.a. evans, j.d.gocayne, p.g. amanatides, s.e. scherer, et al., òthe genome sequence of drosophila melanogaster,ó science 287(5461):21852195,2000) the event featured a very informal atmosphere that promoted social connection and interaction as well as a work environment conducive to the task. the emergence of some level of community curation on amazon and ebay may also provide someuseful hints on how to proceed. in these efforts, community assessment is allowed, but thereõs no overall review of the quality ofthe assessment. nonetheless, users have access to a diverse collection of assessments of and can do their own metaqualitycontrol by deciding which of the reviewers to believe. this model does scale with increasing database size, although consistentcuration is hardly guaranteed. it is an open question worth some investigation as to whether community commentary (perhapssupported with an appropriate technological infrastructure) could result in meaningful data curation.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.culture and research infrastructure36910.3.2.2 publication venuealthough both biologists and computer scientists in academia seek to make their work known totheir respective peer communities, they do so in different ways. for many areas within computerscience, refereed conferences are the most prestigious places to publish leading research.73 in biology,by contrast, conference publications are often considered less prestigious, and biologists tend to preferjournal publications. computer scientists often write abstracts in such a way as to entice the reader toread the full paper, whereas biologists often write abstracts in such a way that the reader need not readthe full paper. in publishing work that refers to a new tool, a computer scientist may be more likely toreference a web site where the tool can be found, while a biologist may be more likely to reference apaper describing the tool.this difference in publication venues strongly affects attempts at collaboration. academic biologistsoften do not understand refereed conferences, and computer scientists often think of journals as mererepositories of papers, rather than a means of communicating results. given that publication is theprimary output of academic research, this disagreement can be very disturbing and can have importantinhibitory effects on collaboration.10.3.2.3 organization of human resourceswhile it is clear to all parties from the start that that the professional expertise of the biologist isneeded to do good work in computational biology, a view that equates computer science with programming can lead biologists to underestimate the intellectual capabilities on the computer science sidenecessary for computationintensive biology problems. thus, many biologists who do see rewards inbridging the interdisciplinary gap (especially in academia) tend to prefer doing so in their own labs, byhiring postdoctoral fellows from physics or computer science to work on their problems, keeping theseventures òin the family,ó rather than by establishing partnerships with more established computerscientists.such an approach has advantages and disadvantages. an advantage is that postdoctoral fellowswith good quantitative and computational background can be exposed to the art of biological experimentation and interpretation as a part of their postdoctoral training, the result of which can be thenurturing of young interdisciplinary scientists. one disadvantage is that by engaging individuals at thebeginning of their careers, the biologist is deprived of the intellectual maturity and insight that generally accompanies more seasoned computer scientistsñand such maturity and insight may be mostnecessary for making headway on complex problems.the integration of computational expertise into a biological research enterprise can be undertakenin different ways. for example, in some instances, a group of computer scientists can work with a groupof biologists, each bringing its own computational approach to the biological problem. in other cases, asingle individual with computational expertise (e.g., a postdoc) can work in an otherwise òpureó biological group, offering expertise in math, modeling, and programming. a second dimension of integration is that the bioinformaticist can play a role as a team member who engages equally with everyoneelse on the research team or as a òbridgeó that serves as intermediate between practitioners of variousdisciplines. these two roles are not mutually exclusive, although the first seems to be more common.10.3.2.4 devaluing the contributions of the othera sine qua non of interdisciplinary work is that intellectual credit be apportioned appropriately.in some cases known to the committee, the expertise of scientists from a nonbiological discipline has73such a practice arose because computer science is a fastmoving field, with a tradition of sharing discovery by onlinedemonstration and discussion. conferences were originally formed as a way to talk together, in person, and with relatively fastpublication of results for the requirements of academia. in this context, journal publication would have been much too slow.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.370catalyzing inquirybeen used, and yet joint authorship or due credit withheld. in one story, a respected biologist heldregular discussions with an excellent mathematician colleague. the biologist assigned a theoreticalproject on a topic already worked out by the mathematician to an inhouse physics postdoctoralfellow instead of pursuing joint work with the mathematician. the results of this inhouse work fellshort of what was possible or desirable but displaced other serious attempts at theoretical analysis ofan interesting problem.this example suggests a view of mathematics and computer science that is ancillary and peripheralto the òrealó substance of biology. the fact that computing and mathematics have developed powerfultools for the analysis of biological data makes it easy for biologists to see the computer scientist as thedata equivalent of a lab technician. however, although programming is an essential dimension of mostcomputer scientistsõ backgrounds, it does not follow that the primary utility of the computer scientist isto do programming. algorithm design, to take one example, is not programming, but because algorithms must be implemented as a computer program, it is easy to confuse the two.in other cases known to the committee, the expertise of biological scientists has been denigrated bythose in computing. for example, computer scientists sometimes view a successful biological experiment as one that òmerelyó produces more data and do not appreciate the fundamental creative actrequired to devise the appropriate experiment. this attitude suggests a view of biology in which theòrealó science resides in the creation of a theory or a computational model, and data are merely what isneeded to populate the model.what accounts for such attitudes? the committee believes one contributing factor is not muchdifferent than loyalty to oneõs discipline. professionals in one discipline quite naturally come to believethat the ways in which they have learned to see their discipline have inherent advantages (if they didnot, they would not be part of the discipline), and challenges to the intellectual paradigms they bring tothe subject may well be met with a certain skepticism.a second point to consider is that interdisciplinary work is not necessarily symmetric. this isespecially true in the mix of academic research activity vis a vis applied or technical support activity.that is, it is often possible to identify one field as being the side where research advances are occurringand the other as applying some kind of support. in some cases, ph.d.level research in computer sciencecan be enriched by what is routinely taught in undergraduate classes in biology, and vice versa.for example, individuals pursuing cuttingedge research in database design may be interested infinding data models to exercise their design. they are interested in finding domain experts to help thembetter understand the complexities of a certain interesting problem domain, such as biology, but thesedatabase researchers see the data and the insights coming from the biologist as helping to define theproblem, but as having little to do with finding the solution. similarly, biologists may be investigatinga new topic in biology and need quantitative or logistical or algorithmic help to accomplish the research,but they feel the real intellectual contributionñto biologyñcomes from their insights on the biologicalside.the primary exception to these scenarios is where a research group in computer science gets teamedup with a research group in biological science. in such instances, the relationship can be truly symmetrical. both parties benefit from a symbiotic relationship. both yield practical value to the other, whilegaining theoretical value for themselves. both operate at an equivalent level of intellectual contribution.both gain an equivalent level of real research coming out of the activity.10.3.2.5 attitudinal issuesbiology laboratories are increasingly dependent on various forms of information technology. highthroughput instrumentation generates large volumes of data very quickly. computerbased databasesare the only way to keep track of a biological literature that is growing at exponential rates. computerprograms are increasingly needed to assemble and understand biological data derived from experiments or resident in databases.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.culture and research infrastructure371with such dependence on it, it would not be surprising if individuals who are especially knowledgeable about information technology were necessary to keep these laboratories running at highefficiency. however, computer scientists are very wary of being put into the role of technician orprogrammer. computer science researchers, facing this prospect from various research disciplines, canbe sensitive about wanting respect for the fundamental research advances they bring to the table.74the roles of intellectual collaborator and coequal partner are largely incompatible with the role oftechnician, and it is understandable that a computer scientist would want to be treated as a coequal. atthe same time, a certain amount of humility and respect is also necessary. that is, the computer scientistmust refrain from jumping to conclusions, must be willing to learn the facts and contemplate biologicaldata seriously, and must not work solely on the refined abstraction problem. it may well be necessaryfor the computer scientist to do some mundane things to earn the confidence of the biologist partnerbefore being able to do more interesting things.the biologist has a role to play in facilitating partnership as well. for example, the biologist mustunderstand that the computer scientist (especially one at the beginning of his or her career) wants to dowork of publishable quality as wellñwork that will earn the respect of colleagues in computer science.as suggested above, programming generally does not meet this test. a second important point is torecognize without condescension the fact that many (most?) computer scientists have very little experience or familiarity with either biological concepts or data. still a third point is the recognition that whileprimary data generation and experiment remain important to the life sciences, analytical work onexisting data can be every bit as valuableñbioinformatics is not simply òtaking someone elseõs data.óthis last point suggests a more subtle risk in partnershipsñthat a person with specialized skills may beregarded as a technician or a standalone consultant rather than as a true collaborator.10.3.3 barriers in academiaone important venue for research at the biocomp interface is academia. universities can provideinfrastructure for work in this area, but institutional difficulties often arise in academic settings for workthat is not traditional or easily identified with existing departments. these differences derive from thestructure and culture of departments and disciplines, and lead to scientists in different disciplineshaving different intellectual and professional goals and experiencing different conditions for theircareer success. collaborators from different disciplines must find and maintain common ground, suchas agreeing on goals for a joint project, but also respect one anotherõs separate priorities, such as havingto publish in primary journals to present at particular conferences or to obtain tenure in their respectivedepartments according to those departmental criteria. such crosspressures and expectations from thehome departments and disciplinary colleagues remain even if the participants develop similar goals fora project.10.3.3.1 academic disciplines and departmental structureuniversities are structured around disciplinary departments and often have considerable difficulty insupporting and sustaining interdisciplinary work. neither fish nor fowl, the interdisciplinary researcher isoften faced with the formidable task of finding an intellectual home within the university that will take theresponsibility for providing tenure, research space, startup funding, and the like. the essential problem isthat a researcher working at the interface between fields x and y is often doing work that does not fallclearly within the purview of either department x or department y. when budgets are expanding and74it is useful to note that research laboratories in both biology and computer science employ technicians and programmers,and such individuals serve very useful functions in each kind of laboratory. but the role of a lab technician in a biology laboratory or programmer in a computer laboratory is quite different from the role of the senior scientist who directs the biology orcomputer laboratory.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.372catalyzing inquiryresources flush, it is easy for department x or department y to take a risk on an interdisciplinary scholar.but as is more often the case today, when resources are scarce, each department is much more likely towant someone who fits squarely within its traditional departmental definitions, and any appointment thatgoes to an interdisciplinary researcher is seen as a lost opportunity.for example, tenure letters may be requested from traditional researchers in the field for an interdisciplinary worker; despite great success, the tenure letters may well indicate that they were unfamiliarwith the candidateõs work. graduate students seeking interdisciplinary training but nominally housedin a given department may have difficulty taking that departmentõs qualifying exam, because theirtraining is significantly different from mainstream students.another dimension of this problem is that publication venues often mirror departmental structures.thus, it may be difficult to find appropriate venues for interdisciplinary work. that is, the forms ofoutput and forums of publication for the interdisciplinary researcher may be different than for eitherdepartment x or department y. for example, even within computer science itself, experimental computer scientists that focus on system building often lack a track record of published papers in refereedjournals, and tenure and promotion committees (often universitywide) that focus on such records formost other disciplines in the university have a hard time evaluating the worthiness of someone whosecontributions have taken the form of software that the community has used extensively or presentationsat refereed conferences. even if biologists are aware in principle of such òpublicationó venues, they maynot be aware that such conferences are heavily refereed or are sometimes regarded as the most prestigious of publication venues. also, prestigious journals known for publishing biology research are oftenreluctant to devote space to papers devoted to computational technique or methodology if it does notinclude specific application to an important biological problem (in which case the computational dimensions are usually given a peripheral rather than primary status).further, the academic tenure and promotion system is biased toward individual work (i.e., work ona scale that a single individual can publish and receive credit for). however, large software systemsñcommon in computer science and bioinformaticsñare constructed by teams. although small subsystemscan be developed by single individuals, it is the whole system that provides primary value, and universitybased research that is usually driven by a singleauthored ph.d. thesis or single faculty members isnot very well suited to such a challenge.75finally, in most departments, it is the senior faculty that are likely to be the most influential withregard to the allocation of resourcesñspace, tenure, personnel and research assistant support, and soon. if these faculty are relatively uninformed or disconnected from ongoing research at the biocompinterface, the needs and intellectual perspectives of interface researchers will not be fully taken intoaccount.10.3.3.2 structure of educational programsstovepiping is also reflected in the structure of educational programs. stovepiping refers to thetendency of individual disciplines to have different points of view on what to teach and how to teach it,without regard for what goes on in other disciplines. in some cases, the methods of the future are stillundeveloped, or are undergoing revolution, so that suitable texts or syllabi are not yet available. further, like individual researchers, departments tend to be territorial, protective of their realms, andinsistent on evergrowing specialized course load requirements for their own students. this discourages or precludes crossdiscipline shopping. novel training creates a need for reeducation of faculty tochange the design of old curricula and modernize the teaching. these changes take time and energy,and require release time from other academic burdens, whether administrative or teaching.75c. koch, òwhat canneurobiology teach computer engineers?,ó division of biology and division of engineering andapplied science, california institute of technology, january 31, 2001,position paper to national research council workshop,available at http://www7.nationalacademies.org/compbiowrkshps/christofkochpositionpaper.doc.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.culture and research infrastructure373related to this point is the tension between breadth and depth. should an individual trained in xwho wishes to work at the intersection of x and y undertake to learn about y on his or her own, or seekto collaborate with an individual trained in y? leadingedge research in any field requires deep knowledge. but work at the interface of two disciplines draws on both of them, and it is difficult to be deep inboth fields; thus, ph.d.level expertise in both computer science and biology may be unrealistic toexpect. as a result, collaboration is likely to be necessary in all but extraordinary cases.thus, what is the right balance to be struck between collaboration and multiskilling of individuals?there is no hardandfast answer to this question, but the answer necessarily involves some of both.even if òcollaborationó with an expert in y is the answer, the individual trained in x must be familiarenough with y to be able to conduct a constructive dialogue with the expert in y, asking meaningfulquestions and understanding answers received. at the same time, it is unlikely that an expert in x coulddevelop in a reasonable time expertise in y comparable to that of a specialist in y, so some degree ofcollaboration will inevitably be necessary.this generic answer has implications for education and research. in education, it suggests thatstudents are likely to benefit from presentations by both types of expert (in x and in y), and theknowledge that each expert has of the otherõs field should help to provide an integrated framework forthe joint presentations. in research, it suggests that research endeavors involving multiple principalinvestigators (pis) are likely to be more successful on average than singlepi endeavors.stovepiping can also cause problems for graduate students who are interested in dissertation work,although for graduate students these problems may be less severe than for faculty. some universitiesmake it easier for graduate students to do interdisciplinary work by allowing a studentõs doctoral workto be supervised by a committee composed of faculty from the relevant disciplines. however, in theabsence of a thesis supervisor whose primary interests overlap with the graduate studentõs work, it isthe graduate student himself or herself who must be the intellectual integrator. such integration requires a level of intellectual maturity and perspective that is often uncommon in graduate students.the course of graduatelevel education in computing and in biology is different in some ways. inbiology, students tend to propose thesis topics earlier in their graduate careers, and then spend theremainder of their time doing the proposed research. in computer science (especially more theoreticalaspects), in contrast, proposals tend to come later, after much of the work has been done. computerscience graduates do not usually obtain postdoctoral positions, more commonly moving directly toindustry or to a tenuretrack faculty position. receiving a postdoctoral appointment is often seen as asign of a weak graduate experience in computer science, making postdoctoral opportunities in biologyseem less attractive.10.3.3.3 coordination costsin general, the cost of coordinating research and training increases with interdisciplinary work.when computer scientists collaborate with biologists, they also are likely to belong to different departments or universities. the lack of physical proximity makes it harder for collaborators to meet, coordinate student training, and share physical resources, and studies indicate that distance has especiallystrong effects on interdisciplinary research.76recognizing the importance of reducing distances between collaborators, stanford universityõsbiox program is designed specifically to foster communication campuswide among the various disciplines in biosciences, biomedicine, and bioengineering. the clark center houses meeting rooms, ashared visualization chamber, lowvibration workspace, a motion laboratory, two supercomputers, the76j. cummings and s. kiesler, kdi initiative: multidisciplinary scientific collaborations, report to national science foundation,2003, available at http://netvis.mit.edu/papers/nsfkdireport.pdf; r.e. kraut, s.r. fussell, s.e. brennan, and j. seigel, òunderstanding effects of proximity on collaboration: implications for technologies to support remote collaborative work,ó pp.137162 in distributed work, p.j. hinds and s. kiesler, eds., mit press, cambridge, ma, 2002.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.374catalyzing inquirysmallanimal imaging facility, and the biofilms center. other core shared facilities available to thestanford research community include a bioinformatics facility, a magnetic resonance facility, amicroarray facility, a transgenic animal facility, a cell sciences imaging facility, a product realization lab,the stanford center for innovation in in vivo imaging, a tissue bank, and facilities for cognitive neuroscience, mass spectrometry, electron microscopy, and fluorescenceactivated cell sorting.77interdisciplinary projects are often bigger than unidisciplinary projects, and bigger projects increasecoordination costs. coordination costs are reflected in delays in project schedules, poor monitoring ofprogress, and an uneven distribution of information and awareness of what others in the project aredoing. coordination costs also reduce peopleõs willingness to tolerate logistical problems that might bemore tolerable in their home contexts. furthermore, they increase the difficulty of developing mutualregard and common ground, and they lead to more misunderstandings.78coordination costs can be addressed in part through changes in technology, management, funding,and physical resources. but they can never be reduced to zero, and learning to live with greater overhead in conducting interdisciplinary work is a sine qua non for participants.10.3.3.4 risks of retraining and conversionretraining or conversion efforts almost always entail reduced productivity for some period of time.this fact is often viewed with dread by individuals who have developed good reputations in theiroriginal fields, and who may worry about sacrificing a promising career in their home field whileentering at a disadvantage in the new one. these concerns are especially pronounced when they involveindividuals in midcareer rather than recently out of graduate school.such fears often underlie the failure of individuals seeking to retool themselves to commit themselves fully to their new work. that is, they seek to maintain some degree of ties to their original fieldsñsome research, some keeping up with the literature, some publishing in familiar journals, some going tofamiliar conferences, and so on. these efforts drain time and energy from the retraining process, butmore importantly they may inhibit the necessary mindset of success and commitment in the newdomain of work. (on the other hand, keeping a foot in their old fields could also be viewed as a rationalhedge against the possibility that conversion may not be successful in leading to a new field of specialization. moreover, maintaining the discipline of continual output is a task that requires constant practice, and oneõs old field is likely to be the best source of such output.)10.3.3.5 rapid but uneven changes in biologybiology is an enormously broad field that contains dozens of subfields. over the past few decades,these subfields have not all advanced or prospered equally. for example, molecular and cell biologyhave received the lionõs share of biological funding and prestige, while subfields such as animal behavior or ecology have faired much less well. molecular and cell biology (and more recently genomics,proteomics, and neuroscience) have swept through as departments modernize, in a kind of òbandwagonó effect, leaving some of the more traditional subfields to lie fallow because promising youngscholars in those subfields are unable to find permanent jobs or establish their careers due to theseshifts.moreover, prospering subfields are highly correlated with the use of information technology. sucha close association of it with prospering fields is likely to exacerbate lingering resentments from nonprospering subfields toward the use of information technology.77for more information see http://biox.stanford.edu/.78j. cummings and s. kiesler, òcollaborative research across disciplinary and institutional boundaries,ó social studies ofscience, in press, available at http://hciresearch.hcii.cs.cmu.edu/complexcollab/pubs/paperpdfs/cummingscollaborative.pdf.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.culture and research infrastructure37510.3.3.6 funding risktight funding environments often engender in researchers a tendency to behave conservatively andto avoid risk. that is, unless special care is taken to encourage them in other directions (e.g., throughspecial programs in the desired areas), researchers seeking funding are likely to pursue avenues ofintellectual inquiry that are likely to succeed. such researchers are therefore strongly motivated topursue work that differs only marginally from previous successful work, where paths to success canlargely be seen even before the actual research is undertaken. these pressures are likely to be exacerbated for senior researchers with successful and wellrespected groups and hence many mouths to feed.this point is addressed further in section 10.3.5.3.10.3.3.7 local cyberinfrastructuresection 7.1 addressed the importance of cyberinfrastructure to the biological research enterprisetaken as a whole. but individual research laboratories need to be able to count on the local counterpartof communitywide cyberinfrastructure. institutions generally provide electricity, water, and libraryservices as part of the infrastructure that serves individual resident laboratories. but information andinformation technology services are increasingly as important to biological research as these moretraditional services, and thus it makes sense to consider that they might be provided as a part of thelocal infrastructure.on the other hand, regarding computing and information services as part of local infrastructure hasinstitutional implications. for example, one important issue is providing centralized support for decentralized computing. useful scientific computing must be connected to a network, and networks mustinteract and must be run centrally, but nonetheless, scientific computing must be accomplished in theway scientific instruments are used, that is, very much under the control of the researcher. how caninstitutions develop a computing infrastructure that delivers the cost effectiveness and the robustnessand the reliability of wellrun centralized systems while at the same time delivering the flexibilitynecessary to support innovative scientific use? in many research institutions, managers of centralizedcomputing regard researchers as cowboys uninterested in exercising any discipline for the larger good,while researchers regard the managers of centralized computing as bureaucrats who are disinterestedin the practice of science. though neither of these caricatures is correct, these divergent views of howcomputing should effectively be deployed in a research organization will continue to exist unless theinstitution takes steps to reconcile them.10.3.4 barriers in commerce and business10.3.4.1 importance assigned to shortterm payoffsin a time frame roughly coincident with the dotcom boom, commercial interest in bioinformaticswas very highñperhaps euphoric in retrospect. large, established, biotechpharmaceutical companies,genomicsera drug discovery companies, and tiny startups all believed in the potential forbioinformatics to revolutionize drug design and even health care, and these beliefs were mirrored invery high stock prices.more recently, market valuations of biotech firms have dropped along with the rest of the technology sector, and these more recent negative trends have affected the prevailing sentiment about thevalue of bioinformatics for drug design, at least for the short term. although the human genomesequencing is complete, only a handful of drugs now in the pipeline stemmed from bioinformaticanalysis of the genome. bioinformatics does not automatically lead to marketable òblockbusteró drugs,and drug companies have realized that the primary bottlenecks involve biological knowledge: notenough is known of the overall biological context of gene expression and gene pathways. in the wordscatalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.376catalyzing inquiryof one person at a 2003 seminar, òthis is work for [biological] scientists, not bioinformaticists.ó for thisreason, further largescale business investment in bioinformaticsñand indeed for any research with along time horizonñis difficult to justify on the basis of relatively shortterm returns and thus is unlikelyto occur.these comments should not be taken to imply that bioinformatics and information technology havenot been useful to the pharmaceutical industry. indeed, bioinformatics has been integrated into theentire drug development process from gene discovery to physical drug discovery, even to computerbased support for clinical trials. also, there is a continuing belief that bioinformatics (e.g., simulations ofbiological systems in silico and predictive technologies) will be important to drug discovery in the longterm.10.3.4.2 reduced workforcesthe cultural differences between life scientists and computer scientists described in section 10.3.2have ramifications in industry as well. for example, a sense that bioinformatics is in essence technicalwork or programming in a biological environment leads easily to the conclusion that the use of formallytrained computer scientists is just an expensive way of gaining a year or two on the bioinformaticslearning curve. after all, if all of the scientists in the company use computers and software as a matterof course and can write sql (structured query language) queries themselves, why should the company have on its payroll a dedicated bioinformaticist to serve as an interface between scientists andsoftware? in a time of expansion and easy money, perhaps such expenditures are reasonable, but whencash must be conserved, such a person on staff seems like an expensive luxury.10.3.4.3 proprietary systemsin all environments, there is often a tension between systems built in a proprietary manner andthose built in an open manner, and the bioinformatics domain is no exception. proprietary systems areoften not compatible or interoperable with each other, and yet vendors often think that they can maximize revenues through the use of such systems. this tendency is particularly vexing in bioinformaticswhere integration and interoperability have so much value for the research enterprise. standards andopen application programming interfaces are one approach to addressing the interoperability problem.but as is often the case, many vendors support standards only to the extent that they are alreadyincorporated into existing product lines.10.3.4.4 cultural differences between industry and academiaas a general rule, private industry has done better than academia in fostering and supportinginterdisciplinary work. the essential reason is that disciplinary barriers tend to be lower and teamworkis emphasized when all are focused on the common goals of making profits and developing new anduseful products. by contrast, the coin of the realm in academic science is individual recognition for aprincipal investigator as measured by his or her publication record.this difference appears to have consequences in a variety of areas. for example, expertise related tolaboratory technique is important to many areas of life sciences research. in an industrial setting, thisexpertise is highly valued, because individuals with such expertise are essential to the implementationof processes that lead to marketable products. these individuals receive considerable reward andrecognition in an industrial setting. although such expertise is also necessary for success in academicresearch, lab technicians rarelyñif everñreceive rewards that are comparable to the rewards accruedby the principal investigator.related to this is the matter of staffing a laboratory. in todayõs job environment, it is common for anewly minted ph.d. to take several postdoctoral positions. if in those positions an individual does notcatalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.culture and research infrastructure377develop a sufficient publication record to warrant a faculty position, he or she is for all intents andpurposes out of the academic research gameña teaching position may be available, but taking a position that primarily involves teaching is not regarded as a mark of success. however, it is exactlyindividuals with such experience that are in many instances the backbone of industrial laboratories andprovide the continuity that is needed for a productõs life cycle.the academic drive for individual recognition also tends to inhibit collaboration. academic research laboratories can and do work together, but it is most often the case that such arrangements haveto be negotiated very carefully. the same is true for large companies that collaborate with each other,but such companies are generally much larger than a single laboratory and intracompany collaborationtends to be much easier to establish. thus, the largest projects involving the most collaborators arefound in industry rather than academia.even òsmalló matters are affected by the desire for individual recognition. for example, academiclaboratories often prepare reagents according to a labspecific protocol, rather than buying standardized kits. the kit approach has the advantage of being much less expensive and faster to put into use,but often does not provide exactly the functionality that custom preparation offers. that is, the academic laboratory has arranged its processes to require such functionality, whereas an industrial laboratory has tweaked its processes to permit the use of standardized kits.the generalization of this point is that because academic laboratories seek to differentiate themselves from each other, the default position of such laboratories is to eschew standardization of reagents, or of database structure for that matter. standardization does occur, but it takes a special effortto do so. this default position does not facilitate interlaboratory collaboration.10.3.5 issues related to funding policies and review mechanismsas noted in section 10.2.5.2, a variety of federal agencies support work at the biocomp interface.but the nature and scale of this support vary by agency, in terms of the procedures for making decisionsabout what proposals are worthy of support.10.3.5.1 scope of supported workfor example, although the nih does support a nontrivial amount of work at the biocomp interface,its approach to most of its research portfolio, across all of its institutes and centers, focuses on hypothesistesting researchñresearch that investigates wellisolated biological phenomena that can be controlled or manipulated and hypotheses that can be tested in straightforward ways with existing methods. this focus is at the center of reductionist biology and has undeniably been central to much ofbiologyõs success in the past several decades.on the other hand, the nearly exclusive focus on hypothesis testing has some important negativeconsequences. for example, experiments that require breakthrough approaches are unlikely to be directly supported. just as importantly, advancing technology that could facilitate research is almostalways done as a sideline. this has had a considerable chilling effect in general on what could havebeen, but the impact is particularly severe for implementation of computational technologies in biological sciences. that is, in effect as a cultural aspect of modern biological research, technology developmentto facilitate research is not considered real research and is not considered a legitimate focus of a standard grant. thus, even computing research that would have a major impact on the advancement ofbiological science is rarely done (box 10.6 provides one example of this reluctance).it is worth noting two ironies. first, it was the department of energy, rather than the nih, thatsupported the human genome project. second, the development of technology to conduct polymerasechain reaction (pcr)ña technology that is fundamental to a great deal of biological research today andwas worthy of a nobel prize in 1993ñwould have been ineligible for funding under traditional nihfunding policy.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.378catalyzing inquiryto illustrate the consequences in more concrete but futureoriented terms, the list below suggestssome of the activities that would be excluded under a funding model that focuses only on hypothesistesting research:¥developing technologies that enable data collection from a myriad of instruments and sensors,including realtime information about biological processes and systems, that permit us to refine andannotate this information and incorporate it into accessible repositories to facilitate scientific study orbiomedical procedures;¥flexible database systems that allow incorporation of multiscale, multimodal information aboutbiological systems by enabling the inclusion (by data federation techniques such as mediation) of information distributed in an unlimited number of other databases, data collections, web sites and so on;¥acquisition of òdiscoverydrivenó data (discovery science, as described in chapter 2) to populatedatasets useful for computational analytical methods, or improvements in data acquisition technologyand methodology that serve this end;¥development of new computational approaches to meet challenges of complex biological systems (e.g., improved algorithmic efficiency, development of appropriate signal processing or signaldetection statistical approaches to biological data); and¥data curation efforts to correct and annotate alreadyacquired data to facilitate greaterinteroperability.these considerations suggest that expanding the notion of hypothesis may be useful. that is, thediscussion above regarding hypothesis testing refers to biological hypotheses. but to the extent that thekinds of research described in the immediately preceding list are in fact part of 21st century biology,nonbiological hypotheses may still lead to important biological discoveries. in particular, a plausibleand wellsupported computational hypothesis may be as important as a biological one and may beinstrumental in advancing biological science.today, a biological research proposal with excellent computational hypotheses may still be rejectedbecause reviewers fail to see a clearly articulated biological hypothesis. to guard against such situabox 10.6agencies and highrisk, highpayoff technology developmentan example of agency reluctance to support technology development of the highrisk, highpayoff variety isoffered by robert mullan cookdeegan:1in 1981, leroy hood and his colleagues at caltech applied for nih (and nsf) funding to support their efforts toautomate dna sequencing. they were turned down. fortunately, the weingart institute supported the initial workthat became the foundation for what is now the dominant dna sequencing instrument on the market. by 1984,progress was sufficient to garner nsf funds that led to a prototype instrument two years later. in 1989, the newlycreated national center for human genome research (nchgr) at nih held a peerreviewed competition for largescale dna sequencing. it took roughly a year to frame and announce this effort and another year to review theproposals and make final funding decisions, which is a long time in a fastmoving field. nchgr wound up fundinga proposal to use decadeold technology and an army of graduate students but rejected proposals by j. craig venterand leroy hood to do automated sequencing. venter went on to found the privately funded institute for genomicresearch, which has successfully sequenced the entire genomes of three microorganisms and has conducted manyother successful sequencing efforts; hoodõs groups, first at caltech and then at the university of washington, wenton to sequence the t cell receptor region, which is among the largest contiguously sequenced expanses of humandna. meanwhile, the army of graduate students has yet [in 1996, eds.] to complete its sequencing of the bacteriumescherichia coli.1r. mullan cookdeegan, òdoes nih need a darpa?,ó issues in science and technology xiii:2528, winter 1996.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.culture and research infrastructure379tions, funding agencies and organizations would be well served by including in the review processreviewers with the expertise to identify plausible and wellsupported computational hypotheses thatmay aid their biological colleagues in reaching a sound and unbiased conclusion about research proposals at the biocomp interface. more generally, these considerations involve changing the value proposition for what researchdollars should support. at an early point in a research fieldõs development, it certainly makes sense toemphasize very strongly the creation of basic knowledge. but as a field develops and evolves, it is notsurprising that a need to consolidate knowledge and make it more usable begins to emerge. in thefuture, a new balance will have to be struck between the creation of new knowledge and making thatknowledge more valuable to the scientific community.10.3.5.2 scale of supported workin times of limited resources (and times of limited resources are always with us), unconventionalproposals are suspect. unconventional proposals are even more suspect when they require largeamounts of money. no better example can be found than the reactions in many parts of the life sciencesresearch community to the human genome project when it was first proposedñwith a projected pricetag in the billions of dollars, the fear was palpable that the project would drain away a significantfraction of the resources available for biological research.79work at the biocomp interface, especially in the direction of integrating stateoftheart computingand information technology into biological research, may well call for support at levels above thoserequired for more traditional biology research. for example, a research project with senior expertise inboth biology and computing may well call for support for coprincipal investigators. just as biologicallaboratories generally require support for lab technicians, a biocomp project could reasonably call forprogrammers and/or system administrators. (a related point is that for a number of years in the recentpast [i.e., during the dotcom boom years] computer scientists commanded relatively high salaries.)in addition, some areas of modern life sciences research, such as molecular biology, rely on largegrants for the purchase of experimental instruments. the financial needs for instrumentation and laboratory equipment to collect the data necessary for undertake the dataintensive studies of 21st centurybiology are significant, and are often at a scale that is unaffordable to all but a small number of academicinstitutions. although large grants are not unheard of in computer science, the acrosstheboard dependence of important subfields of biology on experiment means that a larger fraction of biological researchis supported through such mechanisms than is true in computer science.to the extent that proposals for work at the biocomp interface are more costly than traditionalproposals and supported by the same agencies that fund those traditional proposals, it will not besurprising to find resistance when they are first proposed.what is the scale of increased cost that might be associated with greater integration of informationtechnology into the biological research enterprise? if one believes, as does the committee, that information technology will be as transformative to biology as it has been to many modern businesses, it willaffect the way that biological research is undertaken and the discoveries that are made, the infrastructure necessary to allow the work to be done, and the social structures and organizations necessary tosupport the work appropriately.similar transformations have occurred in fields such as high finance, transportation, publishing,manufacturing, and discount retailing. businesses in these fields tend to invest 510 percent of theirgross revenues in information technology,80 and this is with data that is well structured and understood. it is thus not unreasonable to suggest that a full integration of information technology into thebiological research enterprise might have a comparable cost. today, there is federal support for only avery small fraction of that amount.79see, for example, l. roberts, òcontroversial from the start,ó science 291(5507):11821188, 2001.80see, for example, http://www.bain.com/bainweb/publications/printerready.asp?id=17269.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.380catalyzing inquiry10.3.5.3 the review processwithin the u.s. government, there are two styles of review. in the approach relying mainly on peerreview (used primarily by nih and nsf), a proposal is evaluated by a review panel that judges itsmerits, and the consensus of the review panel is the primary factor that influencing a decision that aproposal does or does not merit funding. when program budgets are limited, as they usually are, theprogram officer decides on actual awards from the pool of proposals designated as meritworthy. in theapproach relying on program officer judgment (used primarily by darpa), a proposal is generallyreviewed by a group of experts, but decisions about funding are made primarily by the program officer.the dominant style of review mechanism in agencies that support life sciences research is peerreview. peer review is intended as a method of ensuring the soundness of the science underlying aproposal, and yet it has disadvantages. to quote an nrc report,81the current peerreview mechanism for extramural investigatorinitiated projects has served biomedicalscience well for many decades and will continue to serve the interests of science and health in the decadesto come. nih is justifiably proud of the peer review mechanism it has put in place and improved over theyears, which allows detailed independent consideration of proposal quality and provides accountabilityfor the use of funds. however, any system that focuses on accountability and high success rates inresearch outcomes may also be open to criticism for discriminating against novel, highrisk proposals thatare not backed up with extensive preliminary data and whose outcomes are highly uncertain. the problem is that highrisk proposals, which may have the potential to produce quantum leaps in discovery, donot fare well in a review system that is driven toward conservatism by a desire to maximize results in theface of limited funding resources, large numbers of competing investigators, and considerations of accountability and equity. in addition, conservatism inevitably places a premium on investing in scientistswho are known; thus there can be a bias against young investigators.almost by definition, peer review panels are also not particularly well suited to considering areas ofresearch outside their foci. that is, peer review panels include the individuals that they do preciselybecause those individuals are highly regarded as experts within their specialties. thus, an interdisciplinary proposal that draws on two or more fields is likely to contain components that a review panel in asingle field is not able to evaluate as well as those components that do fall into the panelõs field.a number of proposals have been advanced to support a track of scientific review outside thestandard peer review panels. for example, the nrc report recommended that nih establish a specialprojects program located in the office of the nih director, funded at a level of $100 million initially toincrease over a period of 10 years to $1 billion a year, whose goal would be to foster the conduct ofinnovative, highrisk research. most importantly, the proposal calls for a set of program managers toselect and manage the projects supported under this program. these program managers would becharacterized primarily by an outstanding ability to develop or recognize unusual concepts and approaches to scientific problems. review panels constituted outside the standard peer review mechanisms and specifically charged with the selection of highrisk, highpayoff projects would provideadvice and input to program managers, but decisions would remain with the program managers.research initially funded through the special projects program that generated useful results would behanded off after 35 years for further development and funding through standard nih peer reviewmechanisms. whether this proposal, or a similar one, will be adopted remains to be seen. different agencies also have different approaches to the proposals they seek. for example, agenciesdiffer in the amount of detail that they insist potential grantees provide in these proposals. dependingon the nature of the grant or contract sought, one agency might require only a short proposal of a fewpages and minimal documentation, whereas another agency might require many more pages, insistingon substantial preliminary results and extensive documentation. an individual familiar with one kind81national research council, enhancing the vitality of the national institutes of health: organizational change to meet new challenges, the national academies press, washington, dc, 2003, p. 93.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.culture and research infrastructure381of approach may not be able to cope easily with the other, and the overhead involved in coping with anunfamiliar approach can be considerable.as one illustration, the committee heard from a professor of computer science, accustomed to thensf approach to proposal writing, who reported that while many biology departments have grantadministrators who provide significant assistance in the preparation of proposals to nih (e.g., tellingthe pi what is required, drafting budgets, filling out forms, submitting the proposal), his department (ofcomputer science) was unable to provide any such assistanceñand indeed lacked anyone at all withexpertise in the nih proposal process. as a result, he found the process of applying for nih supportmuch more onerous than he had expected.10.3.6 issues related to intellectual property and publication creditissues related to intellectual property (ip) are largely outside the scope of this report. however, it ishelpful to flag certain ip issues that are particularly likely to be relevant in advancing the frontiers at theintersection of computer science and biology. specifically, because information technology enables thesensible use of enormous volumes of biological data, biological findings or results that emerge fromsuch large volumes are likely to involve the data collection work of many parties (e.g., different labs).indeed, biology as a field recognizes as significant, and even primary, the generation of good experimental data about biological phenomena. by contrast, multiparty collaborations on a comparable scaleare unusual in the world of computer science, and datasets themselves are less significant. thus, computer scientists may well be taken aback by the difficulties in negotiating permissions and credit.a second issue arises that is related to tensions between open academic research and proprietarycommercialization of intellectual advantages. because of the potential that advances in bioinformaticswill have great commercial value, there are incentives to keep some research in bioinformatics proprietary (hence, not easily accessible to the peer community, less amenable to peer review, and lessrelevant to professional development and advancement). in principle, this is not particularly different atthe biocomp interface than in any other research area of commercial value. nevertheless, the fact thattraditions and practices from two different disciplines (disciplines that are at the forefront of economicgrowth today) are involved rather than just one may exacerbate these tensions.a third point is the potential tension between making data publicly available and the intellectualproperty rights of journal publishers. for example, some years ago a part of the neuroscience community sought to build a functional positron emission tomography database. in the course of their efforts,they found that they needed to add substantial prose commentary to the image database to make ituseful. some of the relevant neuroscience journals were reluctant to give permission to use large extracts from publications in the database. to the extent that this example can be generalized, it suggeststhat efforts to build a farreaching cyberinfrastructure for biology will have to identify and deal withintellectual property issues as they arise.8282in responding to this report in draft, a reviewer argued that by taking collective action, the major research institutions couldexert strong leverage on publishers to relax their copyright requirements. today, many toprated journals require as a conditionof publication the transfer of all copyright rights from the author to the publisher. given the status of these journals, thisreviewer argued that it is a rare researcher who will take his or her paper from a toprated journal to a secondary journal withless stringent requirements in order to retain copyright. however, the researcherõs home institution could adopt a policy inwhich the institution retained the basic copyright (e.g., under the workforhire provisions of current copyright law) but allowedresearchers to license their work to publishers but not to transfer the copyright on their own accord. under such circumstances,goes the argument, journal publishers would be faced with a situation of rejecting work not just from one researcher but from allresearchers at institutions with such a policyña situation that would place far more pressure on journal publishers to relax theirrequirements and would improve the ability of researchers to share their information through digital resources and databases.the committee makes no judgment about the wisdom of this approach, but believes that the idea is worth mention.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.conclusions and recommendations38338311conclusions and recommendations11.1 disciplinary perspectives11.1.1 the biologycomputing interfacethe committee began this study with two key notions. first, it hoped to identify a field of intellectual inquiry associated with the biologycomputing interface that drew equally and bilaterally on computing and biology. second, it hoped to explicate a symmetry between computing and biology in whichthe impact of computing on biology was increasingly deep and profound and in which biology wouldhave a comparable effect on computing.both of these notions proved unfounded in certain important ways. from the standpoint of applications, technology, and practical utility, the committee saw substantial asymmetry. computing has hada huge transformational impact on biology and will span virtually all areas of life sciences research, butthe impact of biology on computing is likely to be much more targeted (i.e., affecting specific problemdomains within computing), and largescale, biologybased technology changes for computing are inthe relatively distant future if they occur at all. at the same time, the committee did find that theepistemological and conceptual frameworks of each field may have in the future some substantialinfluence on the other. the committee believes that an engineering and computational view (as discussed in chapter 6) will increasingly be recognized as an important way of looking at biologicalsystems. in a parallel though somewhat more speculative vein, the committee also believes that insightinto biological mechanisms may have important impact on how certain problems in computing can beapproached (as discussed in chapter 8).the reason for the deep and transformational impact of computing on biology is that insight intothe vast and heterogeneous datasets of 21st century biology will be possible only through the application of computing to analyze and manage those data. (this is not to deny that many quantitativesciences will contribute to biology, although this report has focused primarily on the computing dimensions.) views among biologists about where best to deploy computing resources will surely differ, butthe main contributions of computing to biology will come from new ideas for solving complex biological problems and new models for testing hypotheses; from delivering cyberinfrastructure for biologyresearch, providing ever more computing power, distributed computing and storage, complex software, faulttolerant computing, and so forth; and from training fearless scientists who can find the rightcatalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.384catalyzing inquirycollaborators for whatever difficulties arise at the frontier. that is, specific computingtobiology òtechtransferó of intellectual ideas will have some impact, but the greatest impact of computing on biologywill come from an overall acceleration of the pace of progress.to fulfill the promise of 21st century biology, research scientists from both computer and biologicalscience need to work together more extensively, more often, and more closely than ever before. asquantitative methods are increasingly adopted within the biological sciences, it will be possible toanswer a new range of scientific questions, not just to accelerate research progress. uncovering themeaning implicit in the complete sequence of the human genome to deliver on the promises of theproject for society is an obvious case.a revitalized enterprise driven by this newly trained cadre of interdisciplinary scientists andmaintained through a balance of individual investigatorinitiated and group projects along withcontinued technology and computational advances, will be able (1) to address fundamental questionsin biology such as the relationship of structure to function and the basis for homeostasis; (2) tointegrate biological knowledge across the vast scales of time, space, and organizational complexitythat characterize biology; (3) to translate basic biology to preventive, predictive, and personalizedmedicine and to extend biological knowledge to engineering soft materials and other industrialnanobiotechnology contributions; and (4) to uncover how biology can contribute to energy production and environmental restoration. the committee on the frontiers at the interface of computingand biology believes that such a vision for 21st century biology is realistic, and that the implementation of its recommendations would ensure decades of exponential progress and a major transformation of our understanding of life.on the other side of the interface, biological inspiration for new approaches to computing continuesto be important, in the sense that biology provides existence proofs that informationprocessing technology based on biochemistry rather than on silicon electronics is possible. for areas of computing thatare generally complex and unwieldy in the associated technologies available so far to address them, orareas lacking in empirical and/or theoretical knowledge, inspiration from whatever source is welcomeñand biological inspiration is most likely to be valuable in these areas. (for other areas of computing, whose intellectual terrain is well explored and for which a solid base of empirical and theoreticalknowledge is available, biological inspiration is both unnecessary and less interesting, because goodand useful solutions are available without any kind of biological connection at all.)furthermore, computer scientists tend to be most interested in the general applicability of theirwork and are often less interested in work that is relevant to only one problem domain. individualsfrom this perspective should thus understand the key difference between applicationsdriven researchand applicationsspecific research. that is, problems in the life sciences can be important drivers ofcomputer science research, and in many cases the knowledge developed in seeking solutions to theseproblems will be applicable in other domains.finally, it is worth noting one possible domain of symmetry between the two fields, although it is asymmetry of ignorance rather than one of knowledge. both computing and biology provide objects ofenormous complexity whose behavior is not well understoodñconsider the internet and a cell. it maywell turn out that studying each of these objects as systems can yield insights useful in understandingthe otherñand the same kinds of (yettobedeveloped) formalism may apply to bothñbut the jury isstill out on this possibility.11.1.2 other emerging fields at the biocomp interfaceapart from computingenabled biology and biologically inspired computing, a number of othernew areas of inquiry are also emerging at the biocomp interface, although in addition to biology andcomputing they draw from chemistry, materials science, bioengineering, and biochemistry. some ofthese efforts can be characterized loosely as different flavors of biotechnology, and three of the mostimportant are analytical biotechnology, materials biotechnology, and computational biotechnology.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.conclusions and recommendations3851.analytical biotechnology describes the application of biotechnological tools for the creation ofchemical measurement systems. examples include the creation of sensors from dnabinding proteinsfor the detection of trace amounts of arsenic and lead in ground waters, and the development ofnanoscale dna cascade switches that can be used to identify single molecular events. significantchallenges for analytical biotechnology arise in proteomics, glycomics, and lipidomics.2.materials biotechnology entails the use of biotechnological methods for the fabrication of novelmaterials with unique optical, electronic, rheological, and selective transport properties. examples include novel polymers created from genetically engineered polypeptide sequences and the formation ofnanowires and circuits from metal nanoparticles attached to a dna backbone.3.computational biotechnology focuses on the potential replacement of silicon devices withnanoscale biomolecularbased computational systems. examples include the creation of dna switchesfrom hairpin structures and the programmable selfassembly of dna tiles for the creation of memorycircuits.a common feature of many of the three new biotechnology application areas is that they all requirethe production of wellcharacterized, functional biopolymer nanostructures. the molecular precisionand specificity of the enzymatic biochemical pathways employed in biotechnology can often surpasswhat can be accomplished by other chemical or physical methodsña point that is especially relevant tothe problem of nanoscale selfassembly. it is this fine control of nanoscale architecture exhibited inproteins, membranes, and nucleic acids that researchers hope to harness with these applied biotechnologies.an important enabler of the production of such nanostructures, especially on a large scale, is theavailability of increasingly standardized and increasingly automatable fabrication techniques. in someways, the status of fabrication technologies for these nanostructures is similar to the status of integratedcircuit fabrication technology several decades ago, which evolved from a laboratory activity with trialanderror doping of individual devices to a largescale automated enterprise driven by design automation software over a period of 20 years beginning in the early 1960s.although they draw on biology and computing (along with other disciplines), the tools of theseparent disciplines are being applied by researchers in these new biotechnological areas to a differentand unrelated set of scientific interests and goals, and these areas often attract scientists with no interests in or ties to traditional biology or computing research. indeed, these researchers are likely to findintellectual homes in areas such as neuroscience, robotics, and space exploration.these new areas also have obvious relevance to computing. for example, computational biotechnology is relevant to computing in the same way that lithographic silicon fabrication technologies aretodayñunderpinning these latter technologies are understandings of fundamental physics and welldeveloped electrical engineering techniques and approaches. similarly, computational biotechnologywill draw on materials science and biochemistry as well as biology as it seeks to create highly regulardna nanoparticles, mate dna with submicron electronic structures fabricated in silicon, and createnetworks of interconnecting nanostructures with unique enzyme communication paths. analytical andmaterials biotechnologies are also relevant for enabling memsñmicroelectromechanical systems thatinteract with the physical world (taking in data through various sensors and affecting the world throughvarious actuators).11.2 moving forwardthe committee believes that the most important barriers today impeding the broader integration ofcomputing and information technology into life sciences research are cultural barriers. twentyfirstcentury biology will not entail a diminution of the central role that traditional empirical or experimentalresearch plays, but it will call for the wholehearted embrace of a style of biology that integratesreductionist biology with systems biology research. at the same time, computing and physical sciencecatalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.386catalyzing inquirypractitioners must be wary of underestimating the true complexity of biological systems and, in particular, of inappropriately applying their traditional intellectual paradigms for simplicity to biology.over the long run, a change in the culture of academic life sciences research is required to sustainthe approaches needed for 21st century biology, due to the increased need for disparate skill sets andcollaborative approaches, a change that emphasizes interdisciplinary teams that integrate biology andcomputing expertise. for this reason, the main focus of this chapterõs conclusions and recommendationsconcern actions that can accelerate the required cultural shift. by contrast, reflecting the committeeõsview that the impact of biological research is likely to be more modest in scope and scale, the conclusions and recommendations place less emphasis on biologyõs impact on computing. (in this light,chapters 48 of this report should not be seen as laying out a research agenda for computingenabledbiology or for biologyinspired computing, but rather as suggesting some of the areas in which thefrontiers of the interface have been pushedñand that still hold considerable intellectual interest.)11.2.1 building a new communitythe most important target of promoting cultural change is people. thus, it should be a key objectiveof science policy makers to create a large, multitalented population of individuals who can act as theintellectual translators and mediators along the frontier, a group that will directly foster interdisciplinary research and technology development. true for any discipline or research area involving disparateskill sets, such an approach is especially critical at the interface between the fields of biology andcomputing because these areas are enjoying the most rapid growth and intellectual progress. bothjunior and senior talent must be cultivated, the former to be the basis of a next generation ready todevelop and exploit the technology and conduct the science, and the latter to serve in mentorship andleadership roles.this message is not a new oneñindeed, private programs such the burroughswellcome foundation interfaces in sciences have avidly sought the development of community. nevertheless, it remainstrue that despite many studies, reports, and proclamations, universities and federal funding agencieshave fallen short of the goal of fully facilitating a range of interdisciplinary science and minimizing thebirth pains associated with new hypotheses and directions.1an essential aspect of this community is the ability to build on each otherõs work. indeed, the mostadvanced and sophisticated cyberinfrastructure imaginable will be ineffective if different laboratoriesand researchers are not motivated or are unwilling to work together or to share data and other information. formal collaborations between individual laboratories or researchers do exist, of course, but theseexist entirely on the basis of individually negotiated arrangements between consenting parties. a different, and complementary, model of working together is one in which individuals researchers contributeto and draw from an entire research community. in spirit, this model is the familiar one of publishingresearch articles and supporting information (data, software) for others to cite and use as appropriate intheir own researchñand the dominant ethos of the new community should be one of sharing ratherthan withholding.this section provides some core principles on how individuals and institutions might help tosupport and nurture such work. the core principles described here may come across as òmotherhoodand apple pie,ó but it is often the case that such motherhood is not honored as fully as one might thinkappropriate. the committee does recognize the centrality of providing appropriate incentives for hon1for example, a report was prepared by the national institutes of health (nih) and the national science foundation (nsf) inaugust 2001 addressing many of the cultural issues described in chapter 10. this report on training in bioengineering andbioinformatics, assessing bioengineering and bioinformatics research training, education, and career development, recommended thatmeasures be taken to (1) increase the number of fellowships and institutional training grants at all career levels that includequantitative, computational biology and integrative systems modeling; (2) include funds to support faculty with complementaryexpertise (e.g., computer scientists to teach biologists); and (3) support the development of curricula. in the intervening 2 years,the importance of continued efforts in these areas has not diminished.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.conclusions and recommendations387oring these principles. both institutions and funding agencies have important roles to play in providingincentives for change when these principles are not honored and for continuity when they are. thus, thecore principles for institutions (section 11.2.3) and for funding agencies (section 11.4.1) should be seenpartly in this light.11.2.2 core principles for practitionersthe following items are offered as advice to current and prospective researchers at the biocompinterface. these workers include those seeking to retrain themselves to work at the biocomp interface(e.g., a postdoctoral fellow with a computer science background working in a biology laboratory), thosefacilitating such retraining (e.g., the director of a biology laboratory employing such a postdoc), andthose who collaborate as peers with others (e.g., a tenured professor of computer science working witha tenured professor of biology on some interesting problem). practitioners should:¥respect their partners. neither the biologist who sees the computer scientist only as a craftsmanwriting computer programs for data analysis nor the computer scientist who sees the biologist as aprovider of dirty and unreliable data shows respect for the other. scientists with quantitative backgrounds and scientists with biomedical backgrounds must work as peers if their collaborations are to besuccessful.¥have reasonable expectations. oneõs intellectual partners in an interdisciplinary endeavor will havediffering and often unfamiliar intellectual paradigms. both vocabulary and epistemology will be different, and a respect for other ways of looking at the world reflects an understanding that paradigms canbe different for very sound reasons.¥avoid hype. in the quest for funding and attention, practitioners need to maintain a high degree ofquestioning to avoid hype, unrealistic expectations, or empty promises.¥donõt complain. complaining to close colleagues about the apparently poor science practiced byother disciplines further reinforces xenophobic arrogance and chauvinism. when the other parties sensesuch arrogance, the trust needed to achieve scientific collaboration is no longer available.2¥seek new techniques and intellectual inspiration everywhere. both biology and computer science havetraditions of applying other disciplines to their problems. for example, leeuwenhoekõs optical microscope led to the discovery of cells, electrical recording devices revealed the voltagegated channels inneuronal signaling, and knowledge of crystallography uncovered the helical structure and code ofdna. computer science, originating from a marriage between electrical engineering and mathematics,continues to maintain close intellectual connections to these disciplines.¥nurture young talent. the key to longterm growth of a new field is the ability to sustainand nurture young scientists working in that field. to the extent that attention can be focused onyoung scientists (e.g., targeting this generation with wellplaced, exciting, and novel fundingopportunities), problems of competing for funds with senior groups working on classical topicscan be reduced.the committee understands that these principles will have different meaning to researchers atdifferent stages of their careers. for those early in their careers, these recommendations should betaken as a checklist of things to keep in mind as they engage with colleagues and seek support.however, these items are also relevant to senior researchers who serve as role models for theiryounger colleagues.2g. wiederhold, òscience in two domains,ó unpublished working paper, department of computer science, stanford university, march 2002, updated february 2003.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.388catalyzing inquiry11.2.3 core principles for research institutionsthe following items are offered as advice to institutions that are supporting work at the biocompinterface. these institutions include academic laboratories, research centers, and departments, as wellas business or commercial operations with a research component. collectively, these items are based onthe barriers to collaboration and community discussed in chapter 10. however, no attempt has beenmade to specifically align recommendations with barriers because in most cases, the correspondence ismanytomany rather than manytoone or onetomany.relevant institutions should:¥attract and retain professionals with quantitative, computational, and engineering skills to work inbiological fields. as a rule, recruitment and retention will require reasonable career tracks that hold thepromise of longterm stability and upward mobility. if good individuals are to be attracted to andretained in any enduring interdisciplinary area, they must have career opportunities that offer thepotential for growth. for example, these individuals must be assured that their intellectual work at theinterface will be fairly evaluated. such issues are matters of academic survival for many young faculty,and if processes are not put into place explicitly that ensure an appropriately rigorous but still fairevaluation process, promising faculty may well have strong disincentives to pursue research at theinterface. a corollary is that traditional departments often see considerable opportunity cost in supporting (and granting tenure to) individuals who do not fit squarely in their centers of gravity (section10.3.3); thus, independent support for researchers with interdisciplinary interests, or support that cannot be converted to individuals with traditional interests, helps to remove the threat that departmentsmay see.¥support retraining efforts. because much of the computing talent required at the biocomp interface will have to come from individuals with substantial prior experience in computing, retraining willbe an essential part of efforts to build the talent base. individuals considering retraining will be moremotivated to do so if funding agencies and tenure and promotion committees wishing to support thesefaculty members recognize that retooling takes some time to be successful and do not penalize them forlowered productivity during such periods.¥develop curricula for interdisciplinary teaching of quantitative, computational, and engineering sciencesmade relevant to the biocomp interface. note the desirability of such curricula being made available inmultiple formatsñonline versus in class, 2week courses versus semesterlength courses, and so onñaswell as on multiple topics. over the long run, it is likely that immersion in these curricula will becomea natural part of the educational process for all budding biologists, but today, obtaining this background requires some special effort.¥facilitate networking. especially for newcomers to a line of work, intellectual connection to othersplays an important role in their integration into the new community. an institution can promoteinformal knowledge exchange and the establishment of social relationships on campus through onsiteseminars for likeminded individuals. it can also facilitate offcampus connections by providing support for travel to tutorials, workshops, and seminars.¥nurture partnerships. it is desirable for senior scientists from different intellectual backgrounds towork at the interface and for peer relationships between biologists and computer scientists to develop.partnerships are best undertaken in close proximity with intense interaction, and even small issues suchas office arrangements (e.g., whether or not a computer scientist has an office or a desk in the laboratoryof a collaborator or partner) can seriously inhibit the development of close partnerships.3 many scenarios could promote partnerships, such as sabbatical visits and the establishment of positions at cen3for example, a computer scientist developing software to aid in the analysis of biological data would be well advised to spendenough time in the laboratory to understand the actual needs of his or her biologist colleagues. software delivered òover thetransomó is unlikely to be used easily, a point suggesting that there is more to software design than the development of anappropriate algorithm.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.conclusions and recommendations389ters of excellence. partnerships with industry could ensure sabbaticals in complementary work environments and stimulate knowledge dissemination to commercial applications.¥recognize collaborative work. a corollary of partnerships is that experts from disparate disciplineswill collaborate in publication. institutions thus have a responsibility to provide fair and appropriateevaluation measures for tenure and promotion cases in which the individuals involved have undertaken large amounts of collaborative work. for example, departments may have to be induced toexpand their definitions of tenurable work, or universities may have to establish extradepartmentalmechanisms for granting and holding tenure outside of traditional departments.¥maintain excellence. research at the biocomp interface is inherently interdisciplinary, and evaluation of such research faces all of the problems described above. nevertheless, problem domains thatare at the interface of two disciplines can attract not only highly talented individuals who see interestingand important problems but also individuals of lesser talent who are unable to meet the exactingstandards of one discipline and are seeking a home where the standards of acceptance are lower.individuals in the first category are to be sought and cherishedñindividuals in the second categoryought to be shunned.¥provide mentors. mentors play a strong role in the success of any retraining effort. however,mentoring individuals who have an established track record of success in another field is different. forexample, such individuals may be less able to work autonomously and more likely to flail or driftwithout an activist mentor than someone with a background in the same field. shared mentorships maymake particular sense in these circumstances, as illustrated by the burroughswellcome requirementthat fellowship awardees have a mentor from outside the department of primary appointment.¥reward good behavior. it has been observed that behavior that is rewarded institutionally is behavior that tends to take hold and to be internalized. the institutions with which individual researchers areassociated can play important roles in providing such rewards, especially with respect to the principlesdescribed in section 11.2.2.11.3the special significance of educational innovationat the biocomp interfacethe pursuit of 21st century biology will require a generation of biologists who can appreciatefundamental statistical approaches, evaluate computational tools and use them appropriately, andknow how to choose the best collaborators from the quantitative sciences as a whole. to support theeducation of this generation, an integrative education, whether formal or informal, will be needed.many reports have acknowledged a need for broader training.4 increasingly, bioinformatics programs at both the undergraduate and the graduate level do entail study in mathematics, computerscience, and the natural sciences.11.3.1 contentthe committee fully supports these trends and encourages them further, with the strong caveat thatan appropriate curriculum to deal with the interface of computing and biology should not simply be theunion of course requirements from multiple departments. courses and other work that deal explicitlywith the integrative issues are necessary, and one of the most important skills that such interdisciplinary courses can teach is the ability to communicate among the relevant disciplines. this does not entailsimply learning the jargon of each one (though this is, of course, essential), but also interleaving thetraining in such a way that the student continually sees and explores various parallels between the4see, for example, national research council, bio2010: undergraduate education to prepare biomedical research scientists, thenational academies press, washington, dc, 2003.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.390catalyzing inquirydifferent fields of study. later, in a collaboration, the ability to identify, explain, and exploit theseparallels will be valuable.cultural barriers should be discussed and addressed specifically. where it seems easy to dismisssome math or physics as irrelevant to biology, case studies can be assembled to show successes andcontrast these with failures or counterproductive avenues. where it seems easy to dismiss biology as toodetail oriented and reductionistic, similar case studies showing the need to understand minute detailsof the living machinery are also necessary.it is broadly agreed that an essential element of 21st century biology is the (re)introduction ofquantitative science to the biological science curriculum. the committee recognizes, however, that suchreintroduction should not be equated with an abstract, theoretical approach devoid of experimentationor phenomenology, and educational programs for 21st century biology must provide sound footing inquantitative science alongside a clear understanding of the intricacies of biology.in light of the discussions in chapter 6 regarding the view of biological organisms as engineeredentities, the committee believes that students of 21st century biology would benefit greatly from somestudy of engineering as well. in this view, the committee emphasizes most strongly its support for therecommendations of the bio2010 report for exposure to engineering principles (discussed in chapter10), at the earliest possible time in the training of life scientists. just as engineers must construct physicalsystems to operate in the real world, nature also must operate under these same constraintsñphysicallawsñto òdesignó successful organisms. despite this fundamental similarity, biology students rarelylearn the important analysis, modeling, and design skills common to engineering curricula nor a suite oftopics such as engineering thermodynamics, solid and fluid dynamics, control theory, and so forth, thatare key to the engineerõs (and natureõs) ability to design physical systems.the particular area of engineering (electrical, mechanical, computer, and so forth) is probably muchless relevant than exposure to essential principles of engineering design: the notion of tradeoffs inmanaging competing objectives, control systems theory, feedback, redundancy, signal processing, interface design, abstraction, and the like (box 11.1). ready intellectual access to such notions is likely toenable researchers in this area to search for higherlevel order in the data forest. indeed, as biologycontinues to examine the systemwide functioning of a large number of interacting components, engineering skills may become necessary for successful biological research.11.3.2 mechanismsthe committee believes that the availability of individuals with significant computing expertise isan important limiting factor for the rate at which the biological sciences can absorb such expertise.5 thefield, to include both basic and applied life sciences research, is extraordinarily large and dwarfs mostother fields outside of engineering itself; thus, influx from other fields is not likely to result in largescaleinfusion of computing expertise. only integrated education of new researchers, along with some retraining of existing researchers, can bring benefits of the computing to a large segment of that world,and previous calls from groups such as biomedical information science and technology initiative(bisti) that a new generation of 21st century researchers must be trained remain compelling, true, andoverdue.given this perspective, it is appropriate to offer educational opportunities across a broad front.educational opportunities should span a range in several dimensions, including the following:¥time and format. monthly lectures or seminars, shortduration workshops (of several weeks),survey courses, undergraduate minors, undergraduate majors, graduate degrees, and postdoctoral5this belief is not based on the existence of a òshortageó or òscarcityó in the sense that economists generally recognize. rather,it is rooted in the premise that most of biology could benefit intellectually from the integration of significant computing expertise, and the observation that such integration is more the exception than the rule when taken across biology writ large.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.conclusions and recommendations391(re)training focusing on the biocomp interface can serve to motivate interest (when they require littleinvestment or time commitment) or to serve a strong professional interest (when the time commitmentrequired is substantial).6 shortterm opportunities for crossdisciplinary òpollinationó workshops thatbring together fields from both sides of the interface and provide a vehicle for tutorials and othereducational exchanges are particularly useful in that they have a low cost of entry for participants; thus,those who are dabbling can be enticed more easily.¥content. although genome informatics is perhaps the most obvious topic, computational techniques and approaches will become increasingly relevant to all aspects of biological researchñandeducational opportunities should target a wide range of subfields in biology.¥target audience. given the need for more computing expertise in biology, it is appropriate toprovide instruction at multiple levels of sophistication in different fields. some research biologists havesubstantial informal computing experience but would benefit greatly from more formal exposure; suchbox 11.1some engineering ideas and concepts that biologists may find useful¥control theory (feedback, optimization, game theory)¥model design¥signal processing (gain, signaltonoise, crosstalk)¥engineering thermodynamics and energy¥optimal design theory¥modularity (and protocols)¥robustness¥multiscale and largescale stochastic simulation¥network theory or graph theory¥fluid and solid dynamics or mechanics¥òcollective behavioró from physics¥reverse engineering¥computational complexity (decidability, pnp)¥information theory, source and channel coding¥dynamical systems: dynamics, bifurcation, chaos¥statistical physics: phase transitions, critical phenomena6in this regard, the model of statistics as a discipline may offer a good example for the way in which bioinformatics mightbecome a discipline of its own. many universities offer three types of programs in statistics. the first and most formal program isdesigned for those aspiring to become professional, academic statisticiansñthat is, those aspiring to become researchers in thefield of statistics. this program usually culminates in the ph.d. degree and establishes an absolutely sound theoretical understanding of the foundations of statistics. the second program is intended for individuals who intend to become professionalapplied statisticiansñthat is, those who will work in industry, perhaps in research, and whose primary responsibilities willinvolve carrying out statistical analyses using established statistical methods. often, individuals pursuing this degree track willstop with a masterõs degree and in some cases even with a bachelorõs degree. the third program involves a set of coursesintended for individuals who will be getting a degree in another field, but who have a need for significant understanding ofstatistical methods so that those methods might be applied in the individualõs home field. in very large universities, sometimesthese thirdtrack courses are specialized even further so that we might see courses in business statistics, biological statistics, oreven medical statistics. similarly, it seems reasonably clear that in the field of bioinformatics there will always be a need forresearchers, whose primary interest will be in devising new algorithms, new models, and new methods in bioinformatics. therewill also be a need for applied bioinformaticiansñthose whose primary responsibility will be in applying establishedbioinformatics methods to current projects in biology and biotechnology. it also seems reasonable to suppose that there will bethose whose careers in other disciplines will be enhanced by some knowledge of bioinformatics. (these latter two program typesmay have to provide the biological knowledge to those trained primarily in computation or the computational overview to thosetrained primarily in biology.)catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.392catalyzing inquiryindividuals are in an obviously different situation than those whose only exposure to computing isspreadsheets and word processors.the development of such educational opportunities generally requires resources, such as releasetime; assistance in compiling lecture notes, assembling readings, or grading; funding for developingonline courses, travel to workshops, and so on. furthermore, it is desirable to share the outcomes ofsuch development with the academic community (e.g., in the form of online courses, published books,and open commentary about successes and failures). funding agencies can also provide incentives forsuch cooperative efforts by giving higher funding priority to research proposals that are put forward inpartnerships between or among universities.11.4 recommendations for research funding agenciesthe committee believes that it is possibleñand feasibleñfor agencies to support work at thebiocomp interface that serves to develop simultaneously (1) fundamental knowledge that enablesbroad advances in biology; (2) technical innovations that help to improve the quality of life and enhanceindustrial competitiveness; and (3) the creation and sustenance of a critical mass of talentedscientistsand engineers intellectually capable and professionally positioned to work creatively at the biocompinterface and to train new generations effectively.funding agencies and nongovernmental supporters of research have traditionally been able toinfluence the course of research through the allocation of resources to particular research fields, and thecommittee believes that funding at the biologycomputing interface is no exception. this support hasmade important contributions in the past, and the committee urges that such support be continued andexpanded.11.4.1 core principles for funding agenciesrecognition of the importance in focusing on the biocomp interface amplifies earlier agencycentered studies and reflects its unprecedented richness. responding to the opportunities, the scientificcommunity, private foundations, and the federal government have taken the first steps in recognizingthis enormous intellectual opportunity.however, no single agencyñlet alone any individual program, directorate, institute, center, orofficeñowns the science or the excitement and promise at the interface between computing and biology. neither can a single agency by itself establish and sustain a process to realize the grand opportunities. in their growing commitment to this frontier science effort, the defense advanced research projectsagency (darpa), the national science foundation (nsf), the national institutes of health (nih), andthe department of energy (doe) each have unique objectives and existing expertise. to exploit thepotential fully, the agencies, more than ever before, will have to collaborate and also seek (formal orinformal) partnerships with private foundations and industry. extensive interactions including fullyopen, joint planning exercises and shared support for technical workshops will be central to truecoordination at the agency level.as is the case for individuals and institutions, a number of core principles provide good desideratafor the funding policies and practices of agencies. again, these core principles are not particularlynewñbut remain essential to realizing goals at the biocomp interface. of course, how these principlesare instantiated is key.to obtain maximum impact, funding agencies and foundations should pay appropriate attention tothe following items. agencies and foundations should:¥support awards that can be used for retraining purposes. while a number of agencies have supportedsuch awards for individuals at early stages of their careers, these programs are fewer in number than incatalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.conclusions and recommendations393the past. also, to the best of the committeeõs knowledge, there are no programs that explicitly targetsenior faculty for retraining at the biocomp interface, although, as noted in section 10.2.2.6, nih doessupport a retraining program open to scientists of many backgrounds to undertake biomedical research. to the extent that such programs continue to exist, agencies should seek to publicize thembeyond their usual core constituencies.¥balance quality and excellence against openness to new ideas in the review process. intellectual excellence is central. yet especially in interdisciplinary work, it is also important to invest in work thatchallenges existing assumptions about how research in the field òshouldó be conductedñand the problem is that traditional review mechanisms often have a hard time distinguishing between proposals forsuch work and proposals for work that simply does not meet any reasonable standard of excellence.this point suggests that agencies wishing to support work at the biocomp interface would be wise tofind review mechanisms that can draw on individuals who collectively have the relevant interdisciplinary expertise and, as importantly, an appropriate forwardlooking view of the field.¥encourage team formation. it is important not to discriminate against teamresearched articles inindividual performance evaluations and to provide incentives for universities to reward multiple members of crossdisciplinary teams of investigators. under todayõs arrangements, work performed by anindividual as part of a team often receives substantially less credit than work performed by an individual working alone or with graduate students.¥provide research opportunities for investigators at the interface who are not established enough to obtainfunding on the strength of their track record alone. in these instances, balance must be struck betweentaking a chance on an unproven track record and shutting down nonfruitful lines of inquiry. oneapproach is to set time limits (a few years) on grants made to such individuals, requiring them tocompete on their own against more established investigators after the initial period. (as in otherfields, the duration of òa few yearsó is established by the fact that it is unreasonable to expectsignificant results in less time, and norms of regular funding set an upper limit for this encouragement of work outside the boundaries.)¥use funding leverage to promote institutional change. that is, agencies can give priority or differentialadvantages to proposals that are structured in certain ways or that come from institutions that demonstrate commitments to change. for example, priority or preference could be given to proposals thatñinvolve coprincipal investigators from different disciplines;ñoriginate in institutions that offer grant awardees tenuretrack faculty appointments withminimal teaching responsibilities (as illustrated by the burroughswelcome career awards (section 10.2.2.5.2));ñhave significant and active educational efforts or programs at the biocomp interface; andñmake data available to the larger biological community in standard forms that facilitate reuseand common interpretation.7 (this action is predicated on the existence of such standards, andagencies should continue to support efforts to develop these common data standards.)¥use publication venues to promote institutional change. funding agencies could require as a condition of publication that authors deposit the data associated with a given publication into appropriatecommunity databases in accordance with relevant curation standards. they could also insist that published work describing computational models be accompanied by assurances that detailed code inspection of models is possible under an appropriate nondisclosure agreement.7the committee notes without comment that the desire on the part of science agencies to promote wider data sharing andinteroperability may conflict with requirements emanating from other parts of the federal government with regard to information management in biomedical research. while science agencies are urging data sharing, other parts of the government canimpose restrictions on sharing biomedical data associated with individual human beings in the name of privacy, and theserestrictions can have significant impact on the architecture of biomedical information systems. in some cases, these regulatorycompliance issues have such impact that biomedical scientists have strong incentives to introduce a paper step into their datamanagement processes in order to escape some of the more onerous consequences of these regulations for their informationsystems.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.394catalyzing inquiry¥support cyberinfrastructure for biological research. though the national science foundation hastaken a lead in this area, the issue of supporting cyberinfrastructure for biological research transcendsany single agency. chapter 7 discussed the importance of data repositories and digital libraries incyberinfrastructure, and it is in these areas that other agencies have important roles to play. across theboard, agencies engaged in supporting biological research will need to support mechanisms for longterm data storage and for continuous curation and annotation of the information resources gathered inpublicly supported research for 21st century biology to reach its full potential as a global distributedintellectual enterprise.¥recognize quality publicly. given the role of peer recognition in the value sets of most scientists(especially in their earlier years), public recognition of innovative work can be a strong motivator.public recognition can take many formsñthough by definition the number of people that canbe recognized is necessarily limited. for example, outstanding researchers can be invited to givekeynote addresses at important conferences or profiled in reports to congress or other importantpublic documents.¥recognize the costs of providing access to computing and information resources. especially at thebiocomp interface, collaboration between peers as compared to an investigation conducted by anindividual researcher almost always requires larger grants. researchers need more support for computing and information technology as well as the expertise needed to exploit those capabilities and, ininstances that push the computing state of the art, support for highlevel expertise as well.¥define specific challenge problems that stretch the existing state of the art but are nevertheless amenable toprogress in a reasonable time frame. an agency could pose challenge problems drawn from the problemdomains described in chapter 9. any number of such challenge problems would be arbitrary, but aselected few goals of broad impact would influence more complete participation by the community andmake further funding opportunities by other agencies more likely. note that when common test sets orother common criteria can be provided or used, clearer metrics for success can be established. a corollary is that agencies should obtain community buyin with respect to the specifics of such problems. (asone example, the doe office of biological and environmental research specified what microbes totackle for complete genome sequencing through a series of òwhich bugó workshops to obtain community input on the projects that would be best.)¥work with other agencies. different agencies bring to the table different types of expertise, and forwork at the interface, multiple kinds of expertise are always necessary. thus, agency partnerships (suchas the current collaboration between nihõs national institute of general medical sciences (nigms) andnsfõs mathematical and physical sciences directorate) may allow proposals at the interface to beevaluated more fairly and ongoing projects to be overseen more effectively.8¥provide the funding necessary to capitalize on the intellectual potential of 21st century biology. chapters27 of this report have sought to demonstrate the broad impact of computing and information technology on biology. however, a necessary condition to realize this impact is a funding stream that isadequate in magnitude and sustained over long enough periods. as noted in section 10.3.5.2, a benchmark for comparison is that spending in informationintensive fields such as finance is on the order of5 to 10 percent of overall budgets. a second necessary condition is the use of a peer review process thatis broadly sensitive to the perspectives of researchers in the new field and is willing to take chances onnew ideas and approaches. as always, the public sector should focus on growing the seed corn for bothpeople and ideas on which the future depends. finally, although the committee would gladly endorsean increased flow of funding to the furtherance of a truly integrated 21st century biology, it doesunderstand the realities of a budgetconstrained environment.8this partnership between the nigms and the nsf seeks to award 20 grants in mathematical biology and anticipates morethan $24 million in awards over 5 years. nigms supports research and training in the basic biomedical sciences. nsf fundsmathematical and other quantitative sciences such as physics, computer science, and engineering. see http://www.nigms.nih.gov/news/releases/biomath.html.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.conclusions and recommendations395the following sections are addressed to specific funding agencies.11.4.2 national institutes of healthas the largest funder of life sciences research, the nih has a special responsibility to support andfacilitate the building of bridges between biology and other disciplines, especially including computing.the nih has already taken a number of commendable steps in seeking to collaborate with otheragencies, including the formal nigms partnership with nsf for mathematical biology mentioned in theprevious section and other less formal partnerships with nsf and doe for structural biology and thenational center for research resources (ncrr)nsf collaborations in instrumentation. as noted inchapter 10, a national research council report in 2003 called for nih to increase investment in highrisk, highpotentialpayoff life sciences research that would be supported outside the usual nih peerreview system.such steps, and others like them, are to be encouraged. at the same time, nih must addressobstacles in a number of other areas that impede the building of bridges between biology and computing. one important issue is that cooperation across organizational boundaries within nih leaves muchto be desired. translational medicine will not arise from funding mechanisms that isolate narrow slicesof human biology, and yet the nih structure is oriented toward specific diseases and body functions.9no component of a human works separately, in isolation. most diseases are not singlegene defects,most proteins act in macromolecular assemblies, organ systems interact by chemical messengers, theimmune system and the circulatory system not only work together but impact all organs of the body,and so on. the nih structure has been successful for many years, but the fact remains that its organizational structure tends to place similar restraints on crossinstitute support for collaborative research(box 11.2).a consequence of organization of research fields in biology by subfield (e.g., by disease, or by bodyfunction) is that efforts that can benefit the entire community may suffer, even though specialization isnecessary to achieve depth of knowledge. the true value of the largescale deployment of cyberinfrastructureñand especially its data componentsñis that cyberinfrastructure spans disciplines to integrate findings in one subfield with findings in another subfieldñto connect information from onesubfield to another subfield, perhaps even via a third subfield. in the absence of explicit direction andcoordination, cyberinfrastructure in one subfield is likely to be incompatible in important ways withcyberinfrastructure designed and deployed in another. achieving coordination is likely to require alevel of cooperation across agencies that is substantially greater than has historically been true. it willalso require a level of planning and agency involvement in the actual design of the cyberinfrastructurethat does not typically happen in the funding of research, in which the role of program officers isprimarily to ensure a fair assessment of the science by peer reviewers. in supporting cyberinfrastructure,program officers must act as procurement officers on behalf of the overall scientific community, and notjust as impartial brokers of an independent disciplinefocused review process.9there are 20 institutes within the national institutes of health, including the national cancer institute (nci); the nationaleye institute (nei); the national heart, lung, and blood institute (nhlbi); the national human genome research institute(nhgri); the national institute on aging (nia); the national institute on alcohol abuse and alcoholism (niaaa); the nationalinstitute of allergy and infectious diseases (niaid); the national institute of arthritis and musculoskeletal and skin diseases(niams); the national institute of biomedical imaging and bioengineering (nibib); the national institute of child health andhuman development (nichd); the national institute on deafness and other communication disorders (nidcd); the nationalinstitute of dental and craniofacial research (nidcr); the national institute of diabetes and digestive and kidney diseases(niddk); the national institute on drug abuse (nida); the national institute of environmental health sciences (niehs); thenational institute of general medical sciences (nigms); the national institute of mental health (nimh); the national instituteof neurological disorders and stroke (ninds); the national institute of nursing research (ninr); and the national library ofmedicine (nlm).catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.396catalyzing inquirynih also supports some of the most scientifically sophisticated research environments in the world.as noted in the botsteinsmarr report,10 it is in these environments that it makes the most sense to trainthe leaders of the new generation of biologists with computing expertise. these environments aregenerally mature enough to support the conduct of interdisciplinary research at the interface, and awidespread geographical diffusion of young scientists with such expertise will help to generate thebroad impact sought by nih.perhaps the most important barrier of all is the philosophy that governs much of the current studygroup approach to proposal review. for historical reasons, the most important and prominent supporters of life sciences researchñsuch as nihñhave focused almost exclusively on hypothesistestingresearchñresearch that investigates wellisolated biological phenomena that can be controlled or manipulated and hypotheses that can be tested in straightforward ways with existing methods. this focusis at the center of reductionist biology and has undeniably been central to much of biologyõs success inthe past several decades.at the same time, the nearly exclusive focus on hypothesis testing has some important negativeconsequences. for example, experiments that require breakthrough approaches are unlikely to be directly supported. just as importantly, advancing technology that could facilitate research is almostalways done as a sideline. thus, investigators must often disguise an attempt to undertake the development of tools or models of great generality by applying them to some (any!) biological system. subsequent citations of such papers are almost always for the part that explains the new tool or model ratherthan the phenomenon to which the tool or model was applied.box 11.2the shapiro report on the structure and organization of thenational institutes of healtha 2003 national research council report on the structure and organization of nih came to conclusions andmade recommendations that are consistent with the view of nih described in this report. specifically, theearlier report noted:[t]here is a high payoff potential for carefully selected large and smallscale strategic projects that require theparticipation of numerous organizations working in partnership. . . . wellplanned, broadbased, transnih programs will be necessary to meet most effectively scientific or public health needs. . . . furthermore, there is no formalmandate for nih to identify, plan, and implement such crosscutting strategic initiatives. [such crosscutting initiativesare necessary because] scientific mechanisms, risk factors, and social and behavioral influences on health anddisease cut across traditional disease categories. many patients have multiple chronic conditions, so a patientcentered approach to health care and health promotion will sometimes require integration and synergy across[institutes and centers]. [such issues] lend themselves to a strategic coordinated transnih response in which multiple institutes could collaborate on a research plan that cuts across administrative structures in terms of planning,funding, and sharing and disseminating results. . . . proteomics . . . is [an] example [of such an issue]. . . . [c]oncertedtransnih work on the assessment of existing and emerging technology platforms and database formats utilizingreference specimens, could help to advance the whole field and guide nihsupported studies.the report went on to recommend that initially 5 percent of the nih budget and eventually 10 percent shouldbe allocated to the support of such transnih initiatives.source: national research council, enhancing the vitality of the national institutes of health: organizational change to meet newchallenges, the national academies press, washington, dc, 2003, pp. 8486.10nih working group on biomedical computing, the biomedical information science and technology initiative, june 1999. available at http://www.nih.gov/about/director/060399.htm.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.conclusions and recommendations397this has had a considerable chilling effect in general on what could have been, but the impact isparticularly severe for implementation of computational technologies within the biological sciences.that is, in effect as a cultural aspect of modern biological research, technology development to facilitateresearch is not considered real research and is not considered a legitimate focus of a standard grant.thus, even computing research that would have a major impact on the advancement of biologicalscience is simply not done.the committee believes that 21st century biology will be based on a synergistic mix of reductionistand systems biologies. for systems biology researchers, the committee emphasizes that hypothesistesting research will continue to be central in providing experimental verification of putative discoveriesñand indeed, relevant as much to studies of how components interact as to studies of componentsthemselves. thus, disparaging rhetoric about the inadequacies and failures of reductionist biology andoverheated zeal in promoting systems biology should be avoided. for researchers more oriented toward experimental or empirical work, the committee emphasizes that systems biology will be central informulating novel, interesting, and in some cases, counterintuitive hypotheses to test. the point suggests that agencies that have traditionally supported hypothesistesting research would do well to casta wide òdiscoveryó net that supports the development of alternative hypotheses as well as research thatsupports traditional hypothesis testing.11.4.3 national science foundationthe primary largescale initiative of nsf relevant to 21st century biology is its cyberinfrastructureeffort. efforts in this area, including major community databases, collaborative research networks, andinterdisciplinary modeling efforts, will require grants that are larger than the directorate for biologicalsciences (bio) of nsf has traditionally made, as well as greater continuity and stability. in particular,cyberinfrastructure entails personnel costs (e.g., for programmers, systems administrators, and staffscientists with the necessary computing expertise) that are not associated with the usual biosupportedgrant. as for continuity, windows for support must be consistent with the practical considerations toachieve success. fiveyear awards and initial review at that point against specific milestones anddeliverables to the community are essential, and only at longer intervals should there be open calls forproposals and competitive processes, save in the case of a resource failing to live up to communityexpectations.11the professional biological community at large has at least two important roles to play with respectto cyberinfrastructure. first, it must articulate its needs and explicate how it can best exploit the resources that cyberinfrastructure will make available. second, it must develop a consensus on the expectations that cyberinfrastructure facilities must meet if they are to be continued. society events (e.g.,annual meetings) provide a forum for such discussions to take place.11.4.4 department of energythe doeõs office of science supports a number of programs in genomic studies and structuralbiology (as described in chapter 10). this office has the capacity to provide sufficient funds and a stableenvironment, but doing so has been a challenge in its overall institutional setting. the committeebelieves that the payoffs for doe missions will be extraordinary from the biology supported by theoffice of science, but success requires that priority be given to stable, longterm programs.11in principle, review provisions could be analogous to the sunset considerations for nsfsupported science and technologycenters and engineering research centers.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.398catalyzing inquiry11.4.5 defense advanced research projects agencyof all the federal agencies, darpa appears to be the most heavily involved in exploring thepotential of biology for computing. chapter 8 describes a variety of potential influences of biology oncomputing (the term òapplications of biology for computingó would be promising too much), but intruth, the ultimate value of biology for changing computing paradigms in deep and fundamental waysis as yet unproven. nevertheless, various biological attributesñrobustness, adaptation, damage recovery, and so onñare so desirable from a computing point of view that any intellectual inquiry is valuableif it can contribute to artificial humanly purposive systems with these attributes.in other words, investigations that consider the impact of biology on computing areñin the vernacularñhighrisk, highpayoff studies. they are high risk because biology is not prescriptive in itscontributions and success is far from ensured. they are high payoff because computers that possessattributes associated with biological systems would be enormously valuable. it is for this reason thatthey do logically fall into programs supported by darpa, which has a long tradition of supportinghighrisk, highpayoff work as part of its research portfolio. (as noted in chapter 10, nsf also sponsorsa small grants exploratory research program that supports highrisk research on a small scale.)from the committeeõs perspective, the highlevel goals articulated by darpa and other agenciesthat support work related to biologyõs potential contribution to computing seem generally sensible.this is not to say that every proposal supported under the auspices of these agenciesõ programs wouldnecessarily have garnered the support of the committeeñbut that would be true of any research portfolio associated with any program.one important consequence of supporting highrisk research is that it is unlikely to be successful inthe short term. researchñparticularly of the highrisk varietyñis often more òmessyó and takes longerto succeed than managers would like. managers understandably wish to terminate unproductive linesof inquiry, especially when budgets are constrained. however, shortterm success cannot be the onlymetric of the value of research, and when it is, funding managers invite hyperbole and exaggeration onthe part of proposal submitters, and unrealistic expectations begin to characterize the field. thosebelieving the hyperbole (and those contributing to it as well) thus overstate the importance and centrality of the research to the broader goal of improving computing. when unrealistic expectations are notmet (and they will not be met, almost by definition), disillusionment sets in, and the field becomesdisfavored from both a funding and an intellectual standpoint.from this perspective, it is easy to see why support for fields can rise rapidly only to drop precipitously a few years later. wild budget fluctuations and an unpredictable funding environment that changesgoals rapidly can damage the longterm prospects of a field to produce useful and substantive knowledge.funding levels do matter, but programs that provide steady funding in the context of broadly stated butconsistent intellectual goals are more likely to yield useful results than those that do not.thus, the committee believes that in the area of biologically inspired computing, funding agenciesshould have realistic expectations, and these expectations should be relatively modest in the near term.intellectually, their programs should continue to take a broad view of what òbiological inspirationómeans. funding levels in these areas ought to be established on a òlevelofeffortó basis (i.e., whatdarpa believes is a reasonable level of effort to be expended in this area), taking into account thenumber of researchers doing and likely to do good work in this area and the potential availability ofother avenues to improved computing. also, programmatic continuity should be the rule, with playingrules and priorities remaining more or less constant in the absence of profound scientific discovery ortechnology advances in the area.11.5 conclusions regarding industry over the past decade, the commercial sector has provided important validation for the propositionthat information technology (it) can have a profound impact on the life sciences. as noted in chaptercatalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.conclusions and recommendations39910, there are a host of firms, ranging in size from small startups to established multibilliondollarcompanies that have significant investments in research efforts and products, that make substantial useof it in support of medical and pharmaceutical business.nevertheless, the committee is aware that some large life science companies (e.g., large pharmaceutical companies) have not found their investments in information technology living up to their expectations. some such companies have reported investing a great deal of money and time in bioinformaticssoftware and are now looking for and failing to find economic justification for further investment.the hype of the genome era was as intoxicating to many drug companies, it seems, as the internetwas to mainstream investors, with just as much a comedown. there is a growing realization that theavailability of genomic information is not, by itself, sufficient to lead directly to immediately profitabledrug breakthroughs, regardless of the it available to help manage and analyze that information. indeed, many bottlenecks in drug discovery remain that result from the lack of fundamental biologicalknowledge about specific expression and pathways. whereas the initial expectation was that the genome could be mined for likely drug targets, todayõs approach involves a greater tendency to start withthe biology that is known to select likely targets, and then to look to the genome to find genes thatinteract with those targets.the committee believes that bioinformaticsñand broader uses of information technologyñarelikely to have a positive effect on drug discovery in the long run, but that those enterprises looking toinvestments in it for shortterm gain are likely to continue to be disappointed. commercial advantagesto the use of it will accrue from its integration into the entire process, from gene discovery to clinicaltrials, benefiting both the entire process and the local situation to which information technology isapplied. also, because of rapidly increasing biological knowledge, the promise of discovering appropriate drug targets in the genome remains, although it is likely to be realized primarily in the long term.bioinformatics will also enable a more precise genomebased identification of individuals susceptible toa given drugõs side effects, possibly providing a basis for excluding them from clinical trials andpharmaceutical applications involving that drug.11.6 closing thoughtsthe impact of computing on biology could fairly be considered a paradigm change as biologyenters the 21st century. twentyfive years ago, biology saw the integration of multiple disciplines fromthe physical and biological sciences and the application of new approaches to understand the mechanisms by which simple bacteria and viruses function. the impact of the early efforts was so significantthat a new discipline, molecular biology, emerged, and many biologists, including those working at thelevel of tissues or systems and whole organisms, came to adopt the approaches and often even thetechniques. molecular biology has had such success that it is no longer a discipline but simply part ofbioscience research itself.today, the revolution lies in the application of a new set of interdisciplinary tools: computationalapproaches will provide the underpinning for the integration of broad disciplines in developing aquantitative systems approach, an integrative or synthetic approach to understanding the interplay ofbiological complexes as biological research moves up in scale. bioinformatics provides the glue forsystems biology, and computational biology provides new insights into key experimental approachesand how to tackle the challenges of nature. in short, computing and information technology applied tobiological problems is likely to play a role for 21st century biology that is in many ways analogous to therole that molecular biology has played in biological research across all fields for the last quarter centuryñand computing and information technology will likely become embedded with biological research itself.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.appendixescatalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.appendix a403403athe secrets of life:a mathematicianõs introduction to molecular biologynote: this appendix is a reprint of chapter 1 of the national research council report calculating the secrets of life: contributions of the mathematical sciences to molecular biology (national academy press, washington, dc, 1995), copyright 1995 by thenational academy of sciences.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.appendix a405catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.406catalyzing inquirycatalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.appendix a407catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.408catalyzing inquirycatalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.appendix a409catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.410catalyzing inquirycatalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.appendix a411catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.412catalyzing inquirycatalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.appendix a413catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.414catalyzing inquirycatalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.appendix a415catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.416catalyzing inquirycatalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.appendix a417catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.418catalyzing inquirycatalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.appendix a419catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.420catalyzing inquirycatalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.appendix a421catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.422catalyzing inquirycatalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.appendix a423catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.424catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.425catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.426catalyzing inquirycatalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.appendix a427catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.428catalyzing inquirycatalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.appendix b429429bchallenge problems in bioinformatics andcomputational biology from other reportsb.1 grand challenges in computational biology (david searls)11.protein structure prediction2.homology searches3.multiple alignment and phylogeny construction4.genomic sequence analysis and genefindingb.2 opportunities in molecular biomedicine in the era ofteraflop computing (klaus schulten et al.)21.study proteinprotein and proteinnucleic acid recognition and assembly2.investigate integral functional units (dynamic form and function of large macromolecular andsupramolecular complexes)3.bridge the gap between computationally feasible and functionally relevant time scales4.improve multiresolution structure prediction5.combine classical molecular dynamics simulations with quantum chemical forces6.sample larger sets of dynamical events and chemical species7.realize interactive modeling8.foster the development of biomolecular modeling and bioinformatics9.train computational biologists in teraflop technologies, numerical algorithms, and physical concepts10.bring experimental and computational groups in molecular biomedicine closer together.1d. searls, ògrand challenges in computational biology,ó computational methods in molecular biology, s. salzberg, d. searls,and simon kasif, eds., elsevier science, 1998.2k. schulten, g. budescu, f. molnar, opportunities in molecular biomedicine in the era of teraflop computing, nih resource formacromolecular modeling and bioinformatics, march 34, 1999, rockville, md; see http://whitepapers.zdnet.co.uk/0,39025945,60014729p39000617q,00.htm.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.430catalyzing inquiryb.3 workshop on modeling of biological systems(peter kollman and simon levin)3challenging issues that span all areas of modeling systemsa. integrating data and developing models of complex systems across multiple spatial and temporal scales¥scale relations and coupling¥temporal complexity and coding¥parameter estimation and treatment of uncertainty¥statistical analysis and data mining¥simulation modeling and predictionb. structurefunction relationships¥large and small nucleic acids¥proteins¥membrane systems¥general macromolecular assemblies¥ceilular, tissue, organismal systems¥ecological and evolutionary systemsc. image analysis and visualization¥image interpretation and data fusion¥inverse problems¥two, three and higherdimensional visualization and virtual realityd. basic mathematical issues¥formalisms for spatial and temporal encoding¥complex geometry¥relationships between network architecture and dynamics¥combinatorial complexity¥theory for systems that combine stochastic and nonlinear effects often in partially distributed systemse. data management¥data modeling and data structure design¥query algorithms, especially across heterogeneous data types¥data server communication, especially peertopeer replication¥distributed memory management and process managementb.4 workshop on nextgeneration biology: the role of nextgenerationcomputing (shankar subramaniam and john wooley)4exemplar challenges for bioinformatics and computational biology1.full genomegenome comparisons2.rapid assessment of polymorphic genetic variations3òmodeling of biological systems,ó p. kollman and s. levin (chairs), a workshop at the national science foundation, march 14and 15, 1996, available at http://www.resnet.wm.edu/~jxshix/math490/modeling%20of%20biological%20systems.htm.4s. subramaniam and j. wooley, doensfnih 1998 workshop on nextgeneration biology: the role of next generationcomputing, available at http://cbcg.lbl.gov/ssicsb/nextgenbiows.html.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.appendix b4313.complete construction of orthologous and paralogous groups of genes4.structure determination of large macromolecular assemblies/complexes5.dynamical simulation of realistic oligomeric systems6.rapid structural/topological clustering of proteins7.prediction of unknown molecular structures; protein folding8.computer simulation of membrane structure and dynamic function9.simulation of genetic networks and the sensitivity of these pathways to component stoichiometry and kinetics10.integration of observations across scales of vastly different dimensions and organization toyield realistic environmental models for basic biology and societal needsb.5 technologies for biological computeraided design (masaru tomita)51.enzyme engineering: to refine enzymes and to analyze kinetic parameters in vitro2.metabolic engineering: to analyze flux rates in vivo3.analytical chemistry: to determine and analyze the quantity of metabolites efficiently4.genetic engineering: to cut and paste genes on demand, for modifying metabolic pathways5.simulation science: to efficiently and accurately simulate a large number of reactions6.knowledge engineering: to construct, edit and maintain large metabolic knowledge bases7.mathematical engineering: to estimate and tune unknown parametersb.6 top bioinformatics challenges (chris burge et al.)61.precise, predictive model of transcription initiation and termination: ability to predict whereand when transcription will occur in a genome2.precise, predictive model of rna splicing/alternative splicing: ability to predict the splicingpattern of any primary transcript3.precise, quantitative models of signal transduction pathways:ability to predict cellular responseto external stimuli4.determining effective proteindna, proteinrna and proteinprotein recognition codes5.accurate ab initio structure prediction6.rational design of small molecule inhibitors of proteins7.mechanistic understanding of protein evolution: understanding exactly how new protein functions evolve8.mechanistic understanding of speciation: molecular details of how speciation occurs9.continued development of effective gene ontologiessystematic ways to describe the functionsof any gene or protein10.(infrastructure and education challenge)11.education: development of appropriate bioinformatics curricula for secondary, undergraduate,and graduate educationb.7 emerging fields in bioinformatics (patricia babbitt)71.data storage and retrieval, database structures, annotation2.analysis of genomic/proteomic/other highthroughput information5m. tomita, òtowards computer aided design (cad) of useful microorganisms,ó bioinformatics 17(12):10911092, 2001.6c. burge, òbioinformaticists will be busy bees,ó genome technology, no. 17, january, 2002. available (by free subscription) athttp://www.genometechnology.com/articles/viewarticle.asp?article=20021023161457.7p. babbitt et al., òa very very very short introduction to protein bioinformatics,ó august 2223, 2002, university of california, san francisco, available at http://baygenomics.ucsf.edu/education/workshop1/lectures/w1.print2.pdf.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.432catalyzing inquiry3.evolutionary model building and phylogenic analysis4.architecture and content of genomes5.complex systems analysis/genetic circuits6.information content in dna, rna, protein sequences and structure7.metabolic computing8.data mining using machine learning tools, neural nets, artificial intelligence9.nucleic acid and protein sequence analysesb.8 ten grand challenges (sylvia spengler)81.the origin, structure, and fate of the universe2.the fundamental structure of matter3.earthõs physical systems4.the diversity of life on earth5.the tree of life6.the language of life7.the web of life8.human ecology9.the brain and artificial thinking machines10.integrating earth and human systems11.a knowledge server for planetary managementresearch across domains: data¥information managementñhuman evolution continued¥exponential increase in data and information across domains¥access to information across domainsñas or more important than the information itself¥integration of data across knowledge domains¥apply analytical tools across knowledge domains¥modeling of complex systems¥simulation of phenomenañdescriptive science becomes predictive scienceresearch across domains: people¥share data across disciplines¥build and use analytical and modeling tools across disciplines¥work in collaborative, crossdomain groupsresearch across domains: time¥realtime data access, integration, and analysis¥realtime modeling and effects prediction¥realtime dissemination of research results¥realtime testing by research community¥realtime policy discussions¥realtime policy decisions8s. spengler, lawrence berkeley national laboratory, personal communication to john wooley, january 3, 2005.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.appendix b433b.9 grand challenges in biomedical computing (john a. board, jr.)9biomedical applications from coupling imaging and modeling¥realtime noninvasive threedimensional imaging of many body systems¥realtime generation of threedimensional patientspecific models¥multipletechnology (multimodal) imaging and modeling¥wholeorgan modeling¥multipleorgan system modeling¥patientspecific modeling of organ anomalies¥model support for (partial) restoration of hearing, coarse vision, and locomotion (via both paralyzed and artificial limbs)all of these applications make use of:¥threedimensional models¥increasingly refined grids and increasing levels of tissue discrimination¥anatomically realistic models¥specialpurpose hardware for visualization¥distributed computing techniques.b.10 accelerating mathematicalbiological linkages:report of a joint nsfnih workshop (margaret palmer et al.)10list of top ten problems at the mathematical biology interface1.model multilevel systems: from the cells in people, to human communities in physical, chemical, and biotic ecologies.2.model networks of complex metabolic pathways, cell signaling, and species interactions.3.integrate probabilistic theories: understand uncertainty and risk.4.understand computation: gaining insight and proving theorems from numerical computationand agentbased models.5.provide tools for data mining and inference.6.address linguistic and graph theoretical approaches.7.model brain function.8.build computational tools for problems with multiple temporal and spatial scales.9.provide ecological forecasts.10.understand effects of erroneous data on biological understanding.b.11 grand challenges of multimodal biomedical systems (j. chen et al.)11science challenges1.allow early detection of where and when an infectious disease outbreak occurs, whether it isnaturally occurring or manmade, in real time.9j.a. board, jr., ògrand challenges in biomedical computing, highperformance computing in biomedical research, t.c.pilkington, b. loftis, j.f. thompson, s.l.y. woo, t.c. palmer, and t.f. budinger, eds., crc press, boca raton, fl, 1993.10m. palmer et al., òaccelerating mathematicalbiological linkages: report of a joint nsfnih workshop,ó february 2003,available at www.maa.org/mtc/nihfeb03report.pdf.11j. chen et al., ògrand challenges of multimodal biomedical systems,ó ieee circuits and systems magazine, pp. 4652, 2ndquarter 2005, available at http://gsp.tamu.edu/publications/pdfpapers/papcasmagmbm.pdf.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.434catalyzing inquiry2.develop multidimensional drug profiling databases to facilitate drug discovery and to identifybiomarkers for diagnosis and monitoring the progress of individual disease treatments.3.connect activities and events derived from cellular processes to highlevel cognitions.4.support personalized medical care and clinical decision for patientstechnology challenges and enabling technologies1.formalization of biological knowledge into predictive models for systems biology and systembased analysis2.interdisciplinary training3.development of open source, multiscale modality informatics toolkitsb.12 the department of energyõs genomes to life program1221st century biology requiring òbiocompó tools1.population models, symbiosis, and stability2.discrete growth models3.reaction kinetics4.biological oscillators and switches5.coupled oscillators6.reactiondiffusion, chemotaxis, and nonlocality7.oscillatorgenerated wave phenomena and patterns8.spatial pattern formation with population interactions9.mechanical models for generating pattern and form in development10.evolution and morphogenesis a mathematica for molecular, cellular, and systems biology1.core data models and structures [database management]2.optimized functions [core libraries]3.scripting environment [e.g., python, perl, ruby, etc.]4.database accessors and builtin schemas5.simulation interfaces6.parallel and accelerated kernels7.visualization interfaces (for information visualization and scientific visualization)8.collaborative workflow and group use interfaceshierarchical biological modeling environment1.genetic sequences2.molecular machines3.molecular complexes and modules4.networks + pathways [metabolic, signaling, regulation]5.structural components [ultrastructures]6.cell structure and morphology7.extracellular environment8.populations and consortia12r. stevens, ògtl software infrastructure: a computer science perspective,ó undated presentation, argonne national laboratory, available at www.doegenomics.org/compbio/mtg12202/rickstevens.ppt.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.appendix b435modeling and simulation challenges for 21st century biology1.modeling activity of single genes2.probabilistic models of prokaryotic genes and regulation3.logical models of regulatory control in eukaryotic systems4.gene regulation networks and genetic network inference in computational models and applications to largescale gene expression data5.atomisticlevel simulation of biomolecules6.diffusion phenomena in cytoplasm and extracellular environment7.kinetic models of excitable membranes and synaptic interactions8.stochastic simulation of cell signaling pathways9.complex dynamics of cell cycle regulation10.model simplificationb.13 highperformance computing, communication, and informationtechnology grand challenges (late 1980s, early 1990s)13computing applications to map and sequence human genome1.understanding protein folding2.predicting structure of native protein3.exhaustive discovery and analysis of cancer genes4.molecular recognition and dynamics5.drug discovery13committee on physical, mathematical, and engineering sciences of the federal coordinating council for science, engineering, and technology, u.s. office of science and technology policy, fy1992 blue book: grand challenges: high performance computing and communicationsñthe fy 1992 u.s. research and development program.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.appendix c437437cbiographies of committee members and staffc.1committee membersjohn c. wooley (chair) is the associate vice chancellor for research, university of california at sandiego (ucsd), an adjunct professor in pharmacology, and in chemistry and biochemistry, and a strategicadvisor and senior fellow of the san diego supercomputer center. he received his ph.d. degree in 1975 atthe university of chicago, working with al crewe and robert uretz in biological physics. dr. wooleycreated the first programs within the u.s. federal government for funding research in bioinformatics andin computational biology and has been involved in strengthening the interface between computing andbiology for more than a decade. for the new ucsd california institute for telecommunication andinformation technology (cal(it)2), dr. wooley directs the biology and biomedical layer or applicationscomponent, termed digitallyenabled genomic medicine (degem), a step in delivering personalizedmedicine in a wireless clinical setting. his current research involves bioinformatics and structuralgenomics, while his principal objectives at ucsd are to stimulate new research initiatives for largescale,multidisciplinary challenges. he also collaborates in developing scientific applications of informationtechnology and highperformance computing; creating industryuniversity collaborations; expandingapplied life science opportunities, notably around drug discovery; and establishing a biotechnology andpharmacology science park on ucsdõs health sciences campus zone.adam p. arkin is a faculty scientist in computational and theoretical biology at lawrence berkeleynational laboratory, an assistant professor of bioengineering and chemistry at the university of california, berkeley, and an investigator of the howard hughes medical institute. his focus is on detailedmodeling of genetic and biochemical networks with emphasis on developmental systems. the arkinlaboratory applies theoretical and computational analyses from dynamical systems, stochastic processes, chemical kinetics, and statistical mechanics and methods from molecular biology to determinethe principles of cellular signal processing and to aid in design of custom cellular circuitry that may, forexample, act as sensitive biosensors.eric brill is a researcher in the machine learning and applied statistics group at microsoft research. his research interests include natural language processing (primarily empirical natural language processing), speech recognition and spoken language systems, machine learning, and artificialcatalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.438catalyzing inquiryintelligence. some specific research topics include lexical disambiguation, parsing, classifier combination, spelling correction, and language modeling. before joining microsoft, he was an assistant professorof computer science at johns hopkins university. he has served on the editorial board of computationallinguistics and the journal for artificial intelligence research. dr. brill received his ph.d. in computerscience from the university of pennsylvania in 1993.robert m. corn is a professor in the department of chemistry at the university of california,irvine. dr. corn is a leader in the development and application of surfacesensitive spectroscopic techniques such as surface plasmon resonance (spr) imaging, optical second harmonic generation (shg),and polarization modulation fourier transform infrared (pmftir) spectroscopy. his primary researchinterests include the study of biopolymer (e.g., dna, protein) adsorption onto surfaces and the chemical modification of surfaces for the creation of ultrathin films and adsorptionbased biosensors. professor corn also has ongoing research projects in the implementation of dna computing algorithms atsurfaces and the study of ion transfer processes at liquidliquid interfaces. he received a b.a. in chemistry summa cum laude in 1978 from the university of california, san diego, and earned a ph.d. in 1983from the university of california, berkeley, under the direction of professor herbert l. strauss in theapplication of ftir to the study of motion in molecular solids. from 1983 to1984, professor corn was avisiting scientist at the ibm research laboratory in san jose, california, where he applied the techniques of surface plasmonenhanced raman scattering and optical she to electrochemical surfaces. in1985, professor corn moved to wisconsin where he was a member of the analytical sciences division ofthe department of chemistry and the water chemistry program until 2004. in july of 2004, he moved tothe university of california, irvine, where he joined the department of chemistry. professor corn is acofounder of two companies: gwc technologies, inc., maker of spr instrumentation and other surfacespectroscopic equipment, and gentel biosurfaces, inc.chris diorio is an associate professor of computer science and engineering at the university ofwashington. his research focuses on building electronic systems that employ the computational andorganizational principles used in the nervous systems of living organisms. this work on neurallyinspired computing includes studies of computing with action potentials, silicon learning systems, andimplantable computers. he also works on highspeed circuit design. dr. diorio teaches courses in bothdigital electronics and integratedcircuit (ic) design, and is developing new course material in twoareas: (1) alternative computing paradigms, including neural, quantum, and dna computers, and (2)digital ic design at microwave clock frequencies. he received a national science foundation (nsf)presidential early career award in 1999. dr. diorio was awarded a 5 year packard foundation fellowship in science and engineering in 1998 and also an nsf career award that same year. in 1996, he wasawarded the electron devices societyõs (edsõs) paul rappaport award for the best paper in an instituteof electrical and electronics engineers eds publication. he completed his doctoral research in electricalengineering at the physics of computation laboratory, california institute of technology, in 1997. dr.diorio has also served as a senior staff engineer for trw, inc., and as a senior staff scientist for american systems corporation. he received his b.a. in physics from occidental college in 1983 and his m.s.in electrical engineering in 1984 from the california institute of technology.leah edelsteinkeshet is a professor of mathematics at the university of british columbia. shereceived her ph.d. in 1982 from the weizmann institute of science in rehovot, israel, specializing inapplied mathematics and working with professor lee a. segel. she is a member of the mathematicsdepartment and the institute of applied mathematics at the university of british colombia. she is alsoa former president of the society for mathematical biology. although her main area of interest ismathematical biology, dr. edelsteinkeshet works in several areas, including the molecular biology ofthe cytoskeleton, the dynamics of swarming and social organisms and, more recently, models forneuroinflammation in alzheimerõs disease and pathogenesis of type 1 (autoimmune) diabetes.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.appendix c439mark h. ellisman is professor in the department of neurosciences at the school of medicine andthe department of bioengineering, director of the national center for microscopy and imaging research at ucsd, and chair of the san diego supercomputer center (sdsc) executive committee. dr.ellismanõs research focuses on cellular neurobiology and the dynamic interplay between structure andfunction in the nervous system, with a focus on excitable membrane properties and enabling remoteaccess to largescale scientific instrumentation. at ucsd, dr. ellisman is director of the center forresearch in biological structure and director of the neurosciences laboratory for neurocytology. since1997, he has been the neuroscience thrust leader and crossdisciplinary coordinator for the nationalpartnership for advanced computational infrastructure. dr. ellisman is a member of the americanassociation for the advancement of science, society for neurosciences, and american institute formedical and biological engineering. he has served on numerous editorial boards and has been associate editor of the journal of neurocytology since 1980. dr. ellisman is a also grant reviewer for organizations such as the national institutes of health and the national science foundation, and a consultant forassociations such as the association for advanced technology in the biomedical sciences and pfizer.he has published numerous journal and conference articles and technical reports. he holds a ph.d.degree in biology and an m.a. degree in neurophysiology both from the university of colorado,boulder, and an a.b. degree with honors from the university of california, berkeley.marcus w. feldman is a professor of biological sciences at stanford university. he uses appliedmathematics and computer modeling to simulate and analyze the process of evolution. specific areas ofresearch include the evolution of complex genetic systems that can undergo both natural selection andrecombination and the evolution of learning as one interface between modern methods in artificialintelligence and models of biological processes, including communication. he also studies the evolutionof modern humans using models for the dynamics of molecular polymorphisms, especially dna variants. he is managing editor of theoretical population biology and associate editor of genetics and ofcomplexity. dr. feldman is a member of the american society of naturalists, and the american societyof human genetics, and a fellow of the american academy of arts and sciences. he received his b.sc.in 1964 from the university of western australia, his m.sc. in 1966 from monash university, australia,and his ph.d. in biomathematics from stanford in 1969.david k. gifford is a professor of electrical engineering and computer science at the massachusettsinstitute of technology. he is working on the analysis of rna expression data using graphical models.professor gifford has also developed programmed mutagenesis, a technique for programmaticallyrewriting dna sequences by incorporating sequencespecific oligonucleotides into newly manufactured strands of dna. dr. gifford serves as group leader for the programming systems researchgroup at the mit laboratory for computer science. this group is dedicated to finding new ways ofprogramming existing systems and developing new programmable systems. the groupõs efforts concentrate on combining existing technologies and inventing new ones to deliver new ways of computingin selected areas: programming language development; information discovery, retrieval, and distribution; algebraic and computational video; and most recently, computation using biological substrates.dr. gifford earned his s.b. in 1976 from mit and his m.s. and ph.d. in electrical engineering fromstanford university in 1978 and 1981, respectively. he is a tenured member of the mit faculty, which hejoined in 1982. he was appointed to the karl van tassel career development chair at mit in 1990.takeo kanade received his ph.d. in electrical engineering from kyoto university, japan, in 1974.after being on the faculty in the department of information science, kyoto university, he joined thecomputer science department and robotics institute in 1980. he became associate professor in 1982, afull professor in 1985, the u.a. and helen whitaker professor in 1993, and a university professor in1998. he has been the director of the robotics institute since 1992. he served as the founding chairman(19891993) of the robotics ph.d. program at carnegie mellon university, probably the first of its kind incatalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.440catalyzing inquirythe world. dr. kanade has worked in multiple areas of robotics, ranging from manipulator, sensor,computer vision, and multimedia applications to autonomous robots, with more than 200 papers onthese topics. he is the founding editor of the international journal of computer vision. dr. kanadeõsprofessional honors include election to the national academy of engineering, a fellow of the ieee, afellow of the acm, and a fellow of the american association of artificial intelligence, and severalawards including the joseph engelberger award, yokogawa prize, jara award, otto franc award,and marr prize award.stephen s. laderman is the manager of the molecular diagnostics department, dedicated tomolecular biology, biochemistry, computational biology, and engineering for the development of genetic, genomic, and proteomic analysis systems for biomedical research and molecular diagnostics. heearned his b.a. in physics, magna cum laude, from wesleyan university in 1976 and his ph.d. inmaterials science and engineering from stanford university in 1983. dr. laderman was a postdoctoralscholar from 1982 to 1984 at stanford university and exxon research corporation. before joiningagilent labs, he worked in a variety of positions at hewlettpackard laboratories. dr. laderman was amember of the basic energy sciences advisory committee panel on novel, coherent light sources andchair of the selection committee for the george e. pake prize of the american physical society. he iscurrently a member of the international society for computational biology, american society of human genetics, american physical society, american chemical society, american association for theadvancement of science, and a senior member of the ieee.james s. schwaber is associate professor of pathology, anatomy and cell biology at thomasjefferson university medical college (tju) and is director of the daniel baugh institute for functionalgenomics and computational biology at tju. prior to joining tju in 2000, he was technical leader andresearch fellow of the computational biology program in the core genomics group at dupont. hisinterest is in neuron and neuronal network modeling (e.g., of cardiorespiratory control functions) and,in particular, how alterations in neuron properties will be dependent on input activity over time, bylinking the molecular processes activated by synaptic inputs to cell physiology. his research groupfocuses on computational analysis of genomic datasets from functionally identified neurons as a cornerstone to support modeling of the adaptive intracellular response to synaptic inputs. currently the workis related to systems analysis of gene regulatory circuits, the modeling of neuronal inputs into thesecircuits as modular patterns of transcription factor activation, and the central issue of discoveringprinciples that relate gene output to functional phenotype (electrophysiology; models of ion fluxes) atthe systems level.c.2staff membersherbert s. lin is senior scientist and senior staff officer at the computer science and telecommunications board (cstb), national research council (nrc) of the national academies, where he has beenthe study director for major projects on public policy and information technology. these studies includea 1996 study on national cryptography policy (cryptographyõs role in securing the information society), a1991 study on the future of computer science (computing the future), a 1999 study of defense department systems for command, control, communications, computing, and intelligence (realizing the potential of c4i: fundamental challenges), and a 2000 study on workforce issues in high technology (building aworkforce for the information economy). prior to his nrc service, he was a professional staff member andstaff scientist for the house armed services committee (1986 to 1990), where his portfolio includeddefense policy and arms control issues. he also has significant expertise in math and science education.he received his ph.d. in physics from mit in 1979. avocationally, he is a longtime folk and swingdancer, and a poor magician. in addition to his cstb work, he is published in cognitive science, scienceeducation, biophysics, and arms control and defense policy.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.appendix c441robin schoen is the director of the board on agriculture and natural resources (banr) of the national academies. prior to joining banr in march 2005, she was a senior program officer for theacademiesõ board on life sciences, where she directed several studies, including discovery of antiviralsagainst smallpox; stem cells and the promise of regenerative medicine; the national plant genome initiative:objectives for 20032005; sharing publicationrelated data and materials: responsibilities of authorship in thelife sciences; and a banr study titled predicting invasions of nonindigenous plants and plant pests. robinreceived a b.s. in biology and chemistry from frostburg state college, maryland, and an m.a. in scienceand technology policy from george washington university.c.3report coordinatorruss biagio altman is a professor of genetics, bioengineering and medicine (and of computer scienceby courtesy) at stanford university. his primary research interests are in the application of computingtechnology to basic molecular biological problems of relevance to medicine. he is currently developingtechniques for collaborative scientific computation over the internet, including novel user interfaces tobiological data, particularly for pharmacogenomics. other work focuses on the analysis of functionalmicroenvironments within macromolecules and the application of nonlinear optimization algorithmsfor determining the structure and function of biological macromolecules, particularly the bacterialribosome. dr. altman holds an m.d. from stanford medical school, a ph.d. in medical informationsciences from stanford, and an a.b. from harvard college. he has been the recipient of the u.s.presidential early career award for scientists and engineers, an nsf career award, and the westernsociety of clinical investigation annual young investigator award. he is a fellow of the americancollege of physicians and the american college of medical informatics. he is a pastpresident andfounding board member of the international society for computational biology, an organizer of theannual pacific symposium on biocomputing, and an associate editor of the journal bioinformatics. hecurrently directs the stanford center for biomedical computation and the training program in biomedical informatics, and he won the stanford medical school graduate teaching award in 2000.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.appendix d443443dworkshop participantsto assist in its informationgathering efforts, the committee on frontiers at the interface of computing and biology held three workshops on various topics at the interface of computing and biology.the participants in these workshops are listed below.workshop on bioinspired computing andenabling technologies (january 2001)rick adrion, national science foundationroger brent, molecular sciences instituteanne condon, university of british columbiamita desai, national science foundationstephanie forrest, university of new mexicobob full, university of california, berkeleyjames j. hickman, national science foundationken johnson, gentel, inc.tom knight, massachusetts institute of technologychristof koch, california institute of technologypatricia mead, national academy of engineeringallen northrup, cepheidshankar sastry, defense advanced research projects agencyshihab shamma, university of marylandsylvia spengler, national science foundationgary strong, national science foundationerik winfree, california institute of technologycatalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.444catalyzing inquiryworkshop on challenges and opportunitiesin data management (march 2001)helen berman, rutgers universitypat brown, stanford universitybarb bryant, millennium predictive medicinemike colvin, lawrence livermore national laboratorystephen dahms, california state university program for education and research in biotechnologydan davison, bristol myers squibbjoe deken, southern illinois universityskip garner, southwestern medical centerjim gray, microsoftdavid haussler, university of california, santa cruzdick karp, university of california, berkeleydavid kingsbury, discovery biosciences corporationmichael marron, national center for research resourcesdan masys, university of california, san diegorichard morris, national institute of allergy and infectious diseasesbernhard palsson, university of california, san diegolarry smarr, university of california, san diegopaul spellman, stanford universitysylvia spengler, national science foundationgary strong, national science foundationart toga, university of california, los angeleschris wood, los alamos national laboratoryworkshop on modeling of biological systems (may 2001)rick adrion, national science foundationruzena bajcsy, national science foundationeugene bruce, national science foundationmarvin cassman, national institutes of healthsu chung, geneticxchangejim collins, boston universityjoe decken, university of california, san diegomita desai, national science foundationdrew endy, molecular sciences institutewarren ewens, university of pennsylvaniajoe felsenstein, university of washingtonteresa headgordon, lawrence berkeley national laboratoryjames hickman, national science foundationsri kumar, defense advanced research projects agencysimon levin, princeton universitymichael marron, national center for research resourcesandrew mcculloch, university of california, san diegogarrett odell, university of washingtondave polidori, entelos, inc.terry sejnowski, salk institutesylvia j. spengler, national science foundationgary strong, national science foundationjohn j. tyson, virginia polytechnic institute and state universitycatalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.appendix b445445what is cstb?as a part of the national research council, the computer science and telecommunications board(cstb) was established in 1986 to provide independent advice to the federal government on technicaland public policy issues relating to computing and communications. composed of leaders from industry and academia, cstb conducts studies of critical national issues and makes recommendations togovernment, industry, and academic researchers. cstb also provides a neutral meeting ground forconsideration of complex issues where resolution and action may be premature. it convenes invitationaldiscussions that bring together principals from the public and private sectors, ensuring consideration ofall perspectives. the majority of cstbõs work is requested by federal agencies and congress, consistentwith its national academies context.a pioneer in framing and analyzing internet policy issues, cstb is unique in its comprehensivescope and effective, interdisciplinary appraisal of technical, economic, social, and policy issues. from itsearly work in computer and communications security, cyberassurance and information systems trustworthiness have been crosscutting themes in cstbõs work. cstb has produced several reports regarded as classics in the field, and it continues to address these topics as they grow in importance.to do its work, cstb draws on some of the best minds in the country, inviting experts to participatein its projects as a public service. studies are conducted by balanced committees without direct financialinterests in the topics they are addressing. those committees meet, confer electronically, and buildanalyses through their deliberations. additional expertise from around the country is tapped in arigorous process of review and critique, further enhancing the quality of cstb reports. by engaginggroups of principals, cstb obtains the facts and insights critical to assessing key issues.the mission of cstb is torespond to requests from the government, nonprofit organizations, and private industry for adviceon computer and telecommunications issues and from the government for advice on computerand telecommunications systems planning, utilization, and modernization;monitor and promote the health of the fields of computer science and telecommunications, with attention to issues of human resources, information infrastructure, and societal impacts;initiate and conduct studies involving computer science, computer technology, and telecommunications as critical resources; andfoster interaction among the disciplines underlying computing and telecommunications technologies and other fields, at large and within the national academies.more information about cstb can be obtained online at http://www.cstb.org.catalyzing inquiry at the interface of computing and biologycopyright national academy of sciences. all rights reserved.
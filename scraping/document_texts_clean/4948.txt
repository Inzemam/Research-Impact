detailsdistribution, posting, or copying of this pdf is strictly prohibited without written permission of the national academies press. (request permission) unless otherwise indicated, all materials in this pdf are copyrighted by the national academy of sciences.copyright © national academy of sciences. all rights reserved.the national academies pressvisit the national academies press at nap.edu and login or register to get:œ œ 10% off the price of print titlesœ special offers and discountsget this bookfind related titlesthis pdf is available at sharecontributorshttp://nap.edu/4948evolving the high performance computing and communicationsinitiative to support the nation's informationinfrastructure136 pages | 8.5 x 11 | paperbackisbn 9780309052771 | doi 10.17226/4948committee to study high performance computing and communications: status of amajor initiative, national research councilevolving the high performance computing and communications initiative to support the nation's information infrastructurecopyright national academy of sciences. all rights reserved.evolving the high performancecomputing and communicationsinitiative to support the nation'sinformation infrastructurecommittee to study high performance computing and communications: status of a majorinitiativecomputer science and telecommunications boardcommission on physical sciences, mathematics, and applicationsnational research councilnational academy presswashington, d.c. 1995ievolving the high performance computing and communications initiative to support the nation's information infrastructurecopyright national academy of sciences. all rights reserved.notice: the project that is the subject of this report was approved by the governing board of the national research council, whose members are drawn from the councils of the national academy of sciences, the national academy of engineering, and the institute of medicine.the members of the committee responsible for the report were chosen for their special competences and with regard for appropriate balance.this report has been reviewed by a group other than the authors according to procedures approved by a report review committee consisting of members of the national academy of sciences, the national academy of engineering, and the institute of medicine.the national academy of sciences is a private, nonprofit, selfperpetuating society of distinguished scholars engaged in scientific andengineering research, dedicated to the furtherance of science and technology and to their use for the general welfare. upon the authority ofthe charter granted to it by the congress in 1863, the academy has a mandate that requires it to advise the federal government on scientificand technical matters. dr. bruce alberts is president of the national academy of sciences.the national academy of engineering was established in 1964, under the charter of the national academy of sciences, as a parallelorganization of outstanding engineers. it is autonomous in its administration and in the selection of its members, sharing with the nationalacademy of sciences the responsibility for advising the federal government. the national academy of engineering also sponsors engineering programs aimed at meeting national needs, encourages education and research, and recognizes the superior achievements of engineers.dr. robert m. white is president of the national academy of engineering.the institute of medicine was established in 1970 by the national academy of sciences to secure the services of eminent members ofappropriate professions in the examination of policy matters pertaining to the health of the public. the institute acts under the responsibilitygiven to the national academy of sciences by its congressional charter to be an adviser to the federal government and, upon its own initiative, to identify issues of medical care, research, and education. dr. kenneth i. shine is president of the institute of medicine.the national research council was organized by the national academy of sciences in 1916 to associate the broad community of science and technology with the academy's purposes of furthering knowledge and advising the federal government. functioning in accordancewith general policies determined by the academy, the council has become the principal operating agency of both the national academy ofsciences and the national academy of engineering in providing services to the government, the public, and the scientific and engineeringcommunities. the council is administered jointly by both academies and the institute of medicine. dr. bruce alberts and dr. robert m.white are chairman and vice chairman, respectively, of the national research council.support for this project was provided by the u.s. department of defense through the advanced research projects agency under grantno. mda9729410008. any opinions, findings, conclusions, or recommendations expressed in this material are those of the authors and donot necessarily reflect the views of the department of defense or the advanced research projects agency.library of congress catalog card number 9567707international standard book number 0309052777copyright 1995 by the national academy of sciences. all rights reserved.available from:national academy press2101 constitution avenue, nwwashington, dc 20418b540printed in the united states of americaiievolving the high performance computing and communications initiative to support the nation's information infrastructurecopyright national academy of sciences. all rights reserved.committee to study highperformance computing andcommunications: status of a major initiativefrederick p. brooks, jr., university of north carolina at chapel hill, cochairivan e. sutherland, sun microsystems laboratories, cochairerich bloch, council on competitivenessdeborah estrin, university of southern california/information sciences institutejohn hennessy, stanford universitybutler w. lampson, digital equipment corporationedward d. lazowska, university of washingtonwilliam a. lester, jr., university of california at berkeleyjane preston, telemedical interactive consultative services inc.w. david sincoskie, bell communications research inc.larry smarr, national center for supercomputing applications/university of illinois at urbanachampaignjoseph f. traub, columbia universitystaffmarjory s. blumenthal, directorjames e. mallory, staff officerleslie m. wade, project assistantiiievolving the high performance computing and communications initiative to support the nation's information infrastructurecopyright national academy of sciences. all rights reserved.computer science and telecommunications boardwilliam wulf, university of virginia, chairfrances allen, ibm t.j. watson research centerjeff dozier, university of california at santa barbaradavid j. farber, university of pennsylvaniahenry fuchs, university of north carolinacharles m. geschke, adobe systems inc.james gray, san francisco, californiabarbara j. gross, harvard universitydeborah a. joseph, university of wisconsinrichard m. karp, university of california at berkeleybutler w. lampson, digital equipment corporationbarbara h. liskov, massachusetts institute of technologyjohn major, motorola inc.robert l. martin, at&t network systemsdavid g. messerschmitt, university of california at berkeleywilliam h. press, harvard universitycharles l. seitz, myricom inc.edward shortliffe, stanford university school of medicinecasmir s. skrzypczak, nynex corporationleslie l. vadasz, intel corporationmarjory s. blumenthal, directorlouise a. arnheim, senior staff officerherbert s. lin, senior staff officerjames e. mallory, staff officerrenee a. hawkins, staff associatejohn m. godfrey, research associategloria p. bemah, administrative assistantleslie m. wade, project assistantivevolving the high performance computing and communications initiative to support the nation's information infrastructurecopyright national academy of sciences. all rights reserved.commission on physical sciences, mathematics, and applicationsrichard n. zare, stanford university, chairrichard s. nicholson, american association for the advancement of science, vice chairstephen l. adler, institute for advanced studysylvia t. ceyer, massachusetts institute of technologysusan l. graham, university of california at berkeleyrobert j. hermann, united technologies corporationrhonda j. hughes, bryn mawr collegeshirley a. jackson, rutgers universitykenneth i. kellermann, national radio astronomy observatoryhans mark, university of texas at austinthomas a. prince, california institute of technologyjerome sacks, national institute of statistical sciencesl.e. scriven, university of minnesotaa. richard seebass iii, university of coloradoleon t. silver, california institute of technologycharles p. slichter, university of illinois at urbanachampaignalvin w. trivelpiece, oak ridge national laboratoryshmuel winograd, ibm t.j. watson research centercharles a. zraket, mitre corporation (retired)norman metzger, executive directorvevolving the high performance computing and communications initiative to support the nation's information infrastructurecopyright national academy of sciences. all rights reserved.vievolving the high performance computing and communications initiative to support the nation's information infrastructurecopyright national academy of sciences. all rights reserved.prefacein early 1994, acting through the defense authorization act for fy 1994 (public law 103160), congressasked the national research council (nrc) to examine the status of the highperformance computing andcommunications initiative (hpcci). broadbased interest in and support for the hpcci exist. given its scope andsize, concerns had been raised about its goals, management, and progress. congress asked that at a minimum thestudy address: the basic underlying rationale(s) for the program, including the appropriate balance between federalefforts and privatesector efforts; appropriateness of its goals and directions; balance between various elements of the program; the effectiveness of the mechanisms for obtaining the views of industry and users for the planning andimplementation of the program; likelihood that the various goals of the program will be achieved; management and coordination of the program; and the relationship of the program to other federal support of highperformance computing andcommunications, including acquisition of highperformance computers by federal departments andagencies in support of the mission needs of these departments and agencies.for this study the nrc's computer science and telecommunications board (cstb) convened a committeeof 12 members, expert on pioneering applications of computers and communications and the major components ofthe hpcci: highperformance computing systems, advanced software technology and algorithms, thenational research and education network, basic research and human resources, and information infrastructuretechnology and applications. congress asked the committee to accelerate the normal nrc study process in orderto provide an interim report by july 1, 1994, and a final report by february 1, 1995. the committee was able tomeet this rapid turnaround by drawing on the knowledge and experience of its members as expressed in committeedeliberations and by obtaining input from numerous outside experts.the full committee met six times between march 10, 1994, and december 20, 1994, to hear more than 25highperformance computing and communications users, builders, and scientists; to discuss the hpcci in detail;and to produce this report. additionally, smaller groups of committee members made site visits to discuss firsthand the use of highperformance technologies. these visits involved another 6 individuals from the ford motorcompany and approximately 50 highperformance computing and communications users who had gathered at aworkshop to discuss the use of highperformance systems in environmental research and simulation. in addition toexamining the current status of the program, the committee considered the evolution of the hpcciprefaceviievolving the high performance computing and communications initiative to support the nation's information infrastructurecopyright national academy of sciences. all rights reserved.and its goals and alternate government investment strategies related to technological development. the committeetook into account varying perspectives on the initiative's goals and available assessments of progress towardachieving them.the committee's interim report provided technical background and perspective on the overall development ofhighperformance computing and communications systems, as well as on the hpcci, formally started in 1991.1the interim report made two recommendations: (1) strengthen the national coordination office to help it meetincreasing future demands for program coordination and information functions; and (2) immediately appoint thecongressionally mandated hpcci advisory committee to provide broadbased, active input to the initiative. asthis final report goes to press, both recommendations have yet to be acted on and thus require executive andlegislative attention.this final report, evolving the highperformance computing and communications initiative to support thenation's information infrastructure, purposely adopted a broad perspective so as to examine the hpcci within thecontext of the evolving information infrastructure and national economic competitiveness generally. committeedeliberations consistently pointed out the important contributions that computing and communications researchhave made to the nation's economy, scientific research, national defense, and social fabric. that research hasnourished u.s. leadership in information technology goods, services, and applications. traditionally, the mostpowerful computers and the fastest networks made many of those contributions. recently, however, thewidespread availability of significant computing and communications capabilities on the nation's desktops andfactory floors has also produced many benefits. the broadening and interconnection of more and more computerbased systems call attention to research needs associated with system scale as well as performance.the committee to study high performance computing and communications: status of a major initiative isgrateful for the help, encouragement, and hard work of the nrc staff working with us: marjory blumenthal, jimmallory, susan maurizi, and leslie wade. our meetings went smoothly because of their careful preparation. thisreport came together because of their attention and diligence. they patiently assembled sometimes conflicting textfrom diverse authors and helped reconcile it with the critique of our reviewers. they searched out facts ofimportance to our deliberations. their excellent staff preparation helped us focus on the substance of our task.many others also made valuable contributions to the committee. in addition to individuals listed in appendix fwho briefed the committee, the committee appreciates inputs from sally howe and don austin (nationalcoordination office); bob borchers, paul young, dick kaplan, and robert voight (national sciencefoundation); eric cooper (fore systems); stephen squires (advanced research projects agency); sandymacdonald (national oceanic and atmospheric administration); robert bonometti (office of science andtechnology policy); and al rosenheck (former congressional staffer). it is also grateful to the anonymousreviewers who helped to sharpen and focus the report with their insightful comments. responsibility for thereport, of course, remains with the committee.1 computer science and telecommunications board (cstb), national research council. 1994. interim report on the statusof the highperformance computing and communications initiative. computer science and telecommunications board,washington, d.c.prefaceviiievolving the high performance computing and communications initiative to support the nation's information infrastructurecopyright national academy of sciences. all rights reserved.finally, we want to acknowledge the contributions to our present task of research projects 30 years past. ofcourse this text was all word processed. of course the charts were drafted on computers. of course we usedinternet communication nationwide to plan our meetings, share our thoughts, reconcile our differences, andassemble our report. we plugged our portable computers into a local network at each of our meetings, sendingdrafts to local laser printers. in short, we have partaken fully of the fruits of the hpcci's precursors. we thank thevisionaries of the past for our tools.frederick p. brooksivan e. sutherlandcochairscommittee to study high performance computing and communications: status of a major initiativeprefaceixevolving the high performance computing and communications initiative to support the nation's information infrastructurecopyright national academy of sciences. all rights reserved.xevolving the high performance computing and communications initiative to support the nation's information infrastructurecopyright national academy of sciences. all rights reserved.contents executive summary 11 u.s. leadership in information technology 13 information technology is central to our society 13 information technology advances rapidly 14 retaining leadership in information technology is vital to the nation 15 the federal investment in computing research has paid rich dividends 16 continued federal investment is necessary to sustain our lead 23 today the hpcci is the umbrella for most governmentsponsored computing and communications research 25 notes 272 the high performance computing and communications initiative 28 hpcci: goals and emphases 28 basic objectives 28 expanded objectives 30 hpcci accomplishment 31 the issue of measurement 31 better computing and computational infrastructure 32 increasing researcherdeveloperuser synergy 32 impact on mission agencies 36 five gigabit testbed projects: collaboration and impact 36 evolution of hpcci goals and objectives 37 improving the information infrastructure 37 evolving research directions and relevance for the information infrastructure 39 overall computing and communications r&d planning 41 toward a better balance 42 moving forwardšbasic issues 42 balance of private and public investment 42 coordination versus management 43 coordinating structure 45 budget 49 notes 49contentsxievolving the high performance computing and communications initiative to support the nation's information infrastructurecopyright national academy of sciences. all rights reserved.3 recommendations 53 general recommendations 53 recommendations on highperformance computing 54 recommendations on networking and information infrastructure 57 recommendations on the supercomputer centers and grand challenge program 59 recommendations on coordination and program management in the hpcci 61 comments relating this report's recommendations for highperformance computing andcommunications research to administration priorities 64 notes 66 bibliography 67 appendixes a the highperformance computing and communications initiative: background 77b highperformance communications technology and infrastructure 92c review of the highperformance computing and communications initiative budget 97d current highperformance computing and communications initiative grand challenge activities 105e accomplishments of national science foundation supercomputer centers 108f individuals providing briefings to the committee 118contentsxiievolving the high performance computing and communications initiative to support the nation's information infrastructurecopyright national academy of sciences. all rights reserved.industry, if left to itself, will naturally find its way to the most useful and profitable employment. whence itis inferred that manufacturers, without the aid of government, will grow up as soon and as fast as the natural stateof things and the interest of the community may require.against the solidity of this hypothesis . . . very cogent reasons may be offered . . . [including] the stronginfluence of habit; the spirit of imitation; the fear of want of success in untried enterprises; the intrinsic difficultiesincident to first essays towards [competition with established foreign players]: the bounties, premiums, and otherartificial encouragements with which foreign nations second the exertions of their own citizens ... to produce thedesirable changes as early as may be expedient may therefore require the incitement and patronage ofgovernment.šalexander hamilton, 1791, report on manufacturesprefacexiiievolving the high performance computing and communications initiative to support the nation's information infrastructurecopyright national academy of sciences. all rights reserved.xivevolving the high performance computing and communications initiative to support the nation's information infrastructurecopyright national academy of sciences. all rights reserved.executive summaryinformation technology drives many of today's innovations and offers still greater potential for furtherinnovation in the next decade. it is also the basis for a domestic industry of about $500 billion,1 an industry that iscritical to our nation's international competitiveness. our domestic information technology industry is thrivingnow, based to a large extent on an extraordinary 50year track record of public research funded by the federalgovernment, creating the ideas and people that have let industry flourish. this record shows that for a dozen majorinnovations, 10 to 15 years have passed between research and commercial application (see figure es.1). despitemany efforts, commercialization has seldom been achieved more quickly.publicly funded research in information technology will continue to create important new technologies andindustries, some of them unimagined today, and the process will continue to take 10 to 15 years. without suchresearch there will still be innovation, but the quantity and range of new ideas for u.s. industry to draw from willbe greatly diminished. public research, which creates new opportunities for private industry to use, should not beconfused with industrial policy, which chooses firms or industries to support. industry, with its focus mostly on thenearterm, cannot take the place of government in supporting the research that will lead to the next decade'sadvances.the highperformance computing and communications initiative (hpcci) is the main vehicle for publicresearch in information technology today and the subject of this report. by the early 1980s, several federalagencies had developed independent programs to advance many of the objectives of what was to become thehpcci. the program received added impetus and more formal status when congress passed the highperformance computing act of 1991 (public law 102194) authorizing a 5year program in highperformancecomputing and communications. the initiative began with a focus on highspeed parallel computing andnetworking and is now evolving to meet the needs of the nation for widespread use on a largescale as well as forhigh speed in computation and communications. to advance the nation's information infrastructure there is muchthat needs to be discovered or invented, because a useful ''information highway" is much more than wires to everyhouse.as a prelude to examining the current status of the hpcci, this report first describes the rationale for theinitiative as an engine of u.s. leadership in information technology and outlines the contributions of ongoingpublicly funded research to past and current progress in developing computing and communications technologies(chapter 1). it then describes and evaluates the hpcci's goals, accomplishments, management, and planning(chapter 2). finally, it makes recommendations aimed at ensuring continuing u.s. leadership in informationtechnology through wise evolution and use of the hpcci as an important lever (chapter 3). appendixes a throughf of the report provide additional details on and documentation for points made in the main text.executive summary1evolving the high performance computing and communications initiative to support the nation's information infrastructurecopyright national academy of sciences. all rights reserved.figure es.1 governmentsponsored computing research and development stimulates creation of innovative ideasand industries. dates apply to horizontal bars, but not to arrows showing transfer of ideas and people.executive summary2evolving the high performance computing and communications initiative to support the nation's information infrastructurecopyright national academy of sciences. all rights reserved.information technologyfundamental for society and the economynow and tomorrowcomputers, the devices that process information, affect our lives both directly and indirectly. today, morethan 70 million microcomputers are installed in the united states, and between onefifth and onethird of u.s.households have one.2 entertainment, education, communications, medicine, government, and finance are usingcomputers in more ways to enhance our lives directly through the provision of such services as distributed learningand remote banking. computers are also used to make essential products and activities cheaper and better:airplanes, molded plastics, automobiles, medical imaging, and oil exploration are only a few of many examples. abroader benefit is the $500 billion industry's creation of jobs, taxes, profits, and exports.clearly, the uses and applications of information technology will continue to grow. in fact, the informationrevolution has only just begun. computers will become increasingly valuable to industries and to citizens as theirpower is tapped to recognize and simulate speech, generate realistic images, provide accurate models of thephysical world, build huge automated libraries, control robots, and help with a myriad of other tasks. to do thesethings well will require both computing and communications systems many times more powerful than we havetoday. ongoing advances in knowledge will constitute the foundation for building the systems and developing theapplications that will continue to advance our quality of life and ensure strong u.s. leadership in informationtechnology. strong leadership in information technology in turn supports other sectors including industry, health,education, and defense by serving their needs for equipment, software, and knowhow.the basis for continuing strengthša successful governmentindustry partnershipfederal investment in information technology research has played a key role in the u.s. capability to maintainits international lead in information technology. starting in world war ii publicly funded research has helped tostock the nation's storehouse of trained people and innovative ideas. but our lead is fragile. leadership can shift in afew product generations, and because a generation in the computing and communications industry is at most 2years, our lead could disappear in less than a decade.since the early 1960s the u.s. government has invested broadly in computing research, creating new ideasand trained people. the result has been the development of important new technologies for timesharing,networking, computer graphics, humanmachine interfaces, and parallel computing, as well as major contributionsto the design of very largescale integrated circuits, fast computers and disk systems, and workstations (seefigure es.1; see also chapter 1, box 1.2 for details). each of these is now a multibilliondollar business. fromthese successes we can learn some important lessons:research has kept paying off over a long period.the payoff from research takes time. as figure es. 1 shows, at least 10 years, more often 15, elapsebetween initial research on a major new idea and commercial success. this is still true in spite of today'sshorter product cycles.unexpected results are often the most important. electronic mail and the "windows" interface are only twoexamples; box 1.2 in chapter 1 outlines more.executive summary3evolving the high performance computing and communications initiative to support the nation's information infrastructurecopyright national academy of sciences. all rights reserved.research stimulates communication and interaction. ideas flow back and forth between researchprograms and development efforts and between academia and industry.research trains people, who start companies or form a pool of trained personnel that existing companiescan draw on to enter new markets quickly.doing research involves taking risks. not all public research programs have succeeded or led to clearoutcomes even after many years. but the record of accomplishments suggests that governmentinvestment in computing and communications research has been very productive.government support of research is crucialthe information technology industry improves its products faster than most others: for the last 40 years adollar has bought hardware with twice as much computation, storage, and communication every 18 to 24 months,offering a 100fold gain every decade (patterson and hennessy, 1994, p. 21). this rate will continue at least for thenext decade (see chapter 1, figure 1.1). better hardware in turn makes it feasible to create software for newapplications: electronic and mechanical design, climate mapping, digital libraries, desktop publishing, videoediting, and telemedicine are just a few examples. such applications are often brought to market by newcompanies such as microsoft and sun microsystems, both of which produce revenues of more than $4 billion peryear (computer select, 1994) and neither of which existed 15 years ago.the information technology industry is characterized by great importance to the economy and society, rapidand continuing change, a 10 to 15year cycle from major idea to commercial success, and successive waves ofnew companies. in this environment a broad program of publicly funded research is essential for two reasons: first, industrial efforts cannot replace government investment in basic computing and communicationsresearch. few companies will invest for a payoff that is 10 years away, and even a company that doesmake a discovery may postpone using it. the vitality of the information technology industry dependsheavily on new companies, but new companies cannot easily afford to do research; furthermore, industryin general is doing less research now than in the recent past (geppert, 1994; corcoran, 1994). butbecause today's sales are based on yesterday's research, investment in innovation must go forward so thatthe nation's information industry can continue to thrive. second, it is hard to predict which new ideas and approaches will succeed. the exact course ofexploratory research cannot be planned in advance, and its progress cannot be measured precisely in theshortterm. the purpose of publicly funded research is to advance knowledge and create newopportunities that industry can exploit in the medium and longterm, not to determine how the marketshould develop.executive summary4evolving the high performance computing and communications initiative to support the nation's information infrastructurecopyright national academy of sciences. all rights reserved.the high performance computing and communications initiativegoals and emphasesthe hpcci is the current manifestation of the continuing government research program in informationtechnology, an investment that has been ongoing for more than 50 years. although it emphasizes research inhighperformance computing and communications, the hpcci now has in its budget nearly all of the federalfunding for computing research of any kind. the wisdom of this arrangement is doubtful.the hpcci was initiated to serve several broad goals (nco, 1993): in highperformance computing and networking; disseminate new technologies to serve the economy, national security, education, health care, and theenvironment; and gains in u.s. productivity and industrial competitiveness.the original plans to achieve these goals called for creating dramatically faster computers and networks,stretching their limits with grand challenge problems in scientific computing, setting up supercomputer centerswith the machines and experts needed to attack these challenges, and training people to build and exploit the newtechnology. more recently the focus has been shifting toward broader uses of computing and communications.high performance"highperformance"šwhich involves bringing more powerful computing and communications technology tobear on a problemšhas enabled advances on several fronts. highperformance systems, for example, deliveranswers sooner for complex problems that need large amounts of computing. timely and accurate forecasting ofweather, mapping of oil reservoirs, and imaging of tumors are among the benefits encompassed by the goals listedabove. but "highperformance," which is broader than supercomputing, is a moving target because of the steadyand rapid gains in the performance/cost ratio. yesterday's supercomputer is today's personal computer; today'sleadingedge communications technology will be among tomorrow's mainstream capabilities.information technology evolves as new and valuable applications are found for hardware that gets steadilymore powerful and cheaper. to benefit, users need affordable hardware, but they also need the software thatimplements the new applications. yet learning how to build software takes many years of experimentation. if thisprocess starts only when the hardware has already become cheap, the benefits to users will be delayed by years.research needs to treat today's expensive equipment as a time machine, learning how it will be used when it ischeap and widely available, as it surely will be tomorrow. knowing how to use computers for new tasks soonercan help many industries to become more competitive.to date, the hpcci's focus has been mainly on speed, but speed is not the only measure of highperformance. both speed, measured today in billions of operations per second or billions of bits per second, andscale, measured by the number of millions of users served, are important research issues. however, for the nation'sinformation infrastructure, scale now seems more difficult to achieve. information technology can be thought of as atent, with the height of the center pole as speed and the breadth of the base as scale. widening the tent to allowmore work on scale without decreasing the work on speed requires more cloth; with the same resources, wideningexecutive summary5evolving the high performance computing and communications initiative to support the nation's information infrastructurecopyright national academy of sciences. all rights reserved.the tent would sacrifice research on speed for research on scale. this report recommends ways to reallocate fundswithin the hpcci so as to accommodate greater emphasis on scale.accomplishments to datethe hpcci has focused mainly on parallel computing, networking, and development of human resources.building on progress in research begun before the hpcci, work and accomplishments to date reveal two keytrends: better computing and computational infrastructure and increasing researcherdeveloperuser synergy.despite the difficulty of measuring impact at this early stage, it is the committee's judgment that the hpccihas been generally successful so far. that assessment is necessarily qualitative and experiential now. because thehpcci is only 3 years old, results that can be measured in dollars should not be expected before the next decade.the hpcci has contributed substantially to the development, deployment, and understanding of computingand communications facilities and capabilities as infrastructure. it has helped transform understanding of how toshare resources and information, generating proofs of concept and understanding that are of value not only to thescientific research community but also to the economy and society at large.in parallel computing the fundamental challenge is not building the machines, but learning how to programthem. pioneering users and their software developers must be motivated by machines that are good enough toreward success with significant speedups.3 for this reason, a great deal of money and effort have had to be spent toobtain parallel machines with the potential to run much faster than existing supercomputers. from the base built bythe hpcci, much has been learned about parallel computing.the hpcci has fostered productive interactions among the researchers and developers involved in creatinghighperformance computing and communications technology and researchers who use the technology. buildingon the varying perspectives of the three groups, complex problems are being solved in unique ways. in particular,the hpcci has funded crossdisciplinary teams associated with the grand challenge projects to solve complexcomputational problems and produce essential new software for the new parallel systems.more specifically, the hpcci has:nation's stock of expertise by educating new students and attracting new researchers; made parallel computing widely accepted as the practical route to achieving highperformancecomputing; the feasibility of and initiated deployment of parallel databases; driven progress on grand challenge problems in disciplines such as cosmology, molecular biology,chemistry, and materials science. parallel computation has enhanced the ability to attack problems ofgreat complexity in science and engineering; developed new modes of analyzing and visualizing complex data sets in the earth sciences, medicine,molecular biology, and engineering, including creating virtual reality technologies. many supercomputergraphic techniques of the 1980s are now available on desktop graphics workstations;executive summary6evolving the high performance computing and communications initiative to support the nation's information infrastructurecopyright national academy of sciences. all rights reserved. through the gigabit network testbeds associated with the national research and education networkcomponent, demonstrated the intimate link between computing and communications systems; built advanced networks that are the backbone of the internet and the prototypes for its further evolutioninto the basis for a broader information infrastructure;a highspeed backbone that has kept up with the yearly doubling of the size of the internet, andorganized the impending transition of this backbone away from government funding; and created the mosaic browser for the world wide web, the first new major application in many years thatpromises to greatly increase access to the resources available on the internet. this was an entirelyunexpected result.evolutiona largescale, integrated information infrastructure designed to serve the entire nation is becoming a highpriority for government and industry as well as a source of challenges for research. complex systems withmillions of users pose many problems: performance, management, security, interoperability, compatible evolutionof components, mobility, and reliability are only a few. today's technology can solve these problems for systemswith a few thousand users at most; to do so for millions or hundreds of millions of users is far beyond the currentstate of the art.4 providing users with highbandwidth connections is itself a major problem, but it is only thebeginning. there is a wide gap between enabling a connection and providing a rich array of useful and dependableservices.because the hpcci has become the rubric under which virtually all of the nation's research in informationtechnology is conducted, it is not surprising that its focus has been changing in response to past successes, newopportunities, and evolving societal needs. the recently added information infrastructure technology andapplications (iita) program, broadly construed, addresses many of the problems just mentioned; it is already thelargest component of the hpcci,5 and its continued evolution should be encouraged.but with the policy focusšin the government, the press, and in most of the agenciesšcentered oninformation infrastructure,6 highperformance computing seems to have been downplayed. the committeeemphasizes the importance of retaining the hpcci's momentum at just the time when its potential to supportimprovement in the nation's information infrastructure is most needed.organizationseveral federal agencies participate in the hpcci, most notably the national science foundation (nsf), theadvanced research projects agency (arpa), the national aeronautics and space administration (nasa), andthe department of energy (doe) (see appendix a for a full list). because of its successes the hpcci has become amodel for multiagency collaboration and for the "virtual agency" concept advanced through the nationalperformance review (gore, 1993). each participating agency retains responsibility for its own part of theprogram, but the agencies work together in joint funding of projects, such as the federal portions of the internet;joint reviews of grants and contracts, such as the nsfarpanasa digital library initiative; joint testbeds; andexecutive summary7evolving the high performance computing and communications initiative to support the nation's information infrastructurecopyright national academy of sciences. all rights reserved.consortia, such as the consortium for advanced modeling of regional air quality that joins six federal agencieswith several state and local governments.the hpcci supports a diverse set of contractors at universities, companies, and national laboratoriesthroughout the country. it provides project funding in varying amounts through contracts, grants, and cooperativeagreements awarded according to diverse methods. this diversity is healthy because it allows many views tocompete, resulting in a broad research program that ensures a continuing flow of advances in informationtechnology.some have argued for a more centrally managed program, with thorough planning, precise milestones, andpresumably no wasted effort. tighter management would cost more in bureaucracy and turf wars, but the essentialquestion is whether it would produce better or worse results for the money spent. the committee believes thatbecause of the long time scale of research, diversity is essential for success. no one person or organization iseither smart or lucky enough to plan the best program, no single approach is best, and success often comes inunanticipated ways. because it is a national research program and because of the many different butinterdependent underlying technologies, the hpcci is necessarily and properly far more diverse than a focusedeffort such as the apollo moon landing program or a commercial product development program.in contrast to central management, coordination enhances the benefits of diversity by helping to preventunintended duplication, redundancy, and missed opportunities. the hpcci's national coordination office (nco)serves this purpose, aiding interagency cooperation and acting as liaison for the initiative to the congress, otherlevels of government, universities, industry, and the public. its efforts are reflected in its impressive fy 1994 andfy 1995 "blue books" describing the program's activities and spending.7 strengthening the nco and appointingan advisory committee, as recommended in the committee's interim report (cstb, 1994c), would facilitate regularinfusions of ideas and advice from industry and academia and enable better communication of the hpcci's goalsand accomplishments to its many constituents. this committee should consist of a group of recognized expertsthat is balanced between academia and industry and balanced with respect to application areas and the coretechnologies underlying the hpcci.budgetbecause it grew from earlier programs, a significant portion of the hpcci budget is not new money. thebudget grew from a base of $490 million in preexisting funding in fy 1992 to the $1.1 billion requested for fy1995.8 each year agencies have added to the base by moving budgets for existing programs into the hpcci and byreprogramming existing funds to support the hpcci. congress has also added funding each year to start newactivities or expand old ones.the result is that much of the $1.1 billion requested for fy 1995 is money that was already being spent oncomputing and communications in fy 1992. the request has three elements: (1) funds for activities that predatethe hpcci and were in the fy 1992 base budget, (2) funds for activities that have since been designated as part ofthe hpcci, and (3) new funds for new activities or for growth. although dissecting the budget in this way wouldshed light on the program, the committee was unable to do so because each participating agency treats thenumbers differently.it appears that the fy 1995 request breaks down roughly as onethird for applications, onethird to advancethe essential underlying computing and communications technologies, onequarter for computing andcommunications infrastructure, and small amounts for education and electronics (see appendix c).executive summary8evolving the high performance computing and communications initiative to support the nation's information infrastructurecopyright national academy of sciences. all rights reserved.the future of the hpcci: recommendationsthe committee believes that strong public support for a broadly based research program in informationtechnology is vital to maintaining u.s. leadership in information technology. incorporating this view of theimportance and success of the government's investment in research, the 13 recommendations that follow addressfive areas: general research program, highperformance computing, networking and information infrastructure, thesupercomputer centers and the grand challenge projects, and program coordination and management. within eacharea the recommendations are presented in priority order.general recommendations1. continue to support research in information technology. ensure that the major funding agencies,especially the national science foundation and the advanced research projects agency, have strongprograms for computing and communications research that are independent of any special initiatives.the government investment in computing research has yielded significant returns. ongoing investment, atleast as high as the current dollar level, is critical both to u.s. leadership and to ongoing innovation in informationtechnology. today the hpcci supports nearly all of this research, an arrangement that is both misleading anddangerous: misleading because much important computing research addresses areas other than highperformance(even though it may legitimately fit under the new iita component of the hpcci), and dangerous becausereduced funding for the hpcci could cripple all of computing research. the "war on cancer" did not support all ofbiomedical research, and neither should the hpcci or any future initiative on national infrastructure subsume allof computing research.2. continue the hpcci, maintaining today's increased emphasis on the research challenges posed bythe nation's evolving information infrastructure. the new information infrastructure technology andapplications program of the hpcci focuses on information infrastructure topics, which are also addressed in theinitiative's other four components. the committee supports this continued evolution, which will lead to tangiblereturns on existing and future investments in basic hardware, networking, and software technologies.high performance computing3. continue funding a strong experimental research program in software and algorithms for parallelcomputing machines. today a crucial obstacle to widespread use of parallel computing is the lack of advancedsoftware and algorithms. emphasis should be given to research on developing and building usable applicationsoriented software systems for parallel computers. avoid funding the transfer ("porting") of existingcommercial applications to new parallel computing machines unless there is a specific research need.4. stop direct hpcci funding for development of commercial hardware by computer vendors and for"industrial stimulus" purchases of hardware. maintain hpcci support for precompetitive research incomputer architecture; this work should be done in universities or in universityindustry collaborations andshould be driven by the needs of system and application software. hpcci funding for stimulus purchase oflargescale machines has been reduced, as has the funding of hardware development by vendors. the committeesupports these changes, which should continue except when a mission need demands the development ofnonstandard hardware.executive summary9evolving the high performance computing and communications initiative to support the nation's information infrastructurecopyright national academy of sciences. all rights reserved.public research is best done in universities. not only are academic organizations free to think about longerterm issues, but they also stimulate technology transfer through publication and placement of graduates. thenational experience supports vannevar bush's basic tenet: publicly funded research carried out in universitiesproduces a diversity of excellent ideas, trained people, research results, and technologies that can be commerciallyexploited (osrd and bush, 1945).5. treat development of a teraflop computer as a research direction rather than a destination. the goalof developing teraflop capability has served a valuable purpose in stimulating work on largescale parallelism, butfurther investment in raw scalability is inappropriate except as a focus for precompetitive, academic research.industrial development of parallel computers will balance the low cost of individual, massproduced computingdevices against the higher cost of communicating between them in a variety of interesting ways. in the near future ateraflop parallel machine will be built when some agencies' mission requirements correspond to a sufficientlyeconomical commercial offering. continued progress will surely lead to machines even larger than a teraflop.networking and information infrastructurenew ideas are needed to meet the new challenges underlying development of the nation's informationinfrastructure. the hpcci can contribute most by focusing on the underlying research issues. this shift hasalready begun, and it should continue.this evolution of the research agenda, which would support improvement of the nation's informationinfrastructure, is partly under way: in the fy 1995 implementation plan (nco, 1994, p. 15), over onequarter ofthe nsf and arpa hpcci funding is focused on the iita component, and activities in other components havealso evolved consistent with these concerns. the committee supports this increased emphasis.6. increase the hpcci focus on communications and networking research, especially on the challengesinherent in scale and physical distribution. an integrated information infrastructure that fully serves the nation'sneeds cannot spring fullgrown from what we already know. much research is needed on difficult problems relatedto size, evolution, introduction of new systems, reliability, and interoperability. much more is involved thansimply deploying large numbers of boxes and wires. for example, both hardware and software systems must workefficiently to handle scheduling; bandwidth optimization for transmission of a range of data formats, includingrealtime audio and video data; protocol and format conversion; security; and many other requirements.7. develop a research program to address the research challenges underlying our ability to build verylarge, reliable, highperformance, distributed information systems based on the existing hpccifoundation. although a comprehensive vision of the research needed for advancing the nation's informationinfrastructure has not yet been developed, three key areas for research are scalability, physical distribution, andinteroperative applications.8. ensure that research programs focusing on the national challenges contribute to the developmentof information infrastructure technologies as well as to the development of new applications andparadigms. this dual emphasis contrasts with the narrower focus on scientific results that has driven work on thegrand challenges.supercomputer centers and grand challenge programthe nsf supercomputer centers have played a major role in establishing parallel computing as a full partnerwith the prior paradigms of scalar and vector computing by providing access toexecutive summary10evolving the high performance computing and communications initiative to support the nation's information infrastructurecopyright national academy of sciences. all rights reserved.stateoftheart computing facilities. nsf should continue to take a broad view of the centers' mission of providingaccess to highperformance computing and communications resources, including participating in research neededto improve software for parallel machines and to advance the nation's information infrastructure.the committee recognizes that advanced computation is an important tool for scientists and and engineers andthat support for adequate computer access must be a part of the nsf research program in all disciplines. thecommittee did not consider the appropriate overall funding level for the centers. however, the committee believesthat nsf should move to a model similar to that used by nasa and doe for funding general access tocomputing. the committee prefers nasa's and doe's approach to funding supercomputer centers, where hpccifunds are used only to support the exploration and use of new computing architectures, while nonhpcci fundsare used to support general access.9. the mission of the national science foundation supercomputer centers remains important, but thensf should continue to evaluate new directions, alternative funding mechanisms, new administrativestructures, and the overall program level of the centers. nsf could continue funding of the centers at thecurrent level or alter that level, but it should continue using hpcci funds to support applications thatcontribute to the evolution of the underlying computing and communications technologies, while supportfor general access by application scientists to maturing architectures should come increasingly from nonhpcci funds.10. the grand challenge program is an innovative approach to creating interdisciplinary and multiinstitutional scientific research teams; however, continued use of hpcci funds is appropriate only whenthe research contributes significantly to the development of new highperformance computing andcommunications hardware or software. grand challenge projects funded under the hpcci should beevaluated on the basis of their contributions both to highperformance computing and communicationstechnologies and to the application area. completion of the grand challenge projects will provide valuableinsights and demonstrate the capabilities of new highperformance architectures in some important applications. itwill also foster better collaboration between computer scientists and computational scientists. the committee notesthat a large share of hpcci funding for the grand challenges currently comes from the scientific disciplinesinvolved. however, the overall funding seems to come entirely from hpccilabeled funds. for the same reasonsoutlined in recommendation 9, the committee sees this commingled support as unhealthy in the long run andurges a transition to greater reliance on scientific disciplinary funding using nonhpcci funds.coordination and program management11. strengthen the hpcci national coordination office while retaining the cooperative structure ofthe hpcci and increasing the opportunity for external input. immediately appoint the congressionallymandated advisory committee intended to provide broadbased, active input to the hpcci, or provide aneffective alternative. appoint an individual to be a fulltime coordinator, program spokesperson, andadvocate for the hpcci.in making this recommendation, the committee strongly endorses the role of the current nco as supportingthe mission agencies rather than directing them. the committee believes it vital that the separate agencies retaindirection of their hpcci funds. the value of interagency cooperation outweighs any benefits that might be gainedthrough more centralized management.diverse management systems for research should be welcomed, and micromanagement should be avoided. inthe past, choosing good program officers and giving them freedom to operate independently have yielded goodvalue, and the committee believes it will continue to do so.executive summary11evolving the high performance computing and communications initiative to support the nation's information infrastructurecopyright national academy of sciences. all rights reserved.furthermore, independence will encourage diversity in the research program, thus increasing opportunitiesfor unexpected discoveries, encouraging a broader attack on problems, and ensuring fewer missed opportunities.12. place projects in the hpcci only if they match well to its objectives. federal research fundingagencies should promptly document the extent to which hpcci funding is supporting important longtermresearch areas whose future funding should be independent of the future of the hpcci.a number of preexisting agency programs have entered the hpcci, with two effects: the hpcci's budgetappears to grow faster than the real growth of investment in highperformance computing and communicationsresearch, and important programs such as basic research in computing within nsf and arpa may be in jeopardyshould the hpcci end.13. base mission agency computer procurements on mission needs only, and encourage makingequipment procurement decisions at the lowest practical management level. this recommendation appliesequally to government agencies and to government contractors. it has generally been best for an agency to specifythe results it wants and to leave the choice of specific equipment to the contractor or local laboratory management.notes1. see u.s. doc (1994); the department of commerce utilizes data from the u.s. bureau of the census series, the annual survey ofmanufactures. it places the value of shipments for the information technology industry at $421 billion for 1993. this number omitsrevenue from equipment rentals, fees for aftersale service, and markups in the product distribution channel. it also excludes officeequipment in total. it includes computers, storage devices, terminals and peripherals; packaged software; computer programmanufacturing, data processing, information services, facilities management, and other services; and telecommunications equipment andservices.see also cbema (1994); cbema values the worldwide 1993 revenue of the u.s. information technology industry at $602 billion. inaddition to including office equipment, it shows larger revenues for information technology hardware and telecommunications equipmentthan does the department of commerce.2. microcomputers (personal computers) are defined as computers with a list price of $1,000 to $14,999; see cbema (1994), pp. 6061.forrester research inc. (1994, pp. 23) estimates the share of households with pcs at about 20 percent, based on its survey of householdsand bureau of census data. forrester's model accounts for retirements of older pcs and for households with multiple pcs. this is a lowerestimate than the software publishing association's widely cited 30 percent share. by definition, the microcomputer statistics excludesmall computers and other generalpurpose and specialized devices that also make use of microprocessors and would be counted in a morecomprehensive measurement of information technology.3. earlier experience with three isolated computers, ''illiac 4" (built at the university of illinois) and "c.mmp" and "cm*" (both built atcarnegie mellon university), bears out this point.4. of course, systems specialized for a single application or for homogenous technology, such as telephony, serve millions of users, butwhat is now envisioned is more complex and heterogeneous, involving integration of multiple services and systems.5. the other four programs of the hpcci are advanced software technology and algorithms, basic research and human resources,highperformance computing systems, and the national research and education network.6. notably, references to the computing portion of the hpcci have been overshadowed recently by the ubiquity of speeches anddocuments devoted to the notion of a national information infrastructure (nii). the nii has also been featured in the titles of the 1994 and1995 blue books.7. each year beginning in 1991 the director of the office of science and technology policy submits a report on the hpcci to accompanythe president's budget. the fy 1992, fy 1993, and fy 1994 books were produced by the nowdefunct federal coordinating council forscience, engineering, and technology; the fy 1995 report was produced by the nco (acting for the committee on information andcommunications). the report describes prior accomplishments and the future funding and activities for the coming fiscal year. thesereports have collectively become known as "blue books" after the color of their cover.8. nco (1994), p. 15. note that figures represent the president's requested budget authority for fy 1995. actual appropriated levels werenot available at press time. because the hpcci is synthesized as a crosscutting multiagency initiative, there is no separate and identifiable"hpcci appropriation."executive summary  12evolving the high performance computing and communications initiative to support the nation's information infrastructurecopyright national academy of sciences. all rights reserved.1u.s. leadership in information technologyinformation technology is central to our economy and society. the united states has held a commanding leadin this arena, a lead that we must maintain. meeting the challenges posed by rapid, worldwide change willcontinue to require our best efforts to advance the state of the art in computing and communications technology.now, as in the past, our ability to lead requires an ongoing strong program of longterm research. the federalgovernment has supported such research for 50 years with great success.today, the highperformance computing and communications initiative (hpcci) is the multiagencycooperative effort under which most of this research is funded. for this reason, any discussion of the hpcci mustbe grounded in an understanding of the role and nature of information technology, the information industry, andthe nation's research program in computing. these issues are the subject of this first chapter of the report.information technology is central to our societycomputers affect our lives enormously. we use them directly for everyday tasks such as making an airlinereservation, getting money from an automated teller machine, or writing a report on a word processor. we alsoenjoy many products and services that would not be possible without digital computing and communications.the direct use of computing is growing rapidly. personal computers are already pervasive in our economyand society: in the united states over 70 million are installed, and between onefifth and onethird of u.s.households have a computer.1 the increasing popularity of personal computing is but the tip of the iceberg;education, communication, medicine, government, manufacturing, transportation, science, engineering, finance,and entertainment all increasingly use digital computing and communications to enhance our lives directly byoffering improved goods and services.indirectly, computing and communications are used to make many products cheaper and better. withoutcomputers connected by communication networks, designing the newest u.s. jetliner, the boeing 777, withinacceptable cost and time constraints would have been impossible. advanced hardware and advanced softwareworking together substituted computer models for expensive and timeconsuming physical modeling. the designof complex plastic molded parts, now routinely used for many products, depends on computer simulation of plasticflow and computer control of die making. automobile engines rely on embedded computers for fuel economy andemission control, and doctors use computerassisted tomography (cat) scanners tou.s. leadership in information technology13evolving the high performance computing and communications initiative to support the nation's information infrastructurecopyright national academy of sciences. all rights reserved.see inside the body. computers help us understand and tap earth's resources: our oil is found by computer analysisof geologic data. interconnected computer systems underlie our entire financial system, enabling electronic fundstransfer and services such as home banking. digital communication extends to a large and rapidly increasingnumber of businesses, educational institutions, government agencies, and homes.originally devices for computation and business data processing, computers now are tools for informationaccess and processing in the broadest sense. as such, they have become fundamental to the operation of oursociety, and computing and communications have come to be labeled widely as "information processing."remarkably, given its already enormous direct and indirect impact, information technology is being deployedin our society more rapidly now than at any time in the past.2 if this momentum is sustained, then digitaltechnology and digital informationthe digital information revolutionwill offer a huge range of new applications,create markets for a wide variety of new products and services, and yield a broad spectrum of benefits to allamericans.information technology advances rapidlythe information industry improves its products with amazing speed. for several decadespowered by federaland industrial research and development (r&d) investments in computer science, computer engineering,electrical engineering, and semiconductor physicseach dollar spent on computation, storage, and communicationhas bought twice the performance every 18 to 24 months. over the course of each decade, this sustained rate ofprogress results in a 100fold improvement, as figure 1.1 shows for processor speed and disk storage capacity.with continued investment, we can sustain this rate of progress for at least the next decade. such rapidimprovement is possible because of the nature of information and of the technologies required to process it:integrated circuits, storage devices, and communications systems (box 1.1). significant improvements in hardwareperformance in turn make it feasible to create the software required for computers to do new thingselectronic andmechanical design, desktop publishing, video editing, modeling of financial markets, creation of digital libraries,and the practice of telemedicine, for example.figure 1.1 increase in performance per dollar of processor speed and disk storage from 1989 to 1994, shown on asemilog scale. (the righthand graph uses a linear scale to emphasize the compound effect of successive doublings.)u.s. leadership in information technology14evolving the high performance computing and communications initiative to support the nation's information infrastructurecopyright national academy of sciences. all rights reserved.box 1.1 what drives the progress in information technology?ł integrated circuits improve rapidly. computers are made from integrated circuits, each component of which gets half ashigh and half as wide every 5 years, with the result that the same area can hold four times as many components at thesame cost. also, each component runs twice as fast, and the circuit chips get bigger. the physical limits to thisprogress are still far off. new designs take advantage of advances in integrated circuits. today's microprocessors and memories are made fromvery largescale integrated (vlsi) circuits. although modern vlsi microprocessors and memories may have 10 millioncomponents, they are actually designed in no more time than the integrated circuits of a decade ago that had only100,000 components. advances in designs and design toolsšin our ability to master complexityšhave been, and willcontinue to be, essential to taking advantage of advances in integrated circuit technology. it is cheap to make more devices, and the same integrated circuit foundries can make many different devices. themarginal cost of building more computers is small, because the cost of raw materials is low and the components aremassproduced. further, although an integrated circuit foundry may be expensive to build, it can make many differentproducts, just as a printing press can print many different books. because the same process is used over and overagain, improvements in this process have enormous effects on product cost and quality. new designs can quickly become products. a new digital system is usually built in an existing foundry that operatesdirectly from a digital description of the design. investing in prototypes is not necessary, because it is possible tosimulate a product accurately and automatically from the design. dramatic system advances enable dramatic application advances. the fact that computing power per dollar doublesevery 18 months means that capabilities can migrate from the high end to the consumer rapidly and predictably. it isr&d investment at the high end that creates these capabilities. today's desktop workstation was the supercomputer of amere decade ago. today's solutions to the problems of largescale parallelism will enable us to solve tomorrow'smassmarket problems of onchip parallelism.rapid progress has produced successive waves of new companies in diverse areas related to informationtechnology and its applications: integrated circuits, computer hardware, computer software, communications,embedded systems, robotics, video on demand, and others. many of today's major hardware and software firms,including sun microsystems and microsoft, both with revenues of more than $4 billion per year (computerselect, 1994), did not exist 15 years ago, and none except ibm and unisys were in the information business 40years ago. many of these new companies have drawn on ideas and people from federally funded research projects.retaining leadership in information technology is vital to the nationthe u.s. lead in information technology has brought benefits that are clearly valuable to the nation:the jobs and profits generated by the industry itself. the information technology industry is big:revenues attributable to hardware and software sales plus associated services were on the order of $500billion in 1993.3 due to the limitations of what is actually counted inu.s. leadership in information technology15evolving the high performance computing and communications initiative to support the nation's information infrastructurecopyright national academy of sciences. all rights reserved.any given estimate, this figure may be conservative. the jobs and profits that the information technologyindustry complex delivers are worth careful preservation.the advantages that other sectors gain from early access to the best information technology, ahead of ourinternational competitors. learning how to use computers for new tasks sooner allows firms to capturenew market segments and compete more efficiently in old ones. u.s. competitiveness in engineering,manufacturing, transportation, financial services, and a host of other areas depends on u.s.competitiveness in information technology (cstb, 1994b, 1995).the benefits that our citizens gain from information technology in their daily lives. benefits are evident ineducation, medicine, finance, communication, entertainment, information services, and other areas. thelives of americans are improved by 24hour banking, improved fuel economy in automobiles,noninvasive medical diagnosis, and hundreds of uses of computers for generating film sequences and asthe basis for computerized games. the reach and impact of such applications are increasing rapidly.our lead in information technology is fragile, and it will slip away if we fail to adapt. leadership has oftenshifted in a few product generations, and a generation in the information industry can be less than 2 years. as anation we must continue to foster the flow of fresh ideas and trained minds that have enabled the u.s. informationtechnology enterprise as a whole to remain strong despite the fate of individual companies. internationalcompetition is fierce, and it is likely to increase. in the mid1980s japan and europe recognized the strategicimportance of information technology and began investing massively in the fifth generation and esprit efforts,respectively. although these efforts did not yield new technologies to rival our own, their very significantinvestments in research and technology infrastructure are laying the foundation for future challenges.4today, through the efforts of government, academia, and industry, our nation retains its lead and continues toenjoy the benefits associated with it. although many factors contributed, the committee believes that federalinvestment in the advancement of information technology has played a key role.indeed, for nearly 50 years federal investment has helped to train the people and stimulate the ideas that havemade today's computers and many of their applications possible. federal support early in the life cycle of manyideas has advanced them from novelties to convincing demonstrations that attract private investment to productsand services that ultimately add to the quality of u.s. life.the federal investment in computing research has paid rich dividendsin the 1940s and 1950s, much of the federal investment in computing research was for defense (flamm,1988). technical needs such as fire control and intelligence needs such as cryptography and mission planningrequired great computing power.since the early 1960s the federal government has invested more broadly in computing research, and theseinvestments have profoundly affected how computers are made and used, contributed to the development ofinnovative ideas and training of key people, and led to the kinds of advances sampled in table 1.1. figure 1.2shows the corresponding timelines for progress from research topics commercially successful applications.notable among the government efforts stimulating many of these advances have been the advanced researchprojects agency's (arpa's)u.s. leadership in information technology16evolving the high performance computing and communications initiative to support the nation's information infrastructurecopyright national academy of sciences. all rights reserved.vlsi program, an initiative of the past decade (box 1.2), and federal sponsorship of research in laying thegroundwork for the nowubiquitous field of computer graphics (box 1.3) and for sophisticated database systems(box 1.4).table 1.1 some successes of government funding of computing and communications researchtopicgoalunanticipated resultstodaytimesharinglet many people use acomputer, each as if itwere his or her own,sharing the cost.because many people kepttheir work in one computer,they could easily shareinformation. reduced costincreased the diversity ofusers and applications.even personal computers aretimeshared among multipleapplications. informationsharing is ubiquitous; sharedinformation lives on"servers."computer networkingloadsharing among amodest number of majorcomputerselectronic mail; widespreadsharing of software and data;local area networks (theoriginal networks werewidearea); theinterconnection of literallymillions of computers.networking has enabledworldwide communicationand sharing, access toexpertise wherever it exists,and commerce at ourfingertips.workstationsenough computing tomake interactive graphicusefuldisplaced most other formsof computing and terminals;led directly to personalcomputers and multimediamillions in use for science,engineering, and financecomputer graphicsmake pictures on acomputer."what you see is what youget"almost any image ispossible. realistic movingimages made on computersare routinely seen ontelevision and were usedeffectively in the design ofthe boeing 777."windows and mouseﬂuser interface technologyeasy access to manyapplications anddocuments at once.dramatic improvements inoverall ease of use; theintegration of applications(e.g., spreadsheets, wordprocessors, and presentationgraphics)the standard way to use allcomputers.very large integratedcircuit designnew design methods tokeep pace with integratedcircuit technologyeasy access to "siliconfoundries"; a renaissance incomputer designmany more schools trainingvlsi designers; many morecompanies using thistechnologyreduced instruction setcomputers (risc)computers 2 to 3 timesfasterdramatic progress in the"codesign" of hardware andsoftware, leading tosignificantly greaterperformancemillions in use; penetrationcontinues to increaseredundant arrays ofinexpensive disks (raid)faster, more reliable disksystemsraid is more economical aswell: massive datarepositories ride the price/performance wave ofpersonal computer andworkstations.entering the mainstream forlargescale data storage; willsee widespread commercialuse in digital video serversu.s. leadership in information technology17evolving the high performance computing and communications initiative to support the nation's information infrastructurecopyright national academy of sciences. all rights reserved.topicgoalunanticipated resultstodayparallel computingsignificantly fastercomputing to addresscomplex problemsparallel deskside serversystem; unanticipatedapplications such astransaction processing,financial modeling, databasemining, and knowledgediscovery in datamany computer manufacturersinclude parallel computing as astandard offeringdigital librariesuniversal, multimedia (text,image, audio video) access to,all the information in largelibraries; an essential need istools for discovering andlocating informationpending developmentbeginning developmentfrom this record of success we can draw some important conclusions:research pays off for an extended period. the federal investment and the payoff, including the spawningof numerous corporations and multibilliondollar industries, have been continuous for decades.unanticipated results are often the most important results. information sharing is an unanticipated resultof timesharing; whatyouseeiswhatyouget displays and hypermedia documents are unanticipatedresults of computer graphics; electronic mail is an unanticipated result of networking.the fusion of ideas multiplies their effect. distributed systems, such as automated teller machinenetworks, combine elements of timesharing, networking, workstations, and computer graphics. personaldigital assistants, the emerging generation of truly portable computers, combine these elements with newnetworking and powermanagement technologies.research trains people. a major output of publicly supported research programs has been people. somedevelop or create a new concept and start companies to commercialize their knowledge. others form apool of expertise that allows new or existing companies to move quickly into new technologies.the synergy among industry, academia, and government has been highly effective. the flow of ideas andpeople between governmentsponsored and commercial programs is suggested in figure 1.2.u.s. leadership in information technology18evolving the high performance computing and communications initiative to support the nation's information infrastructurecopyright national academy of sciences. all rights reserved.box 1.2 an example of a successful federal r&d program: the arpa vlsiprogramthe arpa vlsi program began in the late 1970s. this program, inspired by the groundbreaking work of carvermead and lynn conway, envisioned that integrated circuit technology could be made available to system designers andthat this would have tremendous impact. the program funded academic research activities as well as the metal oxidesemiconductor implemention service (mosis). mosis provided lowcost, fastturnaround vlsi fabrication services to theresearch community; established by arpa, it was expanded and had access broadened by the national sciencefoundation. the arpa vlsi program is widely regarded to have been a tremendous success. among its notableachievements are the following: developed the concept of the multichip wafer, which allowed multiple designs to share a single silicon fabrication run.together with tools developed to assemble the designs and provide services for digital submission of chip designs, thiscapability made the concept of a lowcost, fastturnaround silicon foundry a reality. several companies were formedbased on these ideas, with vlsi technology inc. being the best known. stimulated development of the geometry engine and pixel planes projects, which used the capabilities of vlsi tocreate new capabilities in lowcost, highperformance threedimensional graphics. the geometry engine project formedthe basis of silicon graphics inc. pixel planes technology is licensed to ivex and division. stimulated development of berkeley unix, which was funded to provide a research platform for the vlsi design tools.this version of unix eventually became the basis for the operating system of choice in workstations, servers, andmultiprocessors. unix went on to become the most widely used vendorindependent operating system, with the codedeveloped at berkeley being key to this development. accelerated understanding of the importance of lowcost, highquality graphics for vlsi design, inspiring the creation ofthe stanford university network (sun) workstation project. together with the unix development from berkeley, thistechnology formed the basis for sun microsystems. developed two of the three risc experiments, the berkeley risc project and the stanford mips project, which weremajor parts of the vlsi program inspired by the possibilities of vlsi technology. these technologies formed the basisfor many risc designs, including those of mips computer systems (now owned by silicon graphics inc.) and sunmicrosystems. sponsored extensive developments in computeraided design (cad) tool design. these led to revolutionaryimprovements in cad technology for layout, design rule checking, and simulation. the tools developed in this programwere used extensively in both academic research programs and in industry. the ideas were developed in commercialimplementations by companies such as vlsi technology, cadnetix, and more recently, synopsis.overall, the arpa vlsi program was a landmark success, not only in creating new technologies and revolutionizingthe computer industry, but also in forming the basis for major new industrial technologies and a number of companies thathave become major corporations.** interestingly, the success of the arpa vlsi program stands in sharp contrast to the department of defense vhsic program, whichbased entirely in industry and is generally regarded to have had only modest impact either in developing new technologies or inaccelerating technology.u.s. leadership in information technology19evolving the high performance computing and communications initiative to support the nation's information infrastructurecopyright national academy of sciences. all rights reserved.figure 1.2 governmentsponsored computing research and development stimulates creation of innovative ideasand industries. dates apply to horizontal bars, but not to arrows showing transfer of ideas and people. table 1.1 is acompanion to this figure.u.s. leadership in information technology20evolving the high performance computing and communications initiative to support the nation's information infrastructurecopyright national academy of sciences. all rights reserved.box 1.3 an example of the impact of federal r&d support in establishing afield: computer graphicsin the middle 1 960s, using a computer on loan from the air force and financial support from the central intelligenceagency, the computer graphics group at harvard university demonstrated a prototype virtual reality system. this workcontributed significantly to the technological and personnel foundation for the evans and sutherland corporation, whichsubsequently provided computerbased equipment for training pilotsequipment that is used today by the u.s. military andby commercial pilots the world over.in the early 1970s the university of utah was host to a leading program in computer graphics. dave evans went thereto found a computer science department, and realizing that his department could not be all things to all people, hespecialized in computer graphics. arpa provided the main research support.at that time nearly all pictures of threedimensional objects were drawn with lines only. the resulting images appearedto be of wire frames. they were not very realistic. the utah group worked mainly on techniques for increasing the realismby omitting parts of objects that were hidden behind other parts and by shading the surfaces of the objects. the resultingpictures were much more realistic.two key developments had particularly significant impact. first, watkins and others, following a suggestion of evans,developed a set of incremental techniques for computing what parts of an object were hidden. the key observation wasthat two parts of the image must be nearly the same if they are close together. when some part of an image has beencomputed, nearby parts are easier to compute than they would be if computed in isolation.second, gouraud and phong and others developed incremental algorithms for shading solid surfaces. prior to theirwork the best images appeared to be made with flat surfaces; each surface was painted a single shade according to theangle between it, the light, and the observer. many workers sought methods for representing objects with curved surfaces,but it was then and is still difficult. instead, gouraud invented a trick. he painted each surface a gradually changing shadein such a way that at the joints between surfaces they had the same shade. with no demarcation line, the human eyethinks the resulting surface is smooth even though it is made of little flat plates. phong went a step further, computinghighlights as if the surface were curved. gouraud shading and phong shading are in standard use everywhere today. it isparticularly interesting to note that when government support started, no one knew that these technologies were possibleand the people who made the key discoveries were not yet involved.the vast implications of computer graphics (whatyouseeiswhatyouget document creation systems, scientificvisualization, the entertainment industry, virtual reality) were of course totally unforeseen at the time that this fundamentalresearch was undertaken. in addition to the specific developments cited above, an essential contribution was the manyindividuals whose training in universities benefited from arpa support. a few of the more prominent are john warnock ofadobe systems ($300 million per year), jim clark (formerly) of silicon graphics inc. ($1 billion per year), henry fuchs ofthe university of north carolina, and ed catmull of pixar. many others carried the knowledge to companies and academicinstitutions throughout the nation.even for defense applications, supporting research on strategically motivated but broadly applicablecomputing and communications technology has clearly been the right approach. in the past, manydefense applications and requirements presaged commercial applications and requirements. today,commercial computer systems and applications often find use in defense environments.research and development take time. at least 10 years, more often 15, elapse between initial research andcommercial success. this is true even for research of strategic importance. and it is true in spite of therapid pace of today's product development, as indicated in the timeline for recent commercial successessuch as risc and windows (see figure 1.2).u.s. leadership in information technology21evolving the high performance computing and communications initiative to support the nation's information infrastructurecopyright national academy of sciences. all rights reserved.box 1.4 federal r&d support propels database technologythe database industry generated about $7 billion in revenue in 1994 and is growing at a rate of 35 percent per year.all database industry leaders are u.s.based corporations: ibm, oracle, sybase, informix, computer associates, andmicrosoft. in addition, there are two large specialty vendors, both also u.s.based: tandem, selling over $1 billion per yearof faulttolerant transaction processing systems, and at&tteradata, selling about $500 million per year of data miningsystems. in addition to these wellestablished companies, there is a vibrant group of small companies specializing inapplicationspecific databases, objectoriented databases, and desktop databases.a very modest federal research investment, complemented by a modest industrial research investment, led directly tou.s. dominance of this market. it is not possible to list all the contributions here, but three representative research projectsare highlighted that had major impact on the industry.1. project ingres started at the university of california, berkeley, in 1972. inspired by codd's landmarkpaper on relational databases, several faculty members (stonebraker, rowe, wong, and others) started aproject to design and build a system. incidental to this work, they invented a query language (quel),relational optimization techniques, a language binding technique, and interesting storage strategies.they also pioneered work on distributed databases.the ingres academic system formed the basis for the ingres product now owned by computer associates. studentstrained on ingres went on to start or staff all the major database companies (at&t, britton lee, hp, informix, ibm, oracle,tandem, sybase). the ingres project went on to investigate distributed databases, database inference, active databases,and extensible databases. it was rechristened postgres, which is now the basis of the digital library and scientific databaseefforts within the university of california system. recently, postgres spun off to become the basis for a new objectrelational system from the startup illustra information technologies.2. system r was ibm research's response to codd's ideas. his relational model was at first verycontroversial; people thought that the model was too simplistic and that it would never perform well.ibm research management took a gamble and chartered a small (10person) effort to prototype arelational system based on codd's ideas. that effort produced a prototype, system r, that eventuallygrew into the db2 product series. along the way, the ibm team pioneered ideas in queryoptimization, data independence (views), transactions (logging and locking), and security (the grantrevoke model). in addition, the sql query language from system r was the basis for the ansi/isostandard.the system r group went on to investigate distributed databases (project r*) and objectoriented extensibledatabases (project starburst). these research projects have pioneered new ideas and algorithms. the results appear inibm's database products and those of other vendors.3. the university of wisconsin's gamma system was a highly successful effort that prototyped a highperformance parallel database system built of offtheshelf system components.during the 1970s there had been great enthusiasm for database machinesspecialpurpose computers that would bemuch faster than generalpurpose systems running conventional databases. these research projects were often based onexotic hardware like bubble memories, headpertrack disks, or associative random access memory. the problem wasthat generalpurpose systems were improving at a rate of 50 percent per year, and so it was difficult for exotic systems tocompete with them. by 1 980, most researchers realized the futility of specialpurpose approaches, and the databasemachine community switched to research on using arrays of generalpurpose processors and disks to process data inparallel.the university of wisconsin was home to the major proponents of this idea in the united states. funded bygovernment and industry, researchers prototyped and built a parallel database machine called gamma, whose hardwarebase was an early intel scalable parallel machine. that system produced ideas and a generation of students who went onto staff all the database vendors. today, the highly successful parallel database systems from ibm, tandem, oracle,informix, sybase, and at&t all have a direct lineage from the wisconsin research on parallel database systems. the useof parallel databases systems for data mining is now the fastestgrowing component of the database server industry.the gamma project evolved into the exodus project at wisconsin (focusing on an extensible objectorienteddatabase). exodus has now evolved to the paradise system, which combines objectoriented and parallel databasetechniques to represent, store, and quickly process huge earthobserving satellite databases.source: james gray and others for the computing research association; reproduced with permission.u.s. leadership in information technology22evolving the high performance computing and communications initiative to support the nation's information infrastructurecopyright national academy of sciences. all rights reserved.continued federal investment is necessary to sustain our leadwhat must be done to sustain the innovation and growth needed for enhancing the information infrastructureand maintaining u.s. leadership in information technology? rapid and continuing change in the technology, a 10to 15year cycle from idea to commercial success, and successive waves of new companies are characteristics ofthe information industry that point to the need for a stable source of expertise and some room for a longtermapproach. three observations seem pertinent.1. industrial r&d cannot replace government investment in basic research. very few companies are able toinvest for a payoff that is 10 years away. moreover, many advances are broad in their applicability and complexenough to take several engineering iterations to get right, and so the key insights become ''public" and a singlecompany cannot recoup the research investment. public investment in research that creates a reservoir of newideas and trained people is repaid many times over by jobs and taxes in the information industry, more innovationand productivity in other industries, and improvements in the daily lives of citizens. this investment is essential tomaintain u.s. international competitiveness.of course, industrial r&d also contributes to the nation's pool of new ideas, but a company may postponeexploiting its ideas if they disturb existing business. a good example is the evolution of risc processors shown infigure 1.2. risc was invented by john cocke, an ibm researcher, but ibm made no risc products for a decade.only after the ideas were embraced and extended in governmentsponsored work at universities did industry adoptthem, and this adoption was initiated by young companies, including sun microsystems, and startups, includingmips. now, a decade later, ibm is one of the leaders in exploiting risc technology, but the cost to ibm of thisdelay has been significant. firms have regularly failed to adapt to change as evidenced by the departure from thecomputer business of ge, rca, honeywell, philco, perkinelmer, control data, and prime; the folding togetherby merger of other manufacturers; and major downsizing at ibm and dec. it often is easier for a startup to form,raise venture capital, and succeed than for an established firm to abandon a currently successful direction in favorof a new approach just when the old approach is most financially successful. even in a vigorous industrial r&dclimate, then, federal investment in research is necessary, both for its longterm focus and for its ability to incubateideas to the point of clear commercial viability.but the need for federal investment in research is further compounded by the fact that industrial r&d isalready under stress. in the computing hardware and software sector, for example, although a small number of newr&d enterprises have been launched, most notably by microsoft, there has been a general consolidation ofresources by companies such as ibm and dec, including an apparent reduction in their research effort or at least agreater emphasis on shortterm r&dša change in emphasis is evident to insiders and close observers but not easyto document.5 the industrywide level of r&d as a percentage of sales has also been brought down by thetendency of lowprice vendors, such as dell and gateway, to ride on the research conducted by others.the trend toward reduced industrial r&d appears also in the telecommunications industry. the 1984divestiture of at&t led to a smaller bell laboratories and to the creation of bell communications research(bellcore), a shared research facility for the seven regional bell holding companies. recent deregulation hasencouraged a reduction of basic research at both at&t and bellcore. lacking significant research capability at itsindividual service companies, the cable television industry depends on research done by itsu.s. leadership in information technology23evolving the high performance computing and communications initiative to support the nation's information infrastructurecopyright national academy of sciences. all rights reserved.hardware vendors and its shared cablelabs. although more new technology has been deployed intelecommunications since deregulation in the early 1980s, and although in both computing and communicationsthere are more companies selling products now than there were 15 years ago, today's sales are based on yesterday'sresearch and do not guarantee a sufficient foundation for tomorrow's sales. competition in an industry canpromote technological growth, but competition alone is not the source of innovation and leadership.because of the long time scales involved in research, the full effect of decreasing investment in research maynot be evident for a decade, but by then, it may be too late to reverse an erosion of research capability. thus, eventhough many privatesector organizations that have weighed in on one or more policy areas relating to theenhancement of information infrastructure typically argue for a minimal government role in commercialization,they tend to support a continuing federal presence in relevant basic research.62. it is hard to predict which new ideas and approaches will succeed. over the years, federal support ofcomputing and communications research in universities has helped make possible an environment for explorationand experimentation, leading to a broad range of diverse ideas from which the marketplace ultimately has selectedwinners and losers. as demonstrated by the unforeseen applications and results listed in table 1.1, it is difficult toknow in advance the outcome or final value of a particular line of inquiry. but the history of development incomputing and communications suggests that innovation arises from a diversity of ideas and some freedom to take alongrange view. it is notoriously difficult to place a specific value on the generation of knowledge andexperience, but such benefits are much broader than sales of specific systems.3. research and development in information technology can make good use of equipment that is 10 years inadvance of current "commodity" practice. when it is first used for research, such a piece of equipment is often asupercomputer. by the time that research makes its way to commercial use, computers of equal power are nolonger expensive or rare. for example, the computer graphics techniques that are available on desktopworkstations today, and that will soon be on personal computers and settop boxes, were pioneered on themultimilliondollar supercomputers of the 1960s and 1970s. part of the task in information technology r&d is tofind out how today's supercomputers can be used when they are cheap and widely available, and thus to feed theindustries of tomorrow.the largescale systems problems presented both by massive parallelism and by massive informationinfrastructure are additional distinguishing characteristics of information systems r&d, because they imply a needfor scale in the research effort itself. in principle, collaborative efforts might help to overcome the problem ofattaining critical mass and scale, yet history suggests that there are relatively few collaborations in basic researchwithin any industry, and purely industrial (and increasingly industryuniversity or industrygovernment)collaborations tend to disseminate results more slowly than universitybased research.the governmentsupported research program (on the order of $1 billion for information technology r&d) issmall compared to industrial r&d (on the order of $20 billion; coy, 1993), but it constitutes a significant portionof the research component, and it is a critical factor because it supports the exploratory work that is difficult forindustry to afford, allows the pursuit of ideas that may lead to success in unexpected ways, and nourishes theindustry of the future, creating jobs and benefits for ourselves and our children. the industrial r&d investment,though larger in dollars, is different in nature: it focuses on the neartermšincreasingly so, as noted earlieršandis thus vulnerable to major opportunity costs. the increasing tendency to focus on the nearterm isu.s. leadership in information technology24evolving the high performance computing and communications initiative to support the nation's information infrastructurecopyright national academy of sciences. all rights reserved.affecting the body of the nation's overall r&d. despite economic studies showing that the united states leads theworld in reaping benefits from basic research, pressures in all sectors appear to be promoting a shift in universitiestoward nearterm efforts, resulting in a decline in basic research even as a share of university research (cohen andnoll, 1994). thus, a general reduction in support for basic research appears to be taking place.it is critical to understand that there are dramatic new opportunities that still can be developed byfundamental research in information technologyšopportunities on which the nation must capitalize. these includehighperformance systems and applications for science and engineering; highconfidence systems for applicationssuch as health care, law enforcement, and finance; building blocks for globalscale information utilities (e.g.,electronic payment); interactive environments for applications ranging from telemedicine to entertainment;improved user interfaces to allow the creation and use of ever more sophisticated applications by ever broadercross sections of the population; and the creation of the human capital on which the next generation's informationindustries will be based. fundamental research in computing and communications is the key to unlocking thepotential of these new applications.how much federal research support is proper for the foreseeable future and to what aspects of informationtechnology should it be devoted?7 answering this question is part of a larger process of considering how toreorient overall federal spending on r&d from a context dominated by national security to one driven more byother economic and social goals. it is harder to achieve the kind of consensus needed to sustain federal researchprograms associated with these goals than it was under the national security aegis. nevertheless, the fundamentalrationale for federal programs remains (cohen and noll, 1994, p. 73):that r&d can enhance the nation's economic welfare is not, by itself, sufficient reason to justify a prominent role forthe federal government in financing it. economists have developed a further rationale for government subsidies.their consensus is that most of the benefits of innovation accrue not to innovators but to consumers through productsthat are better or less expensive, or both. because the benefits of technological progress are broadly shared,innovators lack the financial incentive to improve technologies as much as is socially desirable. therefore, thegovernment can improve the performance of the economy by adopting policies that facilitate and increaseinvestments in research.today the hpcci is the umbrella for most governmentsponsoredcomputing and communications researchtoday, the hpcci is the umbrella sheltering most governmentsponsored computing and communicationsresearch. the hpcci is thus responsible for sustaining the nation's lead in information technology. it centers onrising performance as the driver for progress across a wide range of technologies."highperformance" means bringing a large quantity of computing and communications to bear on oneproblem. it is far broader than "supercomputing," which was the focus of early public policy in this area. it is also amoving targetšthe threshold for what is considered "highperformance" advances, as everincreasingperformance levels become more broadly available. the supercomputer of this generation is the group server ofthe next generation and the personal computer of the generation after that. the same is true for communications:today's leading edge is tomorrow's mainstream.focusing research on the leading edge of performance accelerates the broad availability of what starts out aslimitedaccess technology, in the following ways:u.s. leadership in information technology25evolving the high performance computing and communications initiative to support the nation's information infrastructurecopyright national academy of sciences. all rights reserved.by advancing the hardware and software systems themselves. many techniques now used to buildmainstream computers and their software were originally developed for highperformance computing:specialized floatingpoint processing, pipelining, multiprocessing, and multiple instruction issue, amongothers;by creating new applications today so that they will be mature when the hardware that can run them ischeap and ubiquitous. it can take much longer to develop and fully exploit a new application than to build anew computer. overcoming this lag is one of the drivers of work on the grand and national challengeconcepts; andby attacking problems that would otherwise be beyond reach for several years, thus speeding up thedevelopment of new fields of science, engineering, medicine, and business. national access to machineswith 100 to 1,000 times the memory and speed of researchers' desktop machines allows them to makequalitative jumps to exploring frontier research problems of higher dimensionality, greater resolution, ormore complexity than would otherwise be possible.fundamental but strategic research under the hpccišwhich now encompasses most of the academiccomputing research sponsored by the federal governmentšcreates a strong pull on the computer science andengineering research community, the user community, and the hardware, software, and telecommunicationsvendors. for example, it was evident to individuals in the computing and communications research communitythat as vlsi circuit technology developed, it would favor computing structures based on the largescale replicationof modest processors, as opposed to the smallscale replication of processors of the highest attainable individualperformance. (the highestspeed circuits are expensive to design, produce, maintain, and operate.) this vision ofhighperformance computing and communications based on parallelism brought three major technical challenges:(1) interconnection and memory architecturešhow to unite large numbers of relatively inexpensive processorsinto systems capable of delivering truly highperformance, and (2) programmingšhow to program suchcollections of processors to solve large and complex problems. in the networking arena, the obvious issues oflargescale, widespread use and highspeed transmission were compounded by added problems of connectingheterogeneous systems and achieving high reliability.the technical and economic imperatives that led to the hpcci are discussed in some detail in appendix a.hpcci was, and continues to be, an appropriate thrust. as chapter 2 discusses, the hpcci can boast a broadrange of very significant accomplishments. however, the committee sees an unhealthy dependence of our nation'scritical leadership in information technology on the fate of a single initiative. first, not all important computingresearch is focused on highperformance, although the politics of funding have encouraged researchers andagencies to paint everything with an hpcci brush. second, we cannot afford to cripple computing research if thenation's attention and resources turn away from any single goal. at the beginning of the hpcci in 1992, itsincreasing momentum made association with it attractive and helped the initiative attain both intellectual andfinancial critical mass. the rise of the national information infrastructure initiative, though, underscores howchangeable the federal funding process may be.we must move toward a more mature approach in which a substantial focus on goals of obvious nationalimportance is combined with a diversified program of longterm exploration of important research problems thatsupport the strategic information technology industry. we can change the hpcci's name, we can change itsorientation, but we must move forward. continuing the momentum of this successful initiative is essential toongoing u.s. prosperity and leadership in information technology.u.s. leadership in information technology26evolving the high performance computing and communications initiative to support the nation's information infrastructurecopyright national academy of sciences. all rights reserved.2the high performance computing and communicationsinitiativethe highperformance computing and communications initiative (hpcci) has been the focal point offederal support for u.s. computing and communications research and development since 1989. it became officialin 1991 with office of science and technology policy (ostp) support and enactment of the highperformancecomputing act of 1991. it includes five programs: advanced software technology and algorithms, basicresearch and human resources (although there is basic research in the other four programs also), highperformance computing systems, national research and education network (nren), and since fy 1994,information infrastructure technology and applications (iita).1 appendix a outlines the origins and early historyof the hpcci, including an explanation of associated technology trends and indications of evolution of theinitiative's emphases. this chapter discusses the hpcci's goals and contributions to date and identifies keysubstantive and practical issues to be considered as the initiative evolves.2hpcci: goals and emphasesthe hpcci has several broad goals (nco, 1993): in highperformance computing and networking technologies; disseminate the technologies to accelerate innovation and serve the economy, national security,education, and the environment; and gains in u.s. productivity and industrial competitiveness.because these goals relate advances in computing and communications technologies to the achievement ofbenefits from their use, the hpcci has from its inception provided for the joint advancement of technologies andapplications. the hpcci has pursued several specific strategic objectives.basic objectivesteraflop capabilitythe specific objective for computer development was to develop teraflop capability by the mid1990s.3 thisobjective was comparable to two that had been achieved earlier by forerunners of today's computer sciencecommunity at the request of the federal government: peak availablethe high performance computing and communications initiative28evolving the high performance computing and communications initiative to support the nation's information infrastructurecopyright national academy of sciences. all rights reserved.computing power was increased by several orders of magnitude during world war ii, when federal interestsin cryptanalysis and other wartime needs led to the development of the vacuum tube computer, and in the late1950s, when federal interests in military command and control led to transistorized computers.4 as economistkenneth flamm has observed, "by tracking the origins and history of key pieces of technology, a simple butimportant point can be established: at certain, crucial moments in history, private commercial interests, in theabsence of government support, would not have become as intensely involved in certain longterm basic researchand radical new concepts" (flamm, 1988, p. 3). the teraflop objective has inspired parallel multimicroprocessorcomputers as the means for providing the next major jump in computer power.5the teraflop objective has generated both attention and misunderstanding. progress required building anumber of machines large and fast enough to reward software researchers and application users with major gainsin performance, thereby motivating them to develop the code that could make the highperformance machinesuseful. (see appendix a for more information on the development of highperformance hardware and softwareand their interaction.) the costliness of this undertaking, compounded by the highly publicized financialdifficulties of two entrepreneurial ventures, thinking machines corporation (tmc) and kendall square research(ksr), aimed at commercializing massively parallel computing systems, attracted criticism of the hpcci.however, that criticism appears largely misdirected. first, entrepreneurial ventures are always risky, and thetwo in question suffered from managerial weakness at least as much as questionable technology choices.6contemporaneously, more established firms (e.g., cray research, ibm, intel supercomputing, convex computer,and silicon graphics inc.; parkersmith, 1994a) have persevered, and others (e.g., hitachi and nec in japan;kahaner, 1994b, and parkersmith, 1994b) have entered or expanded their presence in the parallel systemsmarket. second, focusing attention on the high initial costs for stimulating development and use of parallelprocessing systems detracts from the achievement of successful proofs of concept and dissemination of newapproaches to computation.although the teraflop objective was ambitious for the time scale set, it was intended as a driver and thus isbest viewed as indicating a direction, not a destination; the need for progress in computing will continue beyondthe teraflop capability.7 in that respect, its appropriateness was affirmed by the 1993 branscomb panel.8 theteraflop objective has, in fact, served to focus attention on the task of combining and harnessing vast amounts ofcomputer power from many smaller computers. the technology is now sufficiently developed that a teraflopmachine could be realized today, although exactly when to do so should be left to the economics of users and theirapplications. 9highspeed networksanother directionsetting objective of the hpcci was the achievement of data communications networksattaining speeds of at least 1 gigabit per second. although by the mid 1980s major telecommunications networksalready had gigabitplus trunk circuits in their backbones, the hpcci was intended to lead to much broaderdeployment of and access to gigabitspeed networks connecting generalpurpose computers.10 this objective droveprogress in switching, computing hardware and software, interfaces, and communication protocols.11 (seeappendix b.)grand challengesa third original objective related to applications of highperformance computing and communicationstechnologies: to define and attack grand challenge problems. highperformancethe high performance computing and communications initiative29evolving the high performance computing and communications initiative to support the nation's information infrastructurecopyright national academy of sciences. all rights reserved.computing and communications national centers in the various agencies already were providing access to tensof thousands of researchers in hundreds of institutions when the hpcci began. drawing from this national base ofusers, various hpcci agencies have defined a series of grand challenge problems (see appendix d for list) andchosen teams to attack them. the grand challenge teams are typically both interdisciplinary and multiinstitutional. the scientific problems are picked for their intrinsic scientific merit, the need for highperformancecomputation and communications, the opportunities for synergistic interaction of computer scientists withcomputational scientists, and the scientific and societal benefits to be gained from their solution. for example,better weather prediction involves solving massive sets of equations, experimenting with models, and comparingthe results obtained with them to increasingly large volumes of data collected by weathermonitoringinstruments.12 highperformance computing provides the faster model computation essential to timely assessmentof a sufficiently large volume of alternative weather patterns for a given period (e.g., a month).13 the resultsinclude not only greater scientific understanding, but also the benefits to businesses, individuals, and governmentsthat come from faster, more accurate, and more detailed forecasts.expanded objectivesthe set of hpcci objectives has been expanded through legislative and agency activities. the highperformance computing act of 1991, public law 102194, broadened the applications concerns to include thesocalled national challengesšexplorations of highperformance computing and communications technology forapplications in such areas as education, libraries, manufacturing, and health care. pl 102194 also reinforced thecommunications aspects of the hpcci, elaborating the concept and objectives for the nren program andemphasizing networking applications in education. officials involved with the hpcci have noted that although pl102194 was never complemented by specific appropriations legislation, its principles have driven hpcciactivities in relevant agencies, including early explorations relating to national challenges and the formation ofthe information infrastructure technology and applications (iita) component in fy 1994.14 the nationalchallenges, iita, and the network aspects of pl 102194 also included attention to shortterm and practicalconcerns (e.g., expanding access to technology facilities and capabilities), complementing the longterm, basicresearch problems that remain at the heart of hpcci.15the tension between longterm and shortterm, between basic research and applications, is fundamental to thedifferences in opinion voiced about the hpcci and its merits, accomplishments, and desired directions. based onits direct observations of work funded under the hpcci and on its discussions with others in universities,industry, and the government, the committee affirms the value of the basic research associated with the hpcci,research that is informed by needs associated with important applications of national interest.a fundamental issue shaping the evolution of the hpcci is the balance to be struck between the support ofapplications that use highperformance computing and communications technologies and the support for computerscience research on new highperformance computing and communications technologies.16 the committee'sanalysis of the fy 1995 hpcci budget request (appendix c) shows that out of the total request of $1.15 billion,$352 million (30 percent) would be invested in basic research in computer, software, or communicationtechnologies; $205 million (18 percent) in applied computer science research in common applications support,artificial intelligence, and humanmachine interface; $176 million (15 percent) in direct support of applications andcomputational science; and $297 million (26 percent) in supercomputing and communications infrastructure. it ishard to interpret these statistics, however, without an understanding of the nature and the value of the work labeled"applications." the hpcci has been aimed at catalyzing a paradigm shift, which involves the synergisticinteraction of people developingthe high performance computing and communications initiative30evolving the high performance computing and communications initiative to support the nation's information infrastructurecopyright national academy of sciences. all rights reserved.the technology and people using the technology.17 the hpcci includes missionrelated activities that maydrive computing and communications research and development (r&d) and/or applications that call forsignificant technology development.within the computer science and engineering field, there has been considerable debate over the degree towhich computing research should be driven by applications concerns as opposed to intrinsic computer scienceconcerns, given that both approaches to research have historically yielded considerable spinoffs to other sciencesand the economy.18 to computer scientists and engineers, hpcci is viewed as the first major federal initiativethat emphasizes the science of computing and communications, which is addressed in conjunction with explorationof problems involving other fields of science and engineering, loosely aggregated as computational science. tocomputational scientists, the emphasis is predictably on the problems in their domains and on the difficulty ofdeveloping appropriate domainspecific computational techniques. these differences in outlook result indifferences in what each community calls an "application," as well as differences in requirements for r&d.hpcci accomplishmentsaccomplishments under the hpcci to date reveal two key trends: better computing and computationalinfrastructure and increasing researcherdeveloperuser synergy. in the committee's expert judgment, hpcci hasbeen generally successful. that assessment is necessarily qualitative and experiential, because it is too early yet toobserve the full impact of the initiative.the issue of measurementearly measurement of the impact of hpcci research is problematic. as chapter 1 points out, the time forprogress from idea to product involves a decade or more, well beyond a single fiscal year. independent of impact,individual projects may take a few years simply to reach completion.19 consequently, the accomplishments of thehpcci are only just becoming apparent.moreover, it is difficult to evaluate early on how good individual ideas are and what their worth may prove tobe. many researchers have expressed concern that the push for immediately measurable results has led to anunrealistic emphasis on shortterm gains and has diverted efforts from conducting productive research tomaintaining "paper trails."20 however, the pressures on agencies to maximize the return on limited research fundsseems to discourage funding of more innovativešand therefore riskieršexploration that may not necessarilysucceed (rensberger, 1994). the problem of measurement is compounded by the fact that a considerable amountof hpcci research addresses enabling technologies whose benefits or outcomes may be evident only indirectly.how best to assess results is unclearškey questions include the kinds of reviews already undertaken byagencies and with what effect; how evaluations based on outside expertise should be combined with inhouseagency knowhow; whether to focus on reviewing progress for a program as a whole or progress in individualgrants; the costs in time and money of different approaches and comparison of the benefits in terms of reviewquality, scope, and timeliness to the costs; and so on. the committee recognizes that data and analysis are neededto support decision making about any new approaches to evaluation; it did not have the time or resources to pursuesuch analysis.21 moreover, complementing the committee's observation that much of the evidence on outcomes isanecdotal is a recent national research council study pointing out that good, relevant data (on scientific researchin general) are hard to find and even harder to draw inferences from (cpsma, 1994).the high performance computing and communications initiative31evolving the high performance computing and communications initiative to support the nation's information infrastructurecopyright national academy of sciences. all rights reserved.better computing and computational infrastructurethe hpcci has contributed substantially to the development, deployment, and understanding of computingand communications facilities and capabilities as infrastructure. it has helped transform understanding of how toshare resources and information, generating proofs of concept and understanding that are of value not only to thescientific research community but also to the economy and society at large.the hpcci has directly strengthened academic computing and computational infrastructure, building on thenational science foundation's (nsf's) significant investments in university computing infrastructure over morethan a decade.22 the nsf infrastructure program has stimulated complementary investments by other federalagencies, industry, and universities themselvesšan impact that, like other hpcci contributions to stimulating agrowing foundation of activity, is difficult to assess directly. this academic base, in particular, academic researchin experimental computer science and engineering,23 is fundamental to the development and application of highperformance and other computing and communications technologies (cstb, 1994a).by providing access (often over the internet) to stateoftheart computer resources and to expertise to helpresearchers learn how to use them, the hpcci has also enabled research in a wide range of science andengineering disciplines to be performed that would not otherwise have been possible. appendix d lists relevantexamples from the grand challenge activities, and appendix e points out instances related to the nsfsupercomputer center activities, which fall under the hpcci umbrella despite having some separate roots.within the nren program, nsfnet and other components such as esnet and the nasa science internethave helped to extend networking across the science research community (cstb, 1994d). through theinternetworking provided by the internet, connectivity and experimentation with networkbased infrastructurehave begun to spread rapidly beyond the research community into primary and secondary education, government,industry, and other elements of the private sector (cstb, 1994d). the internet has demonstrated the value ofwidespread access to a common, sophisticated, and increasingly integrated technology base, and it illustrates how asmall investment by the federal government can be highly leveraged by additional investments from industry.24the hpcci approach to developing highperformance computing and communications infrastructure hasbeen affirmed in similar steps taken recently by the japanese government and industry. david kahaner of theoffice of naval research has chronicled fujitsu's progress in developing parallel processing technology, noting itsestablishment of research facilities providing worldwide access to its systems in order to obtain the large user baseneeded to refine its hardware designs and, in particular, to develop the software and applications required to makesystems successful (kahaner, 1994a). kahaner has also reported on japanese plans and progress for upgradinghighperformance capabilities in public institutions, noting, among other things, the japanese government'sincreasing emphasis on basic research.25increasing researcherdeveloperuser synergythe hpcci has fostered productive interactions among the researchers and developers involved in creatinghighperformance computing and communications technology and those who use this technology in their ownwork, most notably computational scientists, but also a broad spectrum of other users. building on the varyingneeds and perspectives of the three groups, complex problems are being solved in unique ways.in particular, the hpcci has funded crossdisciplinary teams associated with the grand challenge projects tosolve complex computational problems and produce new software for the newthe high performance computing and communications initiative32evolving the high performance computing and communications initiative to support the nation's information infrastructurecopyright national academy of sciences. all rights reserved.parallel systems. these teams interact with hardware and operating system/compiler researchers and developers toaddress complex problems through use of the largest computer systems, including those housed at the nsfsupercomputer centers. their work has provided vendors with key insights into the limitations of theirarchitectures.26 although these users' requirements are more specialized than those typical of the commercialmarket for parallel systems, such collaborative work has contributed to enhancing the development and applicationof highperformance computing and communications technologies. for example, astrophysicists' work onproblems in cosmology has stimulated improved handling of fast fourier transforms in highperformance systemcompilers that has also benefited commercial applications of seismology in oil exploration.27like collaboration in other areas, that between computer and computational scientists has not always comeeasily. in particular, there has been some controversy concerning the relative emphasis on advancing disciplinaryknowledge, on the one hand, and advancing the state of the art in highperformance computing andcommunications, on the other. nevertheless, the hpcci has provided a structure and a set of incentives to fostercollaborations that many computational scientists believe would not be supported under programs aimed atnurturing individual disciplines. 28impact of broad collaborationmany notable hpcci accomplishments are the result of broad collaborations. in many instances, they buildon foundations that predated the hpcci, although hpcci funding, facilities, and focus may have provided thepush needed for their realization. the mosaic browser (box 2.1) epitomizes both the cumulative nature and broadimpact of the development of technologies associated with the hpcci. the hpcci has driven progress on grand challenge problems in disciplines such as cosmology,molecular biology, chemistry, and materials science. parallel computation has enhanced the ability to dorapid simulations in science and engineering (kaufmann and smarr, 1993). recognition of thisdevelopment continues to spread across the research community. the hpcci has furthered the development of new modes of analyzing and/or visualizing complex dataand in many cases has contributed to more effective interworking between supercomputers and desktopgraphics workstations. visualizations of the numerical output of the largest computers require specializedgraphics computers, whose speed would have made them supercomputers in their own right a few yearsago. examples include visualization of complex motions of large biomolecules, intricate engineeringassemblies, and the largescale structure in the universe. the hpcci has made parallel computing widely accepted as the practical route to achieving highperformance computing, as can be seen in the recent growth in sales of parallel systems.29 although themarket for largerscale parallelprocessing systems is inherently small, it is nevertheless growing. box 2.2gives a few of many possible examples of the applications being developed.the high performance computing and communications initiative33evolving the high performance computing and communications initiative to support the nation's information infrastructurecopyright national academy of sciences. all rights reserved.box 2.1 mosaic and the world wide webthe development in 1993 of the national center for supercomputing applications (ncsa) mosaic browser shows howthe hpcci has been able to create successful new applications enabled both by new capabilities and by priordevelopments in information technology.the forerunner of the internet (arpanet) was developed in the late 1 960s to link computers and scientistsperforming defenserelated research throughout the united states. by the time of the hpcci's formal initiation in fy 1992,the internet had become the most popular network linking researchers and educators at the postsecondary levelthroughout the world. the development of gopher at the university of minnesota in the early 1 990s was a key step inestablishing the internet as an information resource that could be used through a consistent user interface. at about thesame time, researchers at the european laboratory for particle physics, cern, had developed and implemented theworld wide web (www), a networkbased hypertext system that allowed the linking of one piece of information toanother across the network. users accessed www information through ''browsers" that allowed them to activate ahypertext link in a document to retrieve and display related information regardless of its physical location. early browserswere textbased, presenting the user with a menu of numbered choices, whereas slightly later browsers made use of the"pointandclick" capabilities of the mouse within a graphical user interface. the www and its browsers sought to presentusers with a consistent interface with which to access all existing methods of internet navigation and information retrieval.meanwhile, the hpcci had provided funding for research into advanced networking technologies and for thedeployment of a highcapacity backbone, enabling the rapid transfer of large amounts of data across the network. in 1993,software developers at the ncsa, one of the centers supported by hpcci funds from the national science foundation,developed an easytouse graphical browser for the www known as ncsa mosaic, or sometimes simply mosaic. itallowed the inclusion of images in www documents and even allowed the images themselves to be links to otherinformation. continuing development of mosaic enabled the inclusion of audio and video "clips" within hypermediadocuments. by november 1993, mosaic browsers were available for the three most popular computer operatingenvironments: apple macintosh, microsoft windows, and x window on unix platforms. one year later, users havedownloaded more than 1 million copies of mosaic software, ncsa's scalable www server (the world's busiest) is handlingover 4 million connections per week, and mosaic is credited by many for the current and dramatic surge in use of andinterest in the internet.perhaps even more significantly, mosaic has served as the genesis of a wide range of commercial developments. theuniversity of illinois, which owns the intellectual property associated with mosaic, has named spyglass, inc. as the mastersublicenser for the software. so far over 20 companies have licensed mosaic, creating over 12 million commerciallylicensed copies. in addition, other companies such as netscape, ibm, pipeline, booklink, mcc, and netmanage havecreated alternative www browsers.many other entities have become information providers (a 100fold increase in www servers has occurred in the last 2years), and new security additions to the underlying mosaic/www infrastructure have enabled electronic commerce on theinternet. spectacular growth in commercial use of this new information infrastructure is expected in 1995 and beyondbecause of the relative ease with which the mosaic/www combination allows for highly accessible information servers tobe established on the global network. because of the decentralized nature of the internet, it will be difficult to gauge thetotal business income generated by the introduction of mosaic; however, there is already enough commercial activity tobelieve that there will be significant payback on the critical federal hpcci investment in the nsf supercomputer centersthat led to this unexpected software development. even more important is the paradigm shift in the use of the internet thatmosaic and the www have generated.the high performance computing and communications initiative34evolving the high performance computing and communications initiative to support the nation's information infrastructurecopyright national academy of sciences. all rights reserved.box 2.2 solutions to complex problemsparallel computing has enabled creative solutions to a number of industrial and scientific problems. the followingexamples are but a few of many possible illustrations of such successes. defense. simulation of the interaction between the electromagnetic spectrum and various aircraft design features hasenhanced the performance of stealth aircraft. parallel computing enabled rapid calculations for many different wavelengths and aircraft parts. petroleum. oil exploration and production have been made more productive by threedimensional analysis andvisualization of huge amounts of seismic data. parallel computing enabled the move from two to threedimensionalprocesses. finance. forecasting and simulation of various trading strategies for mortgagebacked security instruments has created anew market and contributed to a reduction in rate spreads. parallel computing enables simultaneous calculations fornumerous instruments within the very short time frames of a fastmoving market. growth of the finance market arenawill provide market pull that should help lower the costs of highperformance processing systems for all types of users. the hpcci has provided large numbers of academic scientists with peerreviewed access to their choiceof highperformance computing architecture to enable their computational science projects. the nsfsupercomputing centers, whose core budgets are entirely within the hpcci, provided access to 23 highperformance computing machines in fy 1995 to 7,500 academic researchers in over 200 universities. thecapacity of the centers' supercomputers was 75 times as great as in fy 1986, their first full year ofoperation, and users represented a broad range of scientific and engineering disciplines (appendix e listsrepresentative projects). the internet, the flagship of the nren component of the hpcci, has become the basis for computermediated communication and collaboration among researchers and others. the geographical dispersionof grand challenge team members has resulted in pioneering use of electronic collaboration methods,beginning with conventional electronic mail and expanding to multimedia electronic mail, audio andvideo conferencing, and shared tools for accessing and using remotely stored data and for controllingremote instruments. development of these methods and tools has been fostered and funded by thehpcci, demonstrating the potential for electronic collaborators and other approaches to usinginformation technology to support distributed work (cstb, 1993, 1994e). the internet plus a collection of advances and applications in data storage, analysis, retrieval, andrepresentationšsome involving highperformance technologyšhas catalyzed exploration of digitallibrary concepts. early internetbased collaborations among computer scientists, information scientists,cognitive and social scientists, and domainspecific groups provided the basis for a multidisciplinaryresearch effort in digital libraries under the iita program that was launched in mid1993 (nsf, 1993). the gigabit network testbeds, an element of the nren component of the hpcci, pioneered in advancingthe frontiers of communication bandwidth essential to achieving enhancements envisioned for thenation's information infrastructure. in the process, they helped to bridge the gap in perspective andemphasis between the computing and communications research communities.the high performance computing and communications initiative35evolving the high performance computing and communications initiative to support the nation's information infrastructurecopyright national academy of sciences. all rights reserved.transfer of expertise and technologyin addition to enabling explicit collaborations, the hpcci has indirectly affected industry and other sectorsoutside of academia by stimulating the spread of human experts and thus the transfer of technology, building on atradition of interaction typical of the computing field. as kenneth flamm observed, "people are clearly the mediumin which computer technology has been stored and transmitted. the history of the computer industry is the historyof trained individuals acquiring knowledgešformal and informalš[and] then applying that knowledge, often in anew organizational or institutional setting" (flamm, 1988, p. 3).30 the importance of educated and trained talent isreflected in the hpcci's basic research and human resources component, which produces the intellectual andhuman capital, the most general benefit and therefore perhaps the least easy to identify.impact on mission agenciesin addition to its broad national accomplishments, the hpcci's contributions to the federal mission agencies,the initial customers for highperformance computing and communications technology, must also be considered.based on its discussions with agency officials and its own insights into the fit between these technologies andagency activities, the committee believes that the existing and potential contribution of highperformancecomputing and communications technology to federal mission agencies does justify the investment. the policydecision to eliminate nuclear weapons testing has greatly increased the need for highperformance computersimulations at the departments of energy and defense, for example, and the need to control costs for defensemateriel makes simulation in the manufacture of defenserelated products an attractive prospect.31 thus much ofthe hpcci effort at the advanced research projects agency (arpa) relates to design and simulation. althoughdefensespecific applications are sometimes unique, applicationsrelated investment can have impacts beyondmeeting agency needs. as in the case of the gigabit testbeds, for example, such work can provide proofs ofconcept that encourage private investment by lowering risks.five gigabit testbed projects: collaboration and impactthe gigabit testbeds provide a case study of how to achieve progress through crosssectoral, developerusercollaboration to advance highperformance computing and communications technologies. since 1989, the testbedshave provided the means to test applications and thus extend the state of the art in gigabit networking to linkhighperformance computers to each other and to applications sites. the five original gigabit testbed projects,funded by nsf and arpa and administered by the corporation for national research initiatives, were started as a5year program.32 they received considerable support from the telecommunications industry, mainly in the formof donated transmission facilities. all three major longdistance carriers and all of the major localexchangecarriers participated. in addition, at least three similar independent testbeds were created through publicprivatepartnerships in the united states, and imitators sprang up in europe. the testbeds were intended to address twokey questions: (1) how does one build a gigabit network?, and (2) what is the utility of such a network?all five became operational by 1993, showing that gigabit networks could be built. they were largely but notcompletely successful in illuminating how best to build a gigabit network.33 the difficulties in many cases werenot with the networks but rather the computers connected by the networks, which could not handle the very highbandwidths. although no research on computerthe high performance computing and communications initiative36evolving the high performance computing and communications initiative to support the nation's information infrastructurecopyright national academy of sciences. all rights reserved.architecture was included in the testbed projects, the projects demonstrated the need for better computer systems,both hardware and software, to achieve better communicationsšthey demonstrated the intimate linkage betweencomputing and communications systems.as to the utility of the gigabit testbeds, opinions in the community differ sharply. demonstrations of thepotential utility of gigabit networks in some grand challenge applications were achieved, including global climatemodeling and radiation treatment planning. what is debated was whether the very high bandwidths actually addedmuch value to the applications. no largescale use of the testbeds for applications research was really possiblebecause of the rather experimental nature of the networking and limited reach of the networks. also, this workdemonstrated that there were essentially no computers that could take advantage of the highbandwidth gigabitlines because their internal buses were too slow. although the gigabit testbeds emphasized speed, discussionswithin the research community, industry, and the hpcci agencies have suggested that future testbeds shouldaddress architecture, scale, and heterogeneity of systems as well as communications speed.evolution of hpcci goals and objectivessince early 1994, the policy context for the hpcci has shifted at least twice, and the change in the congressheralded by the fall 1994 elections suggests the potential for further change.improving the information infrastructurethe first shift in policy affecting the hpcci reflected growing interest in the information infrastructure andthus in the universal, integrated, and automated application of computing and communications technologies tomeet a wide variety of user needs. the increasing linkage between the hpcci and information infrastructure canbe seen in the "blue books," the principal public documentation of the purpose, scope, participation, budget,achievements, and prospects of the hpcci.34 box a.2 in appendix a outlines the evolution of hpcci goals asarticulated in the blue books. box a.3 indicates the broadening of focus from science to other kinds ofapplications and drivers of highperformance computing and communications technologies. as discussed above,pl 102194 marked the first explicit congressional step toward greater attention to information infrastructure; theblue books track concurrent thinking of hpcci agency officials.the president's fy 1995 budget request bundled the hpcci together with other items, notably nonresearchprograms intended to broaden access to communications and information infrastructure, into the nationalinformation infrastructure (nii) initiative.35 the initiative built on the level of technology generally available inthe early 1990s, proofs of concept provided by the nren program, and industry trends, including growing use ofcomputerbased networking within and between a variety of organizations and the rise of a variety of networkbased businesses. once the policy focusin the government, the press, and most of the agenciesšcentered oninformation infrastructure, highperformance computing seemed to be greatly downplayed. in many 1993 and1994 government documents and administration speeches, the first "c" of hpcci effectively disappeared,notwithstanding the fact that achieving many of the goals for improving the information infrastructure woulddepend on rapid continuation of progress in highperformance computing. the nii, previously absent, wasfeatured in subtitles of the 1994 and 1995 blue books even though there is no formal, specific nii researchprogram.the formulation of the nii initiative raised questions about the nature and extent of political support for theoriginal hpcci objectives, and it may have led to expectations that were not embodied in the hpcci as originallyproposed. it perhaps inadvertently underscored thethe high performance computing and communications initiative37evolving the high performance computing and communications initiative to support the nation's information infrastructurecopyright national academy of sciences. all rights reserved.misperception that because the hpcci emphasizes the high end of the technology spectrum, it is less relevant oruseful than the nii initiative. cast in more populist terms, the nii initiative included a variety of efforts to explorebroadening access to increasingly sophisticated computing and communications services and attention toassociated practical concerns. these perceptionsšand misperceptionsšthreaten to slow the momentum of thehpcci at just the time when its potential to support improvement of the information infrastructure is mostneeded.the missing link appears to be the failure of some hpcci critics to appreciate the dynamism of computingand communications technology: almost by definition, relatively few really need and/or can afford leadingedgecomputing and communications. but as demonstrated in chapter 1, the rapid pace of technical developmentquickly brings these technologies into the mainstream, and they become accessible to a broad populace. attentionto performance is justified by the expectation for rapid transitions from leadingedge technologies to costeffective, ubiquitous technologiesas well as the kinds of applications expected to grow. for example, multimediacommunications will require highbandwidth, lowdelay delivery based on high peak network capacity and onprotocol support for negotiating and enforcing service guarantees. the internet and efforts associated with thedevelopment of digital libraries already illustrate the importance of highperformance computing andcommunications to a broad set of information infrastructure capabilities.greater attention to information infrastructure does not imply that performance should be abandoned. butrather than drive toward a narrow goal, such as a teraflop machine or gigabit network, per se, the goal should besystems that scale over several orders of magnitude. this goal should include not only processing rates andcommunication rates, but also storage capacity, data transfer rates, and retrieval times, as well as the problemsinherent in serving millions of users.figure 2.1 scale and speedimportant dimensions of the information technology "tent."one can view information technology as a tent: the height of the center pole corresponds to speed and thebreadth of the base corresponds to scale (figure 2.1). both speed and scale are important research issues. thehpcci's focus has been mainly, though not exclusively, on speed. we can move toward an enhanced nationalinformation infrastructure by adding more cloth to the tent so as to further emphasize scale without deemphasizingspeed, or by shifting the focus somewhat from height to breadth, from the research issues of speed to those ofscale. both changes are appropriate; both dimensions are important for the tent to work. additional opportunitiesand needs are also suggested by the tent metaphor, recognizing that there is more to advancing informationtechnology and the information infrastructure than speed and scale. other important goals include: reliability (will the tent stay up?);productivity (how long to move the tent to a new site?); (can the tent's shape be changed?); humancomputer interface (can people use the tent?); and search and retrieval (can people find what they want in the tent?).the high performance computing and communications initiative38evolving the high performance computing and communications initiative to support the nation's information infrastructurecopyright national academy of sciences. all rights reserved.advancing the information infrastructure presents many practical and some urgent needs, but it has been andcan continue to be a driver for the longterm research challenges addressed by the hpcci. highperformancecomputing and communications will help provide the technologies needed to provide flexible, highrate,affordable data communications. in office of management and budget guidance for developing fy 1996 r&dbudgets, the hpcci is acknowledged as "helping develop the technological foundation upon which the nii will bebuilt," as a prelude to the articulation of several priorities under the broad goal of "harnessing informationtechnology," one of six broad goals.36 see box 2.3 for an illustrative discussion of how telemedicine needs, forexample, can help to drive highperformance computing and communications technology development anddeployment, and how the hpcci can foster paradigm shifts in application domains.evolving research directions and relevance for the information infrastructurethe public debate over information infrastructure is at heart a debate over how to make computing andcommunications systems easier to use, more effective, and more productive. the challenge for research policy is totranslate usability needs into research topics and programs. the hpcci itself was built on the recognition that thefundamental challenge to greater acceptance and use of highperformance technologies is to make them moreusable. since the 1970s it has been recognized that more usable parallel processing machines imply thedevelopment of algorithms, programming support software, and native parallel applications, but the problempersists despite considerable progress. (see appendix a.) for information infrastructure in the fullest sensešreaching to ordinary citizensšthese efforts must be extended to address intuitive models of use and supportinguser interface technologies to enable a class of information appliances that will become a part of everyday life. theacceptance and popularity of mosaic demonstrate the importance of user models, human factors, and other areaswhere research is critically needed.more generally, intelligent information retrieval systems, systems for understanding speech and pictures, andsystems for enabling intelligent dialogues between people and computer systems are capabilities that will build onhpcci research and enhance the usefulness and level of use of information infrastructure. in addition, researchand development of core software technologies are needed to achieve progress in security, privacy, networkmeasurement and management, transaction processing, application integration, and other capabilities that may beless directly visible to individuals but that make computing and communications facilities more usable. forexample, hpcci and other computing and communications research can enhance capabilities for distributed,remote measurements of quantities that relate to the environment, traffic flows and management, or healthconditions. yet other research should build on the movement to digital transmission of more and moreinformation. as this list of possibilities suggests, information infrastructure is bigger than an initiative, althoughone or more initiatives, including the hpcci, can help to organize and accelerate progress in developing and usingit.complicating decision making regarding information infrastructure research is the recognition that anadvanced information infrastructure is not something that will spring fullgrown from any one development.rather, it is something that will grow from new capabilities in many different sectors of the economy and society.having to provide for migration, evolution, integration, and interoperability compounds the technical challenges.the high performance computing and communications initiative39evolving the high performance computing and communications initiative to support the nation's information infrastructurecopyright national academy of sciences. all rights reserved.box 2.3 telemedicine: an example of hpccienabled teleexpertisethe provision of expert professional services, such as medicine, law, and education, is a current consumer of hpcctechnologies as well as a driver of future developments. generically, this provision of services is often referred to as teleexpertise and can be thought of as the live, interactive provision of services and education between individuals who aregeographically separated but electronically connected. teleexpertise holds the promise of reducing costs and lesseninggeographical disparity in the availability of services. in particular, telemedicine will be an important part of the nationalchallenge in health care as evidenced by funding from the national institutes of health and other organizations for severalprojects, including a 3year contract to use advanced network technologies to deliver health services in west virginia.functionally, telemedicine supplies an audio, visual, and data link designed to maximize understanding betweenprovider and patient. in telemedicine, visual contact and scrutiny are particularly important to accurate communication:studies have suggested that body language and facial expression can convey up to 80 percent of meaning. clinically,although touch is currently denied, video zoom capability often augments visual examinations beyond what is the norm infacetoface services. in addition, various endoscopes and physiometers may be utilized across a network to furtherenhance a health care worker's observations.limited telemedicine field trials began in 1958 and expanded with federally funded research demonstrations between1971 and 1980. considerable research was done on reliability, diagnostic replicability, user satisfaction, and multiplespecialty services. currently, a few projects address teleexpertise more broadly by combining telemedicine and distancelearning, and trials are being conducted in montana and texas that encourage the integrated use of remote services inmedicine, industry, law, and education, the "miles" concept. more specifically, telemedicine has made some advances inthe years since the early trials: elaboration and extension of transmission media from early microwave and satellite channels to 384kbps service anddirect fiberoptic links; reduction of costs due to digital signal compression and decreased longdistance rates in constant dollars; and expansion of the number of pieces of medical equipment that may be connected to the remote terminal, chiefly a varietyof endoscopes and physiometers.nonetheless because of health care cost issues and large disparities in the medical services available in differentgeographical areas, telemedicine has great potential impact as a national challenge application for hpcci technologies.telemedicine urgently needs several hpccirelated technologies that can be deployed rapidly and inexpensively and thatscale well. among others, these include: rapid, highcapacity, multipoint switching. the telephone became increasingly useful as improved switching andnetworks enabled rapid expansion across the nation. so it is with interactive videoimproved switching and networkswill activate the distancespanning benefits of the interactive video market. rapid, highcapacity, multipoint switching. the telephone became increasingly useful as improved switching andnetworks enabled rapid expansion across the nation. so it is with interactive videoimproved switching and networkswill activate the distancespanning benefits of the interactive video market. translators to interconnect divergent computing and communications technologies. new technologies are beingdeveloped and deployed so rapidly and in so many different places on the globe that it may be more feasible to developfacile, highperformance translators than to struggle for standards. compact video storage and good retrieval techniques. transparent technologies must be developed to enable aphysician to efficiently store and easily retrieve salient clinical moments without distracting from the clinical challengeat hand.source: committee on information and communication (1994), p. 28.the high performance computing and communications initiative40evolving the high performance computing and communications initiative to support the nation's information infrastructurecopyright national academy of sciences. all rights reserved.although the u.s. telecommunications industry is a world leader in developing and deploying networks on alargescale, the concepts inherent in an advanced national information infrastructure go beyond connecting a largenumber of relatively homogeneous end systems supporting a relatively small number of applications, such astoday's telephones or televisions. learning how to build largescale systems, like learning how to build highperformance ones, requires research; it is not simply a matter of deploying lots of boxes and wires. envisioned foran advanced national information infrastructure is the interconnection of a much larger number and variety ofnetworks than are interconnected today, with more nodes and switches per network and new mixes of wireline andwireless transmission. the end systems of such networks will run a much wider set of applications and call for abroader set of softwarebased support capabilities often referred to as "middleware." there will be greatcomplexity, increasing the emphasis on scale and architecture and on areas such as accommodating heterogeneityof systems, decentralization of control and management, routing, security, information navigation and filtering,and so on, all of which will depend on software.the evolutionary nature of information infrastructure also underscores the importance of engaging industry inthe planning, support, and conduct of research. advisory committees and collaborative projects are but twoexamples of how this engagement can be achieved. see appendix b, box b.1, for a discussion of the developmentof asynchronous transfer mode as an illustration of fruitful industryuniversitygovernment interaction.there have been many government, academic, and industry efforts, some still under way, to identify andclarify research issues associated with improving information infrastructure. the recent cstb report realizing theinformation future (1994d) provides a unifying conceptual framework from which it derives strategic emphasesfor research; a multiparty effort generated several lists of research ideas (vernon et al., 1994); a more focusedworkshop generated ideas for funding under the nsf networking and communications research program (nsf,1994); and arpa's nets program and other programs have continued to develop and enrich a technology basefor bitways and midlevel services to support defenserelevant applications.37common to these various efforts is the need for research to enhance such critical information infrastructuremiddleware capabilities as security and reliability; the basic research underlying many of these concepts had beendone by highperformance computing and communications researchers funded mainly by arpa. in addition, it isimportant to advance true communications research, including such fundamental areas as transmission, switching,coding and channel access protocols realized in electronic, optical, and wireless technologies, as well as basiccomputer networking research in such areas as internetworking protocols, transport protocols, flow and congestioncontrol, and so on. these complement and enable efforts relating to distributed computing, which tends to beconcerned with the upper or applications level of a total system. see box 2.4 in the section "coordination versusmanagement" below and appendix b for an examination of hpcci communications research efforts andappendix c for the larger budget allocation picture. now is the time to explore a wide variety of technicalproblems, enlisting as many approaches and perspectives as possible.overall computing and communications r&d planningthe second major influence on the policy context for hpcci is a broad rethinking of computing andcommunications r&d, building on the reorganization of the federal coordinating structures for r&d and factoringin a broad range of technology and policy objectives. the broadest coordination of computing andcommunications research and development activities across federal agencies is the responsibility of the committeeon information and communications (cic) under the national science and technology council. the cic wasformed in 1994 and is led bythe high performance computing and communications initiative41evolving the high performance computing and communications initiative to support the nation's information infrastructurecopyright national academy of sciences. all rights reserved.the director of defense research and engineering, the associate director for technology of ostp, and the assistantdirector of computer and information science and engineering, nsf. in late1994, the cic launched a strategicplanning activity to provide input into the fy 1996 budgetsetting process, expected to conclude in early 1995, andinform efforts for the next 5 years. indications from briefings based on preliminary versions of that strategic planshow a broader and richer set of concerns than previously evident.strategic focus areas identified in preliminary materials include globalscale information infrastructuretechnologies, scalable systems technologies, highconfidence systems, virtual environments, usercenteredinterfaces and tools, and human resources and education. the hpcci relates at least somewhat to all of thesetopics, and the planning process is examining where other, missionrelated agency activities can build on hpccias well as other activities. key research activities are classified as components, communications, computingsystems, software toolkits, intelligent systems, information management, and applications.38 software andusability are crosscutting themes.toward a better balancethere is a natural evolution of the hpcci, many aspects of which are associated with improvement of theinformation infrastructure. the newest component of the hpcci, the information infrastructure technology andapplications (iita) program, is one of the most visible signs of this evolution, but also important are the trendswithin the programs at both arpa and nsf, which show increasing emphasis on software solutions and tools.arpa, for example, is devoting attention to software and tools to support design and simulation for developmentof defense systems; its emphases on security and scalable systems both involve substantial effort relating tosoftware.39 this evolution should continue and indeed accelerate.practical experience with the hpcci and the volatile policy context both suggest that the ideal researchagenda for highperformance computing and communications should be driven by strategic priorities, but focusedmore broadly than on just those priorities. a stable yet flexible approach would combine substantial focus on goalsof current national importance, including directly targeted research, with a flexible program that sustains a healthybase of computing, computation, and communications science and technology. the comprehensiveness of theemerging cic strategic plan appears to provide a broader platform than previously available for supporting thenation's public computing and communications r&d, including that relating to highperformance technology.also, the commendable inclusion of a technology "maturity model" in cic's preliminary strategic planningmaterial illustrates recognition of the technology "trickledown" phenomenon.moving forwardbasic issuesbalance of private and public investmentthe possibility of reduction or even premature termination of the hpcci, suggested by congressionalrequests for inquiries by the general accounting office (gao) and the congressional budget office (cbo) andfor this committee's report, is troubling. (see appendix a for a brief discussion of issues raised by gao andcbo.)40 some hpcci critics expect industry to pick up the task. they seem to assume possible a larger programof basic research from industry than isthe high performance computing and communications initiative42evolving the high performance computing and communications initiative to support the nation's information infrastructurecopyright national academy of sciences. all rights reserved.reasonable based on history, the growth in competition, which reduces the profit margins needed to sustain r&d,and the economics of innovation generally.leadingedge highperformance computing and communications technology is aimed at the most demandingcustomers, a niche or subset on the order of 10 percent of the larger computing and communications market. trulyhighend systems tend to be nonstandard and to require considerable customer support, for example, which limitstheir market potential. it may be more appropriate, therefore, to assume that truly highend systems are aimed atparticular classes of problem for which the systems and associated software have particular value, rather than toassume that these systems will become universal. for example, better weather prediction would save an enormousamount of money and should be carried out on highperformance computers even if millions of people do not havethem. the lower end of the market will grow as parallel processing vendors reposition their products, addressingbroader industrial and commercial needs for information storage and analysis, multimedia servers, data mining,and intelligent transactions systems.41observers within the computing and communications research communities, including members of thiscommittee, are concerned about the impact of computer and communications industrial restructuring. changes inthe organization of these industries, plus the inherent difficulties incumbent companies face in using researchresults, prevent companies from undertaking the kind of largescale, longrange research needed to tackle thechallenges inherent in advancing the hpcci objectives or the broader objectives associated with informationinfrastructure. this concern is almost impossible to substantiate, because it is inherently intuitive, albeit shaped byexpert judgment and the experience of committee members working in or with a variety of computing andcommunications companies, and because the results of current trends will not be evident for several years.42coordination versus managementthe hpcci became an integrated, multiagency, crosscutting initiative because agency and congressionalofficials recognized that there would be economies of scale and scope from connecting complementary effortsacross research disciplines and funding agencies.43 by cooperating, agency officials have successfully leveragedthe dollars available in the initiative budget, facilitating collaborative efforts with industry. the nreninfrastructure investments, including the nsfnet backbone and gigabit testbeds, provide examples. networkconnections, research tools, and delivery of educational products appear to motivate the broadest interagencyactivity within the hpcci context, helping to extend collaborations beyond the conduct of research per se and into awider circle of agencies.through its accomplishments and esprit de corps, the hpcci has become a model for multiagencycollaboration.44 each agency retains responsibility for its own part of the program, focusing its participation tomeet agency needs and resources. the voluntary compliance of hpcci agencies with the spirit of pl 102194reflects the special cooperation that has characterized the hpcci. these conditions have enabled the initiative togrow and adapt relatively quickly to changing national needs, technology prospects, and the fit between the two.perhaps because they see themselves as principal architects of the program, officials from the four initial hpcciagencies (department of defense (dod), department of energy (doe), national aeronautics and spaceadministration (nasa), and national science foundation), in particular, have carried high levels of enthusiasm,dedication, inventiveness, and energy into undertaking the hpcci. these intangible qualities are widelyrecognized within the computing and communications research community.the level of interagency coordination observed today took time to grow. as one might expect whenorganizations with different missions, budgets, and cultures are faced with a joint task, the hpcci agencies havedisagreed on issues of emphasis and approach over the years. forthe high performance computing and communications initiative43evolving the high performance computing and communications initiative to support the nation's information infrastructurecopyright national academy of sciences. all rights reserved.example, doe and nsf have had different views on evolving the nren program with respect to scope andspeed. what is important for the future of the hpcci, however, is not that differences have arisen but rather thatlegitimate differences owing to varying missions have been respected, and cooperation and coordination haveimproved over time. for example, nsf and arpašwhich respectively emphasize small science and largerprojectsšhave worked well in the management of their joint network and computing research activities, asdescribed in box 2.4.box 2.4 coordination in practice: the case of communications r&dthe hpcci currently includes a relatively modest but vigorous communications research program. three largeprograms account for $77 million of the hpcci communications research budget. research is concentrated mainly in fourareas (see appendix b for more details and context):1. optical networks (the longestterm research),2. gigabit networking (mediumterm research),3. multimedia communications (fairly nearterm research), and4. internetwork scaling (near and mediumterm applied research).the arpa networking program, at $43.1 million, is the largest communications research program activity. themilestones include: demonstration of diverse internet capabilities such as cable and wireless bitways, demonstration of rateadaptive quality of service negotiation in asymmetric networks, demonstration of bandwidth and service reservation guarantees for networks in support of realtime services, demonstration of secure routing systems, and interconnection of gigabit testbeds.the arpa global grid program, at $23 million, intends to accomplish (in 1995): demonstration of multiwavelength reconfigurable optical network architecture, and demonstration of integrated dod and commercial networks in support of crisis management applications.nsf's very high speed networks and optical systems program, at $11 million, supports research in a wide variety ofhighperformance networking technologies, including: gigabit testbed research (switching, protocols, and management); resource discovery; information theory; network security; modulation, detection, and coding for information storage; and optical networking.arpa and nsf have coordinated well to avoid duplication of efforts. arpa funds most of the research oninternetworking, and nsf concentrates on the deployment of internetworking infrastructure via its nsfnet activities. nsfand arpa have jointly funded the gigabit testbed research program, which involves demonstration of crosscountrygigabit networking technologies.with the reinforcement provided by pl 102194, the set of agencies involved in the hpcci grew. thisbroader participation better positions the hpcci to support development and application of computing andcommunications technologies essential to improving the nation's information infrastructure. however, as reflectedin both executive and congressional efforts to promote suchthe high performance computing and communications initiative44evolving the high performance computing and communications initiative to support the nation's information infrastructurecopyright national academy of sciences. all rights reserved.improvements, the information infrastructure raises issues such as deployment, regulation, and other practicalaspects that require engaging a broader and somewhat different set of agencies, such as the federalcommunications commission, the national telecommunications and information administration, and so on, toaddress a wider range of issues than those relating to r&d.the diversity of the hpcci approach allows many views to compete, first for funding, later in the evolutionof thinking among researchers, and finally in the marketplace. it also fosters pursuit of the intellectual questionsposed by the hpcci via a range of complementary modes including classical single principalinvestigator (pi)research, multiplepi experimental research, multiplepi/multiplefield collaborations, intramural research ininstitutes and national laboratories, and joint industrygovernmentacademia experiments or proofs of concept.a variety of mechanisms are used to foster interagency cooperation and coordination: joint funding of projects, from relatively specific or narrow activities to the federal portions of theinternet; consortia, such as the consortium on advanced modeling of regional air quality involving six federalagencies plus several state and local governments; the metacenter concept pursued by nsf supercomputer centers (see appendix e) and extending to otherentities and users via the metacenter regional affiliates program; and crossagency reviews of grants and contracts, such as the nsf, arpa, and nasa digital libraryinitiative, and joint testbeds.the diversity in approach and tactics makes it less likely that the nation will miss some important approach.it also facilitates participation by a variety of agencies, which tend to have different styles as well as emphases forsupporting research or procuring technology, consistent with their different missions, histories, and cultures.as to diversity of mechanisms, the multiplepi/multiple field category is epitomized by the grand challengeteams, which involve multiple institutions attacking frontier research problems with multipleyear horizons, oftendrawing on access to the leadingedge machines in the nsf supercomputer centers and benefiting frominteractions between computer scientists and computational scientists.45 the joint industrygovernmentacademiaexperiment category is currently epitomized by the gigabit network testbeds. more specifically, nasa's fy 1995hpcci effort includes integrated multidisciplinary computational aerospace vehicle design and multidisciplinarymodeling and analysis of earth and space science phenomena (holcomb, 1994).coordinating structurethe coordinating structure of the hpcci has evolved steadily, largely in response to external pressures forimproved visibility of decision making, requirements for accountability for expenditures, and the flow ofinformation into and out of the initiative. some hpcci observers have continued to argue for a more uniformapproach to related activities with thorough planning, precise milestones, and presumably no wasted effort, in amore centralized program. this is the essence of early criticism lodged by the computer systems policy project(1991 and 1993).drawbacks of centralizationthe central question about coordination is whether the special vitality of hpcci would survive and whethercentralized control would convey sufficient benefits, or merely disrupt current arrangements. a more centralizedapproach would have several drawbacks that could vitiate thethe high performance computing and communications initiative45evolving the high performance computing and communications initiative to support the nation's information infrastructurecopyright national academy of sciences. all rights reserved.hpcci: potential loss of variety in perspectives on applications now arising from agencies with differentmissions; greater risk of concentrating on the wrong thing; and increased bureaucratic overhead and costsassociated with efforts to overlay separate programs. moreover, because much of the existing effort involvespreviously existing programs, there is a risk that agencies would not participate in a program that involved a lossof their control to a more centralized authority. this concern, probably paramount to the agencies, arises from therecognition that much of the funding associated with the hpcci is not new, just classified as relevant to theinitiative. a virtue of the current arrangement is that the central coordination is provided by a relatively smallentity that lacks the resources for micromanagement. that approach maximizes the benefits provided by a diversegroup of agencies.national coordination officebox 2.5 national coordination office: staffing and structure through 1994chaired by the director of the national coordination office (nco), the highperformance computing,communications, and information technology (hpccit) subcommittee and its executive committee coordinate planning,budgeting, implementation, and program review for the overall initiative. the hpccit subcommittee has also been themajor vehicle for communication with other federal agencies, the u.s. congress, and numerous representatives from theprivate sector.the director of the nco reports to the director of the office of science and technology policy (ostp). the director ofostp has specified that overall budget oversight for the hpcci be provided by the national science and technologycouncil through the committee on information and communication (cic). actual appropriations for the initiative are madein the regular appropriation bills for each participating agency. the hpccit coordinates program and budget planning forthe initiative and reports to the cic.under the umbrella of the hpccit, several working groups have been formed to help guide and coordinate activitieswithin the five components of the initiative. for example, the information infrastructure technology and applications taskgroup, established in 1993, has encouraged and coordinated participating agencies' plans for research and developmentaimed at providing needed technologies and capabilities for an enhanced nationwide information infrastructure and thenational challenge applications. the science and engineering computing working group coordinates activities andsoftware development relating to grand challenge applications.the direct operating expenses of the nco are jointly borne by the participating agencies in proportion to their hpccibudgets, and further support is provided by the interagency detailing of staff to the nco for varying periods of time.currently the nco has eight permanent staff and two staff ''on loan" from the department of energy. in addition toproviding general administrative functions such as payroll and personnel administration, the national library of medicinealso contributes specialized assistance such as public information functions, budget preparation, legislative analysis andtracking, graphic arts services, procurement, and computing and communications support.sources: see lindberg (1994); nco (1994); and cic (1 994). additional information from letter dated august 8,1994, to marjory blumenthal (cstb) from donald a.b. lindberg (nco/nlm) in response to committee's interim report(cstb, 1994c).the hpcci coordination focus lies in the national coordination office for highperformance computingand communications, which was established in september 1992. it operates under the aegis of the office ofscience and technology policy and the national science and technology council (nstc; see figure 2.2).box 2.5 provides information on the structure and staffing of the nco.the high performance computing and communications initiative46evolving the high performance computing and communications initiative to support the nation's information infrastructurecopyright national academy of sciences. all rights reserved.figure 2.2 organizational context for hpcci coordination.the nco was established to aid interagency cooperation and to serve as liaison for the initiative to thecongress, other levels of government, universities, industry, and the public. it assists the mission agencies incoordinating their separate programs, offering a forum through which the separate agencies can learn of eachother's needs, plans, and actions. as part of its coordinating function, the nco gathers information about thehpcci activities of different agencies and helps to make this information available to congress, industry, and thepublic. since its formation, the nco has produced the impressive fy 1994 and fy 1995 blue books as visiblemanifestations of its coordination efforts. the fy 1995 blue book is the best documentation available of hpcciactivities and where the money is invested.strengthening the nco. the public debate over the hpcci attests to the need for improved communicationregarding the initiative's purpose and accomplishments. because lack of external understanding is damagingšnotleast because it leads to criticisms and investigations that divert energy and resources from pursuing hpcci goalsšthe committee believes that the hpccithe high performance computing and communications initiative47evolving the high performance computing and communications initiative to support the nation's information infrastructurecopyright national academy of sciences. all rights reserved.could benefit from a stronger nco that can do a better job of telling the program's many constituencies about itsgoals and successes; see the committee's interim report (cstb, 1994c) and chapter 3.as authorized in the 1991 highperformance computing act, the nco was to have been assisted by anadvisory committee that could provide regular infusions of ideas from industry and academia. to date, the hpccihas been led mostly by computing visionaries and by people active in science and science applications. that is theright kind of leadership to drive the creation of enabling technology and to create computer architectures that areappropriate for the pursuit of science objectives. but the initiative now also needs the perspective on applicationsand on making computing and communications technologies more usable that would be provided by an advisorycommittee of recognized experts with membership balanced between academia and industry, and balanced withrespect to application areas and the core technologies underlying the hpcci.46 the growing dependence of moreand more people on infrastructure, the rise in potential liabilities of varying kinds, and growth in competitivechallenges from abroad increase industry's stakes in the quality of information technology available. industryinput into such issues as standards, security, reliability, and accounting, for example, becomes more important asadvancing the information infrastructure and "highconfidence systems" come to drive more of the researchagenda.in lieu of having an advisory committee, the nco has taken the initiative to convene some industry and othergroups to obtain focused input on hpccirelated issues and directions. in conjunction with its regular meetingswith federal hpcci agency representatives, the nco has engaged in dialogues with representatives of thecomputer systems, software, and telecommunications industries; managers of academic computing centers; andothers; and it has held a similar discussion with representatives of the mass information storage industry. it hasalso participated in workshops, conferences, and public meetings sponsored by participating agencies and thesubcommittee on highperformance computing, communications, and information technology (hpccit).however, nco leadership notes that given the restrictions on external interactions imposed by the federaladvisory committee act, the absence of an official advisory committee prevents it from obtaining needed input onan ongoing basis, limiting it instead to onetime interactions and thus foregoing the insights that can arise whereparties benefit from repeated interactions.the committee echoed the nco's concern by recommending in its interim report that the longawaitedhpcci advisory committee be established immediately. in view of the delays and difficulties in establishing anhpcci advisory committee and the apparent tendency of federal science policy leaders to enfold hpcci in alarger nii initiative, there is some expectation that one advisory committee may be empaneled to provide inputinto the broader cic agenda, and to the hpcci. this national research council committee thinks that solutionmight work, but it urges some action now.understanding the changing management context. actions taken to reinforce the nco must account forthe larger, evolving administrative and management context in which the nco fits. a key component of thatcontext is the cic. it receives limited staff support from ostp and, presumably, from agencybased staffmembers. its members include directors of computer and communicationsrelated research units from across thefederal government. their participation in the cic provides information exchange and coordination, but the cic isnot an implementation entity. the nco director participates in both cic and hppcit.planning, coordination, and management for the hpcci have been further confounded by the rise ofadditional bodies to address technology policy and other policy relating to the nii initiative. the informationinfrastructure task force (iitf), formed in 1993, has a technology policy working group with overlappingrepresentation with the hpccit. its focus is supposed tothe high performance computing and communications initiative48evolving the high performance computing and communications initiative to support the nation's information infrastructurecopyright national academy of sciences. all rights reserved.be technology policy, versus the hpccit's focus on research and development. the iitf receives input from thenii advisory committee, also formed in 1993. and on networking issues there are yet more special coordinationand advisory entities, such as the federal networking council (fnc) that has associated with it the fnc advisorycommittee. this proliferation of crossagency entities itself presents many possibilities for confusion. moreover,by all accountsfrom virtually every hpcci official the committee has heard from and from several privatesectorpartiesthe processes of communication and decision making have been slowed by a calendarfilling profusion ofmeetings. this situation raises basic practical questions of what work can get done, when, how, and by whom,when committee meetings appear to be the order of the day.budgeta detailed overview of the hpcci budget is presented in appendix c. the overall level has been subject tomisunderstanding.according to the blue books, the hpcci budget has grown from $489.4 million in fy 1992 to the $1.1billion requested for fy 1995. when the hpcci was proposed in the executive budget for fy 1992, the agenciesinvolved identified from their existing fy 1991 activities a base that contributed to the goals of the program. thehpcci's multiagency budget is more complex than it would be had the program been started "from scratch" within asingle agency. although complexity is inherent in multiagency programs and budgets, it has added to theconfusion about spending priorities and accomplishments for the initiative.the agencies that had activities included in the fy 1992 base were the (defense) arpa, doe, nasa, nsf,national institute of standards and technology, national oceanic and atmospheric administration (noaa),environmental protection agency, and national institutes of health/national library of medicine. in eachsubsequent year, agencies have added to this base in two ways: (1) by identifying additional existing programsthat contribute to hpcci goals and (2) by reprogramming and relabeling agency funds to support relevant aspectsof the hpcci. to this base of "identified" activities, congress has added some funding each year for newactivities or the expansion of existing efforts.the result is that the $1.1 billion requested for fy 1995 is composed of three elements: (1) funds for thecontinuation of agency activities that were in existence when the hpcci started and were designated in the fy1992 base budget, (2) funds for existing or redirected programs that have since been designated as being a part ofthe hpcci, and (3) additional funds for new activities or expansion of existing efforts. it is difficult to determineexactly how large each element is and to make interagency comparisons, because each agency has used slightlydifferent approaches for identifying existing efforts and somewhat different formats for supporting program andbudgetary detail. also, this situation has allowed some agencies (e.g., noaa) to be considered participants in theinitiative without receiving any new money.47notes1. the substance of these components is outlined in the hpcci's annual blue books; see fccset (1991, 1993, and 1994).2. note that many of the concerns raised here were expressed or articulated in a cstb report released at the dawn of the hpcci, thenational challenge in computer science and technology (cstb, 1988).3. a teraflop refers to 1012 (or 1 trillion) floating point operations per second (flops), 1,000 times the performance of the best machinesavailable when the hpcci began.4. see flamm (1988); this book discusses the major computer development projects of the 1940s, 1950s, and 1960s and their dependenceon government stimulus and combined government, university, and industry development of technology.the high performance computing and communications initiative 49riginal paper book, not from the original typesetting files. page breaks are true tothe original; line lengths, word breaks, heading styles, and other typesettingspecific formatting, however, cannot be retained, and some typographic errors may have been accidentally inserted. please use theprint version of this publication as the authoritative version for attribution.evolving the high performance computing and communications initiative to support the nation's information infrastructurecopyright national academy of sciences. all rights reserved.5. in turn, the use of multiple microprocessors in largescale parallel machines also exposed problems that would have to be resolved formicroprocessors as the dominant computing element.6. see zatner (1994), pp. 2125, for a discussion of events leading to chapter 11 status for tmc. ksr suffered from an accountingscandal, then had to contend with 12 classaction shareholder lawsuits (snell, 1994). the impact of the lack of software has also beenimplicated as an indicator of management ineffectiveness in the fates of tmc and ksr (lewis, 1994).7. indeed, talk of the next hurdle, the petaflop system, has already begun. nsf, nasa, doe, and dod hosted a 1994 workshop onenabling technologies for petaflop computing. the report is said to argue that that goal can be met at reasonable cost in 20 years usingtoday's paradigms. see anthes (1994), p. 121.8. "recommendation a2: at the apex of the hpc pyramid is a need for a national capability at the highest level of computing power theindustry can support with both efficient software and hardware."a reasonable goal for the next 23 years would be the design, development, and realization of a national teraflopclass capability, subjectto the effective implementation of recommendation b1 and the development of effective software and computational tools for such alarge machine. such a capability would provide a significant stimulus to commercial development of a prototype highend commercialhpc system of the future." (nsf. 1993, p. 11)9. the fundamental computer unit is the microprocessor, which today has a peak speed of around 300 megaflops. it seems premature tobuild a 3,000 processor teraflop machine in 1995, but as the microprocessors increase in speed to i to 2 gigaflops by the late 1990s, itseems reasonable that 512 to 1024processor teraflop machines may be built if the economics of users and their applications require it.for example, kenneth kliewer, director of the center for computational sciences at oak ridge national laboratory, was quoted indecember 1994 as saying: "the scale here is clearly a function of time, but we could have nearly a teraflop computer today by couplingthe oak ridge and los alamos computers with the ones from cornell and maui" (rowell, 1994). in november 1994, a new productannouncement by japan's nec indicated that the maximum configuration, with a total of 512 processors, could be rated at a theoreticalpeak of 1 teraflop (parkersmith, 1994b).the committee notes that if trends at the nsf supercomputer centers continue, the metacenter (which pools some of the centers'resources) could achieve an aggregate teraflop in midfy 1998 and each center would reach a peak teraflop machine by the end of fy1999. by contrast, even the aggregate performance would not reach a teraflop until after the year 2000 if acquisition of higherperformance architectures were to revert to prehpcci levels.10. the hpcci also triggered considerable debate about what broad availability meanswhat capabilities, in what locations, accessible bywhom and at what costanticipating the more recent debates about how universal service in telecommunications should evolve.11. the gigabit goal, as defined in the nsfarpacnri testbeds, was to achieve an endtoend speed of at least i gbps between twocomputers on a network. the telephone achievement was to multiplex about 25,000 64kbps voice conversations onto a transmission lineoperating at 1.7 gbps (late 1980s technology, now more than doubled). the gigabit testbeds have demonstrated endtoend speeds betweentwo computers of about 500 mbps, limited by the internal bus speeds of the computers, not the network.12. comments by sandy macdonald, noaa, at "environmental simulation: the teraflop and information superhighway imperativeworkshop," august 1820, 1994, monterey, calif. he noted an increase from 3,200 numerical observations per day for kansas in 1985 to86,000 daily observations, many from automated instruments.13. comments by steve hammond, national center for atmospheric research, at "environmental simulation: the teraflop andinformation superhighway imperative workshop," august 1820, 1994, monterey, calif. he observed that teraflop computing helpedreduce processing times to 90 seconds per modeled month, yielding 1,000 modeled months in 30 hours of processing time.14. briefings to the committee. legislative codification and appropriation for a broader vision for the hpcci have been attempted buthave been unsuccessful, most recently in connection with s4/h1757, in 1994.15. for example, in addressing networking, pl 102194 also anticipated many of the practical concerns associated with enhancements andexpansion of the nation's information infrastructure, such as user charging and protection of intellectual property rights.16. policy documents emerging from the administration in mid1994 and congressional actions in 19931994 emphasize a commitment tolinking r&d spending to strategic, national concerns (panetta, 1994; nstc, 1994b).17. of course, there will also be people using the technology who are not in close contact with developers and vice versašhence the valueof a solid base of funding for both computing and communications research and for the sciences that increasingly depend on computation.18. see cstb (1988) for a discussion of national challenges within computing. see also cstb (1992) for a discussion of methods ofcombining intrinsic problems with problems inspired by needs in other areas.19. the lengthy time scales associated with developing complex computerbased systems are outlined in cstb (1994a).20. rigorous costaccounting and auditing can be elaborate, costly, and inflexible: "as a result, r&d done under federal contract isinherently more expensive and less effective than r&d done by an organization using its own funds" (cohen and noll, 1994, p. 74).the high performance computing and communications initiative  50evolving the high performance computing and communications initiative to support the nation's information infrastructurecopyright national academy of sciences. all rights reserved.21. the national coordination office (nco) has taken a first step in the evaluation area through its development of the hpcciimplementation plan. the format of the project and program records in that volume provides a basis for subsequent efforts to assessprogress. the nco management has expressed some interest in tracking progress relative to plan elements.22. the nsf infrastructure program was motivated by the recognition in the late 1970s of major deficiencies in the academic environmentfor experimental computer science and engineering. the initial stimulus was the report by feldman and sutherland (1979). when thatreport was written, the discipline of computer science and engineering was perceived to be in crisis: faculty members were underpaidrelative to research positions in industry and were leaving universities at an increasing rate; the number of new ph.d.s fell far short ofmeeting the national demand; most departments lacked experimental computing research facilities; and there was a significant gap inresearch capability between the top three or four departments, which had benefited from a decade of arpa investment, and the rest.23. the feldman and sutherland (1979) report resulted in the establishment in 1981 of the coordinated experimental research (cer)program at nsf. the cer program made awards of approximately $1 million per year (including an institutional match of typically 25percent) for durations of 5 years to support significant experimental research efforts by building the necessary infrastructure. there havebeen very substantial increases in the number of departments producing strong experimental research, the number of departmentsproducing strong students in experimental areas, the number of departments conducting leadingedge research in a significant number ofareas, the overall rate of ph.d. production in the field. and other similar measures.the success of the cer program was important in shaping several subsequent nsf programs that also contributed to the infrastructure ofthe field, such as the engineering research centers (ercs) program. a number of ercs are in computingrelated areas, which in turninfluenced the science and technology centers (stcs) program; three stcs are in computingrelated areas. the cer program itselfultimately became the research infrastructure program and was complemented by an educational infrastructure program. a number ofother agencies instituted research and/or educational infrastructure programs.24. the nsfnet backbone has involved nsf spending authorized on the order of $30 million but complemented by inkind and otherinvestments by ibm and mci through advanced networks and services, which has deployed and operated nsfnet under a cooperativeagreement with nsf. the internet overall has been growing through proliferation of a variety of commercial internet access providers. seecstb (1994d).25. "although the infrastructure, including networking, software applications and tools, visualization capabilities, etc., is still not strongenough, raw computing power is becoming comparable, and in some cases greater than what is available at nsf centers in the u.s. thisincrease in resources comes at a time when the japanese government is also increasing its emphasis on basic research for its own needsand to insure that japan is viewed as a [sic] equitable contributor to the global science community. readers might want to reflect on theimpact the nsf centers have had on u.s. science output and the potential for this to occur in japan." (kahaner, 1994b)26. for example, as a result of detailed interactions between a highperformance computing and communications vendor and a staffmember of an nsf supercomputer center, a grand challenge computer code uncovered previously undiscovered hardware bugs in newlyreleased microprocessors installed in a scalable supercomputer at the center. this led to the vendor using a version of the grand challengecode inside the company as a standard test to uncover both hardware and compiler bugs.27. jeremiah p. ostriker, princeton university observatory, personal communication, december 23, 1994.28. jeremiah p. ostriker, princeton university observatory, personal communication, december 23, 1994.29. examples include the silicon graphics everest/challenge systems (some 3500 challenge, power challenge, onyx, and power onyxsystems were sold in the 15 months following their september 1993 introduction) and the ibm sp2 and power parallel systems; seeparkersmith, 1994a. see also appendix a.30. even the rise and fall of individual ventures shows this generally positive pattern: tmc was launched in part by the expertise of dannyhillis, previously at mit, and his associates. with tmc's contraction in 1994, hillis' team of over 20 engineers from tmc's futuresystems group went to sun microsystems, where they are working on a scalable massively parallel processing system, while other tmctalent continued with a tmc parallel software descendent (riggs, 1994).31. briefings to committee by victor reis (department of defense) and howard frank (advanced research projects agency).32. no followon program to the gigabit testbed projects has yet been announced. in july 1994, an nsf and arpa workshop proposed aresearch agenda for gigabit networking and called for an experimental gigabit research network facility. nsf and arpa are extending theexisting program by a few months, into early 1995.33. note that there is continuing popular confusion over the term "gigabit networks" and the fact that the speed most often quoted for themis 640 megabits per second. each gigabit connection consists of two oneway circuits, each operating at 640 mbps. thus the overall speedof the twoway connection is 1.28 gigabits per second when properly compared to the quoted twoway capacity of application networks.also, the 640mbps circuits in at least one case (aurora) were derived by splitting 2.4gbps trunk circuits.the high performance computing and communications initiative  51evolving the high performance computing and communications initiative to support the nation's information infrastructurecopyright national academy of sciences. all rights reserved.34. each year beginning in 1991 the director of the office of science and technology policy submits a report on the hpcci to accompanythe president's budget. the fy 1992, fy 1993, and fy 1994 books were produced by the nowdefunct fccset: the fy 1995 report wasproduced by the nco (acting for the cic). the report describes prior accomplishments and the future funding and activities for thecoming fiscal year. these reports have collectively become known as "blue books" after the color of their cover.35. the nii initiative was framed in 1993 and included in the fy 1995 budget request.36. other goals, such as "a healthy, educated citizenry," also include applications of computing and communications among theirpriorities.37. cstb (1994b); vernon et al. (1994); and nsf (1994). the arpa nets program is covered by the blue books for fy 1994 and fy1995.38. briefing to committee by edward lazowska, based on a computing research association briefing by john toole, and augmented bybriefings by anita k. jones and howard frank, december 20, 1994.39. briefing to committee by howard frank, december 20, 1994.40. gao (1993); cbo (1993).41. committee briefings by forest baskett, silicon graphics inc., april 13, 1994; justin ratner, intel supercomputer systems division,june 27, 1994; steve nelson, cray research, inc., june 28, 1994; and steven wallach, convex computer corporation, june 28, 1994.also. see lewis (1994) re "gigaflops on a budget." see also furht (1994) for a description of how encore, hewlettpackard. ibm,pyramid, tandem, stratus, and at&t have changed their focus to transaction processing and faulttolerant computing.42. for example, in fy 1994, the nsf centers had an income derived by recovering cycle costs from noncomputer industrial partners ofaround $1 million to $2 million. in comparison to their nsf cooperative agreement level of $16 million per year, this has a small impact.indeed, the situation is even worse, since a typical nsf supercomputer center receives only half its annual budget from the nsfcooperation agreement, the other half coming from state and university matching funds, other grants, and equipment donations bycomputer vendors.the center experience also shows that over the last few years, industry spending to attain center knowhowštraining, softwaredevelopment, information infrastructure application development, virtual reality and visualization projects, and so onšand to use centersas vehicles for collaborative research has increased and exceeds spending on computer processing cycles at some centers. because thecenters have an existing staff for these projects, the industrial income generally covers only the marginal cost of providing that service andtherefore does not increase net "new dollars."43. other crosscutting initiatives contemporaneous with hpcci include advanced manufacturing technology; global change; advancedmaterials and processing; biotechnology; and science, mathematics, engineering, and technology education (fccset, 1993).44. the hpcci is understood by a variety of federal officials to have been a model for the "virtual agency" concept advanced through thenational performance review efforts to improve the organization and effectiveness of the federal government (gore, 1993).45. there are 16 grants, 7 awarded in fy 1992 and 9 in fy 1993. their source of funds can be broken into three parts (nsf/cise, nsf/noncise, and arpa). the fy 1994 and fy 1995 numbers are shown below. as the chart indicates, cise's percentage is less than onethird of the funding. this shows great leverage, even greater than that of the centers, roughly onehalf of whose budget comes cise.nsf/cisensf/noncisearpatotalfy 1994 $m2.775.001.919.68fy 1994 %295220100fy 1995 $m2.794.931.449.16fy 1995 %30541610046. for example, nasa feels pressure from the hpcci objectives to orient its program in certain directions, but is encouraged by theaeronautics industry to orient its activities in other directions. nasa is caught in the middle. aeronautics industry representation at thehpcci leadership level could help guide the hpcci in directions that better support the goal of enhancing u.s. industrialcompetitiveness.47. letter dated august 25, 1994 to marjory blumenthal from jerry d. mahlman (noaa) in response to committee's interim report(cstb, 1994c).the high performance computing and communications initiative  52evolving the high performance computing and communications initiative to support the nation's information infrastructurecopyright national academy of sciences. all rights reserved.3recommendationsthe committee believes that strong public support for a broadly based research program in informationtechnology is vital to maintaining u.s. leadership in information technology. facilitated access for both academicand industrial users to advanced computing and communications technologies has produced further benefits bothin scientific progress and in u.s. industrial competitiveness. the committee's recommendations for the highperformance computing and communications initiative (hpcci) are based on this view of the importance ofinformation technology to the country, as well as on the track record of success for the government's investment ininformation technology research. the committee's 13 recommendations address five different areas: research program;hperformance computing;working and information infrastructure, including work focusing on the national challenges; supercomputer centers and the grand challenge projects; and coordination and program management.within each area the recommendations are presented in priority order.general recommendationsas discussed in chapter 1, government investment has played a major role in maintaining u.s. leadership ininformation technology and in helping to advance the technology, providing benefits to virtually every citizen. thereturn on federal investment has been substantial.recommendation 1. continue to support research in information technology. ensure that the majorfunding agencies, especially the national science foundation and the advanced research projects agency,have strong programs for computing research that are independent of any special initiatives. pastinvestment has yielded significant returns, as demonstrated in chapter 1. continued government investment incomputing research, at least as high as the current dollar level, is critical to continuing the innovation essential tomaintaining u.s. leadership.today the hpcci supports nearly all of this research, an arrangement that is both misleading and dangerous:misleading because much important computing research addresses areas other than highperformance (even thoughit may legitimately fit under the new information infrastructure technology and applications (iita) componentof the hpcci), and dangerousrecommendations53evolving the high performance computing and communications initiative to support the nation's information infrastructurecopyright national academy of sciences. all rights reserved.because reduced funding for the hpcci could cripple all of computing research. the "war on cancer" did notsupport all of biomedical research, and neither should the hpcci or any future initiative on the nation'sinformation infrastructure subsume all of computing research.recommendation 2. continue the hpcci, maintaining today's increased emphasis on the researchchallenges posed by the nation's evolving information infrastructure. in addition to the work on infrastructurecarried out in the new iita program, continuing progress is needed in areas addressed by the hpcci's other fourcomponents (highperformance computing systems, national research and education network (nren),advanced software technology and algorithms, and basic research and human resources).the nsfnet and the gigabit testbeds have demonstrated the ability to build largerscale, higherperformancenetworks, but ongoing research in several areas is still needed before a ubiquitous highperformance informationinfrastructure can be developed and deployed nationwide. the committee supports the hpcci's increasing focuson information infrastructure, emphasizing that successful evolution of the nation's communications capabilityrests on continued investment in basic hardware, networking, and software technologies research. to further thisevolution, which is consistent with administration efforts, including the addition of the iita program, plusgeneral accounting office (gao, 1994) and other recommendations, the committee has identified inrecommendations 3 through 10 program areas that should receive (a) increased emphasis, (b) stay at presentlevels, and (c) have reduced federal support.recommendations on highperformance computingrecommendation 3. continue funding a strong experimental research program in software andalgorithms for parallel machines. it is widely recognized that software for parallel computers lags behindhardware development. progress in software and algorithms for parallel computers will determine how quickly andhow easily we can use them.1 a shift in emphasis toward increased funding for software and algorithm activitiesunder highperformance computing has already begun. this shift properly reflects the urgency of investing more insoftware.the committee recommends the following approach to continue progress in research areas critical todeveloping and building needed software and algorithms: continue research on compilers, programming languages, and tools aimed at making it easier to useparallel computing machines. critical needs include improved portability across machines, improvedability to run programs on machines of different sizes, and better understanding of how best to usedifferent multiprocessor memory organizations. continue to develop experimental operating systems for parallel computers. more operating systemexperience will help us learn how to improve parallel hardware. focus on the underlying researchchallenges posed by parallel machines rather than developing commercial operating systems technology. continue research on database and information systems for parallel machines. such applications haveincreased in importance and represent a promising area for using parallel computing. continue research in the use of parallel computing for graphics and visualization. graphics applicationsare valuable both because they demand much from their software and hardware and because theystimulate effective use of highperformance computing byrecommendations54evolving the high performance computing and communications initiative to support the nation's information infrastructurecopyright national academy of sciences. all rights reserved.offering computational scientists and other endusers the ability to analyze complex data and problems. fund sufficient hardware purchases to ensure needed access for computer scientists and end users tryingto evaluate the effectiveness of new architectures and software technologies. dedicated access toexpensive machines is often required for operating systems development or for controlled measurementof software performance, and sometimes dedicated access is needed to fullscale machines, which arethen most economically housed in a centralized, national center. the importance of the local availabilityof midrange machines for researchers in software for parallel computers was noted in the branscombreport (nsf, 1993). provide resources to help complete the development and distribution of compilers, programming tools,and related infrastructure broadly usable by the software research community. such infrastructurešwhich may be developed by individual research groups or by centers (such as the nsf science andtechnology centers)šhas been crucial to rapid progress. for example, tools for the design of very largescale integrated (vlsi) circuits allowed many researchers to undertake vlsi designs. the committeenotes that funding agencies should avoid turning related infrastructure development efforts into productdevelopment efforts. seek improved integration of parallel computing hardware and software with communication networks,both in software and hardware research. emphasize design and analysis of new algorithms for parallel computing, as well as implementation andevaluation of these algorithms on real parallel machines. opportunities for development of new parallelalgorithms exist in both scientific and information infrastructurerelated applications. the theoreticalperformance and scaling efficiency of new algorithms need to be demonstrated by actual implementationand evaluation on parallel machines, first by computer scientists and then embedded in real enduserapplications. ensure that effective new algorithms for parallel computing are made widely available to endusercommunities to assist in building applications.recommendation 3.1. avoid funding the transfer ("porting") of existing commercial applications tonew parallel computing machines unless there is a specific research need. several existing applications enjoywidespread commercial use on large uniprocessor and vector machines; examples include thirdparty codes inchemistry, biomolecules, engineering fluid dynamics, deformable structures, and database access. it has beenproposed by some that the hpcci should fund transferring, or "porting," such applications to new types ofparallel computers as a way to enhance the attractiveness of new parallel machines. the committee findsinappropriate the use of federal hpcci funding for such porting of applications for several reasons. first, thealgorithms used in these applications were designed for sequential or vector computing, and thus little newknowledge will be gained from merely porting existing applications to a parallel machine without redesigning thealgorithms. second, the open market will fund such transfers if a sufficient user base exists. third, choosing whoseapplication to transfer and to which machines will involve the hpcci in picking winners from among manycommercial vendors.although it recognizes that a federal agency might decide that one of its missions would best be served byporting an existing application to a parallel computer, the committee recommends that funding of such ports bejustified on the basis of the agency mission and not as hpccirecommendations55evolving the high performance computing and communications initiative to support the nation's information infrastructurecopyright national academy of sciences. all rights reserved.research. the committee believes that it is legitimate for groups of agencies to work together to developcommunity codes for common applications needed by their several missions. likewise, carrying out an hpcciresearch program may require that applications be available on a particular parallel machine, in which case thetransfer could be justified by the importance of the research it enabled. finally, the committee also sees alegitimate reason to port existing applications for the purpose of evaluating machines within a research laboratoryor university.recommendation 4. stop direct hpcci funding for development of commercial hardware bycomputer vendors and for ''industrial stimulus" purchases of hardware. maintain hpcci support forprecompetitive research in computer architecture; this work should be done in universities or inuniversityindustry collaborations and should be driven by the needs of system and application software.the development and placement of parallel hardware to date was necessary to establish parallel computing as aviable alternative to sequential and vector computing. (the establishment of this paradigm is discussed throughoutchapter 2 and in appendixes a and e.) industry is now willing and able to improve on the base of ideasestablished by the hpcci, at least for mainstream parallel machines (special government requirements arediscussed below). government development funds should no longer be spent in industry either to further refineparallel machines or to purchase machines as a stimulus for vendors.the committee notes that use of hpcci funds for these purposes has already decreased significantly, a trendthat the committee supports.2 federal funding of hardware developments within companies should continue todecline, unless some special agency need demands the development of nonstandard hardware (e.g., a highperformance system for use on a ship or in an airplane, such as intel corporation's parallel paragon computermade more rugged for military use, or for a highly specialized application). in such cases, agency mission funds,and not hpcci funds, should be used.important precompetitive hardware research problems merit continued federal funding because thedevelopment of parallel computing architecture and gigabit networks will not be the final chapter in the continuingdevelopment of ever more powerful systems. the committee recommends that ongoing research efforts inhardware and architecture be based in academic and research institutions, possibly in collaboration with industry.potential problems can be minimized if the research institution serves as the project lead, and if the researchchallenges rather than commercial development are the focus (cohen and noll, 1994). not only do academicinstitutions have more freedom to think about longerterm issues, but they also stimulate technology transferthrough publication and placement of graduates. the national experience supports a basic tenet of vannevar bush:publicly funded research carried out in universities produces excellence, diversity, fresh ideas, trained people, andtechnology transfer (osrd and bush, 1945). commercial organizations, on the other hand, have powerfulincentives to avoid distributing new ideas widely and may even impede the introduction of new technology whenit competes with existing products.to narrow the gap between parallel computing hardware capabilities and the software needed to use them,research on architecture should be driven by software and applications needs. thus, further integration ofapplication and system software needs into architecture research should be encouraged in any funding ofarchitecture research.recommendation 5. treat development of a teraflop computer as a research direction rather than adestination. the committee believes that federal investment in developing or purchasing machines to demonstrateraw scalability for its own sake is inappropriate, except as a focus for precompetitive, academic research. instead,the focus should be on matching agencies' mission requirements to the emerging sustainable scalablearchitectures. such architectures will very likely reach 1teraflop capability before the end of this decade using1,000 or so highperformance commercial microprocessors.the goals of scalability over many sizes of machine and of demonstrating teraflop performance have beenuseful in pointing toward the use of massproduced devices in largerecommendations56evolving the high performance computing and communications initiative to support the nation's information infrastructurecopyright national academy of sciences. all rights reserved.collections to solve complex computing tasks, but implementation of a machine of any specific size can bepremature. moreover, seeking a common design over a large size range is wasteful because the expensivecommunication paths required to harness large numbers of inexpensive processors together are inappropriate whenscaled down to smaller machines with only a few processors. the pursuit of wide scalability may have deferredearly consideration of sharedmemory parallel computers, the type that today appears promising. in fact, the focuson teraflop capability detracts from other important aspects of highperformance, such as memory and input/output systems, which are critical components of any highperformance system.advances in parallel architectures together with progress in the underlying integrated circuit technology willcontinue to provide improvements in performance/cost ratios that will naturally bring computing power to theteraflop level. most industry analysts see the potential for single microprocessors with 1 to 2gigaflop peakperformance by the end of the decade. combining 512 to 1024 such future microprocessors in a scalable systemwould create a teraflop capacity at roughly the price of today's supercomputers, with capabilities of tens ofgigaflops possible. supporting research into the key technologies needed to achieve and use scalable computing,combined with patience to see how the relative economics of computing power and communications interact,seems to this committee to be the most efficient approach to increasing performance.the committee thus emphasizes that the hpcci should treat the goal of teraflop performance as a milestoneto be reached naturally by computer vendors in due course, not on a forced time scale. the hpcci should continueto fund research on technologies that will contribute to reaching the goal. at some point in the near future ateraflop parallel machine will be built when some agencies' mission requirements correspond to a sufficientlyeconomical commercial offering. continued progress will naturally lead to machines much larger than a teraflop.recommendations on networking and information infrastructurethe committee believes that the successes of the hpcci in establishing scalable compute servers,investigating highperformance networks, and forming interdisciplinary teams of computer and applicationscientists are setting the stage for important new research to support enhancement of the nation's informationinfrastructure. an increased emphasis on the research needed to achieve such an infrastructure is desirable (cstb,1994d).in fact, this shift has already begun; spending on networking and iita activities accounted for nearly 50percent of the hpcci budget requested in fy 1995:3 $177 million for the nren and $282 million for iita. thisis a significant increase over the $114 million that was spent for nren in fy 1993, the year prior to the additionof iita. the committee believes that such a shift is appropriate.recommendation 6. increase the hpcci focus on communications and networking research, especiallyon the challenges inherent in scale and physical distribution. advancing the nation's information infrastructurewill put great demands on digital communications technology for providing broad access to services. ensuringbroad access poses a host of technical and economic questions for which existing solutions are inadequate. thecommittee recommends increased support for learning how to attach millions of users to a digital communicationsstructure that provides a wide array of services and greater integration of services, and how to accommodate thedemands that these users will generate using the novel applications enabled by such an information infrastructure.recommendation 7. develop a research program to address the research challenges underlying ourability to build very large, reliable, highperformance, distributed information systems based on the existinghpcci foundation. an improved infrastructure will need to offerrecommendations57evolving the high performance computing and communications initiative to support the nation's information infrastructurecopyright national academy of sciences. all rights reserved.capability to all facets of our economy on a scale not yet imagined, and no one yet anticipates all of the ways thatusers will use such an information infrastructure.improvements to the nation's information infrastructure and activities related to it have generated a level ofpublic interest matched by only a few technologybased objectives. the committee is concerned that unrealisticexpectations for availability and for the quality and range of services could encourage a shortterm, productoriented focus in funding research activities4 that would not be in our country's best interest. care should be takento apprise policymakers and the public of the long time needed for development and widescale deployment of theservices expected to be available through the information infrastructure.the committee strongly recommends that the hpcci remain focused on the basic research issues arising fromdesired improvements to the information infrastructure, evolving from its early emphasis on parallel, highperformance computing, highperformance networking, and scientific applications to the broader issues ofconnection, scale, distributed systems, and applications. the addition of the iita area to the hpcci was a keystep in accelerating a shift in focus of the research community to the challenges of improving the nation'sinformation infrastructure. the committee has identified three key areas where new emphasis is critical tosupporting the research needs associated with the information infrastructure:1. scalability. while the hpcci has emphasized large computing systems on the order of thousands ofinteracting computers, an enhanced, nationwide information infrastructure will require scaling tomillions of users. in addition, the hpcci has emphasized bringing the highest performance to bear onindividual scientific applications, whereas improving the information infrastructure for the nationrequires using the highest performance to meet the practical needs of millions of simultaneous users.2. physical distribution and the problems it raises. a better information infrastructure will emphasizegeographical distribution with its limitations on bandwidth, increase in latency of communication, andadditional challenges in secure and reliable communication. these challenges have been much lesssevere in localized highperformance parallel systems. research on both distributed and parallelsystems technology will be important in supporting this aspect of a nationalscale informationinfrastructure.3. innovative applications. a shift should occur from a focus on specific grand challenge problems inscience to wellformulated national challenges that affect a wider segment of society. the committeesees an important role for development and demonstration of easily appreciated applications that willdrive the technology of the information infrastructure.improving scalability and physical distribution requires investment in both:hardware and architecture, including systems that efficiently handle a rich mix of text, images, and audioand video data; systems that provide costeffective, highbandwidth, endtoend communications; andsystems that provide information access to large numbers of users; andsoftware, including basic networking software for encryption, routing, flow control, and so on; tools forproviding and building such capabilities as scheduling, bandwidth optimization, video handling, andservice adaptation; and many others. this is the socalled "middleware."recommendations58evolving the high performance computing and communications initiative to support the nation's information infrastructurecopyright national academy of sciences. all rights reserved.the committee believes that building on the current hpcci model of a coordinated program, avoidingcentral control, seems even more crucial for the iita portion of the research program, because the challengesposed by an information infrastructure are inherently less well defined than those addressed by other componentsof the initiative. the committee is encouraged by the early development of cooperative research programs in iitaareas, such as the digital libraries program, which includes nsf, arpa, and the national aeronautics and spaceadministration (nasa), and by recent attempts to identify topics for research, such as discussions among severalhundred researchers and others at a workshop in early 1994 (vernon et al., 1994).recommendation 8. ensure that research programs focusing on the national challenges contribute tothe development of information infrastructure technologies as well as to the development of newapplications and paradigms. the national challenges incorporate socially significant problems of nationalimportance that can also drive the development of information infrastructure. hardware and software researchersshould play a major role in these projects to facilitate progress and to improve the communication with researchersdeveloping basic technologies for the information infrastructure. awards to address the national challengesshould reflect the importance of the area as well as the research team's strength in both the applications and theunderlying technologies. the dual emphasis recommended by the committee contrasts with the narrower focus onscientific results that has driven many of the grand challenge projects.because the national challenges as currently defined are too broad and vague to offer specific targets forlargescale research, the notion of establishing testbeds for a complete national challenge is premature. instead,research funding agencies should regard the national challenges as general areas from which to select specificprojects for limitedscale testbeds or focused software research projects. particular areas in which a focusedresearch target can be identified (e.g., the arpansfnasa digital library testbeds) may be appropriate forslightly higher funding, but the committee believes that very largescale applications development is premature.at this early stage, letting "a thousand flowers bloom" will provide a better payback than funding a few large orfull scale deployments. (box a.3 and related text in appendix a give more information on the nationalchallenges.)recommendations on the supercomputer centers and grand challengeprogramthe four nsf supercomputer centers are the largest single element of the fy 1995 hpcci implementationplan in dollars ($76 million, or 6.6 percent of the requested fy 1995 hpcci budget) and the largest infrastructureproject in the initiative. the centersšwhich give users access to a broad array of powerful tools ranging fromhighly innovative to maturešare a significant national resource for gaining knowledge, experience, andcapability. thanks to their leadership, highperformance computing and communications are now widely acceptedas an important tool in academia, industry, and commerce.the centers have played a major role in establishing parallel computing as a full partner with the priorparadigms of scalar and vector computing. they have contributed by providing access to stateoftheartcomputing facilities to a broad range of users. as new largescale architectures appear, the centers stimulate theirearly use by providing access to these architectures and by educating and training users. (appendix e details theaccomplishments of the nsf centers and of their national user base.)the committee recognizes that advanced computation is an important tool for scientists and engineers andthat support for adequate computer access must be a part of the nsf research program in all disciplines. thecommittee also sees value in providing largescale, centralized computing, storage, and visualization resourcesthat can provide unique capabilities. how suchrecommendations59evolving the high performance computing and communications initiative to support the nation's information infrastructurecopyright national academy of sciences. all rights reserved.resources should be funded and what the longterm role of the centers should be with respect to both new andmaturing computing architectures are critical questions that nsf should reexamine in detail, perhaps via the newlyannounced ad hoc task force on the future of the nsf supercomputer centers program. for example, much ofthe general access to computing resources at the centers is provided on maturing architectures. neither the qualityof the science performed by the users of such technology nor the appropriateness of nsf funding for such generalaccess is in question. the committee did not consider the appropriate overall funding level for the centers.nonetheless, the committee does question the exclusive use by the nsf of hpccispecific funds for support ofgeneral computing access, which in itself does not simultaneously help drive the development of highperformancecomputing and communications technology.in this regard, nsf follows a different approach to funding its supercomputer centers than do nasa and thedepartment of energy (doe), where hpcci funds are used only to support the exploration and use of newcomputing architectures, while nonhpcci funds are used to support general access. the committee believes thatdoe's and nasa's approach to funding general access should be followed across the agencies. also, as thecommittee points out in recommendation 12, including all of the nsf supercomputer centers' funding underhpcci could cause major disruption to the centers' national mission should the hpcci be altered significantly.recommendation 9. the mission of the national science foundation supercomputer centers remainsimportant, but the nsf should continue to evaluate new directions, alternative funding mechanisms, newadministrative structures, and the overall program level of the centers. nsf could continue funding of the centersat the current level or alter that level, but it should continue using hpcci funds to support applications thatcontribute to the evolution of the underlying computing and communications technologies, while support forgeneral access by application scientists to maturing architectures should come increasingly from nonhpccifunds.examination of the supercomputer centers program should include identification of: new roles for the centers in supporting changing national needs; and future funding mechanisms, including charging mechanisms and funding coupled to disciplinarydirectorates.in addition to enabling highperformance scientific computing, several of the nsf centers have developednew software technologies that have significantly affected other parts of the hpcci. the most obvious of these isthe recently developed mosaic world wide web browser. the committee recommends that nsf continue to take abroad view of the centers' mission of providing access to hpcci resources, including, but clearly not limited to,participating in research needed for improved software for parallel machines and for enhancement of the nation'sinformation infrastructure. the centers, and the researchers who use their facilities, should compete for researchfunds by the normal means established by the funding agencies.recommendation 10. the grand challenge program is an innovative approach to creatinginterdisciplinary and multiinstitutional scientific research teams; however, continued use of hpcci fundsis appropriate only when the research contributes significantly to the development of new highperformance computing and communications hardware or software. grand challenge projects fundedunder the hpcci should be evaluated on the basis of their contributions both to highperformancecomputing and communications and to the application area. the grand challenge problems are sufficientlylarge and complex and the research teams addressing them are capable enough to exercise the parallel computingtechnology thoroughly and to test its capability. these efforts have been supported under the hpcci as a validway to involve real users in parallel computing, but as parallel computing becomes an established tool, the need touse the hpcci to stimulate the user community will decrease. furthermore, the use ofrecommendations60evolving the high performance computing and communications initiative to support the nation's information infrastructurecopyright national academy of sciences. all rights reserved.highperformance computing will become more pervasive, making it unreasonable to include all such programswith the hpcci.the committee recommends completion of the initial grand challenges as planned over the next few years.currently, although the scientific disciplines are providing major funding for grand challenge projects (e.g., morethan 50 percent of requested fy 1995 funds for nsf grand challenges come from the scientific and engineeringresearch directorates), virtually all of the grand challenge funding is labeled hpcci. the committee urges thatany followon funding of grand challenges include a significant and growing fraction of nonhpcci scientificdisciplinary funds. this will limit the selection to tasks whose scientific interest justifies their cost, in competitionwith other research in their respective disciplines.the committee sees an ongoing value from the strong interaction between challenging applications and newarchitectures and software systems and from cooperation between computer and computational scientistsšanumber of the grand challenge teams have demonstrated that collaboration can lead to advances in bothcomputing and the particular scientific discipline involved. partial funding of applications research that contributesto the development of new hardware and software systems is a legitimate use of hpcci funds. such activitiesmust be evaluated on the basis of their contributions both to highperformance computing and communicationstechnologies and to the application area.recommendations on coordination and program management in thehpccirecommendation 11. strengthen the hpcci national coordination office (nco) while retaining thecooperative structure of the hpcci and increasing the opportunity for external input. as the committeepointed out in its interim report (cstb, 1994c, p. 9), the dimensions of the need for clear communication aboutthe hpcci have recently become apparent: congressional oversight activities and other indicators suggest that thehpcci is of concern to a growing constituency and that often a variety of audiences need detailed explanations ofit. such an effort will add substantially to the work of the nco, which has been headed by a halftime,permanentposition director who holds a concurrent, halftime appointment as director of the national library ofmedicine (nlm).5 the other nco staff positions are a mix of permanent positions, contract positions, andtemporary positions filled by individuals on loan from other federal agencies for limited periods of time, often nomore than 1 year.6although the nco reports to the office of science and technology policy (ostp) on programmatic matters,administrative functions such as office space, salaries, and benefits have been handled through the nationalinstitutes of health. the temporary nature of some of the nco positions jeopardizes continuity and cumulativeinsight. further, limited staff resources raise questions about the nco's capacity to meet the challenge of thegrowing volume, complexity, and urgency of the outreach efforts needed for the initiative (cstb, 1994c, p. 9).the nco serves an important coordination and communication function both among agencies of thegovernment and between the agencies, congress, industry, and the public. it is to the credit of the nco staff thatthe hpcci has been an effective model of interagency collaboration. in recommending a strengthening of thenco, the committee strongly endorses the current nco's role of supporting the mission agencies rather thandirecting them. the committee believes that it is vital that direction of the hpcci remain in the agencies.by avoiding actual direction the nco leaves mission judgments in the hands of responsible agency officialswho are accountable for the allocation of their resources. by avoiding the appearance of direction the ncoencourages an appropriate diversity of research projects as each agency capitalizes on its best ideas. missionagencies cooperate effectively with each other andrecommendations61evolving the high performance computing and communications initiative to support the nation's information infrastructurecopyright national academy of sciences. all rights reserved.with the nco precisely because it does not threaten their autonomy. this cooperation could easily vanish were thenco seen as functioning with too heavy a hand. the committee believes that the value of interagency cooperationoutweighs the potential benefits that might be gained through more centralized management of the hpcci(cstb, 1994c, p. 8).the committee strongly recommends retaining the model of a cooperative and coordinated interagencyprogram. some individuals and organizations have expressed concern about the lack of centralized management ofthe hpcci. however, the committee believes that the current cooperative structure is one of the initiative'sstrengths, providing room for diversity of thought and action. such diversity is essential to progress, because noone manager or agency has a monopoly on the right ideas for the future of computing and communications.central management of the hpcci could focus its activities too narrowly, as well as lead to reduced interest in theprogram by agencies that found that the centralized agenda no longer matched their interests.7the committee believes that government investment in information technology research has often enjoyedfirstclass leadership. program officers with vision have supported innovative ideas, leading to later successes. thecommittee emphasizes that the best method for making continued research investment is to continue to attracthighly competent program officers and to give them the flexibility to develop effective programs. in the past, thisapproach has yielded solid returns on the research investment. furthermore, it has encouraged necessary diversityin the research program, thus increasing opportunities for unexpected discoveries and ensuring a broad perspectivein addressing problems.recommendation 11.1. immediately appoint the congressionally mandated advisory committeeintended to provide broadbased, active input to the hpcci, or provide an effective alternative. the hpccicould be improved by input from and review by an advisory committee with balanced representation from industryand academia, including current and potential users and developers of highperformance computing andcommunications. if appointment of such a committee is not feasible, some alternate mechanism should soon bedeveloped to provide similar input. the committee is aware of the recent increases in the number of advisorycommittees, as well as the danger of having too many committees. thus, the committee recommends that thehpcci advisory committee have a welldefined role focusing primarily on providing external input into thecoordination and planning for the hpcci.recommendation 11.2. appoint an individual to be a fulltime coordinator, program spokesperson, andadvocate for the hpcci. having a parttime nco director has served well to this point, but the broadening ofthe hpcci demands leadership unencumbered by other major responsibilities. a fulltime person could eitherdirect the nco or could report to the director and would work to strengthen the ties between the hpcci, industry,the congress, and the public. the committee uses the word "coordinator" rather than "manager" to emphasize theneed for coordination and communication that avoid usurping the authority of the mission agencies. the ncoshould remain within the ostp structure.recommendation 12. place projects in the hpcci only if they match well to its objectives. a number ofpreexisting agency programs have entered the hpcci. such administrative changes make the hpcci budgetappear to grow faster than the real growth of investment in highperformance computing and communications.some of these programs exactly match the goals of the hpcci and are properly included. others are onlymarginally relevant and might better be placed elsewhere in agency budgets. the committee sees the possibility of alongterm danger to important programs, such as basic research in computing within nsf and arpa, should thehpcci end.recommendation 12.1. federal research funding agencies should promptly document the extent towhich hpcci funding is supporting important longterm research areas whose future funding should beindependent of the future of the hpcci. the committee found that many research areas predating the hpcciand related only partly to its goals are now under therecommendations62evolving the high performance computing and communications initiative to support the nation's information infrastructurecopyright national academy of sciences. all rights reserved.hpcci umbrella. although encouraging important research areas to include and even focus on hpccirelatedcomponents, the process of coding all funding in a research area as highperformance computing andcommunications can be dangerous. in many cases, areas were recoded as highperformance computing andcommunications without receiving an increase in funds. the danger in this process is that changes in the directionor level of funding for the hpcci could lead to unintentional changes in the funding levels of important researchareas, even if they are largely unrelated to the hpcci and even if they have received none of the hpcciincremental funding!this problem is particularly acute at nsf, where nearly all of the funding in the computer and informationscience and engineering directorate is coded as hpcci funding. given that nsf is not a mission agency and isinvestigatordriven, this approach seems shortsighted. nsf would have to retain funding for computer scienceresearch even in the absence of the hpcci. ongoing funding of important research areas in computer science willbe critical to the nation's future, independent of the future of the hpcci.recommendation 13. base mission agency computer procurements on mission needs only, andencourage making equipment procurement decisions at the lowest practical management level. to stimulatethe use of parallel computing early in the hpcci's 5year time frame, it has been appropriate and necessary toplace into service a reasonable number of highly parallel machines for serious algorithm and softwaredevelopment. early development of an adequate base of parallel computers was essential to shifting the attentionof industry and research organizations toward parallel computing. now, however, it is more appropriate to baseprocurement of computer hardware on mission needs only. one program that claims to have done so already is thedefense highperformance computing modernization program. the committee applauds modernization of thecomputing facilities available to department of defense organizations and the missiondriven nature of theprocurement process, which should be established in all agencies.each agency has responsibility for its own budget and its own requirements. the committee believes thatagencies should base procurements of computing equipment on their needs and budget constraints. agenciesshould be free to purchase parallel computers when they suit agency needs. individual agencies can balance thecost of obtaining applications against the cost of computing equipment so as to best match procurements to theirrequirements. parallel computing is now mature enough to be considered a viable alternative to other forms ofcomputing and may deliver suitable computing power at less cost than other architectures.although the committee firmly recommends that computer purchases be guided by mission needs, it also sees arole for collaboration between mission agencies and industrial or university parallel computing consortia. directagency responsibility for missions, budgets, and equipment purchases can be reconciled with the advantages ofgroup action through participation in appropriate consortia. for example, the nsf centers have proven valuablefor offering exploratory experience with highperformance computing, and it is encouraging to see industrialacademic consortia forming to explore parallel computing. the committee encourages mission agencies toparticipate with the nsf centers and other parallel computing consortia. such participation offers knowledge at lowcost and leads ultimately to more costeffective procurements.the committee's recommendation that equipment be selected at the lowest practical management levelapplies equally to government agencies and to government contractors. the direct manager of a computing facilityis charged with making it work and will do that task best if allowed to select equipment that matches the facility'sneeds. the committee believes that agencywide procurement of standard brands, while promoting collaboration,can weaken the responsibility of the user organizations. likewise, it has generally been best for an agency tospecify the results it wants and to leave the choice of specific equipment to the contractor.8 delegating equipmentselection not only saves toplevel agency decision making resources but also places responsibility for purchasedecisions firmly in the hands of the managers who must deliver results.recommendations63evolving the high performance computing and communications initiative to support the nation's information infrastructurecopyright national academy of sciences. all rights reserved.comments relating this report's recommendations for highperformance computing and communications research toadministration prioritiesthis report's recommendations broadly address much of the computer science and engineering research beingconducted today, as well as the hpcci specifically. in a may 1994 memorandum from the director of the officeof management and budget to all agency heads, the administration outlined its priorities for u.s. research anddevelopment in general. box 3.1 briefly compares applicable parts of that memorandum to the positions taken andactions recommended in this report of the committee to study highperformance computing andcommunications.box 3.1 comparison of administration priorities for harnessing informationtechnology to committee recommendations in this reportcomputing systems"the development of scalable systems with the input/output capabilities, mass storage systems, realtime services,and information security features needed to build and fully utilize the national information infrastructure (nii) should beemphasized. highperformance computing systems capable of 1012 operations per second (a teraflop) on technicalproblems will be achieved by fy 1997. emphasis should be placed on advances in information storage media for both highand low end applications; systems integration of clustered workstations and large parallel systems; development ofadvanced tools and processes for the design and prototyping of faster semiconductor devices; and research onnanotechnology, photonics, flat panel displays, and integrated microelectricalmechanical devices."the committee on highperformance computing and communications recommends continuing the focus on highperformance parallel computing, but decreasing the emphasis on achieving a teraflop computing system on a specific timescale. the committee agrees that scalable highperformance computing and communications technology that supports thenation's emerging information infrastructure is important. the committee also recommends increased emphasis on thesupport of communications within new computer systems.networking and communications"it will be necessary to support the development of the networking technology required for the deployment of nationalgigabit speed networks incorporating heterogeneous carriers including satellite and wireless capability. this means servinghundreds of millions of users and demonstrating mobile and wireless capability. it includes the development ofinteroperability concepts and technologies and the integration of computers, televisions, telephones, wirelesstelecommunications and satellites."the committee thoroughly agrees.recommendations64evolving the high performance computing and communications initiative to support the nation's information infrastructurecopyright national academy of sciences. all rights reserved.software, algorithms, and basic research"the united states should conduct basic research to support the computational requirements of new computingparadigms. there is a need for new methods for data authentication and software verification and validation. thedevelopment of tools and techniques to enable assembly of systems from inexpensive, versatile, reusable softwarecomponents is required."the committee's recommendations emphasize the importance of a complete program of basic research in computerscience and underscore the importance of research on software and algorithms to make the best use of new computingparadigms.information infrastructure services"access to and utilization of the nii will require services, tools, and interfaces that facilitate a wide range ofapplications. these include registries, directories, navigation and resource discovery tools, data interchange formats, andother information services that help users find and query services and components in distributed repositories. there willhave to be new types of humancomputer access and the development of improved collaborative software, groupware,and authoring tools for multimedia will be important. equally important are the development of privacy and securitytechnologies and integrated information and monitoring systems."the committee enthusiastically agrees.humancomputer interaction''new products and applications are enabled by those hardware and software technologies that will allow everyamerican to use easily the nii. development and use of the following should be advanced: virtual reality; simulation; flatpanel displays; video and high definition systems; threedimensional sound, speech interfaces, and vision."the committee did not study humancomputer interaction and specific related areas for research, although these arecertainly a key part of a broad research program on information infrastructure.computing and communications applications"the fy 1996 r&d budget should advance applications of highperformance computing and the nii. the federalhighperformance computing and communications program is helping to develop the technologies and techniquesneeded to solve critical research problems that require more advanced computers, storage devices, algorithms, andsoftware tools. additional effort is needed to accelerate the transfer of these technologies from the laboratory to themarketplace."the committee recommends that applications be funded increasingly from outside the hpcci, especially as thetechnology underlying the application becomes mature. hpcci funds should focus research support on applications thataffect the base computing and communication technologies, as well as solve new applications problems. the committeerecommends that such a policy be followed in funding work on the grand challenges and the emerging nationalchallenges.source: panetta (1994), pp. 1011.recommendations65evolving the high performance computing and communications initiative to support the nation's information infrastructurecopyright national academy of sciences. all rights reserved.notes1. see the subsections "programming" and "algorithms" in appendix a for a discussion of development and achievements to date.2. based on briefings to the committee by anita jones (department of defense); duane adams, howard frank, and john toole (advancedresearch projects agency); and victor reis (department of energy).3. see nco (1994), p. 15. note that figures represent the president's requested budget authority for fy 1995. actual appropriated levelswere not available at press time. because the hpcci is synthesized as a crosscutting multiagency initiative, there is no direct "hpcciappropriation."4. this risk is illustrated in the gao examples of standards setting and other nonresearch activities under the hpcci umbrella.5. a january 6, 1995, press release from the office of science and technology policy announced the resignation of the nco's director.donald a.b. lindberg. lindberg, requesting that a successor be named when his current 2year term ends, recommended that a fulltimedirector be appointed at this point in the evolution of the hpcci.6. letter dated august 8, 1994, to marjory blumenthal (cstb) from donald a.b. lindberg (nco/nlm) in response to the committee'sinterim report (cstb, 1994c).7. the committee shares ostp director john gibbons' concerns about the centralized management advocated by gao (1994, p. 34).8. the committee notes the extreme fruitfulness of this model in the hands of the late sidney fernbach of doe's lawrence livermorenational laboratory, who for a generation kept his laboratory at the forefront of computing and at the same time helped to stimulate thedevelopment of generations of supercomputers.recommendations 66evolving the high performance computing and communications initiative to support the nation's information infrastructurecopyright national academy of sciences. all rights reserved.bibliographyanthes, gary h. 1994. "cray envisions new frontier," computerworld,  july 18, p. 121.anthes, gary h., and mitch betts. 1994. "r&d: measure of success," computerworld, november 14, p. 32.browder, felix e. 1992. "of time, intelligence, and institutions," dædalus 121(1 winter):105110.carlton, jim. 1994. "makers of pcs had very, very happy holiday," wall street journal, december 27, pp. b1 and b4.carnevale, mary lu. 1994. "u.s. awards grants to spur data network," wall street journal, october 13, p. b6.cohen, linda r., and roger g. noll. 1994. "privatizing public research," scientific american 271(3):7277.commission on physical sciences, mathematics, and applications (cpsma); national research council. 1994. quantitative assessments ofthe physical and mathematical sciences: a summary of lessons learned. national academy press, washington, d.c.committee on information and communication (cic). 1994. highperformance computing and communications: technology for thenational information infrastructure, supplement to the president's fiscal year 1995 budget. national science and technologycouncil, washington, d.c.computer and business equipment manufacturers association (cbema). 1994. information technology industry data book 19602004. computer and business equipment manufacturers association, washington, d.c.computer science and technology board (cstb), national research council. 1988. the national challenge in computer science andtechnology. national academy press, washington, d.c.computer science and telecommunications board (cstb), national research council. 1992. computing the future: a broader agenda forcomputer science and engineering. national academy press, washington, d.c.bibliography67evolving the high performance computing and communications initiative to support the nation's information infrastructurecopyright national academy of sciences. all rights reserved.computer science and telecommunications board (cstb), national research council. 1993. national collaboratories: applying informationtechnology for scientific research. national academy press, washington, d.c.computer science and telecommunications board (cstb), national research council. 1994a. academic careers for experimental computerscientists and engineers. national academy press, washington, d.c.computer science and telecommunications board (cstb), national research council. 1994b. information technology in the servicesociety: a twentyfirst century lever. national academy press, washington, d.c.computer science and telecommunications board (cstb), national research council. 1994c. interim report on the status of the highperformance computing and communications initiative. computer science and telecommunications board, washington, d.c.computer science and telecommunications board (cstb), national research council. 1994d. realizing the information future. the internetand beyond. national academy press, washington, d.c.computer science and telecommunications board (cstb), national research council. 1994e. research recommendations to facilitatedistributed work. national academy press, washington, d.c.computer science and telecommunications board (cstb), national research council. 1995. information technology for manufacturing: aresearch agenda. national academy press, washington, d.c., in press.computer select. 1994. data sources report. ziff communications company, new york, december.computer systems policy project (cspp). 1991. expanding the vision of highperformance computing and communications: linkingamerica for the future. computer systems policy project, washington, d.c., december 3.computer systems policy project (cspp). 1993. perspectives on the national information infrastructure: cspp's vision andrecommendations for action. computer systems policy project, washington, d.c.computer systems policy project (cspp). 1994. perspectives on the national information infrastructure: accelerating the development anddeployment of nii technologies. computer systems policy project, washington, d.c.congressional budget office (cbo). 1993. promoting highperformance computing and communications. u.s. government printing office,washington, d.c., june.corcoran, elizabeth. 1994. "the changing role of u.s. corporate research labs," researchtechnology management 37(4):1420.bibliography68evolving the high performance computing and communications initiative to support the nation's information infrastructurecopyright national academy of sciences. all rights reserved.council on competitiveness. 1991. gaining new ground. technology priorities for america's future. council on competitiveness,washington, d.c.coy, peter. 1993. "r&d scoreboard: in the labs, the fight to spend less, get more," business week, june 28, pp. 102124.deng, yuefan, james glimm, and david h. sharp. 1992. "perspectives on parallel computing," dædalus 121(1 winter):3152.economides, nicholas, and c. himmelberg. 1994. "critical mass and network size," presented at the telecommunications policy researchconference, solomons island, md., october 13.faltermayer, edmund. 1993. "invest or die," fortune 127(4):4252.federal coordinating council for science, engineering, and technology (fccset), office of science and technology policy. 1991. grandchallenges: highperformance computing and communications, the fy 1992 u.s. research and development program. committee on physical, mathematical, and engineering sciences, office of science and technology policy, washington, d.c.,february 5.federal coordinating council for science, engineering, and technology (fccset), office of science and technology policy. 1992. grandchallenges 1993: highperformance computing and communications, the fy 1993 u.s. research and development program. committee on physical, mathematical, and engineering sciences, office of science and technology policy, washington, d.c.federal coordinating council for science, engineering, and technology (fccset), office of science and technology policy. 1993. fccsetinitiatives in the fy 1994 budget. office of science and technology policy, washington, d.c., april 8.federal coordinating council for science, engineering, and technology (fccset), office of science and technology policy. 1994. highperformance computing and communications: toward a national information infrastructure. committee on physical,mathematical, and engineering sciences, office of science and technology policy, washington, d.c.feldman, jerome a., and william r. sutherland. 1979. "rejuvenating experimental computer science," communications of the acm, september, pp. 497502.flamm, kenneth. 1987. targeting the computer: government support and international competition. brookings institution, washington,d.c.flamm, kenneth. 1988. creating the computer: government, industry, and high technology. brookings institution, washington, d.c.forrester research inc. 1994. "home pcs: the golden age," the forrester report, october.furht, borko. 1994. "parallel computing: glory and collapse," computer 27(11):7475.geppert, linda. 1994. "industrial r&d: the new priorities," ieee spectrum 31(9):3041.bibliography69evolving the high performance computing and communications initiative to support the nation's information infrastructurecopyright national academy of sciences. all rights reserved.goldfarb, debra. 1994. workstations and highperformance systems. primary applications for highperformance computers. internationaldata corporation, framingham, mass.gore, jr., albert. 1993. from red tape to results, creating a government that works better & costs less: reengineering throughinformation technology, accompanying report of the national performance review. u.s. government printing office,washington, d.c., september.hamilton, alexander. 1791. "report [to the congress] on manufactures," in m.j. frisch (ed.), selected writings and speeches of alexander hamilton, pp. 277318. american enterprise institute for public policy research, washington, d.c., and london.hillis, w. daniel. 1992. "what is massively parallel computing, and why is it important?" dædalus 121(1 winter):115.holcomb, lee b. 1994. statement before the house committee on science, space, and technology: subcommittee on science, may 10.information infrastructure task force. 1993. the national information infrastructure. agenda for action. information infrastructure taskforce, washington, d.c., september 15.international business machines corporation. 1954. preliminary report: specifications for the ibm mathematical formula translatingsystem, fortran. programming research group, applied science division. international business machines, new york.international business machines corporation. 1956. the fortran automatic coding system for the ibm 704 edpm (programmer'sreference manual). international business machines, new york.kahaner, david k. 1994a. "fujitsu parallel computing workshop, 11/94, kawasaki," november 28, distributed electronically.kahaner, david. 1994b. "hitachi parallel computer announcement," april 14, distributed electronically.kahaner, david k. 1994c. "japanese supercomputer purchases," september 2, distributed electronically.kaufmann iii, william j., and larry l. smarr. 1993. supercomputing and the transformation of science. scientific american library, newyork.lewis, ted. 1994. "supercomputers ain't so super," computer 27(11):56.lindberg, donald a.b. 1994. statement before the house committee on science, space, and technology: subcommittee on science, may 10.markoff, john. 1995. "digital devices draw consumers," new york times, january 7, pp. 39 and 50.bibliography70evolving the high performance computing and communications initiative to support the nation's information infrastructurecopyright national academy of sciences. all rights reserved.national coordination office (nco) for highperformance computing and communications. 1993. technology transfer activities withinthe federal highperformance computing and communications program. nco hpcc 9301. national coordination office forhighperformance computing and communications, bethesda, md., april.national coordination office (nco) for highperformance computing and communications, office of science and technology policy. 1994.fy 1995 implementation plan. national coordination office for high performance computing and communications, bethesda, md.national science and technology council (nstc). 1994a. "program description." electronics subcommittee, civilian industrial technologycommittee, national science and technology council, washington, d.c.national science and technology council (nstc). 1994b. science in the national interest. national science and technology council,washington, d.c., august.national science foundation (nsf). 1993. research on digital libraries: announcement. nsf 93141. national science foundation,washington, d.c.national science foundation (nsf). 1994. research priorities in networking and communications. report to the nsf division of networkingand communications research and infrastructure by members of the workshop held may 1214, 1994, airlie house, va.nelson, david. 1994. statement before the house committee on science, space, and technology: subcommittee on science, may 10.new york times. 1994. "experimental fusion reactor at princeton sets a record," november 9, p. a22.nsf blue ribbon panel on highperformance computing. 1993. from desktop to teraflop: exploiting the us. lead in highperformancecomputing. national science foundation, arlington, va., august.office of science and technology policy. 1989. the federal highperformance computing program . executive office of the president,september 8.office of scientific research and development (osrd) and vannevar bush. 1945. science: the endless frontier. u.s. government printingoffice, washington, d.c.panetta, leon e. 1994. "memorandum for the heads of departments and agencies: fy 1996 research and development (r&d) priorities."office of management and budget, washington, d.c., may 12.parkersmith, norris (editor at large). 1994a. "hidemand, loprice dilemma for scalable vendors," hpcwire 3(45):article 1518 (distributedelectronically).parkersmith, norris (editor at large). 1994b. "nec joins parallel parade with wide range of cmos systems," hpcwire 3(45, november11):article 1513, distributed electronically.bibliography71evolving the high performance computing and communications initiative to support the nation's information infrastructurecopyright national academy of sciences. all rights reserved.patterson, david a., and john l. hennessy. 1994. computer organization and design. the hardware/software interface. morgan kaufmannpublishers, san mateo, calif.rensberger, boyce. 1994. "scientific ranks outpace funds: imbalance may put nation's technological primacy at risk." washington post, december 25, pp. al and a20.riggs, brian. 1994. "ibm, sun vie for thinking machines' mpp technology," computer 27(12):9.roach, stephen s. 1994. "economics inside the u.s. economy," u.s. investment research. morgan stanley, new york, july 15.rowell, jan. 1994. "ornl's ken kliewer on how to put the sizzle back in hpcc," hpcwire 3(47, december 2):article 9023, distributedelectronically.schatz, bruce r., and joseph b. hardin. 1994. "ncsa mosaic and the world wide web: global hypermedia protocols for the internet,"science 265(12 august):895901.schwartz, jacob t. 1992. "america's economictechnological agenda for the 1990s," dædalus 121(1, winter):139165.sikorovsky, elizabeth. 1994. "arpa issues $85m networking bonanza," federal computer week, april 25, p. 3.snell, monica. 1994. "supercomputing: same name, different game," computer 27(11):67.toole, john c. 1994. statement before the house committee on science, space, and technology: subcommittee on science, may 10.u.s. department of commerce (doc), international trade administration. 1994. u.s. industrial outlook, 1994. u.s. government printingoffice, washington, d.c.u.s. department of defense. 1960. cobol: initial specifications for a common business oriented language. u.s. government printingoffice, washington, d.c.u.s. department of defense. 1988. bolstering defense industrial competitiveness: report to the secretary of defense by the undersecretaryof defense (acquisition). department of defense, washington, d.c.u.s. general accounting office (gao). 1993. highperformance computing: advanced research projects agency should do more to fosterprogram goals. u.s. general accounting office, washington, d.c., may.u.s. general accounting office (gao). 1994. highperformance computing and communications: new program direction would benefitfrom a more focused effort. gao/aimd956. u.s. general accounting office, washington, d.c., november.vernon, mary k., edward d. lazowska, and stewart d. personick (eds.). 1994. r&d for the nii: technical challenges. report of a workshopheld february 28 and march 1, 1994, in gaithersburg, md. educom, washington, d.c.bibliography72evolving the high performance computing and communications initiative to support the nation's information infrastructurecopyright national academy of sciences. all rights reserved.vishkin, uzi (ed.). 1994. developing a computer science agenda for highperformance computing. acm press, new york.zatner, aaron. 1994. "sinking machines," boston globe, september 6, pp. 2125.bibliography73evolving the high performance computing and communications initiative to support the nation's information infrastructurecopyright national academy of sciences. all rights reserved.bibliography74evolving the high performance computing and communications initiative to support the nation's information infrastructurecopyright national academy of sciences. all rights reserved.appendixesappendixes75evolving the high performance computing and communications initiative to support the nation's information infrastructurecopyright national academy of sciences. all rights reserved.appendixes76evolving the high performance computing and communications initiative to support the nation's information infrastructurecopyright national academy of sciences. all rights reserved.athe high performance computing and communicationsinitiative: backgroundthe technicaleconomic imperative for parallel computingthe united states needs more powerful computers and communicationsthe highperformance computing and communications initiative (hpcci) addresses demandingapplications in many diverse segments of the nation's economy and society. in information technology,government has often had to solve larger problems earlier than other sections of society. government and the restof society, however, have mostly the same applications, and all find their current applications growing in size,complexity, and mission centrality. all sectors are alike in their demands for continual improvement in computerspeed, memory size, communications bandwidth, and largescale switching. as more power becomes increasinglyavailable and economical, new highvalue applications become feasible. in recent decades, for example,inexpensive computer power has enabled magnetic resonance imaging, hurricane prediction, and sophisticatedmaterials design. box a.1 lists additional selected examples of recent and potential applications of highperformance computing and communications technologies. (see also appendix d for a list of applications andactivities associated with the ''national challenges" and appendix e for an outline of supercomputingapplications.)box a.1 examples of important applications of highperformancecomputing and communications technologies continuous, online processing of millions of financial transactions understanding of human joint mechanics modeling of blood circulation in the human heart prediction and modeling of severe storms oil reservoir modeling design of aerospace vehicles linking of researchers and science classrooms digital libraries improved access to government informationa77evolving the high performance computing and communications initiative to support the nation's information infrastructurecopyright national academy of sciences. all rights reserved.conventional supercomputers face cost barriersfor four decades and over six computer generations, there has been a countable demand, much of it arisingfrom defense needs, for a few score to a few hundred supercomputers, machines built to be as fast as the state ofthe art would allow. these machines have cost from $5 million to $25 million each (in current dollars). the smallmarket size has always meant that a large part of the permachine cost has been development cost, tens tohundreds of millions of dollars. such products are peculiarly susceptible to costrise, marketdrop spirals.as supercomputers have become faster, they have become ever more difficult and costly to design, build, andmaintain. conventional supercomputers use exotic electronic components, many of which have few other uses.because of the limited supercomputer market, these components are manufactured in small quantities atcorrespondingly high cost. increasingly, this cost is capital cost for the special manufacturing processes required,and development cost for pushing the state of the component and circuit art.moreover, supercomputers' large central memories require high bandwidth and fast circuits. the speed andcomplexity of the processors and memories demand special wiring. supercomputers require expensive coolingsystems and consume large amounts of electrical power. thoughtful prediction shows that supercomputers facenonlinear cost increases for designing and developing entirely new circuits, chip processes, capital equipment,specialized software, and the machines themselves.at the same time, the end of the cold war has eliminated much of the historical market for speed at any cost.many observers believe we are at, or within one machine generation of, the end of the specializedtechnologysupercomputer line.small computers are becoming faster, cheaper, and more widely usedmeanwhile the opposite costvolume spiral is occurring in microcomputers. massproduction of integratedcircuits yields singlechip microprocessors of surprising power, particularly in comparison to their cost. theeconomics of the industry mean that it is less expensive to build more transistors than to build faster transistors.the pertransistor price of each of the millions of transistors in massproduced microprocessor chips is extremelylow, even though their switching speeds are now quite respectable in comparison to those of the very fastesttransistors, and a single chip will now hold a quite complex computer.while microprocessors do not have the memory bandwidth of supercomputers, the 300megaflopperformance of singlechip processors such as the mips 8000 is about onethird the 1gigaflop performance ofeach processor in the cray c90, a very fast supercomputer. microprocessor development projects costinghundreds of millions of dollars now produce computing chips with millions of transistors each, and these chips canbe sold for a few hundred dollars apiece.moreover, because of their greater numbers, software development for small machines proves much moreprofitable than for large machines. thus an enormous body of software is available for microprocessorbasedcomputers, whereas only limited software is available for supercomputers.a78evolving the high performance computing and communications initiative to support the nation's information infrastructurecopyright national academy of sciences. all rights reserved.parallel computers: high performance for radically lower costmassproduction economics for hardware and software argue insistently for assembling manymicrocomputers with their cheap memories into highperformance computers, as an alternative to developingspecialized highperformance technology. the idea dates from the 1960s, but the confluence of technical andeconomic forces for doing so has become much more powerful now than ever before.challenges of parallel computingorganizing a coherent simultaneous attack on a single problem by many minds has been a majormanagement challenge for centuries. organizing a coherent simultaneous attack on a single problem by a largenumber of processors is similarly difficult. this is the fundamental challenge of parallel computing. it has severalaspects.applicationsit is not evident that every application can be subdivided for a parallel attack. many believe there are classesof applications that are inherently sequential and can never be parallelized. for example, certain phases in thecompilation of a program are by nature sequential processes.many applications are naturally parallel. whenever one wants to solve a problem for a large number ofindependent input datasets, for example, these can be parceled out among processors very simply. such problemscan be termed intrinsically parallel.most applications lie somewhere in between. there are parts that are readily parallelized, and there are partsthat seem sequential. the challenge is how to accomplish as much parallelization as is inherently possible. asecond challenge of great importance is how to do this automatically when one starts with a sequential formulationof the problem solution, perhaps an already existing program.hardware designhow best to connect lots of microprocessors together with each other and with shared resources such asmemory and input/output has become a subject of considerable technical exploration and debate. early attempts torealize the potential performance of parallel processing revealed that too rigid a connection between machinesstifles their ability to work by forcing them into lockstep. too loose a connection makes communication betweenthem cumbersome and slow. the section below, "parallel architectures," sketches some of the design approachesthat have been pursued.numerical algorithmsduring the centuries of hand calculations, people worked one step at a time. ever since computers wereintroduced, the programs run on them have been mainly sequential, taking one small step at a time andaccomplishing their work rapidly because of the prodigious number of steps that they can take in a short time. thecurrent numerical algorithms for attacking problems area79evolving the high performance computing and communications initiative to support the nation's information infrastructurecopyright national academy of sciences. all rights reserved.mostly sequential. even when the mathematics of solution have allowed high degrees of parallel attack, sequentialmethods have generally been used. in fact, most languages used to express programs, such as fortran,cobol, and c, enforce sequential organizations on operations that are not inherently sequential.in the 30 years since parallel computers were conceived, computational scientists have been researchingparallel algorithms and rethinking numerical methods for parallel application. this work proceeded slowly,however, because there were few parallel machines from which to benefit if one did come up with a good parallelalgorithm, and few on which to develop and test such an algorithm. people didn't work on parallel algorithmsbecause they had no parallel machines to motivate the work; people didn't buy parallel machines because therewere few parallel algorithms to make them pay off. the hpcci and its predecessor initiatives broke this viciouscycle. by funding the development of machines for which little market was developed, and by providing them tocomputational scientists to use, the hpcci has vastly multiplied the research efforts on parallel computationalgorithms. nevertheless, 30 years of work on parallel approaches has not yet caught up with four centuries ofwork on sequential calculation.learning new modes of thoughtprogrammers have always been trained to think sequentially. thinking about numerous steps taken in parallelinstead of sequentially may initially seem unnatural. it often requires partitioning a problem in space as well intime. parallel programming requires new programming languages that can accept suitable statements of theprogrammer's intent as well as new patterns of thought not yet understood and formalized, much less routinelytaught to programmers.a new paradigmby responding to the technological imperative for parallel computing, the hpcci has in a major way helpedadd a new paradigm to computing's quiver. parallel computing is an additional paradigm, not a replacement forsequential and vector computing. large numbers of researchers have begun to understand the task of harnessingparallel machines and are debating the merits of different parallel architectures. because the parallel paradigm isnew, no one can yet say which particular approaches will prove most successful. it is clear however, that thishealthy debate and the workings of the market will identify and develop the best solutions.has the parallel computing paradigm really been established as the proper direction for highperformancecomputing? the committee to study highperformance computing and communications: status of a majorinitiative unanimously believes that it has. it is obliged to report, however, that the issue is still being hotly debatedin the technical literature. in the november 1994 special issue of the institute of electrical and electronicengineers computer magazine, borko furht asserts in "parallel computing: glory and collapse" that "the marketfor massively parallel computers has collapsed, but [academic] researchers are still in love with parallelcomputing." furht (1994) argues, ''we should stop developing parallel algorithms and languages. we should stopinventing interconnection networks for massively parallel computers. and we should stop teaching courses onadvanced parallel programming." an editorial by lewis (1994) in the same issue similarly discounts highlyparallel computing.part of the difference of opinion is semantics. computers have had a few processors working concurrently, atleast internal input/output processors, since the late 1950s. modern vector supercomputers have typically had fouror eight processors. the new paradigm concerns highly parallel computing, by which some mean hundreds ofprocessors. the committee believes that thea80evolving the high performance computing and communications initiative to support the nation's information infrastructurecopyright national academy of sciences. all rights reserved.number of processors in "parallel" computers in the field will grow normally from a few processors, to a fewdozen processors, and thence to hundreds. for the next several years, many computer systems will use moderateparallelism.the strongest evidence, and that which convinces the committee that the parallel computing paradigm is alongterm trend and not just a bubble, comes from the surging sales of thirdgeneration parallel processors such asthe sgi challenge, the sgi onyx, and the ibm sp2. sgi's director of marketing reports, for example, that sgihas sold more than 3,500 challenge and onyx machines since september 1993; ibm's wladawskyberger reportsthat 230 orders for the sp2 have been booked since it was announced in summer 1994 (parkersmith, 1994a). infairness to furht and lewis, these surging sales figures have appeared only in the last few months, whereasjournal lead times are long.computer architecturesoverviewfigure a.1 sequential computer organization.sequential, vectorthe simple sequential computer fetches and executes instructions from memory one after the other. eachinstruction performs a single operation, such as adding, multiplying, or storing one piece of data. decisions aremade by conditionally changing the sequence of instructions depending on the result of some comparison or otheroperation. every computer includes memory to store data and results, an instruction unit that fetches and interpretsthe instructions, and an arithmetic unit that performs the operations (see figure a.1).vector computers perform the same instruction on each element of a vector or a pair of vectors. a vector is aset of elements of the same type, such as the numbers in a column of a table. so a single "add" operation in avector computer can cause, for example, one column of 200 numbers to be added, element by element, to anothercolumn of 200 numbers. vector computers can be faster than sequential computers because they do not have tofetch as many instructions to process a given set of data items. moreover, because the same operation will be doneon each element, the flow of vector elements through the arithmetic unit can be pipelined and the operationsoverlapped on multiple arithmetic units to get higher performance.parallelparallel computers also have multiple arithmetic units, intended to operate at the same time, or in parallel,rather than in pipelined fashion. three basic configurations are distinguished according to how many instructionunits there are and according to how units communicate with each other. within each configuration, designs alsodiffer in the patterns, called topologies, fora81evolving the high performance computing and communications initiative to support the nation's information infrastructurecopyright national academy of sciences. all rights reserved.connecting the units to each other to share computational results. thus applications programmed for aparticular computer are not readily portable, even to other computers with the same basic configuration butdifferent topologies.single instruction multiple data. in a single instruction multiple data (simd) computer, one instruction unitgoverns the actions of many arithmetic units, each with its own memory. all the arithmetic units march in lockstep, obeying the one instruction unit but fetching different operands from their own memories (figure a.2).because of the lock step, if any node has to do extra work because of the particular or exceptional values of itsdata, all the nodes must wait until uniform operations can proceed. full utilization of all the processor powerdepends on highly uniform applications.figure a.2 a dataparallel computer organization.multiple instruction multiple data message passing. in a multiple instruction multiple data (mimd)messagepassing computer, each arithmetic unit has its own memory and its own instruction unit. so each node ofsuch a machine is a complete sequential computer, and each can operate independently. the multiple nodes areconnected by communication channels, which may be ordinary computer networks or which may be faster andmore efficient paths if all the nodes are in one cabinet. the several nodes coordinate their work on a commonproblem by passing messages back and forth to each other (figure a.3). this messagepassing takes time andinstructions. various topologies are used to accelerate message routing, which can get complex and take manycycles.there are two quite different forms of mimd computers, distinguished by the network interconnecting theprocessors. one, commonly called a massively parallel processor (mpp), has a collection of processor nodes colocated inside a common cabinet with very highperformance specialized interconnections.the other, often called a workstation farm, consists of a group of workstations connected by conventionallocal area or even wide area networks. such collections have demonstrated considerable success on largecomputing problems that require only modest internode traffic. between the two extremes of the mpp and theworkstation farm lie a number of parallel architectures now being explored. no one can say how this explorationwill come out.figure a.3 a messagepassing parallel computer organization.a82evolving the high performance computing and communications initiative to support the nation's information infrastructurecopyright national academy of sciences. all rights reserved.multiple instruction multiple data sharedmemory. in a multiple instruction multiple data (mimd)sharedmemory computer, the separate nodes share a large common memory. the several nodes coordinate workon a common problem by changing variables in the shared memory, which is a simple and fast operation(figure a.4).each node also has its own memory, generally organized as a cache that keeps local copies of things recentlyaccessed from the shared memory. the use of individual cache memories at each processor radically reducestraffic to the shared memory.figure a.4 a sharedmemory parallel computer organization.the shared memory may be a single physical memory unit, as in the sun sparccenter. this kind ofcomputer organization cannot be scaled indefinitely upwardšthe shared memory and its bus become a bottleneckat some point.a more scalable distributed memory design has a single shared memory address space, but the physicalmemory is distributed among the nodes. this arrangement exploits microprocessors' low memory cost and givesbetter performance for local access. many experts believe this will become the dominant organization formachines with more than a few processors.some distributedmemory machines, such as the convex exemplar, enforce cache coherence, so that allprocessors see the same memory values. others, such as the cray t3d, do not enforce coherence but use very fastspecial circuits to get very low sharedmemory latency. most machines with a shared physical memory maintaincache coherence.generations of parallel computersfirst commercial generation: simdparallel computers with small numbers of processors have been standard commercial fare for 30 years. insome cases, the multiple processors were in duplex, triplex, or quadruplex configurations for high availability; inmost advanced computers there have been processors dedicated to inputoutput. most vector computers have alsobeen modestly parallel for more than a decade. oneofakind highly parallel computers have been built now andthen since the 1960s, with limited success. the advanced research projects agency (arpa) recognized thetechnicaleconomic imperative to develop highly parallel computers for both military and civilian applications andacted boldly to create its highperformance computing program. this stimulus combined with a ferment of newideas and with entrepreneurial enthusiasm to encourage several manufacturers to market highly parallel machines,among them intel, ncube, thinking machines corporation (tmc), and maspar. most of these firstgenerationmachines were simd computers, exemplified by the cm1 (connection machine 1) developed by thinkingmachines.because simd execution lacks the information content of multiple instruction flows, applications have to bemore uniform to run efficiently on simd computers than on other types of parallel computers. compounding thisinherent difficulty, the firstgeneration machines had only primitive software tools. no application software wasavailable off the shelf, and existing codes could not be automatically ported, so that each application had to berebuilt from start. moreover,a83evolving the high performance computing and communications initiative to support the nation's information infrastructurecopyright national academy of sciences. all rights reserved.few of the firstgeneration machines used offtheshelf microprocessors with their economic and softwareadvantages.the first generation of highly parallel computers had some successes but proved to be of too limitedapplicability to succeed in the general market. some naturally parallel applications were reprogrammed for thesemachines, realizing gains in execution speed nearly in proportion to the number of processors applied to theproblem, up to tens of processors. the set of applications for which this was true was quite limited, however, andmost experts agree that the simd configuration has its units too tightly coupled to be used effectively in a widevariety of applications. nonetheless, the creation of this generation of machines, and their provision of a platformfor pioneering and experimental applications, clearly started a great deal of new thinking in academia about how touse such machines.second generation: messagepassing mimdstriving for the wider applicability that would be enabled by a more flexible programming style, parallelcomputer researchers and vendors developed mimd configurations made up of complete microprocessors(sometimes augmented by simd clusters). by and large, these machines used messagepassing for interprocessorcommunication. the thinking machines cm5 is a good example of this second generation. other examples useofftheshelf microprocessors as nodes.although improving somewhat in ease of use, such machines are still hard to program, and users still need tochange radically how they think and the type of algorithms they use. moreover, because these machines weredifferent both from conventional computers and from firstgeneration highly parallel computers, the compilers andoperating systems again had to be redone "from scratch" and were primitive when the machines were delivered.the secondgeneration machines have proven to be much more widely applicable, but primitive operatingsystems, the continuing lack of offtheshelf applications, and the difficulties of programming with elementarytools prevented widespread adoption by computerusing industries. as the market registered its displeasure withthese inadequacies, several of the vendors of first and secondgeneration parallel computers, including tmc andkendall square research, went into chapter 11 protection or retired from the parallel computerbuilding field. abeneficial side effect of these collapses has been the scattering of parallelprocessing talent to other vendors andusers.as parallel computers gained acceptance, existing vector computer vendors claimed their sales were beingharmed by the government promotion and subsidization of a technology that they saw as not yet ready to perform.cray research and convex, among others, saw their sales fall, partly due to performance/cost breakthroughs insmaller computers, partly due to the defense scaleback, and partly due to some customers switching from vectorto parallel computers. the complaints of the vector computer vendors triggered studies of the hpcci by thegeneral accounting office and the congressional budget office (see "concerns raised in recent studies"). crayresearch and convex have since become important vendors of parallel computers.third generation: memorysharing mimdin the third generation, major existing computer manufacturers independently decided that the sharedmemory organization, although limited in ultimate scalability, offered the most viable way to meet present marketneeds. among others, sgi, cray research, and convex have made such systems using offtheshelfmicroprocessors from mips, ibm, dec, and hewlettpackard, respectively. as noted above, market acceptancehas been encouragingšindustrial computer users have been buying the machines and putting them to work. manyusers start by using standarda84evolving the high performance computing and communications initiative to support the nation's information infrastructurecopyright national academy of sciences. all rights reserved.software and running the systems as uniprocessors on breadandbutter jobs, and then expand the utilization of themultiple processors gradually. as parallel algorithms, compilers, languages, and tools continue to develop, thesememoryshared machines are well positioned to capitalize on them.programmingthe development of parallel computing represents a fundamental change not only in the machinesthemselves, but also in the way they are programmed and used. to use fully the power of a parallel machine, aprogram must give the machine many independent operations to do simultaneously, and it must organize thecommunication among the processor nodes. developing techniques for writing such programs is difficult and isnow regarded by the committee as the central challenge of parallel computing.computer and computational scientists are now developing new theoretical concepts and underpinnings, newprogramming languages, new algorithms, and new insights into the application of parallel computing. while muchhas been done, much remains to be done: even after knowledge about parallel programming is better developed,many existing programs will need to be rewritten for the new systems.algorithmsthere is a commonly held belief that our ability to solve ever larger and more complex problems is dueprimarily to hardware improvements. however, a.g. fraser of at&t bell laboratories has observed that formany important problems the contributions to speedups made by algorithmic improvements exceed even thedramatic improvements due to hardware. as a longterm example, fraser cited the solution of poisson's equationin three dimensions on a 50 by 50 by 50 grid. this problem, which would have taken years to solve in 1950, willsoon be solved in a millisecond. fraser has pointed out that this speedup is owing to improvements in bothhardware and algorithms, with algorithms dominating.1during the mid1980s, several scientists independently developed tree codes or hierarchial nbody codes tosolve the equations of the gravitational forces for large multibody systems. for i million bodies, tree codes aretypically 1,000 times faster than classic directsum algorithms. more recently, some of these treecode algorithmshave been modified to run on highly parallel computers. for example, salmon and warren have achieved aspeedup of 445 times when running their codes on a computer with 512 processors as compared with running themon a single processor (kaufman and smarr, 1993, pp. 7374).over the halfcentury that modern computers have been available, vast improvements in problem solvinghave been achieved because of new algorithms and new computational models; a short list from among thenumerous examples includes:ement methods, transforms, carlo simulations, methods,for sparse problems, algorithms, sampling strategies, and analysis.a85evolving the high performance computing and communications initiative to support the nation's information infrastructurecopyright national academy of sciences. all rights reserved.the exponential increase in the sizes of economical main memories has also enabled a host of new tabledriven algorithmic techniques that were unthinkable a decade ago. discovering and developing new algorithms forsolving both generic and specific problems from science, engineering, and the financial services industry, designedand implemented on parallel architectures, will continue to be an important area for national investment.a sketch of the hpcci's historydevelopment and participantsto quote from the 1993 blue book: "the goal of the federal highperformance computing andcommunications initiative (hpcci) is to accelerate the development of future generations of highperformancecomputers and networks and the use of these resources in the federal government and throughout the americaneconomy" (fccset, 1992). this goal has grown, like the hpcci itself, from many roots and has continued toevolve as the initiative has matured. box a.2 illustrates the evolution of the hpcci's goals as presented by theblue book annual reports.box a.2 hpcci goals as stated in the blue booksfy 1992accelerate significantly the commercial availability and utilization of the next generation of highperformancecomputers and networks: extend u.s. technological leadership in highperformance computing and communications. widely disseminate and apply technologies to speed innovation and to serve the national economy, national security,education, and the global environment. spur productivity and competitiveness.fy 1993unchanged.fy 1994goals remained the same with addition of the information infrastructure and technology applications and programelement and mention of the national information infrastructure.fy 1995metagoal ("accelerate significantly . . . ") not mentioned. goals consolidated as: extend u.s. technological leadership in highperformance computing and communications; and widely disseminate and apply technologies to speed innovation and to improve national economic competitiveness,national security, education, health care (medicine), and the global environment.beginning in the early 1980s, several federal agencies advanced independent programs in highperformancecomputing and networking.2 the national science foundation (nsf) built on recommendations from the nationalscience board lax report in 1982,3 as well as a set of internal reports4 that recommended dramatic action to endthe 15year supercomputer famine in u.s.a86evolving the high performance computing and communications initiative to support the nation's information infrastructurecopyright national academy of sciences. all rights reserved.universities. nsf asked congress in 1984 for funds to set up, by a national competition, a number ofsupercomputer centers to provide academic researchers access to stateoftheart supercomputers, training, andconsulting services. very quickly this led to the creation of an nsf network backbone to connect the centers. thisin turn provided a highspeed backbone for the internet. several organizations, including the office ofmanagement and budget and the former federal coordinating council on science, engineering, and technology(fccset) of the office of science and technology policy (ostp), built on these activities and similar efforts inthe department of energy (doe),5 the national aeronautics and space administration (nasa), and thedepartment of defense (dod) to develop the concept of a national research and education network (nren)program (cstb, 1988). these explorations were linked to other concurrent efforts to support advanced scientificcomputing among researchers and to promote related computer and computational science talent development.the result was the highperformance computing program. the program included an emphasis oncommunications technology development and use from the outset.highperformance computing program structure and strategy were discussed intensively within severalfederal agencies in 19871988, resulting in initial formalization and publication of a program plan in 1989 (ostp,1989). ostp provided a vehicle for interagency coordination of highperformance computing andcommunications activities, acting through fccset and specific subgroups, including the committee on physical,mathematical, and engineering sciences; its subordinate panel on supercomputers; its highperformancecommittee (later subcommittee); its research committee (later subcommittee); and its highperformancecomputing, communications, and information technology (hpccit) subcommittee. the initial hpcci effortwas concentrated in four agencies: dod's advanced research projects agency, doe, nasa, and nsf. theseagencies remain the dominant supporters of computing and computational science research. although not then aformal member, the national security agency (nsa) has also always been an influential player in highperformance computing, due to its cryptography mission needs.highperformance computing activities received added impetus and more formal status when congresspassed the highperformance computing act of 1991 (pl 102194) authorizing a 5year program in highperformance computing and communications. this legislation affirmed the interagency character of the hpcci,assigning broad research and development (r&d) emphases to the 10 federal agencies that were then participatingin the program without precluding the future participation of other agencies. the group of involved agenciesexpanded to include the environmental protection agency, national library of medicine (part of the nationalinstitutes of health), national institute of standards and technology (part of the department of commerce(doc), and national oceanographic and atmospheric administration (part of doc) as described in the 1992 and1993 blue books. additional agencies involved subsequently include the education department, nsa, veteransadministration (now the department of veteran affairs), and agency for health care policy and research (partof the department of health and human services). these and other agencies have participated in hpccitmeetings and selected projects either as direct members or as observers.since its legislative inception in 1991, the hpcci has attained considerable visibility both within thecomputer research community and as an important element of the federal government's technology programs.when originally formulated, the hpcci was aimed at meeting several "grand challenges" such as modeling andforecasting severe weather events. it was subsequently broadened to address "national challenges" relating toseveral important sectors of the economy, such as manufacturing and health care, and then the improvement of thenation's information infrastructure. the evolution of emphasis on the grand and national challenges and also thenation's information infrastructure is outlined in box a.3.a87evolving the high performance computing and communications initiative to support the nation's information infrastructurecopyright national academy of sciences. all rights reserved.box a.3 from grand challenges to the national informationinfrastructure and national challenges: evolution of emphasis asdocumented in the blue booksfy 1992 grand challenges featured in title and discussed in textforecasting severe weather eventscancer gene researchpredicting new superconductorssimulating and visualizing air pollutionaerospace vehicle designenergy conservation and turbulent combustionmicroelectronics design and packagingearth biosphere research national challenges not discussedfy 1993 grand challenges featured in title and discussed in textmagnetic recording technologyrational drug designhighspeed civil transports (aircraft)catalysisfuel combustionocean modelingozone depletiondigital anatomyair pollutiondesign of protein structuresvenus imagingtechnology links to education national challenges not discussedfy 1994 national information infrastructure (nii) featured in titlemedical emergency and weather emergency discussed as examples of potential use of nii potential national challenge areas listed in information infrastructure technology and applications discussioncivil infrastructuredigital librarieseducation and lifelong learningenergy managementenvironmenthealth caremanufacturing processes and productsnational securitypublic access to government informationa88evolving the high performance computing and communications initiative to support the nation's information infrastructurecopyright national academy of sciences. all rights reserved. grand challenges discussed as case studies in textclimate modelingsharing remote instrumentsdesign and simulation of aerospace vehicleshighperformance life science: molecules to magnetic resonance imagingnonrenewable energy resource recoverygroundwater remediationimproving environmental decision makinggalaxy formationchaos research and applicationsvirtual reality technologyhighperformance computing and communications and educationguide to available mathematics softwareprocess simulation and modelingsemiconductor manufacturing for the 21st centuryfield programmable gate arrayshighperformance fortran and its environmentfy 1995 national information infrastructure featured in title and discussed in textinformation infrastructure servicessystems development and support environmentsintelligent interfaces national challenge areas discussed in textdigital librariescrisis and emergency managementeducation and lifelong learningelectronic commerceenergy managementenvironmental monitoring and waste minimizationhealth caremanufacturing processes and productspublic access to government information major section devoted to "highperformance living" with future scenario based on the national challenges and thenational information infrastructure grand challenges discussed in text. more than 30 grand challenges illustrated by examples within the following largerareas:aircraft designcomputer scienceenergyenvironmental monitoring and predictionmolecular biology and biomedical imagingproduct design and process optimizationspace sciencea89evolving the high performance computing and communications initiative to support the nation's information infrastructurecopyright national academy of sciences. all rights reserved.concerns raised in recent studiesa 1993 general accounting office (gao) study of arpa activities related to the hpcci and a 1993congressional budget office (cbo) study of hpcci efforts in massively parallel computing have been regardedby some as being critical of the entire hpcci. the committee, which received detailed briefings from the studies'authors, offers the following observations.gao reportthe gao report6 did not attempt to evaluate the entire hpcci but focused instead on research funding,computer prototype acquisition activities, and the balance between hardware and software investments by arpa.it recommended that arpa (1) broaden its computer placement program by including a wider range of computertypes, (2) establish and maintain a public database covering the agency hpcci program and the performancecharacteristics of the machines it funds, and (3) emphasize and provide increased support for highperformancesoftware. the report's authors stated to the committee that although recommending improvements, they had foundthat arpa had administered its program with propriety.the committee notes that progress has been made on each of gao's recommendations, and it has urged thatfurther progress be supported. committee recommendation 4 calls for further reduction in funding of computerdevelopment by vendors and for experimental placement of new machines. these actions should result in a widervariety of machine types as agencies select different machines to meet their mission needs. the nationalcoordination office (nco) has made more program information available and the committee recommends thatfunctions in this area receive added attention by an augmented nco (recommendation 11). likewise thecommittee has called in recommendation 3 for added emphasis on the development of software and algorithms forhighperformance computing.cbo reportthe primary theme of the cbo report (1993) was that because it was aimed primarily at massively parallelmachines, which currently occupy only a small part of the computer industry, the highperformance computingsystems component of the hpcci would have little impact on the computer industry. (the highperformancecommunications and networking segment of the program is not addressed in the cbo report.) the cbo reportassumed that the hpcci was to support the u.s. computer industry, in particular the parallelcomputing portion.although this might be an unstated objective, the explicitly stated goals relate instead to developing new highperformance computer architectures, technologies, and software. the hpcci appears to be fulfilling the statedgoals.the cbo report did not attempt to analyze the impact of the development of highperformance computingand communications technology on the larger computer industry over a longer period of time. the primary focusof the highperformance computing portion of the program is the creation of scalable parallel machines andsoftware. it is widely believed in both the research and industrial communities that parallelism is a key technologyfor providing longterm growth in computing performance, as discussed in the early sections of this appendix. thehpcci has demonstrated a number of successes in academia, in industry, and in government laboratories thatprovide a significant increase in our ability to build and use parallel machines. just as reduced instruction setcomputers (risc) technology, developed partly with arpa funding, eventually became a dominant force incomputing (some 10 years after the program started), the initiative'sa90evolving the high performance computing and communications initiative to support the nation's information infrastructurecopyright national academy of sciences. all rights reserved.ideas are starting to take root in a larger context across the computer industry. since using parallel processorsrequires more extensive software changes than did embracing risc concepts, we should expect thatmultiprocessor technology will take longer to be adopted.notes1. a.g. fraser, at&t bell laboratories, personal communication.2. department of energy (doe) officials point out that their efforts date from the mid1970s. for example, in 1974 doe established anationwide program providing energy researchers with access to supercomputers and involving a highperformance communicationsnetwork linking national laboratories, universities, and industrial sites, the precursor of today's energy sciences network (esnet). seenelson (1994).3. report of the panel on largescale computing in science and engineering, peter lax, chairman, sponsored by the u.s. department ofdefense and the national science foundation, in cooperation with the department of energy and the national aeronautics and spaceadministration, washington, d.c., december 26, 1982.4. a national computing environment for academic research. marcel bardon and kent curtis, nsf working group on computers forresearch. national science foundation, july 1983.5. the doe laboratories had been involved in supercomputing since world war ii and were not particularly affected by the setting up ofthe nsf centers or fccset until the late 1980s or early 1990s as the hpcci emerged.6. see gao (1993). another gao report (1994) was not released in time for the committee to receive a detailed briefing on which to basefurther group deliberations. however, observations from that report are drawn in the body of this report.a  91evolving the high performance computing and communications initiative to support the nation's information infrastructurecopyright national academy of sciences. all rights reserved.bhighperformance communications technology andinfrastructurehighperformance communications technology andinfrastructure advancethe performance/cost imperatives of communications have driven the technology from parallel to serial, fromhundreds of slower wires to a few very fast fibers. fiberoptic transmission offers stunning bandwidths: 100,000telephone calls or 800 video channels on one pair of fibers. communications is different from computing. valueoften comes from whom one can talk to, rather than how rapidly. issues of scaling are very important. the scale ofthe needed networking raises a host of new research issues as to how millions of users can attach to the network.highperformance computing and highperformance communications support each other in complex ways.communications has become digital, and the switching of fast digital signals requires highperformancecomputing technology. on the other hand, very fast computertocomputer communications are crucial for manyapplications. today 16 percent of investment in the highperformance computing and communications initiative(hpcci) is directed at communications. the communications content of the hpcci has two aspects: research anddevelopment to advance communications and related capabilities (see table b.1), and delivery of access tocommunicationsbased infrastructure to researchers to facilitate their work (see table b.2). the introduction of thefifth hpcci component, information infrastructure technology and applications (iita), in 1993 appears toextend the second aspect: awards and activities associated with this component appear to emphasize makingexisting capabilities more useful and more widely used, as opposed to developing new communicationsbasedcapabilities, which appears to be largely supported under the national research and education network (nren)component of the hpcci.the internet is the centerpiece of the present hpcci communications infrastructure program; it includes thenetwork elements (backbone, regional, and ''connections") supported under the nren program. thanks to federalsupport of internetworking technologies that have been applied in the internet generally and specifically innrensupported elements (nsfnet, esnet, and the nasa science internet), the united states has a strong leadin these technologies worldwide. the united states is home to a vital industry that supplies related equipment andsoftware, including businesses begun as spinoffs from academic research activity. see box b.1 for an example ofhow government, academic, and industrial investments can complement each other to accelerate the developmentof a key communications technology.b92evolving the high performance computing and communications initiative to support the nation's information infrastructurecopyright national academy of sciences. all rights reserved.table b.1 hpcci program activities in communications research, fy 1995componentaagencybfunding request for fy 1995 (millionsof dollars)activitynrenarpa43.10networkingiitaarpa23.00global grid communicationsbrhrnsf11.30very high speed networks and optical systemsnrennsa03.50very high speed networkingnrennsa02.60highspeed data protection electronicsnrendoe02.00gigabit research and developmentnrennist01.75metrology to support mobile and fixedbasecommunications networksa nren, national research and education network; iita, information infrastructure technology and applications; brhr, basicresearch and human resources.b arpa, advanced research projects agency; nsf, national science foundation; nsa, national security agency; doe, department ofenergy; nist, national institute of standards and technology.table b.2 hpcci program activities in communications infrastructure, fy 1995componentaagencybfunding request for fy 1995 (millions ofdollars)activitycnrennsf46.16nsfnetnrendoe14.80energy sciences network (esnet)nren12.70nrennrennoaa08.70networking connectivitynrennih06.50nlm medical connections programnrennist02.20nren deployment and performance measuresfor gigabit nets and massively parallelprocessor systemsiitanih02.00nci highspeed networking and distributedconferencingnrenepa00.70state network connectivitya nren, national research and education network; iita, information infrastructure technology and applications.b nsf, national science foundation; doe, department of energy; noaa, national oceanic and atmospheric administration; nih,national institutes of health; nist, national institute of standards and technology; epa, environmental protection agency.c nlm, national library of medicine; nci, national cancer institute.b93evolving the high performance computing and communications initiative to support the nation's information infrastructurecopyright national academy of sciences. all rights reserved.box b.1 federal government participation in the development ofasynchronous transfer modethe federal government, through the hpcci and other programs, played a part in the development of theasynchronous transfer mode, or atm, standard, and most importantly, played a very significant role in developing broadsupport for atm as an important switching technology for highspeed computer networks. atm was developed as aswitching technology by the telecommunications community for application in the socalled broadband integrated servicesdigital network, or bisdn. however, its development did not occur without some controversy.telecommunications switching had always been based on circuitswitching technologies, which are very well suited toproviding fixedrate connections in a network. multirate circuit switching can be used to provide connections that are anymultiple of some basic rate. narrowband isdn is based on these technologies, and most switching and transmissionexperts in the telecommunications industry expected a straightforward extension of isdn circuitswitching technology,based on multiples of a 64kbps basic rate, into the broadband realm of speeds in the 1 55mbps range.variable rate communications services were clearly needed for data communications, as demonstrated by thearpanet in the 1970s. however, it was also recognized that video and even voice services might be provided in a moreefficient manner than was possible through circuit switching if the basic network service could support variable ratecommunications. between 1984 and 1987 researchers in leading telecommunications and academic laboratoriesdeveloped a number of fastpacket switching, or fps, systems that laid the technological groundwork for the atmstandard. a noted academic researcher participating in this effort was jonathan turner, whose work was supported byboth nsf and industry funding. telecommunications industry laboratories such as at&t, bellcore, cnet (france), andbell telephone manufacturing (belgium) also played a significant role. bellcore, for example, built the first prototypebroadband central office in late 1986, and it contained a packet switch that operated at 50 mbps per line. by 1987, this ratehad been extended to nearly 1 55 mbps per line. these efforts, in essence, proved that packet switching was capable ofrunning at the data rates required by bisdn. at this time, arpanet and nsfnet were running at speeds of only 64kbps to 1.5 mbps, and local area networks such as ethernet ran at speeds of only 10 mbps.in late 1986, gordon bell, then at nsf, visited bellcore and received a briefing on fast cmosbased packet switchingtechnology that had been simulated at speeds of 150 mbps. several months later he called a workshop of computernetworking researchers to discuss the future of highspeed computer networking. out of that workshop, from david farber(university of pennsylvania) and bob kahn (corporation for national research initiatives), came a mid1987 proposal tonsf to form the gigabit testbed projects.the telecommunications industry was quite active during this time in developing a standard based on packetswitching. in 1988, the consultative committee on international telephony and telegraphy selected (and named) atm asthe standard for bisdn switching. this action served to focus the telecommunications industry on the development of thistechnology. however, in 1 988 atm was little more than a name, a basic packet format, and a common cause across thetelecommunications industry alone.in 1989, the gigabit testbed projects were formally initiated with (d)arpa and nsf funding. one of the objectives ofthe gigabit testbeds was to determine what switching technologies were appropriate for use in gigabit networking. therewere several major contenders: atm, hippi (a switching technology favored by the supercomputer community), and ptm(a proprietary packetswitching technology advocated by ibm, different from atm primarily in that it used variablesizedpackets). although it would be several years before conclusive results from the testbeds were produced, the use of atm inthe testbeds gave it wide exposure within the federal government's technology community.b94evolving the high performance computing and communications initiative to support the nation's information infrastructurecopyright national academy of sciences. all rights reserved.in 1990, the first company devoted solely to producting atm products was formed. the founders of fore systemswere first exposed to atm technology through their participation in the nectar gigabit testbed. crosstestbed exchangeswere encouraged by the gigabit testbed program, and in 1989 and 1990 there were meetings of the bellcore and mitresearchers working on atm for aurora and the carnegie mellon university researchers working on nectar. fore wasfounded in 1990 and subsisted through its first product development cycle largely on research and prototyping contractsfrom arpa and the naval research laboratory. fore's first products were delivered to the naval research laboratory in1991. fore today is a rapidly growing company with nearly 300 employees and one of the leading manufacturers of atmswitching equipment.in 1990, a collaboration was formed between apple, bellcore, sun microsystems, and xerox to attempt to gain theacceptance of atm as a new local area networking standard. this group, which eventually expanded to include digital andhewlettpackard, produced the first published specification of atm for local networking applications in april 1991. arpaparticipated in informal discussions with this group via its arpa networking principal investigator meetings, looking foropportunities to fund technical work that would be necessary to make atm useful for local area networking. one notableresult of arpa's funding was the first implementation of the q.2931 atm signaling standard, which was required to allowatm to implement switched (as opposed to permanent) virtual circuits. another result was the implementation, by xerox,of an atm switch architecture that is well suited for lowcost implementation. this work, funded in part by arpa, served toaccelerate the development of an industry standard for local atm by the atm forum.once atm had been established as both a computer and telecommunications networking standard, the role of thefederal government turned toward developing an early market for highspeed networking equipment and services. severalnotable programs greatly expanded this early market. one has been the gigabit testbeds. some of the first installations of2.4gbps sonet and atm by several telecommunications carriers were for these testbeds. although the federalgovernment did not spend a single dollar for these facilities, its leadership of the gigabit testbed program was critical ingetting telecommunications carriers to construct these very expensive facilities for the testbeds. telecommunicationsequipment manufacturers benefited directly and saw their development of very high speed (fullduplex 622mbps) atmequipment greatly accelerated.some direct federal procurements of atm and sonet services are still playing a key role in the development of themarketplace. an important procurement of atm and sonet services and technology was the washington area bitway(wabitway), the first significant sale of highspeed sonet services by bell atlantic. in early 1994, nsf announced theselection of a number of companies to provide atmbased communications services for the new nsfnet.telecommunications companies involved in this project include ameritech, mci, mfs datanet, pacific bell, and sprint.eight hpcci program activities are directed primarily at communications infrastructure, principallysupporting deployment of the internet within each agency's community. several of the infrastructure programs arebuilding on early results of the gigabit testbed research. for example, atm and sonet networking technologies,first deployed in the gigabit testbeds in 1992, appear in some form in many of the fy 1995 infrastructureactivities.developing and broadening access to information infrastructure pose many research issues. informationinfrastructure is more complex than networks, per se, and the computing and communications research communityhas already helped to explore and define fundamental concepts, for example, the concept of "middleware" to coverthe kind of internal services that help to transform a network into information infrastructure. research into how toimplement such services has begun under the hpcci umbrella. more specifically, the national aeronautics andspace administration, the national science foundation, and the advanced research projects agency havecombined to fund research to support the development of digital libraries, providing a vehicle for exploring manyconcepts associated with information infrastructure (nsf, 1993).b95evolving the high performance computing and communications initiative to support the nation's information infrastructurecopyright national academy of sciences. all rights reserved.the concept of a better nationwide information infrastructure, itself connected to a global informationinfrastructure, poses yet other concerns associated with interconnecting multiple kinds of networks from multiplekinds of providers to multiple kinds of users offering multiple kinds of services. this construct adds greatcomplexity, increasing the emphasis on scale and architecture and adding in such concerns as heterogeneity ofsystems, decentralization of control and management, routing, security, and so on. there have been manygovernment, academic, and industry studies under way to identify and clarify these research issues. although thenewspapers are filled with announcements of corporate alliances, new venture formations, and new productintroductions more or less linked to the advancement of the nation's information infrastructure, significantadvances call for the solution of many technical problems and therefore for a significant research effort.b96evolving the high performance computing and communications initiative to support the nation's information infrastructurecopyright national academy of sciences. all rights reserved.creview of the high performance computing andcommunications initiative budgetthe committee attempted a nontraditional look at how highperformance computing and communicationsinitiative (hpcci) funds are being invested.1 traditional hpcci budget reports show budget breakdowns byagency and by program component (highperformance computing systems, national research and educationnetwork, advanced software technology and algorithms, information infrastructure technology andapplications, and basic research and human resources). the committee found it informative to examine thefunding from a functional perspective to understand what sort of technical work is being performed and in whatquantity. the committee separated the 88 hpcci program elements into 11 disciplines defined as indicatedbelow:computer technology (cpt)šapplied research directed at advancing the state of computer architectureand hardware technology;software technology (swt)šapplied research directed at advancing the state of computer softwaretechnology;communications technology (cmt)šapplied research directed at advancing the state of communicationstechnology;omputing infrastructure (cpi)šacquisition and operation of supercomputer facilities;communications infrastructure (cmi)šacquisition and operation of highperformance computercommunications networks and services;applications and computational science (app)šcreation of software and computational techniquesdirected at solving specific scientific problems and applications;common applications support (cas)šcreation of software and computational techniques to support arange of applications across multiple disciplines;artificial intelligence and humanmachine interaction (ai)šapplied research directed at solving artificialintelligence and human interface problems.c97evolving the high performance computing and communications initiative to support the nation's information infrastructurecopyright national academy of sciences. all rights reserved.basic hardware technology (bhw)basic electronics research supporting electronic components thatmight be applied to a wide variety of systems, including computers and communications systems;education (edu)štraining and education; andadministration (adm)šnational coordination office.the committee classified 86 of the 88 program elements as coming under i of the 11 disciplines listed above,based on each program element's fy 1995 milestones (table c.1). if a program element appeared to fit into morethan one discipline, the committee categorized it by examining the element's milestones to determine where themajority of the program activity was concentrated. two of the larger advanced research projects agency(arpa) program activities (intelligent systems and software, and information sciences) were split between twodisciplines.table c.2 shows the fy 1993 actual budget, the fy 1995 request,2 and the percentage change in the hpccibudget for each of these 11 disciplines.budget reviewit is interesting to examine the hpcci budget to see which areas are being emphasized and to compare thesewith the hpcci's goals and objectives. as indicated also in chapter 2, the current program goals are as follows: in highperformance computing and networking technologies; disseminate the technologies to accelerate innovation and serve the economy, national security,education, and the environment; and gains in u.s. productivity and industrial competitiveness.the computer technology, software technology, and communications technology disciplines address the goalsof extending technical leadership in computing and communications and providing key enabling technologies forthe information infrastructure. the budget for these three disciplines accounted for 32.9 percent of the fy 1993actual budget and 30.5 percent of the fy 1995 requested budget.the largest part of the hpcci budget is invested in applications and supercomputer computing infrastructureto support applicationsš49.8 percent of the fy 1993 actual budget and 50.1 percent of the fy 1995 requestedbudget. applications and computational science, common applications support, artificial intelligence and humanmachine interaction, and computing infrastructure programs are included. the rest of the budget requested for fy1995 is divided among basic hardware technology (5.1 percent), communications infrastructure (7.9 percent),education (5.6 percent), and a very small amount for program administration.c98evolving the high performance computing and communications initiative to support the nation's information infrastructurecopyright national academy of sciences. all rights reserved.table c.1 mapping of agencies, hpcci budget allocations, program elements, and discipline categoriesagencya1995requested($m)1993actual($m)disciplinebprogram elementcarpa36.1511.35alintelligent systems and softwarearpa36.1511.35swtarpa60.2044.90cptscalable computing systemsarpa44.5033.50bhwmicrosystemsarpa43.1034.80cmtnetworkingarpa33.9038.00swtnationalscale information enterprisesarpa29.6036.50swtscalable softwarearpa23.0000.00cmtglobal grid communicationsarpa10.5015.10alinformation sciencesarpa10.5015.10swtarpa14.0013.90edufoundationsarpa09.8000.00apphealth information infrastructurearpa06.0000.00bhwintegrated command and control technologynsf76.4363.89cpisupercomputer centersnsf46.1630.10cminsfnetnsf35.2500.00casinformation infrastructure technology and applications programnsf25.3521.79swtsoftware systems and algorithmsnsf20.9518.65cpiresearch infrastructurensf20.7018.76cptcomputing systems and componentsnsf20.2415.34edueducation and trainingnsf11.5007.80appbiological sciences (nonnc/gc)nsf11.3009.80cmtvery high speed networks and optical systemsnsf11.0010.40alhuman machine interaction and information accessnsf10.7507.00casgrand challenge applications groupsnsf10.5509.20casresearch centersnsf09.7702.75appphysical sciences (nonnc/gc)nsf07.5905.72cascomputational mathematics (nonnc/gc)c99evolving the high performance computing and communications initiative to support the nation's information infrastructurecopyright national academy of sciences. all rights reserved.agencya1995requested($m)1993actual($m)disciplinebprogram elementscnsf04.2302.17appengineering (nonnc/gc)nsf03.8401.15cpigeosciences (nonnc/gc)nsf03.0100.65appsocial, behavioral, and economic sciences (nonnc/gc)doe35.6035.13cpisupercomputer accessdoe16.001 5.25casbasic research for applied mathematics researchdoe14.8007.68cmienergy sciences network (esnet)doe12.9007.15cpihighperformance computing research centersdoe12.6008.98cassoftware components and toolsdoe09.9007.73cpievaluation of early systemsdoe09.0006.53appenabling energy grand challengesdoe03.4002.44cascomputational techniquesdoe03.0002.36edueducation, training, and curriculumdoe02.0002.60eduresearch participation and trainingdoe02.0001.86cmtgigabit research and developmentdoe02.0001.80apphighperformance research centersglobal climate collaborationdoe01.2000.00casinformation infraservicesdoe01.0000.00cpiadvanced prototype systemsnasa55.3046.80appgrand challenge supportnasa26.4017.60cpitestbedsnasa12.7008.50cminational research and education network (nren)nasa10.7000.00eduinformation infrastructure applicationsnasa09.2005.40swtsystems softwarenasa06.8000.00casinformation infrastructure technologynasa03.8003.30edubasic research and human resourcesnih11.0008.00cpidcrt highperformance biomedical computing programc100evolving the high performance computing and communications initiative to support the nation's information infrastructurecopyright national academy of sciences. all rights reserved.agencya1995request($m)1993actual ($m)disciplinebprogramcnih11.0001.50appnational library of medicine highperformance computing andcommunications health care applicationsnih08.8003.40appncrr information infrastructure technology applicationsnih08.8006.80appncrr advanced software technology and algorithmsnih06.7006.30cpinci frederick biomedical supercomputing centernih06.5000.40cminlm medical connections programnih05.4003.80casnlm iaims grantsnih05.0003.10eduncrr basic research and human resourcesnih04.8004.10appnlm biotechnology informaticsnih04.7005.00casnlm intelligent agent database searchingnih03.6002.90edunlm hpcci training grantsnih02.2001.50appnlm electronic imagingnih02.0000.00cminci highspeed networking and distributed conferencingnih00.7000.40admnational coordination officenih00.6000.00apphighperformance communications for pdq, cancer net, andelectronic publishingnsa26.1000.00cptsupercomputing researchnsa05.7000.00swtsecure operating system developmentnsa03.5000.00cmtvery high speed networkingnsa02.6000.00cmthighspeed data protection electronicsnsa02.0000.00bhwsuperconducting researchnsa00.2300.00edutechnologybased trainingnist25.2000.00cassystems integral for manufacturing applicationsnist07.6000.60appdevelopment and dissemination of scientific software for highperformance computing systemsc101evolving the high performance computing and communications initiative to support the nation's information infrastructurecopyright national academy of sciences. all rights reserved.agencya1995requested($m)1993actual($m)disciplinebprogram elementcnist06.4500.00bhwmetrology for future generations of microelectronicsnist04.0000.00caslanguage, image, and text processingnist04.0000.00swtspecification and testing of highintegrity, distributed systemsnist02.7500.00cassupport for electronic commercenist02.2001.50cmtdeployment and performance measures for gigabit nets and massivelyparallel processor systemsnist01.7500.00cmtmetrology to support mobile and fixedbase communicationsnetworksnist01.2500.00caselectronic libraries and distributed multimedia applicationsnist01.2000.00swtassurance, reliability, and integrity of nren objectsnoaa16.0509.40appadvanced computationnoaa08.7000.40cminetworking connectivitynoaa00.5000.00appinformation dissemination pilotsepa06.4505.33appenvironmental modelingepa05.2501.31appcomputational techniquesepa01.9701.16edueducation and trainingepa00.7000.21cmistate network connectivityepa00.3000.00apppublic data accessa arpa, advanced research projects agency; nsf, national science foundation; doe, department of energy; nasa, nationalaeronautics and space administration; nih, national institutes of health; nsa, national security agency; nist, national institute ofstandards and technology; noaa, national oceanographic and atmospheric administration; epa, environmental protection agency.b al, artificial intelligence and humanmachine interaction; swt, software technology; cpt, computer technology; bhw, basic hardwaretechnology; cmt, communications technology; edu, education and training; app, applications and computational science; cas,common applications support; cpi, computing infrastructure; cmi, communications infrastructure; adm, national coordination office.c nc/gc, national challenge/grand challenge; dcrt, division of computer research and technology (nih); iaims, integratedacademic information management system; nlm, national library of medicine; ncrr, national center for research resources(nih); nci, national cancer institute (nih); pdq, physician data query (nih).source: data on agency budgets and program activities were extracted from the fy 1995 implementation plan prepared by the nationalcoordination office (1994).c102evolving the high performance computing and communications initiative to support the nation's information infrastructurecopyright national academy of sciences. all rights reserved.table c.2 actual fy 1 993 and requested fy 1995 hpcci budget (millions of dollars) categorized by disciplinediscipline19931995percentage changecomputer technology63.66107.0068software technology128.14155.6021communications technology47.9689.4586computing infrastructure165.60204.7223communications infrastructure47.2991.5694applications and computational science102.44176.9673common applications support57.39147.44157artificial intelligence and humanmachine interaction36.8557.6556basic hardware technology33.5058.9576education44.6664.5445administration0.400.7075total program727.891154.5659alternatively, the 11 discipline categories can be used to examine the balance between support fordisciplinespecific scientific research that uses highperformance computing and communications technologies andsupport for computer science research on new highperformance computing and communications technologies.analysis of the fy 1995 hpcci budget request shows that $352 million (30 percent) would be invested in basicresearch in computer, software, and communications technologies; $205 million (18 percent) in applied computerscience research, artificial intelligence, and humanmachine interaction; $176 million (15 percent) in directsupport of applications and computational science; and $297 million (26 percent) in computing andcommunications infrastructure.commentary: many possibilities for misinterpretationthe hpcci has enjoyed a certain amount of political support and is growing even in a time of very tightfederal budgets. the committee believes that this has created a ''bandwagon" effect: the initiative has had its scopeextended by the inclusion of some work not directly related to the hpcci's goals, however valuable it may be, orwork with broad relevance. the result has been a less than focused program.for example, all highperformance computing and communications systems are built from electronics anddepend directly on advancements in basic electronic technology. the arpa microsystems program activity,which constitutes the large majority of the basic hardware discipline, supports research in basic electronicstechnologies. this research will eventually benefitc103evolving the high performance computing and communications initiative to support the nation's information infrastructurecopyright national academy of sciences. all rights reserved.the highperformance computing and communications technology base and help advance the nation's informationinfrastructure, but it could also be used in a wide variety of other contexts.another problem is the possible creation of false expectations about the extent to which the hpcci couldcreate the technology necessary for advancing the nation's information infrastructure. a large amount of workwithin the two applications disciplines is directed primarily toward the use of highperformance computing insolving certain scientific and agency mission problems. only a part of this work, such as the creation of digitallibraries, would apply directly to the goal of enhancing the nation's information infrastructure. some of this workis directed at challenging computational science problems, which have excellent scientific impact but whoseresults are more easily justified as scientific results, rather than hpcci results. also, the hpcci invests muchmore heavily in computing than in communications. less than 16 percent of the fy 1995 request is forcommunications technology and infrastructure.about onethird of the program is directed toward creating new technology directly applicable to advancingthe information infrastructure. the growth in funding in these areas is offset by an unrelated decrease in researchinvestment by industry, spurred in part by competitive changes in the computer and communications industries.as a result, the nation's total amount of research in highperformance computing and communicationstechnologies is considerably less than it appears, and in fact may be insufficient to maintain the strategic u.s. leadin these technologies or to support the rapid deployment of an enhanced information infrastructure.notes1. cpsma (1994), p. 7; this report points out that laborintensive, detailed disaggregation of published data may be the only way tounderstand how research program budgets are spent.2. amounts shown for fy 1995 are executive budget requests. at press time, agency appropriations had been made, but the involvedagencies had not disaggregated the appropriations and reported the hpcci portions to the national coordination office. a 2year timeperiod, fy 1993 to fy 1995, was used to help dampen any singleyear jumps in level.c 104evolving the high performance computing and communications initiative to support the nation's information infrastructurecopyright national academy of sciences. all rights reserved.dcurrent high performance computing and communicationsinitiative grand challenge activitiessince its beginning, the highperformance computing and communications initiative has included grandchallenges, difficult scientific problems whose solution will yield new scientific understanding whilesimultaneously advancing highperformance computing and communications. the following list of current grandchallenge activities is based on the fy 1994 and fy 1995 "blue books" and communications with nationalcoordination office staff.national science foundationaerospace field problemscomputer science learning input/output (i/o) methods for i/ointensive grand challengesenvironmental modeling and prediction environmental modeling coordination of results of predictive models with experimental observations ground motion modeling in large basinshperformance computing for land cover dynamics parallel simulation of largescale, highresolution ecosystem modelsmolecular biology and biomedical imaging designaging in biological researchcomputational approaches to biomolecular modeling and structure determination human joint mechanics through advanced computational modelsproduct design and process optimizationhcapacity atomiclevel simulations for the design of materialsspace science hole binaries: coalescence and gravitational radiation galaxies and largescale structureynthesis imagingd105evolving the high performance computing and communications initiative to support the nation's information infrastructurecopyright national academy of sciences. all rights reserved.department of energyenergy modelingantum chromodynamics calculations reservoir modelingtokamak projectenvironmental monitoring and prediction chemistry (see box d.1 for discussion) global climate modeling transport and remediationmolecular biology and biomedical imaging structural biologyproduct design and process optimization simulation of materials propertiesnational aeronautics and space administration structure and galaxy formationosmology and accretion astrophysics turbulence and mixing in astrophysics solar activity and heliospheric dynamicsnational institutes of health biologyagingnational institute of standards and technology and process optimizationenvironmental protection administration and waterquality modelingnational oceanic and atmospheric administrationchange prediction and weather forecastingd106evolving the high performance computing and communications initiative to support the nation's information infrastructurecopyright national academy of sciences. all rights reserved.box d.1 computational chemistry: applying highperformance computing toscientific problemschemists were among the earliest researchers to pursue the use of computers to extend understanding of their field.at the time of a 1956 texas conference on quantum chemistry, electronic computers had developed to the stage that itwas just feasible to program large scientific computations. however, these computers provided results too crude to be ofinterest to quantum chemists, even by the late 1960s. nonetheless, based on this "progress," the conference passed arecommendation urging that more machines be at the disposal of university departments. several groups at the universityof chicago, the massachusetts institute of technology, and elsewhere pursued the goal of exploiting these new facilities,engaging in huge (for the time) research programs to compute the electronic wave functions for molecules constituted oftwo atoms from the first full row of the periodic chart.by the early 1970s, significant progress had been made in the computation of molecular energies, wave functions,and dynamics of reacting systems and liquid structure by highspeed computers of the day, namely, the i bm 360/65, cdc6600, and univac 1108. important work was also being done using semiempirical methods for systems with as many as 10to 12 atoms. reliable methods had been applied to the calculation of potential energy surfaces for h + h2 and f + h2systemsšmethods that have been essential in the advancement of understanding molecular collisions. there had beensemiquantitative calculations of hydrogen bond strengths and protein conformation, but the facilities to carry out suchwork were available to only a small group of chemists, mostly on the staffs of national laboratories. the need to extendaccess to such facilities coupled with the new goal of bringing together people to work on software development and toattack important problems in chemistry led to the creation of the national resource for computation in chemistry (nrcc)funded jointly by the national science foundation (nsf) and department of energy.with the creation of the nsf supercomputer centers in the 1980s, chemists were able to pursue computationalstudies with requirements well beyond the capability of systems available otherwise even to leading research groups of theperiod. in addition to highspeed computation, the centers made accessible large amounts of disk storage and fosteredlargescale use of highspeed data communication.a major breakthrough of the early 1980s was the recognition by industry of the value of computational chemistry to themarketplace. companies set up research groups that used computational chemistry software for electronic structurestudies (e.g., gaussian and cadpac) and molecular mechanics simulations (e.g., amber and charmm), coupled withgraphics platforms.by the mid1980s an industry had developed in modeling software focused on the chemical, pharmaceutical, andbiotechnology industries. large companies, such as eli lilly and dupont, bought supercomputers to provide the capabilityto model complex molecules and processes important for their businesses.one of the major directions for future work is the application of accurate quantum mechanical approaches tobiological systems. this effort would complement the molecular mechanics method with selected calculations of higheraccuracy to enable explication of important fine points. areas where these efforts might be introduced are enzymaticreactions involving transitionmetal centers and an array of catalytic processes.the additional power provided by massively parallel computer systems is stimulating a push for higher accuracy andimproved algorithms. methods that have had impact for serial processors that were readily modified for vector systemsoften must undergo major modification or replacement for massively parallel processors. a major requirement foradvancement is seamless scalability across systems of different size.with the need for higher accuracy on massively parallel systems will likely come increased attention to monte carlomethods for quantum manybody systems. quantum simulations are naturally parallel and are expected to be usedincreasingly on massively parallel computer systems.d107evolving the high performance computing and communications initiative to support the nation's information infrastructurecopyright national academy of sciences. all rights reserved.eaccomplishments of national science foundationsupercomputer centersintroductionthe national science foundation (nsf) supercomputer centers program preceded the highperformancecomputing and communications initiative but has become an integral and important part of it. the centers wereestablished to provide access to highperformance computingšsupercomputers and related resourcesšfor thebroad science and engineering research community. the program has evolved from one comprising independent,competitive, and duplicative computer centers to a cooperative activity, one that has been characterized as ametacenter.in 1992 the four nsf supercomputer centers (cornell theory center, national center for supercomputingapplications, pittsburgh supercomputer center, and san diego supercomputer center) formed a collaborationbased on the concept of a national metacenter for computational science and engineering: a collection ofintellectual and physical resources unlimited by geographical or institutional constraints. the centers' first missionwas to provide a stable source of computer cycles for a large community of scientists and engineers. the primaryobjective was to help researchers throughout the country make effective use of the architecture or combination ofarchitectures best suited to their work. another objective was to educate and train students and researchers fromacademia and industry to use and test the limits of supercomputing in solving complex research problems. thebest and most adventurous proposals for using an expensive and limited resource were sought.in 1994, the scientific computing division of the national center for atmospheric research joined themetacenter. in addition, the nsf established the metacenter regional affiliates program, under which otherinstitutions could pursue projects of interest in collaboration with metacenter institutions. the metacenter thusbecame a unique resource and a laboratory for computer scientists and computational scientists working togetheron shared tasks.important technology accomplishmentsoriginally set up in 1985 to provide national access to traditional supercomputers, the nsf centers haveevolved to a much larger mission. the centers now offer a wide variety of highperformance architectures from alarge array of u.s. vendors. today work at the centers is dominated by research efforts in software, incollaboration with computer scientists, focusing on operating systems, compilers, network control, mathematicallibraries, and programming languages and environments.e108evolving the high performance computing and communications initiative to support the nation's information infrastructurecopyright national academy of sciences. all rights reserved.supercomputer usage at nsf centersfigure e.1 total historical usage of all highperformance computers in the nsf metacenter. this graph shows thetotal annual usage of all highperformance computers in metacenter facilities. particularly striking is the growthsince 1 992, when microprocessors in various parallel configurations began to be employed. all usage has beenconverted to equivalent processoryears for a cray research ymp, the type of supercomputer that the nsf centersfirst installed in 19851986.table e.1 shows the growth in the number of users and in the availability of cycles at the nsf supercomputercenters from 1986 to 1994. see also figure e.1. the increase in capacity in 1993 was owing mainly to theintroduction of new computing architectures. the slight decrease in the number of users reflects the centers' effortto encourage users able to meet their computational needs with the increasingly powerful workstations of themid1990s to use their own institutional resources.table e.1 supercomputer usage at national science foundation supercomputer centers, 1986 to 1994fiscal yearactive usersusage in normalized cpu hoursa19861,35829,48519873,32695,75219885,069121,61519895,975165,95019907,364250,62819917,887361,03719928,578398,93219937,730910,08819947,4312,249,562a data prior to may 1990 include the john von neumann center.e109evolving the high performance computing and communications initiative to support the nation's information infrastructurecopyright national academy of sciences. all rights reserved.architectures and vendorsthe national research community has been offered access to a wide and continually updated set of highperformance architectures since the beginning of the nsf supercomputer centers program in 1985. the types ofarchitectures and number of vendors are now probably near an alltime high (table e.2), allowing the science andengineering communities maximum choice in selecting a machine that matches their computational needs. a shortlist of architectures offered through the nsf centers program includes single and clustered highperformanceworkstations or workstation multiprocessors, minicomputers, graphics supercomputers, mainframes with orwithout attached processors or vector units, vector supercomputers, and single instruction multiple data (simd)and multiple instruction multiple data (mimd) massively parallel processors.e110evolving the high performance computing and communications initiative to support the nation's information infrastructurecopyright national academy of sciences. all rights reserved.e111evolving the high performance computing and communications initiative to support the nation's information infrastructurecopyright national academy of sciences. all rights reserved.current vendors whose top machines have been made available include ibm, dec, hewlettpackard, silicongraphics inc., sun microsystems, cray research, convex computer, intel supercomputer, thinking machinescorporation, and ncube, plus a number of companies no longer in existence, such as alliant, floating pointsystems, eta, kendall square research, stellar, ardent, and stardent.access and new architecturesin the 1960s, only a few universities had access to stateoftheart supercomputers. by the early 1990s, some15,000 researchers in over 200 universities had used one or more of the supercomputers in the nsf metacenter.this increased use led to new concepts and innovation:achieving production parallelism. cornell theory center became the first member center to achieveproduction parallelism on a vector supercomputer.migration to the unix operating system. in 1987, the national center for supercomputing applications(ncsa) became the first major supercomputer center to migrate its cray supercomputer from ctss tounicos, a unixbased operating system developed at cray research for its supercomputers.access to massively parallel computers. ncsa introduced massively parallel computing (mpp) to theresearch community with the cm2 in 1989, followed by the cm5 in 1991. ncsa has worked closelywith national users and the computer science community to create a wide range of 512way parallelapplication codes that can in 1995 be moved to other large mpp architectures such as the t3d at psc, theintel paragon at sdsc, or the ibm sp2 at ctc.heterogeneous processors. in 1991, the pittsburgh supercomputer center was the first site to distributecode between a massively parallel machine (tmccm2) and a vector supercomputer (cray ymp),linked by a highspeed channel (hippi).workstation clusters. the nsf supercomputer centers were among the first to experiment with clusters ofworkstations as an alternative for scalable computing. the first work was done in the 1980s with looselycoupled clusters of silicon graphics inc. workstations to create frames for scientific visualizations. withthe introduction of the ibm rs6000, several centers moved to study tightly coupled networks anddeveloped job control software. clusters from dec, hewlettpackard, and sun microsystems are nowavailable as well.storage technologies, file format, and file systemswith the vast increase in both simulation and observational data, the metacenter has worked a great deal onproblems of storage technologies, with the greatest progress in software. the creation of a universal file formatstandard, a national file system with a single name space, and multivendor archiving software are some of theresults of metacenter innovation and collaboration.e112evolving the high performance computing and communications initiative to support the nation's information infrastructurecopyright national academy of sciences. all rights reserved.nsfnet and networkingthe 56kbps connection between the nsf supercomputer centers, established in 1986, was the beginning ofthe nsfnet. based on the successful arpanet and the tcp/ip protocol, the nsfnet rapidly grew to provideremote access to the nsf supercomputer centers by the creation of regional and campus connections to thebackbone.although started by the pull from the high end, the nsfnet soon began to provide ubiquitous connectivityto the academic research community for electronic mail, file transport, and remote login, as well assupercomputer connectivity. as a result, the nsfnet backbone of 1995 has 3,000 times the bandwidth of thebackbone of 1986. the centers have also developed prototypes for the highperformance local area networks thatare needed to feed into the national backbone as well as the next generation of gigabit backbones.visualization and virtual realitythe nsf centers were instrumental in bringing the concepts and tools of scientific visualization to theresearch community in the 1980s. center members developed new approaches to understanding large datasets,such as a threedimensional grid of wind velocities and direction in a thunderstorm, by ''visualizing" or creating animage from the data. this led scientists to consider visualization as an integral part of their computational tool kit.in addition, the centers worked closely with the preexisting computer graphics community, encouraging them tocreate new tools for scientists as well as for entertainment.desktop software, connectivity, and collaboration toolsthe history of the centers has overlapped greatly with the worldwide rise of the personal computer andworkstation. it is, therefore, not surprising that software developers focused on creating easytouse software toolsfor desktop machines. these tools have had a major influence on the usefulness of supercomputer facilities toremotely located scientists and engineers, as have tools such as ncsa's telnet, which brought full tcpconnectivity to researchers using ibm and macintosh systems, significantly broadening the base of participationbeyond unix users. collaboration tools have provided the capability to carry on remote digital conferencingsessions between researchers. both synchronous and asynchronous approaches have been explored.development of the nation's information infrastructure requires many software, computing, andcommunications resources that were not traditionally thought to be part of highperformance computing. inparticular, tools need to be developed for organizing, locating, and navigating through information, a task that thensf center staffs and their associated universities continue to address. perhaps the most spectacular success hasbeen the ncsa mosaic, which in less than 18 months has become the internet "browser of choice" for over amillion users and has set off an exponential growth in the number of decentralized information providers. monthlydownload rates from the ncsa site alone are consistently over 70,000.accomplishments in education and outreacheach of the supercomputer centers has developed educational and outreach programs targeted to a variety ofconstituencies: university researchers, graduate students, undergraduates,e113evolving the high performance computing and communications initiative to support the nation's information infrastructurecopyright national academy of sciences. all rights reserved.educators at all levels, and k12 students and teachers. another aspect of outreach is the effort to identify andserve local and regional needs of government, schools, and communities. activities range from the tours given atall metacenter installations through the hosting of visits by national, regional, and local officials andcommissions, to fullscale partnerships. table e.3 summarizes participation in these various activities.table e.3 supercomputer centers' educational activity support summaryeducational activitiesfy 1991fy 1992fy 1993high school/k 12šattendees7151,3701,985research institutesšattendees262377390training courses and workshopsšattendees1,7002,4002,100monthly newsletter circulation234,986247,692165,176visitors13,50616,38016,392researchers and studentsone or twoday workshops offered by metacenter staff to researchers onsite and at associated institutionscover introductions to the computational environments, scientific visualization, and the optimization andparallelization of scientific code. in addition, special workshops have been offered throughout the metacenter onthe use and extension of computational and visualization techniques specific to various disciplines.metacenter institutions have contributed to the research projects of hundreds of graduate students through theprovision of fellowships or similar appointments, stipends, access to resources, and relationships with metacenterresearchers. programs providing research experiences for undergraduates bring in students to work for a summeror a school semester or quarter on specific projects devised by metacenter researchers and/or faculty advisors. inmany instances such projects have resulted in presentations at meetings and publications.k12 educators and studentstraining of high school teachers and curriculum development are among the many metacenter educationalefforts. several programs have been initiated, such as chemviz to help students understand abstract chemistryconcepts; a visualization workshop at supercomputing '93; and superquest, a program involving metacenter sitesthat brings teams of teachers and students from selected high schools to summer institutes to developcomputational and visualization projects that they then work on throughout the following year.broad outreachoutreach is also accomplished by the publications programs of the metacenter, the production of scientificvideos and/or multimedia cdroms, and a collaborative program fore114evolving the high performance computing and communications initiative to support the nation's information infrastructurecopyright national academy of sciences. all rights reserved.maintaining a lively and informative presence on world wide web servers, which make information on themetacenter's programs easily accessible over the nation's information infrastructure.a number of interactive simulation programs are now being tested in classrooms across the country andaround the world. students can change initial conditions and watch a simulation evolve as the parameter space isexplored. the educational programs of the metacenter made available to high schools around the countrydemonstrate the power of the nation's information infrastructure to provide new educational resources.scientific computation and industrial developmentpartnerships between the metacenter and industry are collaborations with major and large industrial firms, aswell as small companies and venture startups. most of these partnerships exist because metacenter expertise hasbeen essential to the introduction of new ways of using the resources of supercomputing: the algorithms,visualization routines, and engineering codes are being combined in ways that result in such advances as highendrapid prototyping of new products.commercialization of the software developed at the metacenter is being undertaken by a number ofcompanies. for example, ncsa telnet has been commercialized by intercon, and spyglass has commercializedncsa desktop imaging tools, as well as its mosaic program. cerfnet, a california wide area network forinternet access, has pioneered in supplying access to library holdings and other large databases, and discos/unitree, a mass storage system, is in use at more than 20 major computer sites. a new molecular modelingsystem, called sculpt, developed at the san diego supercomputing center, is being commercialized by a newcompany, interactive simulations. sculpt enables "draganddrop" molecular modeling in realtime whilepreserving minimumenergy constraints; its output was featured on a may 1994 cover of science.important science and engineering accomplishmentsselected areas and problems, summarized below, indicate the range of projects currently being undertaken bynearly 8,000 researchers at over 200 universities and dozens of corporations and the span of disciplines now usingthis new tool.quantum physics and materials sciencethe great disparity between nuclear, atomic, or molecular scales and macroscopic material scales implies thatvast computing resources are needed to attempt to predict the characteristics of bulk matter from fundamental lawsof physics. since the beginning of the nsf centers program, researchers in this area have been among the mostfrequent users of supercomputers. materials scientists have often been among the first to try out new architecturesthat promise higher computational speeds.listed below are some examples of research areas important to the study of properties of bulk matter inextreme conditions, such as occur in nuclear collisions, the early universe, or the core of jupiter; new materialssuch as nanotubes and hightemperature superconductors; and more practical materials used today such asmagnetic material and glass. transitions in quantum chromodynamics transitions of solid hydrogen nanomaterials predictionse115evolving the high performance computing and communications initiative to support the nation's information infrastructurecopyright national academy of sciences. all rights reserved.of hightemperature superconductors materialsbiology and medicineliving creatures exhibit some of the greatest complexity found in nature. therefore, supercomputers havemade possible unprecedented opportunities to explore these complexities based on the fundamental advances madein biological research of the last 50 years. these activities include using the data from xray crystallography tostudy the molecular structure of macromolecules; learning how to use artificial intelligence to fold polypeptidechains, determined from genetic sequencing, into threedimensional proteins; and determining the function ofproteins by studying their dynamic properties.new fields of computational science, such as molecular neuroscience, are being enabled by academic accessto metacenter computing and visualization resources and staff. corporations are using supercomputers andadvanced visualization techniques in collaboration with the nsf metacenter to create new drugs to fight humandiseases such as asthma. new insights into economically valuable bioproducts are being gained, for instance, bycombining molecular and medical imaging techniques to create "virtual spiders" that can be dissected digitally tounderstand the production of silk. finally, highperformance computers are becoming powerful enough to enableresearchers to program mathematical models of realistic organ dynamics, such as the human heart. examples ofprojects include the following:rystallography intelligence and protein foldingkinase solution neurosciencešserotonin neurosciencešacetylcholinesterasentibodyantigen docking tuning biomolecules to fight asthma virtual spiders and artificial silkengineeringmanmade devices have become so complex that researchers in both academia and industry have turned tosupercomputers in orderto be able to analyze and modify accurate models in ways that complement traditionalexperimental methods. highperformance computers enable academic engineers to study the brittleness of newtypes of steel, improve bone transplants, or reduce the drag of flows over surfaces using riblets. industrial partnersof the individual supercomputer centers within the metacenter are using advanced computational facilities toimprove industrial processes such as in metal forming. better consumer products, such as leakproof diapers ormore efficient airplanes, are being designed. even state agencies are able to use the metacenter facilities toimprove traffic safety or find better ways to use recycled materials. some 70 corporations have taken advantage ofthe metacenter industrial programs to improve their competitiveness.examples of engineeringrelated problems include the following:ahighstrength steelse116evolving the high performance computing and communications initiative to support the nation's information infrastructurecopyright national academy of sciences. all rights reserved. casting of steel a leakproof diaper bioengineeringperformance with riblets better aircraftrashtesting street signsearth sciences and the environmentthe resources of the nsf metacenter are being used to compute and visualize the complexity of the naturalworld around us, from the motions of earth's convective mantle to air pollution levels in southern california. theu.s. army is working with academics to determine how to practice tank maneuvers without endangering thebreeding habits of the sage grouse. pollution is a difficult coupling of chemical reactions and flow dynamics thatmust be understood in detail if corrective measures are to be efficacious. highperformance computers also act astime machines, allowing for fasterthanrealtime computation of severe storms. finally, to improve globalweather or climate forecasts, supercomputers allow researchers to study the physics of such critical processes asmixing at the airocean interface. among the related problems being addressed are the following: of ground water modeling and forecastingsmog mixingulating climate using distributed supercomputersplanetary sciences, astronomy, and cosmologyas was evident in the recent impact of comet shoemakerlevy 9 with jupiter, observatories on earth and inspace have become intimately linked. supercomputers are being integrated into observational facilities, like thegrand challenge berkeley illinois maryland association millimeter observatory, and into observational programssuch as the ones that have led to the discovery of new millisecond pulsars or the first extra solarsystem planet.the ability of numerical methods to solve even the most complex of fundamental physical laws, such aseinstein's equations of general relativity, is increasing understanding of the dynamics of strongfield events, suchas the collision of black holes. in perhaps the grandestscale challenge possible, the universe itself is a subject ofinvestigation by several grand challenge teams using resources of the metacenter to discover how the largescalestructures in the universe evolved from nearly perfect homogeneity at the time of the formation of the microwavebackground. of first extra solarsystem planet searching and discovery hole collision dynamicsimulationse117evolving the high performance computing and communications initiative to support the nation's information infrastructurecopyright national academy of sciences. all rights reserved.evolving the high performance computing and communications initiative to support the nation's information infrastructurecopyright national academy of sciences. all rights reserved.
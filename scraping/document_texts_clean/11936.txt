detailsdistribution, posting, or copying of this pdf is strictly prohibited without written permission of the national academies press. (request permission) unless otherwise indicated, all materials in this pdf are copyrighted by the national academy of sciences.copyright © national academy of sciences. all rights reserved.the national academies pressvisit the national academies press at nap.edu and login or register to get:œ œ 10% off the price of print titlesœ special offers and discountsget this bookfind related titlesthis pdf is available at sharecontributorshttp://nap.edu/11936summary of a workshop on softwareintensive systems anduncertainty at scale78 pages | 6 x 9 | paperbackisbn 9780309108447 | doi 10.17226/11936joan d. winston and lynette i. millett, editors; committee on advancingsoftwareintensive systems producibility; computer science andtelecommunications board; division on engineering and physical sciences;national research councilsummary of a workshop on softwareintensive systems and uncertainty at scalecopyright national academy of sciences. all rights reserved.joan d. winston and lynette i. millett, editorscommittee on advancing softwareintensive systems producibilitycomputer science and telecommunications boarddivision on engineering and physical sciencessummary of a workshop on softwareintensive systems and uncertainty at scalecopyright national academy of sciences. all rights reserved.the national academies press 500 fifth street, n.w. washington, dc 20001notice: the project that is the subject of this report was approved by the governing board of the national research council, whose members are drawn from the councils of the national academy of sciences, the national academy of engineering, and the institute of medicine. the members of the committee responsible for the report were chosen for their special competences and with regard for appropriate balance.support for this project was provided by the ofce of the secretary of defense, department of defense, with assistance from the national science foundation under sponsor award number cns0541636 and by the ofce of naval research under sponsor award number n000140410736. any opinions, ndings, conclusions, or recommendations expressed in this material are those of the authors and do not necessarily re˚ect the views of the agencies and organizations that provided support for the project.international standard book number13: 9780309108447international standard book number10: 0309108446additional copies of this report are available fromthe national academies press500 fifth street, n.w., lockbox 285washington, dc 20055800/6246242202/3343313 (in the washington metropolitan area)http://www.nap.educopyright 2007 by the national academy of sciences. all rights reserved.printed in the united states of americasummary of a workshop on softwareintensive systems and uncertainty at scalecopyright national academy of sciences. all rights reserved.the national academy of sciences is a private, nonprot, selfperpetuating society of distinguished scholars engaged in scientic and engineering research, dedicated to the furtherance of science and technology and to their use for the general welfare. upon the authority of the charter granted to it by the congress in 1863, the academy has a mandate that requires it to advise the federal government on scientic and technical matters. dr. ralph j. cicerone is president of the national academy of sciences.the national academy of engineering was established in 1964, under the charter of the national academy of sciences, as a parallel organization of outstanding engineers. it is autonomous in its administration and in the selection of its members, sharing with the national academy of sciences the responsibility for advising the federal government. the national academy of engineering also sponsors engineering programs aimed at meeting national needs, encourages education and research, and recognizes the superior achievements of engineers. dr. charles m. vest is president of the national academy of engineering.the institute of medicine was established in 1970 by the national academy of sciences to secure the services of eminent members of appropriate professions in the examination of policy matters pertaining to the health of the public. the institute acts under the responsibility given to the national academy of sciences by its congressional charter to be an adviser to the federal government and, upon its own initiative, to identify issues of medical care, research, and education. dr. harvey v. fineberg is president of the institute of medicine.the national research council was organized by the national academy of sciences in 1916 to associate the broad community of science and technology with the academy™s purposes of furthering knowledge and advising the federal government. functioning in accordance with general policies determined by the academy, the council has become the principal operating agency of both the national academy of sciences and the national academy of engineering in providing services to the government, the public, and the scientic and engineering communities. the council is administered jointly by both academies and the institute of medicine. dr. ralph j. cicerone and dr. charles m. vest are chair and vice chair, respectively, of the national research council.www.nationalacademies.orgsummary of a workshop on softwareintensive systems and uncertainty at scalecopyright national academy of sciences. all rights reserved.summary of a workshop on softwareintensive systems and uncertainty at scalecopyright national academy of sciences. all rights reserved.vcommittee on advancing softwareintensive systems producibilitywilliam l. scherlis, carnegie mellon university, chairrobert f. behler, the mitre corporationbarry w. boehm, university of southern californialori a. clarke, university of massachusetts, amherstmichael a. cusumano, massachusetts institute of technologymary ann davidson, oracle corporationlarry druffel, independent consultantrussell frew, lockheed martinjames larus, microsoft corporationgreg morrisett, harvard universitywalker royce, ibmdouglas c. schmidt, vanderbilt universityjohn p. stenbit, independent consultantkevin j. sullivan, university of virginiastafflynette i. millett, study director and senior program ofcerjoan d. winston, program ofcermargaret marsh huynh, senior program assistantsummary of a workshop on softwareintensive systems and uncertainty at scalecopyright national academy of sciences. all rights reserved.vicomputer science and telecommunications boardjoseph f. traub, columbia university, chaireric benhamou, benhamou global ventures, llcfrederick r. chang, university of texas, austinwilliam dally, stanford universitymark e. dean, ibm almaden research centerdeborah estrin, university of california, los angelesjoan feigenbaum, yale universitykevin kahn, intel corporationjames kajiya, microsoft corporationmichael katz, university of california, berkeleyrandy h. katz, university of california, berkeleysara kiesler, carnegie mellon universityteresa h. meng, stanford universityprabhakar raghavan, yahoo! researchfred b. schneider, cornell universityalfred z. spector, independent consultant, pelham, new yorkwilliam stead, vanderbilt universityandrew j. viterbi, viterbi group, llcpeter weinberger, google, inc.staffjon eisenberg, directorkristen batch, associate program ofcerradhika chari, administrative coordinatorrenee hawkins, financial associatemargaret marsh huynh, senior program assistantherbert s. lin, senior scientistlynette i. millett, senior program ofcerdavid padgham, associate program ofcerjanice m. sabuda, senior program assistantted schmitt, consultantbrandye williams, program assistantjoan d. winston, program ofcerfor more information on cstb, see its web site at <http://www.cstb.org>, write to cstb, national research council, 500 fifth street, n.w., washington, dc 20001, call (202) 3342605, or email cstb at cstb@nas.edu.summary of a workshop on softwareintensive systems and uncertainty at scalecopyright national academy of sciences. all rights reserved.viiprefacepursuant to a request by the department of defense, the national research council (nrc) convened a study committee under the auspices of the computer science and telecommunications board (cstb) to assess the nature of the u.s. national investment in software research and, in particular, to consider ways to enhance the knowledge and human resource base needed to design, produce, and employ softwareintensive systems for tomorrow™s weapons and operations systems. many organizations are facing the combination of increasing system scale and increasing complexity of softwareintensive systems. however, the compelling need to interconnect them to realize dod™s vision of ﬁnetcentric warfareﬂ exacerbates the challenges of uncertainty at scale for dod. several recent reports that highlight these challenges1 suggest that the scale and complexity of softwareintensive systems introduce fundamental new challenges and require augmentation of existing approaches by software practices and technologies that more explicitly address these challenges. challenges of uncertainty and scale are faced in largescale enterprise systems of all kinds but are particularly demanding in defense systems owing to the relative lack of precedent in both requirements and engineering designs and also to the need for high levels of quality, secu1 see, for example, software engineering institute (2006), ultralarge scale systems: the software challenge of the future, which noted that current abstractions fail for the levels of complexity that systems require today. also see defense science board (2000), task force on defense software report, which noted, among other things, that strengthening the technology base to rapidly adapt to ˚uid circumstances is important and that the complexity of dod software applications is increasing more than linearly. summary of a workshop on softwareintensive systems and uncertainty at scalecopyright national academy of sciences. all rights reserved.viii softwareintensive systems and uncertainty at scalerity, and safety in an environment with wellresourced adversaries. this suggests that defense, while sharing many particular kinds of requirements with largescale enterprises and infrastructures, is nonetheless a demand leader with respect to many of these requirements, outpacing most enterprise and commercial projects. as part of its study, this committee organized a public workshop on january 17, 2007, to examine uncertainty at scale in current and future softwareintensive systems. workshop sessions examined the challenges related to engineering uncertainty, system complexity, and scale from a range of perspectives. session speakers were given roughly 25 minutes to provide their views on issues identied in the workshop agenda (see appendix a). there was substantial discussion and interaction among the session speakers and moderators, the committee, and others present. the purpose of the workshop was to inform the committee as it conducts its study. this report summarizes the workshop discussions, including speaker presentations and discussions with committee members and others present. it is not a compilation of quotations from particular individuals nor is it a complete synthesis of conclusions or results. although the summary was prepared by the committee based on presentations and discussion at the workshop, the comments do not necessarily re˚ect the views of the committee nor do they represent ndings and recommendations of the nrc. moreover, the summary points for the sessions are a digest of both presentations and discussion. they should not be taken as remarks made solely by the scheduled session speakers because the discussions included remarks by the others in attendance. the committee™s broader consideration of advancing software intensive system producibility will appear in its nal report, to be issued near the end of the study. that report will provide recommendations to the responsible agency, the executive branch, and legislative ofcialsšand to the broader software communityšabout how to improve software development and achieve future goals. the committee thanks all the workshop participants for their thoughtful presentations and discussion. it also thanks the computer science and telecommunications board staff, particularly study director lynette millett and program ofcer joan winston, who have ably managed this project, coordinated the team, and contributed greatly to the development of this report, and to margaret huynh, who has facilitated our meetings and other project activities.william scherlis, chaircommittee on advancing softwareintensive systems producibilitysummary of a workshop on softwareintensive systems and uncertainty at scalecopyright national academy of sciences. all rights reserved.ix acknowledgment of reviewersthis report has been reviewed in draft form by individuals chosen for their diverse perspectives and technical expertise, in accordance with procedures approved by the national research council™s (nrc™s) report review committee. the purpose of this independent review is to provide candid and critical comments that will assist the institution in making its published report as sound as possible and to ensure that the report meets institutional standards for objectivity, evidence, and responsiveness to the study charge. the review comments and draft manuscript remain condential to protect the integrity of the deliberative process. we wish to thank the following individuals for their review of this report:david notkin, university of washington,alfred spector, independent consultant,john vu, boeing corporation,peter weinberger, google, inc., andjeannette wing, carnegie mellon university. although the reviewers listed above have provided many constructive comments and suggestions, they were not asked to endorse the conclusions or recommendations, nor did they see the nal draft of the report before its release. the review of this report was coordinated by susan graham of the university of california, berkeley. appointed by the nrc, summary of a workshop on softwareintensive systems and uncertainty at scalecopyright national academy of sciences. all rights reserved.x softwareintensive systems and uncertainty at scaleshe was responsible for making certain that an independent examination of this report was carried out in accordance with institutional procedures and that all review comments were carefully considered. responsibility for the nal content of this report rests entirely with the authoring committee and the institution.summary of a workshop on softwareintensive systems and uncertainty at scalecopyright national academy of sciences. all rights reserved.xicontents1 introduction and overview 12 summary of workshop discussions 4 session 1: process, architecture, and the grand scale, 4 session 2: dod software challenges for future systems, 12 session 3: agility at scale, 19 session 4: quality and assurance with scale and uncertainty, 23 session 5: enterprise scale and beyond, 333 wrapup discussion and emergent themes 40 architectural challenges in largescale systems, 40 the need for software engineering capability, 41 open questions and research opportunities, 42appendixesa workshop agenda 47b biosketches of committee members and staff 50c biosketches of workshop speakers 60summary of a workshop on softwareintensive systems and uncertainty at scalecopyright national academy of sciences. all rights reserved.summary of a workshop on softwareintensive systems and uncertainty at scalecopyright national academy of sciences. all rights reserved.11 introduction and overviewthis report summarizes the workshop on uncertainty at scale in the context of softwareintensive systems producibility, held january 17, 2007, in washington, d.c., under the auspices of the national research council™s (nrc™s) committee on advancing softwareintensive systems producibility. the workshop was structured to gather inputs and insights from the commercial and military software and system development communities looking at the current and future challenges that surround engineering uncertainty, system complexity, and scale. the purpose of the workshop was to inform the committee as it conducts its study and to stimulate discussion of these issues in the community. this workshop summary, which does not contain ndings or recommendations from the committee, is presented in the spirit of continuing that discussion. as they develop unprecedented systems, software engineers must address many kinds of uncertainty in the course of requirements assimilation, architecture, design, development, deployment, and ongoing use of largescale software systems. for defense systems, the operating environment, coalitiondriven requirements, adversary capabilities, security and safety requirements, and so on also pose signicant uncertainty. the increasing scale and complexity of systems, along with the compelling need to interconnect them, make the challenges of uncertainty at scale increasingly formidable and of particular signicance for the department of defense (dod). exacerbating the situation, requirements for dod softwareintensive systems are emerging and/or not static, and the dod has not enough internal software engineering expertise. in the context of summary of a workshop on softwareintensive systems and uncertainty at scalecopyright national academy of sciences. all rights reserved.2 softwareintensive systems and uncertainty at scalethis workshop, ﬁscaleﬂ refers not just to gross scale (e.g., the numbers of instructions or lines of code), but also to the extent of complexity, interconnection, and interdependence. ﬁuncertaintyﬂ refers to the full range of engineering risks, including the rates at which requirements and features and the operating and technical environments are changing. the briefings for this workshop examined these and other challenges regarding engineering uncertainty, system complexity, and scale from a range of perspectives. several recent reports that highlight these challenges1 suggest that the scale and complexity of softwareintensive systems introduce fundamental new challenges and require augmentation of existing approaches by software practices and technologies that more explicitly address the challenges. challenges of uncertainty and scale are faced in largescale enterprise systems of all kinds but are particularly demanding in defense systems. technical factors contributing to heightened challenges include the relative lack of precedent in both requirements and engineering designs, as well as the need for high levels of quality, security, and safety in an environment with wellresourced adversaries. institutional factors include complexities resulting from the particulars of dod management and procurement. as a result, defense requirements, although having some commonality with those of other largescale enterprises and infrastructures, may outpace those of most enterprise and commercial projects. workshop participants explored the various dimensions of these challenges, focusing particularly on software engineering and the management of uncertainties in requirements, operating environment, and implementation. they were invited to examine the range of problems inherent in building largescale systems and to explore both the current state of software engineering knowledge regarding potential incremental solutions to problems of scale as well as areas where fundamental research is needed to bridge the gap between current practice and the revolutionary challenges offered by future systems. discussions at the workshop took place with an eye to emerging defense needs but in a way that was receptive to lessons to be gleaned from commercial and enterpriselevel efforts. case studies and examples provided at the workshop were explored to assess promising ideas and directions and to identify the fundamental research problems that remain. questions posed to participants included these: are there precedented 1see, for example, software engineering institute (2006), ultralarge scale systems: the software challenge of the future, which noted that current abstractions fail for the levels of complexity that systems require today. also see defense science board (2000), task force on defense software report, which noted, among other things, that strengthening the technology base to rapidly adapt to ˚uid circumstances is important and that the complexity of dod software applications is increasing more than linearly. summary of a workshop on softwareintensive systems and uncertainty at scalecopyright national academy of sciences. all rights reserved.introduction and overview 3architectural concepts from existing systems that can provide a basis for making decisions about the architecture for systems where constant evolution and change are the norm? which best and emerging practices extant today are likely to be benecial? which technological approaches would support largescale decentralized design processes? which specic existing approaches show the most potential? what problems will industry likely solve on its own? what problems require the stimulus of r&d investment? participants were also asked to consider areas of research that are likely to yield fruitful outcomes and how dod, in particular, can both stimulate and assimilate innovative ideas, whether they be advanced practices from industry or ideas from the research laboratory.each succeeding section describes some of the main themes arising from a workshop session. the themes are not conclusions or ndings of the committee; they are ideas extracted from the discussions during each session, drawn not only from the presentations of the speakers but also from the discussions among all the participants (committee, speakers, and attendees) that seem to form the gist of the session. several items should be kept in mind when reading this report. the workshop focused on a particular subset of areas that the committee believed would provide a basis for its work during the next year. the committee plans to gather input on other topics, including through another public workshop; feedback and additional input from readers of this report are welcome. also, in the interests of timely dissemination, the committee elected to defer elaboration and analysis of the workshop discussion and instead to offer here a more succinct summary focused on reporting the issues discussed. accordingly, this report does not provide a freestanding overview of the current state of challenges in software development or largescale systems producibility. moreover, for readability and to promote understanding, background material on some of the topics raised has been interspersed with the record of presentations and discussions. presenters™ views varied, depending on their own experiences and expertise; some speakers provided detailed information while others offered higherlevel, more abstract presentations. for these reasons and because some of the discussion amounted to brainstorming, this summary may contain internal inconsistencies that re˚ect the wide range of views offered by the speakers and other participants.summary of a workshop on softwareintensive systems and uncertainty at scalecopyright national academy of sciences. all rights reserved.42 summary of workshop discussionssession 1: process, architecture, and the grand scalepanelists: john vu, boeing, and rick selby, northrop grumman corporation moderator: michael cusumanopanelist presentations and general discussions at this session were intended to explore the following questions from the perspectives of software development for government and commercial aerospace systems:ł what are the characteristics of successful approaches to architecture and design for largescale systems and families of systems? ł which architecture ideas can apply when systems must evolve rapidly? ł what kinds of management and measurement approaches could guide program managers and developers?synergies across software technologies and business practices enable successful largescale systems context matters in trying to determine the characteristics of successful approachesšdifferent customer relationships, goals and needs, pacing of projects, and degree of precedent all require different practices. for example, different best practices may apply depending on what sort of summary of a workshop on softwareintensive systems and uncertainty at scalecopyright national academy of sciences. all rights reserved.summary of workshop discussions 5system or application is under development. examples discussed include commercial software products, it and internet nancial services, airplanes, and government aerospace systems.ł different systems and software engineering organizations have different customers and strategies. they may produce a variety of deliverables, such as a piece of software, an integrated hardwaresoftware environment, or very large, complicated, interconnected, hardwaresoftware networked systems. ł different systems and software engineering organizations have different goals and needs.  product purposes varyšuser empowerment, business operations, and mission capabilities. projects can last from a month to 10 or 12 years. the project team can be one person or literally thousands. the customer agreement can be a license, servicelevel agreement, or contract. there can be a million customers or just onešfor example, the government. the managerial focus can be on features and time to market; cycle time, work˚ow, and uptime; or reliability, contract milestones, and interdependencies; and so on. ł while some best practices, such as requirements and design reviews and incremental and spiral life cycles, are broadly applicable, specic practice usually varies. although risk management is broadly applicable, commercial, nancial, and government system developers may adopt different kinds of risk management. while government aerospace systems developers may spend months or years doing extensive system modeling, this may not be possible in other organizations or for other types of products. commercial software organizations may focus on daily builds (that is, each day compiling and possibly testing the entire program or system incorporating any new changes or xes); for aerospace systems, the focus may be on weekly or 60day builds. other generally applicable best practices that vary by market and organization include parallel small teams, design reuse, domainspecic languages, opportunity management, tradeoff studies, and portability layers. these differences are driven by the different kinds of risks that drive engineering decisions in these sectors. ł government aerospace systems developers, along with other very large softwaredevelopment enterprises, employ some distinctive best practices. these include independent testing teams and, for some aspects of the systems under consideration, deterministic, simple designs. these practices are driven by a combination of engineering, riskmanagement, and contractual considerations.in a very large1 organization, synergy across software technologies and business practices can contribute to success. participants explored 1very large in this case means over 100,000 employees throughout a supply chain doing systems engineering, systems development, and systems management; managing multiple product lines; and building systems with millions of lines of code.summary of a workshop on softwareintensive systems and uncertainty at scalecopyright national academy of sciences. all rights reserved.6 softwareintensive systems and uncertainty at scalethe particular case of moderately precedented systems2 and major components with controlloop architectures. for systems of this kind there are technology and business practice synergies that have worked well. here are some examples noted by speakers:ł decomposition of large systems to manage risk. with projects that typically take between 6 and as many as 24 months to deliver, incremental decomposition of the system can reduce risk, provide better visibility into the process, and deliver capability over time. decomposition accelerates system integration and testing. ł tablebased design, oriented to a system engineering view in terms of states and transitions, both nominal and offnominal. this enables the use of clear, tabledriven approaches to address nominal modes, failure modes, transition phases, and different operations at different parts of the system operations.ł use of builtin, domainspecic (macro) languages in a layered architecture. the builtin, commandsequencing macro language denes tabledriven executable specications. this permits a relatively stable infrastructure and a runtime system with lowlevel, highly deterministic designs yet extensible functionality. it also allows automated testing of the systems. ł use of precedented and welldened architectures for the task management structure that incorporates a simple task structure, deterministic processing, and predictable timelines. for example, a typical threetask management structure might have highrate (32 ms) tasks, minorcycle (128 ms) tasks, and background tasks. the minor cycle reads and executes commands, formats telemetry, handles fault protection, and so forth. the highrate cycle handles message trafc between the processors. the background cycle adds capability that takes a longer processing time. this is a reusable processing architecture that has been used for over 30 years in spacecraft and is aimed at the construction of highly reliable, deterministic systems. ł gaining advantages from lack of fault proneness in reused components by achieving high levels of code, design, and requirement reuse. one example of code reuse was this: across 25 nasa ground systems, 32 percent of software components were either reused or modied from previous systems (for spacecraft, reuse was said to be as high as 80 percent). designs and requirements can also be reused. typically, there is a large backward 2 precedent refers to the extent to which we have experience with systems of a similar kind. more specically, there can be precedent with respect to requirements, architecture and design, infrastructure choices, and so on. building on precedent leads to routinization, reduced engineering risk, and better predictability (lower variance) of engineering outcomes. summary of a workshop on softwareintensive systems and uncertainty at scalecopyright national academy of sciences. all rights reserved.summary of workshop discussions 7compatibility set of requirements, and these requirements can be reused. requirements reuse is very common and very successful even though the design and implementation might each be achieved differently. design reuse might involve allocation of function across processors in terms of how particular algorithms are structured and implemented. the functions might be implemented differently in the new system, for example, in components rather than custom code or in different programming languages. this is an example of true design reuse rather than code reuse. in addition to these synergies, it was suggested that other types of analyses could also contribute to successful projects. datadriven statistical analyses can help to identify trends, outliers, and process improvements to reduce or mitigate defects. for example, higher rates of component interactions tend to be correlated with more faults, as well as more faultcorrection effort. risk analyses prioritize risks according to cost, schedule, and technical safety impacts. charts that show project risk mitigation over time and desired milestones help to dene specic tasks and completion criteria. it was suggested that each individual risk almost becomes a microcosm of its own project, with schedules and milestones and progressive mitigation of that risk to add value.one approach to addressing the challenge of scale is to divide and conquer. of course, arriving at an architectural design that supports decomposition is a prerequisite for this approach, which can apply across many kinds of systems development efforts. suggestions included the following:ł divide the organization into parallel teams. divide very large 1,000person teams into parallel teams; establish a project rhythm of design cycles and incremental releases. this division of effort is often based on a system architectural structure that supports separate development paths (an example of what is known as conway™s lawšthat software system structures tend to re˚ect the structures of the organizations they are developed by). indeed, without agreement on architectural fundamentalsšthe key internal interfaces and invariants in a systemšdivision of effort can be a risky step. ł innovate and synchronize. bring the parallel teams together, whether the task is a compilation or a component delivery and interface integration. then stabilize, usually through some testing and usage period. ł encourage coarsegrain reuse. there is a lot of focus on very negrain reuse, which tends to involve details about interfaces and dependencies; there is also signicant coarsegrain opportunity to bring together both legacy systems and new systems. a coarsegrain approach makes possible the accommodation of systems at different levels of maturity. summary of a workshop on softwareintensive systems and uncertainty at scalecopyright national academy of sciences. all rights reserved.8 softwareintensive systems and uncertainty at scaleexamples of success in coarsegrain reuse are major system frameworks (such as ecommerce frameworks), servicebased architectures, and layered architectures.ł automate. automation is needed in the build process, in testing, and in metrics. uncertainty is inherent in the development of softwareintensive systems and must be reassessed constantly, because there are always unknowns and incomplete information. waiting for complete information is costly, and it can take signicant time to acquire the informationšif it is possible to acquire it at all. schedules and budgets are always limited and almost never sufcient. the goal, it was argued, should be to work effectively and efciently within the resources that are available and discharge risks in an order that is appropriate to the goals of the system and the nature of its operating environment: establish the baseline design, apply systematic risk management, and then apply opportunity management, constantly evaluating the steps needed and making decisions about how to implement them. thus, it was suggested that appropriate incentives and analogous penalty mechanisms at the individual level and at the organization or supplier level can change behavior quickly. the goal is thus for the incentive structure to create an opportunity to achieve very efcient balance through a ﬁselfmanaging organization.ﬂ in a selfmanaging organization, it was suggested, the leader has the vision and is an evangelist rather than a micromanager, allowing others to manage using systematic incentive structures.some ways to enable software technology and business practices for largescale systems were suggested: ł creating strategies, architectures, and techniques for the development and management of systems and software, taking into account multiple customers and markets and a broad spectrum of projects, from small scale through large. ł disseminating validated experiences and data for best practices and the circumstances when they apply (for example, titles like ﬁcase studies of model projectsﬂ).ł aligning big v waterfalllike systems engineering lifecycle models with incremental/spiral software engineering lifecycle models.33 the v model is a vshaped, graphical representation of the systems development life cycle that denes the results to be achieved in a project, describes approaches for developing these results, and links early stages (on the left side of the v) with evaluation and outcomes (on the right side). for example, requirements link to operational testing and detailed design links to unit testing. summary of a workshop on softwareintensive systems and uncertainty at scalecopyright national academy of sciences. all rights reserved.summary of workshop discussions 9ł facilitating objective interoperability mechanisms and benchmarks for enabling information exchange.ł lowering entry barriers for research groups and nontraditional suppliers to participate in largescale system projects (grand challenges, etc.).ł encouraging advanced degree programs in systems and software engineering.ł dening research and technology roadmaps for systems and software engineering.ł collaborating with foreign software developers.process, architecture, and very largescale systemsremarks during this portion of the session were aimed at thinking outside the box about what the state of the art in architectures might look like in the future for very largescale, complex systems that exhibit unpredictable behavior. the primary context under discussion was largescale commercial aircraft developmentšthe boeing 777 has a few million lines of code, for example, and the new 787 has several million and climbing. it was argued that very largescale, highly complex systems and families of systems require new thinking, new approaches, and new processes to architect, design, build, acquire, and operate. it was noted that these new systems are going from millions of lines of code to tens of millions of lines of code (perhaps in 10 years to billions of lines of code and beyond); from hundreds of platforms (servers) to thousands, all interconnected by heterogeneous networks; from hundreds of vendors (and subcontractors) to thousands, all providing code; and from a welldened user community to dynamic communities of interdependent users in changing environments. it was suggested that the issue for the futureš10 or 20 years from nowšis how to deal with the potential billion lines of code and tens of thousands of vendors in the very diverse, openarchitectureenvironment global products of the future, assembled from around the world. according to the forwardlooking vision presented by speakers, these systems may have the following characteristics: ł very largescale systems would integrate multiple systems, each of them autonomous, having distinctive characteristics, and performing its own functions independently to meet distinct objectives. ł each system would have some degree of intelligence, with the objectives of enabling it to modify its relationship to other component systems (in terms of functionality) and allowing it to respond to changes, perhaps unforeseen, in the environment. when multiple systems are joined summary of a workshop on softwareintensive systems and uncertainty at scalecopyright national academy of sciences. all rights reserved.10 softwareintensive systems and uncertainty at scaletogether, the signicant emergent capabilities of the resulting system as a whole would enable common goals and objectives.ł each very largescale system would share information among the various systems in order to address more complex problems. ł as more systems are integrated into a very largescale system, the channels connecting all systems through which information ˚ows would become more robust and continue to grow and expand throughout the life cycle of the very largescale system. it was argued that a key benet of a very largescale system is the interoperability between operational systems that allows decision makers to make better, more informed decisions more quickly and accurately. from a strategic perspective, a very largescale system is an environment where operational systems have the ability to share information among geographically distributed systems with appropriate security and to act on the shared information to achieve their desired business goals and objectives. from an operational perspective, a very largescale system is an environment where each operational subsystem performs its own functions autonomously, consistent with the overall strategy of the very largescale system.the notion of continuous builds or continuous integration was also discussed. software approaches that depend on continuous integrationšthat is, where changes are integrated very frequentlyšrequire processes for change management and integration management. these processes are incremental and build continuously from the bottom up to support evolution and integration, instead of from the top down, using a plandriven, structured design. they separate data and functions for faster updates and better security. to implement these processes, decentralized organizations and an evolving concept of operations are required to adapt quickly to changing environments. the overall architectural framework for largescale systems described by some participants in this session consists of ve elements: ł governance. these describe the rules, regulations, and change management that control the total system.ł operational. these describe how each operational system can be assembled from preexisting or new components (building blocks) to operate in their own new environment so they can adapt to change.ł interaction. these describe the communication (information pipeline) and interaction between operational systems that may affect the very large system and how the very large system will react to the inputs from the operational systems.summary of a workshop on softwareintensive systems and uncertainty at scalecopyright national academy of sciences. all rights reserved.summary of workshop discussions 11ł integration and change management. these describe the processes for managing change and the integration of systems that enable emergent capabilities.ł technical. these depict the technology components that are necessary to support these systems.it was suggested that largescale systems of that future that will cope with scale and uncertainty would be built from the bottom up by starting with autonomous building blocks to enable the rapid assembly and integration of these components to effectively evolve the very largescale system. the architectural framework would ensure that each building block would be aligned to the total system. building blocks would be assembled by analyzing a problem domain through the lens of an operational environment or mission for the purpose of creating the characteristics and functionality that would satisfy the stakeholders™ requirements. in this missionfocused approach, all stakeholders and modes of operations should be clearly identied; different user viewpoints and needs should be gathered for the proposed system; and stakeholders must state which needs are essential, which are desirable, and which are optional. prioritization of stakeholders™ needs is the basis for the development of such systems; vague and con˚icting needs, wants, and opinions should be claried and resolved; and consensus should be built before assembling the system. at the operational level, the system would be separated from current rigid organization structures (people, processes, technology) and would evolve into a dynamic concept of operation by assembling separate building blocks to meet operational goals. the system manager should ask: what problem will the system solve? what is the proposed system used for? should the existing system be improved and updated by adding more functionality or should it be replaced? what is the business case? to realize this future, participants suggested that research is needed in several areas, including these: ł governance (rules and regulations for evolving systems).ł interaction and communication among systems (including the possibility of negative interactions between individual components and the integrity, security, and functioning of the rest of the system).ł integration and change management.ł user™s perspective and usercontrolled evolution.ł technologies supporting evolution.ł management and acquisition processes.ł an architectural structure that enables emergence.summary of a workshop on softwareintensive systems and uncertainty at scalecopyright national academy of sciences. all rights reserved.12 softwareintensive systems and uncertainty at scaleł processes for decentralized organizations structured to meet operational goals.session 2: dod software challenges for future systemspanelists: kristen baldwin, ofce of the under secretary of defense for acquisitions, technology and logistics, and patrick lardieri,  lockheed martin moderator: douglas schmidtpanelist presentations and general discussions during this session were intended to explore two questions, from two perspectives: that of the government and that of the government contractor: ł how are challenges for software in dod systems, particularly cyberphysical systems, being met in the current environment? ł what advancements in r&d, technology, and practices are needed to achieve success as demands on softwareintensive system development capability increase, particularly with respect to scale, complexity, and the increasingly rapid evolution in requirements (and threats)? dod software engineering and system assurance an overview of various activities relating to dod software engineering was given. highlights from the presentation and discussion follow. the recent acquisition & technology reorganization is aimed at positioning systems engineering within the dod, consistent with a renewed emphasis on software. the director of systems and software engineering now reports directly to the under secretary of defense for acquisition and technology. the mission of systems and software engineering, which addresses evolving systemšand softwarešengineering challenges, is as follows:ł shape acquisition solutions and promote early technical planning.ł promote the application of sound systems and software engineering, developmental test and evaluation, operational test and evaluation to determine operational suitability and effectiveness, and related technical disciplines across dod™s acquisition community and programs.ł raise awareness of the importance of effective systems engineering and raise program planning and execution to the state of the practice.ł establish policy, guidance, best practices, education, and training in collaboration with the academic, industrial, and government communities.summary of a workshop on softwareintensive systems and uncertainty at scalecopyright national academy of sciences. all rights reserved.summary of workshop discussions 13ł provide technical insight to program managers and leadership to support decision making.dod™s software center of excellence is made up of a community of participants including industry, dodwide partnerships, national partnerships, and university and international alliances. it will focus on supporting acquisition; improving the state of the practice of software engineering; providing leadership, outreach, and advocacy for the systems engineering communities; and fostering resources that can meet dod goals. these are elements of dod™s strategy for software, which aims to promote worldclass leadership for dod software engineering. findings from some 40 recent program reviews were discussed. these reviews identied seven systemic issues and issue clusters that had contributed to dod™s poor execution of its software program, which were highlighted in the session discussion. the rst issue is that software requirements are not well dened, traceable, and testable. a second issue cluster involves immature architectures; integration of commercialofftheshelf (cots) products; interoperability; and obsolescence (the need to refresh electronics and hardware). the third cluster involves software development processes that are not institutionalized, have missing or incomplete planning documents, and inconsistent reuse strategies. a fourth issue is software testing and evaluation that lacks rigor and breadth. the fth issue is lack of realism in compressed or overlapping schedules. the sixth issue is that lessons learned are not incorporated into successive buildsšthey are not cumulative. finally, software risks and metrics are not well dened or well managed.  to address these issues, dod is pursuing an approach that includes the following elements: ł identication of software issues and needs through a software industrial base assessment,4 a national defense industrial association (ndia) workshop on top software issues, and a defense software strategy summit. the industrial base assessment, performed by csis, found that the lack of comprehensive, accurate, timely, and comparable data about software projects within dod limits the ability to undertake any bottomup analysis or enterprisewide assessments about the demand for software. although the csis analysis suggests that the overall pool of software developers is adequate, the csis assessment found an imbalance in the supply of and demand for the specialized, upper echelons of software developer/management cadres. these senior cadres can be grown, but it takes time (10 or more years) and 4 center for strategic and international studies (csis), defenseindustrial initiatives group, 2006. software industrial base assessment: phase i report, october 4. summary of a workshop on softwareintensive systems and uncertainty at scalecopyright national academy of sciences. all rights reserved.14 softwareintensive systems and uncertainty at scalea concerted strategy. in the meantime, management/architecture/systems engineering tools might help improve the effectiveness of the senior cadres. defense business system/cots software modication also places stress on limited pools of key technical and management talent. moreover, the true cost and risk of software maintenance deferral are not fully understood. ł creation of opportunities and partnerships through an established network of government software points of contact; chartering the ndia software committee; information exchanges with government, academia, and industry, and planning a systems and software technology conference. top issues emerging from the ndia defense software strategy summit in october 2006 included establishment and management of software requirements, the lack of a software voice in key system decisions, inadequate lifecycle planning and management, the high cost and ineffectiveness of traditional software verication methods, the dearth of software management expertise, inadequate technology and methods for assurance, and the need for better techniques for cots assessment and integration.ł execution of focused initiatives such as capability maturity model integration (cmmi) support for integrity and acquisition, a cmmi guidebook, a handbook on engineering for system assurance, a systems engineering guide for systems of systems (soss), the provision of software support to acquisition programs, and a vision for acquisition reform. soss to be used for defense require special considerations for scale (a single integrated architecture is not feasible), ownership and management (individual systems may have different owners), legacy (given budget considerations, current systems will be around for a long time), changing operations (sos congurations will face changing and unpredictable operational demands), criticality (systems are integrated via software), and role of the network (soss will be networkbased, but budget and legacy challenges may make implementation uneven). to address a complex sos, an initial (incremental) version of the dod™s sos systems engineering guide is being piloted; future versions will address enterprise and netcentric considerations, management, testing, and sustaining engineering. the issue of system assurancešreducing the vulnerability of systems to malicious tampering or accessšwas noted as a fundamental consideration, to the point that cybertrust considerations can be a fundamental driver of requirements, architecture and design, implementation practice, and quality assurance.5 because current assurance, safety, and protection 5 a separate national research council study committee is exploring the issue of cybersecurity research and development broadly, and its report, toward a safer and more secure cyberspace, will be published in nal form in late 2007. see http://cstb.org/projectcybersecurity for more information. summary of a workshop on softwareintensive systems and uncertainty at scalecopyright national academy of sciences. all rights reserved.summary of workshop discussions 15initiatives are not aligned, a comprehensive strategy for system assurance initiatives is being developed, including standards activities and guidance to put new methods into practice. one additional challenge for dod is that, given its shortage of software resources and critical dependence on software, it cannot afford to have stovepipes in its community. to that end, the dod software center of excellence is intended to be a focal point for the community. areas to be explored include, for example, agile methods, software estimation (a harder problem to address for unprecedented systems than for precedented ones), and software testing. challenges in developing dod cyberphysical systemsthis session explored challenges in building cyberphysical systems for dodšsystems that integrate physical processes and computer processes in a realtime distributed fashionšfrom the perspective of a large contractor responsible for a wide range of systems and it services. cyberphysical systems are increasingly systems and softwareintensivešfor example, in 1960, only 8 percent of the f4 ghter capability was provided by software; in 2000, 85 percent of the f22 ghter capability was provided by software. such systems are distributed, realtime systems with millions of lines of code, driven by multiple sensors reporting at a variety of timescales and by multiple weapon system and machinery control protocols. current examples of cyberphysical systems include the joint strike fighter (jsf) and future combat systems (fcs); examples of forthcoming technologies would be teams of autonomous robots or teams of small, fast surface ships. characteristics of these systems exemplify the challenges of uncertainty and scale; they includeł large scaleštens of thousands of functional and performance requirements, 10 million lines of code, and 100 to 1,000 software conguration items;ł simultaneous con˚icting performance requirementsšrealtime processing, bounded failure recovery, security;ł implementation diversityšprogramming languages, operating systems, middleware, complex deployment architectures, 2040 year system life cycles, stringent certication standards; and ł complex deployment architecturesšsystems of systems; mixed wired, wireless and satellite networks; multitiered servers; personal digital assistants (pdas) and workstations; and multiple system congurations.these systems are challenging, complex, and costly. accordingly, system design challenges are frequently simplied by deferring or eliminatsummary of a workshop on softwareintensive systems and uncertainty at scalecopyright national academy of sciences. all rights reserved.16 softwareintensive systems and uncertainty at scaleing capability to bound costs and delivery dates. nevertheless, cost overruns and schedule delays are common. the gao reported that in scal year 2003 (fy03) the dod spent $21 billion on research, development, testing, and evaluation (rdt&e) for new warghting systems; about 40 percent of that may have been spent on reworking software to remedy qualityrelated issues.6 for the f/a22, the gao reported that air force ofcials do not understand avionics software instability well enough to predict when they will be able to resolve its problems.7 because of the complex interrelationships between parts of these cyberphysical systems and the high degree of interactive complexity, piecewise deployment of partial systems is not helpful. an example given was a situation regarding the jsf, where changing an instruction memory layout to accommodate builtin test processing unexpectedly damaged the system™s ability to meet timing requirements. it was suggested that as a result of experiences such as the one with the aegis combat system, where aegis baseline 6, phase i, deployment was delayed for months because of integration problems between two independently designed cyberphysical systems, certi6 government accountability ofce (gao), 2004, ﬁdefense acquisitions: stronger management practices are needed to improve dod™s softwareintensive weapon acquisitions,ﬂ report to the committee on armed services, u.s. senate, gao04393, march.7 government accountability ofce (gao), 2003, ﬁtactical aircraft: status of the f/a22,ﬂ statement of alan li, director, acquisition and sourcing management, testimony before the subcommittee on tactical air and land forces, committee on armed services, house of representatives (gao03603t), february. see also: government accountability ofce (gao), 2005, ﬁtactical aircraft: f/a22 and jsf acquisition plans and implications for tactical aircraft modernization,ﬂ statement of michael sullivan, director, acquisition and sourcing management issues, testimony before the subcommittee on airland, committee on armed services, u.s. senate (gao05519t), april 6, which concluded as follows: the original business case elementsšneeds and resourcesšset at the outset of the program are no longer valid, and a new business case is needed to justify future investments for aircraft quantities and modernization efforts. the f/a22™s acquisition approach was not knowledgebased or evolutionary. it attempted to develop revolutionary capability in a single step, causing signicant technology and design uncertainties and, eventually, signicant cost overruns and schedule delays;and government accountability ofce (gao), 2007, ﬁtactical aircraft: dod needs a joint and integrated investment strategy,ﬂ report to the chairman, subcommittee on air and land forces, committee on armed services, house of representatives (gao07415), april, which concluded as follows:we have previously recommended that dod develop a new business case for the f22a program before further investments in new aircraft or modernization are made. dod has not concurred with this recommendation, stating that an internal study of tactical aircraft has justied the current quantities planned for the f22a. because of the frequently changing osdapproved requirements for the f22a, repeated cost overruns, signicant remaining investments, and delays in the program we continue to believe a new business case is required and that the assumptions used in the internal osd study be validated by an independent source.summary of a workshop on softwareintensive systems and uncertainty at scalecopyright national academy of sciences. all rights reserved.summary of workshop discussions 17cation communities have become extremely conservative and require a static conguration for certication.8despite software™s centrality and criticality in dod cyberphysical systems and in warghting in general, participants suggested that it is underemphasized in highlevel management reviews. for example, the quadrennial defense review calls for more complex systems for advanced warghting capabilities but mentions software only twice. some inherent scientic and research challenges underlying engineering and engineering management of dod cyberphysical systems cited by workshop participants include these: ł the management of knowledge fragmentationšfragmentation among people and teams, geographic areas, organizations, and temporal boundaries; ł design challengesšthe many problems that cannot be clearly dened without specifying the solution and for which every solution is a onetimeonly operation (these are sometime referred to as ﬁwicked problemsﬂ); and ł team collaboration complexityšthousands of requirements, huge teams (hundreds or thousands of engineers), with frequent turnover and highly variable ranges of skill.with respect to knowledge fragmentation (that is, knowledge split across individual minds, knowledge split across different phases of the development cycle and the life cycle, knowledge split across different artifacts, and knowledge split across various components of an organization), system engineering today is a concurrent topdown process. there is ad hoc coordination among engineers (domain engineers, system engineers, software engineers) at different levels and loose semantic coupling between design and specications. there are some problems where it is difcult to say what to do without specifying how and thereby committing to an implementation; participants noted that current tools do not generally help manage the tremendous interdependence between the specication of the problem and the realization of a solution. solutions are not necessarily right or wrong, and designers have to iterate rapidly, switching repeatedly between thinking about problem and solution concerns, along the lines of fred brooks™ description of throwaway prototyping.9 the process is slow, it is error prone because interaction is ad hoc, 8 note that the previous discussion noted the desirability of not having a static conguration in early stages of development. 9 frederick p. brooks, 1995, the mythical manmonth: essays in software engineering,  addisonwesley professional, new york.summary of a workshop on softwareintensive systems and uncertainty at scalecopyright national academy of sciences. all rights reserved.18 softwareintensive systems and uncertainty at scaleit uses imprecise english prose, and automated checking is relegated to the lowest level where formal specications exist. matters become even worse as the program or system grows in size and complexity. large teams managing complex systems must grapple with the issues of large scale in a complex collaborative environment. interactive complexity has two dimensions: coupling (tight or loose) and interactions (complex or linear). systems with high interactive complexityšfor example, nuclear power plants and chemical plantsšpossess numerous hidden interactions that can lead to systems accidents and hazards. interactive complexity can complicate reuse. wellknown cyberphysical system accidents cited by participants included the ariane 5, which reportedly reused a module developed for ariane 4. that module assumed that the horizontal velocity component would not over˚ow a 16bit variable. this was true for ariane 4 but not for ariane 5, leading to selfdestruction roughly 40 seconds after the launch.10 cyberphysical systems typically have high interactive complexity. new systems have more resource sharing, which leads to hidden dependencies. there is limited design time support to understand or reduce interactive complexity. modeling and analytic techniques are difcult to employ and often are underutilized. simulations may not capture the system that is actually built; diagrams are not sufcient to convey all consequences of decisions. thus, present cyberphysical systems rely on human ingenuity at design time and extensive system testing to manage interactive complexity. they also rely on particular knowledge of and experience with specic vendorsourced components in the ﬁtechnology stack.ﬂ for this reason, the structure of the stack tends to resist change, impeding architectural progress and increased complexity in these systems. the resulting long and costly development efforts are expected to run into system accidents. elements of a research agenda for cyberphysical systems that perform predictably were discussed. one goal of such research would be to nd ways to manage the uncertainty that arises from the highlycoupled nature and interactive complexity of system design at very large scale. two areas were focused on during discussions at this session:ł platform technology. one example of a platform technology would be generation of custom runtime infrastructures. current runtime infrastructure is deployed in generalpurpose layers that are not designed for specic applications. it is a signicant challenge to congure controls 10 j.c. lyons, 1996, ﬁariane 5 flight 501 failure: report by the inquiry board,ﬂ paris, july 19. downloaded from http://www.ima.umn.edu/~arnold/disasters/ariane5rep.html on march 15, 2007.summary of a workshop on softwareintensive systems and uncertainty at scalecopyright national academy of sciences. all rights reserved.summary of workshop discussions 19across the layers to achieve performance requirements; analysis is difcult because of many hidden dependencies and because of complex interfaces and capabilities. the generation of custom runtime infrastructure (e.g., websprocket) reduces system complexity. another such technology would be certiable dynamic resource management services. current certication processes are based on extensive analysis and testing (hundreds of manyears) of xed system congurations. furthermore, these are humanintensive evaluation processes with limited technological support that occur over the design, development, and production lifecycle. there is no way to achieve the same level of assurance for untested system congurations that may be generated by an adaptive system in the runtime environmentšand these are the kinds of systems that are likely to be deployed in the future. ł system design tools. modelcentric system design would allow evaluation of the design before nal implementation by developing prototyping systems and using forms of static verication. domainspecic modeling languages enable unambiguous system specications. model generation tools could be used to make models the center of the development process, synchronized with software artifacts. tools that enable automated characterization of the behavior of thirdparty and cots applications would be helpful. and, program transformation tools could be used to make the legacy code base and cots software compatible with new platforms.in addition to platform technologies and design tools, cultural elements are also needed to address the challenges of cyberphysical system development. speakers noted some aspects of these elements:ł independent, neutralparty benchmarking and evaluationšspeakers believed that there is currently insufcient funding for this type of work.ł development challenges that are realistic and at scale, allowing credible evaluation of technology solutions (measure technologies, not just artifacts).ł system design education as part of the undergraduate curriculum.session 3: agility at scalepanelists: kent beck and cynthia andres, three rivers institutemoderator: douglas schmidtthis session addressed the application and applicability of extreme programming™s ﬁagile techniquesﬂ to very large, complex systems from summary of a workshop on softwareintensive systems and uncertainty at scalecopyright national academy of sciences. all rights reserved.20 softwareintensive systems and uncertainty at scalethe perspectives of technology, development practices, and social psychology. for this session, both speakers and workshop participants were asked the following question:ł how can the engineering and management values that the ﬁagileﬂ community has identied be achieved for largerscale systems and for systems that must evolve in response to rapidly changing requirements?values and sponsorshipparticipants noted that extreme programming (xp)šan agile software development methodologyšwas one of the rst methodologies to be explicit about the value system behind its approach and about what is fundamental to this perspective on software development.11 different development approaches have their own underlying values. speakers argued that over the 10 years or so since the coining of extreme programming, the key to success seems to be sponsorshipšseniorlevel commitment to adopting xp ideas within an organization. trying extreme programming can be disruptive, stirring up internal tension and controversy. effective sponsors advocate among their peers and mitigate these effects. seniorlevel sponsors also can help acquire resources to foster teamwork and communication. when trying extreme programming, it was suggested that people tend to focus initially on the more visible and explicit changes to practices, such as pair programming, weekly releases, sitting together in open rooms, or using a test rst approach. if a fundamental value shift is taking place, practices will change accordingly. however, under pressure, people tend to revert back to their old ways. without support at higher levels for changes in approach and underlying values and without sustaining that support through periods of organizational discomfort during the transition, simply trying to put new or different practices in place is not very effective. one speaker noted that seniorlevel commitment and sponsorship are therefore key to changing values and conveying these changes to larger groups and organizations. human issues in software developmentone speaker noted that many of the challenges in software development are human issues: people are the developers and people write the 11 for a brief summary of some of the underlying values in agile software development, see ﬁthe agile manifesto,ﬂ available online at http://www.agilemanifesto.org/. these values include a focus on individuals over process, working software over documentation, and responsiveness to change over following a particular plan. summary of a workshop on softwareintensive systems and uncertainty at scalecopyright national academy of sciences. all rights reserved.summary of workshop discussions 21software. limitations to what can be done with software are often limitations of human imagination and how much complexity can be managed in one person™s mind. innovation requires fresh ideas, and if all parties are thinking similarly, not as many ideas are generated. many problems have multiple solutionsša key is to sort out which solutions are sufcient and doable. it was suggested that one way to promote innovation is to encourage diversity: small projects with diverse groups can be effective in that fostering interaction and coordination across disciplines often results in a stronger, richer set of ideas to choose from. it was suggested that having interesting problems to work on is a nonmonetary motivator for many software and computer science practitioners. a good example of nonmonetary incentives is open source technology. participants also noted that marketing innovation, intellectual curiosity, and creativity as an organizational goals are important. the perception or image of the work can be crucial to attracting new hires, who may know that the organization™s work involves a lot of processes, requires great care, and takes a long time, but not necessarily that it is also interesting and creative work. trust, communication, and riskspeakers argued that much of the effort in extreme programming comes down to nding better ways of building trust. examples were given of ways to begin conversations and to putpeople in contact with one another in order to establish trust. these include the techniques of appreciative inquiry (talking about what works), world café (acquire the collective knowledge, insight, and synergies of a group in a fairly short time), and open space (people talk about the concerns that they have and the issues that matter to them in breakout sessions whose highlights are reported to the rest of the group).12 some of the technical aspects of extreme programming are useful ways for programmers to demonstrate their trustworthiness. it was suggested that enhancing communicationšin part, by using these communication techniquesšcould be useful to dod. tools to make developers™ testing activities more visible, not just to themselves but also to teammates, contribute to accountability and transparency in development and increase communication as well.there are technical things that can be done to reduce risk in projects. some riskreduction principles persist throughout all of extreme programming (xp). however, the bulk of the xp experience is not at the scale that 12 see the appreciative inquiry commons (http://appreciativeinquiry.case.edu/), the world café (http://www.theworldcafe.com/), and open space (http://www.openspaceworld.org/). summary of a workshop on softwareintensive systems and uncertainty at scalecopyright national academy of sciences. all rights reserved.22 softwareintensive systems and uncertainty at scaledod systems manifest. therefore, one issue raised at the meeting was whether and how the agile programming/extreme programming experience can scale. three risk reduction techniques were mentioned: ł reduce the amount of work that is half done. halfdone work is an inherent risk. the feedback cycle has not been closed. no value has yet been received from the effort that has been expended; the mix of done and undone work occupies and distracts people. by gradually reducing the inventory of halfdone work, a project can be made to run more smoothly with lower overall risk. ł find ways to defer the specication of requirement detail. if a project experiences requirement churn, the chances are that too much detail has been specied too soon. there is a clear case to be made for much more carefully specifying the goals of a project up front but not the means for accomplishing those goals.ł testing sooner. the longer a bug lives, the more expensive it becomes. one way of addressing that situation and improving the overall effectiveness of development is nding ways to validate software sooner, such as by developer testing. integration is part of that testing. several research topics were discussed at this session: ł techniques for communication. examine how teams actually communicate and how they could communicate more effectively.13 ł encouragement of multidisciplinary work and collaboration. ł learning how to value simplicity. in complex systems, fewer components in the architecture means fewer possible unpredictable interactions. ł empirical research in software. one example of the results of such research was notedšnamely, the appearance of the power law distributions for object usage in software. that is, many objects are only used once, some are used multiple times, and very few are used very frequently. exploring the implications of this and other phenomena may provide insight into development methodologies and how to manage complexity and scale.ł testing and integration techniques. in a complicated deployment environment, nding better ways to get more assurance sooner in the 13 one example is some research under way at the university of shefeld. psychologists watch teams using xp methodologies and then report on the psychological effects of using xp, as opposed to other metrics such as defect rates. results suggest that people are happier doing things this way. see http://www.shef.ac.uk/dcs/research/groups/vt/research/observatory.html for more information. summary of a workshop on softwareintensive systems and uncertainty at scalecopyright national academy of sciences. all rights reserved.summary of workshop discussions 23cycle and more frequently should improve software development as a whole.session 4: quality and assurance with scale and uncertaintypanelists: joe jarzombek, department of homeland security; kris britton, national security agency; mary ann davidson, oracle corporation; gary mcgraw, cigital moderator: william scherlispanelist presentations and general discussions in this session were intended to address the following questions, from government and industry perspectives: ł what are the particular challenges for achieving particular assurances for software quality and cybersecurity attributes as scale and interconnection increase?ł what are emerging best practices and technologies? ł what kinds of new technologies and practices could assist? this includes especially interventions that can be made as part of the development process rather than after the fact. interventions could include practices, processes, tools, and so on. ł how should costeffectiveness be assessed? ł what are the prospects for certication, both at a comprehensive level and with respect to particular critical quality attributes? the presentations began by describing the goals and activities of two federal programs in software assurance and went on to explore present and future approaches.software assurance considerations and the dhs software assurance programthe u.s. department of homeland security (dhs) has a strategic program (discussed in more detail later in this section) to promote integrity, security, and reliability in software.14 this program for software assur14 the denition of software assurance that dhs uses comes out of the committee on national security systems: namely, it is the level of condence that software is free from vulnerabilities, either intentionally designed into the software or accidentally inserted at any time during its life cycle, and that the software functions in the intended manner. more generally, ﬁassuranceﬂ is about condencešthat is, it is a human judgment, not an objective test/verication/analytic result but rather a judgment based on those results.summary of a workshop on softwareintensive systems and uncertainty at scalecopyright national academy of sciences. all rights reserved.24 softwareintensive systems and uncertainty at scaleance emphasizes security; the risk exposures associated with reliance on software leave a lot of room for improvements. in industry as well as government there is increased concern about security. security is difcult to measure. it is difcult to quantify security or assess relative progress in improving it. participants noted the need for more comprehensive diagnostic capabilities and standards on which to base assurance claims. two suggestions were made: ł the software assurance tool industry has not been keeping pace with changes in software systemsštools that provide point solutions are available, but much of the software industry cannot apply them. as testing processes become more complex, costly, and time consuming, the testing focus frequently narrows to functional testing.ł tools are not interoperable. this leads to more standards but, paradoxically, less standardization. less standardization, in turn, leads to decreased condence and lower levels of assurance. one remedy for this situation would have the following elements:ł government, in collaboration with industry and academia, works to raise expectations on product assurance. this would help to advance more comprehensive diagnostic capabilities, methodologies, and tools to mitigate risks. ł acquisition managers and users start to factor information about suppliers™ software development processes and the risks posed by the supply chain into their decision making and acquisition/delivery processes. information about evaluated products would become available, and products in use could be securely congured. ł suppliers begin to deliver quality products with requisite integrity and make assurance claims about their it and the software™s safety, security, and dependability. to do this, they would need to have and use relevant standards, qualied tools, independent thirdparty certiers, and a qualied workforce. it was suggested that software is an industry that demands only minimal levels of responsible practice compared to some other industries and that this is part of the challenge. but raising the level of responsible practice could increase sales to customers that demand highassurance products. from the perspective of the dhs, critical infrastructure around the united states is often not owned or operated by u.s. interests. as cyberspace and physical space become increasingly intertwined and softwarecontrolled or enabled, these interconnections and controls are often summary of a workshop on softwareintensive systems and uncertainty at scalecopyright national academy of sciences. all rights reserved.summary of workshop discussions 25implemented using the internet. this presents a targetrich environment especially given the asymmetries at work: according to one speaker, extrapolating from data on average defect rates, a deployed software package of a million lines of code will have 6,000 defects. even if only 1 percent of those defects introduce securityrelated vulnerabilities, then there are 60 different vulnerabilities for an adversary to exploit. in an era riddled with asymmetric cyberattacks, claims about system reliability, integrity, and safety must also address the builtin security of the enabling software. security is an enabler for reliability, integrity, and safety. cyberrelated disruptions have an economic and business impact because they can lead to the loss of money and time, delayed or cancelled products, and loss of sensitive information, reputation, even life. from a ceo/cio perspective, disruptions and security ˚aws can mean having to redeploy staff to deal with problems and increase it security, reduced end user productivity, delayed products, and unanticipated patch management costs. results from a survey of cios in 2006 by the cio executive council indicate that reliable and vulnerabilityfree software is a top priority. in that same survey respondents expressed ﬁlow to medium condenceﬂ that software is ﬁfree fromﬂ ˚aws, vulnerabilities, and malicious code. the majority of these cios would like vendors to certify and test software using qualied tools. speakers noted that the second national software summit had identied major gaps in requirements for tools and technologies, as well as major shortcomings in the stateoftheart and the stateofthe practice for developing errorfree software. a national software strategy was recommended in order to enhance the nation™s capability to routinely develop trustworthy software products and ensure the continued competitiveness of the u.s. software industry. this strategy focused on improving software trustworthiness, educating and elding the software workforce, reenergizing software r&d, and encouraging innovation in the u.s. industry.15 in addition to the gaps and shortcomings identied at that software summit, a recent pitac report on national priorities for cybersecurity listed security software engineering and software assurance as among the top ten goals.16 software assurance contributes to trustworthy software systems. the goals of the dhs software assurance (swa) program promote the security of software across its development, acquisition, and implementation 15 center for national software studies, 2005, ﬁsoftware 2015: a national software strategy to ensure u.s. security and competitiveness,ﬂ available online at www.cnsoftware.org/nss2report, april 29.16 president™s information technology advisory committee (pitac), 2005, cybersecurity: a crisis of prioritization, february. summary of a workshop on softwareintensive systems and uncertainty at scalecopyright national academy of sciences. all rights reserved.26 softwareintensive systems and uncertainty at scalelife cycles.17 the swa program is scoped to address trustworthiness, predictable execution, and conformance to requirements, standards, and procedures. it is structured to target people, process, technology, and acquisition.the swa program is processagnostic, providing practical guidance in assurance practices and methodologies for process improvement. a developer™s guide and glossary discussed during this session, securing the software life cycle, is not a policy or standard. instead, it focuses on touch points and artifacts throughout the life cycle that are foundational knowledge, best practices, and tools and resources for building assurance in. integrating security into the systems engineering life cycle enables the implementation of software assurance. it was suggested that software assurance would be wellserved by standards that assign names to practices or collections of practices. standards are needed to facilitate communication between buyer and seller, government and industry, insurer and insured. they are needed to improve information exchange and interoperability among practices and among tools. the goal is to close the gap between art and practice and raise the minimum level of responsible practice. some current standards efforts for software and system life cycle processes include iso sc7, iso sc22, iso sc27, and ieee s2esc. a critical aspect, it was suggested, is language: articulating structured assurance claims supported by evidence and reasoning. for example, the object management group (omg) has been working with industry and federal agencies to help collaboration in a common framework for the analysis and exchange of information related to software trustworthiness. this framework can be used for building and assembling software components, including legacy systems and large systems and networks: looking only at product evaluation overlooks the places where systems and networks are most vulnerable, because it is the interaction of all the components as installed that becomes the problem.one of the challenges often noted regarding standardization of practices is the lag between identication of a best practice and its codication into a standard. this is particularly challenging in areas such as software assurance, where there is rapid evolution of technologies, practices, and related standards. in the future, the goal is for customers to have expectations for product assurancešincluding information about evaluated products, suppliers™ process capabilities, and secure congurations of softwarešand for suppliers to be able to distinguish themselves by delivering quality products with the requisite integrity and to be able to make assurance claims based on relevant standards. 17 the mitre web site, http://www.cwe.mitre.org, can be used to track swa progress.summary of a workshop on softwareintensive systems and uncertainty at scalecopyright national academy of sciences. all rights reserved.summary of workshop discussions 27the national security agency center for assured software according to the historical perspective offered by one speaker, the dod assurance requirements of 30 years ago mostly focused on what became the national information assurance partnership (niap) and the trusted product evaluation program.18 developers were known and trusted intrinsically. by contrast, in today™s environment, the market for software is a global one: even u.s. companies are international. dod has become increasingly concerned about malicious intent. malicious code done very well is going to look like an accident. unfortunately, it was argued, assurance is gained today the same way as it was 30 years ago. various mechanisms for gaining condence in generalpurpose software are also being used for dod software: functional testing, penetration testing, design and implementation analysis, advanced development environments, trusted developers, process, discipline, and so forth are mechanisms to build condence. the intention is for the center for assured software to contribute to the advancement of measurable condence in software.in today™s environment, vendors do not have an incentive to be involved early in the design process, so testing typically is done after the fact, with a thirdparty orientation. the problem with this model is that it is all about penetration analysis, not building security in, and trust is bestowed by a third party. moreover, this model does not scale very well. in one speaker™s view, assurance models for comsec devices will not, for example, scale to the dod™s joint tactical radio system (jtrs) program. in addition, composition has always been a problem in the context of assurance: the current state of knowledge about how to compose systems well and to know what we have, with the inadequacy in assurance, is compounded by the problem of malicious intent.a challenge for nsa™s center for assured software is to be able to scale assurance. to do that, the future assurance paradigm needs to acknowledge the role of the development process in the assurance argument. how software is built, what processes are used, and what tools are used all have to be part of the assurance argument. that is a subtle but important shift in the paradigm. in the speaker™s view, the way to achieve scale in the development process and the way to gain assurance in the development process and in thirdparty analysis is by increasing the extent of automation. the cur18 niap is a u.s. government initiative originated to meet the security testing needs of both information technology consumers and producers and is operated by the nsa (see http://www.niapccevs.org/). the trusted product evaluation program (tpep) was started in 1983 to evaluate cots products against the trusted computer systems evaluation  criteria (tcsec). summary of a workshop on softwareintensive systems and uncertainty at scalecopyright national academy of sciences. all rights reserved.28 softwareintensive systems and uncertainty at scalerent paradigm does not embrace that means of achieving scale very well. previous measurement techniques mostly entailed humans looking for vulnerabilities. what the center for assured software is trying to do is nd correlations between assurance and positive things that can be measuredšfor instance, the properties that are importantšto give condence that the software is indeed built appropriately. another area where work is needed, it was suggested, is to create a science of composition that enables making an argument for levels of assurance at scale. in the mid1980s, there were attempts to do that with the trusted database management system interpretation of the trusted computer system evaluation criteria (often referred to as the orange book), but the results did not scale very well. participants mentioned a variety of ideas being pursued in industry and academia in response to business and government needs in the area of software assurance: anomaly identication, model checking, repeatable methodology for assurance analysis and evaluation, and intermediate representation of executable code.19 suggested research areas mentioned during this discussion include these: ł assurance composition,ł veriable compilation,ł software annotation,ł model checking, ł safe languages and automated migration from unsafe languages,ł software understanding, andł measurable attributes that have strong correlation to assurance. more broadly, participants suggested that it will be important to understand how to build condence from all of these (and other approaches) and to improve these approaches. in particular, it will be important to understand how they ﬁcombineﬂ (that is, what multiple techniques collectively convey regarding condence) since it is at best highly unlikely that one technique will ever by itself be sufcient.software assurance: present and future this vendorperspective presentation and discussion focused on cots (it was suggested that 80 percent of dod systems have at least some cots components) and on taking tactical, practical, and economical steps at the component level to improve assurance. as scale and interconnec19 other approaches include static analysis, extended static checkers, and rulebased  automatic code analysis.summary of a workshop on softwareintensive systems and uncertainty at scalecopyright national academy of sciences. all rights reserved.summary of workshop discussions 29tivity increase, it was argued that the assurance bar for software quality and cybersecurity attributes can be raised by (1) raising the component assurance bar (resources are nite and organizations can spend too much time and too many resources trying to patch their way to security) and (2) getting customers to understand and accept that assurance for custom software can be raised if they are willing to pay more (if customers do not know about costs that are hidden, they cannot accept or budget for them). one set of best practices and technologies to write secure software was described. it includesł secure coding standards,ł developer training in secure coding,ł enabled, embedded security points of contact (the ﬁmissionary modelﬂ),ł security as part of development including functional, design, test (include threat modeling),ł regressions (including destructive security tests),ł automated tools (home grown, commercial of multiple ˚avors),ł lockeddown congurations (delivering products that are secure on installation), andł release criteria for security.however, these practices are not routinely taught in universities. neither the software profession not the industry as a whole can simply rely on a few organizations doing these kinds of things. discussion identied some necessary changes in the long run: ł university curricula. it was argued that university programs should do a better job of teaching secure coding practices and training future developers to pay attention to security as part of software development. if the mindset of junior developers does not change, the problem will not be solved. one participant said, ﬁprocess won™t x stupidity or arrogance.ﬂ incentives to be mindful of security should be integrated throughout the curriculum. when security is embedded throughout the development process, a small core of security experts is not sufcient. one challenge is how to balance the university focus on enduring knowledge and skills against the need for developers to understand particular practices and techniques specic to current technologies. ł automation. automated tools are promising and will be increasingly important, but they are not a cureall. automated tools are not yet ready for universal prime time for a number of reasons, including: tools need to be trained to understand the code base; programmers have difsummary of a workshop on softwareintensive systems and uncertainty at scalecopyright national academy of sciences. all rights reserved.30 softwareintensive systems and uncertainty at scaleculty establishing sound and complete rules; most of today™s tools look only for anticipated vulnerabilities (e.g., buffer overruns) and cannot be readily adapted to new classes of vulnerabilities; there are often too many false positives; scalability is an issue; one size does not t all (it is premature for standards) and therefore multiple tools are needed; and there is not a good system for rating tools. conventional wisdom holds that people will not pay more for secure software. however, people already are paying for bad securityša 2002 study by the national institute of standards and technology (nist) reported that the consequences of bad software cost $59 billion a year in the united states.20 it was argued that from a development standpoint, security costeffectiveness should be measured pragmatically. however, a simple return on investment (roi) is not the right metric. from a developer™s perspective, the goal should be the highest net present value (npv) for cost avoidance of future bugsšnot raw cost savings or the roi from xing bugs in the current year. another way of valuing security is opportunity cost savingsšwhat can be done (e.g., building new features) with the resources saved from not producing patches. from the customer™s perspective, it is the lifecycle cost of applying a patch weighed against the expected cost of the harm from not applying the patch. customers want predictable costs, and the perception is that they cannot budget for the cost of applying patches (even though the real cost driver is the consequences of unpatched systems). if customers know what they are getting, they can plan for a certain level of risk at a certain cost. the goal is to nd the match between expected risk for the customer and for the vendoršhow suitable the product is for its intended use. certication is a way of assessing what is ﬁgood.ﬂ21 but participants were not optimistic when considering prospects for certication of development processes. there is too much disagreement and ideology surrounding development processes. however, there can be some commonality around aspects of good development processes. certifying developers is also problematic. in engineering, there are accredited degree programs and clear licensing requirements. the awarding of a degree in computer science is not analogous to licensing an engineer because there is not the same common set of requirements, especially robustness and safety requirements. in addition, it can be difcult to replicate the results 20 see nist, 2002, ﬁplanning report 023: the economic impacts of inadequate infrastructure for software testing.ﬂ available online at http://www.nist.gov/ director/progofc/report023.pdf.21 a recent nrc study examines the issue of certication and dependability of software systems. see information on the report software for dependable systems: sufcient evidence? at http://cstb.org/projectdependable.summary of a workshop on softwareintensive systems and uncertainty at scalecopyright national academy of sciences. all rights reserved.summary of workshop discussions 31of software engineering processes, making it hard to achieve condence such that developers are willing to sign off on their work. moreover, it was argued that with current curricula, developers generally do not even learn the basics of secure coding practice. there is little to no focus on security, safety, or the possibility that the code is going to be attacked in most educational programs. it was argued that curricula need to change and that computer science graduates should be taught to ﬁassume an enemy.ﬂ  automated tools can give better assurance to the extent that vendors use them in development and x what they nd. running evaluation tools after the fact on something already in production is not optimal.22 it was suggested that there is potential for some kind of ﬁgoodness meterﬂ (a complement to the ﬁbadness meterﬂ described in the next section) for tool use and effectivenessšwhat tool was used, what the tool can and cannot nd, what the tool did and did not nd, the amount of code covered, and that tool use was veried by a third party. software security: building security in discussions in this session focused on software security as a systems problem as opposed to an application problem. in the current state of the practice, certain attributes of software make software security a challenge: (1) connectivityšthe internet is everywhere and most software is on it or connected to it; (2) complexityšnetworked, distributed, mobile code is hard to develop; and (3) extensibilityšsystems evolve in unexpected ways and are changed on the ˚y. this combination of attributes also contributes to the rise of malicious code. massively multiplayer online games (mmogs) are bellwethers of things to come in terms of sophisticated attacks and exploitation of vulnerabilities. these games experience the cutting edge of what is going on in software hacking and attacks today.23 attacks against such games are 22 it was suggested that vendors should not be required to vet products against numerous tools. it was also suggested that there is a need for some sort of common criteria reform with mutual recognition in multiple venues, eliminating the need to meet both common criteria and testing requirements. vendors, for example, want to avoid having to give governments the source code for testing, which could compromise intellectual property, and want to avoid revealing specics on vulnerabilities (which may raise security issues and also put users of older versions of the code more at risk). common criteria is an international standard for computer security. documentation for it can be found at http://www.niapccevs.org/ccscheme/ccdocs/.23 world of warcraft, for example, was described as essentially a global information grid with approximately 6 million subscribers and half a million people playing in real time at any given time. it has its own internal market economy, as well as a signicant black market economy. summary of a workshop on softwareintensive systems and uncertainty at scalecopyright national academy of sciences. all rights reserved.32 softwareintensive systems and uncertainty at scalealso at the forefront of socalled rootkit24 technology. examining attacks on largescale games may be a guide to what is likely to happen in the nongame world. it was suggested that in 2006, security started to become a differentiator among commercial products. around that time, companies began televising ads about security and explicitly offering security features in new products. customers were more open to the idea of using multiple vendors to take advantage of diversity in features and suppliers. security problems are complicated. there is a difference between implementation bugs such as buffer over˚ows or unsafe systems calls, and architectural ˚aws such as compartmentalization problems in design or insecure auditing. as much attention needs to be paid to looking for architectural or requirements ˚aws as is paid to looking for coding bugs. although progress is being made in automation, both processes still need people in the loop. when a tool turns up bugs or ˚aws, it gives some indication of the ﬁbadnessﬂ of the codeša ﬁbadnessometerﬂ of sorts. but when use of a tool does not turn up any problems, this is not an indication of the ﬁgoodnessﬂ of the code. instead, one is left without much new knowledge at all.participants emphasized that software security is not application security. software is everywherešnot just in the applications. software is in the operating system, the rewall, the intrusion detection system, the public key infrastructure, and so on. these are not ﬁapplications.ﬂ application security methods work from the outside in. they work for cots software, require relatively little expertise, and are aimed at protecting installed software from harm and malicious code. system software security works from the inside out, with input into and analysis of design and implementation, and requires a lot of expertise. in one participant™s view, security should also be thought of as an emergent property of software, just like quality. it cannot be added on. it has to be designed in. vendors are placing increased emphasis on security, and most customers have a group devoted to software security. it was suggested that the tools market is growing, for both application security (a market of between $60 million and $80 million) and software security (a market of about $20 million, mostly for static analysis tools). consulting services, however, dwarf the tools market. one speaker described the ﬁthree pillarsﬂ of software security:24 a rootkit is a set of software tools that can allow hackers to continue to gain undetected, unauthorized access to a system following an initial, successful attack by concealing processes, les, or data from the operating system. summary of a workshop on softwareintensive systems and uncertainty at scalecopyright national academy of sciences. all rights reserved.summary of workshop discussions 33ł risk management, tied to the mission or line of business. financial institutions such as banks and credit card consortiums are in the lead here, in part because sarbanesoxley made banks recognize their software risk. ł touchpoints, or best practices. the top two are code review with a tool and architectural risk analysis. ł knowledge, including enterprise knowledge bases about security principles, guidelines, and rules; attach patterns; vulnerabilities; and historical risks. session 5: enterprise scale and beyondpanelists: werner vogels, amazon.com, and alfred spector, azsservicesmoderator: jim larusthe speakers at this session focused on the following topics, from the perspective of industry: ł what are the characteristics of successful approaches to addressing scale and uncertainty in the commercial sector, and what can the defense community learn from this experience? ł what are the emerging software challenges for largescale enterprises and interconnected enterprises? ł what do you see as emerging technology developments that relate to this?life is not a statemachine:  the long road from research to productiondiscussions during this session centered on largescale web operations, such as that of amazon.com, and what lessons about scale and uncertainty can be drawn from them. it was argued that in some ways, software systems are similar to biological systems. characteristics and activities such as redundancy, feedback, modularity, loose coupling, purging, apoptosis (programmed cell death), spatial compartmentalization, and distributed processing are all familiar to softwareintensive systems developers, and yet these terms can all be found in discussions of robustness in biological systems. it was suggested that there may be useful lessons to be drawn from that analogy. amazon.com is very large in scale and scope of operations: it has seven web sites; more than 61 million active customer accounts and over 1.1 million active seller accounts, plus hundreds of thousands of summary of a workshop on softwareintensive systems and uncertainty at scalecopyright national academy of sciences. all rights reserved.34 softwareintensive systems and uncertainty at scaleregistered associates; over 200,000 registered web services developers; over 12,500 employees worldwide; and more than 20 fulllment centers worldwide. about 30 percent of amazon™s sales are made by thirdparty sellers; almost half of its sales are to buyers outside the united states. on a peak shipping day in 2006, amazon made 3.4 million shipments. amazon.com™s technical challenges include how to manage millions of commodity systems, how to manage many very large, geographically dispersed facilities in concert, how to manage thousands of services running on these systems, how to ensure that the aggregate of these services produces the desired functionality, and how to develop services that can exploit commodity computing power. it, like other companies providing similar kinds of services, faces challenges of scale and uncertainty on an hourly basis.over the years, amazon has undergone numerous transformationsšfrom retailer to technology provider, from single application to platform, from web site and database to a massively distributed parallel system, from web site to web service, from enterprise scale to web scale. amazon™s approach to managing massive scale can be thought of as ﬁcontrolled chaos.ﬂ it continuously uses probabilistic and chaotic techniques to monitor business patterns and how its systems are performing. as its lines of business have expanded these techniques have had to evolvešfor example, focusing on tracking customer returns as a negative metric does not work once product lines expand into clothing (people are happy to order multiple sizes, keep the one that ts, and return the rest). amazon builds almost all of its own software because the commercial and open source infrastructure available now does not suit amazon.com™s needs. the old technology adoption life cycle from product development to useful acceptance was between 5 and 25 years. amazon and similar companies are trying to accelerate this cycle. however, it was suggested that for an amazon developer to select and use a research technology is almost impossible. in research, there are too many possibilities to choose from, experiments are unrealistic compared to real life, and underlying assumptions are frequently too constrained. in real life, systems are unstable, parameters change and things fail continuously, perturbations and disruptions are frequent, there are always malicious actors, and failures are highly correlated. in the real world, when the system fails, the mission of the organization cannot stopšit must continue.25 often, complexity is introduced to manage uncertainty. however, there may well exist what one speaker called ﬁconservation laws of complexity.ﬂ that is, in a complex interconnected system, complexity cannot 25 examples of systems where assumptions did not match real life include the titanic, the tacoma narrows bridge, and the estonian ferry disaster.summary of a workshop on softwareintensive systems and uncertainty at scalecopyright national academy of sciences. all rights reserved.summary of workshop discussions 35be reduced absolutely, it can only be moved around. if uncertainty is not taken into account in large scale system design, it makes adoption of the chosen technology fairly difcult. engineers in real life are used to dealing with uncertainty. assumptions are often added to make uncertainty manageable. at amazon, the approach is to apply occam™s razor: if there are competing systems to choose from, pick the system that has the fewest assumptions. in general, assumptions are the things that are really limiting and could limit the system™s applicability to real life.two different engineering approaches were contrasted, one with the goal of building the best possible system (the ﬁrightﬂ system) whatever the cost, and the other with the more modest goal of building a smaller, lessambitious system that works well and can evolve. the speaker characterized the former as being incredibly difcult, taking a long time and requiring the most sophisticated hardware. by contrast, the latter approach can be faster, it conditions users to expect less, and it can, over time, be improved to a point where performance almost matches that of the best possible system. it was also argued that traditional management does not work for complex software development, given the lack of inspection and control. control requires determinism, which is ultimately an illusion. amazon™s approach is to optimize team communication by reducing team size to maximum of 810 people (a ﬁtwopizza teamﬂ). for larger problems, decompose the problem and reduce the size of the team needed to tackle the subproblems to a twopizza group. if this cannot be done, it was suggested, than do not try to solve that problemšit™s too complicated. a general lesson that was promoted during this session was to let go of control and the notion that these systems can be controlled. large systems cannot be controlledšthey are not deterministic. for various reasons, it is not possible to consider all the inputs. some may not have been included in the original design; requirements may have changed; the environment may have changed. there may be new networks and/or new controllers. the problem is not one of control; it is dealing with all the interactions among all the different pieces of the system that cannot be controlled. amazon.com™s approach is to let these systems mature incrementally, with iterative improvement to yield the desired outcome during a given time period. the old, the old, and the newin this session™s discussions, the rst ﬁoldﬂ was the principle of abstractionencapsulationreuse. reuse is of increasing importance everywhere as the sheer quantity of valuable software components continues to grow. the second ﬁoldﬂ was the repeated quest (now using web services summary of a workshop on softwareintensive systems and uncertainty at scalecopyright national academy of sciences. all rights reserved.36 softwareintensive systems and uncertainty at scaleand increasingly sophisticated software tools) to make component reuse and integration the state of practice. progress is being made in both of these areas, as evidenced by investment and anecdotes. the ﬁnewﬂ discussed was the view that highly structured, highly engineered systems may have signicant limitations. accordingly, it was argued, ﬁsemantic integration,ﬂ more akin to internet search, will play a more important role. there are several global integration agendas. some involve broad societal goals such as trade, education, health care, and security. at the rm or organization level, there is supply chain integration and n to 1 integration of many stores focusing on one consumer, as in the case amazon and its many partners and vendors. in addition, there is collaborative engineering, multidisciplinary r&d, and much more. why is global integration happening? for one thing, it is now technically possible, given ubiquitous networking, faster computers, new software methodologies. people, organizations, computation, and development are distributed, and networked systems are now accepted as part of life and business, along with the concomitant benets and risks (including security risks). an emerging trend is the drive to integrate these distributed people and processes to get efciency and costeffective development derived from reuse. another factor is that there are more software components to integrate and assemble. pooling of the world™s software capital stock is creating heretofore unimaginably creative applications. software is a major element of the economy. it was noted that by 2004, the amount of u.s. commercial capital stock relating to software, computer hardware, and telecommunications accounted for almost onequarter of the total capital stock of business; about 40 percent of this is software. software™s real value in the economy could even be understated because of accounting rules (depreciation), price decreases, and improvements in infrastructure and computing power. the it agenda and societal integration reinforce each other. core elements of computer science, such as algorithms and data structures, are building blocks in the integration agenda. the eld has been focusing more and more on the creation and assembly of larger, more ˚exible abstractions. it was suggested that if one accepts that the notion of abstractionencapsulationreuse is central, then it might seem that serviceoriented computing is a done deal. however, the challenge is in the details: how can the benets of the integration agenda be achieved throughout society? how are technologists and developers going to create these large abstractions and use them? when the internet was developed, some detailsšsuch as quality of service and securityšwere left undone. similarly, there are open chalsummary of a workshop on softwareintensive systems and uncertainty at scalecopyright national academy of sciences. all rights reserved.summary of workshop discussions 37lenges with regard to integration and serviceoriented approaches. what are the complete semantics of services? what security inheres in the service being used? what are the failure modes and dependencies? what is the architectural structure of the world™s core it and application services? how does it all play out over time? what is this hierarchy that occurs globally or, for the purposes of this workshop, perhaps even within dod or within one of the branches of the military? serviceoriented computing is computing whereby one can create, ˚exibly deploy, manage, meter and charge for (as appropriate), secure, locate, use, and modify computer programs that dene and implement wellspecied functions, having some general utility (services), often recursively using other services developed and deployed across time and space, and where computing solutions can be built with a heavy reliance on these services. progress in serviceoriented computing brings together information sharing, programming methodologies, transaction processing, open systems approaches, distributed computing technologies, and web technologies. there is now is a huge effort on the part of industry to develop applicationlevel standards. in this approach, companies are presented with the denition of some structure that they need to use to interoperate with other businesses, rather than, for example, having multiple individual efdoms within each company develops unique customer objects.the web services approach generally implies a set of services that can be invoked across a network. for many, web services comprise things such as extensible markup language (xml) and soap (a protocol for exchanging xmlbased messages over computer networks) along with a variety of web service protocols that have now been dened and are heavily used, developed, produced, and standardized (many in a partnership between ibm and microsoft). web services are on the path to fullscale, serviceoriented computing; it was argued that this path can be traced back to the 1960s and the airlines™ sabre system, continuing through arpanet, the internet, and the modern world wide web.web services based on abstractionencapsulationreuse are a new approach to applying structureoriented engineering tradition to information technology (it). for example, integration steps include the precise denition of function (analogous to the engineering specications and standards for transportation system construction), architecture (analogous to bridge design, for example), decomposition, rigorous component production (steel beams, for example), careful assembly, and managed change control. the problem is, there may be limits to this at scale. in software, each of these integration steps is difcult in itself. many projects are inherently multiorganizational, and rapid changes have dire consequences for traditional waterfall methodologies. summary of a workshop on softwareintensive systems and uncertainty at scalecopyright national academy of sciences. all rights reserved.38 softwareintensive systems and uncertainty at scaleit was argued that ﬁsemantic integration,ﬂ a dynamic, fuzzier integration more akin to internet search, will play a larger role in integration than more highly structured engineering of systems. ad hoc integration is a more humble approach to servicebased integration, but it is also more dynamic and interpretive. components that are integrated may be of lower generality (not a universal object) and quality (not so well specied). because they will be of lower generality, perhaps with different coordinate systems, there will have to be automated impendence matching between them. integration may take place on an intermediate service, perhaps in a browser. businesses are increasingly focusing on this approach for the same reasons that simple approaches have always been favored. this is a core motivational component of the web 2.0 mashup focus. another approach to ad hoc integration uses access to massive amounts of informationšwith no reasonable set of predened, parameterized interfaces, annotation and search will be used as the integration paradigm.it is likely that there will be tremendous growth in the standards needed to capitalize on the large and growing it capital plant. there will be great variability from industry to industry and from place to place around the world, depending on the roles of the industry groups involved, differential regulations, applicable types of open source, and national interests. partnerships between the it industry and other industries will be needed to share expertise and methodologies for creating usable standards, working with competitors, and managing intellectual property.a number of topics for serviceoriented systems and semantic integration research were identied, some of which overlap with traditional software system challenges. the serviceoriented systems research areas and semantic integration research areas spotlighted included these: ł basics. is there a, practical, normative general theory of consistency models? are services just a remote procedure call invocation or a complex split between client and server? how are security and privacy to be provided for the real world, particularly if one does not know what services are being called? how does one utilize parallelism? this is an increasingly important question in an era of lessening geometric clockspeed growth.ł management. with so many components and so much information hiding, how does one manage systems? how does one manage intellectual property?ł global properties. can one provide scalability generally? how does one converge on universality and standards without bloat? what systems can one deploy as really universal service repositories?summary of a workshop on softwareintensive systems and uncertainty at scalecopyright national academy of sciences. all rights reserved.summary of workshop discussions 39ł economics. what are realistic costing/charging models and implementations?ł social networking. how does one apply social networking technology to help?ł ontologies of vast breadth and scale.ł automated discovery and transformation.ł reasoning in the control ˚ow.ł use of heuristics and redundancy.ł search as a new paradigm.complexity grows despite all that has been done in computer science. there is valuable, rewarding, and concrete work for the eld of computer science in combating complexity. this area of work requires focus. it could prove as valuable as direct functional innovation. participants identied several research areas to address complexity relevant to serviceoriented systems and beyond, including: meaning, measuring, methodology, system architecture, science and technology, evolutionary systems design, and legal and cultural change.summary of a workshop on softwareintensive systems and uncertainty at scalecopyright national academy of sciences. all rights reserved.403wrapup discussion and  emergent themesseveral major themes emerged from the day™s discussions on the challenges of developing futureoriented, largescale systems that can cope with uncertainty at scale.1 these themes are not ndings or recommendations of the study committeešthose will be presented in the committee™s nal report later in the study. indeed, observations offered in some sessions contradicted those from other sessionsšwhich perhaps re˚ects the notion that there are different kinds of very largescale systems and that different kinds of systems will likely warrant different sorts of approaches. instead, this section presents a brief overview of the interrelated themes that arose over the course of the workshop™s discussions: ł architectural challenges in largescale systems,ł the need for software engineering capability, andł open questions and research opportunities.architectural challenges in largescale systemsthe issue of architecture and frameworks for largescale softwareintensive systems coping with uncertainty at scale was raised repeatedly. one approach to this problem that was put forward in discussions would 1because neither the workshop nor this summary was intended to be (or constituted as) a standalone product, contradictory views also emerged from different presenters during the dayšfor example, the desirability of not producing software inhouse versus the desirability of producing all software inhouse. summary of a workshop on softwareintensive systems and uncertainty at scalecopyright national academy of sciences. all rights reserved.wrapup discussion and emergent themes 41be to develop executable models and architectures that can evolve over the system™s life cycle. as the system develops and evolves, pieces that were originally mockups could be replaced with actual subsystemsšthis would be a way to provide ongoing feedback on the architecture, including system performance, functionality, and so on. a broader question asks what kind of structure and constraints can be imposed at a high level such that the overall system can be decomposed (as needed) into autonomous pieces. what specic architectural rules and approaches will get one there? it was suggested that a framework would be needed, along with clear specications at the interface between the framework and components.closely tied to the question of architecture is the signicant challenge of how to develop systems architectures and denitions for highly decentralized organizations. for developing the sorts of systems under consideration, some believed a loosely coupled organizational style would be needed as systems scale up and more players enter the picture. although such a shift might place intense pressure on organizational culture and management, ﬁcontrolled chaosﬂ rather than a very topdown, structured, and controlled approach might need to become the order of the day. this shift, however, might re˚ect a change of perspective on the essential commonalities that hold an overall system togetheršthere could, for example, be a shift from an overall structural model as the unifying factor to particular agreements on how components and subsystems are to interact with each other through protocols, apis, metadata standards, and the like. it may also be the case that there is an underlying issue driving this tension that is not about coupling or control, but rather about the particular nature of the architectural commonalities that hold a system together. for example, the trend towards dynamic architecture demonstrates that purely structural commonalities are giving way to semantic and other less apparentšbut perhaps more essentialšcommonalities in large systems.this has implications for software engineering capability (see below), in part because the frameworks and architecture for these systems will not go awayšthey and the people involved with them will need to persist for the lifetime of the system. and, of course, the architecture for these systems will transcend particular individual suppliers. the need for software engineering capabilitywriting large, safetycritical, realtime systems requires a commitment to genuine engineering discipline, even if it means constraining the design spacešlimiting ˚exibilityšin order to conform to precedented practices that enable application of this discipline. in addition, management becomes a signicant challenge when it comes to very largescale systems summary of a workshop on softwareintensive systems and uncertainty at scalecopyright national academy of sciences. all rights reserved.42 softwareintensive systems and uncertainty at scalethat will need to depend on very large networks and supply chains,. it was suggested that this may lead to a focus on communitystyle collaboration and integration over the long term. in any event, best practices will differ depending on context, including the type of organization as well as the type of application or system under development. participants noted that process practices will merit attention as well as technological practices. process will not solve everything, of course, but process is vital to assist with, among other things, the decomposition of large systems into incremental subsets for better visibility and reduced risk. it was noted that the focus on reuse may shift to earlier stages of the process and particularly to requirements. in general, supporting technology will be needed to enable the types of architectures and collaborations that largescale systems merit. in particular, nding ways to increase the extent of abstraction and automation in all aspects of software design, implementation, testing, maintenance, and so on, may be a productive avenue, especially with regard to scale and geographic distribution. tools and strategies for coping with design and architectures for very large, physically distributed teams of people and organizations will be increasingly important along with techniques and approaches that support high levels of trust. in addition, participants noted that the ability to use analytic techniques (such as model checking, static analysis, dynamic analysis, and so on) will be important. these types of tools are being used much more now than 5 or 10 years ago, but they are typically still earlygeneration tools. continued research and investment (see below) will help improve and extend them. even so, tools will not solve the problems of assurance or of largescale system development. process, expertise, and skills matter a great deal. open questions and research opportunitiesultimately, many of the challenges related to architecture and capability re˚ect problems that the community does not yet know how to solve. over the course of the workshop, participants articulated several open research questions that should be addressed to make progress in addressing uncertainty at scale for softwareintensive systems. at a high level, the architectural and organizational challenges presented by largescale systems merit investigation: what are the key kinds of commonalities that manifest architectural commitment, beyond a topdown structural design? industry issues were among the other topics that were spotlighted in the wrapup discussion. these included (1) the extent to which serviceoriented architecture (soa) and soa vendors will or will not make progress over the next 18 to 24 months in addressing dod™s need for producibility at scalešfor example, in contrast to the wellestablished enterprise resource planning framework and application servers offered summary of a workshop on softwareintensive systems and uncertainty at scalecopyright national academy of sciences. all rights reserved.wrapup discussion and emergent themes 43by vendorsšand (2) the extent to which defense contractors can allocate resources to address software challenges that fall outside current contract parameters. contracting issues that were noted in the discussion included (1) how to establish collaborative mechanisms for contractors and the dod to work together, particularly in iterative fashion, on software assurance and riskreduction problems, as well as (2) contracting complexities related to the integration of large software systems that include cots subsystems for the dod. another industry issue noted in the discussion was industry™s misgivings about the availability of computer science and computer engineering new hires at the bachelor™s and master™s degree level. related issues were software curricula development and the appropriateness of accreditation for software engineering and computer engineering and the need for computer science and computer engineering faculty and students to have handson industry experience in building systems.* * *discussions at the workshop suggested that many of the key ideas needed to make progress in developing largescale systems and coping with uncertainty at scale will not be found through the traditional incremental advances made in various segments of the industry. while there are lessons to be learned and gleaned from the varieties of experience and perspective presented, the types of systems that are envisioned and that serve as a driver for dod™s interests in software pose signicant management, intellectual, and research challenges. summary of a workshop on softwareintensive systems and uncertainty at scalecopyright national academy of sciences. all rights reserved.summary of a workshop on softwareintensive systems and uncertainty at scalecopyright national academy of sciences. all rights reserved.appendixessummary of a workshop on softwareintensive systems and uncertainty at scalecopyright national academy of sciences. all rights reserved.summary of a workshop on softwareintensive systems and uncertainty at scalecopyright national academy of sciences. all rights reserved.47a workshop agendajanuary 17, 2007 washington, d.c.9:009:15 a.m. opening remarks committee on advancing softwareintensive systems producibility, william scherlis, chair9:1510:30 session 1: process, architecture, and the grand   scale moderator: michael cusumano john vu, boeing rick selby, northrop grumman corporation  what are characteristics of successful approaches to architecture and design for largescale systems and families of systems? what are architecture ideas that can apply when systems must evolve rapidly? what kinds of management and measurement approaches could assist in guiding program managers and developers?10:3010:45 break10:45noon session 2: dod software challenges for future   systems moderator: douglas schmidt kristen baldwin, ofce of the undersecretary of defense for  acquisitions, technology and logistics patrick lardieri, lockheed martin summary of a workshop on softwareintensive systems and uncertainty at scalecopyright national academy of sciences. all rights reserved.48 softwareintensive systems and uncertainty at scale how are challenges for software in dod systems, particularly cyberphysical systems, being met in the current environment? what advances in r&d, technology, and practice do we need to achieve success as demands on this capability increase, particularly with respect to scale, complexity, and the increasingly rapid rate of evolution in requirements (and threat)?  noon12:45 p.m. lunch 12:451:20  session 3: agility at scale moderator: douglas schmidt kent beck, three rivers institute  cynthia andres, three rivers institute how can the engineering and management values that the ﬁagile communityﬂ has identied be achieved for largerscale systems and for systems that must evolve in response to rapidly changing requirements?1:202:30 session 4: quality and assurance with scale and   uncertainty moderator: william scherlis joe jarzombek, department of homeland security  kris britton, national security agency  what are the particular challenges for achieving particular assurances for software quality and cybersecurity attributes as scale and interconnection increase? what are emerging best practices and technologies? what kinds of new technologies and practices could assist? this includes, particularly, interventions that can be made as part of the development process, rather than after the fact. interventions could include practices, processes, tools, and so on. how should costeffectiveness be assessed? what are the prospects for certication, both at a comprehensive level and with respect to particular critical quality attributes? 2:302:45 breaksummary of a workshop on softwareintensive systems and uncertainty at scalecopyright national academy of sciences. all rights reserved.appendix a 492:454:00 session 4 (continued) mary ann davidson, oracle corporation  gary mcgraw, cigital 4:005:15 session 5: enterprise scale and beyond moderator: jim larus werner vogels, amazon.com  alfred spector, azsservices what are the characteristics of successful approaches to addressing scale and uncertainty in the commercial sector, and what can the defense community learn from this experience? what are the emerging software challenges for largescale enterprises and interconnected enterprises? what do you see as emerging technology developments that relate to this?5:156:00 closing discussion moderator: william scherlis all workshop participants and attendees6:00 adjournsummary of a workshop on softwareintensive systems and uncertainty at scalecopyright national academy of sciences. all rights reserved.50b biosketches of committee  members and staffcommittee memberswilliam l. scherlis, chair, is a full professor in the school of computer science at carnegie mellon. he is the founding director of cmu™s doctoral program in software engineering and director of its international software research institute. his research relates to software assurance, software evolution, and technology to support software teams. dr. scherlis joined the cmu faculty after completing a ph.d. in computer science at stanford university, a year at the university of edinburgh (scotland) as a john knox fellow, and an a.b. at harvard university. he was the lead principal investigator of the 4year high dependability computing project, in which cmu leads a collaboration with ve universities to help nasa address longterm software dependability challenges. he is also copi (with two colleagues) of a new project with nasa and diverse industry and laboratory subcontractors focused on dependable realtime and embedded software systems. dr. scherlis is involved in a number of activities related to technology and policy, recently testifying before congress on innovation and information technology and, previously, on roles for a federal chief information ofcer. he interrupted his career at cmu to serve at darpa for 6 years, departing in 1993 as senior executive responsible for coordination of software research. while at darpa he had responsibility for research and strategy in computer security, aspects of highperformance computing, information infrastructure, and other topics. dr. scherlis is a member of the nrc™s committee on improving cybersecurity research in the united states and the darpa information summary of a workshop on softwareintensive systems and uncertainty at scalecopyright national academy of sciences. all rights reserved.appendix b 51science and technology study group. he recently nished chairing a nrc study on information technology, innovation, and egovernment. he has led or participated in national studies related to cybersecurity, crisis response, analyst information management, dod software management, and health care informatics infrastructure. he has been an advisor to major it companies. he served as program chair for a number of technical conferences, including the acm foundations of software engineering symposium. he has more than 70 scientic publications.robert f. behler is a senior vice president in the mitre corporation command and control center for programs and advanced command and control. the center serves mitre™s dod sponsors and focuses on creating a joint command, control, and communications system. mr. behler leads the center™s work for dod sponsors. before joining mitre in april 2006, mr. behler was general manager of precision engagement at johns hopkins university™s applied physics laboratory. in this position he supervised over 250 scientists and engineers working on advanced command, control, intelligence, surveillance, and reconnaissance (c2isr) programs for the dod. under mr. behler™s leadership, the precision engagement organization turned new and emerging technologies into transformational operational capabilities. mr. behler retired from the air force as a major general in 2003. during his distinguished 31year career, he accumulated extensive experience managing and developing advanced command, control, communications, computers, intelligence, surveillance, and reconnaissance (c4isr) technologies at all levels. before retiring, mr. behler was commander of the air force c2isr center at langley air force base, where he was principal c2isr advisor to the secretary and chief of staff of the air force. prior to that, he served as deputy commander of nato joint headquarters north in stavanger, norway, and was the senior u.s. military ofcer in scandinavia. he has also served as director of command, control, communication, computers, and intelligence at the u.s. strategic command at offutt air force base and as chief of the u.s. air forcesenate liaison ofce. mr. behler entered the air force in 1972 as a distinguished graduate of the air force reserve ofcer training corps program at the university of oklahoma. he received his bachelor™s and master™s degrees in aerospace engineering from the university of oklahoma in 1970 and 1972, respectively. he is a graduate of the u.s. air force test pilot school at edwards air force base and has accumulated over 5,000 ˚ying hours in more than 65 aircraft types, including the sr71 and u2. he was a national security fellow at harvard university™s john f. kennedy school of government in 1990 and received a master™s degree in business administration from marymount university in 1991. he is an summary of a workshop on softwareintensive systems and uncertainty at scalecopyright national academy of sciences. all rights reserved.52 softwareintensive systems and uncertainty at scaleassociate fellow of the society of experimental test pilots and a member of the armed forces communications and electronics association.barry w. boehm, nae, is trw professor of software engineering and director, center for software engineering, at the university of southern california. dr. boehm received his b.a. degree from harvard university in 1957, and his m.s. and ph.d. degrees from university of california, los angeles, in 1961 and 1964, all in mathematics. he also received an honorary sc.d. in computer science from the university of massachusetts in 2000. between 1989 and 1992, he served at the dod as director of the darpa information science and technology ofce, and as director of the ddr&e software and computer technology ofce. he worked at trw from 1973 to 1989, culminating as chief scientist of the defense systems group, and at the rand corporation from 1959 to 1973, culminating as head of the information sciences department. he was a programmeranalyst at general dynamics between 1955 and 1959. his current research interest focus on valuebased software engineering, including a method for integrating a software system™s process models, product models, property models, and success models called modelbased (system) architecting and software engineering (mbase). his contributions to the eld include the constructive cost model (cocomo), the spiral model of the software process, the theory w (winwin) approach to software management and requirements determination, the foundations for the areas of software risk management and software quality factor analysis, and two advanced software engineering environments: the trw software productivity system and quantum leap environment. he has served on the boards of several scientic journals, including the ieee transactions on software engineering, ieee computer, ieee software, acm computing reviews, automated software engineering, software process, and information and software technology. he has served as chair of the aiaa technical committee on computer systems, chair of ieee technical committee on software engineering, and as a member of the governing board of the ieee computer society. he has served as chair of the air force scientic advisory board™s information technology panel, chair of the nasa research and technology advisory committee for guidance, control, and information processing, and chair of the board of visitors for the cmu software engineering institute. lori a. clarke is a professor of computer science at the university of massachusetts, amherst. she is an acm fellow, vice chair of the computing research association™s board of directors, and a member of the craw. she is a former ieee distinguished visitor, acm national lecturer, ieee publication board member, associate editor of acm toplas and ieee summary of a workshop on softwareintensive systems and uncertainty at scalecopyright national academy of sciences. all rights reserved.appendix b 53tse, member of the ccr nsf advisory board, acm sigsoft secretary/treasurer, vicechair and chair, as well as a 1990 recipient of the university of massachusetts™ chancellor™s medal, and a 1993 recipient of a university faculty fellowship. dr. clarke has worked in the area of software engineering, particularly on software analysis and testing, for many years. she was one of the primary developers of symbolic execution, a technique used to reason about the behavior of software systems and for selecting test data, and made contributions in the areas of software architecture and object management. recently her work has focused on analysis of concurrent systems. with colleagues, she has developed flavers, a static analysis tool that uses data˚ow analysis techniques to verify userspecied properties. flavers automatically creates a concise, but perhaps imprecise, model of the software system and then allows users to selectively improve the accuracy of the program model as needed to improve the accuracy of the results. the propel system complements flavers and other eventbased, nitestate verication systems by helping users elucidate the details of the properties to be proven. flavers allows users to simultaneously view and construct properties from templates of englishlanguage phrases or nitestate automata. the longterm goal is to develop techniques that welltrained software engineers can use to improve the quality of software systems. she received her b.a. in mathematics (1969) from the university of rochester and her ph.d. in computer science (1976) from the university of colorado.michael a. cusumano is the sloan management review™s distinguished professor at the massachusetts institute of technology™s sloan school of management. he specializes in strategy, product development, and entrepreneurship in the computer software industry, as well as automobiles and consumer electronics. he teaches courses on strategic management, innovation and entrepreneurship, and the software business. he has consulted for some 50 major companies around the world. he has been a director of numega technologies (sold to compuware in 1998 for $150 million) and innium software (sold to ssa global technologies in 2002 for $105 million), as well as other private and public software companies. he is currently a director of patni computer systems (software outsourcing based in india) and entigo (warrantee management software) and an advisor to netnumina solutions (internet architecture and custom solutions), rstrain (wireless and web services software), h5 technologies (digital search technology), and sigma technology group plc (earlystage ventures). he has also served as editorinchief and chairman of the mit sloan management review and writes periodically for communications of the acm, the wall street journal, computerworld, the washington post, and other publications. dr. cusumano has published eight books. his summary of a workshop on softwareintensive systems and uncertainty at scalecopyright national academy of sciences. all rights reserved.54 softwareintensive systems and uncertainty at scalelatest book, the business of software: what every manager, programmer, and entrepreneur must know to thrive and survive in good times and bad, was published in march 2004. dr. cusumano received a b.a. degree from princeton in 1976 and a ph.d. from harvard in 1984. he completed a postdoctoral fellowship in production and operations management at the harvard business school from 1984 to 1986. he is ˚uent in japanese and lived and worked in japan for 7 years. he received two fulbright fellowships and a japan foundation fellowship for studying at tokyo university.mary ann davidson is the chief security ofcer at oracle corporation, responsible for security evaluations, assessments, and incident handling. as a senior executive in the it industry she brings both a military and a business background and indepth experience with and perspective on industrial capacity to respond to defense needs. she represents oracle on the board of directors of the information technology information security analysis center (itisac) and is on the editorial review board of the secure business quarterly. ms. davidson has a b.s.m.e. from the university of virginia and an m.b.a. from the wharton school of the university of pennsylvania. she has also served as a commissioned ofcer in the u.s. navy civil engineer corps, where she was awarded the navy achievement medal.larry druffel recently retired as president and ceo of scra, a public, nonprot research and development corporation engaged in the application of advanced technology. he is a member of the board of directors of teknowledge corporation and a member of the advisory board of amaix corporation. he was the director of the software engineering institute (sei) at carnegie mellon from 1986 to 1996, where he initiated the computer emergency response team (cert) in 1987. before joining sei, he was vice president for business development at rational software. he served on the board of directors of rational from 1986 to 1995. dr. druffel was on the faculty at the u.s. air force academy. he later managed research programs in advanced software technology at darpa. he was founding director of the ada joint program ofce and then served as director of computer systems and software (research and advanced technology) in the ofce of the secretary of defense. he is the coauthor of a computer science textbook and over 35 professional papers, including the chapter ﬁinformation warfareﬂ for the acm ftieth anniversary book beyond computing. he has a b.s. in electrical engineering from the university of illinois, an m.sc. in computer science from the university of london, and a ph.d. in computer science from vanderbilt university. dr. druffel is a fellow of the ieee and a fellow of the acm. he has served on summary of a workshop on softwareintensive systems and uncertainty at scalecopyright national academy of sciences. all rights reserved.appendix b 55engineering advisory boards of the university of south carolina, clemson, and embry riddle university. he was chairman of the board of directors of the advanced technology institute, a nonprot r&d corporation. dr. druffel chaired the air force science advisory board (afsab) study on information architecture and cochaired the defense science board™s study on acquiring defense software commercially. he led the defensive information warfare panel for the afsab™s new world vistas. he has served on numerous afsab, dsb, and nrc committees dealing with the use of information technology, including the nrc study on engineering challenges to the long term operation of the international space station. russell frew is the vice president, programs and technology, for lockheed martin™s electronic systems business area (esba). in this capacity he is responsible for overseeing both technology development and program performance in the business sector. he is frequently called upon to lead engineering assistance teams that engage major programs across the corporation struggling with signicant technical and programmatic issues. in his capacity as the esba chief technical ofcer, he is also responsible for the technology strategy and the investment plan. additionally, mr. frew has executive responsibility for the advanced technology laboratories in cherry hill, new jersey. from 1999 to late 2003, mr. frew was on special assignment from the ms2 staff to the executive vice president, esba. in this capacity he has led major program tiger teams working on f/a22 avionics stability, the f35 joint strike fighter™s mission system redesign, and the f16 block 60 advanced mission computer. as part of the cots revolution, mr. frew authored and leads the lockheed martin proven path electronics program. prior to his appointment as vice president advanced technology for ms2 in 1999, he spent 18 months as vice president, technology, for government electronics systems (ges) in moorestown, new jersey. while with ges he managed leapahead technology programs such as combats and infoscene. from june 1996 to march 1997, mr. frew was executive director of the lockheed martin™s advanced technology laboratories (atl). during his tenure, mr. frew conceived and led a 9month study for darpa on collaborative intelligent software agents. before that, mr. frew managed atl™s articial intelligence lab for 8 years and served as a career military ofcer. the army later loaned him to darpa, where he was one of the original members of the strategic computing program that defeated japan™s fifth generation challenge. mr. frew is on the board of directors of the isx corporation.james larus is a research area manager at microsoft research. he manages several groups: advanced compiler technology, studying compiler and language implement techniques and focused on techniques summary of a workshop on softwareintensive systems and uncertainty at scalecopyright national academy of sciences. all rights reserved.56 softwareintensive systems and uncertainty at scalefor implementing modern, safe language and in compiling for highly parallel hardware; human interaction in programming, which uses hci techniques such as controlled user studies and ethnography to study software developers, testers, managers, and their teams to produce innovative software development tools that address human and social issues; runtime analysis and design, which uses runtime program analysis, including hybrid staticdynamic analysis, statistical sampling, and heap analysis to improve software quality, security, and performance; software reliability research, which applies program verication techniques and software measurement and modeling techniques to improve the quality of software; and concurrency research, which will explore ways to improve parallel programming. his research centers on singularity, a project to focus on the construction of reliable systems through innovation in the areas of systems, languages, and tools: what would a software platform look like if it was designed from scratch with the primary goal of dependability? singularity is working to answer this question by building on advances in programming languages and tools to develop a new system architecture and operating system (named singularity), with the aim of producing a more robust and dependable software platform. prior to joining microsoft, dr. larus was an associate professor in the computer sciences department at the university of wisconsinmadison. he has an m.s. and ph.d. in computer science from the university of california at berkeley and an a.b. in applied mathematics from harvard university. greg morrisett is allen b. cutting professor of computer science at  harvard university. his current research interest is in the application of programming language technology for building secure and reliable systems. in particular, he is interested in applications of advanced type systems, model checkers, certifying compilers, proofcarrying code, and inline reference monitors for building efcient and provably secure systems. he is also interested in the design and application of highlevel languages for new or emerging domains, such as sensor networks. dr. morrisett received his b.s. in mathematics and computer science from the university of richmond (1989) and his ph.d. in computer science from carnegie mellon university (1995). he spent about 7 years on the faculty of the computer science department at cornell university. in the 20022003 academic year, he took a sabbatical at microsoft™s cambridge research laboratory. in january of 2004, he moved to harvard university. walker royce is the vice president of ibm™s worldwide rational lab services. mr. royce joined rational in 1994 and served as vice president of professional services from 1997 through ibm™s acquisition of rational in 2003. over the last 10 years, he has managed large software engineersummary of a workshop on softwareintensive systems and uncertainty at scalecopyright national academy of sciences. all rights reserved.appendix b 57ing projects, consulted with a broad spectrum of rational™s worldwide customer base, and developed a software management approach that exploits an iterative life cycle, industry best practices, and architecturerst priorities. he is the author of software project management, a unied framework (addison wesley longman, 1998) and a principal contributor to the management philosophy inherent in rational™s unied process. before joining rational, mr. royce spent 16 years in software project development, software technology development, and software management roles at trw electronics & defense. he was a recipient of trw™s chairman™s award for innovation for his contributions in distributed architecture middleware and iterative software processes in 1990 and was named a trw technical fellow in 1992. he received his b.a. in physics from the university of california and his m.s. in computer information and control engineering from the university of michigan.douglas c. schmidt is a professor of computer science and associate chair of the computer science and engineering program at vanderbilt university. he has published over 300 technical papers and six books that cover a range of research topics, including patterns, optimization techniques, and empirical analyses of software frameworks and domainspecic modeling environments that facilitate the development of distributed realtime and embedded (dre) middleware and applications running over highspeed networks and embedded system interconnects. dr. schmidt has served as a deputy ofce director and a program manager at darpa, where he led the national r&d effort on middleware for dre systems. dr. schmidt has also served as the cochair for the software design and productivity coordinating group of the u.s. government™s multiagency information technology research and development program, which formulated the multiagency research agenda in software design. in addition to his academic research and government service, dr. schmidt has over 15 years of experience leading the development of ace, tao, ciao, and cosmic, which are widely used, opensource dre middleware frameworks and modeldriven tools that contain a rich set of components and domainspecic languages that implement patterns and productline architectures for highperformance dre systems.john p. stenbit, nae, is an independent consultant. he recently served as assistant secretary of defense for networks and information integration and as dod™s chief information ofcer. mr. stenbit™s career spans more than 30 years of public and privatesector service in telecommunications and command and control. in addition to his recent service, his public service includes 2 years as principal deputy director of telecommunications and command and control systems, and 2 years as staff specialist summary of a workshop on softwareintensive systems and uncertainty at scalecopyright national academy of sciences. all rights reserved.58 softwareintensive systems and uncertainty at scalefor worldwide command and control systems, both in the ofce of the secretary of defense. mr. stenbit previously was executive vice president at trw, retiring in may 2001. he joined trw in 1968 and was responsible for the planning and analysis of advanced satellite surveillance systems. prior to joining trw, he held a position with the aerospace corporation involving commandandcontrol systems for missiles and satellites, and satellite data compression and pattern recognition. during this time, he was a fulbright fellow and aerospace corporation fellow at the technische hogeschool, einhoven, netherlands, concentrating on coding theory and data compression. he has served on numerous scientic boards and advisory committees, including as chair of the science and technology advisory panel to the director of central intelligence and as a member of the science advisory group to the directors of naval intelligence and the defense communications agency.kevin j. sullivan is associate professor and virginia engineering foundation (vef) endowed faculty fellow in computer science at the university of virginia, where he has worked since 1994. his research interests are mainly in software engineering and languages. he has served as associate editor for the journal of empirical software engineering and the acm transactions on software engineering and methodology, and on the program and executive committees of conferences, including the acm sigsoft symposium on the foundations of software engineering, the international conference on software engineering, aspectoriented software development, and acm sigplansigact symposium on principles of programming languages. he and his students are broadly interested in the design and engineering of softwareintensive systems, with an emphasis on the need for a valuebased theory and practice of system design. dr. sullivan received his undergraduate degree from tufts university in 1987 and the m.s. and ph.d. in computer science and engineering from the university of washington in 1994. cstb stafflynette i. millett is a senior program ofcer and study director at the computer science and telecommunications board of the national academies. she is currently involved in several cstb projects, including a comprehensive exploration of biometrics systems, a study of emerging challenges to sustaining computing performance growth, and an examination of the social security administration™s electronic services strategy. her portfolio includes signicant portions of cstb™s recent work on software and on identity systems and privacy. she recently completed the study that produced software for dependable systems: sufcient evidence? and she was the study director for the project that produced the reports summary of a workshop on softwareintensive systems and uncertainty at scalecopyright national academy of sciences. all rights reserved.appendix b 59who goes there? authentication through the lens of privacy and idsšnot that easy: questions about nationwide identity systems. she has an m.sc. in computer science from cornell university, along with a b.a. in mathematics and computer science with honors from colby college, where she was elected to phi beta kappa. joan d. winston is a program ofcer for the computer science and telecommunications board of the national academies. she is currently involved in cstb projects assessing egovernment strategy, the producibility of softwareintensive systems, and the information technology r&d ecosystem. before cstb, she was an assistant director (information technology team) at the government accountability ofce. from 1998 to 2001, she was principal associate at steve walker and associates, llc, which managed earlystage venture funds focusing on information technology. from 1995 to 1998, she was director of policy analysis for trusted information systems, inc. from 1986 to 1995, she held various analytical and project direction positions at the congressional ofce of technology assessment (ota) and was named an ota senior associate in 1993. before ota, she worked brie˚y for the congressional research service of the library of congress. ms. winston started her career as an engineer at the charles stark draper laboratory, inc., in cambridge, massachusetts. she received an s.b. in physics and an s.m. in technology and policy, both from the massachusetts institute of technology.margaret marsh huynh, senior program assistant, has been with cstb since january 1999 supporting several projects. she is currently supporting the projects currently titled whither biometrics, wireless technology prospects and policy, advancing softwareintensive systems producibility, and assessing the impacts of changes in the information technology research and development ecosystem. she previously worked on the projects that produced the reports signposts in cyberspace: the domain name systems and internet navigation; getting up to speed: the future of supercomputing; beyond productivity: information technology, innovation, and creativity; it roadmap to a geospatial future; building a workforce for the information economy; and the digital dilemma: intellectual property in the information age. ms. huynh also assisted with the ntia workshop on improving spectrum management through economic and other incentives (2006), the gao/nrc forum on information resource management and the paperwork reduction act (2005), as well as the workshops on it issues for the behavioral and social sciences. prior to coming to the nrc, ms. huynh worked as a meeting assistant at management for meetings, april 1998august 1998, and as a meeting assistant at the american society for civil engineers from september 1996 to april 1998. ms. huynh has a b.a. (1990) in liberal studies with minors in sociology and psychology from salisbury university, salisbury, maryland.summary of a workshop on softwareintensive systems and uncertainty at scalecopyright national academy of sciences. all rights reserved.60c biosketches of workshop speakerscynthia andres is a coauthor of extreme programming explained: embrace change, 2nd edition. her professional interests include team and individual psychology and facilitating change with largescale transformative conversations. she holds a b.a. in psychology from pacic union college with advanced work in women™s studies at the university of california at santa cruz and psychology at portland state university. kristen j. baldwin works in the ofce of the under secretary of defense, acquisition, technology and logistics for the director, defense systems. ms. baldwin™s responsibilities span both systems engineering and systems integration functional areas. she leads the osd oversight and implementation of software acquisition process improvement legislation, commonly referred to as section 804. ms. baldwin formerly developed and managed the triservice assessment initiative, which is a dod tool for program managers to identify and mitigate system risk through independent expert assessments. previous assignments in her career include serving as a science and technology advisor in the army™s ofce of the deputy chief of staff for operations and plans and at the dismounted battlespace battle lab at fort benning. ms. baldwin received a bachelors degree in mechanical engineering from virginia tech in 1990 and a master™s in systems management from florida tech in 1995.kent beck is the founder and director of three rivers institute. his career has combined the practice of software development with re˚ection, summary of a workshop on softwareintensive systems and uncertainty at scalecopyright national academy of sciences. all rights reserved.appendix c 61innovation, and communication. his contributions to software development include patterns for software, the rediscovery of testrst programming, the xunit family of developer testing tools, and extreme programming. he currently divides his time between writing, programming, and coaching. mr. beck is an author and/or coauthor of extreme programming explained: embrace change, 2nd edition; contributing to eclipse; testdriven development: by example; planning extreme programming; the smalltalk best practice patterns; and the junit pocket guide. he received his b.s. and m.s. in computer science from the university of oregon. kris britton is the director for the national security agency (nsa) center for assured software.  he has been involved with information assurance issues for the past 15 years, during which time he worked to establish commercial standards and programs to aid dod customers in establishing trust in commercial products they purchase. he began his career as a commercial product evaluator in 1989, focusing on trust in operating systems and databases using the dod standard (dod 5200.28/orange book) and later helped to create the national information assurance partnership and was named its rst technical director. more recently he has been involved with software assurance issues, specically working to evolve the nsa™s software assurance paradigm to address today™s evolving and complex it environment.mary ann davidson is the chief security ofcer at oracle corporation, responsible for security evaluations, assessments, and incident handling. as a senior executive in the it industry she brings both a military and a business background and indepth experience with and perspective on industrial capacity to respond to defense needs. she represents oracle on the board of directors of the information technology information security analysis center (itisac) and is on the editorial review board of the secure business quarterly. ms. davidson has a b.s.m.e. from the university of virginia and an m.b.a. from the wharton school of the university of pennsylvania. she has also served as a commissioned ofcer in the u.s. navy civil engineer corps, where she was awarded the navy achievement medal.joe jarzombek serves as director for software assurance in the policy and strategic initiatives branch of the national cyber security division within the department of homeland security (dhs) and, as such, is the focal point on software integrity issues. he leads collaborative efforts in analyzing software lifecycle components, including people, processes, and technology and identies areas for software quality and security improvement with a focus on development, acquisition, and support. summary of a workshop on softwareintensive systems and uncertainty at scalecopyright national academy of sciences. all rights reserved.62 softwareintensive systems and uncertainty at scalemr. jarzombek guides dhs initiatives in analyzing and resolving software challenges; supports the evolution of policy and guidance on software assurance, including assessment of federal policies, procedures, and evaluation schemes, such as the national information assurance partnership. he functions as dhs coordinator for software quality and acquisition initiatives; working with other federal agencies, state agencies, and international allies to focus on identifying and specifying organizational softwarerelated processes and softwareenabled technologies to mitigate risks attributable to software. mr. jarzombek works with federally funded research and development centers (ffrdcs), consortiums, foundations, universities, and standards groups to coordinate relevant initiatives and leverage organizational resources to share best practices, tools, processes, and research to improve software assurance. he serves as dhs liaison on government/industry working groups and serves on nist, ieee, and iso/iec standards committees and advisory groups, and other executive groups to ensure software assurance needs are addressed in standards, best practices, process models and product lifecycle initiatives. he publishes best practices in software security on the web site https://buildsecurityin.uscert.gov/portal/ as information for developers and acquisition managers. in working with government/academic/industry groups, he leads team efforts to develop the software assurance common body of knowledge, which is intended to provide a framework to recommend updates in curriculum to enhance it acquisition and softwarerelated education and training across the federal acquisition workforce curricula, within universities and colleges, and within industrial training programs. mr. jarzombek has an m.s. in computer information systems from the air force institute of technology, dayton, ohio; a b.b.a. in data processing and analysis from the university of texas, austin; and a b.a. in computer science from the university of texas, austin, where he was also an air force rotc distinguished graduate.patrick lardieri is manager of distributed processing programs at the lockheed martin advanced technology laboratory in cherry hill, new jersey. he has spent over 10 years researching the suitability of open, standardsbased middleware, operating systems, and networks for building distributed realtime systems. recently, he has been leading lockheed martin™s software technology initiative, which is focused on developing technologies for managing the complexity of integrating largescale software systems. he received a master™s in electrical engineering from the university of pennsylvania.gary mcgraw, the cto of cigital, inc., researches software security and sets technical vision in the area of software quality management. summary of a workshop on softwareintensive systems and uncertainty at scalecopyright national academy of sciences. all rights reserved.appendix c 63dr. mcgraw is coauthor of ve bestselling books: exploiting software (addisonwesley, 2004), building secure software (addisonwesley, 2001), software fault injection (wiley 1998), securing java (wiley, 1999), and java security (wiley, 1996). his new book, software security: building security in (addisonwesley), was released in february 2006. a world authority on software security, dr. mcgraw consults with major software producers and consumers. dr. mcgraw has written over 75 peerreviewed technical publications and functions as pi on grants from air force research laboratories, darpa, the national science foundation, and nist™s advanced technology program. he serves on advisory boards of authentica, counterpane, and fortify software, as well as advising the computer science department at the university of california, davis, the computer science department at the university of virginia, and the school of informatics at indiana university. dr. mcgraw holds a dual ph.d. in cognitive science and computer science from indiana university and a b.a. in philosophy from the university of virginia. he is a member of the ieee security and privacy task force, and was recently elected to the ieee computer society™s board of governors. he writes a monthly security column for the magazine it architect, is the editor of ﬁbuilding security inﬂ for ieee™s security & privacy magazine, and is often quoted in the press. richard w. selby is the head of software products at northrop grumman space technology in redondo beach, california. he manages a 250person software organization and has served in this position since 2001. previously, he was the chief technology ofcer and senior vice president at pacic investment management company (pimco) in newport beach, california, where he managed a 105person organization for 3 years. from 1985 to 1998, he was a full professor of information and computer science (with tenure) at the university of california at irvine. since 2004, he has held an adjunct faculty position at the university of southern california computer science department at los angeles. in 1993, he held visiting faculty positions at the mit laboratory for computer science and mit sloan school of management in cambridge, massachusetts, and in 1992, he held a visiting faculty position at the osaka university department of computer science in osaka, japan. his research focuses on development and management of largescale systems, software, and processes. he has authored over 100 refereed publications and given over 205 invited presentations at professional meetings. at northrop, he led the $3 billion company to a successful enterprisewide rating of capability maturity model integration (cmmi) level 5 for software. he served as the chief software engineer for the nasa prometheus spacecraft to jupiter. he also received the company™s highest quality award, named after former president tim w. hannemann, for improvements in development, mansummary of a workshop on softwareintensive systems and uncertainty at scalecopyright national academy of sciences. all rights reserved.64 softwareintensive systems and uncertainty at scaleagement, process, and quality. at pimco, he led the $1 billion company to be ranked as the fourth most innovative technology organization in nancial services, according to wall street & technology. at the university of california, irvine, he coauthored an international bestselling book that analyzed microsoft™s technology, strategy, and management: microsoft secrets: how the world™s most powerful software company creates technology, shapes markets, and manages people. the book, written with michael cusumano, has been translated into 12 languages, has 150,000 copies in print, and was ranked as a #6 bestseller in business week. he received his ph.d. and m.s. degrees in computer science from the university of maryland, college park, maryland, in 1985 and 1983, respectively. he received his b.a. degree in mathematics from st. olaf college, northeld, minnesota, in 1981.alfred spector, nae, is currently an independent consultant working with ibm and a few small companies, and performing some government service.  in his previous position as cto and vice president of strategy & technology for ibm™s software group, dr. spector was responsible for its technical and business strategy, standards, software development methodologies, advanced technology, and cuttingedge technical engagements. prior to this position, dr. spector was vice president of ibm™s worldwide services and software research, general manager of marketing and strategy for ibm™s middleware business, and general manager of ibm™s transaction software business. dr. spector was also the founder and ceo of transarc corporation, a pioneer in distributed transaction processing and widearea le systems and a tenured faculty member in the carnegie mellon university computer science department. dr. spector received his ph.d. in computer science from stanford university and his a.b. in applied mathematics from harvard university. he is recognized for his contributions to the design, implementation, and commercialization of reliable, scalable architectures for distributed le systems, transaction systems, and other applications. dr. spector is also an acm and ieee fellow and a recipient of the ieee kanai award in distributed computing.  werner vogels is vice president and chief technology ofcer at amazon.com, where he is responsible for driving the technology vision to continuously enhance the innovation on behalf of amazon™s customers at a global scale. prior to joining amazon, he worked as a research scientist at cornell university, where he was a principal investigator in several advanced research projects that target the scalability and robustness of missioncritical enterprise computing systems. he has held positions of vice president of technology and chief technology ofcer in companies that handled the transition of academic technology into industry. dr. vogels holds a ph.d. summary of a workshop on softwareintensive systems and uncertainty at scalecopyright national academy of sciences. all rights reserved.appendix c 65from the vrije universiteit in amsterdam and has authored a large number of articles for journals and conferences, most of them on distributed systems technologies for enterprise computing.john vu is a technical fellow at boeing™s engineering, operations, and technology. he has worked in various technical and management positions in boeing, including computeraided design and computeraided manufacturing supporting the development of the 777 airplane, leading software and systems process improvement, and managing boeing global software outsourcing. prior to joining boeing, mr. vu worked at teradyne semiconductor; litton industries, motorola, and gte. he led teams to build navigation and avionics systems (f15 and tomahawk cruise missile) and design the array processors for several signal processing systems (awac and several space exploration satellites). mr. vu is a visiting scientist at the software engineering institute (sei), where he focused on the development and implementation of several capability maturity models. as senior scientist at carnegie mellon university, he is conducting research on software trends in the industry, such as process improvement, ebusiness, and outsourcing. he has authored several benchmarking papers on these topics. he published over 40 technical papers on software and systems engineering disciplines, three books on software engineering and has presented papers at various software engineering conferences worldwide. he is a member of the technical advisory board of ieee software, and adjunct faculty at carnegie mellon university and seattle university. summary of a workshop on softwareintensive systems and uncertainty at scalecopyright national academy of sciences. all rights reserved.
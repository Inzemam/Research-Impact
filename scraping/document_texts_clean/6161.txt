detailsdistribution, posting, or copying of this pdf is strictly prohibited without written permission of the national academies press. (request permission) unless otherwise indicated, all materials in this pdf are copyrighted by the national academy of sciences.copyright © national academy of sciences. all rights reserved.the national academies pressvisit the national academies press at nap.edu and login or register to get:œ œ 10% off the price of print titlesœ special offers and discountsget this bookfind related titlesthis pdf is available at sharecontributorshttp://nap.edu/6161trust in cyberspace352 pages | 6 x 9 | hardbackisbn 9780309065580 | doi 10.17226/6161fred b. schneider, editor, committee on information systems trustworthiness,commission on physical sciences, mathematics, and applications, nationalresearch counciltrust in cyberspacecopyright national academy of sciences. all rights reserved.prefaceifred b. schneider, editorcommittee on information systems trustworthinesscomputer science and telecommunications boardcommission on physical sciences, mathematics, and applicationsnational research councilnational academy presswashington, d.c. 1999trustincyberspacetrust in cyberspacecopyright national academy of sciences. all rights reserved.notice: the project that is the subject of this report was approved by the governing boardof the national research council, whose members are drawn from the councils of thenational academy of sciences, the national academy of engineering, and the institute ofmedicine. the members of the committee responsible for the report were chosen for theirspecial competences and with regard for appropriate balance.support for this project was provided by the defense advanced research projectsagency and the national security agency. any opinions, findings, conclusions, or recommendations expressed in this material are those of the authors and do not necessarily reflectthe views of the sponsors.library of congress cataloginginpublication datatrust in cyberspace / fred b. schneider, editor; committee oninformation systems trustworthiness, computer science andtelecommunications board, commission on physical sciences,mathematics, and applications, national research council.p. cm.includes bibliographical references and index.isbn 0309065585 (casebound)1. computer networks—security measures. 2.telecommunication—government policy—united states. 3. internet(computer network)—security measures. i. schneider, fred b. ii.national research council (u.s.). committee on information systemstrustworthiness.tk5105.59 .t78 1999384.3—ddc21 9858061additional copies of this report are available from:national academy press2101 constitution avenue, n.w.box 285washington, dc 20055800/6246242202/3343313 (in the washington metropolitan area)copyright 1999 by the national academy of sciences. all rights reserved.printed in the united states of americatrust in cyberspacecopyright national academy of sciences. all rights reserved.committee on information systemstrustworthinessfred b. schneider, cornell university, chairsteven m. bellovin, at&t labs researchmartha branstad, computer security researcher andentrepreneurj. randall catoe, cable and wirelessstephen d. crocker, steve crocker associatescharlie kaufman, iris associates inc.stephen t. kent, bbn corporationjohn c. knight, university of virginiasteven mcgeady, intel corporationruth r. nelson, information system securityallan m. schiffman, spyrusgeorge a. spix, microsoft corporationdoug tygar, university of california at berkeleyspecial advisorw. earl boebert, sandia national laboratoriesstaffmarjory s. blumenthal, directorjane bortnick griffith, interim director (1998)herbert s. lin, senior scientistalan s. inouye, program officermark balkovich, research associate (until july 1998)lisa l. shum, project assistant (until august 1998)rita a. gaskins, project assistantdavid padgham, project assistantiiitrust in cyberspacecopyright national academy of sciences. all rights reserved.computer science and telecommunications boarddavid d. clark, massachusetts institute of technology, chairfrances e. allen, ibm t.j. watson research centerjames chiddix, time warner cablejohn m. cioffi, stanford universityw. bruce croft, university of massachusetts, amhersta.g. fraser, at&t corporationsusan l. graham, university of california at berkeleyjames gray, microsoft corporationpatrick m. hanrahan, stanford universityjudith hempel, university of california at san franciscobutler w. lampson, microsoft corporationedward d. lazowska, university of washingtondavid liddle, interval researchjohn major, qualcomm inc.tom m. mitchell, carnegie mellon universitydonald norman, nielsen norman groupraymond ozzie, groove networksdavid a. patterson, university of california at berkeleydonald simborg, knowmed systemslee sproull, boston universityleslie l. vadasz, intel corporationmarjory s. blumenthal, directorjane bortnick griffith, interim director (1998)herbert s. lin, senior scientistjerry r. sheehan, senior program officeralan s. inouye, program officerjon eisenberg, program officerjanet briscoe, administrative associatenicci dowd, project assistantrita gaskins, project assistantdavid padgham, project assistantivtrust in cyberspacecopyright national academy of sciences. all rights reserved.vcommission on physical sciences,mathematics, and applicationspeter m. banks, erim international inc., cochairw. carl lineberger, university of colorado, cochairwilliam browder, princeton universitylawrence d. brown,university of pennsylvaniamarshall h. cohen, california institute of technologyronald g. douglas, texas a&m universityjohn e. estes, university of california at santa barbarajerry p. gollub, haverford collegemartha p. haynes, cornell universityjohn l. hennessy, stanford universitycarol m. jantzen, westinghouse savannah river companypaul g. kaminski, technovation inc.kenneth h. keller, university of minnesotamargaret g. kivelson, university of california at los angelesdaniel kleppner, massachusetts institute of technologyjohn r. kreick, sanders, a lockheed martin companymarsha i. lester, university of pennsylvaniam. elisabeth patécornell, stanford universitynicholas p. samios,brookhaven national laboratorychanglin tien, university of california at berkeleynorman metzger, executive directortrust in cyberspacecopyright national academy of sciences. all rights reserved.the national academy of sciences is a private, nonprofit, selfperpetuating society ofdistinguished scholars engaged in scientific and engineering research, dedicated to the furtherance of science and technology and to their use for the general welfare. upon theauthority of the charter granted to it by the congress in 1863, the academy has a mandatethat requires it to advise the federal government on scientific and technical matters.dr.bruce alberts is president of the national academy of sciences.the national academy of engineering was established in 1964, under the charter of thenational academy of sciences, as a parallel organization of outstanding engineers. it isautonomous in its administration and in the selection of its members, sharing with the nationalacademy of sciences the responsibility for advising the federal government. the nationalacademy of engineering also sponsors engineering programs aimed at meeting nationalneeds, encourages education and research, and recognizes the superior achievements of engineers. dr. william a. wulf is president of the national academy of engineering.the institute of medicine was established in 1970 by the national academy of sciencesto secure the services of eminent members of appropriate professions in the examination ofpolicy matters pertaining to the health of the public. the institute acts under the responsibility given to the national academy of sciences by its congressional charter to be an adviser tothe federal government and, upon its own initiative, to identify issues of medical care,research, and education. dr. kenneth i. shine is president of the institute of medicine.the national research council was organized by the national academy of sciences in1916 to associate the broad community of science and technology with the academy’spurposes of furthering knowledge and advising the federal government. functioning inaccordance with general policies determined by the academy, the council has become theprincipal operating agency of both the national academy of sciences and the nationalacademy of engineering in providing services to the government, the public, and the scientific and engineering communities. the council is administered jointly by both academiesand the institute of medicine. dr. bruce alberts and dr. william a. wulf are chairman andvice chairman, respectively, of the national research council.vitrust in cyberspacecopyright national academy of sciences. all rights reserved.experts have known for some time that networked information systems are not trustworthy and that the technology needed to make themtrustworthy has not, by and large, been at hand. our nation is neverthelessbecoming dependent on such systems for operating its critical infrastructures (e.g., transportation, communication, finance, and energy distribution). over the past 2 years, the implications of this dependence—vulnerability to attack and susceptibility to disaster—have become a part of thenational agenda. concerns first voiced from within the defense establishment (under the rubric of “information warfare”) led the executive branchto create the president’s commission on critical infrastructure protectionand, later, the critical infrastructure assurance office. the popular pressembraced the issues, carrying them to a public already sensitized by direct and collateral experience with the failings of computing systems andnetworks. a subject once discussed only in the technical literature is nowappearing regularly on the front pages of newspapers and being debatedin the congress. the present study, initiated at the request of the defenseadvanced research projects agency (darpa) and the national securityagency (nsa) some 2 years ago, today informs a discussion of nationalsignificance. in particular, this study moves the focus of the discussionforward from matters of policy and procedure and from vulnerabilitiesand their consequences toward questions about the richer set of optionsthat only new science and technology can provide.the study committee was convened by the computer science andtelecommunications board (cstb) of the national research councilviiprefacetrust in cyberspacecopyright national academy of sciences. all rights reserved.viiipreface(nrc) to assess the nature of information systems trustworthiness andthe prospects for technology that will increase trustworthiness. the committee was asked to examine, discuss, and report on interrelated issuesassociated with the research, development, and commercialization of technologies for trustworthy systems and to use its assessment to developrecommendations for research to enhance information systems trustworthiness (see box p.1). this volume contains the results of that study: adetailed research agenda that examines the many dimensions of trustworthiness (e.g., correctness, security, reliability, safety, survivability),the state of the practice, and the available technology and science base.since economic and political context is critical to the successful development and deployment of new technologies, that too is discussed.the alert reader will have noted that the volume’s title, trust incyberspace, admits two interpretations. this ambiguity was intentional.parse“trust” as a noun (as in “confidence” or “reliance”) and the titlesuccinctly describes the contents of the volume—technologies that helpmake networked information systems more trustworthy. parse “trust” asa verb (as in “to believe”) and the title is an invitation to contemplate afuture where networked information systems have become a safe placefor conducting parts of our daily lives.1 whether “trust” is being parsedas a noun or the verb, more research is key for trust in cyberspace.committee composition and processthe study committee included experts from industry and academiawhose expertise spanned computer and communications security, software engineering, faulttolerance, systems design and implementation,and networking (see appendix a). the committee did its work throughits own expert deliberations and by soliciting input and discussion fromkey officials in its sponsoring agencies, other government officials, academic experts, and representatives of a wide range of developers andusers of information systems in industry (see appendix b). the committee did not make use of classified information, believing that detailedknowledge of threats was not important to the task at hand.the committee first met in june 1996 and eight times subsequently.three workshops were held to obtain input from a broad range of expertsin systems security, software, and networking drawn primarily from industry (see appendixes c and d). since information about the nsa r21one reviewer, contemplating the present, suggested that a question mark be placed atthe end of the title to raise questions about the trustworthiness of cyberspace today. andthis is a question that the report does raise.trust in cyberspacecopyright national academy of sciences. all rights reserved.prefaceixresearch program is less widely available than for relevant programs atdarpa and other federal agencies, the entire committee visited nsa fora more indepth examination of r2’s research program; subsequent meetings between nsa r2 personnel and a subset of the committee providedstill further input to the study. staff tracked the progress of relevantactivities in the legislative and executive branches in government, including the president’s commission on critical infrastructure protection, thecritical information assurance office, and congressional hearings. staffalso sought input from other governmental and quasigovernmental organizations with relevant emphases. additional inputs included perspectives from professional conferences, the technical literature, and government reports gleaned by committee members and staff.in april 1997, the committee released an interim report that outlinedkey concepts and known technologies. that report, subject to the nrcreview process, generated a number of comments that helped to guide thecommittee in its later work.acknowledgmentsthe committee is grateful to the many thoughtful reviewers of itsinterim and final reports, and it appreciates the efforts of the review coorbox p.1synopsis of statement of task•propose a research agenda that identifies ideas for relevant longterm researchand the promotion of fundamental or revolutionary (as opposed to incremental)advances to foster increased trustworthiness of networked information systems. perspectives on where and what kinds of research are needed should be sought fromacross the relevant technical and business communities.•assess, in part by undertaking dialogue within relevant segments of the technical and business communities, and make recommendations on how to further thedevelopment and deployment of trustworthy networked information systems, subsystems, and components.•assess and make recommendations concerning the effectiveness and directions of the existing research programs in darpa and nsa r2 as they affect thedevelopment of trustworthy networked information systems.•examine the state of the market for security products and capabilities and theextent and emphases of privatesector research activities with an eye toward illuminating where federal r&d efforts can best be targeted.•assess and develop recommendations for technology policy options to improve the commercial security product base (availability, quality, and affordability),expand awareness in industry of the security problem and of available technologyand tools for enhancing protections, and foster technology transfer.trust in cyberspacecopyright national academy of sciences. all rights reserved.xprefacedinator. the committee would like to acknowledge thomas a. berson(anagram laboratories), dan boneh (stanford university), eric a. brewer(university of california, berkeley), dorothy denning (georgetown university), bruce fette (motorola), john d. gannon (university of maryland),li gong (javasoft inc., sun microsystems inc.), russ housley (spyrus ),john c. klensin (mci communications corporation), jimmy kuo (mcafeeassociates inc.), steven b. lipner (mitretek systems), keith marzullo (university of california, san diego), alan j. mclaughlin (massachusetts institute of technology), robert morris, sr. (national security agency [retired]), peter g. neumann (sri international), jimmy omura (cylinkcorporation), stewart personick (drexel university), roy radner (newyork university), morteza rahimi (northwestern university), jeffrey i.schiller (massachusetts institute of technology), michael st. johns(@home network), joseph sventek (hewlettpackard laboratories), j.marty tenenbaum (cngroup inc.), abel weinrib (intel corporation),jeannette m. wing (carnegie mellon university), and mary ellen zurko(iris associates inc.).the committee appreciates the support of its sponsoring agencies andespecially the numerous inputs and responses to requests for informationprovided by howard frank, now at the university of maryland, teresalunt, now at sri international, robert meushaw at nsa, and john davisat nsa and the critical infrastructure assurance office. the support ofk. david nokes at sandia national laboratories was extremely helpful infacilitating this study and the preparation of this report.in addition, the committee would like to thank jeffrey schiller for hisvaluable perspective on internet standards setting. the committee wouldalso like to thank individuals who contributed their expertise to thecommittee’s deliberations: robert h. anderson (rand corporation),ken birman (cornell university), chip boylan (hilb, rogal, and hamiltoncompany), robert l. constable (cornell university), dale drew (mcisecurity services), bill flanagan (perot systems corporation), fredhoward (bell atlantic voice operations), keith marzullo (university ofcalifornia, san diego), j s. moore (university of texas, austin), peter g.neumann (sri international), john pescatore (trusted information systems), john rushby (sri international), sami saydjari (defense advancedresearch projects agency), dan shoemaker (bell atlantic data operations), steve sigmond (wessels arnold investment banking), gadi singer(intel corporation), steve smaha (haystack inc.), kevin sullivan (university of virginia), l. nick trefethen (oxford university), and wernervogels (cornell university). the committee would also like to thank theparticipants at the workshops for their valuable insights.several members of the computer science and telecommunicationsboard provided valuable guidance to the committee and were instrumentrust in cyberspacecopyright national academy of sciences. all rights reserved.prefacexital in the responsetoreview process. for these contributions, the committee would like to thank david d. clark, jim gray, and butler lampson.the committee also acknowledges the helpful feedback from cstb members donald norman and ed lazowska.special thanks are owed steve crocker for his seminal role in launching this study and in helping to shape the committee. the committee—and the chairman especially—benefited from steve’s involvement.finally, the committee would like to acknowledge all the hard workby the staff of the national research council. marjory blumenthal’s contributions to the content and conduct of this study were pivotal. not onlywas marjory instrumental in moving the committee from its initial discussions through the production of an interim report and then to a firstdraft of this report, but her insights into the nontechnical dimensions oftrustworthiness were also critical for chapter 6. this committee was trulyfortunate to have the benefit of marjory’s insights, and this chairman wasthankful to have such a master in the business as a teacher and advisor.alan inouye joined the project midstream. to him fell the enormous taskof assembling this final report. alan did a remarkable job, remainingunfailingly upbeat despite the long hours required and the frustrationsthat accompanied working to a deadline. first leslie wade and later lisashum supported the logistics for the committee’s meetings, drafts, andreviews in a careful yet cheery fashion. as a research associate, markbalkovich enthusiastically embraced a variety of research and factfinding assignments. thanks to jane bortnick griffith for her support as theinterim director of cstb who inherited this challenging project midstreamand did the right thing. herb lin was available when we needed himdespite his numerous other commitments. the contributions of lauraost (editorconsultant) and patricia spellman (copy editor) are gratefullyacknowledged. rita gaskins, david padgham, and cris banks also assisted in completing the report.fred b. schneider, chaircommittee on information systems trustworthinesstrust in cyberspacecopyright national academy of sciences. all rights reserved.trust in cyberspacecopyright national academy of sciences. all rights reserved.executive summary11introduction12trustworthy networked information systems, 13what erodes trust, 15this study in context, 20scope of this study, 21references, 232public telephone network and internettrustworthiness26network design, 27the public telephone network, 27network services and design, 27authentication, 28the internet, 29network services and design, 29authentication (and other security protocols), 33findings, 36network failures and fixes, 37environmental disruption, 37link failures, 37congestion, 38findings, 41xiiicontentstrust in cyberspacecopyright national academy of sciences. all rights reserved.xivcontentsoperational errors, 41findings, 45software and hardware failures, 45finding, 47malicious attacks, 47attacks on the telephone network, 47routing attacks, 48database attacks, 48facilities, 50findings, 50attacks on the internet, 50name server attacks, 51routing system attacks, 51protocol design and implementation flaws, 54findings, 55emerging issues, 55internet telephony, 55finding, 56is the internet ready for “prime time”?, 56findings, 58references, 593software for networked information systems62introduction, 62background, 62the role of software, 64development of a networked information system, 66system planning, requirements, and toplevel design, 66planning and program management, 66requirements at the systems level, 68background, 68the system requirements document, 69notation and style, 70where to focus effort in requirements analysis anddocumentation, 72toplevel design, 74critical components, 76the integration plan, 77project structure, standards, and process, 78barriers to acceptance of new software technologies, 81findings, 81building and acquiring components, 82componentlevel requirements, 82trust in cyberspacecopyright national academy of sciences. all rights reserved.contentsxvcomponent design and implementation, 84programming languages, 85systematic reuse, 86commercial offtheshelf software, 87the changing role of cots software, 87general problems with cots components, 89interfacing legacy software, 90findings, 91integrating components into a trustworthy system, 92system integration, 92system assurance, 94review and inspection, 94formal methods, 95testing, 101system evolution, 102findings, 103references, 1044reinventing security109introduction, 109evolution of security needs and mechanisms, 110access control policies, 111shortcomings of formal policy models, 115a new approach, 118findings, 120identification and authentication mechanisms, 121networkbased authentication, 121cryptographic authentication, 122tokenbased mechanisms, 123biometric techniques, 123findings, 124cryptography and publickey infrastructure, 124findings, 127the keymanagement problem, 127keydistribution centers, 127certification authorities, 128actual deployments of largescale keydistributioncenters and certification authorities, 129publickey infrastructure, 130findings, 132network access control mechanisms, 132closed user groups, 132virtual private networks, 133trust in cyberspacecopyright national academy of sciences. all rights reserved.xvicontentsfirewalls, 134limitations of firewalls, 135guards, 137findings, 138foreign code and applicationlevel security, 139the activex approach, 141the java approach, 142findings, 142finegrained access control and application security, 143findings, 145languagebased security: software fault isolation andproofcarrying code, 146findings, 149denial of service, 149findings, 150references, 1515trustworthy systems from untrustworthy154componentsreplication and diversity, 155amplifying reliability, 155amplifying security, 157findings, 158monitor, detect, respond, 158limitations in detection, 158response and reconfiguration, 159perfection and pragmatism, 160findings, 161placement of trustworthiness functionality, 161public telephone network, 162internet, 163minimum essential information infrastructure, 164findings, 167nontraditional paradigms, 168finding, 169references, 1696the economic and public policy context171risk management, 172risk assessment, 173nature of consequences, 174risk management strategies, 176selecting a strategy, 179trust in cyberspacecopyright national academy of sciences. all rights reserved.contentsxviifindings, 180consumers and trustworthiness, 180consumer costs, 181direct costs, 181indirect costs, 182failure costs, 183imperfect information, 184issues affecting risk management, 186some market observations, 188findings, 189producers and trustworthiness, 190the larger marketplace and the trend toward homogeneity, 190risks of homogeneity, 191producers and their costs, 192costs of integration and testing, 193identifying the specific costs associated withtrustworthiness, 193time to market, 194other issues, 194the market for trustworthiness, 196supply and demand considerations, 197findings, 198standards and criteria, 199the character and context of standards, 199standards and trustworthiness, 201securitybased criteria and evaluation, 204findings, 209cryptography and trustworthiness, 210export controls, 210key recovery, 211factors inhibiting widespread deployment ofcryptography, 211cryptography and confidentiality, 214findings, 214federal government interests in nis trustworthiness, 215publicprivate partnerships, 219the changing marketgovernment relationship, 220findings, 221the roles of the nsa, darpa, and other federalagencies in nis trustworthinessresearch and development, 221national security agency, 224partnerships with industry, 226trust in cyberspacecopyright national academy of sciences. all rights reserved.xviiicontentsr2 program, 228issues for the future, 230findings, 232defense advanced research projects agency, 232issues for the future, 235findings, 236references, 2377conclusions and research recommendations240protecting the evolving public telephone networkand the internet, 241meeting the urgent need for software that improvestrustworthiness, 244reinventing security for computers and communications, 247building trustworthy systems from untrustworthycomponents, 250social and economic factors that inhibit the deploymentof trustworthy technology, 251implementing trustworthiness research and development, 253appendixesastudy committee biographies259bbriefers to the committee267cworkshop participants and agendas269dlist of position papers prepared for the workshops279etrends in software281fsome related trustworthiness studies285gsome operating system security examples291htypes of firewalls293isecrecy of design296jresearch in information system security andsurvivability funded by the nsa and darpa298kglossary300index319trust in cyberspacecopyright national academy of sciences. all rights reserved.trustincyberspacetrust in cyberspacecopyright national academy of sciences. all rights reserved.contentsxxthis is the tale of the infosys folk:multics to unix to dos.we once had protection that wasn’t a jokemultics to unix to dos.now hackers and crackers and similar nerdspass viruses, horses, and horrible wordsthrough access controls that are for the birds.multics to unix to dos.—with apologies to franklin p. adamstrust in cyberspacecopyright national academy of sciences. all rights reserved.1the nation’s security and economy rely on infrastructures for communication, finance, energy distribution, and transportation—all increasingly dependent on networked information systems. when these networked information systems perform badly or do not work at all, theyput life, liberty, and property at risk. interrupting service can threatenlives and property; destroying information or changing it improperly candisrupt the work of governments and corporations; and disclosing secretscan embarrass people or hurt organizations. the widespread interconnection of networked information systems allows outages and disruptions to spread from one system to others; it enables attacks to be wagedanonymously and from a safe distance; and it compounds the difficulty ofunderstanding and controlling these systems. with an expanding fractionof users and operators who are technologically unsophisticated, greaternumbers can cause or fall victim to problems. some see this as justification for alarm; others dismiss such fears as alarmist. most agree that thetrends warrant study and better understanding.recent efforts, such as those by the president’s commission on critical infrastructure protection, have been successful in raising public awareness and advocating action. however, taking action is constrained bylimited knowledge and technologies for ensuring that networked information systems perform properly. research is needed, and this reportgives, in its body, a detailed agenda for that research. specifically, thereport addresses how the trustworthiness of networked information systems can be enhanced by improving computing and communications techexecutive summarytrust in cyberspacecopyright national academy of sciences. all rights reserved.2trust in cyberspacenology. the intent is to create more choices for consumers and vendorsand, therefore, for the government. the report also surveys technical andmarket trends, to better inform public policy about where progress islikely and where incentives could help. and the report discusses a largernontechnical context—public policy, procedural aspects of how networked information systems are used, how people behave—because thatcontext affects the viability of technical solutions as well as actual risksand losses.trustworthy networked information systems—benefits, costs, and contextnetworked information systems (niss) integrate computing systems,communication systems, people (both as users and operators), procedures,and more. interfaces to other systems and control algorithms are theirdefining elements; communication and interaction are the currency oftheir operation. increasingly, the information exchanged between nissincludes software (and, therefore, instructions to the systems themselves),often without users knowing what software has entered their systems, letalone what it can do or has done.trustworthiness of an nis asserts that the system does what is required—despite environmental disruption, human user and operator errors, and attacks by hostile parties—and that it does not do other things.design and implementation errors must be avoided, eliminated, or somehow tolerated. addressing only some aspects of the problem is not sufficient. moreover, achieving trustworthiness requires more than just assembling components that are themselves trustworthy.laudable as a goal, ab initio building of trustworthiness into an nishas proved to be impractical. it is neither technically nor economicallyfeasible for designers and builders to manage the complexity of suchlarge artifacts or to anticipate all of the problems that an nis will confrontover its lifetime. experts now recognize steps that can be taken to enhance trustworthiness after a system has been deployed. it is no accidentthat the market for virus detectors and firewalls is thriving. virus detectors identify and eradicate attacks embedded in exchanged files, andfirewalls hinder attacks by filtering messages between a trusted enclaveof networked computers and its environment (from which attacks mightoriginate). both of these mechanisms work in specific contexts and address problems contemplated by their designers; but both are imperfect,with user expectations often exceeding what is prudent.the costs of nis trustworthiness are borne by a system’s producersand consumers and sometimes by the public at large. the benefits arealso distributed, but often differently from the costs. the market hastrust in cyberspacecopyright national academy of sciences. all rights reserved.executive summary3responded best in dimensions, such as reliability, that are easy for consumers (and producers) to evaluate, as compared with other dimensions,such as security, which addresses exposures that are difficult to quantifyor even fully articulate. few have an incentive to worry about securityproblems since such problems rarely prevent work from getting done,and publicizing them sometimes even tarnishes the reputation of the institution involved (as in the case of banks).market conditions today strongly favor the use of commercial offtheshelf (cots) components over custombuilt solutions, in part becausecots technology is relatively inexpensive to acquire. the cots market’searliest entrants can gain a substantial advantage, so cots producers areless inclined to include trustworthiness functionality, which they believecan cause delay. cots producers are also reluctant to include in theirproducts mechanisms to support trustworthiness (and especially security) that can make systems harder to configure or use. while today’smarket for system trustworthiness is bigger than that of a decade ago, themarket remains small, reflecting current circumstances and perceptions:to date, publicized trustworthiness breaches have not been catastrophic,and consumers have been able to cope with or recover from the incidents.thus, existing trustworthiness solutions—though needed—are not beingwidely deployed because often they cannot be justified.today’s climate of deregulation will further increase nis vulnerability in several ways. the most obvious is the new cost pressures on whathad been regulated monopolies in the electric power and telecommunications industries. one easy way to cut costs is to reduce reserve capacityand eliminate rarely needed emergency systems; a related way is to reduce diversity (a potential contributor to trustworthiness) in the technology or facilities used. producers in these sectors are now competing onthe basis of features, too. new features invariably lead to more complexsystems, which are liable to behave in unexpected and undesirable ways.finally, deregulation leads to new interconnections, as some services aremore costeffectively imported from other providers into what once weremonolithic systems. apart from the obvious dangers of the increasedcomplexity, the interconnections themselves create new weak points andinterdependencies. problems could grow beyond the annoyance levelthat characterizes infrastructure outages today, and the possibility of catastrophic incidents is growing.the role of government in protecting the public welfare implies aninterest in promoting the trustworthiness of niss. contemporary examinations of issues, ranging from information warfare to critical infrastructure,have advanced hypotheses and assumptions about specific, substantial,and proactive roles for government. but their rationales are incomplete.part of the problem stems from the difficulty of describing the appropritrust in cyberspacecopyright national academy of sciences. all rights reserved.4trust in cyberspaceate scope for government action when the government’s own niss arecreatures of privatesector components and services. the rise of electronic commerce and, more generally, growing publication and sharingof all kinds of content through niss are generating a variety of differentmodels for the role of government and the balance of public and privateaction. in all of these contexts, debates about cryptography policy and thealleged inhibition of the development and deployment of technology(encryption and authentication) that can advance many aspects of trustworthiness make discussion of government roles particularly sensitiveand controversial. the necessary public debates have only just begun,and they are complicated by the underlying activity to redefine conceptsof national and economic security.technology offers the opportunities and imposes the limits facing allsectors. research and development changes technological options andthe cost of various alternatives. it can provide new tools for individualsand organizations and better inform private and public choices and strategies. once those tools have been developed, demands for trustworthiness could be more readily met. due to the customary rapid rate ofupgrade and replacement for computing hardware and software (at leastfor systems based on cots products), upgrades embodying enhancedtrustworthiness could occur over years rather than decades (impededmostly by needs for backward compatibility). moreover, the predominance of cots software allows investments in cots software that enhance trustworthiness to have broad impact, and current events, such asconcern about the “year 2000” and the european union monetary conversion, are causing older software systems to be replaced with new cotssoftware. finally, communications infrastructures are likely to undergoradical changes in the coming years: additional players in the market,such as cable and satellitebased services, will not only lead to new pricing structures, but will also likely force the introduction of new communications system architectures and services. taken together, these trendsimply that now is the time to take steps to develop and deploy bettertechnology.an agenda for researchthe goal of further research would be to provide a science base andengineering expertise for building trustworthy niss. commercial andindustrial software producers have been unwilling to pay for this research, doing the research will take time, and the construction of trustworthy niss presupposes appropriate technology for which this researchis still needed. therefore, the central recommendations of this studyconcern an agenda for research (outlined below). the recommendationstrust in cyberspacecopyright national academy of sciences. all rights reserved.executive summary5are aimed at federal funders of relevant research—in particular, the defense advanced research projects agency (darpa) and the nationalsecurity agency (nsa). but the research agenda should also be of interest to policymakers who, in formulating legislation and initiating otheractions, will profit from knowing which technical problems do have solutions, which will have solutions if research is supported, and which cannot have solutions. those who manage niss can profit from the agendain much the same way as policymakers. product developers can benefitfrom the predictions of market needs and promising directions for addressing those needs.research to identify and understand nis vulnerabilitiesbecause a typical nis is large and complex, few people are likely tohave analyzed one, much less had an opportunity to study several. theresult is a remarkably poor understanding today of design and engineering practices that foster nis trustworthiness. careful study of deployedniss is needed to inform nis builders of problems that they are likely toencounter, leading to moreintelligent choices about what to build andhow to build it. the president’s commission on critical infrastructureprotection and other federal government groups have successfully begunthis process by putting nis trustworthiness on the national policy agenda.the next step is to provide specific technical guidance for nis designers,implementers, and managers. a study of existing niss can help determine what problems dominate nis architecture and software development, the interaction of different aspects of trustworthiness in design andimplementation or use, and how to quantify the actual benefits of usingproposed methods and techniques.the public telephone network (ptn) and the internet, both familiarniss, figure prominently in this report. both illustrate the scope andnature of the technical problems that will confront developers and operators of future niss, and the high cost of building a global communicationsinfrastructure from the ground up implies that one or both of these twonetworks will furnish communications services for most other niss. thetrustworthiness and vulnerabilities of the ptn and the internet are thuslikely to have farreaching implications. but ptn trustworthiness, forexample, would seem to be eroding as the ptn becomes increasinglydependent on complex software and databases for establishing calls andfor providing new or improved services to customers. protective measures need to be developed and implemented. some internet vulnerabilities are being eliminated by deploying improved protocols, but theinternet’s weak qualityofservice guarantees, along with other routingprotocol inadequacies and dependence on a centralized namingservicetrust in cyberspacecopyright national academy of sciences. all rights reserved.6trust in cyberspacearchitecture, remain sources of vulnerability for it; additional researchwill be needed to significantly improve the internet’s trustworthiness.operational errors today represent a major source of outages for boththe ptn and the internet. today’s methods and tools for facilitating anoperator’s understanding and control of an nis of this scale and complexity are inadequate. research and development are needed to produceconceptual models (and ultimately methods of control) that can allowhuman operators to grasp the state of an nis and initiate actions that willhave predictable, desired consequences.research in avoiding design and implementation errorsthe challenges of software engineering, formidable for so manyyears, become especially urgent when designing and implementing annis. and new problems arise in connection with all facets of the systemdevelopment process. systemlevel trustworthiness requirements mustbe transformed from informal notions into precise requirements thatcan be imposed on individual components, something that all too oftenis beyond the current state of the art. when an nis is being built,subsystems spanning distributed networks must be integrated andtested despite their limited visibility and limited control over their operation. yet the trend has been for researchers to turn their attentionaway from such integration and testing questions—a trend that needs tobe reversed by researchers and by those who fund research. even modest advances in testing methods can have a significant impact, becausetesting so dominates system development costs. techniques for composing subsystems in ways that contribute directly to trustworthinessare also badly needed.whereas a large software system, such as an nis, cannot be developed defect free, it is possible to improve the trustworthiness of such asystem by anticipating and targeting vulnerabilities. but to determine,analyze, and—most importantly—prioritize these vulnerabilities requiresa good understanding of how subsystems interact with each other andwith the other elements of the larger system. obtaining such an understanding is not possible without further research.niss today and well into the foreseeable future are likely to includelarge numbers of cots components. the relationship between the use ofcots components and nis trustworthiness is unclear—does the increased use of cots components enhance or detract from trustworthiness? how can the trustworthiness of a cots component be improvedby its developers and (when needed) by its users? moreover, more sothan most other software systems, niss are developed and deployed intrust in cyberspacecopyright national academy of sciences. all rights reserved.executive summary7crementally, significantly evolving in functionality and structure over asystem’s lifetime. yet little is known about architectures that can supportsuch growth and about development processes that facilitate it; additional research is required.there are accepted processes for component design and implementation, although the novel characteristics of niss raise questions about theutility of these processes. modern programming languages include features that promote trustworthiness, such as compiletime checks and support for modularity and component integration, and the potential existsfor further gains from research.the performance needs of niss can beinconsistent with modular design, though, and this limits the applicability of many extant software development processes and tools.formal methods should be regarded as an important piece of technology for eliminating design errors in hardware and software; increasedsupport for both fundamental research and demonstration exercises iswarranted. formal methods are particularly well suited for identifyingerrors that only become apparent in scenarios not likely to be tested ortestable. therefore, formal methods could be viewed as a technology thatis complementary to testing. research directed at the improved integration of testing and formal methods is likely to have payoffs for increasingassurance in trustworthy niss.new approaches to computer and communications securitymuch security research during the past two decades has been basedon models that focus on protecting information from unauthorized accessby specifying which users should have access to data or other systemresources. these models oversimplify: they do not completely accountfor malicious or erroneous software, they largely ignore denialofserviceattacks, and they are unable to represent defensive measures, such asvirus scan software or firewalls—mechanisms that, in theory, should notwork or be needed but do, in practice, hinder attacks. the practical impacts of this “absolute security” paradigm have been largely disappointing. a new approach to security is needed, especially for environments(like niss) where foreign and mobile code and cots software cannot beignored. the committee recommends that rather than being based on“absolute security,” future security research be based on techniques foridentifying vulnerabilities and making design changes to reposition thosevulnerabilities in light of anticipated threats. by repositioning vulnerabilities, the likelihood and consequences of attacks can be reduced.effective cryptographic authentication is essential for nis security.but obstacles exist to more widespread deployment of keymanagetrust in cyberspacecopyright national academy of sciences. all rights reserved.8trust in cyberspacement technology, and there has been little experience with publickeyinfrastructures—especially largescale ones. issues related to the timelynotification of revocation, recovery from the compromise of certification authority private keys, and namespace management all requirefurther attention. most applications that make use of certificates havepoor certificatemanagement interfaces for users and for system administrators. research is also needed to support new cryptographicauthentication protocols (e.g., for practical multicast communicationauthentication) and to support faster encryption and authentication/integrity algorithms to keep pace with rapidly increasing communication speeds. the use of hardware tokens holds promise for implementing authentication, although using personal identification numbers constitutes a vulnerability (which might be somewhat mitigated throughthe use of biometrics).because niss are distributed systems, network access control mechanisms, such as virtual private networks (vpns) and firewalls, can play acentral role in nis security. vpn technology, although promising, is notbeing used today in largerscale settings because of the proprietary protocols and simplistic keymanagement schemes found in products. furtherwork is needed before wholesale and flexible vpn deployments will become realistic. firewalls, despite their limitations, will persist into theforeseeable future as a key defense mechanism. and as support for vpnsis added, firewall enhancements will have to be developed for supportingsophisticated security management protocols, negotiation of traffic security policies across administratively independent domains, and management tools. the development of increasingly sophisticated networkwideapplications will create a need for applicationlayer firewalls and a betterunderstanding of how to define and enforce useful traffic policies at thislevel.operating system support for finegrained access control would facilitate construction of systems that obey the principle of least privilege,which holds that users be accorded the minimum access that is needed toaccomplish a task. this, in turn, would be an effective defense against avariety of attacks that might be delivered using foreign code or hidden inapplication programs. enforcement of applicationspecific security policies is likely to be a responsibility shared between the application program and the operating system. research is needed to determine how topartition this responsibility and which mechanisms are best implementedat what level. attractive opportunities exist for programming languageresearch to play a role in enforcing such security policies.finally, defending against denialofservice attacks can be critical forthe security of an nis, since availability is often an important systemproperty. this dimension of security has received relatively little attentrust in cyberspacecopyright national academy of sciences. all rights reserved.executive summary9tion up to now, and research is urgently needed to identify ways to defend against such attacks.research in building trustworthy systemsfrom untrustworthy componentseven when it is possible to build them, highly trustworthy components are costly. therefore, the goal of creating trustworthy niss fromuntrustworthy components is attractive, and research should be undertaken that will enable the trustworthiness of components to be amplifiedby the architecture and by the methods used to integrate components.replication and diversity can be employed to build systems that amplify the trustworthiness of their components, and there are successfulcommercial products (e.g., hardware faulttolerant computers) in the marketplace that do exactly this. however, the potential and limits of theapproach are not understood. for example, research is needed to determine the ways in which diversity can be added to a set of software replicas, thereby improving their trustworthiness.trustworthiness functionality could be positioned at different placeswithin an nis. little is known about the advantages and disadvantagesof the various possible positionings and system architectures, and ananalysis of existing niss should prove instructive along these lines. onearchitecture that has been suggested is based on the idea of a broadlyuseful core minimum functionality—a minimum essential informationinfrastructure (meii). but building an meii would be a misguided initiative, because it presumes that such a “core minimum functionality” couldbe identified, and that is unlikely to be the case.monitoring and detection can be employed to build systems that enhance the trustworthiness of their components. but limitations intrinsicin system monitoring and in technology to recognize incidents such asattacks and failures impose fundamental limits on the use of monitoringand detection for implementing trustworthiness. in particular, the limitsand coverage of the various approaches to intruder and anomaly detection are necessarily imperfect; additional study is needed to determinetheir practicality.a number of other promising research areas merit investigation. forexample, systems could be designed to respond to an attack or failure byreducing their functionality in a controlled, graceful manner. and a variety of research directions involving new types of algorithms—selfstabilization, emergent behavior, biological metaphors—may be useful in designing systems that are trustworthy. these new research directions arespeculative. thus, they are plausible topics for longerrange research thatshould be pursued.trust in cyberspacecopyright national academy of sciences. all rights reserved.10trust in cyberspaceimplementing the research agendaresearch in nis trustworthiness is supported by the u.s. government, primarily through darpa and nsa, but also through other department of defense and civilian agencies. much of darpa and nsafunding goes to industry research, in part because of the nature of thework (i.e., fostering the evaluation and deployment of research ideas)and, in part, because the academic personnel base is relatively limited inareas relating to security. there is also industryfunded research anddevelopment work in nis trustworthiness; that work understandablytends to have more direct relevance to existing or projected markets (itemphasizes development relative to research). a firm calibration of federal funding for trustworthiness research is difficult, both because of conventional problems in understanding how different projects are accountedfor and because this is an area where some relevant work is classified. inaddition, the nature of relevant research often implies a necessary systemsdevelopment component, and that can inflate associated spendinglevels.darpa’s information technology office provides most of thegovernment’s external research funding for nis trustworthiness. increasingly, dod is turning to cots products, which means that darpa canjustifiably be concerned with a much broader region of the presentdaycomputing landscape. but darpafunded researchers are being subjected to pressure to produce shortterm research results and rapid transitions to industry—so much so that the pursuit of highrisk theoretical andexperimental investigations is seemingly discouraged. this influenceswhat research topics get explored. many of the research problems outlined above are deep and difficult, and expecting shortterm payoff canonly divert effort from the most critical areas. in addition, darpa hasdeemphasized its funding of certain securityoriented topics (e.g., containment, defending against denialofservice attacks, and the design ofcryptographic infrastructures), which has caused researcher effort andinterest to shift away from these key problems. therefore, darpa needsto increase its focus on information security and nis trustworthiness research, especially with regard to longterm research efforts. darpa’smechanisms for communicating and interacting with the research community are generally effective.nsa funds information security research through r2 and other of itsorganizational units. the present study deals exclusively with r2. incontrast to darpa, nsa r2 consumes a large portion of its budget internally, including significant expenditures on nonresearch activities. nsa’stwo missions—protecting u.s. sensitive information and acquiring foreign intelligence information—can confound its interactions with otherstrust in cyberspacecopyright national academy of sciences. all rights reserved.executive summary11in the promotion of trustworthiness. its defensive mission makes knowing how to protect systems paramount; its offensive need to exploit system vulnerabilities can inhibit its sharing of knowledge. this tension isnot new. what is relevant for future effort is the lingering distrust for theagency in the academic research community and some quarters of industry, which has had a negative impact on r2’s efforts at outreach. the riseof niss creates new needs for expertise in computer systems that nsa ischallenged to develop internally and procure externally. r2’s difficulty inrecruiting and retaining highly qualified technical research staff is a reason for “outsourcing” research, when highly skilled research staff areavailable elsewhere. r2’s effectiveness depends on better leveraging oftalent both outside and inside the organization.the committee believes that increased funding is warranted for bothinformation security research in particular and nis trustworthiness research in general. the appropriate level of increased funding should bebased on a realistic assessment of the size and availability of the currentpopulation of researchers in relevant disciplines and projections of howthis population of researchers may be increased in the coming years.trust in cyberspace?cyberspace is no longer science fiction. today, networked information systems transport millions of people there to accomplish routine aswell as critical tasks. and the current trajectory is clear: increased dependence on networked information systems. unless these systems are madetrustworthy, such dependence may well lead to disruption and disaster.the aphorism “where there’s a will, there’s a way” provides a succinctway to summarize the situation. the “way,” which today is missing, willrequire basic components, engineering expertise, and an expanded science base necessary for implementing trustworthy networked information systems. this study articulates a research agenda so that there willbe a way when there is a will.trust in cyberspacecopyright national academy of sciences. all rights reserved.12trust in cyberspace12the security of our nation, the viability of our economy, and thehealth and wellbeing of our citizens rely today on infrastructures forcommunication, finance, energy distribution, and transportation. all ofthese infrastructures depend increasingly on networked information systems. that dependence, with its new levels and kinds of vulnerabilities,is attracting growing attention from government and industry. withinthe last 2 years, the office of science and technology policy in the whitehouse, the president’s national security telecommunications advisorycommittee, the president’s commission on critical infrastructure protection, the defense science board, and the general accounting office haveeach issued reports on the vulnerabilities of networked information systems.1 congressional hearings,2 articles in the popular press, and concern1introduction1seecybernation: the american infrastructure in the information age: a technical primer onrisks and reliability (executive office of the president, 1997), reports from the eight nstacsubcommittee investigations (nstac, 1997), critical foundations: protecting america’s infrastructures (pccip, 1997), report of the defense science board task force on information warfaredefense (iwd) (defense science board, 1996), and information security—computer attacks atdepartment of defense pose increasing risks: a report to congressional requesters (u.s. gao,1996).2such as testimony titled “weak computer security in government: is the public at risk?”presented before the senate governmental affairs committee on may 19, 1998, and testimony titled “future threats to the department of defense information systems: y2k &frequency spectrum reallocation,” presented before the senate armed services committeeon june 4, 1998.trust in cyberspacecopyright national academy of sciences. all rights reserved.introduction13about the impending year 2000 problem have further heightened publicawareness. most recently, presidential decision directive 633 has calledfor a national effort to assure the security of our increasingly vulnerablecritical infrastructures.although proposals for action are being advanced, their proceduralemphasis reflects the limitations of available knowledge and technologies for tackling the problem. these limitations constrain effective decision making in an area that is clearly vital to all sectors of society.creating a broader range of choices and more robust tools for building trustworthy networked information systems is essential. to accomplish this, new research is required. and since research takes time tobear fruit, the nation’s dependence on networked information systemswill greatly exceed their trustworthiness unless this research is initiatedsoon.articulating an agenda for that research is the primary goal of thisstudy; that detailed agenda and its rationale constitute the core of thisreport.trustworthy networked information systemsnetworked information systems (niss) integrate computing systems,communications systems, and people (both as users and operators). thedefining elements are interfaces to other systems along with algorithms tocoordinate those systems. economics dictates the use of commercial offtheshelf (cots) components wherever possible, which means that developers of an nis have neither control over nor detailed informationabout many system components. the use of system components whosefunctionality can be changed remotely and while the system is running isincreasing. users and designers of an nis built from such extensiblesystem components thus cannot know with any certainty what softwarehas entered system components or what actions those components mighttake. (appendix e contains a detailed discussion of likely developmentsin software for those readers unfamiliar with current trends.)a trustworthy nis does what people expect it to do—and not something else—despite environmental disruption, human user and operatorerrors, and attacks4 by hostile parties. design and implementation errorsmust be avoided, eliminated, or somehow tolerated. it is not sufficient to3available online at <http://www.ciao.gov>.4in the computer security literature, “vulnerability,”“attack,” and “threat” are technicalterms. a vulnerability is an error or weakness in the design, implementation, or operationof a system. an attack is a means of exploiting some vulnerability in a system. a threat isan adversary that is motivated and capable of exploiting a vulnerability.trust in cyberspacecopyright national academy of sciences. all rights reserved.14trust in cyberspaceaddress only some of these dimensions, nor is it sufficient simply to assemble components that are themselves trustworthy. trustworthiness isholistic and multidimensional.trustworthy niss are challenging systems to build, operate, andmaintain. there is the intrinsic difficulty of understanding what can andcannot happen within any complex system and what can be done tocontrol the behavior of such a system. with the environment only partially specified, one can never know what kinds of attacks will be launchedor what manifestations failures may take. modeling and planning for thebehavior of a sentient adversary are especially hard.the trustworthiness of an nis encompasses correctness, reliability,security (conventionally including secrecy, confidentiality, integrity, andavailability), privacy, safety, and survivability (see appendix k for definitions of these terms). these dimensions are not independent, and caremust be taken so that one is not obtained at the expense of another. forexample, protection of confidentiality or integrity by denying all accesstrades one aspect of security—availability—for others. as another example, replication of components enhances reliability but may increaseexposure to attack owing to the larger number of sites and the vulnerabilities implicit in the protocols to coordinate them. integrating the diversedimensions of trustworthiness and understanding how they interact arecentral challenges in building a trustworthy nis.various isolated dimensions of trustworthiness have becomedefining themes within professional communities and government programs:•correctness stipulates that proper outputs are produced by thesystem for each input.•availability focuses on ensuring that a system continues to operatein the face of certain anticipated events (failures) whose occurrences areuncorrelated.•security is concerned with ensuring that a system resists potentially correlated events (attacks) that can compromise the secrecy, integrity, or availability of data and services.while individual dimensions of trustworthiness are certainly important, building a trustworthy system requires more. consequently, a newterm—“trustworthiness”—and not some extant technical term (with itsaccompanying intellectual baggage of priorities) was selected for use inthis report. of ultimate concern is how people perceive and engage asystem. people place some level of trust in any system, although theymay neither think about that trust explicitly nor gauge the amount realistically. their trust is based on an aggregation of dimensions, not on a fewtrust in cyberspacecopyright national academy of sciences. all rights reserved.introduction15narrowly defined or isolated technical properties. the term “trustworthiness” herein denotes this aggregation.to be labeled as trustworthy, a system not only must behave as expected but also must reinforce the belief that it will continue to produceexpected behavior and will not be susceptible to subversion. the question of how to achieve assurance has been the target of several researchprograms sponsored by the department of defense and others. yet currently practiced and proposed approaches for establishing assurance arestill imperfect and/or impractical. testing can demonstrate only that aflaw exists, not that all flaws have been found; deductive and analyticalmethods are practical only for certain small systems or specific properties.5 moreover, all existing assurance methods are predicated on anunrealistic assumption—that system designers and implementers knowwhat it means for a system to be “correct” before and during development.6 the study committee believes that progress in assurance for theforeseeable future will most likely come from figuring out (1) how tocombine multiple approaches and (2) how best to leverage addon technologies and other approaches to enhance existing imperfect systems.improved assurance, without any pretense of establishing a certain or aquantifiable level of assurance, should be the aim.what erodes trustthe extent to which an nis comes to be regarded as trustworthy isinfluenced, in large part, by people’s experiences in using that system.however, generalizations from individual personal experience can bemisleading. the collection of incidents in neumann (1995) and its associated online database suggests something about the lay of the land, although many kinds of attacks are not chronicled there (for various reasons). other compilations of information on the trustworthiness ofspecific infrastructures can be found at the cert/cc web site7 and othersources. but absent scientific studies that measure dominant detractors ofnis trustworthiness, it is hard to know what vulnerabilities are the mostsignificant or how resources might best be allocated in order to enhance asystem’s trustworthiness. rigorous empirical studies of system outagesand their causes are a necessary ingredient of any research agenda in5see chapter 3 for a more detailed discussion.6requirements invariably change through the development process, and the definition ofsystem correctness changes accordingly.7the computer emergency response team (cert)/coordination center (cc) is an element of the networked systems survivability program in the software engineering institute at carnegie mellon university. see <http://www.cert.org>.trust in cyberspacecopyright national academy of sciences. all rights reserved.16trust in cyberspacetended to further nis trustworthiness. empirical studies of normal system operations are also important, because having baseline data can behelpful for detecting failures and attacks by monitoring usage (ware,1998).but perceptions of trustworthiness are just that and, therefore, can beshaped by the popular press and information from organizations thathave particular advocacy agendas. a predominant cause of nis outagesmight not be a good topic for newspaper stories, although anecdotes ofattacks perpetrated by hackers seem to be.8trust in an nis is not unduly eroded when catastrophic natural phenomena in a region, such as earthquakes or storms, disrupt the operationof niss only in that region. but when environmental disruption hasdisproportionate consequences, trust is eroded. regional and longdistance telephone outages caused by a backhoe accidentally severing a fiberoptic cable (neumann, 1995) and a power outage disrupting internetaccess in the silicon valley area as a result of rodents chewing cableinsulation (neumann, 1996) are just two illustrations. the good news isthat the frequency and scope of accidental manmade and natural disruptions are not likely to change in the foreseeable future. building a trustworthy nis for tomorrow that can tolerate today’s levels of such disruptions should suffice.errors made in the operation of a system also can lead to systemwidedisruption. niss are complex, and human operators err: an operatorinstalling a corrupted toplevel domain name server database at networksolutions effectively wiped out access to roughly a million sites on theinternet in july 1997 (wayner, 1997); an employee’s uploading of an incorrect set of translations into a signaling system 7 processor led to a 90minute network outage for at&t tollfree telephone service in september 1997 (perillo, 1997). automating the human operator’s job is notnecessarily a solution, for it simply exchanges one vulnerability (humanoperator error) for another (design and implementation errors in the control automation).controlling a complex system is difficult, even under the best of circumstances. whether or not human operators are involved, the geographic scope and the speed at which an nis operates mean that assembling a current and consistent view of the system is not possible. thecontrol theory that characterizes the operation of such systems (if knownat all) is likely to be fraught with instabilities and to be highly nonlinear.when operators are part of the picture, details of the system’s operating8the classification and restricted distribution of many government studies about vulnerability and the frequency of hostile attacks, rather than informing the public about realrisks, serve mostly to encourage speculation.trust in cyberspacecopyright national academy of sciences. all rights reserved.introduction17status must be distilled into a form that can be understood by humans.moreover, there is the difficulty of designing an operator interface thatfacilitates human intervention and control.the challenge of implementing software that satisfies its specificationis well known, and failing to meet that challenge invariably compromisessystem trustworthiness. nis software is no exception. an oftcited example is the january 1990 9hourlong outage (blocking an estimated 5million calls) that at&t experienced due to a programming error in software for its electronic switching systems (neumann, 1995). more recently, software flaws caused an april 1998 outage in the at&t framerelay network (a nationwide highspeed data network used by business)(mills, 1998), and in february 1998 the operation of the new york mercantile exchange and telephone service in several major east coast citieswere interrupted by a software failure in illuminet, a private carrier(kalish, 1998).the challenges of developing software can also be responsible forproject delays and cost overruns. problems associated with software thuscan undermine confidence and trust in a system long before the systemhas been deployed. nis software is especially difficult to write, because ittypically integrates geographically separated system components thatexecute concurrently, have idiosyncratic interfaces, and are sensitive toexecution timings.finally, there are the effects of hostile attacks on nis trustworthinessand on perceptions of nis trustworthiness. evidence abounds that theinternet and the public telephone networks not only are vulnerable toattacks but also are being penetrated with some frequency. in addition,hackers seeking the challenge and insiders seeking personal gain or revenge have been successful in attacking business and critical infrastructure computing systems. accounts of successful attacks on computersystems at military sites are perhaps the most disturbing, since tightersecurity might be expected there; box 1.1 contains just a few examples ofrecent attacks on both critical and noncritical dod computers. the defense information systems agency (disa) estimates that dod may haveexperienced as many as 250,000 attacks on its computer systems in arecent year and that the number of such attacks may be doubling9 eachyear (u.s. gao, 1996). the exact number of attacks is not known becausedisa’s own penetration attempts on these systems indicate that onlyabout 1 in 150 attacks is actually detected and reported (u.s. gao, 1996).9specifically, defense installations reported 53 attacks in 1992, 115 in 1993, 255 in 1994,and 559 in 1995.trust in cyberspacecopyright national academy of sciences. all rights reserved.18trust in cyberspacesimilarly troubling statistics about privatesector computer breakins havebeen reported (hardy, 1996; power, 1996; war room research llc, 1996).attacks specifically directed at niss running critical infrastructuresare not frequent at present, but they do occur. according to fbi directorlouis freeh speaking at the march 1997 computer crime conference innew york city, a swedish hacker shut down a 911 emergency call systemin florida for an hour (milton, 1997). and in march of 1997, a series ofcommands sent from a hacker’s personal computer disabled vital servicesto the federal aviation administration control tower at the worcester,massachusetts, airport (boston globe, 1998).box 1.1sampler of department of defense computer penetrations•rome laboratories discovered that more than 150 internet intrusions weremade into 30 computer systems on its network between march 23 and april 16,1994. the attacks, which used trojan horses and network “sniffers,” had beenlaunched by a 16yearold british hacker and an unknown accomplice from commercial internet providers. the attackers took control of laboratory support systemsand stole tactical and artificial intelligence research data (u.s. gao, 1996).•the u.s. naval academy computer system was successfully penetrated indecember 1994. sniffer programs were installed on servers, the system’s name andaddress were changed (making the system inaccessible to authorized users), fileswere deleted, password files were compromised, and more than 12,000 passwordswere changed (u.s. gao, 1996).•in march 1997, a computing system at anderson air force base in guam waspenetrated by a 15yearold working from croatia and using programs freely available on the internet (associated press, 1997).•during the gulf war, email and information about troop movements andmissile capabilities were stolen from department of defense (dod) computers byhackers based in eindhoven, the netherlands. the information was then offered forsale to the iraqis, who rejected the offer, thinking it a hoax (schultz, 1997).•as part of a june 1997 exercise (“eligible receiver”), an nsa hacker teamdemonstrated how to break into dod computers and the u.s. electric power gridsystem. they simulated a series of rolling power outages and 911 emergency telephone overloads in washington, d.c., and other cities. they also succeeded inshowing how to break into unclassified systems at four regional military commandsand the national military command center in washington, d.c. and they showedhow to gain supervisorylevel access to 36 networks, enabling email and telephoneservice disruptions (gertz, 1998; myers, 1998).•in october 1997, the u.s. state department shut down portions of one of itsinternational computer systems after the general accounting office discovered evidence of an intruder in computers at two overseas posts. the affected computersystem links computers in washington, d.c., with 250 u.s. embassies and consulates (zuckerman, 1996).trust in cyberspacecopyright national academy of sciences. all rights reserved.introduction19to a first approximation “everything” is becoming interconnected.the june 1997 pentagon cyberwar game “eligible receiver” (gertz, 1998;myers, 1998) demonstrated that computers controlling electric power distribution are, in fact, accessible from the internet. it is doubtless only amatter of time before the control network for the public telephone network is discovered to be similarly connected—having just one computerconnected (directly or indirectly) to both networks suffices. thus, theinternet will ultimately give ever larger numbers and increasingly sophisticated attackers access to the computer systems that control critical infrastructures. the study committee therefore concluded that resisting attackis a dimension of trustworthiness that, although not a significant sourceof disruption today, has the potential to become a significant cause ofoutages in the future.interconnection within and between critical infrastructures furtheramplifies the consequences of disruptions, making the trustworthiness ofone system conditional on that of another. the lesson of the northeastpower blackout in the late 1960s was that disruptions can propagatethrough a system with catastrophic consequences. three decades later, injuly 1998, a tree shorting a powerline running to a power plant in idahobrought about cascading outages that ultimately took down all three ofthe main california–oregon transmission trunks and interrupted service for 2 million customers (sweet and geppert, 1997). was the lessonlearned?the interdependence of critical infrastructures also enables disruption to propagate. an accidental fiber cut in january 1991 (neumann,1995) blocked 60 percent of the longdistance calls into and out of newyork city but also disabled air traffic control functions in new york,washington, d.c., and boston (because voice and data links to air trafficcontrol centers use telephone circuits) and disrupted the operation of thenew york mercantile exchange and several commodity exchanges (because buy and sell orders, as well as pricing information, are communicated using those circuits). the impact of such a disruption could easilyextend to national defense functions.10 furthermore, a climate of deregulation is promoting cost control and product enhancements in electricpower distribution, telecommunications (board on telecommunicationsand computer applications, 1989), and other critical infrastructures—10in march 1997, disa disclosed that a contract had been awarded to sprint for a globaltelecommunications network designed primarily to carry signal intelligence data to fortmeade (brewin, 1997). according to the defense science board (1996), the u.s. governmentprocures more than 95 percent of its domestic telecommunications network services fromu.s. commercial carriers.trust in cyberspacecopyright national academy of sciences. all rights reserved.20trust in cyberspaceactions that increase vulnerability to disruption by diminishing the cushions of reserve capacity and increasing the complexity of these systems.this study in contextnetwork security, information warfare, and criticalinfrastructureprotection have already been the subject of other national studies. themost visible of these studies—summarized in appendix f—have focusedon the expected shape and consequences of widespread networking, defending against information warfare and other cyberthreats, the coordination of federal and privatesector players in such a defense, and national policies affecting the availability of certain technological buildingblocks (e.g., cryptography). the absence of needed technology has beennoted, and aggressive programs of research to fill broadly characterizedgaps are invariably recommended.a computer science and telecommunications board study almost adecade ago anticipated the role networked computers would play in oursociety along with the problems that they could create (cstb, 1991). itsopening paragraph summarized the situation—then and today—with remarkable clarity:we are at risk. increasingly, america depends on computers. theycontrol power delivery, communications, aviation, and financial services. they are used to store vital information, from medical records tobusiness plans to criminal records. although we trust them, they arevulnerable—to the effects of poor design and insufficient quality control, to accident, and perhaps most alarmingly, to deliberate attack. themodern thief can steal more with a computer than with a gun. tomorrow’s terrorist may be able to do more damage with a keyboard thanwith a bomb.more recently, in october 1997, the president’s commission on critical infrastructure protection released a report (pccip, 1997) that discussesthe vulnerability of u.s. infrastructures to physical as well as cyberthreats. based substantially on the commission’s recommendations andfindings, presidential decision directive 63 (white house national security council, 1998) outlines a procedure and administrative structure fordeveloping a national infrastructure protection plan. the directive ordersimmediate federal government action, with the goal that, within 5 years,our nation’s critical infrastructures will be protected from intentional actsthat would diminish the functioning of government, public services, theorderly functioning of the economy, and the delivery of essential telecommunications, energy, financial, and transportation services. among thedirective’s general principles and guidelines is a request that research forprotecting critical infrastructures be undertaken.trust in cyberspacecopyright national academy of sciences. all rights reserved.introduction21the present study offers a detailed agenda for that research. it is anagenda that was developed by analyzing current approaches to trustworthiness and by identifying science and technology that currently do not,but could, play a significant role. the agenda thus fills the gap left bypredecessor studies, with their focus on infrastructure vulnerabilities andthe wider consequences. articulating a research agenda is a necessaryfirst step in obtaining better methods of infrastructure protection.the research agenda should be of interest to researchers, who willultimately execute the agenda, and to funders of research, who will wantto give priority to research problems that are urgent and approaches thatare promising. the research agenda should also be of interest to policymakers who, in formulating legislation and initiating other actions, willprofit from knowing which technical problems do have solutions, whichwill have solutions if research is supported, and which cannot have solutions. nis operators can profit from the agenda in much the same way aspolicymakers will. and product developers should be interested in theresearch agenda for its predictions of market needs and promising directions to address those needs.scope of this studythe premise of this report is that a “trust gap” is emerging betweenthe expectations of the public (along with parts of government) and thecapabilities of niss. the report is organized around an agenda and callfor research aimed at improving the trustworthiness of niss and therebynarrowing this gap. to develop this agenda, the study committee surveyed the state of the art, current practice, and trends with respect tocomputer networking and software. the committee also studied connections between these technical topics and current economic and politicalforces; those investigations, too, are summarized in the report.some of the research problems in the proposed agenda are new. others are not new but warrant revisiting in light of special requirements andcircumstances that nis developers and operators face. the networkedenvironment imposes novel constraints, enables new types of solutions,and changes engineering tradeoffs. characteristic elements of niss(cots software, extensible components, and evolution by accretion) affect software development practices. and the need to simultaneouslysupport all of the dimensions of trustworthiness invites reconsideringknown approaches for individual dimensions of trustworthiness with aneye toward possible interactions.the internet and public telephone network figured prominently inthe study committee’s thinking, and that emphasis is reflected in chapter2 of this report. the attention is justified on two grounds. first, thetrust in cyberspacecopyright national academy of sciences. all rights reserved.22trust in cyberspaceinternet and public telephone network are themselves large and complexniss. studying extant niss is an obvious way to understand the technicalproblems that will be faced by developers and operators of future niss.second, the high cost of building a global communications infrastructurefrom the ground up implies that one or both of these two networks islikely to furnish communications services for most other niss.11 withsuch a pivotal role, the trustworthiness and vulnerabilities of these communications fabrics need to be understood.commercial software packages and systems—and not systems custombuilt from scratch—are also a central subject of this report, as is mostevident in chapter 3 on software development. this focus is sensiblegiven the clear trend in government and military procurement to adaptand depend on commodities and services intended for the mass market.12research that ignores cots software could have little impact on trustworthiness for tomorrow’s niss.13 in the past, computer science researchprograms serving military needs could safely ignore commercial softwareproducts and practices; that course now invites irrelevance.chapter 4 concerns security. the extensive treatment of this singledimension of trustworthiness merits comment, especially given the relativeinfrequency with which attacks today are responsible for nis outages. aresearch agenda must anticipate tomorrow’s needs. hostile attacks are thefastestgrowing source of nis disturbances. indications are that this trendwill continue14 and that, because they can be coordinated, attacks are potentially the most destabilizing form of trustworthiness breach. furthermore, the study committee found that past approaches to security (i.e., the11for example, during the persian gulf conflict, the internet was used to disseminateintelligence and counterintelligence information. moreover, defense experts believe thatpublic messages originating within regions of conflict will, in the future, provide warningsof significant political and military developments earlier than normal intelligence gathering. these experts also envision the internet as a backup communications medium if otherconventional channels are disrupted during conflicts (u.s. gao, 1996).12according to the report of the defense science board task force on information warfaredefense (iwd) (defense science board, 1996), cots systems constitute over 90 percent ofthe information systems procured by dod. moreover, the widespread use of cots systems in military systems for the coming century is urged in national defense panel (1997).13research that takes into account cots commodities and services is likely to be applicable to the development of customdesigned systems as well. methods suitable for systemsbuilt from scratch, however, may not apply in the presence of the added constraints thatcots purchases impose.14the present study was conducted without access to classified material. unclassifiedstudies, such as u.s. general accounting office (1996), point to the growing incentive toattack infrastructure and defense computing systems, as these systems become more critical, and to the expanding base of potential attackers that is accompanying the growth of theinternet.trust in cyberspacecopyright national academy of sciences. all rights reserved.introduction23“orange book” [u.s. dod, 1985] and its brethren) are less and less relevantto building a trustworthy nis: inappropriate disclosure of information isonly one of many security policies of concern, and custom constructionand/or complete analysis of an entire nis or even significant parts of annis is impractical. the typically complex trust relationships that existamong the parts of an nis add further complication.the“holy grail” for developers of trustworthy systems is technology tobuild trustworthy systems from untrustworthy components. the subject ofchapter 5, this piece of the research agenda is the most ambitious. what isbeing sought can be achieved today for single dimensions of trustworthiness, lending some credibility to the vision being articulated. for example,highly reliable computing systems are routinely constructed from unreliable components (by using replication). as another example, firewallsenable networks of insecure processors to be protected from certain formsof attack. and new algorithmic paradigms and system architectures couldresult in the emergence of desirable system behavior from seemingly random behaviors of system components. without further research, though, itis impossible to know whether approaches like these will actually bear fruitfor nis trustworthiness. fleshing out highly speculative research directions with details is impossible without actually doing some of the research,so the discussions in chapter 5 are necessarily brief.the viability of technological innovations is invariably determined bythe economic and political context, the subject of chapter 6. the economics of building, selling, and operating trustworthy systems is discussed,because economics determines the extent to which technologies for trustworthiness can be embraced by system developers and operators, and itdetermines whether users can justify investments in supporting trustworthiness. the dynamics of the cots marketplace and an implied limiteddiversity have become important for trustworthiness so they, too, arediscussed. risk avoidance is but a single point in a spectrum of riskmanagement strategies; for niss (because of their size and complexity) itis most likely an unrealistic one. thus, alternatives to risk avoidance arepresented in the hope of broadening the perspectives of nis designersand operators. finally, since there is more to getting research done thanarticulating an agenda, the chapter reviews the workings of darpa andnsa (likely candidates to administer this agenda), u.s. cryptographypolicy, and the general climate in government regarding regulation andtrustworthiness.referencesassociated press. 1997. “fifteen year old hacker discusses how he accessed u.s. military files,” associated press, march 1.trust in cyberspacecopyright national academy of sciences. all rights reserved.24trust in cyberspaceboard on telecommunications and computer applications, national research council.1989.growing vulnerability of the public switched networks: implications for nationalsecurity emergency preparedness. washington, dc: national academy press.boston globe. 1998. “youth faces computer crime charges: u.s. attorney says federalcase is first involving a juvenile,”boston globe, march 18. available online at<http://www.boston.com>.brewin, bob. 1997. “disa discloses secret nsa pact with sprint,”federal computerweek, march 10. available online at <http://www.fcw.com/pubs/fcw/1997/0310/disansa.htm>.computer science and telecommunications board (cstb), national research council.1991.computers at risk: safe computing in the information age. washington, dc:national academy press.defense science board. 1996. report of the defense science board task force on informationwarfare defense (iwd). washington, dc: office of the under secretary of defense foracquisition and technology, november 21.executive office of the president, office of science and technology policy. 1997. cybernation: the american infrastructure in the information age: a technical primer on risks andreliability. washington, dc: executive office of the president.gertz, bill. 1998. “‘infowar’ game shut down u.s. power grid, disabled pacific command,”washington times, april 16, p. a1.hardy, quentin. 1996. “many big firms hurt by breakins,”wall street journal, november21, p. b4.kalish, david e. 1998. “phone outage hits east coast,” associated press, february 25.available online at <http://wire.ap.org>.mills, mike. 1998. “at&t high speed network fails red cross, banks scramble to adjust,”washington post, april 14, p. c1.milton, pat. 1997. “fbi director calls for effort to fight growing danger of computercrime,” associated press, march 4.myers, laura. 1998. “pentagon has computers hacked,” associated press, april 16.national defense panel. 1997. transforming defense: national security in the 21st century.arlington, va: national defense panel, december.national security telecommunications advisory committee (nstac). 1997. reports fromthe eight nstac subcommittee investigations. tysons corner, va: nstac, december1011. available online at <http://www.ncs.gov/nstac/nstacreports.html>.neumann, peter g. 1995. computer related risks. new york: acm press.neumann, peter g. 1996. “rats take down stanford power and silicon valley internetservice,”risks digest, vol. 18, issue 52, october 12. available online at <http://catless.ncl.ac.uk/risks/18.52.html#subj1>.perillo, robert j. 1997.“at&t database glitch caused ‘800’ phone outage,”telecomdigest, vol. 17, issue 253, september 18. available online at <http://massis.lcs.mit.edu/telecomarchives/archives/back issues/1997.volume.17/vol17.iss251300>.power, richard g. 1996. testimony of richard g. power, computer security institute,before the permanent subcommittee on investigations, committee on governmentaffairs, u.s. senate, washington, dc, june 5.president’s commission on critical infrastructure protection (pccip). 1997. critical foundations: protecting america’s infrastructures. washington, dc: pccip, october.schultz, gene. 1997. “crackers obtained gulf war military secrets,”risks digest, vol. 18,issue 96, march 31. available online at <http://catless.ncl.ac.uk/risks/18.96.html#subj6>.sweet, william, and linda geppert, eds. 1997. “main event: power outages flag technology overload, rulemaking gaps,”ieee spectrum, 1997 technology analysis and forecast.trust in cyberspacecopyright national academy of sciences. all rights reserved.introduction25u.s. department of defense (dod). 1985. trusted computer system evaluation criteria,department of defense 5200.28std, the “orange book.” ft. meade, md: nationalcomputer security center, december.u.s. general accounting office (gao). 1996. information security—computer attacks atdepartment of defense pose increasing risks: a report to congressional requesters. washington, dc: u.s. gao, may.war room research llc. 1996. 1996 information systems security survey. baltimore, md:war room research llc, november 21.ware, willis h. 1998. the cyberposture of the national information infrastructure. washington, dc: rand critical technologies institute. available online at <http://www.rand.org/publications/mr/mr976/mr976.html>.wayner, peter. 1997. “human error cripples the internet,”new york times, july 17. available online at <http://www.nytimes.com/library/cyber/week/071797dns.html>.white house national security council. 1998. white paper: the clinton administration’spolicy on critical infrastructure protection: presidential decision directive 63. washington, dc: the white house, may 22.zuckerman, m.j. 1996.“postcold war hysteria or a national threat,”usa today, june 5,p. 1a.trust in cyberspacecopyright national academy of sciences. all rights reserved.26trust in cyberspace26the public telephone network (ptn) and the internet are both largeniss. studying their trustworthiness thus gives insight into the technicalproblems associated with supporting trustworthiness in an nis. identifying the vulnerabilities in these networks is also valuable—any nis islikely to employ one or both of these networks for its communication andcould inherit those vulnerabilities. in some ways, the internet and ptn are very similar. no singleentity owns, manages, or can even have a complete picture of either.•the ptn in the united states comprises five distinct regional belloperating companies and a large number of independent local telephonecompanies, all interconnected by longdistance providers.1•the u.s. portion of the internet consists of a few major internetservice providers (isps) along with a much larger number of local orregional network providers, sometimes referred to as downstream service providers (dsps). the isps are interconnected, either by direct linksor by using network access points distributed around the country.•both networks involve large numbers of subsystems operated bydifferent organizations. the number and intricate nature of the interfacesthat exist at the boundaries of these subsystems are one source of complexity for these networks. the increasing popularity of advanced services is a second source.2public telephone networkand internet trustworthiness1additional consolidation among the regional operating companies remains a real possibility; at the same time, pressure for competition in the local telephone market will probably increase the number of major players.trust in cyberspacecopyright national academy of sciences. all rights reserved.public telephone network and internet trustworthiness27the vulnerabilities of the ptn and internet are exacerbated by thedependence of each network on the other. much of the internet usesleased telephone lines as its physical transport medium. conversely,telephone companies rely on networked computers to manage their ownfacilities, increasingly employing internet technology, although not necessarily the internet itself. thus, vulnerabilities in the ptn can affect theinternet, and vulnerabilities in internet technology can affect the telephone network.this chapter, a study of vulnerabilities in the ptn and the internet,has three parts. the first discusses the design and operation of bothnetworks. the second examines environmental disruption, operationalerrors, hardware and software design and implementation errors, andmalicious attacks as they apply to the networks. finally, the chapterconcludes by analyzing two emerging issues: internet telephony and theexpanding use of the internet by business.network designthe public telephone networknetwork services and designthe ptn has evolved considerably over the past decades. it is nolonger simply a network comprising a set of linked telephone switches,many of which are connected by copper wires to each and every telephone instrument in the country. there are now many telephone companies that provide advanced services, such as tollfree numbers, call forwarding, networkbased programmable call distribution, conferencecalling, and message delivery. the result is a network that is perhapsmore flexible and responsive to customer needs but also more complex.the flexibility and complexity are sources of vulnerability.some of the advanced services also have intrinsic vulnerabilities.with call forwarding, for example, a caller can unknowingly reach anumber different from the one dialed. consequently, a caller can nolonger make assumptions about what number a call will reach, and therecipient no longer knows what number a caller is intending to reach.havoc could result if an attacker modified the telephone network’s database of forwarding destinations.2 as a second example, with network2in one recent case, a plumber call forwarded his competitor’s telephone number to hisown, thereby gaining the callers’ business without their knowledge of the deception. callforwarding could also subvert the purpose of dialback modems used for security. here,the presumption is that only authorized users have access to certain telephone numbers.trust in cyberspacecopyright national academy of sciences. all rights reserved.28trust in cyberspacebased programmable call distribution, a voice menu greets callers andallows a company to direct its incoming calls according to capabilities indifferent offices, time zones, and so on. the menus and distributioncriteria can be modified directly by the company and uploaded into atelephone network database. but, as with call forwarding, a database thatcan be modified by telephone network customers constitutes a potentialvulnerability.the telephone network is made up of many different kinds of equipment that can be divided roughly into three major categories: signaling,transmission, and operations. signaling equipment is used to set up andtear down calls. this category also includes databases and adjunct processors used for number translation and call routing. transmission equipment carries the actual conversations. operations equipment, includingthe operations support system (oss), is used for provisioning, databaseupdates, maintenance, billing, and the like.all communication between modern centraloffice switches takesplace over a dedicated data network using protocols, such as signalingsystem 7 (ss7), which the switches use to set up calls, establish who paysfor the call, return busy signals, and so on. such outofband signalinghelps prevent fraud (such as the deceptions of the 1960s and 1970s madepossible by the infamous “blue boxes,” which sent network control tonesover the voice path) and helps conserve resources (i.e., no voice path needever be allocated if the target number is busy). however, outofbandsignaling does introduce new vulnerabilities.3 failure of the signalingpath can prevent completion of a call, even if there is an available routefor the call itself.authenticationauthentication is a key part of any scheme for preventing unauthorized activity. in a network containing programmable elements, authentication is an essential ingredient for protecting those elements from perwhen such users try to log in, the site calls them back. but the system has no way ofknowing whether the person who answers the callback is really the authorized user, andcall forwarding could cause the callback to be redirected.3ss7 messages are carried over a mix of private and public x.25 (data) networks, providingoutofband signaling. however, such networks, especially public ones, are subject to variousforms of attacks. there is even a curious semicircularity here, since the x.25 interswitchtrunks usually are provisioned from telephone company longdistance circuits, although notfrom the switched circuits that ss7 manages. owing to deregulation designed to foster competition, telephone companies must allow essentially anyone to connect into ss7 networks fora modest fee ($10,000). ss7 is a system that was designed for use by a closed community, andthus embodies minimal security safeguards. it is now employed by a much larger community, which makes the ptn subject to a broad range of “insider” attacks.trust in cyberspacecopyright national academy of sciences. all rights reserved.public telephone network and internet trustworthiness29forming actions illicitly requested by attackers. specifically, in the ptn,the osss must be able to authenticate requests in order to control changesin the configuration of the elements constituting the network. in addition, authentication is required to support certain advanced services, suchas caller id.4 to prevent caller id from subversion, all elements in thepath from the caller to the recipient must be authenticated.the need for authentication by osss is growing because interconnections among previously isolated networks has increased the risk of external intrusions. as the ptn’s management networks convert to the transmission control protocol/internet protocol (tcp/ip) and are connectedto other tcp/ipbased networks, ignoring authentication may prove disastrous. historically, proprietary protocols and dedicated networks wereused for the network’s management, so knowledge of these was restrictedto insiders, and there was little need for authentication or authorization ofrequests.the internetnetwork services and designthe internet, a successor to the arpanet (mcquillan and walden,1977), is a worldwide packetswitched computercommunications network.it interconnects two types of processors: hosts and routers. hosts are thesource and destination for all communications; routers5 forward packetsreceived on one communications line to another and thereby implement acommunication. a shared set of protocols and service architecture wasdesigned to provide support for various forms of robust communication(e.g., email, remote terminal access, file transfer, the world wide web)despite outages and congestion. little design effort was devoted to resisting attacks, although subsequent department of defense research has doneso. and the designers elected to eschew service guarantees in favor ofproviding service on a “best effort” basis. for example, the internet protocol (ip), a datagram service used extensively by the internet, does not guarantee delivery and can deliver duplicates of messages.64caller id is an advanced service that identifies the originator of a telephone call to asuitably equipped receiver. as this service becomes more pervasive, it will be used moreand more for identification and authentication by systems employing the telephone network for communications. here, then, is a vulnerability that can propagate from a communications fabric into an nis that is built on top of that fabric.5routers sometimes act as hosts for purposes of network management and exchangingrouting protocol messages.6isps are now beginning to offer quality of service features (e.g., using rsvp), so the bestefforts notion of ip service may change over the next few years.trust in cyberspacecopyright national academy of sciences. all rights reserved.30trust in cyberspacethe internet’s protocols have proven remarkably tolerant to changesin the size of the network and to decades of order of magnitude improvements in communications bandwidth, communications speed, and processor capacity. in electing for “best effort” services, the internet’s designers made it easier for their protocols to tolerate outages of hosts,routers, and communications lines. selecting the weaker service modelalso simplified dealing with router memory and processing capacity limitations. the internet protocols were designed to operate over a range ofnetwork technologies being explored by the military in the 1970s from 56kbps arpanet trunks to 10mbps ethernets and a mix of satellite andlowspeed tactical packet radio networks. despite two decades of network technology evolution, these protocols perform relatively well intoday’s internet, which has a backbone and other communications linesthat are far faster.routing protocols in the internet implement networktopology discovery, calculation of shortest routes, and recovery (i.e., alternate route selection) from link and router outages. initially, all of the internet’s routerswere owned and operated by a single entity, making it reasonable to assume that all routers were executing compatible protocols and none wouldbehave maliciously. but as the internet matured, ownership and control ofthe routers became disbursed. more robust but less cooperative routingprotocols were developed, thereby limiting the internet’s vulnerability tomalicious and faulty routers. the exterior gateway protocol (mills, 1984)was originally employed for communication with routers outside an originating domain; today, the border gateway protocol (bgp) (rekhter and li,1995; rekhter and gross, 1995; traina, 1993, 1995) is used.a routing protocol must resolve the tension between (1) performancegains possible given information about the far reaches of the network and(2) increased vulnerability that such dependence can bring. by trustinginformation received from other domains, a router can calculate nearoptimal routes, but such routes are useless if based on inaccurate information provided by malicious or malfunctioning routers. conversely, restricting the information that routers share allows routing tables to besmaller, hence cheaper to compute, but sacrifices control over route quality. today’s internet routing protocols generally favor cost over routequality, but isps override this bias toward minimum hop routes in thecontext of interdomain routing.7communication in the internet depends not only on the calculation ofrouting tables but also on the operation of the domain name service7isps use the local policy feature of the border gateway protocol (bgp) to favor routes thatmight not be selected by bgp on a minimumhop basis. this is necessary to balance trafficloads and to reduce vulnerability to configuration errors, or malicious attacks, on bgp.trust in cyberspacecopyright national academy of sciences. all rights reserved.public telephone network and internet trustworthiness31(dns) (mockapetris, 1987a,b). the most important function of this service is to map host names, such as <www.nas.edu>, into numeric ipaddresses. dns also maps ip addresses into host names, defines inboundmail gateways, and so on. the name space implemented by dns is treestructured. the top level has a handful of generic names (.com, .net,.gov, and the like)8 as well as twoletter names corresponding to international organization for standardization (iso) country codes (.us, .uk,.de, .ru, and so forth). definitive information for each level of the tree ismaintained by a single master server; additional servers for a domaincopy their information from it. subtrees of the name space can be (andgenerally are) delegated to other servers. for example, .com and .netcurrently reside by chance on the same server as do the root name servers;.us, though, is delegated. individual sites or machines may cache recently retrieved dns records; the intended lifetime of such cache entriesis controlled by the source of the cached records.network management tasks in the internet are implemented usingthe simple network management protocol (snmp) (case et al., 1990).snmp itself is quite elementary—it merely uses the user datagrams protocol (udp) to read and alter predefined parameters. these parameters,called management information bases (mibs), are organized in a treestructure with branches representing mib type, protocol structure, devicetype, and vendor. the hard task in managing a network is not the mechanics of changing values of parameters; it is knowing what mib variables to set in order to effect some desired change in network behavior.snmp provides no assistance here. most of the deployed implementations of snmp also lack good security features, so the protocol has beenused primarily to retrieve data from mibs in managed devices, not tomake changes to these mibs. instead, telnet, a protocol that can be usedwith a variety of user authentication technologies, is often used for modification of mibs. the latest version (3) of snmp promises to overcomethese security limitations.perhaps the most visible internet service is the world wide web.9the web is implemented by servers that communicate with web browsers (clients) using the hypertext transfer protocol (http) (bernerslee etal., 1996) to retrieve documents represented in hypertext markup language (html) (bernerslee and connolly, 1995). html documents con8at this time, there is an active debate over how many new toplevel names to add andwho should make the decisions. the outcome of this debate may change some of thedetails presented here; the overall structure, however, is likely to remain the same. severalof the generic toplevel domain names are decidedly u.s.centric. .mil and .gov arerestricted to u.s. military and government organizations, and most of the entries in the.edu domain are from the united states.9indeed, many think that the web is the internet.trust in cyberspacecopyright national academy of sciences. all rights reserved.32trust in cyberspacetain data (text, images, audio, video, and so on), as well as uniform resource locators (urls) (bernerslee et al., 1994) to reference other htmldocuments. an html document can be a file stored by a web server orthe output from a program, known as a common gateway interface (cgi)script, run by the web server in response to a client request. cgi scripts,although not necessarily installed or managed by system administrators,are basically network servers accessible to internet users. bugs, therefore,can be a source of vulnerability.http treats each client request as separate and independent. thus,information about past interactions must be stored and retrieved explicitly by the server in processing each request, usually an unnatural style ofprogramming. the information can be stored by the client, as “cookies”(kristol and montulli, 1997) or as hidden fields in urls and forms, or itcan be stored by the server, or it can be stored as part of a secure socketlayer10 (ssl) session index (if the http session is being cryptographicallyprotected). observe that with the latter two schemes, the server’s statebecomes visible to the client and the client must implement any security.http uses tcp and makes large numbers of shortlived tcp connections (even between the same pairs of hosts). tcp, however, was designed to support comparatively longlived connections. web browsersthus cannot benefit from tcp’s congestioncontrol algorithms (stevens,1997; jacobson, 1988). that means that the load imposed by the web onthe internet’s routers and communications lines not only is disproportionately high but also reduces network throughput. although http 1.1(fielding et al., 1997) is mitigating this particular problem, it does exemplify a broader concern: deploying an application that does not matchassumptions made by the internet’s designers can have a serious globalimpact on internet performance.for implementing a trustworthy nis, the internet’s“best effort” service semantics is probably not good enough. bandwidth, latency, routediversity, and other quality of service (qos) guarantees are likely to beneeded by an nis. efforts are under way to correct this internet deficiency. but accommodating qos guarantees seems to require revisiting afundamental architectural tenet of the internet—that intelligence and stateexist only at the network’s periphery. the problem is that, without adding state to routers (i.e., the “inside” of the network), the internet’s routerswould lack a basis for processing some packets differently from others toenforce differing qos guarantees.the most ambitious scheme to provide qos guarantees in the internet relies on the new resource reservation protocol (rsvp) (braden et al.,1997). this protocol transmits bandwidth requests to the routers in a10available on line at <http://home.netscape.com/eng/ssl3/ssltoc.html>.trust in cyberspacecopyright national academy of sciences. all rights reserved.public telephone network and internet trustworthiness33communications path on a hopbyhop basis. the receiver makes a request of an adjacent router; that router, in turn, passes the request to itspredecessor, and so on, until the sender is reached. (special messagesconvey the proper path information to the receiver, and thence to eachrouter.) the rsvp bandwidth requests feed the internet’s integrated services model (shenker and wroclawski, 1997) with parameters that include bandwidth, latency, and maximum packet size. with rsvp, bandwidth reservations in routers are not permanent. they may berelinquished explicitly or, if not periodically refreshed, they expire.note that rsvp reservations are not required for packets to flow. theterm“soft state” has been coined for such saved information—information whose loss may impair performance but does not disrupt functionalcorrectness (i.e., the internet’s“best effort” semantics). the use of softstate in rsvp means that changes in routings or the reboot of a routercannot cause a communications failure, and packets will continue to flow,albeit without performance guarantees. by periodically refreshing reservations, performance guarantees can be reactivated.differentiated service, an alternative to rsvp for providing qos inthe internet, employs bits in packet headers to indicate classes of service.each class of service has associated service guarantees. the bits are inspected at network borders, and each network is responsible for takingappropriate measures in order to satisfy the guarantees.authentication (and other security protocols)concern about strong and useable authentication in the internet isrelatively new. the original internet application protocols used plaintextpasswords for authentication, a mechanism that was adequate for casuallogins but was insufficient for more sophisticated uses of a network,especially in a local area network environment. rather than build propercryptographic mechanisms—which were little known in the civilian sector at that time—the developers of the early internet software for unixresorted to networkbased authentication for remote login and remoteshell commands. the servers checked their clients’ messages by converting the sender’s ip address into a host name. user names in such messages are presumed to be authentic if the message comes from a hostwhose name is trusted by the server. senders, however, can circumventthe check by misrepresenting their ip address11 (something that is moredifficult with tcp).11a number of different attacks are known. they can be accomplished in a number of ways,such as sequence number guessing (morris, 1985) or route corruption (bellovin, 1989). alternatively, the attacker can target the addresstoname translation mechanism (bellovin, 1995).trust in cyberspacecopyright national academy of sciences. all rights reserved.34trust in cyberspacebut cryptographic protocols—a sounder basis for network authentication and security—are now growing in prominence on the internet.linklayer encryption has been in use for many years. (see box 2.1 for thenames and descriptions of various network layers.) it is especially usefulwhen just a few links in a network need protection. (in the latter days ofthe arpanet, milnet trunks outside the continental united stateswere protected by link encryptors.) although linklayer encryption hasthe advantage of being completely transparent to all higherlayer devicesand protocols, the scope of its protection is limited. accordingly, attention is now being focused on networklayer encryption (see box 2.2).networklayer encryption requires no modification to applications, and itcan be configured to protect hosttohost, hosttonetwork, or networktonetwork traffic. cost thus can be traded against granularity of protection.networklayer encryption is instantiated in the internet as the ip security (ipsec) protocol, which is designed to run on the internet’s hostsand routers, or on hardware outboard to either.12 the initial deploymentof ipsec has been in networktonetwork mode. this mode allows virtualprivate networks to be created so that the otherwise insecure internet canbe incorporated into an existing secure network, such as a corporate netbox 2.1open systems interconnection network layersphysical link:mechanical, electrical, and procedural interfaces to the transmissionmedium that convert it into a stream that appears to be free of undetected errorsnetwork:routes from sender to receiver within a single network technologyand deals with congestion (x.25, frame relay, and asynchronoustransfer mode fall into this layer)internetwork:sometimes combined with the network layer; provides routing andrelay functions from the sender to the receiver and deals with congestion (internet protocol falls into this layer)transport:responsible for endtoend delivery of data (transmission controlprotocol and user datagram protocol fall into this layer)session:allows multiple transportlayer connections to be managed as a single unit; not used on the internetpresentation:chooses common representations, typically application dependent,for data; rarely used on the internetapplication:deals with applicationspecific protocols12rfc 2401, security architecture for the internet protocol, and rfc 2411, ip security document roadmap, are both forthcoming (<ftp://ftp.isi.edu/innotes>).trust in cyberspacecopyright national academy of sciences. all rights reserved.public telephone network and internet trustworthiness35box 2.2a history of networklevel encryptionlinklevel encryption is an old idea. it first emerged in the form of vernam’sonline teletype encryptor in 1917 (kahn, 1976). various forms were used by assorted combatants during world war ii. but link encryption has a number of drawbacks,notably a very limited scope of protection. this is especially problematic for a multinode network like the arpanet or the internet, in which every single link must beprotected and messages exist in plaintext at every intermediate hop. encryption atthis level is also a rather complex problem if the link level itself is a multiaccessnetwork.the military used link encryption with arpanet technology to protect the communications lines connecting interface message processors (imps) in several department of defense packet networks. the difficulties of scaling this technology economically to some environments led to the development of the private line interface (pli)encryptor (bbn, 1978), which operated at (for the arpanet) the network layer. withthe advent of the internet and the presumed imminent arrival of open systems interconnection (osi) networks, it rapidly became obvious that a more flexible encryptionstrategy was necessary. the result was blacker (weissman, 1992), which sat betweena host and an imp and operated on x.25 packets. blacker ignored internet protocol (ip)addresses (although these had been mapped algorithmically into x.25 addresses by thehost); it did, though, look at the security labels in the ip header.as imps fell out of favor as the preferred switches, a new hardware strategy wasnecessary. furthermore, the national security agency wanted to use publickeytechnology—a success in the secure telephone unit iii (stu iii) deployment—fordata. accordingly, the secure data network system (sdns) project devised a truenetworklayer encryption standard known as security protocol at level 3 (sp3).sp3could operate directly over x.25 networks; it also could (and generally did) operatewith osi or ip networklayer headers below it. it could handle hosttohost, hosttonetwork, and networktonetwork encryption. several sp3 devices, such as caneware and the network encryption system (nes), were built and deployed.this standard achieved a fundamental advance by enabling network managers ordesigners to trade cost for granularity of protection. the other fundamental advancein sp3 was the separation of the keymanagement protocol from the actual cryptographic layer. in effect, key management became just another application, tremendously simplifying the entire concept. sp3 served as the model for osi’s networklayer security protocol (nlsp), but the protocol was complicated by the need towork with both connectionoriented and connectionless network layers, and veryfew nlsp products were ever deployed.both sdns and osi also specified transportlevel encryption protocols (sp4 andtlsp, respectively). these never caught on, and they appear to be an evolutionarydead end.sp3 was the inspiration for swipe (ioannidis and blaze, 1993), a simple hostbased ip encryptor. this, in turn, gave rise to the internet engineering task force’sworking group on ipsec. although ip security (ipsec) is, in many ways, very similarto sp3, its overall model is more complete. much more attention was paid to issuessuch as firewall integration, selective bypass (one need not encrypt traffic to all destinations), and so on. the initial deployment of ipsec appears to be in networktonetwork mode; hosttonetwork mode, for telecommuters, appears to be followingclosely behind.trust in cyberspacecopyright national academy of sciences. all rights reserved.36trust in cyberspacework. the next phase of deployment for ipsec will most likely be thehosttonetwork mode, with individual hosts being laptops or home machines. that would provide a way for travelers to exploit the global reachof the internet to access a secure corporate network.it is unclear when general hosttohost ipsec will be widely deployed.although transparent to applications, ipsec is not transparent to systemadministrators—the deployment of hosttohost ipsec requires outboardhardware or modifications to the host’s protocol system software. because of this impediment to deploying ipsec, the biggest use of encryption in the internet is currently above the transport layer, as ssl embedded into popular web browsers and servers. ssl, although quite visibleto its applications, affects only those applications and not the kernel or thehardware. ssl can be deployed without supervision by a central authority, the approach used for almost all other successful elements of internettechnology.higher still in the protocol stack, encryption is found in fairly widespread use for the protection of electronic mail messages. in this manner,an email message is protected during each simple mail transfer protocol(postel, 1982), while spooled on intermediate mail relays, while residing inthe user’s mailbox, while being copied to the recipient’s machine, and evenin storage thereafter. however, no secure email format has been bothstandardized by the internet engineering task force (ietf) and acceptedby the community. two formats that have gained widespread support ares/mime (dusse et al., 1998a,b) and pgp (pretty good privacy) (zimmerman, 1995). both have been submitted to the ietf for review.findings1.the ptn is becoming more vulnerable as network elements become dependent on complex software, as the reliance on calltranslationdatabases and adjunct processors grows, and as individual telephonecompanies increasingly share facilities with the internet.2.as the ptn is increasingly managed by osss that are less proprietary in nature, information about controlling osss will become morewidespread and osss will be vulnerable to larger numbers of attackers.3.new user services, such as caller id, are increasingly being used toprovide authenticated information to customers of the ptn. however,the underlying telephone network is unable to provide this informationwith high assurance of authenticity.4.the internet is becoming more secure as its protocols are improvedand as enhanced security measures are more widely deployed at higherlevels of the protocol stack. however, the internet’s hosts remain vulnerable, and the internet’s protocols need further improvement.trust in cyberspacecopyright national academy of sciences. all rights reserved.public telephone network and internet trustworthiness375.the operation of the internet depends critically on routing andname to address translation services. this list of critical services willlikely expand to include directory services and publickey certificate servers, thereby adding other critical dependencies.6.there is a tension between the capabilities and risks of routingprotocols. the sharing of routing information facilitates route optimization, but such cooperation also increases the risk that malicious or malfunctioning routers can compromise routing.network failures and fixesthis section examines some causes for internet and ptn failures.protective measures that already exist or might be developed are alsodiscussed. the discussion is structured around the four broad classes ofvulnerabilities described in chapter 1: environmental disruption, operational errors, hardware and software design and implementation errors,and malicious attacks.environmental disruptionin this report, environmental disruption is defined to include naturalphenomena, ranging from earthquakes to rodents chewing through cableinsulation, as well as accidents caused by human carelessness. environmental disruptions affect both the ptn and the internet. however, theeffects and, to some extent, the impact of different types of disruptiondiffer across the two networks.link failuresthe single biggest cause of ptn outages is damage to buried cables(nric, 1997). and the single biggest cause of this damage is constructioncrews digging without proper clearance from telecommunications companies and other utilities. the phenomenon, jocularly known in the tradeas“backhoe fading,” is probably not amenable to a technological solution. indeed, pursuant to the network reliability and interoperabilitycouncil (nric) recommendation, the federal communications commission (fcc) has requested legislation to address this problem.13the impact of backhoe fading on network availability depends on theredundancy of the network. calls can be routed around failed links, butonly if other links form an equivalent path. prior to the 1970s, most of the13both the proposed text and the letter to congress are available online at <http://www.fcc.gov/oet/nric>.trust in cyberspacecopyright national academy of sciences. all rights reserved.38trust in cyberspacenation’s telephone network was run by one company, at&t. as a regulated monopoly, at&t was free to build a network with spare capacityand geographically diverse, redundant routings. multiple telephone companies compete in today’s market, and cost pressures make it impracticalfor these telephone companies to build and maintain such capacious networks. furthermore, technical innovations, such as fiber optics and wavedivision multiplexing, enable fewer physical links to carry current levelsof traffic. the result is a telephone network in which failure of a singlelink can have serious repercussions.one might have expected that having multiple telephone companieswould contribute to increased capacity and diversity in the telephonenetwork. it does not. major telephone companies lease circuits from eachother to lower their own costs. this practice means that backup capacitymay not be available when needed. to limit outages, telephone companies have turned to newer technologies. synchronous optical network(sonet) rings, for example, provide redundancy and switchover at alevel below the circuit layer, allowing calls to continue uninterruptedwhen a fiber is severed. despite the increased robustness provided bysonet rings, the very high capacity of fiber optic cables results in agreater concentration of bandwidth over fewer paths because of economicconsiderations. this means that the failure, or sabotage, of a single linkwill likely disrupt service for many customers.the internet, unlike the ptn, was specifically designed to toleratelink outages. when a link outage is detected, the internet routes packetsover alternate paths. in theory, connections should continue uninterrupted. in practice, though, there may not be sufficient capacity to accommodate the additional traffic on alternate paths. the internet’s routing protocols also do not respond immediately to notifications of linkoutages. having such a delay prevents routing instabilities and oscillations that would swamp routers and might otherwise arise in response totransient link outages. but these delays also mean that, although packetsare not lost when a link fails, packet delivery can be delayed. in additionto the route damping noted here, there is a disturbing trend for isps torely on static configuration of primary and backup routes in bgp borderrouters. this means that internet routing is less dynamic than was originally envisioned. the primary motivations for this move away from lessconstrained dynamic routing are a desire for increased route stability andreduced vulnerability to attacks or configuration errors by isps and dsps.congestioncongestion occurs when load exceeds capacity. environmental disruptions cause increased loads in two ways. first, the load may cometrust in cyberspacecopyright national academy of sciences. all rights reserved.public telephone network and internet trustworthiness39from outside the network—for example, from people checking by telephone with friends and relatives who live in the area of an earthquake.second, the load may come from within the network—existing load thatis redistributed in order to mask outages caused by the environmentaldisruption. in both scenarios, network elements saturate, and the consequences are an inability to deliver service, perhaps at a time when it ismost needed.the ptn is able to control congestion better than the internet is. whena telephone switch or telephone transmission facility reaches saturation,new callers receive “reorder” (i.e., “fast” busy) signals and no further callsare accepted. this forestalls increased load and congestion. ptn operations staff can even block call attempts to a given destination at sources,thereby saving network resources from being wasted on calls that areunlikely to be completed. for example, when an earthquake occurs nearsan francisco, the operations staff might decide to block almost all incoming calls to the affected area codes from throughout the entire ptn.congestion management in the internet is problematic, in part, because no capabilities exist for managing traffic associated with specificusers, connections, sources, or destinations, and it would be difficult toimplement such capabilities. all that a simple router can do14 is discardpackets when its buffers become full. to implement fairness, routerswould have to store information about users and connections, somethingthey are not built to do. retaining such information would require largeamounts of storage. managing this storage would be difficult, becausethe internet has no callteardown messages that are visible to routers.furthermore, the concept of a “user”—that is, an entity that originates orreceives traffic—is not part of the network or transport layers of the internet protocols.chokingback load offered by specific hosts (in analogy with ptnreorder signals) is also not an option for preventing internet congestion,since an ipcapable host can have connections open to many destinationsconcurrently. stopping all flows from the host is clearly inappropriate.more generally, avoiding congestion in the internet is intrinsically hardbecause locales of congestion (i.e., routers and links) have no straightforward correspondence to the communications abstractions (i.e., connections) that end points see. this problem is particularly acute for thehighly dynamic traffic flows between isps. here, very high speed (e.g.,14in fact, routers can transmit an icmp (internet control message protocol) sourcequench message to advise a host of congestion, but there has never been a standard, accepted response to receipt of a source quench, and many hosts merely ignore such messages. in such circumstances the resources needed to construct and send the source quenchmay be wasted and may compound the problem!trust in cyberspacecopyright national academy of sciences. all rights reserved.40trust in cyberspaceoc12) circuits are used to carry traffic between millions of destinationsover short intervals, and the traffic mix can completely change over a fewseconds.although congestion in the internet is nominally an iplayer phenomena—routers have too many packets for a given link—measures fordealing successfully with congestion have resided in the tcp layer(jacobson, 1988). some newer algorithms work at the ip level (floyd andjacobson, 1993), but more research is needed, especially for defining andenforcing flexible and varied policies for congestion control. one suggestion involves retaining information about flows from which packets havebeen repeatedly dropped. such flows are deemed uncooperative and, assuch, are subjected to additional penalties (floyd and fall, 1998); cooperating flows respond to indications of congestion by slowing down theirtransmissions.more research is also needed to measure and understand currentinternet traffic as well as expected future trends in that traffic. some workhas been done (e.g., thompson et al., 1997), but far too little is knownabout usage patterns, flow characteristics, and other relevant parameters.having such information is likely to enable better congestion controlmethods. however, usage patterns are dictated by the application designs and, as new applications arise and become popular, traffic characteristics change. today, the use of the web has changed packet sizesradically compared to a time when file transfer and email were the principal applications. even within the web environment, when a very popular web site arises, news of its location spreads quickly, and traffic flowsshift noticeably!two further difficulties are associated with managing congestion innetworks. first, there appears to be a tension between implementingcongestion management and enforcing network security. a congestioncontrol mechanism may need to inspect and even modify traffic beingmanaged, but strong network security mechanisms will prohibit readingand modifying traffic en route. for example, congestion control in theinternet might be improved if ip and tcp headers were inspected andmodified, but the use of ipsec will prevent such actions.a second difficulty arises when a network comprises multiple independent but interconnected providers. in the internet, no single party iseither capable of or responsible for most endtoend connections, and localoptimizations performed by individual providers may lead to poor overallutilization of network resources or suboptimal global behavior. in theptn, which was designed for a world with comparatively few telephonecompanies but in which switches can be trusted, competitive pressures arenow forcing telephone companies to permit widespread interconnectionstrust in cyberspacecopyright national academy of sciences. all rights reserved.public telephone network and internet trustworthiness41between switches that may not be trustworthy. this opens telephone networks to both malicious and nonmalicious failures (nric, 1997).findings1.technical and market forces have reduced reserve capacity and thenumber of geographically diverse, redundant routings in the ptn. failure of a single link can now have serious repercussions.2.current internet routing algorithms are inadequate. they do notscale well, they require cpu (central processing unit)intensive calculations, and they cannot implement diverse or flexible policies. furthermore, little is known about how best to resolve the tension between thestability of routing algorithms and the delay that precedes a routingchange in response to an outage.3.a better understanding is needed of the internet’s current trafficprofile and how it will evolve. in addition, fundamental research isneeded into mechanisms for supporting congestion management in theinternet, especially congestion management schemes that do not conflictwith enforcing network security.4.networks formed by interconnecting extant independent subnetworks present unique challenges for controlling congestion (because localprovider optimizations may not lead to good overall behavior) and forimplementing security (because trust relationships between network components are not homogeneous).operational errors“to err is human” the saying goes, and human operator errors areindeed responsible for network outages, as well as for unwittingly disabling protection mechanisms that then enable hostile attacks to succeed.located in a network operations center (see box 2.3), operators take actions based on their perceptions of what the network is doing and what itwill do, but without direct knowledge of either. in these circumstances,the consequences of even the most carefully considered operator actionscan be surprising—and devastating.with regard to the ptn, the network reliability and interoperabilitycouncil found that operational errors caused about one in every fourtelephone switch failures (nric, 1996). mistakes by vendors, mistakes ininstallation and maintenance, and mistakes by system operators all contributed. for example, in 1997, an employee loading an incorrect set oftranslations into an ss7 processor led to a 90minute network outage fortollfree telephone service (perillo, 1997), and the recent outage of thetrust in cyberspacecopyright national academy of sciences. all rights reserved.42trust in cyberspace15two independent software bugs also contributed to this frame relay network outage.at&t frame relay network (mills, 1998) was attributed in part to operational procedures.15the internet has also been a victim of operational errors, although thefrequency and specific causes have not been analyzed thoroughly as for theptn. examples abound, however. perhaps the most serious incident occurred in july 1997, when a process intended to generate a major part of thedns from a database failed. automated mechanisms alerted operatorsthat something was wrong, but a system administrator overrode the warning, causing the apparent deletion of most machines in that zone. there arealso numerous instances of the bogus information stored by misconfigureddns servers propagating into name server caches and then confusing machines throughout the internet. similar problems have occurred with regard to internet routing as well. for example, in april 1997, a small ispbox 2.3network operations centerseach public telephone network (ptn) or internet constituent has some form ofnetwork operations center (noc). for a small downstream service provider (dsp), thenoc may be a portion of a room in a home or office. for a local telephone company,longdistance carrier, or nationallevel internet service provider (isp), an noc couldoccupy considerably more space and likely will involve substantial investments inequipment and infrastructure. a large network provider may have multiple, geographically dispersed nocs in order to share the management load and provide backup.the purpose of an noc is to monitor and control the elements of a network:switches, transmission lines, access devices, and so on. human operators monitor avariety of graphical images of network topology (physical and logical) that show thestatus of network elements. ordinary computer monitors often serve as these displaydevices.1 a typical display could indicate which switch interfaces or switches appear to be malfunctioning, or which circuits are out of service. some displays mayeven indicate which links are approaching saturation.the displays rarely tell an operator how to solve a problem whose symptoms arebeing depicted. human understanding of network operation (with help from automated tools) must be brought to bear. for example, ptn switches are configuredwith secondary and tertiary routes (selected through the use of offline network analysis tools) that can be used when a primary link fails or becomes saturated. andinternet routers execute algorithms to determine automatically the shortest routes toeach destination. but there is also considerable manual configuration of constraintson routing, especially at the interfaces between isps.most noc operators are trained to deal with common problems. if the operatordoes not know how to deal with a problem, then an operations manual usually istrust in cyberspacecopyright national academy of sciences. all rights reserved.public telephone network and internet trustworthiness43claimed to be the best route to most of the internet. its upstream ispbelieved the claim and passed it along. routing in the internet was thendisrupted for several hours because of the traffic diverted to this small isp.exactly what constitutes an operational error may depend on systemcapacity. a system operating with limited spare capacity can be especially sensitive to operational missteps. for example, injecting inappropriate, but not technically incorrect, routing information led to a daylongoutage of netcom’s (a major isp) own internal network in june 1996 as thesheer volume of resulting work overloaded the isp’s relatively small routers. and this incident may foreshadow problems to come—many routersin the internet are operating near or at their memory or cpu capacity. itis unclear how well the essential infrastructure of the internet could copewith a sudden spike in growth rates.that operator errors are prevalent should not be a surprise. the ptnand the internet are both complex systems. large numbers of separateand controllable elements are involved in each, and the control param1many nocs also have one or more televisions, usually tuned to news channels such as cnn,to provide information about events such as natural disasters that may affect network traffic (e.g.,earthquakes). some events can cause disruption of service owing to equipment failures, or maycreate traffic surges because of breaking news (e.g., announcement of a tollfree number).available for consultation. the manual is important because of the complexity of thesystems and the difficulty of attracting, training, and retaining highly skilled operatorsto provide 24hour, 7day coverage in the noc. however, operations manualsusually cover only a predetermined set of problems; combinations of failures caneasily lead to symptoms and problems not covered by the manual. for problems notcovered, the usual procedure is to contact an expert, who may be on call for suchemergencies. in the internet environment, the expert might be able to access thenoc (e.g., via a dialup link) to assist in diagnosis and corrective action. (note,though, that having facilities for remote access introduces new vulnerabilities.)the set of controls available to noc operators is network specific. in the ptn,there are controls for rerouting calls through switches and multiplexors, for blockingcalls to a particular area code or exchange during natural disasters, and so on. in anisp, there are controls for changing router tables and multiplexors, among otherthings. in both the ptn and an isp, the noc will have provisions for calling outphysical maintenance teams when, for example, a cable breaks or a switching element fails. a telephone company often services its own equipment, but externalmaintenance must be ordered for the equipment of another provider; external maintenance in the internet is common because isps typically rely on equipment provided by many vendors, including longdistance and local telephone companies. consolidation in the internet business may blur these distinctions, as most longdistancetelephone companies are also major isps.trust in cyberspacecopyright national academy of sciences. all rights reserved.44trust in cyberspaceeters for these elements can affect network operation in subtle ways.operator errors can be reduced when a system does the following:•presents its operators with a conceptual model that allows thoseoperators to predict the effects of their actions and their inaction (wickenset al., 1997; parasuraman and mouloua, 1996);•allows its operators to examine all of the system’s abstractions, fromthe highest to the lowest level, whichever is relevant to the issue at hand.the entire system must be designed—from the outset—with controllability and understandability as a goal. the reduction of operationalerrors is more than a matter of building flashy windowbased interfaces.the graphics are the easy part. moreover, with an nis, there is the addedproblem of components with different management interfaces providedby multiple vendors. rarely can the nis developer change these components or their interfaces, which may make the support of a cleansystemwide conceptual model especially difficult.an obvious approach to reducing operational errors is simply toimplement automated support and remove the human from the loop.the routeconfiguration aids used by ptns are an example of such automation. more generally, better policybased routing mechanisms andprotocols will likely free human operators from lowlevel details associated with setting up network routes. in the internet, isps currently havejust one policy tool: their bgp configurations (rekhter and li, 1995;rekhter and gross, 1995; traina, 1993, 1995). but even though bgp is apowerful hammer, the sorts of routing policies that are usually desired donot much resemble nails. not surprisingly, getting bgp configurationsright has proven to be quite difficult. indeed, the internal network failurementioned above was directly attributable to an error in use of the bgppolicy control mechanisms.finally, operational errors are not only a matter of operators producing the right responses. maintenance practices—setting up user accountsand access privileges, for example—can neutralize existing security safeguards. and poor maintenance is an oftcited opening for launching asuccessful intrusion into a system. the network operations staff at themassachusetts institute of technology, for example, reports that about 6weeks after running vulnerabilityscan software (e.g., cops) on a publicunix workstation, the workstation will again become vulnerable to intrusion as a result of misconfiguration. managers of corporate or university networks often cite similar problems with firewall and router configuration which, if performed improperly, can lead to access controlviolations or denial of service.trust in cyberspacecopyright national academy of sciences. all rights reserved.public telephone network and internet trustworthiness45findings1.operational errors are a major source of outages for the ptn andinternet. some of these errors would be prevented through improvedoperator training and contingency planning; others require that systemsbe designed with operator understandability and controllability as aninitial design goal.2.improved routing management tools are needed for the internet,because they will free human operators from an activity that is error prone.3.research and development is needed to devise conceptual modelsthat will allow human operators to grasp the state of a network andunderstand the consequences of control that they may exert. also, research is needed into ways in which the state of a network can be displayed to a human operator.software and hardware failuresthe ptn and internet both experience outages from errors in designand implementation of the hardware and software they employ. a survey by the nric (1996) found that software and hardware failures eachaccounted for about onequarter of telephone switch outages. this finding is inconsistent with the commonly held belief that hardware is relatively bug free but software is notoriously buggy. a likely explanationcomes from carefully considering the definition of an outage. withintelephone switches, software failures are prone to affect individual telephone calls and, therefore, might not always be counted as causing outages.comparable data about actual outages of internet routers do not seemto be available. one can speculate that routers should be more reliablethan telephone switches, because router hardware is generally newer androuter software is much simpler. however, against that, one must askwhether routers are engineered and provisioned to the same high standards as telephone switches have been. moreover, most failures in packetrouting are comparatively transient; they are artifacts of the topologychanges that routing protocols make to accommodate a failure, ratherthan being direct consequences of the failure itself.one thing that is fairly clear is that the internet’s end points, including servers for such functions as the dns, are its least robust components.these end points are generally ordinary computers running commercialoperating systems and are heir to all of their attendant ills. (by contrast,telephony end points either tend to be very simple, as in the case of theordinary telephone, or are built to telephone industry standards.) twoexamples illustrate the fragility of the internet’s end points. first, manytrust in cyberspacecopyright national academy of sciences. all rights reserved.46trust in cyberspaceproblems have been reported with bind, the most common dns serverused on the internet (e.g., cert advisories ca 98.05, april 1998, and ca97.22, august 199716); some of these result in corrupted data or in dnsfailures. second, the socalled “ping of death” (cert advisory ca96.26,december 1996) was capable of crashing most of the common end pointson the internet. fortunately, cisco routers were not vulnerable; if theyhad been, the entire infrastructure would have been at risk.even without detailed outage data, it can be instructive to comparethe ptn and internet; their designs differ in rather fundamental ways,and these differences affect how software and hardware failures arehandled. the ptn is designed to have remarkably few switches, and itdepends on them. that constraint makes it necessary to keep all itsswitches running virtually all the time. consequently, switch hardwareitself is replicated, and the switch software is tasked with detecting hardware and software errors. upon detecting an error, the software recoversquickly without a serious outage of the switch itself. individual calls inprogress may be sacrificed, though, to restore the health of the switch.this approach does not work for all hardware and software failures.that was forcefully illustrated by the january 1990 failure of the at&tlongdistance network. that outage was caused by a combination ofhardware and software, and the interaction between them:17the incident began when a piece of trunk equipment failed and notifieda switch of the problem. per its design, the switch took itself offline for afew seconds while it tried to reinitialize the failing equipment; it alsonotified its neighbors not to route calls to it. when the switch came backonline, it started processing calls again; neighboring switches were programmed to interpret the receipt of new call setup messages as an indication that the switch had returned to service. unfortunately, a timingbug in a new version of that process caused those neighboring switchesto crash. this crash was detected and (correctly) resulted in a rapidrestart—but the failure/restart process triggered the same problem intheir neighbors.the“switches” for the internet—its routers—are also intended to bereliable, but they are not designed with the same level of redundancy orerror detection as ptn switches. rather, the internet as a whole recoversand compensates for router (switch) failures. if a router fails, then itsneighbors notice the lack of routing update messages and update their16cert advisories are available online at <http://www.cert.org>.17based on cooper (1989).trust in cyberspacecopyright national academy of sciences. all rights reserved.public telephone network and internet trustworthiness47own route tables accordingly. as neighbors notify other neighbors, thefailed router is dropped from possible packet routes. in the meantime,retransmissions by end points preserve ongoing conversations by causingpackets that might have been lost to reenter the network and traversethese new routes.findinginsufficient data exist about internet outages and how the internet’smechanisms are able to deal with them.malicious attacksattacks on the ptn and internet fall into two broad categories, according to the nature of the vulnerability being exploited. first, there areattacks related to authentication. this category includes everything fromeavesdroppers’ interception of plaintext passwords to designers’ misplaced trust in the network to provide authentication. in theory, theseattacks can be prevented by proper use of cryptography. the secondcategory of attacks is harder to prevent. this category comprises attacksthat exploit bugs in code. cryptography cannot help here (blaze, 1996),nor do other simple fixes appear likely. software correctness (see chapter3) is a problem that does not seem amenable to easy solutions. yet, aslong as software does not behave as intended, attackers will have opportunities to subvert systems by exploiting unintended system behavior.attacks on the telephone networkmost attacks on the ptn perpetrate toll fraud. the cellular telephonyindustry provides the easiest target, with caller information being broadcast over unencrypted radio channels and thus easily intercepted (cstb,1997). but attacks have been launched against wireline telephone serviceas well. toll fraud probably cannot be prevented altogether. fortunately,it does not have to be, because it is easily detected with automated trafficanalysis that flags for investigation of abnormal patterns of calls, creditcard authorizations, and other activities.the nric (1997) reports that security incidents have not been a majorproblem in the ptn until recently. however, the council does warn thatthe threat is growing, for reasons that include interconnections (oftenindirect) of osss to the internet, an increase in the number and skill levelof attackers, and the increasing number of ss7 interconnections to newtelephone companies. the report also notes that existing ss7 firewalls areneither adequate nor reliablein the face of the anticipated threat. astrust in cyberspacecopyright national academy of sciences. all rights reserved.48trust in cyberspacenoted earlier, this threat has increased dramatically because of the substantially lower threshold now associated with connection into the ss7system.routing attacks.to a wouldbe eavesdropper, the ability to control callrouting can be extremely useful. installing wiretaps at the end points ofa connection may be straightforward, but such taps are also the easiest todetect. interoffice trunks can yield considerably more information to aneavesdropper and with a smaller risk of detection. to succeed here, theeavesdropper first must determine which trunks the target’s calls willuse, something that is facilitated by viewing or altering the routing tablesused by the switches. second, the eavesdropper must extract the calls ofinterest from all the calls traversing the trunk; access to the signalingchannels can help here.how easy is it for an eavesdropper to alter routing tables? as it turnsout, apart from the usual sorts of automated algorithms, which calculateroutes based on topology, failed links, or switches, the ptn does havefacilities to exert manual control over routes. these facilities exist toallow improved utilization of ptn equipment. for example, there isgenerally a spike in business calls around 9:00 a.m. on weekdays whenworkers arrive in their offices. if telephone switches in, say, new yorkare configured to route other east coast calls through st. louis or pointsfurther west (where the workday has not yet started), then the 9:00 a.m.load spike can be attenuated. however, the existence of this interface forcontrolling call routing offers a point of entry for the eavesdropper, whocan profit from exploiting that control.database attacks. osss and the many databases they manage are employed to translate telephone numbers so that the number dialed by asubscriber is not necessarily the number that will be reached. if an attacker can compromise these databases, then various forms of abuse anddeception become possible.the simplest such attack exploits networkbased speed dialing, a feature that enables subscribers to enter a one ortwo digit abbreviation and have calls directed to a predefined destination. if the stored numbers are changed by an attacker, then speeddialedcalls could be routed to destinations of the attacker’s choice. beyondharassment, an attacker who can change speed dialing numbers can impersonate a destination or can redial to the intended destination whilestaying on the line and eavesdropping. other advanced telephone services controlled by osss and databases include call forwarding, tollfreenumbers, call distribution, conference calling, and message delivery. allcould be affected by oss and database vulnerabilities. in one successfulattack, the database entry for the telephone number of the probation oftrust in cyberspacecopyright national academy of sciences. all rights reserved.public telephone network and internet trustworthiness49fice in del ray beach, florida, was reconfigured. people who called theprobation office when the line was busy had their calls forwarded to atelephone sex line in new york (cooper, 1989).18because a subscriber’s chosen longdistance carrier is stored in a telephone network database, it too is vulnerable to change by attackers. herethe incentive is a financial one—namely, increased market share for acarrier. in a process that has come to be known as “slamming,” customers’ longdistance carriers are suddenly and unexpectedly changed. thisproblem has been pervasive enough so that numerous procedural safeguards have been mandated by the fcc and various state regulatorybodies.looking to the future, more competition in the local telephone market will lead to the creation of a database that enables the routing ofincoming calls to specific local telephone carriers. and, given the likelyuse of shared facilities in many markets, outgoing local calls will need tobe checked to see what carrier is actually handling the call. in addition,growing demand for “local number portability,” whereby a customer canretain a telephone number even when switching carriers, implies the existence of one more database (which would be run by a neutral party andconsulted by all carriers for routing of local calls). clearly, a successfulattack on any of these databases could disrupt telephone service across awide area.in contrast to the internet, the telephone system does not depend onhaving an automated process corresponding to the internet’s dns translation from names to addresses.19 one does not call directory assistancebefore making every telephone call, and success in making a call does notdepend critically on this service. thus, in the ptn, an internet’s vulnerability is avoided but at the price of requiring subscribers to dial telephonenumbers rather than dialing subscriber names. furthermore, unlike dns,the telephone network’s directory service is subject to a sanity test by itsclients. if a human caller asks directory assistance for a neighbor’s numberand is given an area code for a town halfway across the country, the callerwould probably doubt the accuracy of the number and conclude that thedirectory assistance service was malfunctioning. still, tampering with directory assistance can cause telephone calls to be misdirected.18there is even a historical precedent for such attacks. the original telephone switch wasinvented by an undertaker; his competitor’s wife was a telephone operator who connectedanyone who asked for a funeral home to her own husband’s business.19this is not strictly true; calls to certain classes of telephone numbers (e.g., 800, 888, and900) do result in a directory lookup to translate the called number into a “real” destinationtelephone number. in these instances, the analogy between the ptn and the internet isquite close.trust in cyberspacecopyright national academy of sciences. all rights reserved.50trust in cyberspacefacilities. the nature of the telephone company physical plant leads toanother class of vulnerabilities. many central offices normally are unstaffed and, consequently, they are vulnerable to physical penetration,which may go entirely undetected. apart from the obvious problems ofintruders tampering with equipment, the documentation present in suchfacilities (including, of course, passwords written on scraps of yellowpaper and stuck to terminals) is attractive to “phone phreaks.”20 a similar vulnerability is present in less populated rural areas, which are servedby socalled remote modules. these remote modules perform localswitching but depend on a central office for some aspects of control.remote modules are invariably deployed in unstaffed facilities, hencesubject to physical penetration.findings1.attacks on the telephone network have, for the most part, beendirected at perpetrating billing fraud. the frequency of attacks is increasing, and the potential for more disruptive attacks, with harassment andeavesdropping as goals, is growing.2.better protection is needed for the many number translation andother databases used in the ptn.3.ss7 was designed for a closed community of telephone companies.deregulation has changed the operational environment and created opportunities for insider attacks against this system, which is fundamentalto the operation of the ptn.4.telephone companies need to enhance the firewalls between osssand the internet and safeguard the physical security of their facilities.attacks on the internetthe general accessibility of the internet makes it a highly visible target and within easy reach of attackers. the widespread availability ofdocumentation and actual implementations for internet protocols meansthat devising attacks for this system can be viewed as an intellectualpuzzle (where launching the attacks validates the puzzle’s solution). internet vulnerabilities are documented extensively on cert’s web site,21and at least one ph.d. thesis (howard, 1997) is devoted to the subject.20a phone phreak is a telephone network hacker.21the computer emergency response team (cert)/coordination center is an elementof the networked systems survivability program in the software engineering institute atcarnegie mellon university. see <http://www.cert.org>.trust in cyberspacecopyright national academy of sciences. all rights reserved.public telephone network and internet trustworthiness51this subsection concentrates on vulnerabilities in the internet’s infrastructure, since this is what is most relevant to nis designers. vulnerabilities in end systems are amply documented elsewhere. see, for example, garfinkel and spafford (1996).name server attacks.the internet critically depends on the operation ofthe dns. outages or corruption of dns root servers and other topleveldns servers—whether owing to failure or successful attacks—can leadto denial of service. specifically, if a toplevel server cannot furnish accurate information about delegations of zones to other servers, then clientsmaking dns lookup requests are prevented from making progress. theclient requests might go unanswered, or the server could reply in a waythat causes the client to address requests to dns server machines thatcannot or do not provide the information being sought. cache contamination is a second way to corrupt the dns. an attacker who introducesfalse information into the dns cache can intercept all traffic to a specifictargeted machine (bellovin, 1989). one highly visible example of thisoccurred in july 1997, when somebody used this technique to divert requests for a major web server to his own machines (wall street journal,1997).in principle, attacks on dns servers are easily dealt with by extending the dns protocols. one such set of extensions, secure dns, is basedon publickey cryptography (eastlake and kaufman, 1997) and can bedeployed selectively in individual zones.22 perhaps because this solutionrequires the installation of new software on client machines, it has notbeen widely deployed. no longer merely a question of support softwarecomplexity, the internet has grown sufficiently large so that even simplesolutions, such as secure dns, are precluded by other operational criteria. a scheme that involved changing only the relatively small number ofdns servers would be quite attractive. but lacking that, techniques mustbe developed to institute changes in largescale and heterogeneous networks.routing system attacks. routing in the internet is highly decentralized.this avoids the vulnerabilities associated with dependence on a smallnumber of servers that can fail or be compromised. but it leads to othervulnerabilities. with all sites playing some role in routing, there aremany more sites whose failure or compromise must be tolerated. the22however, configuration management does become much harder when there is partialdeployment of secure dns.trust in cyberspacecopyright national academy of sciences. all rights reserved.52trust in cyberspacedamage inflicted by any single site must somehow be contained, eventhough each site necessarily serves as the authoritative source for someaspect of routing. decentralization is not a panacea for avoiding thevulnerabilities intrinsic in centralized services. moreover, the trustworthiness of most niss will, like the internet, be critically dependent bothon services that are more sensibly implemented in a centralized fashion(e.g., dns) and on services more sensibly implemented in a decentralized way (e.g., routing). understanding how either type of services canbe made trustworthy is thus instructive.the basis for routing in the internet is each router periodically informing neighbors about what networks it knows how to reach. thisinformation is direct when a router advertises the addresses of the networks to which it is directly connected. more often, though, the information is indirect, with the router relaying to neighbors what it has learnedfrom others. unfortunately, recipients of information from a router rarelycan verify its accuracy23 because, by design, a router’s knowledge aboutnetwork topology is minimal. virtually any router can represent itself asa best path to any destination as a way of intercepting, blocking, or modifying traffic to that destination (bellovin, 1989).most vulnerable are the interconnection points between major isps,where there are no grounds at all for rejecting route advertisements. evenan isp that serves a customer’s networks cannot reject an advertisementfor a route to those networks via one of its competitors—many larger sitesare connected to more than one isp.24 such multihoming becomes amixed blessing, with the need to check accuracy, which causes trafficaddressed from a subscriber net arriving via a different path to be suspectand rejected, being pitted against the increased availability that multihoming promises. some isps are now installing bgp policy entries thatdefine which parts of the internet’s address space neighbors can provideinformation about (with secondary route choices). however, this approach undermines the internet’s adaptive routing and affects overallsurvivability.somehow, the routing system must be secured against false advertisements. one approach is to authenticate messages a hop at a time. anumber of such schemes have been proposed (badger and murphy, 1996;hauser et al., 1997; sirois and kent, 1997; smith et al., 1997), and a majorrouter vendor (cisco) has selected and deployed one in products. unfor23in a few cases it actually is possible to reject inaccurate information. for example, anisp will know what network addresses belong to its clients, and neighbors of such a routergenerally will believe that and start routing traffic to the isp.24the percentage of such multihomed sites in the internet is currently low but appears tobe rising, largely as a reliability measure by sites that cannot afford to be offline.trust in cyberspacecopyright national academy of sciences. all rights reserved.public telephone network and internet trustworthiness53tunately, the hopatatime approach is limited to ensuring that an authorized peer has sent a given message; nothing ensures that the message isaccurate. the peer might have received an inaccurate message (from anauthorized peer) or might itself be compromised. thus, some attacks areprevented but others remain viable.the alternative approach for securing the routing system against falseadvertisements is, somehow, for routers to employ global informationabout the internet’s topology. advertisements that are inconsistent withthat information are thus rejected. schemes have been proposed (e.g.,perlman, 1988), but these do not appear to be practical for the internet.perlman’s scheme, for example, requires sourcecontrolled routing overthe entire path. routing protocol security is an active research area, andappropriately so.routing in the internet is actually performed at two levels. inside anautonomous system (as)—a routing domain under the control of oneorganization—an interior routing protocol is executed by routers. attacking these routers can affect large numbers of users, but wiretappingof these systems appears to be rare and therefore of limited concern.25 ofpotentially greater concern are attacks on bgp, the protocol used to distribute routing information among the autonomous isps around theworld. because bgp provides the basis for all internet connectivity, asuccessful attack can have wideranging effects. as above, it is easy tosecure bgp against false advertisements on a hopatatime basis anddifficult to employ global information about topology. moreover, even iffalse advertisements could be discarded, successful attacks against bgprouters or against the workstations used to download configuration information into the bgp routers could still have devastating effects on internet connectivity.to secure bgp against a full range of attacks, a combination of security features involving both the routers and a supporting infrastructure25attacks against an interior routing protocol or against an organization’s routers candeny or disrupt service to all of the hosts within that as. if the as is operated by an isp,then the affected population can be substantial in size. countermeasures to protect linkstate intradomain routing protocols have been developed (murphy and hofacker, 1996) buthave not been deployed, primarily because of concerns about the computational overheadassociated with the signing and verification of routing traffic (specifically, link state advertisements). countermeasures for use with distance vector algorithms (e.g., dvrp) are evenless well developed, although several proposals for such countermeasures have been published recently. because all of the routers within an as are under the control of the sameadministrative entity, and because there is little evidence of active wiretapping of intraaslinks, there may be a perception that the proposed cryptographic countermeasures are tooexpensive relative to the protection afforded.trust in cyberspacecopyright national academy of sciences. all rights reserved.54trust in cyberspaceneeds to be developed and deployed. each bgp router must be able toverify whether a routing update it receives is authentic and not a replay,or a previous, authentic update, where an authentic routing update is onethat no attacker can modify (undetectably) and one for which the sourceof the update can be verified to be the “owner” of the portion of the ipaddress space being advertised.26 thus, implementing bgp security involves creating an infrastructure that codifies the assignment to organizations (e.g., isps, dsps, subscribers) of as numbers and portions of ipaddress space. because of the bgp routing system’s size (approximately50,000 routes and 4,000 isps), deployment of these countermeasures is nota certainty. moreover, after deployment some residual bgp vulnerabilities will still remain. for example, a router that is authorized to advertisea route to a network may suppress propagation of route withdrawal messages it receives, thus continuing to advertise the route for some time. butthis can cause traffic to the network in question to be discarded.it is worth noting that the routing system of the internet closely mirrors call routing in the ptn, except that, in the ptn, a separate management and control network carries control functions. any site on the internet can participate in the global routing process, whereas subscribers inthe ptn do not have direct access to the management and control network. the added vulnerabilities of the internet derive from this lack ofisolation. as network interconnections increase within the ptn, it maybecome vulnerable to the same sorts of attacks as the internet is now.protocol design and implementation flaws.the design and implementation of many internet protocols make them vulnerable to a variety ofdenialofservice attacks (schuba et al., 1997). some attacks exploit buggycode. these are perhaps the easiest to deal with; affected sites need onlyinstall newer or patched versions of the affected software. other attacksexploit artifacts of particular implementations, such as limited storageareas, expensive algorithms, and the like. again, updated code often cancure such problems.the more serious class of attacks exploits features of certain protocols.for example, one type of attack exploits both the lack of source addressverification and the connectionless nature of udp to bounce packets between query servers on two target hosts (cert advisory ca96.01). thisprocess can continue almost indefinitely, until a packet happens to bedropped. and, while the process continues, computation and networkbandwidth are consumed. the obvious remedy would be for hosts todetect this attack or any such denialofservice attack, much the same way26because of the route and address aggregation features of bgp, the route verificationrequirements are even more complex than described here.trust in cyberspacecopyright national academy of sciences. all rights reserved.public telephone network and internet trustworthiness55virusscreening software detects and removes viruses. but, if it is cheaperfor an attacker to send a packet than it is for a target to check it, then denialof service is inevitable from the sheer volume of packets. even cryptography is not a cure: authenticating a putatively valid packet is much harder(it requires substantial cpu resources) than generating a stream of byteswith a random authentication check value to send the victim.27findings1.new countermeasures for name server attacks are needed thatwork well in largescale, heterogeneous environments.2.cryptography, while not in itself sufficient, is essential to the protection of both the internet and its end points. wider deployment ofcryptography is needed. algorithms for authentication only are largelyfree from export and usage restrictions, yet they can go a long way toward helping.3.cryptographic mechanisms to secure the dns do exist; however,deployment to date has been limited.4.no effective means exist to secure routing protocols, especially onbackbone routers. research in this area is urgently needed.5.attacks that result in denial of service are increasingly common.wider use of updated software and patches, new product development,and better software engineering are needed to deal with this problem.emerging issuesinternet telephonywhat are the security implications if, as predicted by many pundits,today’s traditional telephone network is replaced by an internetbasedtransport mechanism? will telephony become even less secure, owing toall the security problems with the internet discussed earlier in this chapter? or will some portion of the internet used only for telephony beresistant to many of the problems described in the preceding sections?recall that many current ptn vulnerabilities are related either to theservices being provided or to the physical transport layer. rehosting theptn on the internet will have no effect on these vulnerabilities. thus, theosss and database lookups related to advanced ptn services, with their27encryption is even worse in this regard, as the cost of decryption is often greater thanthe cost of authentication and because a receiver might have to both decrypt and authenticate a packet to determine if it is valid. the encapsulating security payload (esp) protocolof ipsec counters this denialofservice vulnerability by reversing the order in which theseoperations are applied (i.e., a receiver authenticates ciphertext prior to decrypting it).trust in cyberspacecopyright national academy of sciences. all rights reserved.56trust in cyberspaceassociated vulnerabilities, would be unaffected by the move to an internetbased telephone system. similarly, if access to the internetbasedtelephone system is accomplished by means of twisted pairs (albeittwisted pairs carrying something like integrated services digital network(isdn) or asymmetric digital subscriber line (adsl)), then interconnections of some sort will still be needed. these would likely be routers orswitches, but such interconnections are at least as programmable and atleast as vulnerable.call routing in an internetbased telephone system would be different, but likely no more secure. at the very least, ip routing would beinvolved. most probably, a new database would be introduced to maptelephone numbers to domain names or ip addresses. both, of course,raise serious security and reliability concerns.in at least two respects, both noted earlier in this chapter, an internetbased telephone system could be significantly more vulnerable to attackthan today’s ptn. the primary active elements of an internetbased network—the routers—are, by design, accessible from the network they control, and the network’s routing protocols execute inband with the communications they control. by contrast, virtually the entire ptn isnow managed by outofband channels. considerable care will be neededto deliver the security of outofband control by using inband communications. the other obvious weakness of the internet is its end points,personal computers and servers, because attacks on them can be used toattack the telephone system.findingthe ptn is likely to become more vulnerable with the rise of internettelephony, most notably because internetbased networks use inbandchannels for routing and have end points that are prone to failure. attention to these issues is needed.is the internet ready for “prime time”?whether the internet is “ready for business” depends on the requirements of the business. there are already numerous examples of businesses using the internet for advertising, marketing, sales of products andservices, coordination with business partners, and various other activities. on the other hand, the internet is also viewed—and rightly so—asbeing less reliable and less secure than the ptn. specifically, the internetis perceived as more susceptible to interception (i.e., eavesdropping) andhas proved to be more susceptible to active attacks (e.g., server flooding,trust in cyberspacecopyright national academy of sciences. all rights reserved.public telephone network and internet trustworthiness57web site modification). consequently, most internetsavvy business users restrict what they entrust to the internet.the internet is also more prone to outages than the ptn. thus, itwould be unwise for utility companies and other critical infrastructureproviders to abandon the ptn and rely on remote access through theinternet for controlling power distribution substations, because individualisps are less likely than individual telephone companies to survive localpower interruptions.28few established businesses seem willing to forgo their telephone order centers for internetonly access, although a small and growing number of newer businesses, such as virtual vineyards and amazon.com, domaintain an internetonly presence. abandoning the ptn for the internetseems unwise for businesses such as brokerage houses or mailorder catalog companies, where continued availability of service is critical. forexample, during the stock market frenzy on october 2728, 1997, customers of internetbased brokerage systems experienced unusual delays inexecuting trades. but the magnitude of their delays was relatively smalland was commensurate with the delays suffered by telephonebased access and even some of the stock market’s backend systems. still, it issobering to contemplate the effect of an internetrelated failure that coincided with a spike in market activity.mailorder firms, brokerage houses, and others do make extensiveuse of the internet as an avenue of customer access. but it is not the onlyavenue of access, and neither the customers nor the business have becomewholly dependent on it. if, for example, these and similar businessesreduced their other avenues of access (e.g., to save money), then an internet outage could have a significant impact. consider a scenario in whichbanks acquire the capability to download customer money onto smartcards through the internet. over time, banks might reduce the number ofautomatic teller machines available (just as the numbers of physical bankbranches and tellers have fallen as automated teller machines have proliferated). a prolonged failure of this internet cash distribution mechanismcould overload the few remaining available machines and tellers.in theory, the risks associated with using the internet can be evaluated and factored into a risk management model (see chapter 6). mostbusinesses, however, are not fully cognizant of these risks nor of thereturn on investments in protection. as a result, the level of protection28internet service providers have differing plans for dealing with power system failures,which may make it impossible to access computers and data following such a failure. thefailure need not even be widespread. by contrast, telephone networks are under centralcontrol, can easily implement backup power systems, and require very little electrical current for an ordinary telephone line.trust in cyberspacecopyright national academy of sciences. all rights reserved.58trust in cyberspaceadopted by many business users of the internet does not seem commensurate with that afforded their physical assets. for example, it seems asthough the quality of burglar alarms and physical access control systemsdeployed by most businesses is considerably higher than the level ofinternet security countermeasures they deploy (see chapter 4).moreover, businesses that make extensive use of internet technologymay do so in a fashion that externalizes the risks associated with such use.if infrastructure suppliers, such as telephone companies and electric andgas utilities, do not take adequate precautions to ensure the availability oftheir systems in the face of malicious attacks over the internet, then thepublic will bear the brunt of the failure. because many of these businessesoperate in what is effectively a monopoly environment, the freemarketforces that should eventually correct such cost externalization may not beeffective.of particular concern is that most of the security countermeasuresadopted by businesses connecting to the internet are designed only tothwart the most common attacks used by hackers. most hackers, however, are opportunistic and display only a limited repertoire of skills.protection against that hacker threat is insufficient for warding off morecapable, determined threats, such as criminals or terrorists.and while in one sense the internet poses no new challenges—a system that can be accessed from outside only through a cryptographicallyprotected channel on the internet is at least as secure as the same systemreached through a conventional leased line—new dangers arise preciselybecause of pervasive interconnectivity. the capability to interconnectnetworks gives the internet much of its power; by the same token, itopens up serious new risks. an attacker who may be deflected by cryptographic protection of the front door can often attack a less protected administrative system and use its connectivity through internal networks tobypass the encryption unit protecting the real target. this often makes amockery of firewallbased protection.findings1.the internet is ready for some business use, but it is not at a pointwhere it would be prudent for businesses to abandon the ptn in favor ofthe internet. for managing critical infrastructures, the internet is toosusceptible to attacks and outages to be a viable basis for control.2.risk management, especially to guard against highly skilled attackers, deserves further attention in the business community.trust in cyberspacecopyright national academy of sciences. all rights reserved.public telephone network and internet trustworthiness59referencesbadger, m.r., and s.l. murphy. 1996. “digital signature protection of the ospf routingprotocol,” pp. 93102 in proceedings of the symposium on network and distributed systemsecurity. los alamitos, ca: ieee computer society press.bellovin, steven m. 1989. “security problems in the tcp/ip protocol suite,”computercommunications review, 19(2):3248.bellovin, steven m. 1995. “using the domain name system for system breakins,” pp.199208 in proceedings of the 5th usenix unix security symposium, salt lake city, utah.berkeley, ca: usenix association.bernerslee, t., and d. connolly. 1995. hypertext markup language—2.0. rfc 1866. november.bernerslee, t., l. masinter, and m. mccahill. 1994.uniform resource locators (url). rfc1738. december.bernerslee, t., r. fielding, and h. frystyk. 1996. hypertext transfer protocol—http 1.0.rfc 1945. may.blaze, matt. 1996.afterword. 2nd ed. applied cryptography, bruce schneier, ed. new york:john wiley & sons.bolt, beranek, and newman (bbn). 1978. “appendix h: interfacing a host to a privateline interface,”specification for the interconnection of a host and an imp. bbn report1822. cambridge, ma: bbn, may.braden, r., l. zhang, s. berson, s. herzog, and s. jamin. 1997. resource reservation protocol (rsvp)—version 1 functional specification. rfc 2205. september.case, j.d., m. fedor, m.l. schoffstall, and c. davin. 1990. simple network managementprotocol (snmp). rfc 1157. may.computer science and telecommunications board (cstb), national research council.1997.the evolution of untethered communications. washington, dc: national academy press.cooper, brinton. 1989. “phone hacking,”risks digest, vol. 8, issue 79, june 14. availableonline at <http://catless.ncl.ac.uk/risks/8.79.html#subj4>.dusse, s., p. hoffman, and b. ramsdell. 1998a. s/mime version 2 certificate handling. rfc2312. march.dusse, s., p. hoffman, b. ramsdell, l. lundblade, and l. repka. 1998b. s/mime version 2message specification. rfc 2311. march.eastlake, d., and c. kaufman. 1997. domain name system security extensions. rfc 2065.january.fielding, r., j. gettys, j. mogul, h. frystyk, and t. bernerslee. 1997. hypertext transferprotocol—http 1.1. rfc 2068. january.floyd, s., and k. fall. 1998.“promoting the use of endtoend congestion control in theinternet,”ieee/acm transactions on networking. available online at <ftp://ftp.ee.lbl.gov/papers/collapse.feb98.ps>.floyd, s., and v. jacobson. 1993. “random early detection gateways for congestionavoidance,”ieee/acm transactions on networking, 1(4):397413.garfinkel, s., and e. spafford. 1996. practical unix and internet security. newton, ma:o’reilly and associates.hauser, r., t. przygienda, and g. tsudik. 1997. “reducing the cost of security in linkstate routing,” pp. 93101 in proceedings of the symposium on network and distributedsystem security.los alamitos, ca: ieee computer society press.howard, john d. 1997. “an analysis of security incidents on the internet 19891995,”ph.d. thesis, department of engineering and public policy, carnegie mellon university, pittsburgh, pa.trust in cyberspacecopyright national academy of sciences. all rights reserved.60trust in cyberspaceioannidis, john, and matt blaze. 1993. “the architecture and implementation of networklayer security in unix,” pp. 2939 in security iv, santa clara, california. berkeley, ca:usenix association.jacobson, v. 1988. “congestion avoidance control,” pp. 314329 in sigcomm 88, stanfordcalifornia. los alamitos, ca: ieee computer society.kahn, david. 1976. the code breakers. 8th ed. new york: macmillan.kristol, d., and l. montulli. 1997. http state management mechanism. rfc 2109. february.mcquillan, j.m., and d.c. walden. 1977. “the arpa network design decisions,”computer networks, august, pp. 243289.mills, d.l. 1984. exterior gateway protocol formal specification. rfc 904. april.mills, mike. 1998. “at&t high speed network fails: red cross, banks scramble toadjust,”washington post, april 14, p. c1.mockapetris, p.v. 1987a. domain names—concepts and facilities. rfc 1034. november.mockapetris, p.v. 1987b. domain names—implementation and specification. rfc 1035. november.morris, robert t. 1985. a weakness in the 4.2 bsd unix tcp/ip software. murray hill, nj:at&t bell laboratories, february.murphy, jamie, and charlie hofacker. 1996.“explosive growth clogs the internet’s backbone,”new york times, july 3.network reliability and interoperability council (nric). 1996. network reliability: thepath forward. washington, dc: federal communications commission. availableonline at <www.fcc.gov/oet/info/orgs/nrc/>.network reliability and interoperability council (nric). 1997. final report of the networkreliability and interoperability council. washington, dc: federal communications commission, july 15.parasuraman, raja, and mustapha mouloua, eds. 1996. automation and human performance:theory and applications. mahwah, nj: lawrence erlbaum associates.perillo, robert j. 1997.“at&t database glitch caused ‘800’ phone outage,”telecom digest, vol. 17, issue 253, september 18. available online at <http://massis.lcs.mit.edu/telecomarchives/archives/back issues/1997.volume.17/vol17.iss251300>.perlman, radia. 1988. “network layer protocols with byzantine robustness,” ph.d. thesis, computer science department, massachusetts institute of technology, cambridge,ma.postel, j. 1982. simple mail transfer protocol. rfc 821. august.rekhter, y., and p. gross. 1995. application of the border gateway protocol in the internet.rfc 1772. march.rekhter, y., and t. li. 1995. a border gateway protocol 4 (bgp4). rfc 1771. march.schuba, christoph l., ivan krsul, markus g. kuhn, eugene h. spafford, aurobindosundaram, and diego zamboni. 1997. “analysis of a denial of service attack ontcp,” pp. 208233 in proceedings of 1997 ieee symposium on security and privacy. losalamitos, ca: ieee computer society press.shenker, s., and j. wroclawski. 1997. general characterization parameters for integrated service network elements. rfc 2215. september.sirois, k.e., and stephen t. kent. 1997. “securing the nimrod routing architecture,” pp.7484 in proceedings of the annual internet society (isoc) symposium on network anddistributed system security. los alamitos, ca: ieee computer society press.smith, b.r., s. murthy, and j.j. garcialunaaceves. 1997. “securing distancevector routing protocols,” pp. 8592 in proceedings of the annual internet society (isoc) symposiumon network and distributed system security. los alamitos, ca: ieee computer societypress.trust in cyberspacecopyright national academy of sciences. all rights reserved.public telephone network and internet trustworthiness61stevens, w. 1997. tcp slow start, congestion avoidance, fast retransmit, and fast recoveryalgorithms. rfc 2001. january.thompson, kevin, george j. miller, and rick wilder. 1997. “widearea internet trafficpatterns and characteristics,”ieee network, 11(6):1023.traina, p. 1993. experience with the bgp4 protocol. rfc 1773. march.traina, p. 1995. bgp4 protocol analysis. rfc 1774. march.wall street journal.1997.“an internet stunt causes trouble for kashpureff,”november 4.weissman, clark. 1992. “blacker: security for the ddn: examples of a1 security engineering trades,” pp. 286292 in proceedings of the 1992 ieee symposium on security andprivacy. los alamitos, ca: ieee computer society press.wickens, christopher d., anne s. mavor, and james p. mcgee, eds. 1997. flight to thefuture: human factors in air traffic control. washington, dc: national academypress.zimmerman, philip r. 1995. the official pgp user’s guide. cambridge, ma: mit press.trust in cyberspacecopyright national academy of sciences. all rights reserved.62trust in cyberspace62introductionbackgroundcomputing power is becoming simultaneously cheaper and more dispersed. generalpurpose computers and access to global informationsources are increasingly commonplace on home and office desktops. perhaps most striking is the exploding popularity of the world wide web. aweb browser can interact with any web site, and web sites offer a widevariety of information and services. a less visible consequence of cheap,dispersed computing is the ease with which specialpurpose networkedinformation systems (niss) can now be built.an nis built to support the activities of a health care provider, suchas a mediumsized health maintenance organization (hmo) serving awide geographic area, is used as an illustration here and throughout thischapter. hmo services might include maintenance of patient records,support for administration of hospitals and clinics, and support for equipment in laboratories. the nis would, therefore, comprise computer systems in hospital departments (such as radiology, pathology, and pharmacy), in neighborhood clinics, and in centralized data centers. byintegrating these individual computer systems into an nis, the hmomanagement would expect both to reduce costs and to increase the quality of patient care. for instance, although data and records—such aslaboratory test results, xray or other images, and treatment logs—previ3software for networkedinformation systemstrust in cyberspacecopyright national academy of sciences. all rights reserved.software for networked information systems63ously might have traveled independently, the information now can betransmitted and accessed together.in building an nis for an hmo, management is likely to have chosena“webcentric” implementation using the popular protocols and facilities of the world wide web and the internet. such a decision would besensible for the following reasons:•the basic elements of the system, such as web servers and browsers, can now be commercial offtheshelf (cots) components and, therefore, are available at low cost.•a large, growing pool of technical personnel is familiar with thewebcentric approach, so the project will not become dependent on asmall number of individuals with detailed knowledge of locally writtensoftware.•the technology holds promise for extensions into consumer telemedicine, whereby patients and health care providers interact by usingthe same techniques as are commonly used on the rest of the internet.clearly, the hmo’s nis must exhibit trustworthiness: it must engender feelings of confidence and trust in those whose lives it affects. physicians must be confident that the system will display the medical record ofthe patient they are seeing when it is needed and will not lose information; patients must be confident that physicianentered prescriptions willbe properly transmitted and executed; and all must be confident that theprivacy of records will not be compromised. achieving this trustworthiness, however, is not easy.nis trustworthiness mechanisms basically concern events that arenot supposed to happen. nonmalicious users living in a benign andfaultfree world would be largely unaffected were such mechanisms removed from a system. but some users may be malicious, and the world isnot fault free. consequently, reliability, availability, security and all otherfacets of trustworthiness require mechanisms to foster the necessary truston the part of users and other affected parties. only with their failure orabsence do trustworthiness mechanisms assume importance to a system’susers. users seem unable to evaluate the costs of not having trustworthiness mechanisms except when they experience actual damage from incidents (see chapter 6 for an extended discussion). so, while market forcescan help foster the deployment of trustworthiness mechanisms, theseforces are unlikely to do so in advance of directly experienced or highlypublicized violations of trustworthiness properties.although the construction of trustworthy niss is today in its infancy,lessons can be learned from experience in building fullauthority andother freestanding, highconsequence computing systems for applicationstrust in cyberspacecopyright national academy of sciences. all rights reserved.64trust in cyberspacesuch as industrial process control and medical instrumentation. in suchsystems, one or more computers directly control processes or deviceswhose malfunction could lead to significant loss of property or life. evensystems in which human intervention is required for initiating potentiallydangerous events can become highconsequence systems when humanusers or operators place too much trust in the information being displayed by the computing system.1 to be sure, there are differences between niss and traditional highconsequence computing systems. anintent of this chapter is to identify those differences and to point outlessons from highconsequence systems that can be applied to niss, aswell as unique attributes of niss that will require new research.the role of softwaresoftware plays a major role in achieving the trustworthiness of annis, because it is software that integrates and customizes generalpurpose components for some task at hand. in fact, the role of software in annis is typically so pervasive that the responsibilities of a software engineer differ little from those of a systems engineer. nis software developers must therefore possess a systems viewpoint,2 and systems engineersmust be intimately familiar with the strengths (and, more importantly,the limitations) of software technology.with software playing such a pervasive role, defects can have farreaching consequences. it is notoriously difficult to write defectfree software, as the list of incidents in, for example, leveson (1987) or neumann(1995) confirms. beyond the intrinsic difficulty of writing defectfree software, there are constraints that result from the nature of niss. theseconstraints derive from schedule and budget; they mean that a softwaredeveloper has only limited freedom in selecting the elements of the software system and in choosing a development process:•an nis is likely to employ commercial operating systems, purchased“middleware,” and other applications, as well as specialpurposecode developed specifically for the nis. the total source code size for thesystem could range from tens to hundreds of millions of lines. in thissetting, it is infeasible to start from scratch in order to support trustworthiness.1this is a particularly dangerous state of affairs, since designers may assume that systemoperation is being monitored, when in fact it is not (leveson, 1995).2once succinctly stated as, “you are not in this alone.” that is, that you need to considernot only the narrow functioning of your component but also how it interacts with othercomponents, users, and the physical world in achieving systemlevel goals. another aspectof the “systems viewpoint” is a healthy respect for the potential of unexpected side effects.trust in cyberspacecopyright national academy of sciences. all rights reserved.software for networked information systems65•future niss will, of necessity, evolve from the current ones. thereis no alternative, given the size of the systems, their complexity, and theneed to include existing services in new systems. techniques for supporting trustworthiness must take this diversity of origin into account. itcannot be assumed that niss will be conceived and developed withoutany reuse of existing artifacts. moreover, components reused in nissinclude legacy components that were not designed with such reuse inmind; they tend to be large systems or subsystems having nonstandardand often inconvenient interfaces. in the hmo example, clinical laboratories and pharmacies are likely to have freestanding computerized information systems that exemplify such legacy systems.•commercial offtheshelf software components must be used tocontrol development cost, development time, and project risk. a commercial operating system with a variety of features can be purchased for afew hundred dollars, so development of specialized operating systems isuneconomical in almost all circumstances. but the implication is thatachieving and assessing the trustworthiness of a networked informationsystem necessarily occur in an environment including cots softwarecomponents (operating systems, database systems, networks, compilers,and other system tools) with only limited access to internals or controlover their design.•finally, the design of nis software is likely to be dictated—at least,in part—by outside influences such as regulations, standards, organizational structure, and organizational culture. these outside influences canlead to system architectures that aggravate the problems of providingtrustworthiness. for example, in a medical information system, goodsecurity practices require that publicly accessible terminals be logged offfrom the system after relatively short periods of inactivity so that anunauthorized individual who happens upon an unattended terminal cannot use it. but in emergency rooms, expecting a practitioner to log inperiodically is inconsistent with the urgency of emergency care thatshould be supported by an nis in this setting.fortunately, success in building an nis does not depend on writingsoftware that is completely free of defects. systems can be designed sothat only certain core functionality must be defect free; defects in otherparts of the system, although perhaps annoying, become tolerable because their impact is limited by the defectfree core functionality. it nowis feasible to contemplate a system having millions of lines of source codeand embracing cots and legacy components, since only a fraction of thecode has to be defect free. of course, that approach to design does depend on being able to determine or control how the effects of defectspropagate. various approaches to software design can be seen as providtrust in cyberspacecopyright national academy of sciences. all rights reserved.66trust in cyberspaceing artillery for attacking the problem, but none has proved a panacea.there is still no substitute for talented and experienced designers.development of a networked information systemthe development of an nis proceeds in phases that are similar to thephases of development for other computerized information systems:•decide on the structure or architecture of the system.•build and acquire components.•integrate the components into a working and trustworthy whole.the level of detail at which the development team works forms avshaped curve. effort starts at the higher, systems level, then dips downinto details as individual software components are implemented andtested, and finally returns to the system level as the system is integratedinto a cohesive whole.of the three phases, the last is the most problematic. developmentteams often find themselves in the integration phase with componentsthat work separately but not together. theoretically, an nis can grow byaccretion, with service nodes and client nodes being added at will. theproblem is that (as illustrated by the internet) it is difficult to ensure thatthe system as a whole will exhibit desired global properties and, in particular, trustworthiness properties. on the one hand, achieving a level ofconnectivity and other basic services is relatively easy. these are theservices that generalpurpose components, such as routers, servers, andbrowsers, are designed to provide. and even though loads on networksand demands on servers are hard to predict, adverse outcomes are readilyovercome by the addition or upgrade of generalpurpose components.on the other hand, the consequences of failures or security breachespropagating through the system are hard to predict, to prevent, and toanalyze when they do occur. thus, basic services are relatively simple toprovide, whereas global and specialized services and properties—especially those supporting trustworthiness—are difficult to provide.system planning, requirements,and toplevel designplanning and program managementa common first step in any development project is to produce a planning and a requirements document. the planning document containsinformation about budget and schedules. cost estimation and schedulingtrust in cyberspacecopyright national academy of sciences. all rights reserved.software for networked information systems67are hard to do accurately, so producing a planning document is not astraightforward exercise. just how much time a large project will require,how many staff members it will need (and when), and how much it willcost cannot today be estimated with precision. the techniques that exist,such as the constructive cost model (cocomo) (boehm, 1981), are onlyas good as the data given them and the suitability of their models for agiven project. estimation is further complicated if novel designs and theimplementation of novel features are attempted, practices common insoftware development and especially common in leadingedge applications such as an nis.although every attempt might be made to employ standard components (e.g., operating system, network, web browsers, database management systems, and userinterface generators) in building an nis, the waysin which the components are used are likely to be sufficiently novel thatgeneralizing from past experiences with the components may be uselessfor estimating project costs and schedules. for example, it is not hard toconnect browsers through a network to a server and then display what ison the server, but the result does not begin to be a medical records system,with its varied and often subtle trustworthiness requirements concerningpatient privacy and data integrity. the basic services are even fartherfrom a complete telemedicine system, which must be trusted to correctlyconvey patient data to experts and their diagnoses back to paramedicalpersonnel. all in all, confidence in budget and schedule estimates for annis, as for any engineering artifact, can be high only when the new system is similar to systems that already have been built. such similarity israre in the software world and is likely to be even rarer in the nascent fieldof nis development.the difficulties of cost estimation and scheduling explain why someprojects are initiated with unrealistic schedules and assignments of staffand equipment. the problem is compounded in commercial productdevelopment (as opposed to specialized, oneofakind system development) by marketing concerns. for softwareintensive products, early arrival in the marketplace is often critical to success in that marketplace.this means that software development practice becomes distorted tomaximize functionality and minimize development time, with little attention paid to other qualities. thus, functionality takes precedence overtrustworthiness.a major difficulty in project management is coping with ambiguousand changing requirements. it is unrealistic to expect correct and complete knowledge of requirements at the start of a project. requirementschange as system development proceeds and the system, and its environment, become better understood. moreover, software frequently is regarded (incorrectly) as something that can be changed easily at any pointtrust in cyberspacecopyright national academy of sciences. all rights reserved.68trust in cyberspaceduring development, and software change requests then become routine.the effect of the changes, however, can be traumatic and lead to designcompromises that affect trustworthiness.another difficulty in project management is selecting, tailoring, andimplementing the development process that will be used. the waterfalldevelopment process (pressman, 1986), in which each phase of the lifecycle is completed before the next begins, oversimplifies. so, when thewaterfall process is used, engineers must deviate from it in ad hoc ways.nevertheless, organizations ignore better processes, such as the spiralmodel (boehm, 1988; boehm and demarco, 1997), which incorporatescontrol and feedback mechanisms to deal with interaction of the lifecyclephases.also contributing to difficulties in project management and planningis the high variance in capabilities and productivity that has been documented for different software engineers (curtis, 1981). an orderofmagnitude variation in productivity is not uncommon between the most andthe least productive programmers. estimating schedules, assigning manpower, and managing a project under such circumstances are obviouslydifficult tasks.finally, the schedule and cost for a project can be affected by unanticipated defects or limitations in the software tools being employed. forexample, a flawed compiler might not implement certain language features correctly or might not implement certain combinations of languagefeatures correctly. configuration management tools (e.g., rochkind, 1975)provide other opportunities for unanticipated schedule and cost perturbation. for use in an nis, a configuration management tool not only musttrack changes in locally developed software components but also mustkeep track of vendor updates to cots components.none of the difficulties are new revelations. brooks, in his classicworkthe mythical manmonth (brooks, 1975), noted similar problemsmore than two decades ago. it is both significant and a cause for concernthat this book remains relevant today as evidenced by the recent publication of a special 20th anniversary edition. the difficulties, however, become even more problematic within the context of large and complexniss.requirements at the systems levelbackgroundthere is ample evidence that the careful use of established techniquesin the development of large software systems can improve their quality.yet many development organizations do not employ techniques that havetrust in cyberspacecopyright national academy of sciences. all rights reserved.software for networked information systems69been known for years to contribute to success. nowhere is this refusal tolearn the lessons of history more pronounced than with respect to requirements documents.whether an nis or a simple computer game is being implemented, arequirements document is useful. in specialpurpose systems, it forms acontract between the customer and the developer by stating what thecustomer wants and thereby what the developer must build. in projectsaimed at producing commercial products, it converts marketing and business objectives into technical terms. in the development of large systems,it serves as a vehicle for communication among the various engineeringdisciplines involved. and it also serves as a vehicle for communicationbetween different software engineers responsible for developing software,as well as between the software engineers and those responsible for presenting the software to the outside world, such as a marketing team.it is all too common, however, to proceed with system developmentwithout first analyzing and documenting requirements. in fact, requirements analysis and documentation are sometimes viewed as unnecessaryor misdirected activities, since they do not involve creating executablecode and are thought to increase time to market. can system requirements not be learned by inspecting the system itself? requirements derived by such a posteriori inspections, however, run the risk of beingincomplete and inaccurate. it is not always possible to determine a posteriori which elements of an interface are integral and which are incidentalto a particular implementation. in the absence of a requirements document, project staff must maintain a mental picture of the requirements inorder to respond to questions about what should or could be implemented. each putative requirements change must still be analyzed andnegotiated, only now the debate occurs out of context and risks overlooking relevant information. such an approach might be adequate for smallsystems, but it breaks down for systems having the size and complexity ofan nis.the system requirements documentthe system requirements document states in as much detail as possible what the system should (and should not) do. to be useful for designers and implementers, a requirements document should be organizedas a reference work. that is, it should be arranged so that one can quicklyfind the answer to a detailed question (e.g., what should go into an admissions form?). such a structure, more like a dictionary than a textbook,makes it difficult for persons unfamiliar with the project to grasp how thenis is supposed to work. as a consequence, requirements documents aresupplemented (and often supplanted) with a concept of operationstrust in cyberspacecopyright national academy of sciences. all rights reserved.70trust in cyberspace(conops) that describes, usually in the form of scenarios (socalled “usecases”), the operation of the nis. a conops for the example hmo systemmight, for example, trace the computer operations that support a patientfrom visiting a doctor at a neighborhood clinic, through diagnosis of acondition requiring hospitalization, admission and treatment at the hospital, discharge, and followup visits to the original clinic. other scenariosin the conops might include home monitoring of chronic conditions,emergency room visits, and so forth. the existence of two documentscovering the same ground raises the possibility of inconsistencies. whenthey occur, it is usually the conops that governs, because the conops isthe document typically read (and understood) by the sponsors of theproject.review and approval of system requirements documents may involve substantial organizational interaction and compromise when onceindependent systems are networked and required to support overallorganizational (as opposed to specific departmental) objectives. the compromises can be driven more by organizational dynamics than by technical factors, a situation that may lead to a failure to meet basic objectiveslater on. that risk is heightened in the case of the trustworthiness requirements, owing (as is discussed below) to the difficulty of expressing suchrequirements and compounded by the difficulty of predicting the consequences of requiring certain features. in the case of the hmo system, forexample, advocates for consumer telemedicine might insist on home computer access to the network in ways that are incompatible with maintaining even minimal medical records secrecy in the face of typical hackers.anticipating and dealing with such a problem require predicting whatsorts of attacks could be mounted, what defenses might be available incots products, and how attacks will propagate through an nis whosedetailed design might not be known for several years. making the worstcase assumption (i.e., all cots products are completely vulnerable andall defenses must be mounted through the locally developed software ofthe nis) will likely lead to unacceptable development costs. similar situations arise for other dimensions of trustworthiness, such as data integrityor availability.notation and stylerequirements documents are written first in ordinary english, whichis notorious for imprecision and ambiguity. most industrial developersdo not use even semiformal specification notations, such as the scr/a7tabular technique (heninger, 1980). the principal reason for using natural language (in addition to the cynical observation that without ambiguity there can be no consensus) is that, despite significant r&d investmenttrust in cyberspacecopyright national academy of sciences. all rights reserved.software for networked information systems71in the 1970s (ross, 1977), no notation for systemlevel requirements hasshown sufficiently commanding advantages to achieve dominant acceptance.finally, many—if not most—software developers are forced to lead“unexamined lives.” the demand for their services is so great that theymust move from one project to the next without an opportunity for reflection or consideration of alternatives to the approaches they used before.the paradoxical result of this situation is that the process of developingsoftware, which has had revolutionary impact on many aspects of societyand technology, is itself quite slow to change.one common strategy for coping with the problems inherent in natural language is to divide the requirements into two classes: criteria forsuccess (often called “objectives” or “goals”) and criteria for failure (sometimes called “absolute requirements”). the criteria for success can be amatter of degree: situations where “more is better” without clear cutoffpoints. the criteria for failure are absolute—conditions, such as causing afatality, that render success in other areas irrelevant. in the hmo example, a criterion for success might be the time needed to transfer amedical record from the hospital to an outpatient facility—quicker is better, but unless some very unlikely delays are experienced, the system isacceptable. a criterion for failure might be inaccessibility of informationabout a patient’s drug allergies. if the patient dies from an allergic reaction that could have been prevented by the timely delivery of drug allergydata, then nothing else the system has done right (such as the smoothnessof admission, proper assignment of diagnostic codes, or the correct interfacing with the insurance carrier) really matters.it is often posited that requirements should state what a particularcriterion is but not how that criterion should be achieved. in realworldsystems development, this dictum can lead to unnecessarily convolutedand indirect formulations of requirements. the issue is illustrated byturning to building codes, which are a kind of requirements document.building codes distinguish between performance specifications and design specifications. a performance specification states, “interior wallsshould resist heat of x degrees for y minutes.” a design specificationstates,“interior walls should use 5/8inch type x sheetrock.” performance specifications leave more room for innovation, but determiningwhether they have been satisfied is more difficult. design specificationstend to freeze the development of technology by closing the market toinnovations, but it is a simple matter to determine whether any givendesign specification has been fulfilled. more realistic guidance for whatbelongs in a requirements document is the following: if it defines eitherfailure or success, it belongs in the requirements document, no matterhow specific or detailed it is.trust in cyberspacecopyright national academy of sciences. all rights reserved.72trust in cyberspacea distinction is sometimes made between functional requirementsand nonfunctional requirements. when this distinction is made, functional requirements are concerned with services that the system shouldprovide and are usually stated in terms of the system’s interfaces; nonfunctional requirements define constraints on the development process,the structure of the system, or resources used during execution (sommerville, 1996). for example, a description of expected system outputs inresponse to various inputs would be considered a functional requirement. stipulations that structured design be employed during systemdevelopment, that average system response time be bounded by somevalue, or that the system be safe or secure exemplify nonfunctional requirements.nonfunctional requirements concerning execution theoretically canbe translated into functional requirements. doing that translation requires knowledge of system structure and internals. the resulting inferred functional requirements may concern internal system interfacesthat not only are unmentioned in the original functional requirements butalso may not yet be known. moreover, performing the translation invariably will involve transforming informal notions, such as “secure,”“reliable,” or “safe,” into precise requirements that can be imposed on theinternals and interfaces of individual modules. formalizing informalproperties at all and decomposing systemwide global properties intoproperties that must be satisfied by individual components are technically very challenging tasks—tasks often beyond the state of the art (abadiand lamport, 1993; mclean, 1994).where to focus effort in requirementsanalysis and documentationthe process of requirements analysis is complicated by the fact thatany nis is part of some larger system with which it interacts. an understanding of the application domain itself and mastery of a variety ofengineering disciplines other than software engineering may be necessary to perform requirements analysis for an nis. identification of system vulnerabilities is one process for which a broad understanding of thelarger system context (including users, operators, and the physical environment) is particularly important. techniques have been developed todeal with some of these issues. modeling techniques, such as structuredanalysis (constantine and yourdon, 1979), have been developed for constructing system descriptions that can be analyzed and reviewed by customers. rapid prototyping tools (tanik et al., 1989) offer a means toanswer specific questions about the requirements for a new system, andprototyping is today a popular way to determine user interface requiretrust in cyberspacecopyright national academy of sciences. all rights reserved.software for networked information systems73ments. systematic techniques have been developed for determining application requirements by either interviewing application experts or observing the actions of potential users of the system (potts et al., 1994).interviews conducted in the 1970s with experienced project managersrevealed their skepticism about making significant investments in systemlevel requirements documents (honeywell corporation, 1975). thoseveterans of largescale aerospace and defense projects believed that anysignificant efforts regarding requirements should be directed to the levelof subsystems or components. they argued that systemlevel requirements documents were seldom consulted after detailed componentlevelrequirements were written. change—sometimes significant change—insystemlevel requirements was quite common and rendered obsolete asystemlevel requirements document.changes in requirements originate from a variety of sources:•the outside environment may change––the example hmo couldmerge, restructure, or be affected by new statutory or regulatory forces.•the advent of new technology could generate a desire for the enhanced capability that the technology provides. this factor would beamplified for the hmo’s nis by the current rapid development pace ofinternetrelated technology (socalled internet time) and the false perception that components and features can be added to an nis with relativeease.requirements errors are the most expensive to fix, because they typically are not found until significant resources have been invested in system design, implementation, and, in some cases, testing and deployment.the high cost of repairing such errors would then justify expending additional resources on systems requirements analysis and documentation.but that argument is incomplete, for it presumes that the additional expenditures could prevent such errors. published (glass, 19813) and unpublished (honeywell corporation, 1975) studies of requirements errorsindicate that errors of omission are the most common. experienced program managers, who have internalized the experience of unpleasant surprises resulting from combinations of inputs and internal states (or otherphenomena that were thought to be impossible), understand that noamount of effort is likely to produce a complete requirements document.resources expended in requirements analysis and documentation are,nevertheless, usually well spent. the activity helps a system’s developersto better understand the problem they are attacking. design and coding3this reference contains the classic “reason for error” entry in a trouble report: “insufficient brain power applied during design.”trust in cyberspacecopyright national academy of sciences. all rights reserved.74trust in cyberspacedecisions are thus delayed until a clearer picture of needs and constraintshas emerged. it is not the documentation but the insight that is the important work product. conceivably, other techniques could be developed foracquiring this insight. however, systems requirements documents servealso for communication within a project team as well as with customersand suppliers; any alternative technique would have to address this needas well.doing a bad job at requirements analysis actually can have harmfullongterm repercussions for a development effort. requirements analysisinvariably goes astray when analysts are insufficiently familiar with theanticipated uses of the system being contemplated or with the intendedimplementation technology. it also can go astray when analysts becomegrandiose and formulate requirements far in excess of what is actuallyneeded. finally, inevitable changes in context and technology mean thatrequirements analysis and documentation should be an ongoing activity.to the extent possible, requirements should be determined at the outset ofdevelopment and updated as changes occur during development. inpractice, requirements analysis and documentation mostly occur early inthe process.toplevel designthe trustworthiness of a system depends critically on its design. oncethe system’s requirements and (optionally) the conops are approved, thenext step is development of a toplevel design. this document is oftencalled an “architecture” to emphasize just how much detail is being omitted. during development of the toplevel design, basic types of technology are selected, the system is divided into components and subsystems,and requirements for each component are defined. this process has beencalled“programming in the large,” to distinguish it from writing code, or“programming in the small” (deremer and kron, 1976).components are building blocks for integration, and subsystems areclusters of components that are integrated first as a group and then theassemblage integrated into the whole. for software that is being developed (as opposed to purchased), the size of a component or subsystem isdetermined by the number of lines of code, the programming languageused, and the complexity of the algorithms involved. a rough rule ofthumb is that a component (or “module”) is a body of software that can befully grasped4 by one or two programmers. using the same principle, a4that is, some member of the team can answer any question about the subsystem; it is notnecessary (or even desirable) that every member of the team be able to answer every question.trust in cyberspacecopyright national academy of sciences. all rights reserved.software for networked information systems75subsystem is a body of code that can be fully grasped by a team of three tofive programmers, which happens also to be the maximum size groupthat can be supervised effectively by a team leader.there exist no generally accepted notations for toplevel design. mostdesigns are described using diagrams. such diagrams rarely have precisely defined semantics, so they are not always helpful for determiningwhether a toplevel design includes all the necessary functions or satisfiesall of its requirements.a dependency analysis (parnas, 1974) should be performed on thetoplevel design, where a dependency is defined to exist between components a and b if the correct operation of a depends on the correct operation of b. the results of a dependency analysis are captured in a dependency diagram.5 experienced designers attempt to move functions amongcomponents to eliminate cycles in the dependency diagram. in a cycle,the correctness of one component depends directly or indirectly on thecorrectness of another, and the correctness of the second depends directlyor indirectly on the correctness of the first, thereby forming a circularrelationship. where a cycle exists, all components in the cycle must beintegrated and tested as a unit. in the extreme case—socalled“big bang”integration—all components are integrated at one time; that process seldom has a positive outcome. at present there is no scientific foundationfor determining, analyzing, or changing dependency relationships amongcomponents in largescale systems.many would argue that interface determination and design are theessence of system design (lampson, 1983). therefore, an important output of the toplevel design activity is precise specifications for the system’sinterfaces. these specifications define the formats and protocols for interactions between components and subsystems. a rigorous interface description is particularly important when the interface being defined isbetween subsystems implemented by different teams.6 the definition ofinterfaces and the determination of which interfaces are sufficiently important to warrant control by project management are, like the rest of toplevel design, more an art than a science.5as with the toplevel design itself, there exist no generally accepted notations for suchdiagrams, nor do there exist widely used tools to support the development of dependencydiagrams.6there is an element of program management lore called conways’s law whose essenceis that the human organization of a software project and the technical organization of thesoftware being produced will be congruent. the law was originally stated as,“if you havefour teams working on a compiler, you get a fourpass compiler.” a more general formulation is that “a system’s structure resembles the organization that produces it” (raymondand steele, 1991).trust in cyberspacecopyright national academy of sciences. all rights reserved.76trust in cyberspacedespite the innovative design concepts that have appeared in theliterature in areas such as objectoriented design (meyer, 1988) and architectural description languages (garland and shaw, 1996), still no comprehensive approach to the design and analysis of niss exists. importantchallenges remain in design visualization, design verification, design techniques (that accommodate longterm evolution), cots, and legacy components, as well as tool support for the creation and analysis of designs.among the most critical issues are design verification and design evolution, since assuring that a design will continue to implement the necessary trustworthiness properties—even as the system evolves—is centralto building an nis. moreover, because toplevel design occurs relativelyearly in the life cycle, detection of defects during the toplevel designstage has great leverage.perhaps the greatest design challenges concern techniques to compose subsystems in ways that contribute directly to trustworthiness. nissare typically large and, therefore, they must be developed and deployedincrementally. significant features are added even after an nis is firstdeployed. thus, there is a need for methods to identify feature interactions, performance bottlenecks, omitted functionality, and critical components in an nis that is being developed by composition or by accretion.there exists a widening gap between the needs of software practitioners and the ability to evaluate software technologies for developing moderate to largescale systems. the expense of building such systems renders infeasible the traditional form of controlled scientific experiment,where the same system is built repeatedly under controlled conditionsbut using differing approaches. benefits and costs must be documented,risks enumerated and assessed, and necessary enhancements or modifications identified and carried out. one might, instead, attempt to generalize from the experiences gained in different projects. but to do so andreach a sound conclusion requires understanding what aspects of a system interact with the technology under investigation. some advantageswould probably accrue if only software developers documented theirpractices and experiences. this activity, however, is one that few programmers find appealing and few managers have the resources to support.critical componentsa critical component is one whose failure would result in an undetected and irrecoverable failure to satisfy a trustworthiness requirement.experienced designers attempt to produce toplevel designs for which thenumber of components that depend on critical components is not constrained but the critical components themselves depend on as few othertrust in cyberspacecopyright national academy of sciences. all rights reserved.software for networked information systems77components as possible. this strategy achieves two things: it enablesdevelopers to use freestanding tests and analyses to build trust in thecritical components, and it permits an orderly integration process in whichtrusted components become available early. unless the critical components come from vendors with impeccable credentials, developmentteams generally prefer, wherever feasible, to implement the critical components themselves. that way, all aspects of the design, implementation,and verification of critical components can be strictly controlled. thereare two risks in pursuing this approach. one is that the criticality of acomponent has been overlooked—a danger that is increased by the lackof a scientific basis to assess the criticality of components. a second isthat it may not be feasible to implement a critical component inhouse or,for a vendorprovided critical component, it may not be possible to obtainsufficient information to be convinced of that component’s trustworthiness.7the integration planonce the basic structure of the system has been established, the integration plan is produced. ideally, the plan involves two activities:1.integration of components into subsystems that reside on singlenetwork nodes; and2.connection of network nodes into subsystems that perform definable functions and whose behavior can be observed and evaluated, followed by the connection of the subsystems into the final nis.the essence of the integration process is progress toward a completelyoperational system on a stepbystep basis. observed defects can be localized to the last increment that was integrated—if one build passes its testsand the next build fails its tests, then the most likely sources of difficultyare those components that turned the first build into the second. workingin this manner, the integration team should not have to revisit previouslyintegrated components or subsystems during the integration process.and this process avoids a cycle of “fix and test and fix again” that couldcontinue until time, money, or management patience runs out. note thatfor the integration process to be successful, the toplevel design mustexhibit proper dependency relationships between components. an inte7in the case of a browser, which might well be a critical component in an nis, this situation is ameliorated by netscape’s recent decision to release the netscape navigator sourcecode. a development team now can examine the code and possibly eliminate unwantedfunctionality.trust in cyberspacecopyright national academy of sciences. all rights reserved.78trust in cyberspacegration plan thus can serve another purpose: to force the detailed analysis of a toplevel design. toplevel designs lacking straightforward integration plans are likely to be ambiguous, incomplete, or just plain wrong.integration skills today are developed only through experience. thereis essentially no theoretical basis for deciding what should constitute abuild, nor has the problem received serious scientific examination. system integration continues to be practiced as a craft that is passed alongthrough apprenticeship. the drift of university computer science researchfrom emphasizing large experimental systems projects (such as multics,c.mmp, and berkeley unix) toward undertaking smaller engineering efforts is of particular concern. looking back at the master’s and ph.d.thesis topics at the massachusetts institute of technology (as an example)during the multics era, it is striking how many concern software that hadto be integrated into the larger system in a planned and disciplined manner. the shrinking of this skills base in orderly integration is furtherexacerbated by the reward system of the personal computer market. financial benefits flow principally to authors of the freestanding application or component (the socalled “killer app”) that attracts large numbersof consumers or is selected for use in information systems assembledfrom cots components. this latter case involves a different set of skillsfrom those required to design, implement, and integrate a large systemfrom scratch.project structure, standards, and processother branches of engineering rely heavily on controlling the development process to ensure the quality of engineering artifacts. the softwareengineering institute’s capability maturity model (cmm) is a step in thatdirection for software design and development (see box 3.1). as withrequirements definition and analysis, there is considerable anecdotal evidence and some experimental evidence that having a systematic process inplace contributes to the quality of software systems that an organizationdevelops. there is, however, little evidence that any one process can bedistinguished from another, nor is there evidence that different characteristics of development processes are correlated with product quality.rigorous, repeatable processes are sometimes thought to result whensoftware development standards are imposed on organizations. suchstandards typically prescribe overall process structure, documents to beproduced, the order of events, techniques to be used, and so on. a recentstudy found 250 different standards that apply to the engineering of software, yet the authors of the study found that the standards were largelyineffective and concluded that software technology is too immature tostandardize (pfleeger et al., 1994).trust in cyberspacecopyright national academy of sciences. all rights reserved.software for networked information systems79box 3.1the sei capability maturity model for softwarethe software engineering institute’s (sei) capability maturity model (cmm) forsoftware was first introduced in the late 1980s. the current version, version 1.1, wasintroduced in 1993.1 according to the sei: “the capability maturity model forsoftware (swcmm or cmm) is a model for judging the maturity of the softwareprocesses of an organization and for identifying the key practices that are required toincrease the maturity of the processes. the swcmm is developed by the softwarecommunity with stewardship by the sei” (paulk et al., 1993).the cmm defines a maturity framework that has five levels: (1) initial, (2) repeatable, (3) defined, (4) managed, and (5) optimized. the five levels are carefully defined and based on key process areas (kpas). the kpas are, as the name suggests,the most important aspects of software processes. at cmm level 2, for example,requirements management is a kpa.it is important to understand that the cmm is intended only to measure maturity.it is not a software development process standard or a mechanism for assessing specific software development techniques. it also is not a means of achieving highlevels of either productivity or software quality (although some users report that bothtend to improve after higher cmm levels have been achieved). rather, the cmmaims to assess the ability of an organization to develop software in a repeatable andpredictable way. thus, an organization possessing a high cmm level will not necessarily develop software more quickly or of better quality than an organization havinga lower level. the higherranked organization will, however, develop software in amore predictable way and will be able to do so repeatedly.after a careful analysis, an assessed organization is rated at one of the five levelsof the cmm framework. attainment of some specified minimum cmm level is sometimes required to bid on certain government contracts. (the practice seems to bebecoming more common within the department of defense.) whether having sucha minimum cmm level ensures higherquality work is not clear, but it has succeededin making corporate management aware of the importance of software developmentprocesses.a second benefit of the cmm has been reported by organizations seeking toimprove their ratings. the staff of such organizations become more conscious of thesoftware technology they are using and how it can be improved. esprit de corpstends to be generated when the entire staff is involved in a single processimprovement goal.although there is no specific intention that higher cmm rankings will be associated with higher quality or productivity, there is some evidence that more matureprocesses do yield those advantages. watts and his colleagues document a variety ofbenefits and important lessons they observed at hughes aircraft after moving fromcmm level 2 to level 3 (watts et al., 1991). dion reports increased productivity andcontinues on next pagetrust in cyberspacecopyright national academy of sciences. all rights reserved.80trust in cyberspace1the cmm of the software engineering institute is available online at <http://www.sei.cmu.edu/technology/cmm.html>.large cost savings at raytheon after it moved from level 2 to level 3 (dion, 1993).and motorola, which observed the development performance of 34 different projectswith roughly equal numbers of projects rated at each cmm level (diaz and sligo,1997), has reported reduced cycle time, reduced defect rates, and improved productivity as cmm level increased.however, a recent paper by mcgarry, burke, and decker (1997) is less favorablein discussing the correlation between cmm level and software development metricsbased on data from more than 90 projects within one organization (a part of computer sciences corporation). the results of the study were mixed, and in most casesimprovements were not correlated with cmm level.impacts of process improvement have also been surveyed. brodman and johnson(1996) report survey data in the form of return on investment to industry. theirresults document a wide variety of benefits associated with achieving higher cmmlevels. lawlis, flowe, and thordahl (1995) investigated the effect of cmm level onsoftware development cost and schedule. they found a positive correlation betweencmm level and cost and schedule performance. another survey reporting positiveresults of using the cmm has been published by herbsleb and goldenson (1996).the actual cmm assessment process has also been studied. kitson and masters(1993) identify which kpas are major factors affecting cmm ratings, thereby suggesting areas of weakness in industrial software practice.although many successes of the cmm have been reported, the cmm itself hasalso been criticized. bollinger and mcgowan (1991) raised a number of importantquestions about the practical benefits of an initial version of the cmm in the contextof government contracting. their concerns were mainly with the relative simplicityof the assessment process and the fact that cmm levels would be used for ratinggovernment contractors. the criticisms of bollinger and mcgowan were addressedby the developers of the cmm in watts and curtis (1991). more recently, fayad andlaitinen (1997) criticized aspects of the cmm ranging from the cost of assessment tothe fact that a single assessment scheme is used for organizations of all sizes. although these criticisms have merit, they do not appear to be fundamental flaws in thecmm concept.box 3.1 continuedtrust in cyberspacecopyright national academy of sciences. all rights reserved.software for networked information systems81barriers to acceptance of new software technologiesthe high costs associated with adopting new software technologiesmake managers less likely to do so. the concern is that, despite claimedbenefits, problems might arise in using the new technology and theseproblems might lead to missed deadlines or budget overruns. stickingwith technology that has been used before—the conservative course—reduces the risks.managers’ fears are well founded in many cases, as many new software technologies do not work when tried on industrialscale problems.things that work well in the laboratory are not guaranteed to work wellin practice. all too often, laboratory assessments of software technologyare based on experiences with a few small examples. the need to investigate the scaling of a new technology is common to all branches of engineering but, as already discussed, the expense of performing largescalesoftware experiments makes such experiments infrequent. to assess anew software technology, the technology should be observed in fullscaledevelopment efforts. any research program that aspires to relevanceshould include plans for compelling demonstrations that the resultanttechnology is applicable to industrialscale problems and that its benefitsjustify the costs of learning and applying it.many new software technologies are also toolintensive. they try toimprove software development practices by replacing or supplementinghuman effort. testing an interactive application that employs a graphicuser interface, for example, requires the manipulation of complex software structures, the management of extensive detail, and the applicationof sophisticated algorithms. it all could be undertaken by hand, but having computers perform as much of the work as possible is preferable. yet,software tools are notoriously expensive to develop because, although theessence of a new idea might be relatively simple to implement, providingall the basic services that are needed for practical use is neither simple norinexpensive. in addition, learning to use new software tools takes time.the result is one more barrier to the success of any new software technology.findings1.although achieving connectivity and providing basic services arerelatively easy, providing specialized services—especially trustworthyones—is much more difficult and is complicated by the decentralized andasynchronous nature of niss.2.project management, a longstanding challenge in software development, becomes even more problematic in the context of niss because oftrust in cyberspacecopyright national academy of sciences. all rights reserved.82trust in cyberspacetheir large and complex nature and the continual software changes thatcan erode trustworthiness.3.whereas a large software system cannot be developed defect free,it is possible to improve the trustworthiness of such a system by anticipating and targeting vulnerabilities. but to determine, analyze, and, mostimportantly, prioritize these vulnerabilities requires a good understanding of how the software interacts with the other elements of the largersystem.4.it seems clear from anecdotal evidence that using any methodicaland tested technique for the capture and documentation of requirements—no matter what its shortcomings—is better than launching directly intodesign and implementation.5.no notation for systemlevel requirements has shown sufficientlycommanding advantages to become dominant.6.systemlevel trustworthiness requirements typically are first characterized informally. the transformation of the informal notions intoprecise requirements that can be imposed on system components is difficult and often beyond the current state of the art.7.niss generally are developed and deployed incrementally. thus,techniques are needed to compose subsystems in ways that contributedirectly to trustworthiness.8.there exists a widening gap between the needs of software practitioners and the problems that are being attacked by the academic researchcommunity. in most academic computer science research today, researchers are not confronting problems related to largescale integration andstudents do not develop the skills and intuition necessary to developsoftware that not only works but also works in the context of softwarewritten by others.9.although systematic processes may contribute to the quality ofsoftware systems, specific processes or standards that accomplish thisgoal have not been demonstrated.10.since the investment of resources needed for a large software development project is substantial, managers are reluctant to embrace newsoftware technologies because they entail greater risks.building and acquiring componentscomponentlevel requirementsit is useful to distinguish between two kinds of componentlevel requirements: allocated or traceable requirements, which devolve directlyfrom system requirements, and derived requirements, which are consequences of the system architecture. in the hmo system, for example,trust in cyberspacecopyright national academy of sciences. all rights reserved.software for networked information systems83there might be an overall trustworthiness requirement that medicalrecords must be available 24 hours a day, 7 days a week. one way to meetthat need would be to replicate records on two different servers; the datamanagement software then has the derived requirement of ensuring theconsistency of the data on the two servers. the requirement is “derived”because it results not so much from an interpretation or clarification of theoriginal trustworthiness requirement but rather from the architecturalstrategy—replication—being used to satisfy the trustworthiness requirement.a common practice is to insist that all requirements at the componentlevel be testable. that is, each requirement must be accompanied by someexperiment for assessing whether that requirement is satisfied. thesetests must be chosen with care because, in actual practice, cost and schedule pressures drive a development team toward making sure their component passes the tests as a first priority. if a test is not chosen carefullyand described unambiguously, then a component that does not satisfy thespirit or even the letter of the actual requirements statement might bedeemed acceptable.the relationship between the requirements, which capture intent, anda test, which determines acceptance, is especially problematic for nonfunctional requirements in support of trustworthiness concerns. continuing with the hmo medical record example, the test may check thatthe two copies of the medical record are synchronized within so manyseconds of a change having been made, that the failure of the primaryserver is detected by the switchover logic within so many seconds, thatswitchover is accomplished in so many seconds, and so on. the problemis that the list of tests is not equivalent to the requirement being tested(i.e., availability 24 hours a day, 7 days a week). for example, the tests donot take into account simultaneous or cascading failures (e.g., primaryfails while secondary is running backup, secondary fails immediately after switchover, synchronization request comes in at just the wrong time asswitchover is being initiated, and so on). there are thus circumstances inwhich the component or subsystem will pass its tests but fail to satisfy theintent of the requirement.detailed, componentlevel requirements for user interfaces are difficult to write. socalled storyboards, which show display configurationsfor various inputs, outputs, and states of the system, can be hard to follow. however, the popularity of graphical user interfaces has led to thedevelopment of tools that enable designers to rapidly prototype user interfaces. generally speaking, prototyping is sensible in requirementsanalysis and can even serve as an executable requirements document.but the cost of building prototypes can be high, thereby preempting otherhigherpayoff forms of requirements analysis. for example, devoting tootrust in cyberspacecopyright national academy of sciences. all rights reserved.84trust in cyberspacemuch effort to prototyping a user interface can lead to software in whichan elaborate user interface surrounds a poorly thoughtout core.component design and implementationto project managers, component design and implementation are theleast visible of the phases. a large number of activities are proceeding inparallel, the staff are focused on their individual tasks (perhaps ignoringthe global view), and the tasks themselves are highly technical. all conspire to make measuring progress or even anecdotal observations of status extremely difficult. while there is an extensive literature on the problem of demonstrating that a component satisfies its specification, there isconsiderably less literature devoted to determining whether a componentlevel specification properly reflects or contributes toward satisfyingsystem requirements.for code written in traditional languages (such as c) running on asingle node, and interacting in limited and controlled ways with usersand other software, the craft of programming has evolved into a generallyaccepted process. as practiced within the aerospace, defense, and otherlargescale computing system development communities (but not necessarily in commercial practice) over the last two decades, that processconsists of roughly the following steps:•review the component requirements document for sanity.•prepare a component design in some notation, often called“pseudocode.” (pseudocode is usually a mixture of programming language statements and some less detailed notation, not excluding naturallanguage.)•conduct an organized inspection of the component design (“structured walkthrough”) with an emphasis on the logic flow.•write component test scripts or test drivers to exercise the component after it has been written.•write the component in some appropriate higherlevel language(“source code”).8•conduct a structured walkthrough of the source code.•compile the component into executable form.8this and the preceding step are often reversed, and the test drivers are not written untilafter the component is. the order given in the text is preferable because the detailed designand coding of a test driver force implementers to rigorously analyze and understand componentlevel requirements.trust in cyberspacecopyright national academy of sciences. all rights reserved.software for networked information systems85•exercise the component (“unit test” or “level 1 testing”) using thetest scripts or drivers.•release the component to the integration process.this process, and ones like it, have been synthesized from the wreckage of expensive failures, and a significant percentage, if not a majority, ofexperienced practitioners would caution that any of these steps are omitted at one’s peril. one variation is to repeat the cycle frequently, makingvery small changes at each iteration. this approach was used successfully in the multics project (clingen and van vleck, 1978) and has longbeen part of the program management lore in highconsequence realtimesystems.today’s turnover rate among software personnel somewhat reducesthe effectiveness of the componentdevelopment process just described.software development is still typically learned through apprenticeship.yet personnel shortages, the potential financial rewards and short lifecycles of startup companies, and the deterioration of corporate loyalty asa result of downsizing and restructuring make it less likely that a juniorpractitioner will witness a complete project life cycle, much less severalprojects conducted in the same organization. ultimately, this will impedethe development of an adequate skill base in critical areas, like synthesisand analysis of design, integration, or structuring of development organizations.the above component development process is predicated on startingwith a modular design. achieving modularity is intellectually challenging and costly; it requires management and design discipline. in addition, modular systems often are larger and slower. so there is a tensionbetween system modularity and cost (along a variety of cost dimensions);it can be hard to know when system modularity is needed and when it isnot worth the cost. moreover, certain nis building blocks—mobile codeand web browsers with helper applications, for example—compromisethe advantages of modular design by permitting unrestricted interactionsbetween different software components.programming languagesmodern programming languages, such as c++, java, and ada, includecompiletime checks to detect a wide range of possible errors. the checksare based on declaring or inferring a type for each object (i.e., variablesand procedures) and analyzing the program to establish that objects areused in ways consistent with their types. this kind of automated supportis especially helpful for detecting the kinds of errors (such as passingarguments that overflow a corresponding parameter) so successfully usedtrust in cyberspacecopyright national academy of sciences. all rights reserved.86trust in cyberspaceby attackers of operating system and network software. ever more expressive type systems are a continuing theme in programming languageresearch, with considerable attention being directed recently at the representation of security properties using types (digital equipment corporation, 1997). success would mean that compiletime checks could play aneven bigger role in supporting trustworthiness properties.modern programming languages also contain features to supportmodularity and component integration. ada, for example, provides typechecking across separate compilations; ada also integrates componentlinking with compilation, so that statements whose validity depends onthe order in which compilation occurs can be checked. other modernlanguages provide equivalent features. at the other end of the spectrum,scripting languages (ousterhout, 1998) (such as visual basic and tcl) aretoday attracting everlarger user communities. these languages are typically typeless and designed to facilitate gluing together software components. the preponderance of cots and legacy components in a typicalnetworked information system assures the relevance of scripting languages to the enterprise.also of interest to nis developers are veryhighlevel languages anddomainspecific languages, which provide farhigherlevel programmingabstractions than traditional programming languages do. the presenceof the higherlevel abstractions enables rapid development of smaller,albeit often less efficient, programs. moreover, programming with abstractions that have rich semantics and powerful operations reduces theopportunity for programming errors and permits more sophisticated compiletime checking.there is much anecdotal and little hard, experimental evidence concerning whether the choice of programming language can enhance trustworthiness. one report (cstb, 1997) looked for hard evidence but foundessentially none. further study is needed and, if undertaken, could beused to inform research directions in the programming language community.systematic reusesystematic reuse refers to the design and implementation of components specifically intended for instantiation in differing systems. it is oneof the most soughtafter goals in software research, because it offers thepotential for substantial software productivity improvements.9 more9it is worth noting that the infamous year 2000 problem would be far easier to address ifa small number of date packages had been reused in datesensitive applications. therewould still be the problem of database conversion, though, once the date format is changed.trust in cyberspacecopyright national academy of sciences. all rights reserved.software for networked information systems87over, components intended for reuse can be more intensely scrutinized,since the higher cost of analysis can be amortized over multiple uses. thecurrent economic emphasis on shortterm results, however, serves to inhibit the acceptance of any method of systematic reuse that requires (asappears inevitable) upfront investment.certain commercial vendors, such as sap, whose r/3 enterpriseapplications software (hernandez, 1997) has captured onethird of theworldwide clientserver market for business systems, claim to have solvedthe systematic reuse problem in a costeffective manner for large classesof applications. r/3 is an integrated software package that includes interwoven reusable components for all the major functions of a commercialenterprise, from order entry and accounting through manufacturing andhuman resources. in addition, r/3 is built to use a cots operatingsystem along with cots database management systems, browsers, anduserinterface software. other commercially driven attempts at providing components or infrastructure for systematic reuse include the c++standard template library (stl) (musser and saini, 1996), common objectrequest broker architecture (corba),10 common object model (com)(microsoft corporation and digital equipment corporation, 1995), distributed common object model (dcom) (brown and kindel, 1998), andjavabeans (hamilton, 1997).there is always a tension between the pressure to innovate and thestability associated with components intended for reuse. that tension isparticularly acute for cots components, for which the addition of newfeatures and time to market are such strong forces. new features areusually accompanied by new bugs; careful analysis of components enhances stability but delays product release. moreover, when bugs incots components do get fixed, the fixes are often bundled in a releasethat also introduces new features. the cots component user must thenchoose between living with a bug and migrating to a release that may beless stable due to new bugs.commercial offtheshelf softwarethe changing role of cots softwaresuccess for a cots software component often leads to deployment insettings never intended. a component might start as an interesting pieceof software at the periphery of trustworthiness concerns and ultimately10cobra 3.0 was introduced by the object management group (omg) in december,1994. additional information is available online at <http://www.omg.org>.trust in cyberspacecopyright national academy of sciences. all rights reserved.88trust in cyberspacebecome a critical component in some nis. in 1994, it would have beenabsurd to suggest that a bug in a web browser could kill someone. yet inthe hmo system we are using as an example, a webbased telemedicineapplication could allow precisely that outcome. that software can beused for tasks not envisioned by its developers is a doubleedged sword,especially if cots development practices cause developers to compromise trustworthiness for other requirements.cots software development practices in the personal computer (pc)era arose in a technical and economic environment that tended to ignoretrustworthiness. pc operating systems and applications ran on isolateddesktops; the consequences of failure were limited to destruction of perhaps valuable, but certainly not lifecritical, data. failures had no way ofpropagating to other machines. therefore, an organizational and programming culture arose that was very accepting of errors and malfunctions, epitomized by the notorious shrinkwrap license whose primaryfeature is a total disclaimer of responsibility by the developer.this climate was amplified by economic conditions of the early pcera. software was purchased separately rather than being bundled with aleased computer, as in the mainframe era. consequently, there was lessfinancial leverage for dissatisfied customers to affect vendor, and therefore developer, attitudes. a customer’s financial leverage was limited toconsuming vendor resources in calls to telephone helplines, which couldbe ignored by inept or uncaring vendors,11 and refusing to purchase othersoftware or the next revision of the malfunctioning product from thatvendor. the latter option is reduced by the diminishing diversity of themarketplace, the need to exchange data with other users, and the investment the customer may have in data that can be processed only by theproduct in question.as the pc market exploded, visionary entrepreneurs realized thatmarket share was the dominant factor in corporate survival and personalfinancial success. market share is heavily influenced by market entrytime. specifically, the first product to reach a market has the greatestopportunity both to gain market share and to establish the de facto standard upon which the software industry currently operates. another influence on market share is the richness of features and user interface,which impresses users and reviewers in the technical press. somethingmust be sacrificed, and it has been trustworthiness aspects such as robustness and security.one way to reduce time to market is to reduce the time spent in11this situation is changing. a vendor, albeit of hardware, has recently settled a classaction suit requiring an increase in warranty and support coverage (manes, 1998). similaractions against software vendors are likely to follow from this precedent.trust in cyberspacecopyright national academy of sciences. all rights reserved.software for networked information systems89testing. by making early releases (beta test versions) available to interested users and by freely distributing incremental updates to productionsoftware, vendors enlist the help of the user community in finding errors.from a societal perspective, the pc software industry’s attitude towarderrors was relatively unimportant, since the worst consequence of pcsoftware errors was the time lost by individuals trying to reconstructdestroyed work or otherwise get their pcs to do their bidding. but today,cots software is moving toward being a business of providing components—and possibly critical components—for niss that can be high consequence, either because they were explicitly designed that way or because people assign to them a level of trust that their designers neverintended.general problems with cots componentsthe use of cots components presents special problems for the responsible developer of an nis. cots software typically is full of featuresthat vary in quality and are a source of complexity. the complexity, inturn, means that specifications for cots components are likely to beincomplete, and users of those components will discover features by experimentation. being conservative in exploiting these discoveries is prudent—semantics not documented in an accompanying written specification may or may not have been intended and consequently may or maynot persist across releases. moreover, wise developers learn to avoid themore complex features of cots components because these are the mostlikely to exhibit surprising behavior and their behavior is least likely toremain stable across releases. when these features cannot be avoided,encapsulating components with wrappers, effectively narrowing their interfaces, can protect against undesirable behaviors.the cots developer’s reliance on customer feedback12 as a significant, or even primary, quality assurance mechanism can lead to unevenquality levels in different subsystems or functionality in a single cotsproduct. press coverage is not guaranteed to be accurate and may notconvey the implications of the problem being reported.13 for example,security vulnerabilities in components such as web browsers, which are12handling calls to customersupport telephone helplines is sometimes claimed to be asignificant portion of cots software costs. the committee was unable to explore the veracity of this claim. however, the use of customer feedback in place of other quality controlmechanisms does allow a software producer to externalize costs associated with producttesting.13see, for example, the february 1997 coverage of the chaos computer club demonstration of a supposed security flaw in microsoft’s internet explorer.trust in cyberspacecopyright national academy of sciences. all rights reserved.90trust in cyberspaceused directly by the public, receive widespread coverage, as do ultimatelyinconsequential (and unsurprising) exploits, such as the use of large numbers of machines on the internet to “break” cryptographic algorithms bybruteforce searches. feedback from customers and the press, by its verynature, occurs only after a product has been distributed. and experiencewith distribution of bug fixes clearly indicates that many sites do not, fora variety of reasons, install such upgrades, thereby leaving themselvesvulnerable to attack through the now highly publicized methods.14 reliance on market forces to select what gets examined and what gets fixed ishaphazard at best and is surely not equivalent to performing a methodical search for vulnerabilities prior to distribution.finally, using cots software in an nis has the advantages and disadvantages that accompany any form of “outsourcing.” cots components can offer rich functionality and may be better engineered and testedthan would be costeffective for components developed from scratch for arelatively smaller user community. but an nis that uses cots components becomes dependent on a third party for decisions about a component’s evolution and the engineering processes used in its construction(notably regarding assurance). in addition, the nis developer must tracknew releases of those cots components and may be forced to makeperiodic changes to the nis in response to those new releases. it all comesdown to a tradeoff between cost and risk: the price of cots componentscan be attractive, especially if the functionality they provide is a goodmatch for what is needed, but the risk of ceding control may or may notbe sensible for any given piece of an nis.interfacing legacy softwarelegacy software refers to existing components or subsystems thatmust be retained and integrated more or less unchanged into a system.legacy software is used when developing an nis because reusing anexisting system is cheaper and less risky than completely reimplementingit, especially given the migration costs (training, rebuilding online records)associated with deploying a replacement system. in the hmo example, itwould be very likely that the clinical laboratory or pathology departments had been operating for decades with freestanding computerizedsystems. incorporating such a freestanding system into an nis posesspecial problems:14even when administrators diligently apply securitybug fixes, the fixes can then be lostwhen a crashed system is restored from backup media. since such restorations are oftendone in a crisis atmosphere, the need to perform the additional update step is easily overlooked in the rush to restore service.trust in cyberspacecopyright national academy of sciences. all rights reserved.software for networked information systems91•nis designers must recognize that they might be dealing with anoperational subsystem that performs critical functions and cannot be rendered inactive for days or even hours.•the legacy subsystem might not have been designed with networking in mind, and conversion to one that supports networking mightnot be feasible.•the system may be an “orphan” product whose vendor no longerexists or supports this version. or, if the system was developed locally,documentation and expertise about its internals might have evaporatedover time.the general approach to dealing with these problems is to fool someinterface of the legacy system into thinking it is operating in isolationwhen, in fact, it is connected to a network. often, an existing interface ofthe legacy system can be wrapped in a new layer of software (called awrapper) that hides the network, perhaps by making the network look tothe legacy software like an existing user interface (e.g., a keyboard anddisplay). and a legacy system might be adapted to use a new communications protocol in place of an old one by writing software that uses theold protocol to simulate the functionality of the new one; this is calledtunneling. the risk with such schemes is that the legacy system’s interface, designed to serve one type of client, might not be able to handle thecharacteristics of the new load. for example, the volume of transactionsarriving over the network might overwhelm an interface that was writtento serve a single human user typing at a terminal. inadequate or incomplete documentation for a legacy system’s interfaces also can complicateemploying the approach.findings1.it is difficult to devise componentlevel acceptance tests that fullycapture the intent of requirements statements. this is particularly true fornonfunctional and user interface requirements.2.high turnover of programming staff is impeding the developmentof an adequate skill base in critical areas, such as nis synthesis and analysis of design, integration, or structuring.3.there are some accepted processes for component design and implementation. however, the performance needs of niss can be inconsistent with modular design, and this fact can limit the applicability of aneffective design tool to nis design.4.modern programming languages include features, such as compiletime checks and support for modularity and component integration,that promote trustworthiness. the potential may exist for further gainstrust in cyberspacecopyright national academy of sciences. all rights reserved.92trust in cyberspaceby developing even more expressive type systems and other compiletime analysis techniques.5.there is inadequate experimental evidence to justify the utility ofany specific programming language or language feature with respect toimproving trustworthiness.6.despite theoretical concerns,15 as a practical matter the use of higherlevel languages increases trustworthiness to a degree that outweighs therisks.7.basing the development of an nis on libraries of reusable trustedcomponents and using those components in critical areas of the systemcan provide a costeffective way to implement componentlevel dimensions of trustworthiness.8.new commercial software that includes usable components or infrastructure for systematic reuse is increasingly available, but it is tooearly to know how successful it will be.9.cots software originally evolved in a standalone environment inwhich trustworthiness was not a primary concern. furthermore, marketpressures contribute to reducing time spent on testing before releasingsoftware to users, while emphasizing features that add to complexity butare useful for only a minority of applications.10.cots software offers both advantages and disadvantages to annis. cots components may be less expensive, have greater functionality, and be better engineered and tested than is feasible for customizedcomponents. yet the use of cots makes developers dependent on outside vendors for the design and enhancement of important components;specifications may be incomplete and may compel users to discover features by experimentation.11.incorporating legacy software into an nis poses risks for trustworthiness because problems may arise as a result of including a previouslyfreestanding system into a networked environment for which it was unintended.integrating components intoa trustworthy systemsystem integrationsubsystem integration is the orderly aggregation of unittested components into a subsystem, along with incremental testing to increase confidence in the subsystem’s correctness. there are three basic approaches:15for example, a theoretically effective attack based on a maliciously modified compilerwas described over a decade ago in thompson (1984).trust in cyberspacecopyright national academy of sciences. all rights reserved.software for networked information systems93bottomup integration, topdown integration, and thread integration. toillustrate, consider the clinical laboratory subsystem for our hmo’s nis.lowerlevel components in the subsystem would control the keyboardand display, maintain local data files, and control interactions with testinstruments; upperlevel management components would select which ofthe lowerlevel ones are activated and in what order. in bottomup integration, a series of programs (called test drivers) iswritten that simulates the upperlevel components of the subsystem. thelowerlevel components (e.g., the ones that control test instruments in theclinical laboratory subsystem) are aggregated first, and only when theircorrect interactions have been observed are the upperlevel componentsadded. the origin of the name “bottom up” should be clear. the approach was popular in the early days of realtime control systems. computer memory was a scarce resource then, and the integration team obtained an early warning of excessive software size by proceeding from thebottom up. bottomup integration carries with it the significant disadvantage that the overall logical operation of the subsystem is observedonly relatively late in the process, when limited time and resources areavailable to deal with incorrect behavior. the opposite of bottomup integration is topdown integration. inthis approach, upperlevel components are integrated first. the components are tested using routines (called stubs) that simulate the behavior ofthe lowerlevel components. the stubs are then replaced one by one withthe components that they are simulating. with topdown integration,logical correctness of the subsystem is established first, but the actual sizeof the entire system is not determined until relatively late in the integration process. thus, if system size is not an issue, topdown integration issuperior to bottomup integration; if size is an issue, then with topdownintegration, failure would likely be due to size problems rather than incorrect logical operation of the system.in both topdown and bottomup integration, confidence in correctbehavior is gained through the use of simulated rather than actual components; stubs are used in topdown integration, and test drivers are usedin bottomup integration. clearly, the use of the actual components wouldbe preferable, so software developers devised a more sophisticated approach known as thread integration or thread testing.in thread integration, the components being joined are selected subsets of the overall subsystem, and test cases are carefully defined to activate only the subset of components under test. there are two ways toselect a subset of components to integrate. one is to select a subset of thesystemlevel requirements. this works when the requirements map ontothe toplevel design in a straightforward manner. the second and morecommon approach is to select subsets of components according to thetrust in cyberspacecopyright national academy of sciences. all rights reserved.94trust in cyberspacetoplevel design and the sequence of component activations (the calltree).16as an example, a single build in a thread integration of our clinicallaboratory subsystem might combine the keyboard and display component, the management component, and an interface to a single test instrument (say, for blood sugar). a thread test of this build would involve anoperator sitting at the console and initiating a blood sugar test; the factthat, say, the hepatitis antibody test components are not yet integrateddoes not matter, since these components would not be activated by thetest.17 when all the builds are complete, confidence is increased that thecomponents not only work properly in isolation (which is the concern ofunit testing) but also work together. in traditional software development, the word “subsystem” in thepreceding discussion could be replaced by the word “system.” once theintegration of a single node was complete, the job was done. however,the structure of an nis adds another level to the integration process.disparate nodes in a network must interact to perform a single, coordinated task. relatively little is known about approaches to performing thisadditional level of integration compared with what is known about subsystem integration. by their very nature, networks pose special problemsto an integration team. for one thing, inputs may have to be submittedmiles from where corresponding outputs must be observed. for another,system behavior might be load dependent, but operational loads are veryhard to simulate (notwithstanding various efforts over many years). infact, when public networks are being used, various aspects of networkbehavior become uncontrollable, which means certain tests might not bepossible and others might not be repeatable.system assurancereview and inspectionone commonly used technique for improving software quality is toundertake technical reviews, sometimes known as inspections (fagan,1986), in which objective critics examine a design or artifact in detail. asubsequent meeting of the critics allows discussion of specific defects thattheir examinations have revealed; the meeting also facilitates brainstorming about more systemic flaws that were observed. a great deal of effort16the“thread of control”—hence the name of the technique.17in actual use, stubs are incorporated to raise alarms if the decisionmaking componentactivates the wrong thing.trust in cyberspacecopyright national academy of sciences. all rights reserved.software for networked information systems95has gone into studying various types of technical reviews and variousways of organizing them, and much is known about the benefits of theapproaches (porter et al., 1997), yet their utility in security is not welldocumented. for example, no evidence could be identified to confirmwhether traditional forms of technical reviews could facilitate the detection of security vulnerabilities in an implementation.18 a simple checklistbased review might be helpful for eliminating wellknown vulnerabilities, such as failure to validate arguments, but the overall impact ofthis activity on trustworthiness properties has not been determined andshould be studied. it might also be possible to employ technical reviewsin order to identify assumptions being made by designers of a system––assumptions that can become vulnerabilities should an attacker causethem to be violated.formal methodsformal methods is the name given to a broad class of mathematicallybased techniques for the description and analysis of hardware, software,and entire computing systems. the descriptions may range, on the onehand, from general statements about desirable system properties, as mightbe found in a requirements document or highlevel specification, to, onthe other hand, detailed depictions of intended behavior for specific piecesof software or hardware. the analyses enable developers to derive andcheck whether specific properties are implied by the formal descriptions.a system developer, for example, might employ a formal method tocheck whether a description of requirements is sensible (i.e., not contradictory, unambiguous, and complete) or simply implies some specificproperty of interest, like (for the hmo system example) “at any time, atmost one surgery is scheduled for a given operating theater.” or, for aprogram text or a more abstract description of an algorithm (viz., anydetailed description of behaviors), a formal method could be used toestablish that some general condition on execution holds, like “variablesand arguments declared with type integer are only assigned values thatare integers,” or that some specific characterization of behavior is entailed,like“messages sent using the network are delivered uncorrupted and arenot reordered.”formal methods attempt to extend the capabilities of developers byeliminating the need for exhaustive case analyses and/or by facilitating18the emphasis of “red teaming,”“vulnerability assessment,” and “penetration testing”is to focus on selected areas in which intuition, experience, or other evidence indicates thatproblems may arise (weissman, 1995). this contrasts with technical reviews as discussedin fagan (1986), which seek to examine all logical paths in a component.trust in cyberspacecopyright national academy of sciences. all rights reserved.96trust in cyberspacethe construction of long and intricate arguments, so that some property ofinterest can be certified for a given (formal) description. they are mosteffective when the property of interest is subtle but can be rigorouslydefined and when either the description of the object being analyzed isrelatively small or the formal method being used supports analyses thatcan be automated.formal methods, however, are useful only when the developer canpose the right questions. for example, establishing that a system implements multilevel security using mandatory access control, whether byformal methods or any other means, does not imply the absence of security vulnerabilities in that system, nor does it imply that the resultingsystem is capable of performing useful computation. moreover, someproperties (e.g., “the absence of security vulnerabilities”) have no systemindependent formalization and, therefore, are not amenable to directanalysis using formal methods.19growth in costeffective desktop computing power continues to movethe field of formal methods toward computeraided and fully mechanized formal methods from more manual ones. a second significantforce has been the need to build confidence when programming everricher system behaviors (involving time, other physical processes, faulttolerance, security) as well as when using complex programming constructs (for parallel and distributed systems, object orientation, and soon).early work in formal methods emphasized logics and theorem proving. a practitioner constructed proofs largely by hand, with automatedassistance limited to proof checking and the synthesis of lowlevel inferences. the inability to construct a proof could signify a flaw in the implementation being analyzed, but it could equally well reflect insufficientcreativity by the person attempting the proof. more recently, with modelchecking, raw computing cycles have replaced the manual constructionof proofs. model checking always terminates, reporting that the implementation satisfies the given specification or giving a scenario that showsinconsistency of the implementation with the specification. inherentlylimited to systems having finitesized state spaces, today it is possible toapply model checking to systems having upwards of 200 state variablesand 10120 states (making the approach powerful enough for industrial usein hardware design); ongoing research into abstraction techniques continues to push the limits ever higher.19for any given system, there will exist properties that together imply “the absence ofsecurity vulnerabilities.” but careful thought by a system developer is required to identifythese constituents, and there is no formal way to ever establish that the system developerhas listed them all.trust in cyberspacecopyright national academy of sciences. all rights reserved.software for networked information systems97formal methods are being used increasingly in commercial and industrial settings.20 hardware efforts have provided the most visiblesuccesses so far, perhaps because specifications for hardware tend to berelatively stable, the specifications are short relative to the size of implementations, there is agreement on the choice of languages for writingspecifications, and the cost of design flaws in chips is very high. examples of successes include the following:•intel has used formal methods in the development of its p5 processor (pentium processor) and p6 processor (pentium pro processor) toprove that the hardware implements the required functionality specifiedat the register transfer level and to prove that hardware realizations ofsome of the more complex protocols correctly implement higherlevelspecifications.21•a tool called verity has been used widely within ibm for designing processors, including the powerpc and system/390 (kuehlmann etal., 1995).•model checkers have enabled bugs to be found in the ieee futurebus+ standard (clarke et al., 1993) and the ieee scalable coherent interface (sci) standard (dill et al., 1992).•the acl2 theorem prover was used to find bugs in the floatingpoint squareroot microcode for the amd5k86 processor as well as tofind pipeline hazards in the motorola complex arithmetic processor(cap), a digital signal processor intended for use in a secure multimodejointservice programmable radio (brock et al., 1996).•the microarchitecture and fragments of the microcode for thecollins aamp5 (srivas and miller, 1995) and aampfv avionics processors were analyzed using sri’s pvs theorem prover (owre et al., 1995).commercial and industrial software efforts have also benefited fromformal methods. formal methods applied to requirements analysis hasled to some of the more visible of these industrial successes. by formulating requirements in a language having unambiguous semantics, developers can better understand requirements and can use automated tools todiscover ambiguity, inconsistency, and incompleteness. the entire set ofrequirements need not be formalized to enjoy the benefits—often, themost costeffective course is to treat a carefully chosen subset (with onlythose elements of concern present). the intricate or novel aspects of the20see clarke and wing (1996), dill and rushby (1996), rushby (1995), and craigen et al.(1993) or its summary (craigen et al., 1995) for the many more examples and details thancan be given here.21based on a telephone interview with gadi singer, general manager of design technology, intel corporation, on june 8, 1998.trust in cyberspacecopyright national academy of sciences. all rights reserved.98trust in cyberspacerequirements are thereby checked without formalizing an entire set ofrequirements, which, as observed above in the section on systemlevelrequirements, is likely to be neither complete nor stable. some of thebetterknown successful industrial uses of formal methods for analyzingrequirements include these:22•with the software cost reduction (scr) program’s tool suite, engineers at rockwell were able to detect 24 errors—many of them significant—in the requirements specification for a commercial flight guidancesystem (miller, 1998). also using the scr tool suite, lockheed engineersformalized the operational flight program for the c130j hercules aircraftand found six errors in nondeterminism and numerous type errors.23•an informal english specification for the widely deployed aircraftcollision avoidance system tcas ii was abandoned for a formal versionwritten in requirements state machine language (rsml) after the englishspecification was deemed too complex and unwieldy. that formal specification has since been mechanically checked for completeness and consistency (heimdahl and leveson, 1996).formal methods were originally developed as an alternative to exhaustive testing for increasing one’s confidence that a piece of softwaresatisfies a detailed behavioral specification. to date, this use for formalmethods has been applied outside the laboratory only for relatively smallsafetycritical or highconsequence computing systems, for which development cost is not really a concern but flaws are. examples include theverification of safetycritical software used in the hercules cl30 aircraft(croxford and sutton, 1995), parts of the nextgeneration commandandcontrol ground system for the ariane rocket launcher (devauchelle etal., 1997), and highly secure operating systems (saydjari et al., 1989).constructing extremely large proofs is infeasible today and for theforeseeable future, so formal methods requiring the construction of proofsfor an entire system are not practical when developing an nis having tensto hundreds of millions of lines of code. even if size were not an issue,cots components are rarely accompanied by the formal specificationsnecessary for doing formal verification of an nis built from cots components. it would be wrong, however, to conclude that formal verificationcannot contribute to the construction of an nis.22in addition to clarke and wing (1996) and craigen et al. (1993), further examples of thisuse of formal methods appear in easterbrok et al. (1998).23as connie heitmeyer, u.s. naval research laboratory, described at the nrc’s information systems trustworthiness committee workshop, irvine, ca, february 56, 1997.trust in cyberspacecopyright national academy of sciences. all rights reserved.software for networked information systems99for one thing, critical components of an nis can be subject to formalverification, thereby reducing the number of flaws having systemdisabling impact.24 the aircraft handoff protocol (marzullo et al., 1994) inthe advanced automation systems airtraffic control system built by ibmfederal systems division illustrates such an application of formal methods. second, entire (large) systems can be subject to formal verification ofproperties that are checkable mechanically. this is the impetus for recentinterest by the software engineering community in socalled lightweightformal methods, like the lclint tool, which is able to check c programsfor a variety of variable type and use errors (detlefs, 1996), and eraser, atool for detecting data races in lockbased multithreaded programs (savage et al., 1997).size problems can be circumvented by subjecting a model of the nisto analysis instead of analyzing the entire nis. the model might besmaller than the original in some key dimension, as when confidence isbuilt in a memory cachecontroller by analyzing a version that handlesonly a small number of cachelines. alternatively, a model might besmaller than the original by virtue of the details it ignores—checking ahighlevel description of an algorithm or architecture rather than checking its implementation in a real programming language. illustrative ofthis latter approach are the various logics and tools for checking highlevel descriptions of cryptographic protocols (burrows et al., 1990; loweand roscoe, 1997; meadows, 1992). for instance, with a logic of authentication (burrows et al., 1990), successive drafts of the ccitt x.509 standard were analyzed and bugs were found, including a vulnerability toreplay attacks even when keys have not been compromised.observe that a great deal of benefit can be derived from formal methods without committing a project to the use of formal notations either forbaseline specifications or throughout. some argue that formal methodsanalyses are more effective when performed later, to shake out those lastfew bugs, rather than earlier, when less costly techniques can still bearfruit.a welldocumented example of industrial use of formal methods inbuilding an nis was the development by praxis of the ccf display information system (cdis) component of the central control function (ccf) airtraffic management subsystem in the united kingdom (hall, 1996).25 here,various formal methods were used at different stages of the development24at least for those properties that can be described formally.25this system involved 100 processors linked by using dual local area networks andconsisted of approximately 197,000 lines of c code (excluding comments), a specificationdocument of approximately 1,200 pages, and a design document of approximately 3,000pages.trust in cyberspacecopyright national academy of sciences. all rights reserved.100trust in cyberspaceprocess: vdm (jones, 1986) was used during requirements analysis, vvsl(middleburg, 1989) was used for writing a formal specification for the system, and csp (hoare, 1985) was used for describing concurrency in cdisand its environment. with automated assistance, proofs of correctnesswere constructed for a few critical protocols. and hall (1996) reports thatproductivity for the project was the same or better than has been measuredon comparable projects that used only informal methods. moreover, thedefect rate for the delivered software was between two and ten times betterthan has been reported for comparable software in air traffic control applications that did not use formal methods.beyond the successful industrial uses of formal methods discussedabove and in the work cited, there are other indications that formalmethods have come of age. today, companies are marketing formalverification tools for use in hardware design and synthesis.26 andthere are anecdotal reports that the number of doctoral graduates inmechanized formal methods is now insufficient to fill the current demands of industry.27although once there was a belief that the deployment of formal methods required educating the entire development team, most actual deployments have simply augmented a development team with formal methodsexperts. the job of these experts was beautifully characterized by j s.moore:28like a police swat team, members are trained in the use of “specialweapons,” in particular, mathematical analysis tools. but they are alsoextremely good at listening, reading between the lines, filling in gaps,generalizing, expressing precisely the ideas of other people, explainingformalisms, etc. their role is not to bully or take credit, but to formalizea computing system at an appropriate level of abstraction so that certainbehaviors can be analyzed.here, the absence of shared application assumptions with the development team actually benefits the formal methods expert by facilitating thediscovery of unstated assumptions.formal methods are gaining acceptance and producing results forindustry. what are the impediments to getting broader use and even26examples include formal check from lucent technologies, rulebase from ibm corporation, vformal from compass, and checkoff from view logic.27as john rushby described at the nrc’s information systems trustworthiness committee workshop, irvine, ca, february 56, 1997.28position statement on the state of formal methods technology submitted for thecommittee’s workshop held on february 56, 1997, in irvine, ca. moore credits carl pixleyof motorola with the swatteam simile.trust in cyberspacecopyright national academy of sciences. all rights reserved.software for networked information systems101further leverage from formal methods? with minor exceptions (taylor,1989), the formal methods and testing communities have worked independently of each other, to the advantage of neither. also, the need forbetterintegrated tools has been articulated by researchers and formalmethods practitioners alike (craigen et al., 1993), and research efforts arenow being directed toward combining, for example, model checkers andproof checkers. another trend is the development of editors and librarysupport for managing larger proofs and for facilitating development ofreusable models and theories.over the last decade, formal methods researchers survived only bydevoting a significant fraction of their effort to performing realistic demonstration exercises (and these have helped to move formal methods fromthe research laboratory into industrial settings). morefundamental research should be a priority. significant classes of properties remain difficult or impossible to analyze, with faulttolerance and security high onthe list. methods for decomposing a global property into local ones (whichcould then be checked more easily) would provide a basis for attackinglimitations that bar some uses of formal methods today.finally, there is a growing collection of pragmatic questions about theuse of formal methods. a key to building usable models of niss is knowing what dimensions can be safely ignored. answering that question willrequire a better understanding about the role of approximation and ofsimplifying assumptions in formal reasoning. frictionless planes haveserved mechanical engineers well—what are the analogous abstractionsfor computing systems in general and niss in particular? idealized models of arithmetic, for example, can give misleading results about realcomputations, which have access only to finiteprecision fixed or floatingpoint arithmetic. and any assumption that might be invalidated constitutes a system vulnerability, so analysis predicated on assumptions willbe blind to certain system vulnerabilities.there are also questions about the application of formal methods:where can they give the greatest leverage during system development?when does adding details to a model become an exercise in diminishingreturns, given that most errors in requirements and specification are errors of omission (and therefore are likely to be caught only as details areadded)? and—a question that is intimately linked to the problem ofidentifying and characterizing threats—how does one gain confidencethat a formal specification is accurate?testingtesting is a highly visible process; it provides confidence that a system will operate correctly, because the system is seen to be operatingtrust in cyberspacecopyright national academy of sciences. all rights reserved.102trust in cyberspacecorrectly during testing. and industry today relies heavily on testing.unfortunately, most real systems have inputs that can take on large numbers of possible values. testing all combinations of the input values isimpossible. (this is especially problematic for systems employing graphical user interfaces, where the number of possible pointandclick combinations is unworkably large.) so, in practice, only a subset of all possibletest cases is checked, and testing rarely yields any quantifiable information about the trustworthiness of a program. the characteristics of networked information systems—geographic distribution of inputs and outputs, uncontrollable and unmonitorable subsystems (e.g., networks andlegacy systems), and large numbers of inputs—make this class of systemespecially sensitive to the inadequacy of testing only subsets of the inputspace.much of the research in testing has been directed at dealing withproblems of scale. the goal has been to maximize the knowledge gainedabout a component or subsystem while minimizing the number of testcases required. approaches based on statistical sampling of the inputspace have been shown to be infeasible if the goal is to demonstrate ultrahigh levels of dependability (butler and finelli, 1993), and approachesbased on coverage measures do not provide quantification of usefulmetrics such as mean time to failure. the result is that, in industry,testing is all too often defined to be complete when budget limits arereached, arbitrary milestones are passed, or defect detection rates dropbelow some threshold. there is clearly room for research—especially todeal with the new complications that niss bring to the problem: uncontrollable and unobservable subsystems.system evolutionsoftware systems typically are modified after their initial deploymentto correct defects, to permit the use of new hardware, and to provide newservices. accommodating such evolution is difficult. unless great care istaken, the changes can cause the system structure to degenerate. that, inturn, can lead to new defects being introduced with each subsequentchange, since a poorly structured system is both difficult to understandand difficult to modify. in addition, coping with system evolution requires managing the operational transition to new versions of that system. system upgrade, as this is called, frequently leads to unexpecteddifficulties, despite extensive testing of the new version before the upgrade. in some cases, withdrawal of the new system once it has beenintroduced is a formidable problem, because data formats and file contrust in cyberspacecopyright national academy of sciences. all rights reserved.software for networked information systems103tents have already changed. the popular press is full of incidents inwhich system failures are attributed to system upgrades gone awry.new facilities can be added to an nis, and especially a webbasednis, with deceptive ease: a new server that provides the desired service isconnected to the network. however, such action can affect performanceand reliability. the dispersed nature of an nis user community can makeit difficult to gauge the impact of new features. and the lack of qualityofservice controls can make one nis a hostage to changes in the load orfeatures in another.another potential area of difficulty for nis evolution is having criticalcots components change or be rendered obsolete. the advent of socalled“push” technology, in which commercial offtheshelf software issilently and automatically updated when the user visits the vendor’s website, can cause cots components to drift away from the configurationthat existed during test and acceptance; the situation leads to obscure anddifficulttolocate errors.findings1.very little is known about the integration of subsystems into annis. yet methods for network integration are critical for building an nis.niss pose new challenges for integration because of their distributed nature and the variability of network behavior.2.even though technical reviews are generally considered by thepractitioner community to be effective, the utility of technical reviews forestablishing trustworthiness properties is not well documented.3.formal methods are most effective when the property of interest issubtle but can be rigorously defined, and when either the description ofthe object being analyzed is relatively small or the formal method beingused supports analyses that can be automated.4.formal methods are moving from more manual methods towardcomputeraided and fully mechanized approaches.5.formal methods are being used with success in commercial andindustrial settings for hardware development and requirements analysisand with some success for software development.6.formal methods should be regarded as but one piece of technologyfor eliminating design errors in hardware and software. formal methodsare particularly well suited for identifying errors that become apparentonly in scenarios not likely to be tested or testable.7.fundamental research problems in formal methods should not beneglected in favor of demonstration exercises. research progress in coretrust in cyberspacecopyright national academy of sciences. all rights reserved.104trust in cyberspaceareas will provide a basis for making significant advances in the capabilities of the technology.8.although the large size of an nis and the use of cots limit the useof formal methods for analyzing the entire system, formal verification canstill contribute to the development process.9.testing subsets of a system does not adequately establish confidence in an nis given its distributed nature and uncontrollable and unobservable subsystems.10.research in testing that addresses issues of scale and concurrencyis needed.11.postdeployment modification of software can have a significantnegative impact on nis trustworthiness and security.12.research directed at better integration of testing and formal methods is likely to have payoffs for increasing assurance in trustworthy niss.referencesabadi, martin, and leslie lamport. 1993. “composing specifications,”acm transactionson programming languages and systems, 15(1):73132.boehm, b. 1981. software engineering economics. englewood cliffs, nj: prenticehallinternational.boehm, b. 1988. “a spiral model of software development and enhancement,”ieeecomputer, 21(5):6172.boehm, b., and t. demarco. 1997. “software risk management,”ieee software, 14(3):1719.bollinger, terry, and clement mcgowan. 1991. “a critical look at software capabilityevaluations,”ieee software, 8(4):2541.brock, bishop, matt koffman, and j strother moore. 1996. “acl2 theorems about commercial microprocessors,” pp. 275293 in proceedings of formal methods in computeraided design. berlin: springerverlag.brodman, judith g., and donna l. johnson. 1996. “return on investment from softwareprocess improvement as measured by u.s. industry,”crosstalk: the journal of defensesoftware engineering, 9(4). reprint available online at <http://www.stsc.hill.af.mil/crosstalk/1996/apr/>.brooks, frederick p., jr. 1975. the mythical manmonth. essays on software engineering.reading, ma: addisonwesley.brown, nat, and charlie kindel. 1998. distributed component object model protocol—dcom/1.0. microsoft corporation, january. available online at <http://www.microsoft.com/oledev/olecom/draftbrowndcomv1spec02.txt>.burrows, michael, martin abadi, and roger needham. 1990.“a logic of authentication,”acm transactions on computer systems, 8(1):1836.butler, r., and g. finelli. 1993. “the infeasibility of quantifying the reliability of lifecritical realtime software,”ieee transactions on software engineering, 19(1):312.clarke, edmund m., o. grumberg, h.s. jha, d.e. long, k.l. mcmillan, and l.a. ness.1993.“verification of the futurebus+ cache coherence protocol,”transactions a(computer science and technology), a32:1530.clarke, edmund m., and jeanette m. wing. 1996. “formal methods: state of the art andfuture directions,”acm computing surveys, 28(4):626643.trust in cyberspacecopyright national academy of sciences. all rights reserved.software for networked information systems105clingen, c.t., and t.h. van vleck. 1978. “the multics system programming process,”pp.278280 in proceedings of the 3rd international conference on software engineering. newyork: ieee press.computer science and telecommunications board (cstb), national research council.1997. ada and beyond: software policies for the department of defense. washington, dc:national academy press.constantine, l.l., and e. yourdon. 1979. structured design. englewood cliffs, nj: prenticehall.craigen, dan, susan gerhart, and ted ralston. 1993. an international survey of industrialapplications of formal methods. gaithersburg, md: national institute of standards andtechnology, computer systems laboratory, march.craigen, dan, susan gerhart, and ted ralston. 1995. “formal methods reality check:industrial usage,”ieee transactions on software engineering, 21(2):9098.croxford, m., and j. sutton. 1995. “breaking through the v and v bottleneck,” pp. 334354inproceedings of ada in europe, in frankfurt/main, germany. new york: springer.curtis, bill. 1981. “substantiating programmer variability,”proceedings of the ieee,69(7):846.deremer, f., and h.h. kron. 1976. “programminginthelarge versus programminginthesmall,”ieee transactions on software engineering, 2(3):8086.detlefs, d. 1996. “an overview of the extended static checking system,” pp. 19 inproceedings of the first workshop on formal methods in software practice. new york:acm press.devauchelle, l., p.g. larsen, and h. voss. 1997. “picgal: lessons learnt from a practical use of formal specification to develop a highreliability software,”european spaceagency sp 199,409:159164.diaz, michael, and joseph sligo. 1997. “how software process improvement helpedmotorola,”ieee software, 14(5):7581.digital equipment corporation. 1997. workshop on security and languages. palo alto, ca:digital equipment corporation, systems research center, october 3031. available online at<http://www.research.digital.com/src/personal/martinabadi/sal/home.html>.dill, david l., a.j. drexler, a.j. hu, and c.h. yang. 1992. “protocol verification as ahardware design aid,” pp. 522525 in proceedings of the ieee international conferenceon computer design: vlsi in computers and processors. los alamitos, ca: ieee computer society press.dill, david l., and john rushby. 1996. “acceptance of formal methods: lessons fromhardware design,”ieee computer, 29(4):1630.dion, raymond. 1993. “process improvement and the corporate balance sheet,”ieeesoftware, 10(4):2835.easterbrok, steve, robyn lutz, richard covington, john kelly, and yoko ampo. 1998.“experiences using lightweight formal methods for requirements modeling,”ieeetransactions on software engineering, 24(7):413.fagan, m.e. 1986.“advances in software inspections,”ieee transactions on software engineering, 12(7):744751.fayad, mohamed, and mauri laitinen. 1997. “process assessment considered harmful,”communications of the acm, 40(11):125128.garland, david, and mary shaw. 1996. software architecture: perspectives on an emergingdiscipline. englewood cliffs, n.j.: prenticehall.glass, r.l. 1981. “persistent software errors,”ieee transactions on software engineering,7(2):162168.hall, anthony. 1996. “using formal methods to develop an atc information system,”ieee software, 13(6):6676.trust in cyberspacecopyright national academy of sciences. all rights reserved.106trust in cyberspacehamilton, graham, ed. 1997. javabeans. palo alto, ca: sun microsystems.heimdahl, m., and nancy g. leveson. 1996. “completeness and consistency in hierarchical statebased requirements,”ieee transactions on software engineering, 22(6):363377.heninger, k. 1980. “specifying software requirements for complex systems: new techniques and their application,”ieee transactions on software engineering, 6(1):213.herbsleb, james, and dennis goldenson. 1996. “a systematic survey of cmm experienceand results,” pp. 323330 in proceedings of the 18th international conference on softwareengineering (icse).los alamitos, ca: ieee computer society press.hernandez, j.a. 1997. the sap r/3 handbook. new york: mcgrawhill.hoare, c.a.r. 1985. communicating sequential processes. englewood cliffs, nj: prenticehall.honeywell corporation. 1975. aerospace and defense group software program, final report.waltham, ma: honeywell corporation, systems and research center.jones, c.b. 1986. systematic software development using vdm. englewood cliffs, nj:prenticehall.kitson, david, and stephen masters. 1993. “an analysis of sei software process assessment results: 19871991,” pp. 6877 in proceedings of the 15th international conference onsoftware engineering (icse15). los alamitos, ca: ieee computer society press.kuehlmann, a., a. srinivasan, and d.p. lapotin. 1995. “verity—a formal verificationprogram for custom cmos circuits,”ibm journal of research and development,39(1/2):149165.lampson, butler w. 1983. “hints for computer system design,”operating systems review,17(5):3348.lawlis, patricia k., robert m. flowe, and james b. thordahl. 1995. “a correlational studyof the cmm and software development performance,”crosstalk: the journal of defensesoftware engineering, 8(9):2125.leveson, nancy. 1995. safeware. reading, ma: addisonwesley.leveson, nancy g. 1987. software safety. pittsburgh, pa: carnegie mellon university,software engineering institute, july.lowe, gavin, and bill roscoe. 1997. “using csp to detect errors in the tmn protocol,”ieee transactions on software engineering, 23(10):659669.manes, stephen. 1998. “settlement near in technical helpline suit,”new york times,march 3, p. f2.marzullo, k., fred b. schneider, and j. dehn. 1994. “refinement for fault tolerance: anaircraft handoff protocol,” pp. 3954 in foundations of ultradependable parallel anddistributed computing, paradigms for dependable applications. amsterdam, the netherlands: kluwer.mcgarry, frank, steve burke, and bill decker. 1997. “measuring impacts of softwareprocess maturity in a production environment,”proceedings of the 21st goddard software engineering laboratory software engineering workshop. greenbelt, md: goddardspace flight center.mclean, john. 1994. “a general theory of composition for trace sets closed underselective interleaving functions,” pp. 7993 in proceedings of the ieee computer societysymposium on research in security and privacy. los alamitos, ca: ieee computersociety press.meadows, catherine. 1992. “applying formal methods to the analysis of a key management protocol,”journal of computer security, 1(1):536.meyer, bertrand. 1988.objectoriented software construction. englewood cliffs, nj: prenticehall.trust in cyberspacecopyright national academy of sciences. all rights reserved.software for networked information systems107microsoft corporation and digital equipment corporation. 1995. the component object modelspecification (com). microsoft corporation and digital equipment corporation, october. available online at <http://www.microsoft.com/oledev/olecom/title.htm>.middleburg, c.a. 1989. “vvsl: a language for structured vdm specifications,”formalaspects of computing, 1(1):115135.miller, steven p. 1998. “specifying the mode logic of a flight guidance system in coreand scr,” pp. 4453 in proceedings of the 2nd workshop on formal methods in softwarepractice. new york: acm press.musser, david r., and atul saini. 1996. stl tutorial and reference guide: c++ programmingwith the standard template library. reading, ma: addisonwesley.neumann, peter g. 1995. computer related risks. new york: acm press.ousterhout, john k. 1998. “scripting: higherlevel programming for the 21st century,”ieee computer, 31(3):2330.owre, sam, john rushby, natarajan shankar, and frederich von henke. 1995. “formalverification for faulttolerant architectures: prolegomena to the design of pvs,”ieeetransactions on software engineering, 21(2):107125.parnas, d.l. 1974. “on a ‘buzzword’: hierarchical structure,” pp. 335342 in programmingmethodology. a collection of articles by members of the ifip congress, d. gries, ed. berlin: springerverlag.paulk, mark c., bill curtis, mary beth chrissis, and charles v. weber. 1993. “capabilitymaturity model for software version 1.1,”ieee software, 10(4):1827.pfleeger, s.l., n. fenton, and s. page. 1994. “evaluating software engineering standards,”ieee computer, 27(9):7179.porter, a.a., h.p. siy, c.o. toman, and l.g. votta. 1997. “an experiment to assess thecostbenefits of code inspections in large scale software development,”ieee transactions on software engineering, 23(6):329346.potts, colin, kenji takahashi, and annie i. anton. 1994. “inquirybased requirementsanalysis,”ieee software, 11(2):2132.pressman, roger s. 1986. software engineering: a practitioner’s approach. new york:mcgrawhill.raymond, eric, and guy l. steele. 1991. the new hacker’s dictionary. cambridge, ma:mit press.rochkind, marc j. 1975.“the source code control system,”ieee transactions on softwareengineering, 1(4):364370.ross, douglas t. 1977.“guest editorial—reflections on requirements,”ieee transactionson software engineering, 3(1):25.rushby, j. 1995. formal methods and their role in certification of critical systems. menlopark, ca: sri international, march.savage, stefan, michael burrows, greg nelson, patrick sobalvarro, and thomas e. anderson. 1997. “eraser: a dynamic data race detector for multithreaded programs,”operating systems review, 31(5):2737.saydjari, o. sami, j.m. beckman, and j.r. leaman. 1989. “lock trek: navigating uncharted space,” pp. 167175 in proceedings of the ieee symposium on security and privacy. los alamitos, ca: ieee computer society press.sommerville, ian. 1996. software engineering. 5th ed. reading, ma: addisonwesley.srivas, mandayam k., and steven p. miller. 1995. “formal verification of the aamp5microprocessor,”applications of formal methods, michael g. hinchey and jonathan p.bowden, eds. englewood cliffs, nj: prenticehall.tanik, murat m., raymond t. ye, and guest editors. 1989. “rapid prototyping in softwaredevelopment,”ieee computer magazine, vol. 22, special issue (5).trust in cyberspacecopyright national academy of sciences. all rights reserved.108trust in cyberspacetaylor, t. 1989. “ftlsbased security testing for lock,”proceedings of the 12th nationalcomputer security conference. washington, dc: u.s. government printing office.thompson, kenneth. 1984. “reflections on trusting trust,”communications of the acm,27(8):761763.watts, humphrey, and bill curtis. 1991. “comment on ‘a critical look,’”ieee software,8(4):4246.watts, humphrey, terry synder, and ronald willis. 1991. “software process improvement at hughes aircraft,”ieee software, 8(4):1123.weissman, clark. 1995.“penetration testing,”information security, m.d. abrams, s. jajodia,and h.j. podell, eds. los alamitos, ca: ieee computer society press.trust in cyberspacecopyright national academy of sciences. all rights reserved.109introductionincreasing the immunity of a networked information system (nis) tohostile attacks is a broad concern, encompassing authentication, accesscontrol, integrity, confidentiality, and availability. any solution will almost certainly be based on a combination of system mechanisms in addition to physical and personnel controls.1 the focus of this chapter is thesesystem mechanisms—in particular, what exists, what works, and what isneeded. in addition, an examination of the largely disappointing resultsfrom more than two decades of work based on what might be called the“theory of security” invites a new approach to viewing security for niss—one based on a “theory of insecurity”—and that, too, is discussed.4reinventing security1personnel security is intrinsic in any nis, since some set of individuals must be trustedto some extent with regard to their authorized interactions with the system. for example,people manage system operation, configure external system interfaces, and ultimately initiate authentication of (other) users of a system. in a similar vein, some amount of physicalsecurity is required for all systems, to thwart theft or destruction of data or equipment. thephysical and personnel security controls imposed on a system are usually a function of theenvironment in which the system operates. individuals who have access to systems processing classified information typically undergo extensive background investigations andmay even require a polygraph examination. in contrast, most employers perform must lessstringent screening for their information technology staff. similarly, the level of physicalsecurity afforded to the niss that support stock markets like the nyse and amex is greaterthat that of a typical commercial system. although physical and personnel controls areessential elements of system security, they are largely outside the scope of this study.trust in cyberspacecopyright national academy of sciences. all rights reserved.110trust in cyberspacethe choice of system security mechanisms employed in building annis should, in theory, be a function of the environment, taking into account the security requirements and the perceived threat. in practice,niss are constructed with commercial offtheshelf (cots) components.what security mechanisms are available is thus dictated by the builders ofthese cots components. moreover, because most cots components areintended for constructing a range of systems, their security mechanismsusually are not tailored to specific needs. instead, they reflect perceptionsby a productmarketing organization about the requirements of a fairlybroad market segment.2the task faced by the nis security architect,then, is determining (1) how best to make use of the given generic securitymechanisms and (2) how to augment those mechanisms to achieve anacceptable level of security. the nis security architect’s task is all themore difficult because cots products embody vulnerabilities, but few ofthe products are subjected by their builders to forms of analysis thatmight reveal these vulnerabilities. thus, the nis security architect willgenerally be unaware of the residual vulnerabilities lurking in a system’scomponents.this chapter’s focus on security technology should not be misconstrued—an overwhelming majority of security vulnerabilities are causedby“buggy” code. at least a third of the computer emergency responseteam (cert) advisories since 1997, for example, concern inadequatelychecked input leading to character string overflows (a problem peculiarto c programming language handling of character strings). moreover,less than 15 percent of all cert advisories described problems that couldhave been fixed or avoided by proper use of cryptography. avoidingdesign and implementation errors in software (the subject of chapter 3) isan essential part of the security landscape.evolution of security needs and mechanismsin early computing systems, physical controls were an effective meansof protecting data and software from unauthorized access, because thesesystems were physically isolated and singleuser. the advent of multiprogramming and timesharing invited sharing of programs and dataamong an often closed community of users. it also created a need formechanisms to control this sharing and to prevent actions by one user2some cots products do allow a system integrator or site administrator to select fromamong several options for security facilities, thereby providing some opportunity forcustomization. for example, one may be able to choose between the use of passwords,challengeresponse technology, or kerberos for authentication. but the fact remains thatcots components limit the mechanisms available to the security architect.trust in cyberspacecopyright national academy of sciences. all rights reserved.reinventing security111from interfering with those of another or with the operating system itself.as computers were connected to networks, sharing became even moreimportant and access control problems grew more complex. the move todistributed systems (e.g., clientserver computing and the advent of widespread internet connectivity) exacerbated these problems while providing ready, remote access not only for users but also for attackers fromanywhere in the world. closed user communities are still relevant insome instances, but more flexible sharing among members of very dynamic groups has become common. see box 4.1 for a discussion of threatsfrom within and outside user communities.the evolution of computing and communication capabilities has beenaccompanied by an evolution in security requirements and increased demands on security mechanisms. computing and communication featuresand applications have outpaced the ability to secure them. requirementsfor confidentiality, authentication, integrity, and access control have become more nuanced; the ability to meet these requirements and enforcesuitable security policies has not kept up. the result: successful attacksagainst niss are common, and evidence suggests that many go undetected (u.s. gao, 1996). the increasing use of extensible systems and foreign or mobile code (e.g., java “applets” and activex modules deliveredvia networks) further complicates the task of implementing nis security.of growing concern with regard to controlling critical infrastructuresis denialofservice attacks, which compromise availability. the attackmay target large numbers of users, preventing them from using a networked information system, or may target individuals, destroying theirability to access data, or may target a computing system, preventing itfrom accomplishing an assigned job. only recently have denialofserviceattacks become a focus of serious countermeasure development. clearly,these attacks should be of great concern to nis security architects.access control policiesit is common to describe access controls in terms of the policies thatthey support and to judge the effectiveness of access control mechanismsrelative to their support for those policies. this might leave the impression that access control policies derive from first principles, but that wouldbe only partly true. access control policies merely model in cyberspacenotions of authorization that exist in the physical world. however, incyberspace, programs—acting on behalf of users or acting autonomously—and not the users themselves are what interact with data andaccess other system objects. this can be a source of difficulty since actionsby users are the concern but action by programs is what is governed bythe policy.trust in cyberspacecopyright national academy of sciences. all rights reserved.112trust in cyberspacebox 4.1insiders versus outsidersa debate has raged for some time over whether the major threat to system security arises from attacks by “insiders” or by “outsiders.” insiders have been blamed forcausing 70 to 80 percent of the incidents and most of the damage (lewis, 1998). butindependent of the reliability of this estimate, it is clear that insiders do pose a seriousthreat. two questions then arise: what is the definition of insider? how is damageassessed?there are three plausible definitions for an insider:1.a person with legitimate physical access to computer equipment. thus, ajanitor is an insider, but a burglar or casual visitor is not.2.a person with some sort of organizational status that causes members of theorganization to view requests or demands as being authorized. in this view, a purchasing agent is an insider, but a vendor is not.3.a person with some level of privilege or authority with regard to the computersystem. this characterization is particularly difficult to assess in today’s networkedenvironment. a casual internet user accessing a vendor’s web site and running awellknown attack would, in the eyes of most observers, constitute an “outsider”attack. and if the administrator of the web site used authorizations to make mischief, that would generally be viewed as an “insider” attack. but what of the vendorwho is granted putatively limited privilege on a corporate network and then attemptsto increase that privilege or otherwise gain information about competitors? it isequally unclear whether a traditional spy or saboteur, operating from the inside atthe behest of an outside organization, is an “insider,” an “outsider,” or yet a thirdclass of entity.assessing damage from attacks is equally problematic. overestimation of damage is rife when prosecution or insurance claims are involved. perhaps the mostegregious case of overestimation occurred in connection with the socalled “knightlightning” case. a prosecutor claimed that a particular item of intellectual propertywas worth $70,000, but closer examination showed that copies were sold by itsowners for $30.00 and that the information in the document was made available,again by its owners, in other forms for free (sterling, 1992). on the other hand,damage is (it is rumored) allegedly underreported in the financial community toavoid loss of customer confidence. only recently have commercial institutions begun to come forward, albeit under the cloak of anonymity (war room research llc,1996).arguably, the nature of the reporting process inflates the relative numbers ofinsider incidents, as they are often easier to discover and report. sophisticated outsider attacks leave minimal traces and force those suspecting an attack to go to greatlengths to convince authorities that one is under way (stoll, 1989). furthermore,insider attacks, when discovered, tend to be prosecuted more energetically and togain more publicity than other forms of whitecollar crime (schwartz, 1997). variousestimates add to the confusion. the federal bureau of investigation estimated totaltrust in cyberspacecopyright national academy of sciences. all rights reserved.reinventing security113damages to the u.s. economy from computer crime to be on the order of $300billion. yet reported damages totaled “only” $100 million (war room research llc,1996). if the otherwise unverified estimate of 70 percent insider damage is accurate,then the possible range of damages is $70 million to $210 billion. a more accurateestimate will not be possible until comprehensive reporting mechanisms are in placeand are used.most would classify as insiders embezzlers and disgruntled employees operatingalone or as part of a conspiracy who mount frauds and destroy data. but the insiderswho can cause the most damage are the administrators of the network and its attached computers. they typically have both the knowledge and the authority toalter, copy, or destroy data, cover their tracks by modifying audit logs, and thenmodify audits and other information to direct suspicion at other individuals.organizations today tend to array their defenses around the perimeter of their computing network and rely on deterrence mechanisms, such as audits, to discourageinsider attacks. finegrained access control is absent inside these perimeters becauseit can get in the way of users, especially during emergencies. technical controls on theactions or authorities of administrators are minimal. there is, however, a growingconcern about the inherent limitations of perimeter security (see the section titled “firewalls” in this chapter). as a result, some organizations are turning to internal networkaccess controls as a way of buttressing perimeter security. ironically, this latter accesscontrol technology is more consistent with the traditional meaning of the term“firewall” as imposing unbreachable partitions in a structure.intrusiondetection systems frequently are advocated for combating the insiderthreat, as well as for detecting outsider attacks that have successfully breached perimeter defenses. these systems collect data on computer and network usage, applypattern matching or heuristics, and trigger alarms if they detect what appears to be apattern of improper activity.1 when directed toward insiders, intrusiondetectionsystems have proved deficient. the amount of data that must be collected imposesa performance penalty and, in many cases, raises concerns about improper workplace surveillance. although the assumption underlying most heuristics for recognizing improper activity is that users exhibit fairly constant patterns of behavior, thisassumption is generally invalidated, for example, during emergencies, the very timewhen a deluge of security alarms is least tolerable. adept users can also subvert aheuristic by making gradual shifts in their behavior, such as slowly increasing thenumber of files accessed each day so that file accesses that once would trigger an“improper browsing” alarm are now treated as normal.the insider threat is a classic example of security as a management problem.technical defenses tend to be expensive, cumbersome, or largely ineffective. themost practical solution is to know the people who have significant authority on thesystem and to work to maintain their loyalty to the organization.1most of these systems look for specific attack “signatures” rather than attempt to detectdeviation from nominal behavior. in this sense, such systems are much like antivirusprograms.trust in cyberspacecopyright national academy of sciences. all rights reserved.114trust in cyberspacethe evolution of access control policies and access control mechanisms has attempted, first, to keep pace with the new modes of resourcesharing supported in each subsequent generation of systems, and, second, to repel a growing list of attacks to which the systems are subjected.the second driving force is easily overlooked, but crucial. access controlscan enforce the principle of least privilege.3 in this fashion, they preventand contain attacks.before suggesting directions for the future, it is instructive to examinethe two basic types of access control policies that have dominated computer security work for over two and a half decades: discretionary accesscontrol and mandatory access control.discretionary access control policies allow subjects, which model users or processes, to specify for objects what operations other subjects arepermitted to perform. most of the access control mechanisms implemented and deployed enforce discretionary access control policies. individual users or groups of users (or computers) are identified with subjects; computers, networks, files or processes, are associated with objects.for example, read and write permissions might be associated with filesystem objects (i.e., files); some subjects (i.e., users) might have read accessto a given file while other subjects do not. discretionary access controlwould seem to mimic physicalworld policies of authorization, but thereare subtleties. for instance, transitive sharing of data involving intermediary users or processes can subvert the intent of discretionary accesscontrol policies by allowing a subject to learn the contents of an object(albeit indirectly) even though the policy forbids (direct) access to thatobject by the subject.mandatory access control policies also define permitted accesses toobjects for subjects, but now only security administrators, rather thanindividual users, specify what accesses are permitted.4 mandatory accesscontrol policies typically are formulated for objects that have been labeled, and the policies typically are intended to regulate information flowfrom one object to another. the bestknown example of mandatory accesscontrols arises in connection with controlling the flow of data accordingto military classifications. here, data are assigned classification labels(e.g.,“top secret” and “unclassified”) and subjects are assigned clearances; simple rules dictate the clearance needed by a subject to access datathat have been assigned a given label.3the principle of least privilege holds that programs and users should operate using theleast set of privileges necessary to complete the job.4in fact, there exist policies that are mandatory access control but user processes do havesome control over permissions. one example is a policy in which a user process couldirrevocably shed certain permissions.trust in cyberspacecopyright national academy of sciences. all rights reserved.reinventing security115mandatory access controls can prevent trojan horse attacks; discretionary access controls cannot. a trojan horse is a program that exploitsthe authorization of the user executing a program for another user’s malicious purposes, such as copying information into an area accessible by auser not entitled to access that information. mandatory controls blocksuch attacks by limiting the access of all programs—including the trojanhorse—in a manner that cannot be circumvented by users. discretionaryaccess controls are inherently vulnerable to trojan horse attacks becausesoftware executing on behalf of a user inherits that user’s privilege without restriction (boebert and kain, 1996).shortcomings of formal policy modelsdespite the lion’s share of attention from researchers and actual support in deployed system security mechanisms, many security policies ofpractical interest cannot be formulated as discretionary and mandatoryaccess control policies. discretionary and mandatory access control focuson protecting information from unauthorized access. they cannot modelthe effects of certain malicious or erroneous software, nor do they completely address availability of system resources and services (i.e., protection against denialofservice attacks). and they are defined in an accesscontrol model—defined by the trusted computer system evaluation criteria (u.s. dod, 1985)—that has only limited expressive power, rendering the model unsuitable for talking about certain applicationdependentaccess controls.the access control model defined by the trusted computer systemevaluation criteria, henceforth called the dod access control model, presupposes that an organization’s policies are static and have precise andsuccinct characterizations. this supposition is questionable. organizations’ security policies usually change with perceived organizationalneeds and with perceived threat. even the department of defense’spolicy—the inspiration for the bestknown form of mandatory access control (bell and la padula, 1973)—has numerous exceptions to handle special circumstances (commission on protecting and reducing governmentsecrecy, 1997). for example, senior political or military officials can downgrade classified information for diplomatic or operational reasons. butthe common form of mandatory access control does not allow nonsensitive objects to be derived from sensitive sources, because the dod accesscontrol model does not associate content with objects nor does it (or canany model) formalize when declassifying information is safe.5 policies5this also means that the underlying mathematical model is unable to capture the most basicoperation of cryptography, in which sensitive data become nonsensitive when enciphered.trust in cyberspacecopyright national academy of sciences. all rights reserved.116trust in cyberspaceinvolving applicationspecific information also cannot be handled, sincesuch information is not part of the dod access control model.6at least two policy models that have been proposed do take intoaccount the application involved. the clark/wilson model (clark andwilson, 1987) sets forth rules for maintaining the integrity of data in acommercial environment. it is significant that this model contains elements of the outside world, such as a requirement to check internal data(e.g., inventories) with the physical objects being tabulated. the “chinesewall” model (brewer and nash, 1989) expresses rules for separating different organizational activities for conformance with legal and regulatorystrictures in the financial world.still, from the outset, there has been a gap between organizationalpolicy and the 1970s view of computing embodied by the dod accesscontrol model: users remotely accessing a shared, central facility throughlowfunctionality (“dumb”) terminal equipment. and, as computing technology advanced, the gap has widened. it is significant that, in a glossaryof computer security, brinkley and schell (1995) use a passive database (alibrary) as the example and include the important passage: . . . the mapping between our two ‘worlds’:1. the world independent of computers, of people attempting toaccess information on paper.2. the world of computers, with objects that are repositories forinformation and subjects that act as surrogates for users in the attemptto access information in objects.processes, for example, are complex, ephemeral entities without clearboundaries, especially in the distributed and multithreaded systems oftoday. a modern computing network comprises independent computersthat are loosely linked to each other and to complexes of servers. andmodern programs likely have their own access controls, independent ofwhat is provided by the underlying operating system and the dod accesscontrol model. an access control model that does not capture this aspectof computing systems is fatally flawed.subsystems more and more resemble operating systems, and theyshould be treated as such. to be sure, a subsystem cannot exceed permissions granted to it by an underlying operating system. and even though6it should be noted that a formal access control model of a complex application has beendefined, and the corresponding implementation subjected to extensive assurance activity.the exercise explored many issues in the construction of such models and is worth study.see landwehr et al. (1984) for details.trust in cyberspacecopyright national academy of sciences. all rights reserved.reinventing security117the resources that a subsystem protects are the user’s own, that protectionserves an important function. moreover, even if the access control modeldid capture the policies of subsystems, there still remains the problem ofcomposing those policies with all the other policies that are being enforced. such composition is difficult, especially when policies are in conflict with each other, as all too often is the case.the object abstraction in the dod access control model also can be asource of difficulty. real objects seldom have uniform security levels,despite what is implied by the dod access control model. consider amailbox with multiple messages. each message may have fielddependent security levels (sensitive or nonsensitive message body, sensitive ornonsensitive address list, and so on), and there may be multiple messagesin the mailbox. what is the level of the mailbox as a whole? the alternative is to split messages so that individual fields are in individual objects,but that leads to a formulation that could be expensive to implement withfidelity.the allornothing nature of the dod access control model also detracts from its utility. designers who implement the model are forced toerr on the side of being restrictive, in which case the resulting systemmay be unusable, or to invent escapes, in which case knowing that asystem adheres to the model has limited practical significance. in thebattle between security and usability, usability loses. moreover, sincethe dod access control model does not account for contemporary defensive measures, such as virus scans, approaches to executable contentcontrol, or firewalls, the system architect who is bound by the model hasno incentive to use these technologies. deploying them makes no progress toward establishing that the system is consistent with the modeland, in addition, transforms the model into an incomplete characterization of the system’s defensive measures (thereby again limiting themodel’s practical utility).evidence that dod has recognized some of the problems inherent inbuilding systems that enforce the dod access control model appears inthe new dod goal security architecture (dgsa; see box 4.2). dgsadoes not legislate that only the dod access control model be used; instead it supports a broad set of security policies that go far beyond thetraditional informationflow policies. dgsa also does not discouragedod end users from employing the latest in objectbased, distributedsystems, networks, and so on, while instituting rich access control, integrity, and availability policies. however, dgsa offers no insights abouthow to achieve an appropriate level of assurance that these policies arecorrectly implemented (despite upping the stakes significantly regardingwhat security functionality must be supported). thus it remains to beseen if the dgsa effort will spur significant progress in system security.trust in cyberspacecopyright national academy of sciences. all rights reserved.118trust in cyberspacea new approachone can view the ultimate goal as the building of systems that resistattack. attackers exploit subtle flaws and side effects in security mechanisms and, more typically, exploit interactions between mechanisms. testing can expose such previously hidden aspects of system behavior, but nobox 4.2dod goal security architecturethe dod goal security architecture (dgsa) (disa, 1996) has evolved over thelast decade as a series of architecture documents. most of the principles have remained constant during this evolution.dgsa is oriented toward supporting a range of access controls and integrity policies in an objectoriented, distributedsystem environment. the range of securitypolicies to be supported goes far beyond the bellla padula information flow securitypolicy that has dominated dod security for more than 20 years. multiparty authorization, multilevel objects, originator control of release, rolebased authorization,and variable levels of availability are among the security features offered by dgsa.dgsa embraces commercial offtheshelf (cots) products and commercial network resources. commercial networks can readily be employed through the use of(conventional, highassurance) network security devices. but there is the matter ofachieving availability in excess of what most commercial users seek.1 if commercialnetworks are vulnerable to disruption on a global or targeted basis, then dod communications traversing these networks would be vulnerable to denialofservice attacks.use of cots operating systems and applications raises questions about how tocreate multilevel information objects and how to enforce appropriate informationflow security, as labeling is generally not supported in such commercial offerings.perimeter security devices (e.g., firewalls and guards) are limited in the granularity atwhich they can enforce data separation, especially in the absence of labels.at present, dgsa must be viewed more as a list of goals than as an architecturalspecification. available (cots) technology and even research and developmentprototypes lag far behind what dgsa calls for. most of the goals will require substantial research, and some of the goals may be unattainable relative to credible,nationallevel threats. moreover, dgsa still embodies a notion of “absolute protection” despite the practical impossibility of attaining that. an excellent overview ofdgsa, including a characterization of some of the research and development challenges it poses, is offered by feustel and mayfield (1998).1commercial users with high realtime communication availability concerns do not nowdepend on the internet. for example, u.s. stock exchanges employ redundancy at multiple layers to achieve sufficient availability using commercial communications. see chapter 2 for additional discussions of vulnerabilities in the public telephone network andinternet.trust in cyberspacecopyright national academy of sciences. all rights reserved.reinventing security119amount of testing can demonstrate the absence of all exploitable flaws orside effects.an alternative to finding flaws in a system is to demonstrate directlythat the system is secure by showing the correspondence between thesystem and some model that embodies the security properties of concern.one problem (system security) is thus reduced to another (model security) presumably simpler one. sound in theory, success in this endeavorrequires the following:1.models that formalize the security policies of concern.2.practical methods for demonstrating a correspondence between asystem and a formal model.but the arguments given earlier suggest that suitable formal modelsfor nis security policies, which invariably include stipulations aboutavailability and application semantics, do not today exist and would bedifficult to develop. moreover, establishing a correspondence between asystem and a formal model has proved impractical, even for systems builtspecifically with the construction of that correspondence in mind and forwhich analysts have complete knowledge and access to internals. establishing the correspondence is thus not a very realistic prospect for cotscomponents, which are not built with such verification activities in mindand, generally, do not offer the necessary access to internals.experience has taught that systems—and, in particular, complex systems like niss—can be secure, but only up to a point. there will alwaysbe residual vulnerabilities, always a degree of insecurity. the questionone should ask is not whether a system is secure, but how secure thatsystem is relative to some perceived threat. yet this question is almostnever asked. instead, notions of absolute security, based on correspondence to formal models, have been the concern. perhaps it is time tocontemplate alternatives to the “absolute security” philosophy.consider an alternative view, which might be summarized in three“axioms”:1.insecurity exists.2.insecurity cannot be destroyed.3.insecurity can be moved around.with this view, the object of security engineering would be to identifyinsecurities and move them to less exposed and less vulnerable parts of asystem. military cryptosystems that employ symmetrickey cryptography illustrate the approach. radio transmissions are subject to interception, so they are enciphered. this encryption does not destroy the insecutrust in cyberspacecopyright national academy of sciences. all rights reserved.120trust in cyberspacerity (disclosure of message contents) but rather moves the insecurity tothe cryptographic keys, whose compromise would lead to the disclosureof intercepted transmissions. the keys must be distributed. and theyare, subject to elaborate physical controls and auditing that are impractical for radio transmissions.7 so, the use of encryption moves insecurityfrom one part of the system to another and does so in a manner thatdecreases the overall vulnerability of the system relative to some perceived threats. (in a world where monitoring radio transmissions wasdifficult but kidnapping diplomatic couriers bearing cryptographic keyswas easy, the perceived threats would be different and the encryptionsolution no longer would be appropriate.)vulnerability assessments provide a wellknown way to identify system insecurities. here, attack by an adversary is simulated using a teamwhose technical and other resources are comparable to the actual threat.the team undertakes an unconstrained search for vulnerabilities, examining the system in its context of use and attempting to exploit any aspect ofthe system, its implementation, or its operational context to cause a security breach. a methodical approach to this process is described in weissman (1995).vulnerability assessment has the advantage that all aspects of thesystem are stressed in context. but it does have disadvantages. no overtevidence of security is presented. the approach is potentially quite costly,because assessment must be carried out on a per system basis. andfinally, systematic methods do not yet exist for predicting how vulnerabilities and attacks can propagate in systems. were it possible to analyzevulnerability and attack propagation, designers could begin to think abouta design philosophy based on relocating insecurities, to move them awayfrom threats. the result would be a methodology especially attractive forsecuring niss—an alternative to the “absolute security” philosophy.findings1.existing formal policy models have only limited utility becausethey concern only some of the security properties of interest to nis builders. to the extent that formal models are useful (as descriptive vehiclesand for inferring consequences from policies) further development isneeded to remove the limits of existing policies, both with regard to thesystem model and with regard to what types ofsecurity are captured.2.demonstrating the correspondence between a system and a formalmodel is not a practical approach for gaining assurance that an nis is7although even these precautions do not guarantee security, as the celebrated “walkercase” showed (kneece, 1986).trust in cyberspacecopyright national academy of sciences. all rights reserved.reinventing security121resistant to attack. an alternative to this “absolute security” philosophyis to identify insecurities and make design changes to reposition them inlight of the nature of the threat. further research is needed to determinethe feasibility of this new approach to the problem.3.some practical means for evaluating the security characteristics(both security features and residual vulnerabilities) of cots system components is essential. evaluation must not be so costly or timeconsumingthat vendors will shun it or that evaluated products will be obsolete (relative to their nonevaluated counterparts).identification and authentication mechanismsidentification is an assertion about an entity’s identity. in the simplest case, this assertion could be a claim that the entity makes. authentication refers to the process by which a system establishes that an identification assertion is valid. a number of authentication mechanisms arecommonly used in practice; each has advantages and disadvantages. historically, the mechanisms have been characterized as something youknow, something you have, or something you are. the latter refers toinnate biological properties of the user and therefore is not applicable forcomputertocomputer authentication.8networkbased authenticationnetworkbased authentication relies on the underlying network (andpossibly the host computer) to authenticate the identity of the source ofnetwork traffic. the reliability of the approach is thus closely tied tocharacteristics of the underlying network. for example, chapter 2 discusses the ease with which internet protocol (ip) addresses in the internetand caller id information in the public telephone network (ptn) can beforged, and so using these for authentication would probably be imprudent.when implemented with a moderate degree of assurance, networkbased authentication can be appealing. it relies on a third party—thenetwork provider—rather than burdening end users or servers. the network provider arguably even has a business incentive to provide such aservice and may be able to justify larger investments in the developmentof a highassurance service than any single client of that service could.but positioning an authentication service at the network provider is notconsistent with the principle of least privilege and thus is a questionabledesign choice.8attempts have been made, though, to use “signatures” of analog radio devices.trust in cyberspacecopyright national academy of sciences. all rights reserved.122trust in cyberspacecryptographic authenticationsecure forms of authentication for an nis generally rely on cryptography.9 while many different schemes are used, all involve possession ofa secret by the entity being authenticated. if this secret is compromised,then so is the authentication process.the simplest form of cryptographic authentication is based on animplicit property of encryption: if an entity does not possess the properkey, then encrypted messages sent or received by that entity decrypt intorandom bits. moresophisticated forms employ cryptographic protocols—stylized exchanges between two or more parties—to authenticatecallers and to distribute shortterm cryptographic keys. but the design ofsuch protocols is a subtle business, and flaws have been found in manypublished protocols (abadi and needham, 1994).a major advantage of cryptographic authentication is that it can provide continuous authentication, whereby each packet sent during a session is authenticated. the alternative is to validate the identity of anentity only at the time the authentication process is invoked (typically atthe start of a session), but that alternative is vulnerable to session “hijacking” whereby an attacker impersonates a previously authenticated entity(joncheray, 1995). as the sophistication of attackers increases, the needfor continuous authentication has become more critical.cryptographic authentication can be based on symmetric (conventional) or on asymmetric (publickey) cryptosystems. for deployment inlargescale contexts, both types of cryptosystems typically require the useof a trusted third party to act as an intermediary, and the existence of thisthird party constitutes a potential vulnerability. for symmetric cryptosystems, the third party (e.g., a kerberos keydistribution center is discussed later in this chapter) is usually accessed in real time as part of thekeydistribution process; for asymmetric cryptosystems, interaction withthis third party (e.g., a certification authority) can be offline.cryptographic authentication mechanisms require the possession, andthus storage, of a secret or private key. for a human user, if no auxiliarystorage is available, such as a smart card or other hardware token, thesecret/private key is commonly derived from a conventional password. ifthis is done, the cryptographic communications protected by this key canbe attacked using password guessing (gong et al., 1993). such attacks havebeen reported against s/key (haller, 1994) and kerberos (neuman andts’o, 1994). although techniques to guard against the attacks are known(bellovin and merritt, 1992; gong et al., 1993), they are rarely employed.9cryptographicbased authentication is usually based on authentication and integrity algorithms (e.g., digital signatures and keyed oneway hash functions, not on encryptionalgorithms).trust in cyberspacecopyright national academy of sciences. all rights reserved.reinventing security123tokenbased mechanismsan authentication technique that has gained popularity over the lastfew years is use of socalled hardware tokens. a number of differenttypes of hardware tokens are available. all contain a cryptographic key in(nominally tamperresistant) storage. some use the key to encrypt a local(current) clock value; others use the key to transform a challenge suppliedby the server; and still others execute a complete cryptographic protocol.some sort of personal identification number (pin) or password is usually required in order to enable a hardware token. because an attacker isassumed not to have access to the token itself or to its memory contents,such a pin is not susceptible to dictionary and other forms of passwordguessing attacks unless the token has been stolen. theft is further discouraged by employing a counter to trigger erasure of the hardware token’s keystorage after a few incorrect entries. tokens that can be electrically connected to a user’s computer, such as smart cards, java rings, and pc cards,are often used to support cryptographic authentication protocols. the degree of tamper resistance provided by these tokens varies widely, so theirresistance to attacks involving physical theft is uneven. hardware tokensare evolving into fullfledged, personal cryptographic devices, capable ofproviding services beyond authentication.biometric techniquesbiometric authentication techniques rely on presumedunique characteristics of individuals: voiceprint systems, fingerprint readers, retinalor iris scanners, and so forth. apart from questions about the reliability ofthe methods themselves, principal disadvantages of biometric techniquesare the cost and availability of suitable input devices and the unwillingness of people to interact with such input devices. few computers comeequipped with fingerprintscanning hardware, and few people are willing to subject their eyes to retinal scanning. consequently, biometricauthentication is employed only in highthreat settings. when used acrossa network environment, cryptography must complement the biometrics,since a recording of a thumbprint transmitted across a network is just assusceptible to interception and replay as a plaintext, reusable password.as personal computers and workstations have acquired more sophisticated audiovisual interfaces, there is renewed interest in employing biometric authentication technology in the network environment. for example, a growing number of computers now come equipped withmicrophones, and lowcost video cameras are also becoming more common. however, a limitation is the need for security of the capture medium.for example, biometric authentication data offered by a personal computertrust in cyberspacecopyright national academy of sciences. all rights reserved.124trust in cyberspacecould have been generated by the presumed scanning device or it could bea bit string supplied by an attacker. thus, to the extent that it is possible togenerate bit strings that appear to be valid biometric data, these systems arevulnerable. moreover, possession of the template needed to validate abiometric scan, plus knowledge of the algorithm used to create that template, probably provides enough information to generate such bit strings(for any user whose template is compromised); disclosure of template datastored at any biometric authentication server could compromise use of thatbiometric technique for the affected users, forever!findings1.networkbased authentication technology is not amenable to highassurance implementations. cryptographic authentication represents apreferred approach to authentication at the granularity that might otherwise be provided by network authentication.2.cryptographic protocols are difficult to get right. legitimate needswill arise for new cryptographic authentication protocols (e.g., practicalmulticast communication authentication), but the technology for verifying these protocols is far from mature. further research into techniquesand supporting tools should be encouraged.3.the use of hardware tokens holds great promise for implementingauthentication. cost will be addressed by the inexorable advance of digital hardware technology. but interface commonality issues will somehowhave to be overcome. the use of pins to enable hardware tokens is avulnerability that the use of biometrics could remove. when tokens arebeing used to sign data digitally, then an interface should be provided sothat a user can know what is being signed.4.biometric authentication technologies have limitations when employed in network contexts. still, for use in a closed nis, biometric techniques that employ existing (or envisioned) interfaces in personal computers (e.g., microphones, lowcost cameras) are worth exploring.cryptography and publickey infrastructureit is impractical to provide strong physical, personnel, and proceduralsecurity for a geographically distributed, heterogeneously administeredcomputing system like an nis. cryptographic mechanisms, however, canprovide security for this setting. they have not been widely deployed,especially in largescale distributed systems. so even where the theory iswell understood, there is much to be learned about the practical aspects ofdeployment and use. the discussion that follows outlines some of theproblems that will have to be confronted by nis developers.trust in cyberspacecopyright national academy of sciences. all rights reserved.reinventing security125the subject of cryptography ranges from foundational mathematicsto applied engineering topics, and a great deal of reference material exists(schneier, 1996; cstb, 1996; menezes et al., 1996). for this report, familiarity with some basic cryptographic services, as sketched in box 4.3, willsuffice.the two fundamental types of cryptographic systems are secretkey(or symmetric key) cryptography and publickey (or asymmetric key)cryptography. secretkey cryptography has been known for thousands ofyears. publickey cryptography is a relatively recent invention, first described in the public literature10 in 1976.with secretkey cryptography, the key used to encrypt a message isthe same as the key used to decrypt that message, and the key used tocompute the message integrity code (mic) is the same as the key used toverify it. this means that pairs of communicating parties must share asecret, and if that secret becomes known to some third party, then thatthird party becomes empowered (1) to decrypt and modify messages intransit undetectably and (2) to generate spurious messages that appear tobe authentic. arranging for both parties of a conversation—and nobodyelse—to know a secret is one of the central challenges in cryptographicsystem design.box 4.3basic cryptographic servicespreserving confidentiality of data. this service is implemented by the senderencrypting the data and the receiver decrypting that data. wiretappers see onlyencrypted data, which (by definition) reveals nothing about the original data.protecting the integrity of data. this service is implemented by using a messageintegrity code (mic), a relatively short (fixedsize) value computed by the sender ofdata and validated by the receiver. the mic is a complex function of both the databeing protected and a cryptographic key.authenticating parties in a conversation. this service is frequently implementedusing a challenge/response protocol, in which one party picks a random number andchallenges the other to encrypt (or decrypt) it. only parties with knowledge of asecret key are able to satisfy the challenge.nonrepudiation of message origins. this service allows the receiver of a message not only to authenticate the sender but also to prove to a third party that themessage came from that sender.10a recent disclosure indicates that the bestknown publickey techniques were actuallyinvented first in a classified setting several years before their development in the academiccommunity (see <http://www.cesg.gov.uk>).trust in cyberspacecopyright national academy of sciences. all rights reserved.126trust in cyberspacewith publickey cryptography, different keys are used to encryptand decrypt messages, and the decryption key cannot be derived fromthe encryption key. similarly, different keys are used to generate anintegrity check and to verify it, and the generation key cannot be derivedfrom the verification key. the keys used for decryption and integritycheck generation are called private keys; they are kept secret and generally known only to a single party. the keys used for encryption andintegritycheck verification are called public keys; these can be freelypublished (hence the name “publickey cryptography”). having separate public and private keys simplifies the distribution of keys, especiallyin large systems.publickey cryptography can implement cryptographic services thatcannot be built with secretkey cryptography.11 for example, a digitalsignature is an integrity check that can be verified by any party. (anintegrity check generally can be verified only by the intended recipient ofa message.) digital signatures can be implemented using publickey cryptography—a private key is used by the sender to “sign” the message andthat sender’s public key (which is accessible to all) is used to verify thesignature—but not by using secretkey cryptography.12versatility does have its cost. publickey cryptography is considerably more (computationally) expensive to use than secretkey cryptography. therefore, most cryptographic systems that make use of publickeycryptography are, in fact, hybrids. for confidentiality, publickey cryptography is employed to encrypt a secret key that, in turn, is used withsecretkey cryptography to encrypt data. and, to compute a digital signature of a message, a digest13 of the message is computed and only thedigest is signed. this hybrid approach minimizes the number of publickey operations required. even so, it requires cryptographic algorithmsthat keep pace with communications transmission speeds.11note that not all publickey algorithms can offer both confidentiality protection andintegrity protection. for example, the diffiehellman algorithm (diffie and hellman, 1976)cannot support signatures, and the digital signature algorithm cannot support encryption.12several signature schemes have been developed based on secretkey cryptography, butthey are too cumbersome to be seriously considered for “real” systems.13a message digest function is more comparable to a secretkey cryptographic algorithmin its performance and technology. it computes a collisionproof fixedlength “checksum”of any message.“collisionproof” means that it is practically impossible to find two messages with the same checksum. because it is collisionproof, a given message digest hasonly one corresponding message (that one can find), and signing it is as secure as signingthe entire message.trust in cyberspacecopyright national academy of sciences. all rights reserved.reinventing security127findings1.application programming interfaces (apis) for cryptographic services will promote greater use of such services in niss. cryptographicservices are an extremely effective means for solving certain security problems in geographically distributed systems.2.faster encryption and authentication/integrity algorithms will berequired to keep pace with rapidly increasing communication speeds andto deploy this technology in a wider range of applications, such as authentication, integrity, and confidentiality for multicast groups.the key management problemthe security of a cryptographic system depends, in large part, on thesecurity of the methods and practices used to generate and distributekeys. for small systems, keys can be distributed by manually installingthem. but this solution does not work for larger systems. there are twowellknown approaches to the keydistribution problem in medium tolargescale systems: keydistribution centers (for secretkey cryptography) and certification authorities (for publickey cryptography).keydistribution centersa keydistribution center (kdc) is an online automated secretkeyprovider. the kdc shares a secret distribution key with every party itserves, so its storage requirements are linear in the number of its clients.if client a wants to talk with client b, then that fact is communicated tothe kdc. the kdc then randomly generates a new secret (session) keyfor a and b to use, and distributes that session key, encrypted under boththe distribution key it shares with a and the distribution key it shareswith b. the messages sent by the kdc must be both integrity and confidentiality protected, and they must give the identities of the parties whowill be using the session key (so that each party can securely know theidentity of the other).variations of this protocol satisfy additional requirements, but all variants require that the kdc be online and all involve the kdc having access(at one time or another) to each session key generated. the requirementthat the kdc be online means that to serve client systems having stringentavailability requirements, the kdc itself and the communications links to itmust be highly available. because the kdc has had access to all sessionkeys, it is an ideal target for an attacker trying to decipher previously intercepted traffic. some kdc designs are especially vulnerable, because theyemploy longterm key distribution keys. undetected kdc penetrations aretrust in cyberspacecopyright national academy of sciences. all rights reserved.128trust in cyberspacethe most serious, as the attacker is then free to impersonate any client of thekdc and (in some designs) to read old messages.certification authoritieswith publickey cryptography, the challenge is distributing the public keys in a secure fashion.14 confidentiality is not an issue becausepublic keys are not secret, but integrity protection is. if a wants to sendan encrypted message to b and a can be misled by an attacker about b’spublic key, then a can be tricked into encrypting messages for b using theattacker’s public key. the encrypted message would then be accessibleonly to the attacker. the solution is to employ a trusted third party calleda certification authority (ca). the ca uses publickey cryptography tosign certificates; each certificate binds a subscriber identity to a publickey. if a knows the public key of the ca and a has a casigned certificate binding a public key to subscriber identity b, then a can verify theca’s signature on the certificate to determine whether the certificate isgenuine. and provided the ca is careful about authenticating eachsubscriber’s identity before issuing certificates, a casigned certificatebinding a public key to subscriber identify b becomes a reliable way for ato learn b’s public key.cas are, in some respects, easier to secure than keydistribution centers. in theory, cas do not have to be online or highly available. acertification authority need only be available to issue certificates whennew parties are being added to the system and, therefore, offline caoperation is feasible. offline operation is even preferable, because itmakes access by attackers more difficult, thereby helping to preserve casecurity. but in practice, an increasing number of cas are being operatedonline—fast response time for issuing certificates is important to (wouldbe) subscribers, and online operation is the only way to keep responsetime low.even so, exploiting a compromised ca is considerably more difficultthan exploiting a compromised kdc. once a ca has been compromised,it can sign and issue bogus certificates. but that behavior in no way compromises previously signed or encrypted traffic. moreover, if certificatesare being posted publicly anyway, then a ca that suddenly posts uncharacteristically large numbers of certificates will arouse suspicion. compromise of a ca does become problematic when certificates are used forauthentication by an authorization system. sometimes access control dataare even stored in certificates. covert compromise of a ca then can be aserious matter because the attacker can then grant access permissions.14distributing the private keys, since each is known to a single party, is not necessary.trust in cyberspacecopyright national academy of sciences. all rights reserved.reinventing security129a certificate should be revoked whenever the corresponding privatekey has been compromised or the attributes that the certificate is bindingto a public key are no longer accurate. for example, a certificate containing access control data must be revoked whenever access control permissions described in that certificate are changed. implementing timely revocation of certificates requires some sort of service that is highlyavailable, so that users can check the status of a certificate just before use.this server availability requirement somewhat offsets the arguments infavor of cas (and publickey cryptography) over kdcs (and secretkeycryptography): the ca may not need to be highly available, but publickey cryptography, like secretkey cryptography with its kdc, does needto have some form of highly available service (for checking about revocations).actual deployments of largescalekeydistribution centersand certification authoritiesthe u.s. dod first developed kdcbased key management systemsin the early 1970s. the stuii secure telephone system, which servedabout 40,000 users, was perhaps the largest system deployed by the u.s.government that was based on kdc technology. stuii was supersededby the stuiii system in the early 1980s; stuiii uses publickey certificates and serves more than 500,000 users. instances of the kerberos system (neuman and ts’o, 1994) and osf/dce (an industry standard forunixbased distributed systems that uses kerberos) appear to be thelargestscale kdc deployments in the commercial sector.pretty good privacy (pgp) (a secure email technology) and lotusnotes (a popular “groupware” product) probably represent the largestdeployed publickey systems. like osf/dce, lotus notes is usuallyemployed on an interorganizational basis, so that the estimated 10 millioncertificates associated with lotus notes users are distributed over manyorganizations. some pgp use is tied to cliques of users, but pgp also isused more globally to provide secure email among an extremely broadset of users. the absence of a formal ca structure within pgp makes itdifficult to determine connectivity among users. numerous examples ofinauthentic pgp keys resident in various public servers raise questionsabout the actual size of pgp’s deployment.web browsers employ server certificates, usually issued by publiccas (see below), in using the secure socket layer (ssl) protocol to establish encrypted, oneway, authenticated communication paths.15 this de15ssl also permits twoway authentication, through the use of client certificates, but thisoption is not often invoked.trust in cyberspacecopyright national academy of sciences. all rights reserved.130trust in cyberspaceployment of publickey cryptography has been crucial for providing thesecure paths necessary to send credit card numbers and other sensitivedata in support of ecommerce on the internet. but the biggest demandfor certificates promises to come from secure email (e.g., s/mime)16available in version 4 of both the netscape and microsoft browsers andfrom client certificates used to authenticate users to servers. deploymentof the secure electronic transaction (set) protocol for credit card transactions over the internet has been slower than expected, but ultimately it,too, could cause millions of certificates to be issued to the existing users ofvisa, mastercard, american express, and discover cards.publickey infrastructurethe term “publickey infrastructure” (pki) is used in the literature,and especially in trade publications, for a collection of topics related topublickey management. here, pki refers to technical mechanisms, procedures, and policies that together provide a management framework forenabling publickey cryptography deployment in a range of applications:•the technical mechanisms generally include publickey digital signature and oneway hash algorithms, the syntax of publickey certificatesand certificate revocation lists (crls), communication protocols for theissuance, reissuance, and distribution of certificates and crls, and algorithms for validating sequences of related certificates and associated crls.•the procedures generally concern issuance, reissuance, and requests for revocation of certificates, and the distribution of crls.•the policies encompass the semantics associated with digital signatures, the semantics of certificate issuance and revocation, the operation of certification authorities, legal liability concerns, and so on.most of this management framework is concerned with certificatesand, therefore, it is instructive to retrace their origin. when publickeycryptography was first described in the open literature, no mention wasmade of certificates—the public keys associated with identities were simply presumed to be available whenever needed. an mit bachelor’s thesis(kornfelder, 1978) suggested the idea of a publickey certificate. but certificates only transform the problem of acquiring some subject’s publickey into the problem of acquiring some certificate issuer’s public key (sothat the certificate containing a subject’s public key can be verified). theeffort expended to acquire a certificate issuer’s public key to verify a16see s/mime resources, available online at <http://www.rsa.com/smime/html/resources.html>.trust in cyberspacecopyright national academy of sciences. all rights reserved.reinventing security131certificate becomes leveraged if there are relatively few issuers and theysign certificates for many subjects. and most pkis adopt this strategy. acommercial or governmental organization issues certificates to its employees, its customers, or the public in general. the organization alsorevokes certificates when appropriate. notice, though, that the ca hasnow ascended to a somewhat more formal role in the management ofcertificates, concerned with preserving meaning or accuracy of the bindings in its certificates as well as with the mechanics of disseminating thosebindings.although pkis based on cas are the most common, they are not theonly model for certificate issuance. any user with a public key can issuea certificate whose subject is any other user. pgp works in this fashion; itscertification model is called a web of trust. this usercentric model forcertification has advantages. initial deployment is especially easy, forexample. but a web of trust also does not scale well to large numbers ofsubjects. in addition, with a diverse set of certificate issuers, certificatesno longer will have a standard meaning—one user’s standard of proof forissuing a certificate might not be the same as another’s. without agreement on certification policies, applications are unable to interpret certificates, and the goal of enabling deployment of publickey cryptography isundermined.several models of pki have started to emerge. first, companies suchas verisign, cybertrust, and certco offer pki services to all comers.these same companies also offer socalled privatelabel ca services forother companies, acting as processing agents and issuing certificates onbehalf of the other companies. second, some organizations have startedto issue their own certificates in support of internet business models thatcall for identifying clients by certificates. finally, there are companiesissuing certificates for internal intranet use, irrespective of external customer requirements. the u.s. postal service has announced ambitions tobecome a ca on a grand scale. it has not yet realized these ambitions, butif it does, then a new category of certificate issuers will be born—onecloser to government and for which new legal issues may arise and newcustomer benefits may be possible. despite the competition among thesemodels, there are good arguments (kent, 1997) that users will requiremultiple certificates, issued by a variety of cas. this suggests a world inwhich many cas coexist, both domestically and in the international environment.given the minimal experience to date with pkis, many aspects of pkitechnology merit further research. this research should focus not only onthe issuer (ca) aspects of pki, but also on the client or consumer side.most applications that make use of certificates, for example, have poorcertificatemanagement interfaces for users and system administrators;trust in cyberspacecopyright national academy of sciences. all rights reserved.132trust in cyberspacethe result is an unnecessary operational vulnerability. toolkits for certificate processing are not much better. the development of intel’s commondata security architecture (cdsa) as an application program interface(api) for a variety of cryptographic services does not alleviate the problem, as the complex issues associated with certificate validation are belowthe level of this specification.the ca models described all focus on binding a public key to anidentity, and that identity is presumed to have some realworld semantics. another approach to certificate use is embodied by what are called“keycentric” systems, such as the secure distributed security infrastructure (sdsi), in which all names bound to public keys are viewed as having only local significance, for the syntactic convenience of users. thesimple publickey infrastructure (spki) working group of the internetengineering task force (ietf) is attempting to codify these notions intoan internet standard. however, no products that make use of certificateshave adopted spki or sdsi notions.findings1.obstacles exist to more widespread deployment of key management technology. some of the obstacles are understood; others will become apparent only as largescale deployments are attempted.2.although pki technology is intended to serve very large populations with diverse administrative structures, issues related to timely notification of revocation, recovery from compromise of ca private keys, andname space management all require further attention.network access control mechanismsoperating system access control mechanisms manage the use andsharing of resources implemented and managed by that operating system. analogous mechanisms have been developed for network resources—subnetworks, physical and logical channels, network services, and thelike. interest in such network access control mechanisms is relativelynew, probably because the need for them became apparent only afternetworks started playing a central role. this section examines severalmechanisms commonly used today to effect access control in networksand makes recommendations regarding additional research.closed user groupsvirtual circuit data networks, such as x.25, frame relay, and asynchronous transfer mode (atm) networks, often include a mechanism fortrust in cyberspacecopyright national academy of sciences. all rights reserved.reinventing security133controlling whether network subscribers should be permitted to communicate. in closed user groups (cugs), subscriber communication is controlled based on network authentication (i.e., identities represented bynetwork layer addresses), although in some instances other informationmay come into play as well. for example, inbound versus outbound callinitiation (and reverse charging) may be parameters to an access controllist check. however, cugs usually are limited to entities on a singlenetwork that are implemented in a single networking technology, managed by a single administration. in an internet environment, which increasingly characterizes the networked world, the singlenetwork restriction means that cugs will become increasingly irrelevant.17virtual private networksvirtual private networks (vpns) have been implemented both fordata and for voice. the idea is to use a public network and to create theillusion of a network comprising transmission and switching resourcesthat are devoted exclusively to subscribers of the vpn. the centrexservice offered by local telephone companies is one example; it is usuallyimplemented through administrative controls in central office switches.in data networks, a vpn can be supported in a similar manner. however,vpns implemented in this way are vulnerable to wiretapping attacksconducted on the underlying real network and to administrative configuration errors.to prevent wiretapping, cryptographic protocols can be employed ateither the network or internet layer. many such schemes have been developed and deployed over the last 20 years, supported by governmentfunded programs. the first packet network vpn technology was theprivate line interface (pli) developed by bolt, beranek, and newman inthe mid1970s (bbn, 1978). the pli was approved to protect classifieddata for transmission over the arpanet, creating a vpn for a set ofdod secretlevel subscribers. later examples of such technology (developed with government funding or for government use) include the bcrand blacker (kdcbased vpn systems), the xerox xeu and the wangtiu (manually keyed lan vpn systems), and the motorola nes andcaneware (certificatebased, internet vpn systems).in the commercial arena, various systems have also been developedand deployed, including systems for use with x.25 and atm networks, aswell as those for internet devices. although vpnenabled products havebeen available from vendors, they typically employ proprietary proto17however, by relying on cryptography, a virtual private network can circumvent thissinglenetwork limitation.trust in cyberspacecopyright national academy of sciences. all rights reserved.134trust in cyberspacecols, making interoperability across vendor product lines difficult. moreover, many vpnenabled products employ manual keymanagement, andthat prevents their deployment in largerscale settings. the adoption ofthe internet protocol security (ipsec) protocol standards (see chapter 2) isexpected not only to increase the number of products incorporating cryptographic vpn capabilities but also to ensure interoperability and promote the use of automated (certificatebased) key management protocols.widespread use of vpn technology in the internet will almost surelyfollow.ipsec cryptographically protects traffic between subscribers. becauseipsec operates at the internet layer, it can protect traffic across any localarea network or wide area network technology, and it can be terminatedat end systems (e.g., personal computers, workstations, or servers) as wellas at security gateways (e.g., firewalls). access control in ipsec is basedon cryptographic authentication, effected initially through key distribution and on a continuing basis through the use of a keyed message authentication function. the granularity of access control is determined bylocal policy and can range from subnetlevel protection to peruser andperapplication controls.ipsec is also noteworthy because it includes an optional antireplayfacility, which prevents certain forms of denialofservice attacks. thisnot only has intrinsic value but also constitutes important recognitionthat network security is more than just an extension of access control.however, other degradation or denialofservice attacks—namely thosedirected at the switching and transmission media that implement a vpn—are not prevented by ipsec, nor can they be by any vpn implementation.a vpn cannot defend against attacks directed at the resources used tobuild the vpn.firewallsfirewalls (cheswick and bellovin, 1994; chapman and zwicky, 1995)are a defensive mechanism typically deployed at the boundary of a trustedand an untrusted computer network (appendix h briefly describes thefour basic kinds of firewalls). safe—or presumed safe—messages transitthe firewall; others are blocked. thus, computers inside the boundary areprotected from (some) attacks originating at computers located outsidethe boundary. in theory, firewalls should not be necessary. if a singlecomputer can be hardened against attacks, then, in principle, all computers can be. and if all computers on a network are hardened, then there isno need for an additional perimeter defense. in practice, firewalls dooffer benefits.first, hardening computers against attack is not simple. and systemstrust in cyberspacecopyright national academy of sciences. all rights reserved.reinventing security135often must run commercial offtheshelf protocols for which a perimeterdefense is the only protection available. as an example, even when cryptographic authentication can be provided in a product, vendors oftenchoose to use more vulnerable networkbased authentication. for suchproducts, users have no choice but to rely on addon protective measuressuch as firewalls.a second, more subtle, benefit of firewalls concerns vulnerabilities resulting from software that contains bugs. the best cryptography in theworld cannot protect a service if at one end of the connection is an attackerand at the other end is software whose bugs make compromise possible.since today’s software invariably does have bugs, with no solution in sight(see chapter 3), prudence suggests blocking system access by outsiders.firewalls allow access by insiders while denying access to outsiders.third, it is easier to administer software on one or a small number offirewalls than to do so for the entire collection of workstations, personalcomputers, and servers composing an organization’s computing network.physical access to a computer’s console might be necessary for setting orchecking its configuration, for example. moreover, a firewall can providea network security administrator with a single point of policy control foran entire network. thus, while configuration and policy errors on individual computers are not eliminated by deploying a firewall, its presencedoes reduce outside exposure and thereby prevents those errors frombeing exploited.finally, firewalls often are deployed to present a defense in depth.even if a system is believed to be secure, with proper authentication andpresumedreliable software, a firewall can provide a layer of insurance.limitations of firewallsfirewalls can enforce only policies defined in terms of restrictions oninbound and outbound traffic. for example, a policy stipulating that alloutbound email is logged could be enforced using a firewall: an authorized mail gateway (which presumably does the logging) would be theonly computer whose email packets are passed to the outside, and allother machines would send their email to that gateway for forwarding tothe outside. but there are limits to what can be accomplished using restrictions on inbound and outbound traffic. for example, an insider prevented from communicating directly with a web server—a policy implemented by restricting outbound traffic to port 80—could set up a webproxy server that monitors port 8000 (say) on some machine outside thefirewall. traffic to port 8000 would not be blocked by the firewall, so theinsider could now surf the web using the outside proxy. more generally,firewalls cannot protect against inside attacks (see box 4.1). also, usingtrust in cyberspacecopyright national academy of sciences. all rights reserved.136trust in cyberspacefirewalls is pointless when paths exist to the outside that bypass thosefirewalls: an authorized link to some outside organization, an unprotected modem pool, or even a careless employee dialing out to an internetservice provider.the decision regarding what protocols are allowed to pass throughthe firewall is critical for success. an air gap is a more secure and cheapersolution if no protocols are being allowed to send packets through thefirewall. some protocols will be allowed through but as the number ofsuch protocols increases, so do the chances that an attack could be wagedby exploiting a flaw in one of them. the transmission of executable content provides a further challenge for firewalls. for example, macros inmicrosoft word or excel attachments to messages can be dangerous aswell as difficult to filter. similarly, mailers (from a wide variety of vendors) are susceptible to buffer overflow attacks when overly long filenames appear in attachments (cert advisory ca98.10, august 199818).a single filter, at the firewall, can protect a whole network of machines.other limitations of firewalls come from the protocol layer at whichthe firewall operates. there are four basic types of firewalls: packetfilters, circuit relays, application gateways, and dynamic (or stateful)packet filters. the first three correspond to layers of the protocol stack;the fourth tends to incorporate features of both network and applicationlayer systems. attacks conveyed using protocol layers higher than theone at which the firewall operates cannot be blocked by the firewall,because the firewall cannot filter those messages. for example, a packetfilter firewall operating at the internet layer is unable to defend againstweaknesses in an application layer protocol such as the simple mail transfer protocol (smtp). similarly, an applicationlayer firewall that didmonitor smtp packets could not protect against attacks conveyed by email attachments, since such attachments only have interpretations abovethe layer at which smtp operates—an email application cognizant ofattachment types would have to be involved in that defense.the utility of a firewall is also limited by the use of endtoend cryptography. it is obviously impossible for a firewall to inspect the contentsof an encrypted packet, so encrypted packets cannot be blocked. similarly, address translation and other forms of packet modification thatsome firewalls use are not possible if a packet is going to be cryptographically authenticated. the usual solution is to terminate cryptographic associations at the firewall. in some cases, multiple levels of cryptographicprotection are used, with an outer layer permitting passage through thefirewall and the inner layer being end to end.18cert advisories are available online at <http://www.cert.org>.trust in cyberspacecopyright national academy of sciences. all rights reserved.reinventing security137in addition to the intrinsic limitations of firewalls by virtue of whatthey do, there are pragmatic limitations by virtue of how they are built.most firewalls are implemented as applications on top of standard operating systems and, consequently, are vulnerable to attacks directed at theunderlying operating system. a firewall developer may strip out thoseportions of an operating system that are considered sources of vulnerabilities, but given the size and complexity of a modern operating system,only limited forms hardening will be achieved in this way. the alternative, building the firewall on a custom operating system, introduces thepossibility of new vulnerabilities that have not been detected and remedied through the examination and experience of a large community ofusers. perhaps for this reason and the cost, only a small number of thefirewalls that have been developed employ custom operating systems.many firewalls operate application “proxies,” and all of the concernscited later in this chapter regarding application security apply to them.moreover, it is common for an application proxy to be developed usingexisting application code as a base. in such cases, vulnerabilities in thebase application may be preserved in the proxy. also, modifications toapplication code needed to convert it into a proxy, or an incomplete understanding of the application protocol, can be a source of vulnerabilities.guardsguards have been used in military computing systems for two decades to control the flow of classified electronic information. most oftenthey are used to permit the flow of information from a lowersensitivityenvironment to a highersensitivity enclave in support of mandatory access control policies, blocking possible reverse information flow thatmight accompany protocol acknowledgment and flowcontrol traffic. automated filters within guards have been designed to ensure that all trafficconforms to specified criteria, including fieldbyfield restrictions on typesor values. traffic that does not conform to these criteria is rejected andnot permitted to pass the guard. but as traffic formats become moreflexible and field values have greater range, it becomes less likely that anautomated filter can correctly detect all prohibited traffic. some designssend all questionable traffic to a human for visual review. traffic reviewtends to be monotonous work, and humans may be only slightly bettersuited to do the filtering than the machine processes.despite the limitations of guards, they are one of the most prevalentaccess control mechanisms for electronic information systems in use todayby the military. the security architecture of the missi program (see box4.4) relies on the use of guards to support electronic mail, directory services,and file transfer across enclave boundaries. for example, the defense mestrust in cyberspacecopyright national academy of sciences. all rights reserved.138trust in cyberspacesaging system (dms) relies on the use of the secure network system (sns)guard to permit electronic mail to flow in and out of highly sensitive enclaves and to facilitate communication with lesssensitive dms subscribers.findings1.closed user groups have some utility in individual, circuit switchednetworks, but they will become increasingly irrelevant as networkingmigrates to the internet proper or to internet technology.2.vpn technology is quite promising. proprietary protocols andsimplistic keymanagement schemes in most products have preventedbox 4.4multilevel information system security initiativethe multilevel information system security initiative (missi) is a program initiated by the national security agency (nsa) in the early 1990s. the original goal wasto provide a set of products and an architectural framework that would facilitate thedevelopment of multilevel secure niss. the primary components of the architectureoriginally included the following:1.fortezza—a pcmcia crypto card suitable for use with unclassified data,2.caneware—an inline encryption system,3.secure network system (sns)—a guard,4.applique—a multilevel secure operating system (based on tmach) and corresponding crypto card for use with classified data, and5.network management system (nms)—a collection of software for managingthe security of the other components.missi evolved over time, and its focus changed. the applique component wasnever developed. the nms component was reduced in scope to encompass onlycertificate management. fortezza was redefined to be suitable for protecting secretdata, at least in some contexts. the sns component was reduced somewhat inscope, but still functions as a highassurance guard, primarily for separating topsecret enclaves from less sensitive network environments. only the caneware component emerged largely intact, but it is the end product of a series of nsafundednetwork security efforts at motorola dating back to the late 1970s.a comprehensive multilevel network security architecture has not emerged frommissi and, instead, the hallmark “managed risk” has become among its most visiblecontributions. in principle, the message in “managed risk” is consistent with recommendations made elsewhere in this report: the security of a system should rely on anappropriate combination of components organized to counter a perceived threat—highly trusted components need not be used throughout. in practice, however,“managed risk” has been used to justify use of low or mediumassurance components to secure classified data (especially at the secret level) without much analysisof the threat or evaluation of the adequacy of the offered countermeasures. thatapproach is not consistent with the recommendations of this study.trust in cyberspacecopyright national academy of sciences. all rights reserved.reinventing security139vpn adoption in largerscale settings. the deployment of ipsec can eliminate these impediments, thus facilitating vpn deployment throughoutthe internet.3.much work remains to further facilitate wholesale and flexible vpndeployments. support for dynamic location of security gateways, accommodation of complex network topologies, negotiation of traffic securitypolicies across administratively independent domains, and support formulticast communication are all topics requiring additional work. also,better interfaces for vpn management will be critical for avoiding vulnerabilities introduced by management errors.4.firewalls, despite their limitations, will persist as a key defensemechanism into the foreseeable future. as support for vpns is added,enhancements will have to be developed for supporting sophisticatedsecurity management protocols, negotiation of traffic security policiesacross administratively independent domains, and management tools.5.the development of increasingly sophisticated networkwide applications will create a need for applicationlayer firewalls and a betterunderstanding of how to define and enforce useful traffic policies at thislevel.6.guards can be thought of as special cases of firewalls, typicallyfocused at the application layer. thus, all the issues cited for firewalls areapplicable here, but with increased emphasis on assurance and mandatory access control policies.foreign code and applicationlevel securitymost users today execute software written by others. the software iseither purchased from commercial vendors (e.g., microsoft, lotus,netscape, intuit, and others) or obtained at no cost from other users as socalled freeware or shareware.19 purchased software has traditionally beendelivered in some sort of shrinkwrap package that is difficult to counterfeit or tamper with, so it is easy to trust that the package contains what theproducer intended. presumably, the reputation of the producer engenders trust that the software does what it should (to the extent that anysoftware does) and that it does nothing that it should not.2019scripting languages and other veryhighlevel programming vehicles (see appendix e)make it relatively easy for a nonprogrammer to cobble together software that might be bothuseful to and usable by others. and there is an ethic that encourages the development,distribution, and constant improvement of freeware.20a 1998 release of microsoft’s spreadsheet program excel 97 apparently contained a flightsimulator that could be accessed by the right combination of keystrokes, starting from a blankwork sheet. the existence of gratuitous functionality in commercial software is apparently notrare, and the term “easter egg” has been coined to describe such surprising features.trust in cyberspacecopyright national academy of sciences. all rights reserved.140trust in cyberspacebut a second delivery mechanism has been made possible by theinternet and world wide web. clicking on a web page enables softwareto be downloaded to a user’s machine and automatically installed. employed at first for freeware, this electronic avenue for distribution is beingused increasingly by commercial vendors because it is both convenientand cheap. but no longer is there the shrinkwrap and, in the case offreeware, producers have no financial stake in preserving their reputations. embedding an attack inside this software is not difficult. cautioususers do have the option, though, of being selective about what softwarethey download and from where.with the functionality in place to associate executables with webpages, the next step was not large. programs downloaded and executedby a user’s computer could be used to enhance a provider’s web pageswith animation and other locally generated special effects. java “applets”and activex modules are the bestknown examples of this technology.here, delivery and execution of the socalled foreign code can occur without a user’s knowledge.21 the number of potential software providers fora given computer is now significantly increased; the control that usersexert about what providers to trust and what code to run is significantlydecreased. weak operating system security facilities in personal computers exacerbate the problem, since any software executing under such operating systems has virtually unconstrained access to resources on the pc.not only can executables be associated with web pages, but foreigncode is also increasingly being associated with other forms of documents.postscript is a portable representation language for printing, but it ispossible to write postscript programs that do more than control document printing.22 microsoft word documents can contain macros thataccess a user’s files, destroying or exfiltrating data as shown by the widelydisseminated word “concept virus.” moreover, word macros are largelyplatform independent and are excellent vehicles for writing viruses. industry trends are toward even greater use of “active document” technology (e.g., apple opendoc and microsoft ole), which means that moreblurring of documents and executable content is likely to occur.the increased use of foreign code may enable enhanced functionality,but it also will create a problem: system trustworthiness will erode unlesssecurity mechanisms are developed and deployed for confining the ef21for example, the default configuration for the netscape and microsoft browsers enables javascript and java. thus a user may have no warning that foreign code has beenintroduced into her or his environment.22if one views (rather than prints) a postscript document using an application such asghostscript, the document can contain a trojan horse that can access and exfiltrate (ordestroy) data on the user’s computer.trust in cyberspacecopyright national academy of sciences. all rights reserved.reinventing security141fects of foreign code. these security mechanisms might exploit uniquecharacteristics of the delivery mechanism or source of the foreign code, orthey might be tied to the environment in which the foreign code is executed. if the problem is clear, the solution is not. the remainder of thissection, therefore, surveys the problem in more detail and outlines someapproaches to a solution.the activex approachthe activex security mechanisms allow modules to be digitallysigned pieces of code. users check this signature and, based on that,decide whether a module should be permitted to execute. the signature,analogous to a brand name or the corporate logo on shrinkwrappedsoftware, is thus intended to engender trust that the activex module willbehave as intended. the signature also identifies a responsible partyshould the activex module misbehave.underlying this activex authenticode approach is the presumptionthat users can decide whether to run a module based on knowing theidentity or seeing some credential of a vendor or distributor. this presumption has questionable validity, as the successful deployment in february 1997 of a malicious activex module by the chaos computer clubof hamburg confirmed (van eng, 1997). users either do not bother tolook at a signature or cannot make an informed decision upon seeing asignature.23 the intended analogy between signatures and shrinkwrappackaging is likely flawed. physical distribution channels impose numerous impediments to the distribution of malicious shrinkwrap softwarethat the authenticode approach does not. these impediments serve an(unintended) security function by raising the barrier for market entry andby facilitating the tracing of malicious software (due to accounting andshipping trails).a second difficulty with authenticode signatures concerns revocation. compromised signing keys could be used by malicious individualsto sign hostile activex modules. even if the existence of these compromised keys were discovered, recovery would require revocation acrossthe entire internet, whose population is, by and large, technically unsophisticated users.24 moreover, it is likely that enough prospective vendors of activex modules will be certified that some inadvertently provide23the difficulty of attaching semantics to a signature is not unique to activexauthenticode. it is a difficulty that exists today for all uses of signatures in the internet.24in fact, verisign has maintained a revocation list for activex signatories since early1997. it is checked by the microsoft mobile code platform, but it has seldom been used byusers and administrators.trust in cyberspacecopyright national academy of sciences. all rights reserved.142trust in cyberspaceopportunities to introduce malicious code. poor physical, personnel, procedural, or computer security practices at any one, for example, couldlead to the unintentional signing of malicious code.the java approachwith java, security is enforced by executing code in a confining environment known as the java virtual machine (jvm). early versions forcedcode to be run with either very tight restrictions or almost none, depending on whether or not the code came from a trusted source. the systemhas since evolved, and increasingly flexible and expressive permissionbased access controls have been added (gong et al., 1997).the jvm interprets java byte code, a stackbased intermediate language that is designed to be platform independent. java programs, inbyte code format, carry type information about their variables, the configuration of the runtime stack throughout execution, and the signaturesof routines that are defined and invoked. when a byte code program isloaded, an initial check is performed to verify that the program conformsto certain rules, including typesafety rules. the jvm continues carryingout typesafety and other security checks throughout the execution of thejava program.java programs were designed to be compiled to java byte code and theresult interpreted by a jvm. for a variety of reasons, but notably achievingperformance improvements, some java compilers directly generate machine code native to the platform that will execute the program. runningsuch native code can weaken system security because the java securitymodel is not designed for controlling execution of nonjava programs.early deployments of java were flawed by implementation and design bugs in the jvm, and the resulting vulnerabilities attracted considerable press attention. the absence of careful and complete definitions forthe java programming language and the jvm doubtless contributed tothe problem. the allornothing access control model in the earliest versions of java was too simple to be very useful—it was impossible to buildsystems consistent with the principle of least privilege. the securitymodel implemented by the new jdk 1.2 is richer but also more complex.jdk 1.2 programmers must now master this complexity. also, users andprogrammers must now correctly assess and configure suitable sets ofaccess rights for executing foreign code.findings1.foreign code is a growing threat to the security of most desktopsystems as well as other systems that employ cots software.trust in cyberspacecopyright national academy of sciences. all rights reserved.reinventing security1432.authenticating the author or provider of foreign code has not andlikely will not prove effective for enforcing security. users are unwillingand/or unable to use the source of a piece of foreign code as a basis fordenying or allowing execution. revocation of certificates is necessaryshould a provider be compromised, but is currently not supported by theinternet, which limits the scale over which the approach can be deployed.3.confining foreign code according to an interpreter that provides arich access control model has potential, provided programmers and usershave a means to correctly assess and configure suitable sets of accessrights.finegrained access control and application securityenforcing access control in accordance with the principle of least privilege is an extremely effective defense against a large variety of attacks,including many that could be conveyed using foreign code or applicationprograms. support for finegrained access control (fgac) facilitates thisdefense by allowing a user or system administrator to confine accessesmade by each individual software module. each module is granted access to precisely the set of resources it needs to get the job done. thus, amodule that is advertised as offering a mortgage calculator function (withkeyboard input of loan amount, interest, and duration) could be prevented from accessing the file system or network, and a spelling checkermodule could be granted read access to a dictionary and to the text filesthe user explicitly asks to have checked but not to other files.operating systems usually do provide some sort of access controlmechanism, but invariably the controls are too coarse and concern onlycertain resources.25 fgac is not supported. for example, access to largesegments of memory is what is controlled, but it is access to small regionsthat is needed. and virtually no facilities are provided for controllingaccess to abstractions implemented above the level of the operating system, including accesses that might be sensitive to the state of the resourcebeing controlled and/or the state of the module requesting the access.2625the notable exception is domain and type enforcement (dte)based operating systems(boebert and kain, 1996) that are employed in certain limited contexts. in these systems,processes are grouped into domains and are labeled accordingly. all system objects arealso given labels, which define their types. a central table then specifies the kinds ofaccesses each domain can have to each type and to each other domain. the approach,although flexible, is tedious to specify and use. to address this difficulty, extensions areproposed in badger et al. (1996).26a limited form of fgac is available for java programs running under the jdk 1.2security architecture, but statesensitive access decisions are not (easily) supported thereand the technology is limited to programs written in the single programming language.trust in cyberspacecopyright national academy of sciences. all rights reserved.144trust in cyberspacemechanisms for managing fgac solve only part of the problem,though. once fgac support is in place, users and system managersmust configure access controls for all the resources and all the modules.being too liberal in setting permissions could allow an attack to succeed;being too conservative could cause legitimate computations to incur security violations. experience with users confronting the range of securityconfiguration controls available for compartmented mode workstations,which deal with both discretionary (identitybased, userdirected) andmandatory (rulebased, administratively directed) access policies, suggests that setting all the permissions for fgac could be daunting. theproblem is only exacerbated by the alltoofrequent mismatch betweenapplicationlevel security policies, which involve applicationlevel abstractions, and the lowlevel objects and permissions constituting anfgac configuration.fgac is important, but there is more to application security thanaccess control. the lack of sound protected execution environments forprocesses limits what applications can do to protect themselves againstusers and against other applications. the fundamental insecurity of mostdeployed operating systems further undermines efforts to develop trustworthy applications: even when users are offered applications with apparent security functionality, they must question any claimed security.for example, web browsers now incorporate cryptographic mechanismsto protect against wiretapping attacks. however, the keys used are (optionally) protected by being encrypted with a userselected password andstored in a file system managed by an (insecure) operating system. thus,an attacker who can gain unauthorized access to the computer (as a resultof an operating system flaw) has two obvious options for underminingthe cryptographic security employed by the browser:•steal the file with the keys and attack it using password searching, or•plant a trojan horse to steal the key file when it is decrypted by theuser and then email the plaintext keys back to the attacker.for some applications, security properties best enforced using cryptographic means are important.27 for example, security for email entailspreventing unauthorized release of message contents, sender authentication, message integrity, and maybe nonrepudiation with proof of submission and/or receipt. and because implementing cryptographic protocolsis subtle, a number of efforts are under way to free application developersfrom this task. the ietf has developed a series of specifications for27note, however, that neither cryptography nor any other applicationlevel mechanismwill provide protection in the face of operating system vulnerabilities.trust in cyberspacecopyright national academy of sciences. all rights reserved.reinventing security145making simplified, cryptographically protected (stream or message) communications available using the generic security services application programming interface (gssapi). intel’s multilayered cdsa api aims toprovide an integrated framework for cryptography, key and certificatemanagement, and related services. cdsa has been submitted to theopen software foundation for adoption as a standard, and it has thebacking of several major operating system vendors.more generally, the applications programmer must either build suitable mechanisms or harness existing mechanisms when enforcing anyparticular application’s security policy. there will always be many moreapplications than operating systems, applications will arise and evolvemuch faster, and applications will be developed by a much wider range ofvendors. these facts of life were understood by the early advocates ofsecure operating system technology and are even truer today, due to theincreasing homogeneity of the operating system marketplace and the advent of mobile code. thus, it is easy to see why government research anddevelopment on computer security in the past focused on securing operating systems.yet these efforts have been largely unsuccessful in the marketplace.moreover, modern applications tend to involve security policies definedin terms of applicationlevel abstractions rather than operating systemones. thus, while there remains a need for security mechanisms in anoperating system, it seems clear that enforcing security increasingly willbe a responsibility shared between the operating system and the application. research is needed to understand how the responsibilities mightbest be partitioned, what operating system mechanisms are suitable forassisting in applicationlevel security implementation, and how best tospecify and implement security policies within applications.findings1.operating system implementations of fgac would help supportthe construction of systems that obey the principle of least privilege. that,in turn, could be an effective defense against a variety of attacks thatmight be delivered using foreign code or application programs.2.access control features in commercially successful operating systems are not adequate for supporting fgac. thus, new mechanismswith minimum performance impact are required.3unless the management of fgac is shown to be feasible and attractive for individual users and system administrators, mechanisms tosupport fgac will not be usable in practice.4.enforcing applicationlevel security is likely to be a shared responsibility between the application and security mechanisms that are protrust in cyberspacecopyright national academy of sciences. all rights reserved.146trust in cyberspacevided by lower levels of a system. little is known about how to partitionthis responsibility or about what mechanisms are best implemented at thevarious levels of a system.5.the assurance limitations associated with providing applicationlayer security while employing a cots operating system that offers minimum assurance need to be better understood.languagebased security:software fault isolation and proofcarrying codevirtually all operating system and hardwareimplemented enforcement of security policies has, until recently, involved monitoring systemexecution (box 4.5). actions whose execution would violate the securitypolicy being enforced are intercepted and aborted; all other actions areexecuted normally. but another approach to security policy enforcementis also plausible—execute only those programs that cannot violate thesecurity policies of interest:•by modifying a program before execution commences, it may bepossible to add checks and prevent program behavior that will violate thesecurity policy being enforced.•by analyzing a program before execution commences, it may bepossible to prove that no program behavior will violate the security policybeing enforced.both schemes depend on analysis techniques developed by programming language researchers. and both require incorporating programanalysis or some other form of automated deduction into the trustedcomputing base.the idea of program rewriting to enforce security was first proposedin connection with memory safety, a security policy stipulating thatmemory accesses (reads, writes, and jumps) are confined to specified regions of memory. the naive approach—add a test and conditional jumpbefore each machine language instruction that reads, writes, or jumps tomemory—can slow execution significantly enough to be impractical. software fault isolation (sfi) (wahbe et al., 1993) does not add tests. instead,instructions and addresses are modified (by “anding” and “oring”masks) so that they do not reference memory outside the specified regions. the behavior of programs that never attempt illegal memory accesses is unaffected by the modifications; programs that would have violated memory safety end up accessing legal addresses instead. note thatthe use of program modification to enforce security policies is not limitedto memory safety, and any security policy that can be enforced by monitrust in cyberspacecopyright national academy of sciences. all rights reserved.reinventing security147box 4.5operating system access controlconceptually, access control mechanisms divide into two subsystems, a decisionsubsystem and an enforcement subsystem. the decision subsystem examines thesecurity attributes of objects and processes according to a security policy and decides whether each particular access (e.g., read, write, execute) should be allowed;the enforcement subsystem then ensures that the decision cannot be circumventedby user or software action. see appendix g for a summary of the security attributesin some commercial operating systems.the decision subsystemdecision subsystems for discretionary access control usually employ access control lists (acls). an acl is associated with each data object and consists of a list ofusers, enumerating what accesses to the object each user is permitted to exercise.acls can be difficult to administer. expressing authorization for a large numberof users becomes awkward when it entails managing lists comprising large numbersof entries. unix systems therefore employ a modified scheme: for each object, theowner only specifies object access permissions for the user, for a small number ofspecified groups of users, and for all other users. windows nt also addresses thisadministration problem by supporting access permissions for groups.the decision subsystem for an aclbased discretionary policy simply obtains thename of the user on whose behalf a particular process is executing, checks the aclfor an entry containing that user name, and grants accesses according to the aclentry that is found. this has been called a listoriented approach.an alternative to acls is to associate with each process a list of capabilities, eachof which names an object along with the kinds of access to that object that thecapabilityholder is permitted (kain and landwehr, 1986). the decision subsystemfor a capabilitybased access control mechanism checks the list of capabilities associated with the process making the access to see if a capability is present for thedesired data object and access mode. this has been called a ticketoriented approach.the enforcement subsystemenforcement subsystems commonly operate in one of two ways. the first, oftencalled file mapping, employs a processor’s memorymanagement hardware. thedecision subsystem initializes this hardware upon the transfer of a file to active memory, and no further software actions occur. the memorymanagement hardwarethen enforces accesses. the second method (for which there is no generally accepted name), distributes enforcement throughout the elements of the operating systemthat are responsible for transferring data from passive (e.g., disk) storage to activememory and those that are responsible for performing other securitysensitive operations. many operating systems use both kinds of enforcement subsystems.trust in cyberspacecopyright national academy of sciences. all rights reserved.148trust in cyberspacetoring execution can be enforced using a generalization of sfi (schneider,1998).with proofcarrying code (pcc) (necula, 1997), a program is executedonly if an accompanying formal, machinecheckable proof establishes thatthe security policies of interest will not be violated. the approach worksespecially well for programs written in strongly typed programming languages because proof generation can then be a side effect of compilation.of course, the feasibility of automatic proof generation depends on exactly what security policy is being enforced. (proof checking, which isdone before executing a program, is, by definition, automatable. but itcan be computationally intensive.28) initial versions of pcc focused onensuring that programs do not violate memory safety or attempt operations that violate type declarations. however, in reality, the approach islimited only by the availability of proofgeneration and proofcheckingmethods, and richer security policies can certainly be handled.sfi and pcc are in their infancy. so far, each has been tried only onrelatively small examples and only on a few kinds of security policies.each presumes that an entire system will be subject to analysis, whereas,in reality, cots products may not be available in a form that enables suchprocessing. and, finally, each is limited by available technology for program analysis, a field that is still moving ahead. in short, there is a greatdeal of research to be done before the practicality and limits of theseapproaches can be assessed. some of that research involves questionsabout programming language semantics and automated deduction; otherresearch involves trying the approaches in realistic settings so that anyimpediments to deployment can be identified.sfi and pcc might well represent the vanguard of a new approach tothe enforcement of some security policies—an approach in which programming language technology is leveraged to obtain mechanisms thatare more efficient and that are better suited to the higherlevel abstractions that characterize applicationslevel security. most programmingtoday is done in highlevel typed languages, and good use might be madeof the structural and type information that highlevel languages provide.moreover, certain security policies, like informationflow restrictions, cannot be enforced by monitoring execution but can be enforced by analyzing entire program texts prior to execution. any security policies that canbe enforced by a secure operating system or by the use of hardwarememory protection can be effected by sfi or pcc (schneider, 1998).28specifically, proof checking for existing versions of proofcarrying code can be polynomial in the size of the input. proofs, in practice, are linear in the size of the program but intheory can be exponential in the size of the program.trust in cyberspacecopyright national academy of sciences. all rights reserved.reinventing security149findings1.software fault isolation (sfi) and proofcarrying code (pcc) arepromising new approaches to enforcing security policies.2.a variety of opportunities may exist to leverage programming language research in implementing system security.denial of serviceaccess control has traditionally been the focus of security mechanisms designed to prevent or contain attacks. but for computing systemsthat control infrastructures, defending against denialofservice attacks—attacks that deny or degrade services a system offers to its clients—is alsoquite important. probably of greatest concern are attacks against systemwide services (network switching resources and servers supporting manyusers), as disruption here can have the widest impact.whenever finitecapacity resources or servers are being shared, thepotential exists for some clients to monopolize use so that progress byothers is degraded or denied. in early timesharing systems, the operating system had to prevent a user’s runaway program from entirely consuming one or another resource (usually processor cycles), thereby denying service to other users. the solutions invariably involved are these:•mechanisms that allowed executing programs to be preempted,with control returned to the operating system; and•scheduling algorithms to arbitrate fairly among competing serviceand resource requests.such solutions work if requests can be issued only by agents that areunder the control of the operating system. the control allows the operating system to limit load by blocking the agents making unreasonabledemands. also implicit in such solutions is the assumption that, in thelong run, demand will not outstrip supply.29defending against denialofservice attacks in an nis is not as simple.first, in such systems, there is no single trusted entity that can control theagents making requests. individual servers might ignore specific clientrequests that seem unreasonable or that would degrade/deny service toothers, but servers cannot slow or terminate the clients making thoserequests. because the cost of checking whether a request is reasonableconsumes resources (e.g., buffer space to store the request, processing29for example, in early timesharing systems, a user was not permitted to log on if therewas insufficient memory or processing capacity to accommodate the increased load.trust in cyberspacecopyright national academy of sciences. all rights reserved.150trust in cyberspacetime to analyze the request), a denialofservice attack can succeed even ifservers are able to detect and discard attacker requests. such an attack,based on the lack of source address verification and the connectionlessnature of the user datagram protocol (udp), is the basis of cert advisory ca96.01.there is also a second difficulty with adopting the timesharing solution suggested for preventing denialofservice attacks in an nis. thedifficulty derives from the implicit assumptions that accompany any statistical approach to sharing fixedcapacity resources. in a large, highlyinterconnected system, like an nis, no client accesses many services, although most clients are able to access most of the services. server capacity is chosen accordingly, and scheduling algorithms are used to allocateservice among contending clients. but scheduling algorithms are conditioned on assumptions about offered workload, and that means that anattacker, by violating those assumptions and altering the character of theoffered workload, can subvert the scheduling algorithm. for example, anattacker might wage a denialofservice attack simply by causing a largenumber of clients to make seemingly reasonable requests. on the internet, such a coordinated attack is not difficult to launch because pcs andmany other internet hosts run operating systems that are easy to subvertand because the web and foreign code provide a vehicle for causingattack code to be downloaded onto the hosts.not all denialofservice attacks involve saturating servers or resources, though. it suffices simply to inactivate a subsystem on which theoperation of the system depends. causing such a critical subsystem tocrash is one obvious means. but there are also more subtle means ofpreventing a subsystem from responding to service requests. as discussed in chapter 2, by contaminating the internet’s domain name service (dns) caches, an attacker can inactivate packet routing and diverttraffic from its intended destination. and, in storage systems where updates can be “rolled back” in response to error conditions, it may bepossible for an attacker’s request to create an error condition that causes apredecessor’s updates to be rolled back (without that predecessor’s knowledge of the lost update), effectively denying service (gligor, 1984).findings1.no mechanisms or systematic design methods exist for defendingagainst denialofservice attacks, yet defending against such attacks isimportant for ensuring availability in an nis.2.the ad hoc countermeasures that have been successful in securingtimesharing systems from denialofservice attacks seem to be intrinsically unsuitable for use in an nis.trust in cyberspacecopyright national academy of sciences. all rights reserved.reinventing security151referencesabadi, martin, and roger needham. 1994. prudent engineering practice for cryptographicprotocols. palo alto, ca: digital equipment corporation, systems research center,june.badger, l., daniel f. sterne, david l. sherman, and kenneth m. walker. 1996. a domainand type enforcement unix prototype. vol. 9, unix computing systems. glenwood,md: trusted information systems inc.bell, d.e., and leonard j. la padula. 1973. secure computer systems: mathematical foundations and model. mtr 2547, vol. 2. bedford, ma: mitre, november.bellovin, steven m., and m. merritt. 1992. “encrypted key exchange: passwordbasedprotocols secure against dictionary attacks,” pp. 7284 in proceedings of the ieee symposium on security and privacy. los alamitos, ca: ieee compter society press.boebert, w. earl, and richard y. kain. 1996. “a further note on the confinement problem,” pp. 198203 in proceedings of the ieee 1996 international carnahan conference onsecurity technology. new york: ieee computer society.bolt, beranek, and newman (bbn). 1978. “appendix h: interfacing a host to a privateline interface,”specification for the interconnection of a host and an imp. bbn report1822. cambridge, ma: bbn, may.brewer, d., and m. nash. 1989. “the chinese wall security policy,” pp. 206214 in proceedings of the ieee symposium on security and privacy. los alamitos, ca: ieee computersociety press.brinkley, d.l., and r.r. schell. 1995.“concepts and terminology for computer security,”information security, m.d. abrams, s. jajodia, and h.j. podell, eds. los alamitos, ca:ieee computer society press.chapman, d. brent, and elizabeth d. zwicky. 1995. internet security: building internetfirewalls. newton, ma: o’reilly and associates.cheswick, william r., and steven m. bellovin. 1994. firewalls and internet security. reading, ma: addisonwesley.clark, d.d., and d.r. wilson. 1987. “a comparison of commercial and military computer security policies,” pp. 184194 in proceedings of the ieee symposium on securityand privacy. los alamitos, ca: ieee computer society press.commission on protecting and reducing government secrecy, daniel patrick moynihan,chairman. 1997. secrecy: report of the commission on protecting and reducing government secrecy. 103rd congress (pursuant to public law 236), washington, dc, march 3.computer science and telecommunications board (cstb), national research council.1996.cryptography’s role in securing the information society, kenneth w. dam andherbert s. lin, eds. washington, dc: national academy press.defense information systems agency (disa). 1996.the department of defense goal securityarchitecture (dgsa). version 3.0. 8 vols. vol. 6, technical architecture framework forinformation management. arlington, va: disa.diffie, whitfield, and martin e. hellman. 1976. “new directions in cryptography,”ieeetransactions on information theory, 22(6):644654.feustel, e., and t. mayfield. 1998. “the dgsa: unmet information security challengesfor operating systems designers,”operating systems review, 32(1):322.gligor, virgil d. 1984. “a note on denialofservice in operating systems,”ieee transactions on software engineering, 10(3):320324.gong, li, m.a. lomas, r.m. needham, and j.h. saltzer. 1993. “protecting poorly chosensecrets from guessing attacks,”ieee journal on selected areas in communications,11(5):648656.trust in cyberspacecopyright national academy of sciences. all rights reserved.152trust in cyberspacegong, li, marianne mueller, hemma prafullchandra, and roland schemers. 1997. “goingbeyond the sandbox: an overview of the new security architecture in the java development kit 1.2,” pp. 103112 in proceedings of the usenix symposium on internettechnologies and systems, monterey, california. berkeley, ca: usenix association.haller, neil m. 1994. the s/key onetime password system. morristown, nj: bellcore.joncheray, laurent. 1995. “a simple active attack against tcp,”proceedings of the 5thusenix unix security symposium, salt lake city, utah. berkeley, ca: usenix association.kain, richard y., and landwehr, carl w. 1986. “on access checking in capabilitybasedsystems,” pp. 95101 in proceedings of the ieee symposium on security and privacy. losalamitos, ca: ieee computer society press.kent, stephen t. 1997. “how many certification authorities are enough?,” computingrelated security research requirements workshop iii, u.s. department of energy,march.kneece, jack. 1986. family treason. new york: stein and day.kornfelder, loren m. 1978. “toward a practical publickey cryptosystem,” b.s. thesis,department of electrical engineering, massachusetts institute of technology, cambridge, ma.landwehr, carl e., constance l. heitmeyer, and john mclean. 1984. “a security modelfor military message systems,”acm transactions on computer systems, 9(3):198222.lewis, peter h. 1998. “threat to corporate computers often the enemy within,”new yorktimes, march 2, p. 1.menenzes, alfred j., paul c. van oorschot, and scott a. vanstone. 1996. handbook ofapplied cryptography. crc press series on discrete mathematics and its applications.boca raton, fl: crc press, october.necula, george c. 1997. “proofcarrying code,” pp. 106119 in proceedings of the 24thsymposium on principles of programming languages. new york: acm press.neuman, b. clifford, and theodore ts’o. 1994. “kerberos: an authentication service forcomputer networks,”ieee communications magazine, 32 (9):3338. available onlineat <http://gost.isi.edu/publications/kerberosneumantso.html>.schneider, fred b. 1998.enforceable security policies, technical report tr981664, computer science department, cornell university, ithaca, ny. available online at<http://cstr.cs.cornell.edu:80/dienst/ui/1.0/display/ncstrl.cornell/tr981664>.schneier, bruce. 1996. applied cryptography. 2nd ed. new york: john wiley & sons.schwartz, john. 1997. “case of the intel ‘hacker,’ victim of his own access,”washingtonpost, september 15, p. f17.sterling, bruce. 1992. the hacker crackdown: law and disorder on the electronic frontier.new york: bantam books.stoll, clifford. 1989. the cuckoo’s egg. new york: doubleday.u.s. department of defense (dod). 1985. trusted computer system evaluation criteria,department of defense 5200.28std,the“orange book.” ft. meade, md: nationalcomputer security center, december.u.s. general accounting office (gao). 1996. information security––computer attacks atdepartment of defense pose increasing risks: a report to congressional requesters. washington, dc: u.s. gao, may.van eng, ray. 1997. “activex used to steal money online,”world internet news digest(w.i.n.d.), february 14. available online at <http://www.cosmo21.com/wind/news97/w029706.htm>.trust in cyberspacecopyright national academy of sciences. all rights reserved.reinventing security153wahbe, robert, steven lucco, thomas e. anderson, and susan l. graham. 1993. “efficientsoftwarebased fault isolation,” pp. 203216 in proceedings of the 14th acm symposiumon operating systems principles. new york: acm press.war room research llc. 1996. 1996 information systems security survey. baltimore, md:war room research llc, november 21.weissman, clark. 1995. “penetration testing,”information security, m.d. abrams, s. jajodia,and h.j. podell, eds. los alamitos, ca: ieee computer society press.trust in cyberspacecopyright national academy of sciences. all rights reserved.154trust in cyberspace154it is easy to build a system that is less trustworthy than its least trustworthy component. the challenge is to do better: to build systems thatare more trustworthy than even their most trustworthy components. suchdesigns can be seen as “trustworthiness amplifiers.” the prospect that asystem could be more trustworthy than any of its components mightseem implausible. but classical engineering is full of designs that accomplish analogous feats. in building construction, for example, one mightfind two beams that are each capable of supporting a 200pound loadbeing laminated together to obtain an element that will support in excessof 400 pounds. can this sort of thing be done for trustworthiness ofcomputing components, services, and systems? for some dimensions oftrustworthiness it already has. today, many computing services areimplemented using replication, and multiple processors must fail beforethe service becomes unavailable—the service is more reliable than anysingle component processor. secrecy, another dimension of trustworthiness, provides a second example: encrypting an already encrypted text,but with a different key, can (although not always; see menenzes et al.,1997) increase the effective key length, hence the work factor for conducting a successful attack. again, note how design (multiple encryption, inthis case) amplifies a trustworthiness property (secrecy).replication and multiple encryption amplify specific dimensions oftrustworthiness. but the existence of these techniques and others likethem also suggests a new approach for implementing networked information system (nis) trustworthiness: a system’s structure, rather than5trustworthy systems fromuntrustworthy componentstrust in cyberspacecopyright national academy of sciences. all rights reserved.trustworthy systems from untrustworthy components155its individual components, should be the major source of trustworthiness.this chapter explores that theme. by pointing out connections betweenwhat is known for specific trustworthiness dimensions and what isneeded, the intent is to inspire investigations that would support a visionof trustworthiness by design. detailed descriptions of specific researchproblems would be premature at this point—too little is known. accordingly, this chapter is more abstract than the other technical chapters inthis volume. getting to the point where specific technical problems havebeen identified will itself constitute a significant step forward.replication and diversitydiversity can play a central role in implementing trustworthiness.the underlying principle is simple: some members of a sufficiently diverse population will survive any given attack, although different members might be immune to different attacks. long understood in connection with the biological world, this principle can also be applied forimplementing fault tolerance and certain security properties, two keydimensions of trustworthiness.amplifying reliabilitya server can be viewed abstractly as a component that receives requests from clients, processes them, and produces responses. a reliableservice can be constructed using a collection of such servers. each clientrequest is forwarded to a sufficient number of servers so that a correctresponse can be determined, even if some of the servers are faulty. theforwarding may be performed concurrently, as in active replication(schneider, 1990), or, when failures are restricted to more benign sorts,serially (forwarding to the next server only if the previous one has failed),as in the primary backup approach (alsberg and day, 1976).this use of replication amplifies the reliability of the components.observe that the amplification occurs whether or not the servers employed are especially reliable, provided the servers fail independently.the failureindependence requirement is actually an assumption aboutdiversity. specifically, in this context, “attacks” correspond to server failures, and failureindependence of servers is equivalent to positing a serverpopulation with sufficient diversity so that each attack fells only a singleserver. processors that are physically separated, powered from differentsources, and communicate over narrowbandwidth links approximatesuch a population, at least with respect to the random hardware failures.so, this replicationbased design effectively amplifies server fault tolerance against random hardware failures. error correcting codes, used totrust in cyberspacecopyright national academy of sciences. all rights reserved.156trust in cyberspacetolerate transient noise bursts during message transmissions, and alternativepath routing, used to tolerate router and link outages, can also beviewed in these terms—reliability is achieved by using replicas that failindependently.notice, however, that replication can diminish another aspect of trustworthiness—privacy—because replicating a service or database increasesthe number of locations where the data can be compromised (randell anddobson, 1986). use of selective combinations of secret sharing and cryptographic techniques (socalled threshold cryptography) may, in somecases, reduce the exposure (desantis et al., 1994). and replication is notthe only example in which techniques for enhancing one aspect of trustworthiness can adversely affect another.design and implementation errors in hardware or software components are not so easily tolerated by replication. the problem is that replicas of a single component define a population that lacks the necessarydiversity. this is because attacks are now the stimuli that cause components to encounter errors and, since all replicas share design and implementation errors, a single attack will affect all replicas. however, if differently designed and implemented components were used, the necessarydiversity would be present in the population. this approach was firstarticulated in connection with computer programming by elmendorf,1who called it “faulttolerant programming” (elmendorf, 1972), and subsequently it has been refined by researchers and employed in a variety ofcontrol applications, including railway and avionics (voges, 1988). however, the approach is expensive—each program is developed and testedindependently n times and by separate development teams. more troubling than cost, though, are the experimental results that raise questionsabout whether separate development teams do indeed create populationswith sufficient diversity when these teams start with the identical specifications (knight and leveson, 1986). see ammann and knight (1991) foran overall assessment of the practical issues concerning design diversity.there are circumstances, however, in which replication can amplifyresilience to software design and implementation errors. program execution typically is determined not only by input data but also by otheraspects of the system state. and, as a result of other system activity, thesystem state may differ from one execution of a given program to the next,causing different logic to be exercised in that program. thus, an error that1dionysius lardner in 1834 also pointed out the virtues of this approach to computing.see voges (1988), page 4, for the lardner quote: “the most certain and effectual check uponerrors which arise in the process of computation is to cause the same computations to bemade by separate and independent computers; and this check is rendered still more decisive if they make their computations by different methods.”trust in cyberspacecopyright national academy of sciences. all rights reserved.trustworthy systems from untrustworthy components157causes one execution of the program to fail might not be triggered in asubsequent execution, even for the same input data. experiences alongthese lines have been reported by programmers of tandem systems inwhich system support for transactions makes it particularly easy to buildsoftware that reruns programs after apparent software failures (gray andreuter, 1997). further supporting experiences are reported in huang et al.(1995), who show that periodic server restarts decrease the likelihood ofserver crashes. interestingly, it is this same phenomenon that gives rise tosocalled heisenbugs (gray and reuter, 1997)—transient failures that aredifficult to reproduce because they are triggered by circumstances beyondthe control of a tester. particularly troubling are heisenbugs that surfaceonly after a tester adds instrumentation to facilitate debugging a system.amplifying securitydiversity not only can amplify reliability, but it can also be used toamplify immunity to more coordinated and hostile forms of attack. forsuch attacks, simple replication of components provides no benefit. theseattacks are not random or independent; after successfully attacking onereplica, an attacker can be expected to target other replicas and repeat thatattack. a vulnerability in one replica constitutes a vulnerability for allreplicas, and a population of identical replicas will lack the necessarydiversity to survive. but a more diverse population—even though itsmembers might each support the same functionality—can provide a measure of immunity from attacks.the diversity necessary for deflecting hostile attacks can be viewed interms of protocols, interfaces, and their implementations. any attack willnecessarily involve accessing interfaces because attacks exploiting vulnerabilities in standard protocols can be viewed as attacks against aninterface. the attack will succeed owing to vulnerabilities associated withthe semantics of those interfaces or because of flaws in the implementation of those interfaces. different components or systems that provide thesame functionality might do so by supporting dissimilar interfaces, bysupporting similar interfaces having different implementations, or by supporting similar interfaces having similar implementations. with greatersimilarity comes increased likelihood of common vulnerabilities. for example, in unix implementations from different vendors, there will besome identical interfaces (because that is what defines unix) with identical implementations, some identical interfaces in which the implementations differ, and some internal interfaces that are entirely dissimilar. awindows nt implementation is less similar to a unix system than another unix system would be. thus, a successful attack against one uniximplementation is more likely to succeed against the other unix impletrust in cyberspacecopyright national academy of sciences. all rights reserved.158trust in cyberspacementations than against windows nt. unfortunately, realities of themarketplace and the added complexities when diverse components areused in building a system reduce the practicality of aggressively employing diversity in designing systems.findings1.replication and diversity can be employed to build systems thatamplify the trustworthiness of their components. research is needed tounderstand the limits and potential of this approach. how can diversitybe added to a collection of replicas? how can responses from a diverse setof replicas be combined so that responses from corrupted components areignored?2.research is also needed to understand how to measure similaritiesbetween distinct implementations of the same functionality and to determine the extent to which distinct implementations share vulnerabilities.monitor, detect, respondmonitoring and detection constitute a second higherlevel design approach that can play a role in implementing trustworthiness: attacks orfailures are allowed to occur, but they are detected and a suitable andtimely response is initiated. this approach has been applied both withrespect to security and to fault tolerance. its use for fault tolerance isbroadly accepted, but its role in providing security is somewhat controversial.physical plant security typically is enforced by using such a combined approach—locks keep intruders out, and alarms, video surveillance cameras, and the threat of police response not only serve as deterrents but also enable the effects of an intrusion to be redressed. thiscombined approach is especially attractive when shortcomings in prevention technology are suspected. for example, in addition to antiforgerycredit card technology and authorization codes for each transaction, creditcard companies monitor and compare each transaction with profiles ofpast cardholder activity. a combined approach may be even more costeffective than solely deploying prevention technology of sufficientstrength.limitations in detectionwhatever the benefits, the monitordetectrespond approach is limited by the available detection technology—response is not possible without detection. for example, when this approach is used for security, thetrust in cyberspacecopyright national academy of sciences. all rights reserved.trustworthy systems from untrustworthy components159detection subsystem must recognize attacks (and report them) or mustrecognize acceptable behavior (and report exceptions) (lunt, 1993). torecognize attacks, the detection subsystem must be imbued with somecharacterization of those attacks. this characterization might be programmed explicitly (perhaps as a set of patternmatching rules for someaspect of system behavior) or derived by the detection subsystem itselffrom observing attacks. notice that whatever means is employed, newattacks might go unrecognized. systems that recognize acceptable behavior employ in effect some model for that behavior. again, whether themodel is programmed explicitly or generated by observing past acceptable behavior, the detection subsystem can be fooled by new behavior—for example, the worker who stays uncharacteristically late to meet adeadline.with only approximate models to drive the detection subsystem,some attacks might not be detected and some false alerts might occur.undetected attacks are successful attacks. and with false alerts, one detection problem is simply transformed into another one, with false alertsbeing conveyed to human operators for analysis. an operator constantlydealing with false alerts will become less attentive and less likely to noticea bona fide attack. attackers might even try to exploit human frailty bycausing false alerts so that subsequent real attacks are less likely to attractnotice.any detection subsystem must gather information about the system itis monitoring. deploying the necessary instrumentation for this surveillance may require modifications to existing systems components. that,however, could be difficult with commercial offtheshelf components,since their internals are rarely available for view or modification. it alsomay become increasingly difficult if there is greater use of encryption forpreserving confidentiality of communications, since that restricts theplaces in the system where monitoring can be performed. data must becollected at the right level, too. logs of lowlevel events might be difficultto parse; keeping only logs of events at higher levels of abstraction mightenable an attack to be conducted below the level of the surveillance. afinal difficulty with using the monitordetectrespond approach to augment prevention mechanisms is its implicit reliance on prevention technology. the surveillance and detection mechanisms must be protectedfrom attack and subversion.response and reconfigurationfor the monitordetectrespond paradigm to work, a suitable responsemust be available to follow up the detection of a failure or attack.when it is failures that are being detected, system reconfiguration totrust in cyberspacecopyright national academy of sciences. all rights reserved.160trust in cyberspaceisolate the faulty components seems like a reasonable response. for systems whose components are physically close, solutions for this systemmanagement problem are understood reasonably well. but for systemsspanning a widearea network, like a typical networked information system (nis), considerably less is known. the problem is that communication delays now can be significant, giving rise to open questions abouttradeoffs involving the granularity and flexibility of the systemmanagement functions that must be added to implement reconfigurations. andthere is also the question of how to integrate partitions once they can bereconnected.when hostile attacks are being detected, further concerns come intoplay. isolating selected subsystems might be the sensible response, butknowing how and when to do so requires additional research into how todesign an nis that can continue functioning, perhaps in a degraded mode,once partitioned. having security functionality be degraded in responseto an attack is unwise though, since the resulting system could then admita twophase attack. the first phase causes the system to reconfigure andbecome more vulnerable to attack; the second phase of the attack exploitsone of those new vulnerabilities. finally, system reconfiguration mechanisms also must be protected from attacks that could compromise systemavailability. triggering the reconfiguration mechanism, for example,could be the basis for a denialofservice attack.perfection and pragmatismthe monitordetectrespond paradigm is theoretically limited by,among other things, the capabilities of the detection subsystem that itemploys. this is more of a problem for attack monitoring than for failuremonitoring. specifically, a failure detector for a given system is unlikelyto grow less effective over time, whereas an attack detector will grow lesseffective because new attacks are constantly being devised. other common defensive measures, such as virus scanners and firewalls, are similarly flawed in theory but useful nevertheless.there is nothing wrong with deploying theoretically limited solutions. what is known as “defense in depth” in the security communityargues for using a collection of mechanisms so that the burden of perfection is placed on no single mechanism. one mechanism covers the flawsof another. implicit in defense in depth, however, is a presumption aboutcoverage. an attack that penetrates one mechanism had better not penetrate all of the others. unfortunately, this coverage presumption is onethat is not easily discharged—attack detectors are never accompanied byuseful characterizations of their coverage, partly because no good characterizations exist for the space of attacks. analogous to the error bars andtrust in cyberspacecopyright national academy of sciences. all rights reserved.trustworthy systems from untrustworthy components161safety factors that structural engineers employ, security engineers needways to understand the limitations of their materials. what is needed canbe seen as another place where the research into a “theory of insecurity”(advocated in chapter 4) would have value, by providing a method bywhich vulnerabilities could be identified and their systemwide implications understood.findings1.monitoring and detection can be employed to build systems thatamplify the trustworthiness of their components. but research is neededto understand the limits and potential of this approach.2.limitations in system monitoring technology and in technology torecognize events, like attacks and failures, impose fundamental limits onthe use of monitoring and detection for implementing trustworthiness.for example, the limits and coverage of the various approaches to intruder and anomaly detection are not well understood.placement of trustworthiness functionalityin traditional uniprocessor computing systems, functionality for enforcing security policies and tolerating failures is often handled by thekernel, a small module at the lowest level of the system software. thatarchitecture was attractive for three reasons:•correct operation of the kernel—hence, security and faulttolerance functionality for the entire system—depended on no other softwareand, therefore, could not be compromised by flaws in other system software.•keeping the kernel small facilitated understanding it and gainingassurance in the entire system’s security and faulttolerance functionality.•by segregating security and faulttolerance functionality, both ofwhich are subtle to design and implement, fewer programmers with thoseskills were required, and all programmers could leverage the efforts ofthe few.whether such an architecture is suitable for building an nis seemsless clear. for such a system to be scalable and to tolerate the failure of anysingle component, the “kernel” would have to span some of the networkinfrastructure and perhaps multiple processors. and, because nis components are likely to be distributed geographically, ensuring unimpededaccess to a “kernel” might force it, too, to be geographically distributed.a“kernel” that must span multiple, geographically distributed procestrust in cyberspacecopyright national academy of sciences. all rights reserved.162trust in cyberspacesors is not likely to be small or easily understood, making alternativearchitectures seem more attractive. for example, an argument might bemade for placing security and faulttolerance functionality at the perimeter of the system, so that processors minimize their dependence on network infrastructure and other parts of the system.an effort was made, associated with the trusted network interpretation (the socalled red book) of the trusted computer system evaluationcriteria (tcsec), to extend the “kernel” concept, for the security context,from a single computer to an entire network (u.s. dod, 1987). accordingto the red book, there was a piece of the “kernel” in each processingcomponent, and communication between components was assumed to besecure. this approach was found to be infeasible for large networks oreven relatively small nonhomogeneous ones.too few niss have been built, and even fewer have been carefullyanalyzed, for any sort of consensus to have emerged about what architectures are best or even about what aspects of an nis and its environmentare important in selecting an architecture. the two extant niss discussedin chapter 2—the public telephone network (ptn) and the internet—givesome feel for viable architectures and their consequences. a proposedthird system under discussion within government circles, the socalledminimum essential information infrastructure (meii), gives insight intodifficulties and characteristics associated with specifying a sort of “kernel” for an nis. therefore, the remainder of this section reviews thesethree systems and architectures. while only a start, this exercise suggeststhat further research in the area could lead to insights that would behelpful to nis designers.public telephone networkthe ptn is structured around a relatively small number of highlyreliable components. a single modern telephone switch can handle all ofthe traffic for a town with tens of thousands of residents; longdistancetraffic for the entire country is routed through only a few hundredswitches. all of these switches are designed to be highly available, withdowntime measured in small numbers of minutes per year. control ofthe ptn is handled by a few centrally managed computers. the endsystems (telephones) do not participate in ptn management and are notexpected to have processing capacity.the use of only a small number of components allows telephonecompanies to leverage their scarce human resources. ptn technicians areneeded to operate, monitor, maintain, test, and upgrade the software inonly a relatively small number of machines. having centralized controlsimplifies networkwide load management, since the state of the systemtrust in cyberspacecopyright national academy of sciences. all rights reserved.trustworthy systems from untrustworthy components163is both accessible and easily changed. but the lack of diversity and centralization does little to prevent widespread outages. first, shared vulnerabilities and commonmode failures are more than a possibility; theyhave already occurred. second, after propagating only a short distance(i.e., through a relatively small number of components), a failure or attackcan affect a significant portion of the system.as discussed in chapter 2, the ptn maintains state for each call beinghandled. this, in turn, facilitates resource reservations per call that enable quality of service guarantees per call—a connection, once established,receives 56 kbps (kilobits per second) of dedicated bandwidth. but, establishing a connection in the ptn is not guaranteed. if a telephone switchdoes not have sufficient bandwidth available, then it will decline to processa call. consequently, existing connections are in no way affected by increases in offered load.2internetthe internet, by and large, exemplifies a more distributed architecture than the ptn. it is built from thousands of routers that are run bymany different organizations and (as a class) are somewhat less reliablethan telephone switches. control in the internet is decentralized, anddelivery of packets is not guaranteed. routers communicate with eachother to determine the current network topology and automatically routepackets, or discard them for lack of resources. the end systems (i.e.,hosts) are responsible for transforming the internet’s“best effort” serviceinto something stronger, and hosts are assumed to have processing capacity for this purpose.the reliability of the internet comes from the relatively high degree ofredundancy and absence of centralized control. to be sure, any given endsystem on the internet experiences lower availability than, for instance, atypical telephone. however, the network as a whole will remain updespite outages. no single make of computer or operating system is runeverywhere in the internet, though many share a common pedigree. diversity of hardware and software protects the internet from some commonmode design and implementation failures and contributes to thereliability of the whole. but the internet’s routing infrastructure is builtusing predominantly cisco routers, with bay and a few other companiessupplying the rest. in that regard, the internet is like the ptn, relying2if the call is declined by a switch, then the call may be routed via other switches or itmay be declined altogether by returning a busy signal to the call initiatior.trust in cyberspacecopyright national academy of sciences. all rights reserved.164trust in cyberspacelargely on switches from lucent, with nortel, siemens, and a few otherssupplying the rest.with protocol implementations installed in the tens of millions of endsystems, it is relatively difficult to install changes to the internet’s protocols. this, then, is one of the disadvantages of an architecture that depends on endsystem processing. even installing a change in the internet’srouters is difficult because of the large number of organizations involved.as discussed in chapter 2, the internet’s routers, by design, do notmaintain state for connections—indeed, connections are known only tothe end systems. different packets between a pair of end systems cantravel different routes, and that provides a simple and natural way totolerate link and router outages. the statelessness of the internet’s routers means that router memory capacity does not limit the number of endsystems nor the number of concurrently open connections. however,there is a disadvantage to this statelessness: routers are unable to offerhosts true service guarantees, and the service furnished to a host can beaffected by increases in load caused by other hosts.in addition to supporting endsystem scaling, the statelessness of theinternet helps avoid a problem often associated with distributed architectures: preserving constraints that link the states of different system components. preservation of constraints, especially when outages of components must be tolerated, can require complex coordination protocols. notethat consistency constraints do link the routing tables in each of theinternet’s routers. but these are relatively weak consistency constraintsand are, therefore, easy to maintain. even so, the internet experiencesroutingstate maintenance problems, known as “routing flaps.” (routingresponse is dampened to help deal with this problem, at the level of theborder gateway protocol.) state per connection would be much harder tomaintain because of the sheer numbers and the shortlived nature of theconnections.minimum essential information infrastructurea minimum essential information infrastructure (meii) is a highlytrustworthy communications subsystem—a network whose services areimmune to failures and attacks. the notion of an meii was originallyproposed in connection with providing support for niss that control critical infrastructures.3 the meii essentially was to be a “kernel” for many, ifnot all, niss.3according to anderson et al. (1998), the term “meii” is credited to rich mesic, a randresearcher who was involved in a series of informationwarfare exercises run by randstarting in 1995.trust in cyberspacecopyright national academy of sciences. all rights reserved.trustworthy systems from untrustworthy components165the study committee believes that implementing a single meii for thenation would be misguided and infeasible. an independent study conducted by rand (anderson et al., 1998) also arrives at this conclusion.one problem is the incompatibilities that inevitably would be introducedas nonhardened parts of niss are upgraded to exploit new technologies.niss constantly evolve to exploit new technology, and an meii that didnot evolve in concert would rapidly become useless.a second problem with a single national meii is that “minimum” and“essential” depend on context and application (see box 5.1), so one sizecannot fit all. for example, water and power are essential services. losing either in a city for a day is troublesome, but losing it for a week isunacceptable, as is having either out for even a day for an entire state. ahospital has different minimum information needs for normal operation(e.g., patient health records, billing and insurance records) than it doesduring a civil disaster. finally, the trustworthiness dimensions thatshould be preserved by an meii depend on the customer: local lawenforcement agents may not require secrecy in communications whenhandling a civil disaster but would in daytoday crime fighting.despite the impracticality of having a single national meii, providingall of the trustworthiness functionality for an nis through a “kernel”could be a plausible design option. here are likely requirements:•the“kernel” should degrade gracefully, shedding less essentialfunctions if necessary to preserve more essential functions. for example,lowspeed communications channels might remain available after highspeed ones are gone; recent copies of data might, in some cases, be used inplace of the most current data.4•the“kernel” should, to the extent possible, be able to functioneven if all elements of the infrastructure are not functioning. an exampleis the ptn, whose essential components have backup battery power enabling them to continue operating for a few hours after a power failureand without telephone company emergency generators (which might notbe functioning).•the“kernel” must be designed with restart and recovery in mind.it should be possible to restore the operation, starting from nothing, ifnecessary.note that neither the ptn nor the internet exhibits all three of thesecharacteristics, although the ptn probably comes closer than the inter4applications that depend on a gracefully degrading meii must themselves be able tofunction in the full spectrum of resource availability that such an meii might provide.trust in cyberspacecopyright national academy of sciences. all rights reserved.166trust in cyberspacebox 5.1taxonomy of applications for support by a minimum essentialinformation infrastructure•military.shortterm strategic communications and information managementneeds of the armed forces as required to operate national defense systems, gatherintelligence, and conduct operations against hostile powers.•nonmilitary federal government.communications and information needs ofthe federal government to communicate with the military and local governments, tocoordinate civil responses to natural disasters, and to direct national law enforcement against internal threats, terrorists, and organized crime.•national information and news.infrastructure required to communicate national issues rapidly to the u.s. public. current examples include national radio andtelevision networks (both broadcast and cable) and the national emergency broadcast program and national newspapers.•national power and telecommunications services.communications requiredto operate natural gas distribution, fuel distribution, the electric power distributiongrids, and the public switched telephone network at a moderate level allowing nonmilitary communication.•national economy.communications required to operate public and privatebanking systems, stock exchanges, and other economic institutions; the concept mayalso extend to social service programs, which include income distribution components.•local government. communications and information management needs ofstate and municipal governments to coordinate civil responses to natural disasters, tocommunicate with federal authorities, and to direct local law enforcement, fire, andhealth and safety personnel.•local information and news. infrastructure required to communicate localinformation to a local area rapidly. current examples include local television, radio,and newspapers.•nongovernment civil. communications and information management needsof civil institutions, such as the red cross, hospitals, ambulance services, and othercritical and safetyrelated civil institutions.•local power and telecommunications. communications required to operatelocal power grids and telephony networks at a restricted level.•local economic and mercantile. communication infrastructure required tooperate local banks, markets, stores, and other essential mercantile infrastructure.•transportation. communications infrastructure needed to manage air traffic,signaling and control infrastructure for controlling railroads, and infrastructure forautomobile traffic signaling and control of traffic congestion in cities.net.5 the development of a “kernel” exhibiting all three of the characteristics might well require new research, and an attempt to build such a“kernel” could reveal technical problems that are not, on the surface,apparent. implementing an nis using such a “kernel” could also be a5there is some question as to whether the ptn can be disconnected and then restartedfrom scratch.trust in cyberspacecopyright national academy of sciences. all rights reserved.trustworthy systems from untrustworthy components167useful research exercise, since it might reveal other important characteristics the “kernel” should possess.an alternative vision of the specification for a trustworthy “kernel” isas a computer network—hardware, communications lines, and software—that has a broad spectrum of operating modes. at one end of the spectrum, resource utilization is optimized; at the other end—entered in response to an attack—routings are employed that may be suboptimal butmore trustworthy because they use diverse and replicated routings. inthe more conservative mode, packets might be duplicated or fragmented6by using technology that is effective for communicating information evenwhen a significant fraction of the network has been compromised.7notice that for such a multimode meii implementation to be viable, itmust possess some degree of diversity. thus, there might well be a pointafter which hardening by using trustworthy components should defer todesign goals driven by diversity. second, detecting the occurrence of anattack is a prerequisite to making an operatingmode change that constitutes a defense in this meii vision. tools for monitoring the global statusof the network thus become important, especially since a coordinatedattack might be recognized only by observing activity in a significantfraction of the network.a third plausible architecture for supporting trustworthiness functionality is to use some sort of a service broker that would monitor thestatus of the communications infrastructure. this service broker wouldsense problems and provide information to restore service dynamically,interconnecting islands of unaffected parts of the communications infrastructure. for example, it might be used in commandeering for priorityuses some unaffected parts that normally operate as private intranets.findings1.attempting to build a single meii for the nation would be misguided and a waste of resources because of the differing requirements ofniss.2.little is known about the advantages and disadvantages of different nis system architectures and about where best to allocate in a systemthe responsibility for trustworthiness functionality. a careful analysis of6see, for example, rabin (1989).7note that this multimode scheme implements resistance to attacks by using techniquestraditionally used for supporting fault tolerance, something that seems especially attractivebecause a single mechanism is then being used to satisfy multiple requirements for trustworthiness. on the other hand, single mechanisms do present a common failure mode risk.trust in cyberspacecopyright national academy of sciences. all rights reserved.168trust in cyberspaceexisting systems would be one way to learn about the trustworthinessconsequences of different architectures.3.the design of systems that exhibit graceful degradation has greatpotential, but little is known about supporting or exploiting such systems.nontraditional paradigmsother less architecturally oriented design approaches have been investigated for amplifying trustworthiness properties, most notably amplifying fault tolerance. these approaches are more algorithmic in flavor.further research is recommended to develop the approaches and to betterunderstand the extent and domain of their applicability.selfstabilization, for example, has been used to implement systemservices that recover from transient failures (schneider, 1993). informally,a selfstabilizing algorithm is one that is guaranteed to return to somepredefined set of acceptable states after it has been perturbed and to do sowithout appealing to detectors or centralized controllers of any sort. forexample, some communications protocols depend on the existence of atoken that is passed among participants and empowers its holder to takecertain actions (e.g., send a message). a selfstabilizing token management protocol would always return the system to the state in which thereis a single token, even after a transient failure causes loss or duplication ofthe token. more generally, the design of network management and routing protocols could clearly benefit from a better understanding of controlalgorithms having similar convergent properties. the goal should becontrol schemes that are robust by virtue of the algorithm being usedrather than the robustness of individual components.it may also be possible to develop a science base for algorithms thatamplify resilience or other dimensions of trustworthiness by relying ongroup behavior. metaphors and observations about the nature of ournatural world—flocking birds, immunological systems,8 and crystallinestructures in physics—might provide ideas for methods to manage networks of computers and the information they contain. the design approaches outlined above—population diversity and monitordetectrespond—have clear analogies with biological concepts. studying theorganization of free markets and game theory for algorithmic contentmight be another source of ideas. of course, there are significant differences between an nis and the natural world; these differences mightrestrict the applicability of natural group behavior algorithms to niss.8with regard to the immunology metaphor, sophisticated attacks are like biologicalweapons, which have always proven effective in overcoming natural immunity.trust in cyberspacecopyright national academy of sciences. all rights reserved.trustworthy systems from untrustworthy components169for example, the actions and behaviors of natural systems arise not fromdeterministic programming but from complex, sometimes random, interactions of the individual elements. instead of exhibiting the desirablerobust behaviors, collections of programmed computers might insteadbecome synchronized or converge in unintended ways. clearly, researchis needed to establish what ideas can apply to an nis and to understandhow they can be leveraged. see anderson et al. (1998) for a discussion ofhow biological metaphors might be applied to the design of an meii.findinga variety of research directions involving new types of algorithms—selfstabilization, emergent behavior, biological metaphors—have the potential to be useful in defining systems that are trustworthy.their strengths and weaknesses are not well understood, and furtherresearch is called for.referencesalsberg, p.a., and j.d. day. 1976. “a principle for resilient sharing of distributed resources,” pp. 627644 in proceedings of the 2nd international conference on software engineering. los alamitos, ca: ieee computer society press.ammann, p.e., and j.c. knight. 1991. “design fault tolerance,” reliability engineering andsystem safety, 32(1):2549.anderson, robert h., phillip m. feldman, scott gerwehr, brian houghton, richard mesic,john d. pinder, and jeff rothenberg. 1998. a “minimum essential information infrastructure” for u.s. defense systems: meaningful? feasible? useful? santa monica, ca:rand national defense research institute, in press.desantis, a., y. desmedt, y. frankel, and m. yung. 1994.“how to share a function securely,” pp. 522533 in proceedings of the 26th acm symposium on the theory of computing. new york: acm press.elmendorf, w.r. 1972. “faulttolerant programming,” pp. 7983 in proceedings of the 2ndinternational symposium on faulttolerant computing (ftcs2). los alamitos, ca: ieeecomputer society press.gray, james, and andreas reuter. 1997. transaction processing: concepts and techniques.san mateo, ca: morgan kaufmann publishers.huang, yennun, chandra kintala, nick kolettis, and n. dudley fulton. 1995.“softwarerejuvenation: analysis, module, and applications,” pp. 381390 in proceedings of the25th symposium on faulttolerant computing. los alamitos, ca: ieee computer society press.knight, j.c., and nancy g. leveson. 1986. “an experimental evaluation of the assumption of independence in multiversion programming,”ieee transactions on softwareengineering, 12(1): 96109.lunt, teresa f. 1993. “a survey of intrusion detection techniques,”computers and security, 12(4):405418.menenzes, alfred j., paul c. van oorschot, and scott a. vanstone. 1996. handbook ofapplied cryptography. crc press series on discrete mathematics and its applications.boca raton, fl: crc press, october.trust in cyberspacecopyright national academy of sciences. all rights reserved.170trust in cyberspacerabin, m.o. 1989.“dispersal of information for security, load balancing, and fault tolerance,”communications of the acm, 36(2):335348. available online at <http://www.acm.org/pubs/citations/journals/jacm/1989362/p355rabin>.randell, b., and j. dobson. 1986. “reliability and security issues in distributed computingsystems,” pp. 113118 in proceedings of the fifth symposium on reliability in distributedsoftware and database systems. los alamitos, ca: ieee computer society press.schneider, fred b. 1990. “implementing faulttolerant services using the state machineapproach: a tutorial,”acm computing surveys, 22(4):299319.schneider, marco. 1993. “selfstabilization,”acm computing surveys, 25(1): 4567.u.s. department of defense (dod). 1987. trusted network interpretation of the trustedcomputer system evaluation criteria, ncsctg005, library number s228,526, version1, the “red book.” ft. meade, md: national computer security center.voges, udo. 1988. software diversity in computerized control systems. vol. 2 in the seriesdependable computing and fault tolerance systems. vienna, austria: springerverlag.trust in cyberspacecopyright national academy of sciences. all rights reserved.171factors that cause networked information systems (niss) to be lesstrustworthy than they might be—environmental disruption, human userand operator errors, attacks by hostile parties, and design and implementation errors—are examined in this report. in a number of instances,research and development efforts have yielded stateoftheart technological solutions that could be deployed to enhance nis trustworthiness.why are such technological solutions not used more widely in practice?some experts posit that the benefits from increased trustworthinessare difficult to estimate or trade off, and consumers therefore direct theirexpenditures toward other investments that they perceive will have moredefinitive returns. similarly, producers tend to be reluctant to invest inproducts, features, and services that further trustworthiness when theirresources can be directed (e.g., toward increasing functionality) where thelikelihood of profit appears greater. thus, there seems to be a marketfailure for trustworthiness. other factors, such as aspects of public policy,also tend to inhibit the use of existing solutions.as this report makes clear, while the deployment of extant technologies can improve the trustworthiness of niss, in many critical areas answers are not known. research is needed. most of the research activityrelated to trustworthiness involves federal government funding. (although the private sector conducts “research,” most of this effort is development that is directed toward specific products.) inasmuch as the federal government is the major funder of basic and applied research incomputing and communications, this chapter examines its interests and6the economic andpublic policy contexttrust in cyberspacecopyright national academy of sciences. all rights reserved.172trust in cyberspaceresearch emphases related to trustworthiness. certain aspects of trustworthiness (e.g., security) are historically critical areas for federal agencies responsible for national security interests. the national securityagency (nsa) and defense advanced research projects agency(darpa), both part of the department of defense (dod), have particularly influential roles in shaping research priorities and funding for trustworthiness.in this chapter, there is a greater emphasis on security than on otherdimensions of trustworthiness, because the federal government has placedtremendous emphasis on computer and communications security consistent with the importance of this technology in supporting national security activities. as the broader concept of trustworthiness becomes increasingly important, especially in light of the recent concern forprotection of critical infrastructures, increased attention to the nonsecuritydimensions of trustworthiness by the federal government may be warranted. this is not to say that attention to security is or will becomeunimportant—indeed, security vulnerabilities are expected to increase inboth number and severity in the future. additionally, the success ofsecurity in the marketplace is mixed at best, so a discussion of the reasonsfor this situation merits some attention here.this chapter begins with a discussion of risk management, whichprovides the analytical framework to assess rationales for people’s investment in trustworthiness or their failure to do so. the risk managementdiscussion leads to an analysis of the costs that consumers encounter intheir decisions regarding trustworthiness. these first two sections articulate reasons that there is a disincentive for consumers to invest in trustworthiness. producers also face disincentives (but different ones) to invest in trustworthiness, as discussed in the third section. then there is adiscussion of standards and criteria and possible roles that they may playto address the market failure problem. the important role of cryptography is explicated in chapters 2 and 4; here, the focus is on the question ofwhy cryptography is not more widely used. the federal government’smany interests in trustworthiness include facilitating the use of technology to improve trustworthiness today and fostering research to supportadvances in trustworthiness. this chapter concludes with a discussion ofthe federal agencies involved with conducting and/or sponsoring research in trustworthiness. two agencies with central roles in this arena—the nsa and darpa—are examined in some detail.risk managementthe motivation to invest in trustworthiness is to manage risks. whileit is conceivable to envision positive benefits deriving from trustworthitrust in cyberspacecopyright national academy of sciences. all rights reserved.the economic and public policy context173ness,1 the primary rationale for investment in trustworthiness is to helpensure that an nis does what people expect it to do—and not somethingelse.2 the study of risk management involves the assessment of risk andits consequences, a framework for analyzing alternatives to prevent ormitigate risks, and a basis for making decisions and implementing strategies. although there are a number of analytical tools available to assist inrisk management, each step in the process is subject to uncertainty andjudgment.risk assessmentrisk assessment differs depending on whether the emphasis is onsecurity or on safety and reliability. threat, for example, is a conceptmost commonly associated with security. threat assessment is bothspeculative and subjective, as it necessitates an evaluation of attackerintent.3 speculation is associated with vulnerability assessment, becausetheexistence of a vulnerability can be shown by experiment, but the absence of vulnerabilities cannot be shown by experiment or any other definitive means. there always exists the possibility that some aspect ofthe system can be exploited in some unexpected way. whereas securitycritical information systems have to defend against such malicious attacks, safetycritical systems typically do not.in the security arena, risk is the combination of two probabilities: first,the probability that a threat exists that will attempt to locate and exploit avulnerability; and second, the probability that the attempt will succeed.security risk assessment compounds two uncertainties—one human andone technical. the human uncertainty centers on the question, wouldanybody attack? the technical uncertainty centers on the question, if theydid, would they locate and exploit a residual vulnerability?a vulnerability, once discovered, may be exploited again and again.in the internet era, a vulnerability may even be publicized to the world in1a hypothetical example could entail the use of trustworthiness as a marketing advantage, akin to the federal express creed of “when it absolutely, positively has to be there.”2there is also the notion that some forms of business activities require or are facilitated bya particular level of trustworthiness (e.g., security as an enabler). in the electronic commerce area, as an example, the availability of secure socket layer (ssl) encryption for webtraffic has caused consumers to feel more comfortable about sending credit card numbersacross the internet, even though the real risk of credit card theft is on the merchants’ servers—and that is not addressed by ssl.3the example of residential burglary may help to clarify this point. one may suspectthrough a series of observations that one’s neighborhood has been targeted by burglars:strange cars driving slowly by, noises in the night, phone callers who hang up immediatelywhen the telephone is answered, and so on. one is only sure that burglars are operatingwhen a burglary happens––too late for any practical preventive steps to be taken.trust in cyberspacecopyright national academy of sciences. all rights reserved.174trust in cyberspacethe convenient form of an “attack script” that enables the vulnerability tobe easily exploited, even by those who are unable to understand it.4 suchbehavior means that probabilities are nonindependent in a statisticalsense. by contrast, risk assessment in the context of safety or reliability issignificantly different. risk in safety or reliability analysis is a function ofthe probability that a hazard arises and the consequences (e.g., cost) of thehazard. the most common function is the product of the two numbers,yielding an expected value. informally, risk can be thought of as theexpected damage done per unit of time that results from the operation ofa system. because the probability of failure per unit of time is nonzero,the risk is nonzero, and damage must be expected. if the estimated risk5is unacceptably high, then either design or implementation changes mustbe made to reduce it, or consideration has to be given to withholdingdeployment. but if a safety incident should occur (e.g., an accident), theprobability of a second accident remains unchanged, or may even decrease as a consequence.6a major challenge for risk management with regard to trustworthiness is the growing difficulty of differentiating attacks from incompetence and failure or lack of reliability. it is one of several factors that raisethe question of whether comprehensive probability estimation or hazardanalysis is possible.nature of consequencesattitudes and behavior depend on the nature of consequences. safetycritical information systems often control physical systems, where the4a simple example is a oneline command that may allow an individual to steal passwords. access the url <http://xxx.xxx.xxx/cgibin/phf?qalias=x%0a/bin/cat%20/etc/passwd>, substituting “xxx.xxx.xxx” with the target site of interest. for some web sites, theencrypted passwords will be returned to you. if this oneline command works, it is becausethere is a flawed version of phf in the /cgibin directory. phf allows users to gain remoteaccess to files (including the /etc/passwd file) over the web. one can run a passwordcracking program on the encrypted passwords obtained.5risk estimation is a systems engineering issue, and it involves careful, extensive, andthorough analysis of all aspects of a safetycritical system by systems engineers, safetyengineers, domain experts, and others. an important initial activity in the process is hazard analysis, an attempt to determine the hazards that would be manifested if the systemwere to fail. a hazard is a condition with the potential for causing an undesired consequence. a hazard of operating a nuclear plant, for example, would be the release of radiation into the environment. a hazard of using a medical device might be patient injury.various guidelines, procedures, and standards for carrying out hazard analyses have beendeveloped. the central issue with hazard analysis is completeness—it is very importantthat all hazards be identified if at all possible.6for example, because of greater operator diligence.trust in cyberspacecopyright national academy of sciences. all rights reserved.the economic and public policy context175consequences of failure include the possibility that lives will be threatened and/or valuable equipment may be damaged (e.g., an air trafficcontrol system). the consequences of failure of nonsafetyrelated systems include the possibility that data will be corrupted or stolen, or thatessential services will be unavailable. while the latter are serious outcomes, these consequences are not perceived to be as serious as thoseassociated with safetycritical systems. financial consequences, especiallywithin the private sector, have also attracted considerable attention because these consequences can be reasonably quantified and the implications to the financial bottom line are readily understood.7consequences are not static. consequences that are currently tolerable may become intolerable in the future. for example, as the speed ofcommunications channels continues to increase and applications are designed to rely on this speed, the availability8 of a connection may not besufficient for those applications that depend on high bandwidth and lowdelay. moreover, as applications become more dependent on quality ofservice guarantees from networks, a degradation in service may disruptfuture applications more than current ones.it is the nature of an nis that outages and disruptions of service inlocal areas may have very uneven consequences, even within the area ofdisruption. failure of a single internet service provider (isp) may or maynot affect transfer of information outside the area of disruption, depending on how the isp has configured its communications. for example,caching practices intended to reduce network congestion problems helpedto limit the scope of a domain name service (dns) outage.9 corporations that manage their own interconnection (socalled intranets) may bewholly unaffected. even widespread or catastrophic failures may notharm some users, if they have intentionally or unconsciously providedredundant storage or backup facilities. the inability to accurately predictconsequences seriously complicates the process of calculating risk andmakes it tempting to assume “best case” behavior in response to failure.a discussion about consequences must also address the questions ofwho is affected by the consequences and to what extent. while cata7in contrast to privacy, for example.8increased dependence on connections promotes attention not only to the number ofoutages but also to the length of outages. for example, a onesecond outage in a voiceconnection may require redialing to reestablish a connection; in a client/server applicationover a widearea network, it could require rebooting computers, restarting applications,and considerable other delays that yield a multiplier as compared to voice.9the master file for “.com,” a major address domain, was corrupted; however, mostsites only queried the master file for entries not in their caches. entries that were cached—and those generally included all the usual peers of any given site—were used, despite theirapparent deletion from the master file.trust in cyberspacecopyright national academy of sciences. all rights reserved.176trust in cyberspacestrophic failure garners the most popular attention, there are many dimensions to trustworthiness and consequences may involve various subsets of them with varying degrees of severity. for example, cellular telephony fraud has two principal variants approximately equal in size: creditfraud, whereby the cellular telephone owner transfers the account to asecond provider and does not pay the first; and cloning, the transfer to anew device of numbers that identify a radio and customer account. inboth cases, the service provider loses revenue. under some circumstances,a legitimate caller may be denied service if illegitimate users saturate thenetwork.10 in the case of telephone cloning, if the clone user does notsaturate the network, the provider loses revenue but users do not incur animmediate cost.11 understanding consequences is essential to formingbaseline expectations of private action and what incentives may be effective for changing private action, but that understanding is often hampered by the difficulty of quantifying or otherwise specifying the costsand consequences associated with risks.risk management strategiesrisk management strategies are approaches to managing tradeoffs.12these strategies address questions about whether it is better to add, for example, a small degree of security to a large number of products or substantialsecurity to a smaller number of specific products, to use highsecurity/lowavailability solutions or lowsecurity/highavailability ones, or to increaseassurance or the ability to identify and quarantine attackers. tradeoffs canbe made in system design and engineering; they can also be made in decidingwhether to invest in technology, procedure, insurance, or inaction.10note that the cost of denied service to the legitimate caller may far exceed the price ofthe telephone call itself. for example, a delay in requesting emergency services (e.g., a callto the fire department) may carry catastrophic costs.11however, to the extent that the cellular carrier is responsible for the resulting wirelineand longdistance charges from the telephone clone, a rise in the cellular carrier’s rates maybe forthcoming.12it is essential (1) that the actual system matches the model underlying the analysis asclosely as possible, and (2) that the failure rates achieved by system components match theestimates used in the model. the former is a systems/safety engineering issue, whereas thelatter involves all the engineering disciplines engaged in preparing the components. theprocess usually followed to achieve these two goals is in two parts: the first is carefulmanagement of the development process; the second is iterative evaluation of the systemdesign as it is developed. if changes are made for any reason, the risk estimation might berepeated. if necessary, elements of the system design can be modified to reduce the risk.for example, if a nuclear plant’s cooling system is shown to be unable to meet its dependability requirements because a particular type of pump tends to fail more often than isacceptable, then the design can be modified to include a backup pump.trust in cyberspacecopyright national academy of sciences. all rights reserved.the economic and public policy context177risk avoidance is a strategy that seeks to reduce risk to the lowestpossible value. reducing risk takes precedence over cost or effect on theoperational characteristics of the system in question. risk avoidance strategies arose in the context of highconsequence systems, such as nuclearweapon command and control or the protection of nuclear weapon stockpiles. at the time these systems were developed, there was a clear boundary between highconsequence applications and “ordinary” software—whose malfunctions could be expensive and annoying but did notthreaten human life or significant assets. with the increasing use of internet technology, this boundary is becoming blurred.the underlying assumption of risk avoidance strategies, when security is emphasized, is that there exists a highly capable threat that willexpend great effort to achieve its goals. the achievement of those goalswill involve such extreme consequences (e.g., uncommanded nuclearweapon release) that all possible effort should be devoted to preventingsuch consequences from being realized. risk avoidance strategies, ingeneral, incorporate every protection mechanism and invoke every possible assurance step. many of these assurance steps, which are discussedin detail in chapter 3, can handle only certain classes of designs or implementation technologies. when these limitations are imposed in additionto those of the rigid design guidance, the result is very often a system thatis expensive, slow to deploy, and cumbersome and inefficient to use.experience with risk avoidance strategies indicates that residual vulnerabilities will remain irrespective of the number of assurance steps taken.these vulnerabilities will often require quite exotic techniques to exploit;exotic, that is, until they are discovered by a threat or (worse yet) published on the internet.13however, the costs associated with avoiding all risks are prohibitive.thus, risk mitigation is more typical and is generally encountered whenmany factors, including security and reliability, determine the success ofa system. risk mitigation is especially popular in marketdriven environments where an attempt is made to provide “good enough” security orreliability or other qualities without severely affecting economic factorssuch as price and time to market. risk mitigation should be interpretednot as a license to do a shoddy job in implementing trustworthiness, butinstead as a pragmatic recognition that tradeoffs between the dimensionsof trustworthiness, economic realities, and other constraints will be thenorm, not the exception. the risk mitigation strategies that are most13some exotic strategies require specialized hardware or physical access to certain systems, whereas other exotic strategies may require only remote access and appropriate software to be executed. it is this latter class of strategies that is particularly susceptible todissemination via the internet.trust in cyberspacecopyright national academy of sciences. all rights reserved.178trust in cyberspacerelevant to trustworthiness can generally be characterized according totwo similar models:•the insurance model. in this model, the cost of countermeasures isviewed as an “insurance premium” paid to prevent (or at least mitigate)loss. the value of the information being protected, or the service beingprovided, is assessed and mechanisms and assurance steps are incorporated up to, but not exceeding, that value.•the work factor model. a definition in cryptology for the term “workfactor” is the amount of computation required to break a cipher through abruteforce search of all possible key values.14 recently, the term hasbeen broadened to mean the amount of effort required to locate andexploit a residual vulnerability. that effort may involve more efficientprocedures rather than exhaustive searches. in the case of fault tolerance,the assumptions made about the types of failures (benign or arbitrary)that could arise are analogous to the concept of work factor.the two models are subject to pitfalls distinctive to each and somethat are common to both. in the insurance model, it is possible that thevalue of information (or disruption of service) to an outsider is substantially greater than the value of that information or service to its owners.thus, a “high value” attack could be mounted, succeed, and the “insurance premium” lost along with the target data or service. such circumstances often arise in an interconnected or networked world. for example, a local telephone switch might be protected against deliberateinterruption of service to the degree that is justified by the revenue thatmight be lost from such an interruption. but such an analysis ignores theattacker whose aim is to prevent a physical alarm system from notifyingthe police that an intrusion has been detected into an area containingvaluable items. another example is an instance in which a hacker expends great effort to take over an innocuous machine, not because itcontains interesting data but because it provides computing resourcesand network connectivity that can be used to mount attacks on highervalue targets.15 in the case of the work factor model, it is notoriouslydifficult to assess the capabilities of a potential adversary in a field asunstructured as that of discovering vulnerabilities, which involves seeingaspects of a system that were overlooked by its designers.14if the cryptography is easily broken (e.g., because the keys are stored in sharedmemory), the work factor may be almost irrelevant.15a specific example of this comes from the early days of electromechanical cryptosystems.at that time, governments typically deployed an array of different cryptosystems of differentstrengths: simple (and easier to break) cryptosystems for less sensitive data, and elaboratetrust in cyberspacecopyright national academy of sciences. all rights reserved.the economic and public policy context179selecting a strategyrisk management seeks to provide an analytical framework for deciding how close to the edge one dares to go. risk avoidance carries withit the danger of overengineering to the point at which the system is neverused. risk mitigation carries with it the danger of underengineering tothe point at which the system is defeated, very possibly over and overagain. the compound uncertainties of risk management preclude anyrigorous method, but it is possible to articulate a few guidelines:•understand how long the system will be used in harm’s way.threats are not static; they become more capable over time, through therelease of oncesecret information from disgruntled former employeesand other sources, access to onceesoteric equipment, and through othermeans.16•assess how much work is needed to exploit a known residualvulnerability. does the attack require specialized equipment? is this thesort of equipment that will drop drastically in cost over the next fewyears? is it the sort of equipment that is freely accessible in open environments such as universities? does the attack require a level of physicalaccess that can be made hard to achieve?•context is extremely important. it is necessary to understand howthe system might be used, how it is connected to or interacts with othersystems, and how it might be exploited in the course of attacking something else.•can the systemsupport infrastructure react to vulnerabilities? aresystem updates possible, and if so, at what cost? how many instances ofelectromechanical devices to encipher highly sensitive data (called, respectively, “lowgrade” and “highgrade” systems). this approach can be looked at as a riskmitigationstrategy, on either the insurance or work factor model, depending on how the decision ofwhich system protected which data was used. only security that was “good enough” wasimposed. what the designers of these systems were slow to realize, however, was that thehighgrade systems (e.g., the german enigma machine) were vulnerable to “knownplaintext” attacks where the cryptanalyst was able to match unenciphered and encipheredcharacters and thereby recover the key that deciphered other, previously unknown, messages. the nature of military and diplomatic communication is such that much text is “cutand pasted” from innocuous messages to more sensitive ones. breaking the lowgradeciphers then provided the “known plaintext” that facilitated attacks on the highgrade ciphers.16the socalled “cloning” attack, which is responsible for a large percentage of cellularfraud today, was at one time understandable only by a small handful of electronic engineers and required expensive, custommade equipment. today that attack is embodied inclandestine consumer products and can be mounted by any individual with the will and afew hundred dollars. the will has increased for many because there are more targets:highuse areas make listening for identification numbers more feasible.trust in cyberspacecopyright national academy of sciences. all rights reserved.180trust in cyberspacethe system will be deployed and how widely are they dispersed? is therea mechanism for security recalls?17 can the infrastructure continue critical operations at a reduced and trusted level if attacked?the difficulties of anticipating and avoiding most risks can lead tostrategies that emphasize compensatory action: detecting problems andresponding to minimize damage, recovering, and seeking redress in somecircumstances. the difficulty with this approach is the implicit assumption that all attacks can be identified. anecdotal reports of success by“tiger teams” seeking to compromise systems suggest that detection maycontinue to be a weak vehicle for the future.18findings1.security risks are more difficult to identify and quantify than thosethat arise from safety or reliability concerns. safety and reliability risks donot involve malice; the tangible and often severe consequences may oftenbe easily articulated. these considerations facilitate the assessment of riskand measurement of consequences for safety and reliabilityrelated risks.2.although a riskavoidance strategy may maximize trustworthiness,the prohibitive cost of that strategy suggests that risk mitigation is thepragmatic strategy for most situations.3.consequences may be uneven and unpredictable, especially forsecurity risks, and may affect people with varying levels of severity.safetyrelated consequences are generally perceived to be more seriousthan other consequences.consumers and trustworthinessthe spending decisions made by consumers have a profound impacton the trustworthiness of niss. the consumers of trustworthiness may bepartitioned into two groups: information system professionals, who acton behalf of groups of relatively unsophisticated users, and the generalpublic. information system professionals often have only a modest understanding of trustworthiness because of the limited attention devoted17for example, in gsm cellular phones, the security algorithms are embedded in persubscriber smart cards and in a small number of authentication stations. this permits therelatively easy phaseout of an algorithm that has been cracked, although it remains to beseen whether providers will indeed replace the comp128 algorithm. see <http://www.isaac.cs.berkeley.edu/isaac/gsm.html> for details.18for example, consider the success of the “eligible receiver” exercise in which a team of“hackers” posing as paid surrogates for north korea could have disabled the networkedinformation systems that control the u.s. power grid (gertz, 1998).trust in cyberspacecopyright national academy of sciences. all rights reserved.the economic and public policy context181to trustworthiness within college curricula and professional seminars.even information system professionals who concentrate on security issues vary greatly in their understanding of issues associated with trustworthiness.19 the larger group of consumers is the general public, mostlyunsophisticated with respect to trustworthiness despite a growing familiarity with information technology in general. the rise of an informationsystems mass market during the last two decades, and the concomitantinflux of unsophisticated users, exacerbates the asymmetric distributionof understanding of trustworthiness concerns.consumer costsconsumer costs include all costs associated with trustworthiness thatare borne by the user. some of these costs are associated with the prevention or detection of breaches in trustworthiness; other costs are related torecovery from the effects of inadequate trustworthiness. consumer costsinclude expenditures for the acquisition and use of technology, the development and implementation of policies and practices, insurance, legalaction, and other activities. consumer costs may be divided into directcosts, indirect costs, and failure costs.direct costsdirect costs are those expenditures that can be associated unambiguously with trustworthiness. this category includes the purchases of products such as firewalls or antivirus software. sometimes, direct costs mayrepresent the incremental cost for products that offer superior trustworthiness compared with alternatives (e.g., faulttolerant computers). services may also be categorized as direct costs, as in the case of maintaininghot sites,20 consulting and training to improve operational practices, analyzing system audit data, or upgrading hardware to improve reliability.direct costs vary widely, depending on the requirements of the consumer. historically, specialized users have had the most demanding requirements and incurred the most costs; the canonical example is themilitary, but other institutions such as banking, air traffic control systems,and nuclear power facilities also have exacting requirements for security,safety, and reliability. the direct costs relative to trustworthiness are19this conclusion was derived from discussions at several committee meetings.20hot sites are physical locations where an organization may continue computer operations in the case of a major disruption, such as an earthquake that renders the normaloperating site largely unusable. organizations may maintain their own hot sites or maycontract for this service with specialty firms.trust in cyberspacecopyright national academy of sciences. all rights reserved.182trust in cyberspaceoften incurred by central information service units rather than charged toindividuals or user departments, because the costs involve systemwidecharacteristics that cannot be apportioned easily among users.indirect coststhe implementation of measures to improve trustworthiness oftenentails costs beyond those that are obvious and immediate. for example,the implementation of cryptography requires increased central processing unit (cpu) power21 and probably communications resources. theintroduction of trustworthiness improvements also often increases system complexity (e.g., the implementation of security controls), therebycausing users to require additional technical support for problems thatthey otherwise might have been able to resolve themselves. changes tocomplex systems increase the possibilities for bugs and, correspondingly,the costs for system maintenance and troubleshooting. unintended consequences may also result from changes to complex systems, because it isvirtually impossible to understand and anticipate all of the ramificationsof changes. while it is attempting to improve aspects of trustworthiness,an intervention may introduce new vulnerabilities.an important indirect cost is often attributable to the “hassle factor.”efforts to improve trustworthiness seldom simplify the use of a systemfor a consumer. for example, security controls may compel users to takeadditional steps and time to log in and access information and remembermore elaborate policies and practices.another form of indirect cost is incurred when an element of trustworthiness prevents the consumer from performing some important function. in some cases these costs can be substantial, such as when a securitymechanism denies a physician remote access to the medical records of anemergency patient injured when traveling, or when a flight control system prevents a pilot from moving controls in a particular way during anairborne emergency not anticipated by the design team. such examplesillustrate the difficult balance between overengineering in an attempt toprevent adverse consequences and underengineering in an attempt toavoid monetary and convenience costs.21most desktop pcs and workstations have ample cpu capacity most of the time for dataencryption. this is not true for servers and other multiuser machines. in any case, publickey operations are expensive on all platforms. servers are, in general, multitasking machines; cpu power spent encrypting one user’s traffic is not available to process anotheruser’s queries. furthermore, servers often need their highspeed network interfaces tohandle the aggregate demand from many users. ubiquitous use of softwarebased encryption would indeed cause noticeable degradation in total throughput; thus, many servers arebeing equipped with cryptographic hardware.trust in cyberspacecopyright national academy of sciences. all rights reserved.the economic and public policy context183failure costsfailure costs arise when the failure or absence of a trustworthinessmechanism permits some adverse outcome to occur, such as loss of service, fraud, sabotage, or the compromise of sensitive information. forexample, billing data provide a relatively good indicator of telecommunications fraud, which seems to show a bimodal distribution: a small number of extremely large thefts of service and a large number of small incidents.22 theft of notebook computers and other devices, a rapidlyincreasing form of corporate security exposure,23 illustrates a differentkind of denial of service.another kind of failure cost is associated with recovery. perceivedgrowth in those costs is motivating growth in the market for insuranceagainst computerrelated (and telecommunicationsrelated) mishaps. although that market remains immature,24 recent developments have suggested growing interest among insurers.25 traditional commercial insurance frameworks intended for physical property, equipment, and liabilityare being adapted for electronic contexts, although the difficulties in valuing information assets, diagnosing and reporting problems, and lack ofhistorical data have constrained the growth of computer and telecommunicationsrelated insurance. insurance demand appears to be growingwith loss experience, including losses arising from legal actions precipitated by information systems problems, and with increased attention toinformation systems in auditing and, where applicable, regulatory oversight. although insurance can provide a negative incentive (“moral hazard”) to the extent that its presence discourages greater effort in preventing loss, the terms and conditions of coverage may be designed to limitpayment to those circumstances where some preventive action, such asthe use of code signing,26 was taken.some consumers prefer to insure themselves. instead of purchasingan insurance policy, a consumer could make provisions for disaster recovery, either directly or through a thirdparty contractor. another alternative is inaction. a consumer could react to incidents after the fact andinitiate whatever action is deemed to be necessary. this would be consis22committee discussion with michael diaz and bruce fette of motorola, september 19,1997.23for example, see masters (1998).24personal communication, vincent “chip” boylan, executive vice president of hilb, eogaland hamilton company, september 1997.25in april 1998, lloyds of london initiated coverage for firms to protect against hackers,viruses, and computer sabotage. see lemos (1998).26the need for evidence may help to motivate such approaches as code signing (as discussed in chapter 4): signing mobile code does not provide security; it provides a basis fora value judgment about potential trustworthiness of code based on reputation.trust in cyberspacecopyright national academy of sciences. all rights reserved.184trust in cyberspacetent with consumer behavior in analogous areas (e.g., home security). it isoften stated that most residential alarm sales occur after a home has beenburgled, either the home of the purchaser or a neighbor’s home.the failure costs discussed so far are those costs that affect a specificconsumer (e.g., the operator of an nis that runs an electric utility). asystem failure resulting from a breach in trustworthiness has costs for thepublic at large. an electric outage may interrupt the conduct of business(and result in possible loss of revenue) and inconvenience the public.such costs are not borne by the service provider, the electric utility in thisexample, or the suppliers of any part of an nis (because the conventionalpractice in the information technology industry is to disclaim all liabilitiesthat may arise for any reason).imperfect informationconsumers operate within an environment in which a great deal isunknown. the benefits deriving from greater reliability, availability, orsecurity are difficult to articulate in detail, much less to quantify. moreover, the consequences of inadequate trustworthiness are difficult to articulate in detail and quantify as well. there is a reluctance to make dataabout incidents and consequences publicly available,27 so whatever dataare available are likely to represent a biased sample. not surprising, then,is the observation that relatively little information on trustworthiness isreadily available to consumers. economists refer to this state of affairs as“imperfect information,” which distorts market transactions because under high levels of uncertainty, consumers will tend to purchase less of agiven product or service than they otherwise would.the difficulty of assessing the environment is compounded by thedifficulty of assessing a technically complex system. most buyers are notknowledgeable about the technical aspects of trustworthiness and, therefore, cannot conduct the informed assessment that is needed for sounddecision making. other industries, such as pharmaceuticals, have comparable characteristics, but have resolved the problem by requiring the development and disclosure of information through regulatory mandate. aconsumer may not be able to assess accurately whether a particular drugis safe but can be reasonably confident that drugs obtained from approved sources have the endorsement of the food and drug administra27the reluctance to make such data publicly available is intended to minimize the publicperception and awareness that systems are vulnerable and have been breached. the lack ofdata about the likelihood, actual incidence, and consequences of problems is not a newobservation; it was emphasized in computers at risk (cstb, 1991) and the pccip report(pccip, 1997).trust in cyberspacecopyright national academy of sciences. all rights reserved.the economic and public policy context185tion (fda), which confers important safety information.28 computer system trustworthiness has nothing comparable to the fda. the problem isboth the absence of standard metrics and a generally accepted organization that could conduct such assessments. there is no consumer reportsfor trustworthiness.29metrics can be reasonably defined for some dimensions of trustworthiness (e.g., availability), while other dimensions (e.g., security) seemingly defy straightforward characterization. any metric must be definedwith respect to some formal model. the act of defining a model, however,suppresses details that might constitute vulnerabilities. for example, a“workfactor” metric for cryptosystems could be characterized by howmuch computation an attacker must perform to enumerate and check allpossible keys for a given piece of encrypted text. the metric does notconsider clever attacks and thereby renders the workfactor metric to beof dubious practical value.30 whatever formal model is conceived cannotinclude all possible modes of attack, because some attacks may not evenhave been invented. since the definition of security metrics is problematic, the definition of aggregate trustworthiness metrics must necessarilybe problematic as well.how much risk is assumed knowingly is unclear. anecdotal evidencesuggests that in sectors accustomed to assessing and managing risk such asbanking, buyer decision making relating to trustworthiness may be moreexplicit. banking representatives suggested to this committee31 and to federal study groups recently (e.g., the president’s commission on criticalinfrastructure protection, pccip) that at least some choices about using theinternet in their business reflected risk assessment. other testimony to thecommittee underscored that even in the military, pursuing the primarymission may result in compromises of trustworthiness: as one representative of the dod observed,32 one cannot necessarily shut down communica28the situation might be worse for information systems than for pharmaceuticals. thepharmaceutical interface is defined by a chemical that may be more readily understoodthan software, and the testing of the interaction between a chemical and the human bodymay be more straightforward than that for an information system. the issues here fallwithin a larger class of risk regulation concerns. roger noll, an economist at stanforduniversity, has described the uncertainties that confound citizens and government officialsand the benefits of better identifying risks and effective responses to them. see noll (1996).29the international computer security association does “certify” securityoriented products and services, but so far its testing does not appear to be rigorous.30consider monoalphabetic ciphers, which are sufficiently simple to solve by hand thatthey are the basis for daily puzzles in some newspapers. such a cipher has a key lengthequivalent to about 80, far above what is currently considered exportable. one does not solvesuch a cipher by an exhaustive search of the key space. more powerful techniques are used.31during the committee’s first workshop, in october 1996.32during the committee’s first workshop, in october 1996.trust in cyberspacecopyright national academy of sciences. all rights reserved.186trust in cyberspacetions in the battlefield simply because security is breached. it is possiblethat compromised communication is preferred to the absence of all communication in some contexts.security experts and others who are knowledgeable about the variousdimensions of trustworthiness often argue that consumers spend too littleon trustworthiness because of imperfect information.33 limited actualexperience with loss also tends to discourage investments in trustworthiness.34 of course, limited actual experience is not equivalent to an absence of risk. some losses or problems may not even be visible, and mostpeople have not experienced a catastrophe.issues affecting risk managementconsumers are sensitive to the perceived opportunity cost from notindulging in risky behavior. the movement toward lowinventory, justintime production in various industries; outsourcing of a variety of inputs to production of goods and services; and direct computermediatedinteraction with actual and potential buyers, suppliers, partners, and competitors is motivated by factors deemed essential to commercial vitality:reduction of costs, rapidity of time to market, and responsiveness to customers. the opportunity cost of not relying more on information systemsmay be not being in business.35the combination of more open networking environments (e.g., theinternet) and more direct electronic transactions implies greater automated interactions among organizations. this increasing level of automated interactions is expected to result in increasing demand for majorbusiness automation systems such as peoplesoft and sap. how suchinteraction can proceed in a trustworthy manner and how differencesamong policies and preferences across organizations can be negotiatedand arbitrated are among the questions now emerging.36 one technolo33current tax treatment of software, databases, and other information assets reinforceand contribute to what many feel is a tendency to undervalue information assets relative tophysical assets; difficulties in appraising value for associated “property” also contributes toslow and uneven growth of insurance coverage for inadequate trustworthiness.34for example, in 1997, the council on competitiveness hosted a workshop for the presidential commission on critical infrastructure protection on education and training issuesrelating to development and use of critical systems. a theme of the discussion was thatcorporate security officers and academic experts found little interest in or motivation forincreasing trustworthiness by good practice. the pccip report emphasized shortcomingsin awareness in its findings and recommendations.35see computer science and telecommunications board (1994).36the intelligence community once had a marking (orcon) that means “originatorcontrolled.” essentially, this marking states, “i pass this to you but i don’t want you totrust in cyberspacecopyright national academy of sciences. all rights reserved.the economic and public policy context187gist with diverse industry experience made an analogy to the spread ofaids, noting new concerns about the trustworthiness of the people whoconstitute one’s social network and the dire consequences that could result from the indiscriminate expansion of one’s contacts.37another important factor for consumer risk management is the continuing growth in computerbased interaction and interdependenceamong individuals and organizations—the rise of a cyberspace economyand society. greater communication among dispersed parties and collaboration and support for access for those who are mobile or in unconventional locations are easy extrapolations from current conditions. increasingly, fewer assumptions can be made about whose information orsoftware is running at a given time on a particular hardware, software,and communications platform. a future of greater decentralization hasimportant implications for the locus of control for information and systems. the concepts of control inherent in traditional approaches to security, reliability, and safety may be less and less applicable during thecoming years. in contrast to established niss, where users are oftenpreselected in some way (e.g., bank automated teller machines or the airtraffic control system), new participants increasingly will include anybody who requests access. furthermore, some of these new users will beinvolved in shortlived and spontaneous interactions, a situation that willcreate more concerns for ensuring trustworthiness.among the various nearterm issues, the year 2000 (y2k) problem hasfostered examination and in a variety of instances changes in informationsystems. the publicity associated with y2k may well influence some ofthe decision making; there is more speculation than data about the natureand number of changes being made, which range from focused fixes tomore wholesale change.38 another relatively nearterm influence is theintroduction of the european currency unit (ecu),39 which is promptinglarge banks and possibly other entities to alter systems to support the newcurrency and the likely demise of other currencies over time. the timepass it on to anybody else without my permission.” commercial nondisclosure agreementsalmost uniformly contain similar clauses. this simple and easily understood policy hasproved resistant to any kind of technical enforcement in shared computer systems exceptby mechanisms so draconian that no one will put up with them. however, schemes toprotect intellectual property seem to be raising the issues again as people explore controlsnot only on passing something along but also on the potential number of people involvedand under what conditions.37william flanagan, during the committee’s third workshop, in september 1997.38see <http://www.2ktimes.com/y2kpaper.htm> for articles, news clips, and other reports about y2k. see also de jager (1993) and clausing (1998).39according to the terms of the european monetary union, the ecu will become theeuro on january 1, 1999 (cummins, 1998).trust in cyberspacecopyright national academy of sciences. all rights reserved.188trust in cyberspacepressures associated with y2k and the ecu phenomena illustrate howbusinesses scramble to solve problems, even though these problems couldhave been anticipated well beforehand. moreover, businesses are unlikelyto apply relevant extant knowledge to their problems.40 these pressuresalso foster shifts from custom solutions to selection of recognized, majorthirdparty software systems, such as sap, thereby contributing to the increasing popularity of commercial offtheshelf (cots) software but inhibiting diversity, which can lead to commonmode failures and shared vulnerabilities.some market observationsthe demand for primary functionality—the main purpose of a computing or communications device or system—continues to grow and isfueling demand for features. when confronted with a choice of where tospend an extra dollar, buyers tend to emphasize primary functionality;this is as evident in requests for proposals (rfps) and actual procurementfrom the dod as in the consumer or general business marketplace. somelevel of trustworthiness is deemed to be essential and after that level,trustworthiness becomes a secondary differentiator. even where thetradeoff may not be obvious, perceived needs to contain costs result indevelopment and acquisition of systems that minimize redundancy, diversity, and other features that might otherwise enhance trustworthiness.products that address problems experienced by consumers have beenwell received, as are products (e.g., firewalls) that appear to address specific wellknown problems. consumers buy firewalls because they haveassociated that mechanism with the ability to connect to the internet, eventhough considerable risks may remain despite the use of firewalls. someconsumers who have full knowledge of the limited effectiveness of mechanisms such as firewalls may still use them with the goal of appearing tohave trustworthiness, but without undertaking the hard work that achieving true trustworthiness demands; this may be the era of patent medicines for information technology.the development of the mass market has been accompanied by a shiftin systems development and expertise from user organizations to vendors. the proliferation and falling relative prices for commercial technology means that organizations that once would develop systems theywanted themselves are more likely to buy at least components if not entiresystems.41 this trend toward cots systems and an increasing homoge40william flanagan, during the committee’s third workshop, in september 1997.41at the committee’s workshop in september 1997, iang jeon of liberty financial, forexample, observed that up until 3 to 4 years earlier financial institutions had to set uptrust in cyberspacecopyright national academy of sciences. all rights reserved.the economic and public policy context189neity of computing platforms, communications infrastructure, and software is discussed in the next section as a major force in the producerlandscape.findings1.the costs associated with improved trustworthiness are often incurred by central units of an organization because such costs reflectsystemwide characteristics of an nis and cannot be easily apportioned.2.one important cost of greater trustworthiness is related to the “hasslefactor.” trustworthy systems tend to be more cumbersome to use. this isone reason that costs for the consumer are not equivalent to price.3.decision making about trustworthy systems occurs within the context of imperfect information, which increases the level of uncertaintyregarding the benefits of trustworthiness initiatives and therefore servesas a disincentive to invest in trustworthiness, thus distorting the marketfor trustworthiness. the absence of standard metrics and a recognizedorganization to conduct assessments of trustworthiness is an importantcontributing factor to the problem of imperfect information. in someindustries, such as pharmaceuticals, regulatory mandate has resolved thisproblem by requiring the development and disclosure of information.4.useful metrics for the security dimension of trustworthiness areunlikely to be developed because the corresponding formal model for anyparticular metric is necessarily incomplete. therefore, useful aggregatemetrics for trustworthiness are not likely to be developed either.5.the combination of more open and decentralized networking environments and an increasing use of electronic communications and transactions suggests an increasing demand for major business automationsystems. this continuing decentralization may render less and less applicable the concepts of control inherent in traditional approaches to security, reliability, and safety. in particular, there will be an increasing needfor more individuals to be able to make trustworthiness judgments on anad hoc, realtime basis.6.other things being equal, consumers prefer to purchase greaterfunctionality rather than improved trustworthiness. products that address problems that have been experienced by consumers or are perceived to address specific wellknown problems have been well received.software and telecom systems themselves to support electronic distribution, whereas nowit is easier to rely on people whose business is developing packaged software and delivering telecommunications services.trust in cyberspacecopyright national academy of sciences. all rights reserved.190trust in cyberspaceproducers and trustworthinessthe larger marketplace and the trend toward homogeneitybefore the producers of trustworthiness products, services, and features are discussed, a brief note is warranted on the important trendsconcerning cots components and homogeneity in the general marketplace, and the implications of those trends for trustworthiness. currentcomputing platforms, as well as communications infrastructure and software, are generally homogeneous. operating systems and computingplatforms are dominated by microsoft windows and the intel x86 compatible processor family.42 secondary characteristics—display, networkinterfaces, disks—are made uniform by the adoption of technological standards (e.g., vga graphics interface or ide and scsi disk interfaces) or arepresented to application software as common interfaces by operating systems software in the form of device drivers and hardware adaptationlayers.the communications infrastructure today is also fairly homogeneous.local area networks are typically ethernets or token rings, althoughsome increased diversity is being introduced by asynchronous transfermode (atm) networks and the various highspeed ethernets. wide areanetworks are constructed from routers, most of which are sold by a fewmanufacturers.43 the software that controls these networks is also homogeneous at multiple levels. a single stack of protocols manages the internet, and all the internet protocol implementations descend from a few.the core internet protocol (ip) works well over a diverse set of networktechnologies, further contributing to homogeneity.in addition to the existing state of relative homogeneity with respectto computing platforms and communications, the important trends insoftware suggest a continuing decrease in heterogeneity in the comingyears. an important reason for this decrease in heterogeneity is the risingpopularity of cots software that is driven by cost considerations andrisk reduction, insofar as cots products are known entities and readilyavailable. scripting languages and cots software provide the context42in 1997, a significant majority of computer systems sold (85 percent of personal computers and servers by unit volume) contained some version of intel’s“x86” microprocessor(manufactured by either intel corporation or one of a small number of others) to implement an ibmcompatible pc architecture. when deployed as personal computers, a significant majority are running a version of the microsoft windows operating system. less than10 percent of personal computers are a variant of the architecture designed and sold byapple computer; a small percentage are variant architectures made by sun microsystems,silicon graphics, digital equipment corporation, and others. many among this last groupof systems run versions of the unix operating system.43cisco systems and bay networks, for example, dominate the router market.trust in cyberspacecopyright national academy of sciences. all rights reserved.the economic and public policy context191for the reuse of components and for their assembly into required configurations, with only limited new programming required for custom components. consequently, user organizations have less need for systems development expertise. the success of large middleware packages underscoresthe economic and other benefits that users perceive in cots software.the continued use of sap, the web (e.g., hypertext transfer protocol[http]), and a few other software packages favor particular softwarecomponents, data formats, work flows, and vocabularies.risks of homogeneitythe similarity intrinsic in the component systems of a homogeneouscollection implies that these component systems share vulnerabilities. asuccessful attack on one system is then likely to succeed on other systemsas well—the antithesis of what is desired for implementing trustworthiness. moreover, today’s dominant computing and communications environments are based on hardware and software that were not designedwith security in mind; consequently, these systems are not difficult tocompromise, as discussed in previous chapters.there is, therefore, some tension between homogeneity and trustworthiness. powerful forces make technological homogeneity compelling(see box 6.1), but some attributes of trustworthiness benefit from diversity (see chapter 5). on the other hand, a widely used trustworthy operating system might be superior to a variety of nontrustworthy operatingsystems; diversity, per se, is not equivalent to increased trustworthiness.box 6.1the rationale for homogeneitythe existence of a homogeneous computing and communications environmentis not an accident. strong forces favor homogeneity:•homogeneity is advantageous for the sale and use of popular software. alarger market gives providers of hardware and software incentives for entry, andproviders can also exploit economies of scale.•enormous leverage results when computers can communicate and share data,especially in ways that are not anticipated when the computers are procured or thedata are created. homogeneity simplifies interoperability between systems.•homogeneity supports more efficient transfer of skills within organizations,effectively lowering the cost of computerizing additional functions.•homogeneity also leads to increased skilllifetimes, because a skill is likely toremain useful even after computing platforms are upgraded.•homogeneity enables aggregations of resources to strengthen design, implementation, and testing.trust in cyberspacecopyright national academy of sciences. all rights reserved.192trust in cyberspacetechnological convergence may also be realized through the marketdominance of a few suppliers of key components, with monopoly as thelimit case when technological homogeneity is dictated by the monopolist.44 however, the number of suppliers could grow as a result of thediffusion of computing into embedded, ubiquitous environments; the diversification and interoperability of communications services; and thecontinued integration of computing and communications into organizations within various market niches.producers and their costsinsofar as trustworthiness is integral to the design of informationtechnology products and services, trustworthiness should be pervasivethroughout the marketplace for such products and services. however,trustworthiness is often considered only after a system is implemented,so there are firms that develop and market products and services specifically targeted at improving the trustworthiness of operational niss. themarketplace for trustworthiness—in both of these senses—will be explored in some detail after some of the key issues associated with the costsof producing trustworthiness are discussed.the costs of trustworthiness are difficult to assess and cannot all bequantified, even using orderofmagnitude estimates. time is a major“currency” cited by vendors, who worry about time from product concept until commercial release. data on relevant costs are scarce; thosecited may be of questionable quality, and analyses of costs tend to belimited at best.the costs associated with developing trustworthiness features, products, and services have a major labor component. some vendors alsoincur researchrelated expenditures in their efforts to bring trustworthiness products to market, although most of this “research” is actually development. the costs associated with security mechanisms are emphasized in this section because of the pivotal role that security controls playas enablers of other aspects of trustworthiness and the expectation that, inthe future, trustworthiness problems will be associated increasingly withsecurity concerns. the purpose of this section is not to provide an exhaustive articulation of all producer costs; instead, the intent is to highlight those producer cost issues that are particularly germane to trustworthiness.44although both standards and monopolies can provide the benefits of homogeneity,only standards enable the competition necessary to ensure that consumers may affect thetrustworthiness of available products. standards are discussed in detail in the section titled“standards and criteria.”trust in cyberspacecopyright national academy of sciences. all rights reserved.the economic and public policy context193costs of integration and testingnis trustworthiness is inherently a systemlevel property, and, therefore, the costs associated with improving trustworthiness inevitably involve the costs of integration and testing. these costs will vary, depending on whether or to what extent a mechanism is integrated into a system.a relatively standalone mechanism, such as an initial password screen toenter a system, might be written as a software module independentlyfrom the remaining modules of the project and have minimal impact onsystem integration, testing, documentation, and training activities. thecosts are readily identifiable and low. another example of a relativelystandalone solution is firewalls.security controls that have a moderate effect on software development and cost include those that impose multiple access modes within asystem. some menus, data sets, data items, or other appropriate subsetsof the system may have unlimited access, whereas others may limit accessto certain individuals, organizations, or time of day, or limited functionality (e.g., read access only). these controls affect functionality throughoutthe system and, therefore, impose a moderate impact on system integration, testing, documentation, and training activities.finally, costs are high and difficult to identify specifically in systemswhere controls are pervasive: the authentication of each user is rigorous;each transaction is scrutinized for its validity and verified against appropriate databases; external transactions are subject to encryption; audittrails are maintained to facilitate routine and ad hoc audits of transactions; and general access levels may also be employed. if security or otherattributes are integral to much of the functionality throughout the system,associated controls greatly affect system integration, testing, documentation, and training activities. the controls contribute to the complexity ofthe system; the debugging activity is more difficult and may require alonger period.identifying the specific costs associated with trustworthinessaccurate estimation of the direct costs associated with specific projectfeatures requires a complex and timeconsuming analysis that seems tobe seldom performed.45 except in the case of standalone products, it isoften difficult to separate the costs of “regular” functionality from thecosts of “enhanced trustworthiness capability.” this allocation can bearbitrary. the same could be said for the further distinction between thecosts associated with trustworthiness and general overhead costs. com45a committee conclusion based on its deliberations.trust in cyberspacecopyright national academy of sciences. all rights reserved.194trust in cyberspacepounding the difficulty of ascertaining accurate cost data is the fact thatadvocates or opponents of a particular trustworthiness intervention mayattempt to manipulate cost data in marshalling their arguments.costing methodologies have been published, and they address variation in costs and tradeoffs owing to product requirements, producer practices, and other sensitivity factors. these models tend to cover only thedevelopment cycle, and their assumptions about the way effort is expended in a software project may not apply in the contemporary marketenvironment, in which some “development” may be purposely postponedto an upgrade in the effort to reduce the time to market.46time to marketmany of the segments within the information technology marketplace are intensely competitive, where market share—not profit margin—is the primary business objective. in such markets, a product (e.g., webbrowsers) that is available early has the opportunity to develop a customer base or become established as the de facto standard. consequently,minimizing the time to market is a critical consideration for producers.each feature is examined to determine whether its inclusion in theproduct is necessary for the product to be competitive in the marketplace.generally, those features with direct customer appeal win. subtle, hardtodemonstrate, and pervasive properties—which tend to characterizetrustworthiness attributes—tend to be rejected. trustworthiness featuresthat require extensive integration throughout a product also tend to beomitted, because of the time required to properly integrate and test suchfeatures.other issuesto some extent, costs may occur and be traded off at varying points inthe life cycle of a product. the discussion in chapter 3 suggests that thecost of effecting a software change increases through the developmentcycle (i.e., the later a change is instituted, the more it will cost). costs may46the constructive cost model (cocomo), a welldeveloped cost model for softwareengineering, is the centerpiece of barry boehm’s book, software engineering economics(boehm, 1981). boehm discusses security and privacy issues and the reasons these areexcluded in cocomo (p. 490). standard cocomo does not include such effects as addedproduct features (security markings, operational controls), reduced access to documentation, and added documentation control. since these requirements in their stringent formare relatively rare, and even then generally add only 10 percent to project costs, cocomodoes not include this as an added factor on the grounds of model parsimony.trust in cyberspacecopyright national academy of sciences. all rights reserved.the economic and public policy context195also be traded off from the development to the support phase of thesystem life cycle. a poor implementation of trustworthiness characteristics during development can translate into higher costs for technical support operations.47 not only may costs be shifted over time, but costs mayalso be incurred by different organizational units or by consumers.the difficulty of demonstrating and sustaining success in achievingtrustworthiness—one can, at best, test a product or practice against arecognized risk—imply a dynamic process of iteration.48 in some cases, alot of care goes into anticipating risks and addressing them preemptively,49 in other cases the trial and error process seems less systematic,and in all cases actual experience drives improvement. antivirus software provides an example of the inherent limit of anticipation since virusproducers continually introduce new strains against which antivirus software might not work. thus, the antivirus product development processinvolves frequent upgrades in response to new forms of viruses.netscape’s approach of offering a reward for detection of security flawsputs another face on iteration: it implies that the cost of finding problems,and perhaps of developing fixes, could be shared between the producerand the consumer, and it may increase the rate and level at which problems are reported.50 the reality of iteration makes it difficult to estimatecosts fully up front, except to the extent that an iteratively escalatingprocess can be modeled and costed. it also argues for the benefit of retrospective analysis to support such costing.research relating to trustworthiness could help to reduce costs, butthat outcome depends on better understanding of the nature and incidence of costs. having ways to think about cost (“cost models”), even inthe absence of appropriate data, can help in understanding how trustworthiness is perceived or valued and how potential incentives for increasingit may evolve. the expectation that discontinuities will occur—that inci47both the fact that later life cycle costs are not borne directly by the developers (i.e.,technical support is often a distinct organizational unit from development) and the fact thatthese costs are deferred could act as inducements to shift costs to later stages in the productlife cycle.48the iterative process has been compared to an arms race, an escalation of measures andcountermeasures as new problems are discovered, some arising in response to previousfixes. note that target risks may be poorly understood or unspecified, such as the goal ofavoiding system crashes due to bugs or unexpected attacks.49from a research perspective, the staged nature of progress raises questions about therelative payoff to investing in successor (major improvement) technologies relative to incremental improvements to existing technologies.50an attacker might discover vulnerabilities and not report them, hoping to exploit themfor more substantial gains later. this is a highconsequence, but not necessarily a highlikelihood, prospect.trust in cyberspacecopyright national academy of sciences. all rights reserved.196trust in cyberspacedents attributable to inadequate trustworthiness will result in correctiveaction and new efforts at prevention or recovery—suggests that how costsare identified and calculated may be relatively fluid.51the market for trustworthinessthe supply of trustworthiness technology includes both products andservices specifically offered to support one or more aspects of trustworthiness and the trustworthiness of niss generally. this definition is verybroad and could be interpreted to include nearly anything that assists inthe design, development, integration, testing, operation, or maintenanceof an nis. this discussion focuses on those products and services that areintended primarily to promote trustworthiness. because of the specialenabling role that security plays with respect to trustworthiness, securityproducts and services are emphasized.trustworthiness is a systemwide attribute. the cost required to secure a system is not strictly proportional to the number of people usingthat system.52 consequently, as an nis is implemented and the numberof connections increases, it is plausible to discover that the perconnectioncost declines. some technologies, such as those associated with virtualprivate networks and higherquality user authentication, do impose someperuser or percomputer costs. another important reason that securityexpenditures, as separately identifiable data, are likely to decline resultsfrom the integration of security features into generalpurpose information technology products. for example, version 4 of the netscape browserincludes support for ssl and s/mime, which implement security properties. if this browser were categorized as a “nonsecurity” product, thenthe market statistics for security would be understated. another suchexample is a packetfiltering router—it is a router, but it also implementssecurity. finally, as in other segments of the information technologymarketplace, competitive pressures and technological innovations exert51committee members noted the experience of the market research firm gartner group,which found its assessment of the costs of pc ownership reduced to a soundbite—raisingquestions about assumptions and about popular capacity to consider more than a singlenumber. the likelihood of change does not diminish the value of studying costs for oldertechnologies and strategies, but it does raise questions about where it is sensible to extrapolate from the past. it also points to the need to understand sensitivity factors and assumptions.52one way of looking at this is the “hard on the outside, soft and chewy on the inside”phenomenon, in which a collection of unprotected nodes (whose individual security cost isessentially zero, so that the aggregate is independent of the number of nodes) are huddledbehind a small number of firewall/gateway nodes. security does not become cheaper asthe internal network grows.trust in cyberspacecopyright national academy of sciences. all rights reserved.the economic and public policy context197downward pressure on prices. these observations also suggest that assecurity and other aspects of trustworthiness are increasingly incorporated into other products, the task of compiling accurate market data andforecasts for security or trustworthiness will become ever more difficult.the committee did review a limited number of industry analyses thatwere compiled by various market research analysis or financial servicescompanies. the data reviewed supported the argument that while themarket for security products is growing, this market is declining in relative terms because of the higher growth rate in other sectors of the information technology marketplace. however, the committee was ambivalent about the inclusion of any such data in this report, because suchinclusion could be construed as an endorsement of the selected data,methodology, analysis, or firm. the committee was not in a position tomake such a determination.in 1997 and 1998, rapid consolidation was taking place in the computer and network security marketplace, turning small companies intolarger and more aggressive firms. the rapid growth of the internet hasdriven increased demand, especially by larger and more sophisticatedcustomers who have greater knowledge and demands for security requirements and desire integrated security solutions. thus, the consolidation in this market is expected to continue. general computer and communications vendors are also increasingly interested in security, therebyfurther contributing to the turbulent state of the computer and networksecurity marketplace.53supply and demand considerationsavailability is an aspect of trustworthiness that is readily measurableand is highly valued by the public; it certainly contributes to the success offaulttolerant computer systems (e.g., tandem and stratus). some marketsuccesses also exist within the security marketplace, although the demandfor security continues to be relatively limited. niches exist for targetedproducts, such as firewalls and antivirus software, and for services such asonline updates of antivirus software. these two niches are very competitive; satisfying thirdparty assessment is provided through trade magazines54 or the international computer security’s association certificationrequirements and constitutes an important competitive advantage.53for example, note the significant security content in nt version 5, and cisco’s recentacquisition of a proxy firewall supplier.54jimmy kuo, mcaffee associates, during the committee’s third workshop, in september1997.trust in cyberspacecopyright national academy of sciences. all rights reserved.198trust in cyberspaceof course, vendors are very keen to provide what potential customersdesire with respect to the nature, quantity, pricing, and efficacy of trustworthiness features, products, and services. however, vendors havefound that, although people claim that trustworthiness is important in theabstract, when it comes time to spend money, nontrustworthiness expenditures often take precedence. an illustrative case is the effort by digitalequipment corporation (dec) to develop a system that would satisfydod’s most stringent criteria for socalled trusted systems. after makinga considerable investment, dec canceled the project when it became clearthat sufficient demand for the system would not materialize. experiments with trusted operating systems were also terminated by other major system vendors when they, too, were discouraged by a lack of commercial interest.findings1.current computing platforms, communications infrastructure, andsoftware are relatively homogeneous, and the degree of homogeneity isexpected to increase in the future. homogeneity tends to cause niss to bemore vulnerable.2.the increasing use of cots software is causing user organizationsto decrease their level of expertise in system development.3.production costs associated with trustworthiness are difficult toassess. an improved understanding and better models are needed. thereis a paucity of data. the data that are available are questionable, in partbecause of the difficulties in distinguishing trustworthiness costs fromother direct product costs and overhead costs.4.production costs associated with integration and testing representa substantial proportion of a producer’s total costs for improving trustworthiness.5.timetomarket considerations discourage the inclusion of trustworthiness features and encourage the postponement of trustworthinessto later stages of the product life cycle.6.the average expenditure for security per internet/intranetcapableconnection has been declining. this trend is expected to continue becausesecurity (and trustworthiness generally) expenditures are relatively independent of the number of connections or users, although the use of virtualprivate networks and higherquality user authentication technologiesdoes impose some peruser or percomputer costs. additional influencesinclude competitive pressures that are driving prices down and the potential to understate security expenditures as they become more difficultto identify specifically from general expenditures for information technology products and services.trust in cyberspacecopyright national academy of sciences. all rights reserved.the economic and public policy context199standards and criteriathe development and adoption of standards constitute one responseto the challenge of appraising trustworthiness and mitigating difficultiesthat arise from imperfect information. standards can simplify thedecisionmaking process for the purchasers of trustworthiness. they canalso simplify the design and production decisions for the producers oftrustworthiness by narrowing the field of choices (e.g., adherence tointeroperability standards facilitates interconnection among subsystems).compliance with standards or guidelines supplied by the federal government or an authoritative independent standardssetting organization—such as the federal information processing standards (fips) of the national institute of standards and technology (nist), standards of theamerican national standards institute (ansi), or standards that mayresult from the information infrastructure standards panel (iisp)—provides both thirdparty validation of a selection of technology and potential relief from liability.55 there is also the broader notion of criteria (e.g.,the u.s. trusted computer system evaluation criteria [tcsec]), whichincludes the consideration of processes and attributes that cannot be assessed by direct examination of the artifact in question. for example,criteria may involve explicit or implicit comparisons with other productsor systems. criteria may also take the form of authoritative statements ofhow a system should or should not be designed and operated, complemented by some means of demonstrating compliance.56the character and context of standardsthe data encryption standard (des) fips is an example of an interoperability standard; it defines the mathematical function that a compliant device must implement to ensure that data encrypted by manufacturer a’s des box can be decrypted using a box made by manufacturer b,and there are a set of tests used to determine if the function has been55technology transfer and avoidance of at least some known problems lie behind pastgovernment efforts to promulgate guidelines and criteria for trusted systems—tcsec andmore recent international harmonized criteria that build on the u.s. tcsec and comparable efforts overseas. lack of widespread adoption of such guidelines and criteria appearsto relate at least as much, and probably more, to nontechnological aspects (e.g., distrust ofor limited communication with government sponsors of these programs, delays associatedwith compliance testing, little market demand) as to issues of technical compliance (e.g.,difficulty in satisfying the standard).56such criteria have increased trustworthiness for transportation equipment, devices thattransmit radio frequency, and other complex systems that operate in networked environments.trust in cyberspacecopyright national academy of sciences. all rights reserved.200trust in cyberspaceimplemented.57 by contrast, fips 1401 (security requirements for cryptographic modules) is largely a performance standard encompassing security functionality and assurance. it is definitely not an interoperabilitystandard. standards arising in the internet context are expected to promote the implementation of encryption (e.g., ipsec, s/mime, ssl), whilefostering interoperability. apart from some consideration of key lengthand algorithm choice, these standards do not treat cryptographic strengthor resistance to attack by other means.in the internet environment, the internet engineering task force(ietf; see box 6.2) has focused on the security aspects of internet standards, addressing both specific security standards and the larger problemof reviewing other standards to ensure that they either are secure or canhave security added when needed.58 in other venues, such as trade associations, standards setting for computing and communications is intendedto foster interoperability and/or proactively forestall government intervention. computing and communications trade associations and relatedgroups are directing increasing attention to standards related to trustworthiness. for example, the information technology industry council hasaddressed a range of standards and security concerns, and security andprivacy are emphases of the smart card forum. a number of theseindustrybased efforts emphasize security to protect company assets, andthey are often undertaken to deter regulation.there is more history of standards setting in the areas of safety andreliability. in an effort to ensure that the best available techniques areused in certain classes of safetycritical systems, a variety of standardshave been developed by government agencies, industry groups, and individual companies (see box 6.3 for examples). the use of specific techniques and procedures in development is in many cases influencedheavily by these standards, and in some cases their use is required forsystems to be supplied to a government or for systems that may affectpublic safety. domainspecific standards facilitate the needs of the particular domain, but they deter common solutions across market segments.57this fips consists of an algorithm description, a set of test vectors, and a very subsidiary set of implementation cautions. it is in no sense a security standard, except implicitlyin that its “fipsness” implies that somebody in the government said it was good enoughfor certain use. in particular, one cannot exceed the standard and be more secure than des,since that would take a different algorithm and fail the interoperability test. if someone goes off and puts des in some stupid box that, for example, coughs up the key ondemand, then someone built a stupid box, but it would not be in violation of the fips. thisfips does not specify how one must implement the des internally; it specifies only theinterface.58placing emphasis on the “larger problem” is a recent phenomenon.trust in cyberspacecopyright national academy of sciences. all rights reserved.the economic and public policy context201standards and trustworthinessthe notion of specification is at the core of all characterizations oftrustworthiness attributes. unless a precise, testable definition for anattribute such as reliability exists, it will not be possible to determinewhether the requirements of the definition have been fulfilled. the defibox 6.2internet standards and the internet engineering task forcemost internet standards are developed by a group called the internet engineeringtask force (ietf). although this is by no means a requirement––any protocol can berun on top of the basic internet protocols, and there are other bodies that developstandards for specific areas, such as the world wide web consortium (w3c)—mostof what we use today on the internet was codified by the ietf. although the ietfwas initially funded by the national science foundation, the ietf has no formalendorsement from the federal government. the ietf’s estimated 1998 operatingbudget is $1.7 million (wilson, 1998).the ietf is unusual in a number of respects. there is no formal membership; asa consequence, there is no voting. instead, standards are accepted by “rough consensus and running code.” standards are developed, and rfcs (nominally “requestsfor comments”) are written by assorted working groups. the working groups areorganized into a handful of areas; the directors of these areas collectively form theinternet engineering steering group (iesg). overall architectural development isnominally directed by the internet architecture board (iab). the membership of theiesg and iab is chosen by a nominating committee that is randomly selected from agroup of volunteers who are ietf attendees. final approval is vested in the board oftrustees of the internet society (isoc).given this procedural context, the process of adopting an internet standard iscomplex. apart from prescribed milestones, the iesg occasionally promulgates anew policy that will apply to all standardstrack rfcs. in the spring of 1997, justsuch a policy was adopted with respect to security: security is important. specifically, it was decided that the hoary phrase “security considerations are not addressed inthis memo” will no longer be permitted in rfcs. instead, a real security analysismust be done. protocol designers must consider what vulnerabilities are present andwhat the consequences would be if each were exploited. furthermore, the designersmust analyze existing security mechanisms to see if some other standard would solvethe problems. only if none is suitable should custom mechanisms be designed.one choice has been ruled out: cleartext conventional passwords are not permitted.when passwords are to be used, some cryptographic mechanism must be employedfor authentication purposes.further, the iab and iesg jointly adopted a statement endorsing strong cryptography (carpenter and baker, 1996). limited key lengths, mandatory key recovery, andexport controls were specifically rejected. although this statement does conflict withvarious national policies, including those of the united states, the belief was that aninternational technical organization should use only technically sound mechanisms,regardless of limitations imposed by particular governments.trust in cyberspacecopyright national academy of sciences. all rights reserved.202trust in cyberspacenitions in use by the community permit availability and reliability to bemeasured and compared, thereby allowing a system to be regarded as“sufficiently reliable,” for example, if the measured or predicted reliability of the system meets or exceeds some prescribed threshold. an analogous situation does not exist for security, where there does not seem to bea testable definition and where a specification cannot anticipate all of theproblems that may arise.there are exceptions, as is illustrated by the des, whose presence andwidespread adoption clearly benefited all concerned. yet security expertsbox 6.3examples of safety standardsthe rtca1 standard do178b, entitled “software considerations in airbornesystems and equipment certification,” is a standard developed by the commercialair transport industry for software used in commercial aircraft and is adhered to byvirtually all developers of aircraft systems as a part of the aircraft certification process. do178b defines criticality levels for aircraft software, and different development techniques are required for each level. the standard prescribes developmentpractices, documentation, and recording requirements for all phases of the softwarelife cycle. in addition to defining many aspects of software development, the standard specifies assurance requirements through which the developer demonstratescompliance with the standard to regulatory agencies. the british ministry of defense standard 0055, “requirements for safetyrelatedsoftware in defense equipment,” is a controversial standard because it mandates theuse of mechanical analysis techniques. section 36.5 of the standard states, for example, the following about the source code for a safetyrelated system:•“static analysis2 in accordance with 26.2 shall be performed on the whole ofthe source code to verify that the source code is well formed and free of anomaliesthat would affect the safety of the system.”•“proof obligations3 shall be: (a) constructed to verify that the code is a correctrefinement of the software design and does nothing that is not specified; (b) discharged by means of formal argument.”1rtca used to be an acronym for radio technical commission for aeronautics, but theorganization’s name was formally changed to just rtca in 1991.2this is the analysis of a computer program by any means other than executing it. agrammar checker is an example of a simple static analyzer.3a proof obligation is a proposition that must be true in order for some larger aspect of asystem to hold. it is something that has to be shown to be true independently of the largeraspect, usually to allow a proof to be developed for a theorem. a theoremproving systemmight be able to establish that a program satisfies its specification, but only if it is able tomake certain “assumptions.” for the theorem to be proven, the assumptions must beshown to hold separately.trust in cyberspacecopyright national academy of sciences. all rights reserved.the economic and public policy context203consider des to be an unusual case, given other experiences with standards, which illustrate the risk of treating standards as indicators of assurance (see box 6.4).box 6.4cryptographic challengesthe design and implementation of secure cryptographic algorithms, as well asprotocols that make use of such algorithms, have proven to be difficult. over the last20 years (the interval during which public interest in cryptography has grown substantially), there have been many examples of missteps:•symmetric and publickey cryptographic algorithms and oneway hash functions developed by respected members of the academic and commercial cryptographic community all too often have succumbed to cryptanalysis within a few yearsafter being introduced. examples include the merklehellman trapdoor knapsackpublickey algorithm, some versions of the feal cipher, the snefru oneway hashfunction, and the md4 hash algorithm.•authentication and keymanagement protocols have suffered a similar fate, asthey have been shown to be vulnerable to various sorts of attacks that undermine thesecurity presumed provided by them. examples include the original needhamschroeder keymanagement protocol and the various protocols that were intended torepair its flaws (needham and schroeder, 1978, 1987; denning and sacco, 1981).these experiences emphasize the need for cryptographic algorithm standardsand security protocol standards that have been carefully developed and vetted. because implementations of security technology represent a major source of vulnerabilities, there is also a need for highassurance implementations of this technology.this latter need has sometimes been met through the use of government or thirdparty evaluation programs for hardware or software components supporting cryptography or cryptographic protocols (e.g., in connection with fips 1401 and ansix9.17 standards).as an example, consider the data encryption standard (des). the des was developed initially by ibm and submitted as a fips in the mid1970s. even though thedesign of des was public, the algorithm met with considerable skepticism from somemembers of the largely academic cryptographic community because the design principles were not disclosed and because of concerns over the key size. over time, asthis community developed improved cryptanalytic methods, des actually came tobe viewed as a welldesigned algorithm. des became widely used, promoting interoperability among a number of security products and applications. des hardware(and, later, software) was evaluated and certified by nist, providing independentassurance of an implementation.however, the key size is now too short for today’s technology, as demonstratedin july 1998, when a team under the auspices of the electronic frontier foundation(1998) designed and built a key search engine for less than $250,000 (the cost of theparts). although des has exceeded its originally projected lifetime, it is an openquestion at what time in the past bruteforce cracking became economically feasible,especially for nationstates (wiener, 1994; meissner, 1976; hellman, 1979).trust in cyberspacecopyright national academy of sciences. all rights reserved.204trust in cyberspacetechnical standards imply extensive discussion, review, and analysisby experts and stakeholders, which minimizes the number of remainingflaws.59 however, the existence of standards also introduces risks. technical standards may provide an adversary with detailed technical information that facilitates the discovery of flaws. interoperability facilitateslegitimate use, but it also allows a vulnerability to be exploited in multiple contexts. finally, it is easier to mount attacks against multiple representatives of a single standard than against differing implementations ofseveral standards.securitybased criteria and evaluationeuropean and north american governments60 are moving to establish a unified security criteria, called the common criteria for information technology security evaluation. the common criteria (ccv2)61attempts to reconcile the requirements of the canadian trusted computer product evaluation criteria (ctcpec) (canadian system securitycentre, 1993), the european information technology security evaluationcriteria (itsec) (senior officials group, 1991), and the united statestrusted computer system evaluation criteria (tcsec) (u.s. dod, 1985).all these criteria share two underlying dimensions: the extent of thesecurity mechanisms being rated, often called the functionality axis, andthe degree to which the mechanisms can be trusted to perform their functions correctly, often called the assurance axis (figure 6.1). examples ofsecurity functionality include authentication mechanisms, access controllists, and cryptographic features. examples of assurance steps are testing,examination by independent teams, use of formal methods, and the degree of rigor in the development process.the rating received by a given product or system is a combination ofboth components (see box 6.5). for illustrative purposes and to avoid thesemantic baggage of using a particular criterion’s terminology, the discussion that follows uses a hypothetical rating system of 1 to 5 on eachaxis, where [f1,a1] is a system with minimal security functions and minimal trustworthiness, and [f5,a5] is one that exhibits state of the art in each.the reader should assume that a “reasonable” definition may be articulated for each, which is a nontrivial assumption. the discussion that59this is especially true for standards that are a result of consortia or other cooperativeefforts among the stakeholders. for de facto standards that derive from a dominant vendor, one might also expect reduced design flaws, or at least a general awareness of theproblems and workarounds identified.60united states, canada, france, germany, the united kingdom, and the netherlands.61information available online at <http://csrc.nist.gov/cc/ccv20/ccv2list.htm>.trust in cyberspacecopyright national academy of sciences. all rights reserved.the economic and public policy context205follows is not dependent on any specific definition or process to assignthe values.laying out the possible ratings on a twodimensional grid quicklymakes clear distinct “zones” in the grid (see figure 6.1). along the diagonal ([f1,a1] to [f5,a5]) lie what could be viewed as “balanced” systems, inwhich the effort placed in assurance matches the security functionalitybeing provided. on one side of the diagonal is the “danger zone” of highfunctionality relative to assurance, such as [f3,a1]. the danger is, ofcourse, that the product exhibits superficial characteristics of security butcannot be trusted, because no significant effort has been made to showthat the features work as promised, especially in the face of hostile analysis and attack. interestingly, this is the zone in which nearly all commercial security products lie today, because features sell, whereas assuranceis the concern of the specialist. on the other side of the diagonal is the“conservative zone,” in which mechanisms are placed under a high degree of scrutiny relative to functionality, such as a [f1,a3] system.how much is a high rating worth? while it is plausible to concludethat a higher rating has a higher market value than a lower one, the ratingis only one of many factors that consumers consider in their decisionmaking, and the discussion in the cost section suggests that other considerations, such as functionality, take precedence. vendors are keenly interested in the value of a rating because all current rating systems compelfigure 6.1 hypothetical securitybased criteria.trust in cyberspacecopyright national academy of sciences. all rights reserved.206trust in cyberspacethe vendor to invest in satisfying the criteria and, in some cases, in payingfor the evaluation process itself. the investments can be substantial, particularly in terms of opportunity cost and lost sales because the extendedtime to market is added to the direct cost of becoming “evaluation ready.”a rating is also useful as a reflection of the ability of a product to resistanalysis and manipulation by the threat; in this context, the value of arating is called the “operational value.” as noted before, threats are everincreasing, and therefore, the operational value of a rating correspondingly decreases over time. (one way to retard growth in the sophistication of attackers is to keep some aspects of a design implementation secret. see appendix i for a discussion.) this depreciation of a rating’sbox 6.5the tcsec, itsec, and common criteria: two values or one?the oldest set of criteria is the trusted computer system evaluation criteria(tcsec) (u.s. dod, 1985). during its development there was a substantial debateas to the format of the rating. one school felt that the rating should directly reflectthe underlying twoaxis structure. a rating would therefore consist of two parts.education and discretion on the part of the evaluators would prevent “danger zone”products from being deployed. a second school, and the one that prevailed, heldthat a twopart rating was both excessively complex and risky. the ratings wereaccordingly devised as a single value that attempted to define “balanced” systemsalone.1 although the singlevalue approach precluded the “danger zone,” it alsoprecluded the “conservative zone,” where many potentially useful products couldexist, especially in a networked world.2the common criteria version 2 (ccv2) follows the information technology security evaluation criteria (itsec) in taking the opposite approach. ratings have twovalues: a “protection profile” that seeks to capture the security functionality, and an“evaluation assurance level” that seeks to capture the degree of trust one could placein that functionality. the ccv2 is then conceptually the mirror image of the tcsec:“danger zone” products are possible and must be discouraged through educationand de facto regulatory steps; “conservative zone” products are allowed; and a product’s rating is both more complex and more informative. also, the ability to add newprotection profiles allows for the possibility that the criteria can adapt to new technologies and increased threat. the common criteria are also more modular, lessconfidentiality centric, and more current3than the tcsec.1as finally devised, the diagonal was not completely followed. an a1 rating adds assurance steps to a b3 rating but maintains the same security functions. thus, if a b3 rating isseen as [f4,a4], an a1 is [f4,a5].2the choice of balance is indeed a problem. refer to neumann (1990) and ware (1995)for critical analyses of tcsec and the criteria approach.3for example, the common criteria includes provisions for nonrepudiation, a critical concern in electronic commerce systems.trust in cyberspacecopyright national academy of sciences. all rights reserved.the economic and public policy context207value occurs on both axes. on the functional axis, for example, cryptographic key lengths that are perfectly adequate at one time may becomewholly inadequate several years later, owing to the increased computingpower available to the threat. a similar phenomenon occurs on the assurance axis. assurance steps attempt to uncover flaws before a product isexposed to the threat; in some sense they attempt to take a “deeper look”at a mechanism than any element of the threat could afford. assurancesteps that “look deeper” than a single attacker can look may have beenadequate before the onset of the internet, but are made obsolete by anenvironment that facilitates anonymous and unplanned technical collaboration among likeminded individuals. the depreciation of the operational value of a particular rating has not been a concern for individualproducts, because it has been slower than the rate at which productsbecome obsolete or uncompetitive for other reasons.criteria, because they must cover a variety of products and technologies, are inevitably written in general terms. when applied to a specificproduct they must be interpreted, and anyone who has gone through theprocess of having a system evaluated against criteria will attest that theinterpretation sets the height of the bar that the product must clear. thissituation, combined with the background of an everescalating threat,leads to tensions in the evaluation process.on the one hand, there is significant pressure to maintain consistencybetween evaluations of different products over time. that is, the difficulty of achieving, say [f3,a3], in 1995 should be about the same as achieving it in 1998. the motive is fairness. since it is likely that the marketvalue (as reflected in increased sales of product) of an [f3,a3] rating will bethe same in 1998 as it was in 1995, it is arguably unfair for the later vendorto be subjected to a more stringent set of interpretations (and the associated increased cost) than the earlier one.on the other hand, evaluators are aware of the decreased operationalvalue of a rating (as manifest in a particular set of interpretations) overtime. they are, accordingly, under pressure to increase the stringency ofthe interpretations over time, a process called “criteria creep” in thetcsec arena.the dilemma inherent in the process then is as follows: if the interpretations are constant over time, then the operational value of a given ratingbecomes progressively less and products are placed in harm’s way withprogressively less protection relative to the threat. if the interpretationsbecome more stringent over time, the ratings maintain their operationalvalue but vendors are discouraged from participating because the investment required to achieve a given rating increases over time. this contradiction has not been resolved to date in the tcsec evaluations. the commoncriteria effort hopes to overcome this by adding new protection profiles totrust in cyberspacecopyright national academy of sciences. all rights reserved.208trust in cyberspacerespond to the increased threat. given the inevitable bureaucratic andregulatory pressures to maintain fixed objectives, it is doubtful that thecriteria evolution can keep pace with the evolution of the threat.the history of national and international criteria and evaluation systems also raises questions about institutional roles and responsibilities.the national and international criteria have featured government agencies in prominent roles, attributable to both subject matter expertise andagency missions associated with national security. the latter missionshave, in turn, inspired distrust and discomfort in the private sector inasmuch as either criteria or evaluation elements and rationales have beenincompletely communicated or understood and have been controlledtightly by the national government.62the evaluation under the tcsec has been done by government (including government contractor organizations) at government expense;according to anecdotes from vendors who have gone through the experience, evaluators appear to have been junior with little computer systemdevelopment experience and little motivation to expedite evaluations orpromote successful outcomes. costs incurred by vendors undergoingevaluation processes include delay and obsolescence of products, extradocumentation costs, and costs of additional work needed to addressconcerns uncovered by evaluators. industry has called for selfrating or abroader system of evaluators to expedite the process. a principal concernvoiced by vendors is that of degree: the perception of the tcsec philosophy as “more is better” is associated with the perception that tcseccompliance and evaluation is excessively costly.the itsec and common criteria assume involvement of commercially licensed evaluation facilities (clefs), several of which exist today(e.g., in germany and the united kingdom, which have an agreement formutual recognition of evaluation results), vendor payments to clefs,and publicly available evaluation manuals. the clefbased evaluationsare less expensive and more expeditious than governmentally operatedevaluations.63 the nist, building on a broad program of commercialevaluation of standards compliance, the national voluntary laboratoryaccreditation program, has guided commercial evaluation proceduresfor fips 1401, and it will also build on that program for evaluation ofinformation security products using the common criteria under the newnational information assurance partnership.6462concerns about completeness revolve around the evaluation process, as opposed to thecriteria per se. note that in criteria or standards, completeness concerns tend to arise inspecifications for cryptography.63 based on committee members’ personal experiences and committee deliberations.64see nist,“national information assurance partnership gets industry support,” department of commerce news (press release), october 7, 1997.trust in cyberspacecopyright national academy of sciences. all rights reserved.the economic and public policy context209experiences with criteria for “trusted systems” have demonstrated anumber of practical problems ranging from how criteria are specified tohow systems are evaluated. the central conundrum of criteria (or standards) for trustworthiness is this: if a criterion or standard is written as aperformance specification, then evaluation is difficult, but if it is writtenas a design specification, then the criterion is incomplete because no design specification can cover the range of implementations. the evaluationprocesses associated with criteria raise questions about openness (whatdo evaluators say to whom, including the developers) and quality (theimplications of what the process emphasizes and what the evaluatorsseem to know and understand about the development process and theproduct). the processes also impose costs and raise other issues associated with having a certifier at the site where a system is deployed if thecertifier needs to know what a system will be used for. if trustworthinessin the system depends on trust in the administrator, problems arise wherethe designer, administrator, and certifier disagree on security objectives.65another difficulty with the concept of criteria is that ratings can relateonly to a particular component, not to an entire nis. in principle, securityevaluated components are used as building blocks and could be combinedwith rigorous system analysis of assembled systems. however, there is adwindling set of evaluated components and little or no rigorous methodology for assessing the security of whole systems, as discussed in chapter 4.findings1.there is an increasing interest in the standards associated withtrustworthiness by governments, industry associations, and the internetengineering task force.2.a precise and testable definition is required to assess whether astandard has been fulfilled or not. such definitions may often be articulated for some dimensions of trustworthiness such as reliability, but areoften difficult to articulate for security.3.the development and evolution of a standard attract scrutiny thatwill work toward reducing the number of remaining design flaws andthereby promote trustworthiness. at the same time, the existence of standards promotes the wide availability of detailed technical information abouta particular technology, and therefore serves as a basis for assessing wherevulnerabilities remain. moreover, standards that facilitate interoperabilityincrease the likelihood that successful attacks in a system may prove effec65this issue was discussed at the 1997 ieee symposium on security and privacy, oakland, california, may 57, 1997, according to an informal email report by mary ellen zurkoof the open group research institute.trust in cyberspacecopyright national academy of sciences. all rights reserved.210trust in cyberspacetive in other systems. thus, the relationship between standards and trustworthiness is indeterminate.4.there is a tension in evaluation processes that yield ratings. if interpretations are constant over time, then the operational value decreases asproducts provide progressively less protection relative to threats. if interpretations become more stringent over time, vendors are discouraged fromparticipating, because the increased investment required to achieve a givenrating increases over time. the common criteria effort hopes to mitigatethis tension, but within the context of the inevitable bureaucratic and regulatory pressures to maintain fixed objectives, it is doubtful that criteriaevolution will keep pace with evolving threats.5.commercial licensed evaluation facilities are less costly and moretimely than those that are government sponsored or operated.6.while securityevaluated components might be used as buildingblocks with rigorous system analysis of the assembled system, there is adwindling supply of evaluated components and little or no rigorous methodology for assessing the security of networked information systems assembled from evaluated components. this suggests that criteria mayhave limited usefulness for niss.cryptography and trustworthinessas articulated in chapters 2 and 4, the committee concluded thatgreater deployment of cryptography is essential to the protection of theinternet and its end points. but why is cryptography not deployed morewidely? the most visible reasons are public policy concerns: exportcontrols and demands for key recovery.export controlsu.s. export controls have undeniably retarded the worldwide availability of products incorporating encryption; indeed, this has been thestated goal of u.s. policy in this area, and u.s. vendors are in broadagreement that u.s. export controls on products incorporating encryptionhave a negative impact on their ability to make foreign sales of many oftheir products. to the extent that vendors have been reluctant to producetwo versions of a product rather than one (to produce one for domesticsale and one for export, or to hinder interoperability between domesticand export versions), u.s. export controls have also hindered the domestic availability of products incorporating encryption.66 however, if for66see computer science and telecommunications board (1991, 1996). also see diffie andlandau (1998).trust in cyberspacecopyright national academy of sciences. all rights reserved.the economic and public policy context211eign vendors begin to step into the void left by u.s. export controls, theavailability and use of information security products may be less constrained by the unavailability of u.s. products. 67key recoveryan encryption product can be designed in such a way that the keyrequired to decrypt an encrypted message can be made available to thirdparties (i.e., a party that is not either the sender or the receiver) withoutthe explicit action of either the sender or the receiver.68 since 1993, lawenforcement agencies have been in the forefront of the encryption policydebate, insisting that products be designed to provide key recovery forlaw enforcement purposes with proper legal authorization. product vendors have insisted just as firmly that the design and sale of encryptionproducts with key recovery should be driven by the market, rather thanby government fiat. furthermore, keyrecovery encryption products areby design less secure than encryption products without key recovery,because they provide access to decryption keys through a channel thatcan be compromised. as of this writing, the public policy debate over keyrecovery continues unabated. the crisis report (cstb, 1996) arguedthat key recovery was an unproven though promising technology, andthat aggressive deployment and promotion of key recovery were not appropriate as a matter of public policy; this committee sees no reason toalter that assessment today.to the extent that public policy is unsettled and does not set cleardirection, the resulting uncertainty, fear, and doubt affect the marketplace by making it difficult for users and producers to plan for the future.vendors are reluctant to bring to market products that support security,and potential users are reluctant to adopt information security productsthat may become obsolete if and when the legal and regulatory environment changes.factors inhibiting widespread deployment of cryptographyalthough export controls and key recovery are important factors, thecommittee has found that there are other important reasons for the lim67this occurrence would not necessarily be all to the good. such a development mightwell reduce u.s. economic strengths by ceding increasingly large market shares to foreignvendors of information technology. u.s. national security interests might also suffer (seethe section “the changing marketgovernment relationship” for further discussion).68for encryption products that manage stored files rather than messages, the sender andreceiver are the same party. in this case, a “third party” is someone that the file creatordoes not explicitly wish to have decryption capability.trust in cyberspacecopyright national academy of sciences. all rights reserved.212trust in cyberspaceited deployment of cryptography in the united states. for example, cryptographically based security measures often reduce the convenience andusability of the nis they protect. indeed, the purpose of a security measure is to make the nis impossible for an unauthorized party to use, agoal that almost always conflicts with the design goal of making the niseasily accessible to an authorized user. as noted above, the need toundertake even a modest amount of extra work or to tolerate even amodest inconvenience for protection that is not directly related to theprimary function of the device is likely to discourage the use of suchprotection. security functions that are not transparent to the user andautomatically applied are likely to be perceived by the user as costs thatinterfere with his or her ability to get work done.a related point is that applications operating in a networked environment must be interoperable with each other. in some cases, the use ofcertain security measures such as cryptography can detract from the compatibility of applications that may have interoperated in the absence ofthose measures. for example, the use of network encryption may rendernetworks inoperative because network address translators may not workanymore. loss of interoperability may be a very high price to pay foradding security measures.a good example is email. email systems often communicate witheach other via translating gateways, which were necessary because of thelack of homogeneous email systems. these translating gateways sendand receive email fairly well. however, the introduction of encryptioninto email systems would cause the gateways to fail. it is difficult toenvision security standards until there are standards for general emailcommunication. attempts at email security that apply to only some ofthe major email software systems will not be effective—all major products must be included. the lack of easytouse email software that hasencryption built into it and the lack of a publickey infrastructure suggestthat widespread, routine, and transparent email encryption will be difficult to achieve.a third point is that cryptographically based information securitymeasures often consume computational resources, such as execution timeor memory. for example, routine encryption often slows down a serverthat provides encryption services. although it is true that processorsincrease in speed at a very rapid rate, so, too, do user expectations anddesires. as a result, increases in computational capability may well beconsumed by increased functionality, leaving little for security.the mere availability of security products is not necessarily sufficient.to be useful across a broad range of users and applications, users wouldalso need access to a national or international infrastructure for managingtrust in cyberspacecopyright national academy of sciences. all rights reserved.the economic and public policy context213and exchanging keys. without such an infrastructure, encryption mayremain a niche feature that is usable only through ad hoc methods replicating some of the functions that an infrastructure would provide and forwhich demand would thus be limited (cstb, 1996). for example, even ifcryptography had been included in the unix rlogin command, a keyinfrastructure (publickey infrastructure or private/symmetric algorithms) would be necessary for the cryptographic features to be usedeffectively on a wide scale.many of the algorithms that are useful in cryptography are protectedby patents. even though a number of key patents have expired (or willexpire soon enough), patents still cover some important ideas, likemicali’s69 and schnorr’s.70 there are also many patents covering everything from encrypting account numbers to constructing keys from hashes.today, those writing cryptographic software run substantial risks of infringement. in other cases, vendors are confused by the legal argumentsamong patent holders about the validity of various patents. and, evenwhen a patent on a particular algorithm is undisputed, the fact that theholder may impose various fees and use restrictions on the patent maywell inhibit the implementation of certain forms of cryptography. suchinhibitions also exist within academia, despite “free licenses for noncommercial use” that are available, because the source code that is developedcannot be given away, even if it is restricted to the united states.the patent situation and export policy have particularly chilling effects on universities, because universities do not have the economic incentive to overcome the additional costs that are a consequence (e.g., recoupthe costs of obtaining an export license). the impact on universities is ofgreat concern because much of the software in use on the internet wasdeveloped or inspired at universities.finally, for the vast majority of electronically carried or representedinformation, existing niss do provide adequate protection simply because the content of that information is not valuable enough for an unauthorized party to go to the bother of obtaining it. for example, most usersof niss have an inhouse cable plant or a cable plant that runs throughtelephone company facilities, which are presumed to be sufficiently secure. in general, a hardwired link is secure enough for most information,although perceptions regarding the adequacy of this security may varywidely. wireless communications are a different story, and a great dealof attention has been paid in recent years to protecting them.69micali’s patents are 5,276,737 (january 1994) and 5,315,658 (may 1994).70schnorr’s patent is 4,995,082 (february 1991).trust in cyberspacecopyright national academy of sciences. all rights reserved.214trust in cyberspacecryptography and confidentialitychapters 2 and 4 discuss the value of the authentication aspects ofcryptography. the committee emphasized the importance of authentication (over confidentiality) for both technical and policy reasons. thetechnical reason is that authentication is the first line of defense againstthe most serious threats to niss for critical infrastructures—intruders attempting to deny the benefits of using niss to authorized users. it is stillimportant to recognize, however, that confidentiality is an important capability for protecting privacy in general, for securing access to legacysystems, and in providing “defense in depth” for protecting against improper access (e.g., encrypting a password file or bulk transmissions andthereby obscuring the data traffic so that the analysis of this traffic is moredifficult).the policy reason for the committee’s emphasis on authentication isthat it does not generally involve conflicts among stakeholders. since1990 (and before 1990, informally), liberal rules have governed the exportof information security products whose functionality is limited to authentication or integrity,71 a fact that suggests that on balance, national security interests are not significantly affected by widespread foreign access tosuch products. indeed, law enforcement authorities have not demandedaccess to the cryptographic keys underlying authentication and integrityproducts.findings1.the public policy controversy surrounding export controls andkey recovery inhibits the widespread deployment of cryptography.however, there are other important reasons why cryptography is notmore widely deployed. these reasons include reduced convenienceand usability, possible loss of interoperability, increased computationaland communications requirements, lack of a national or internationalkey infrastructure, restrictions resulting from patents, and the fact thatmost information is already secure enough relative to its value to anunauthorized party.2.insofar as information is not secure enough relative to its value toan unauthorized party, the use of cryptography to promote increasedconfidentiality in niss would contribute to improved trustworthiness.71“liberal rules” mean such products were regulated exclusively under the departmentof commerce and governed by the commodities control list, rather than the more restrictive international traffic in arms regulations of the state department.trust in cyberspacecopyright national academy of sciences. all rights reserved.the economic and public policy context215federal government interests in nistrustworthinessthe federal government has multiple interests and roles in enhancingnis trustworthiness:•to respond to changing government information technology infrastructures,•to accomplish agency missions, and•to promote and protect national interests.the spread of computer networking and activities such as electroniccommerce in procurement and acquisition, electronic dissemination oflegislative and agency information, the systems adoption and modernization associated with a wide range of efforts to streamline and enhancegovernment services (e.g., national partnership for reinventing government),72 and the introduction or revision of legislation and administrativeguidelines shaping the use of computerbased systems in governmentindicate that most if not all agencies of the government have a direct,missionbased interest in nis trustworthiness. for example, the information technology management reform act73 highlighted the importanceof strong highlevel management of information technology in federalagencies by requiring the designation of a chief information officer forevery agency. the computer security act74 and the paperwork reduction act75 resulted in office of management and budget circular a130,appendix iii, which provides guidance for all federal agencies on theirresponsibilities regarding computer security.in addition to missionbased goals and activities, two importanttrends are influencing government interest in nis trustworthiness. thefirst is that the economics of using cots products and services, includingsecurity and other trustworthyspecific products and services, is irresistible for all consumers, including government, and represents a major shiftfrom the government’s historical use of custommade information technology. the second trend is the relatively recent rise of concerns about“information warfare” and protection of critical infrastructure. information warfare—at least in a strategic sense—blends traditional national72the nprg (formerly the national performance review) is an initiative for reengineeringgovernment programs and services. see <http://www.npr.gov>. the nprg was a springboard for an effort by the federal networking council to outline a framework for federalinternet security.73public law 104106.74public law 100235.75public law 10413.trust in cyberspacecopyright national academy of sciences. all rights reserved.216trust in cyberspacesecurity interests with less traditional defense concerns over economicsecurity and protection of the civilian economy. although informationwarfare (or the issue of information assurance, defined approximately aswhat is needed to combat the information warfare threat76) has been thefocus of many recent studies (see chapter 1 and appendix f),uncertaintyabounds about the actual threat associated with nis vulnerabilities. pronouncements and programs have been based on uneven and anecdotalevidence, and acknowledgment of the deficient information base is combined routinely with attempts to forecast the nature, uses, and ramifications of information technology. these two trends are related insofar ascots products and services are available to all and, therefore, tend toreduce the technological superiority of the united states as comparedwith other nations.the awareness of information systems trustworthiness issues has beenheightened by recent initiatives aimed at promoting the development anduse of information systems generally, such as the high performance computing and communications initiative, which coordinated research anddevelopment and has become the computing, information, and communications r&d program; the national information infrastructure initiative and the information infrastructure task force, which promoted research and economywide use of information infrastructure;77 and thepresidential framework for electronic commerce (office of the president,1997).on may 22, 1998, the president signed presidential decision directive63 (pdd63) on critical infrastructure protection, which calls for a nationaleffort to ensure the security of the increasingly vulnerable and interconnected infrastructures of the united states. such infrastructures includetelecommunications, banking and finance, energy, transportation, and essential government services. the directive requires immediate federalgovernment action, including risk assessment and planning to reduceexposure to attack, and stresses the critical importance of cooperationbetween the government and the private sector by linking designatedagencies with privatesector representatives.pdd63 also established the critical infrastructure assurance office(ciao) to support the national coordinator, charged with integratingthe various sector plans into a national infrastructure assurance plan andcoordinating analyses of the u.s. government’s own dependencies on76pccip favored the term “information assurance,” reintroducing a concept used in earlier years at darpa that has the benefit of not referring to warfare and, outside the community of security experts, is sufficiently ambiguous to support multiple interpretations.77the iitf included activities by the security issues forum, the technology policy working group, and activities through darpa, nsa, nist, doe, and other agencies.trust in cyberspacecopyright national academy of sciences. all rights reserved.the economic and public policy context217critical infrastructures. the president’s commission on critical infrastructure protection (pccip), the predecessor of the ciao and the firstnational effort to address the vulnerabilities created in the new information age, was established in july 1996 by executive order 13010.78across the federal government, the dod conducts the largest effortin information systems trustworthiness, through its work on informationsecurity as it relates to the nation’s security interests. for example, incommunications security, the national communications system groupand its parent defense information systems agency (disa) coordinatewith the service provideroriented national security telecommunicationsadvisory committee (nstac) to ensure that national security and emergency preparedness needs for telecommunications services are met;79these and other dod agencies depend on a significant nsa effort forhighgrade communications security. the primary agencies within dodthat support and facilitate research and development on information security are the nsa and darpa, whose roles are discussed in detail laterin this chapter.on the civilian side of the federal government, the federal bureau ofinvestigation (fbi) has interests in nis trustworthiness as a part of its lawenforcement mission. during the last several years, the fbi has substantially increased its activity in addressing computerrelated crimes. thefbi’s most visible involvement with the information security issue hasbeen to warn of the dangers that encryption poses to the law enforcementcommunity and to push for the installation of keyrecovery features in allencryption products and provide law enforcement authorities with thetechnical capability to access decryption keys surreptitiously andnonconsensually under courtapproved wiretap orders. in february 1998,the national infrastructure protection center was established within thefbi to serve as the federal government’s focal point to detect, deter, assess, warn of, respond to, and investigate computer intrusions and unlawful acts, both physical and “cyber,” that threaten or target u.s. criticalinfrastructure.8078details available online at <http://www.pccip.gov>.79the ncs is an interagency group of about 23 federal departments and agencies that“coordinates and plans ns/ep [national security/emergency preparedness] telecommunications to support any crisis or disaster.” the nstac provides industry perspective, advice, and information to the president and executive branch “regarding policy and enhancements to ns/ep telecommunications.” ncs was formed in 1963 on a smaller scaleafter command, control, and communications (c3) failures during the cuban missile crisis;nstac was formed in 1982 in anticipation of the at&t divestiture and evolving c3 capabilities and needs.80information available online at <http://www.fbi.gov/nipc/index.htm>.trust in cyberspacecopyright national academy of sciences. all rights reserved.218trust in cyberspaceunder the computer security act, the national institute of standardsand technology (nist) has governmentwide responsibility for civiliangovernment systems and systems handling sensitive but unclassified information. this act also provided for the provision of technical expertiseand advice by the nsa for nist, where appropriate. although nistdoes carry out its mission within budget constraints, the reality is thatnist’s budget is too limited for it to acquire or use significant levels ofexpertise, with the result of perpetuating nsa’s de facto authority andinfluence in the information security domain.81 in 1997, advisors to thensa and pccip called for greater involvement of nist with nsa inareas of mutual interest—which, given the dependence of the defenseinformation infrastructure on the national information infrastructure,could be quite extensive.agencies that regulate the safety of goods and services have begun toaddress information system component trustworthiness in products ranging from medical devices (food and drug administration) to aircraft andthe air traffic control system (federal aviation administration). in theseinstances, information systems trustworthiness refers to safety and reliability as well as to the traditional domain of information security. theseagencies focus their activities in the context of specific products and circumstances of use, influencing system design, implementation, and useby requiring impact analysis and testing, and they may declare (e.g., byevaluation relative to a standard and/or regulation and certification)products safe or unsafe for use in a particular context.the regulation of telecommunications services has been extended tothe promotion of reliability and interoperability. for example, the network reliability and interoperability council, established under the auspices of the federal communications commission and later privatized,has promoted industry monitoring and the minimization of outages. it isworth noting, however, that this regulatory response could be viewed asa corrective response to the erosion of trustworthiness that some attributeto regulatory changes that promote competition.82by contrast, in the81the principal vehicles for nist action have included federal information processingstandards, research relating to associated measurement issues, focused workshops, hostingof the computer system security and privacy advisory board, consultation with and education of federal agency personnel on security practices and issues, and coordination withother agencies; it has not had the resources for and therefore a track record in relevantresearch. see, for example, computer science and telecommunications board (1991, 1996).82these changes have been linked to greater sensitivity to cost and time to market amongtelecommunications providers. results include decreasing redundancy of facilities, an increase in reliance on software, proliferation of features and services (e.g., call forwarding)that promote complexity in telecommunications systems (and unreliability), and other costcontaining steps that can increase vulnerabilities. see board on telecommunications andcomputer applications (1989).trust in cyberspacecopyright national academy of sciences. all rights reserved.the economic and public policy context219finance sector, regulation has promoted incident reporting, auditing, andother actions that motivate or reinforce plans and procedures to promotetrustworthiness, and financial incidents receive special law enforcementassistance via the u.s. secret service.publicprivate partnershipsa telling sign of the growing importance of the commercial information technology sector relative to government is the rise in rhetoric aboutpublicprivate partnerships. experiences with information security suggest that outside certain safety and reliabilitycritical contexts, government mandates and controls on technology are decreasingly effective andthat some form of cooperation is the logical alternative. at the same time,neither the computer security act nor any other legislation assigns responsibility for assisting nongovernmental entities to protect their information systems and networks.83the pccip has called expressly for publicprivate partnerships toincrease information systems trustworthiness, as has the white houseoffice of science and technology policy (executive office of the president, 1997). complementary work was undertaken earlier and concurrently by the nstac and its information assurance task force, whichdrew on participants from private firms.today, the meaning of “partnership” must be developed and translated into action. what can and will happen will depend on developingincreased trust between the private and public sectors, and in particular,the degree of trust in the government. the cryptography policy debatessuggest a loss of trust in government by the commercial information technology sector that must be acknowledged in formulating new policiesand approaches. trade and advocacy organizations84articulate industrypositions to congress and executive branch agencies, and a wide range ofissues relating to trustworthiness are now argued in government circlesthat previously might have been simply decided with minimal consultation with the private sector or even ignored. unilateral government insistence on its position or its preferred solutions—even if cloaked in the83the absence of an effective structure for addressing civilian and commercial needs washighlighted in two cstb reports, computers at risk: safe computing in the information age(cstb, 1991) and cryptography’s role in securing the information society (cstb, 1996).84such organizations include the information technology information council (formerlythe computer and business equipment manufacturers association), the information technology association of america (formerly the association of data processing systems organizations), the software publishers association, the business software alliance, the computer systems policy project, the electronic frontier foundation, and the electronic privacyinformation center, among others.trust in cyberspacecopyright national academy of sciences. all rights reserved.220trust in cyberspaceguise of promoting partnerships with or education of nongovernmentalentities—is unlikely to result in lasting or stable engagement with theprivate sector.if equipped with resources adequate to do the job and to appearindependent in its action, nist could facilitate such partnerships; itsmoves to facilitate commercial system evaluation (i.e., national information assurance partnership) support this prospect. the pccip endorseda greater role for nist while calling for more involvement of a number ofagencies in the information assurance cause. one ongoing experiment iscalled the manhattan cyber project, a privatesector group with government inputs aimed at documenting attacks and incidents (harreld, 1997).the changing marketgovernment relationshipin the notsodistant past, the number of commercial firms capable ofproviding trustworthiness products or services was relatively small.thus, the federal government needed to influence only a small number oforganizations in order to promote greater trustworthiness. these organizations had incentives to respond positively to federal government concerns because of a formal relationship that existed with the federal government (e.g., at&t as a regulated monopoly), or because they weremotivated to be cautious as a consequence of ongoing antitrust investigations (in the case of ibm), or because they sold products in large quantities to the federal government (in the case of both at&t and ibm).today’s vendors of trustworthinessrelated products are many anddiverse, ranging in size from small startups to fortune 100 companies.many of today’s product vendors and service providers have arisen in amore competitive and libertarian culture, and market responsiveness isthe most highly held value for these companies. despite some degree ofconcentration in the supply of computing systems (in both hardware platforms and software), it is now harder to find large telecommunications orcomputer systems providers with both the market penetration and thetradition of responding to publicsector requests for reliability that historically characterized at&t and ibm. although the federal governmentcontinues to be the largest customer of computing and communicationsproducts and services, its market share has decreased dramatically during the past few decades—with a concomitant decline in the federalgovernment’s influence in the marketplace.the emergence of a number of important suppliers from other countries complicates matters further, as foreign governments and firms haveeven less motivation to be friendly to u.s. government or societal interests. examples raised by people who note this concern include siemens,alcatel, checkpoint, and sap, on the basis of ownership rather than anytrust in cyberspacecopyright national academy of sciences. all rights reserved.the economic and public policy context221specific evidence. for example, baan and sap are nonu.s. companieswhose significant number of u.s.based customers will entrust their operating models and internal manufacturing system knowledge to their products and, by extension, sales forces (edmondson et al., 1997). checkpoint,an israeliowned company, is one of the leading firewall vendors. indeed, there arises the possibility that these nonu.s. firms may be responsive to their home governments rather than the u.s. government.findings1.the federal government has a broad and increasing interest in nistrustworthiness. trustworthy niss are important for the government toaccomplish agency missions, address changing government informationtechnology infrastructures, protect national interests, and facilitate andsupport research and development in areas critical to the nation.2.federal government mandates and controls on technology are decreasingly effective. therefore, some form of cooperation with the private sector (e.g., partnerships) is appropriate. building trust between theprivate and public sectors is essential to achieving increased cooperationin efforts to improve nis trustworthiness.3.the federal government has less influence on vendors than in thepast because the number of vendors of trustworthiness products and services has increased considerably and these vendors include small startups that, in particular, are focused on marketplace demands. as trustworthinessrelated products and services are increasingly provided bynonu.s. companies, the influence of foreign firms and governments onthe trustworthiness marketplace is a new concern.the roles of the nsa, darpa, and other federalagencies in nis trustworthiness researchand developmentresearch relating to nis trustworthiness is conducted and supportedby many federal government organizations. some agencies conduct research directly (e.g., nsa, department of energy national laboratories);others fund research that is conducted externally (e.g., darpa); and afew agencies support both internal and external research (e.g., nsa). internal research, some of which is classified, is difficult to assess; timeconstraints precluded further consideration in this report. as discussedearlier in this chapter, industry also conducts “research,” but it emphasizes applied research and development in its activities and rarelyachieves depth in any given area of inquiry (mayfield et al., 1997). thisshortterm emphasis by the private sector may lead to products, but ittrust in cyberspacecopyright national academy of sciences. all rights reserved.222trust in cyberspacealso creates an enduring federal role in trustworthiness research. moreover, some requirements that are unique to the federal government areunlikely to be met by the commercial market.through its national laboratories, the department of energy (doe)has supported projects that have developed information security tools fornetwork inspection and workstation protection; these tools are availableto the entire doe community, including its contractors. the lawrencelivermore national laboratory is the host for the computer securitytechnology center, which serves the entire federal government with respect to information security needs. sandia national laboratories conducts a variety of research activities that support the development ofhighassurance software, more from a reliability and safety rather than asecurity standpoint. in addition, sandia national laboratories has a longhistory of conducting vulnerability assessments of highconsequence systems, such as those intended to prevent uncommanded release of nuclearweapons.the national aeronautics and space administration (nasa), throughits assessment technology branch (atb), develops advanced methods forthe specification, design, and verification of complex software systemsused in critical aerospace applications to minimize the frequency of design errors and to promote fault tolerance in the presence of componentfailures. atb’s work focuses on formal methods for assuring safety andintegrity and develops measures of system quality and tools to applythose measures. techniques and approaches showing significant potential for improving the quality or safety of aerospace computing systemsare transferred to u.s. aerospace interests and to other u.s. customers. inaddition to coordinating its work with that of the dod, atb works withthe federal aviation administration to transfer applicable research results to civil aircraft certification guidelines, specifications, and recommended procedures.85 nasa also supports the software independentverification and validation facility, whose role is to assist customers inthe development of highquality software.finally, the national science foundation (nsf) supports some research on information systems trustworthiness. for example, the software engineering and languages program in the division of computingand communications research supports research on technical issues thatunderlie the design, validation, and evolution of softwarebased systems.research topics include domainspecific languages for specification and85description adapted from material available online at <http://atbwww.larc.nasa.gov/atbcharter.html>.trust in cyberspacecopyright national academy of sciences. all rights reserved.the economic and public policy context223design; various approaches to software design and evolution; issues ofsoftware modularity and composition; techniques to enhance confidenceand quality; software security; and software design environments thatincorporate semantic knowledge.86 the nsf has also funded cryptography projects as a part of its efforts in computational and complexitytheory.incomplete and incompatible statistics complicate an assessment ofrelevant research support across federal agencies, and the tendency forindividual agency programs to change regularly (as projects start andfinish and as programs are revised) compounds the problem. some grossobservations can be made to characterize the situation as of this writing.within the federal government, external research relating to informationsystems trustworthiness is coordinated by the interagency computing,information, and communications (cic) r&d subcommittee. about 12federal departments and agencies participate in coordinating programplanning, budgeting, and review. the cic r&d subcommittee is dividedinto five components, and trustworthiness activity is largely associatedwith the high confidence systems (hcs) component.87 in terms of research support, nsa and darpa dominate the cic agencies involvedwith hcs, with fy 1997 spending listed as $7.3 millon and $10 million,respectively, out of a $30 million component total. other componentsinclude high end computing and computation, large scale networking, human centered systems, and education, training, and human resources—each of which can contribute to or be affected by trustworthiness.the federal government has sought to promote coordination amongentities on trustworthiness r&d, and it has linked defense and civilianand mission and research agencies through the hcs working group.there is also an evolving information security (infosec) research councilthat includes darpa, disa, nsa, nist, doe, the cia, and the militaryservices. the pccip has recommended additional interagency coordination structures, building on the teams it assembled while conducting itswork.86description adapted from material available online at <http://www.cise.nsf.gov/ccr/selhome.html>.87the hcs program was announced as one of six focus areas in the 1995 strategic implementation plan of the committee on information and communications (cic) r&d, whichcoordinates computing and communications r&d across the federal government. cicplanning includes r&d activity in the areas of components, communications, computingsystems, support software and tools, intelligent systems, information management, andapplications.trust in cyberspacecopyright national academy of sciences. all rights reserved.224trust in cyberspacethe focused coordination effort comes from the darpansadisajoint technology office (jto).88 specifically, the role of the informationsystems security researchjoint technology office (issrjto) is “to optimize use of the limited research funds available, and strengthen the responsiveness of the programs to disa, expediting delivery of technologies that meet disa’s requirements to safeguard the confidentiality,integrity, authenticity, and availability of data in department of defenseinformation systems, provide a robust first line of defense for defensiveinformation warfare, and permit electronic commerce between the department of defense and its contractors.”89national security agencythe national security agency is responsible for (1) providing intelligence through the interception, collection, decryption, translation, andprocessing of foreign communications signals and (2) developing cryptographic and other information security techniques to protect classifiedand unclassified (but sensitive) u.s. communications and computer systems associated with national security.90in support of its informationsecurity mission, the nsa historically has developed very high qualitycryptographic equipment and keying material for the department of defense and other customers in the u.s. government (e.g., the state depart88the joint technology office (jto) was announced in the 1995 “arpa/disa/nsamemorandum of agreement concerning the information systems security research jointtechnology office.” complementing darpa’s ongoing research program relating to system security as well as nsa’s research efforts, the jto is intended to further coordination ofresearch and technology development relevant to meeting dod’s needs for trustworthysystems. it also aims to make the goals and decisionmaking processes for such r&d moreopen and responsive to public needs and concerns. organized as a “virtual” entity thatdraws on personnel and resources otherwise housed at the participating agencies, the jtois expected to harmonize the individual agency programs much as the high performancecomputing and communications initiative has harmonized those of its component agencies, while leaving research management (e.g., broad area announcements in the case ofdarpa) and ultimate source selection decision making to those agencies.89see“memorandum of agreement between the advanced research projects agency,the defense information systems agency, and the national security agency concerningthe information systems security research joint technology office”; moa effective april2, 1995. the full text of the moa is available online at <http://www.ito.darpa.mil/researchareas/informationsurvivability/moa.html>.90under the national security act of 1947, a restructured intelligence community wascreated. subsequent executive orders have revised or reordered the intelligence community (and continue to do so). the national security agency (which replaced the armedforces security agency) was created by presidential directive by president truman in 1952.a number of documents that describe nsa’s mission are classified, but a basic missionstatement is now available on an nsa web site, <http://www.nsa.gov:8080>.trust in cyberspacecopyright national academy of sciences. all rights reserved.the economic and public policy context225ment). for years, the primary focus of the nsa was on protecting theconfidentiality of communications. as the boundary between communications and computing has blurred, the nsa has focused its protection oninformation security rather than more narrowly on communications security (see box 6.6).the growing dependence on cots technology in the dod necessitates a strong nsa interest in cots trustworthiness and the integrationof cryptography into cots products. nsa’s special customer market issmall enough and the potential for nsa control is sufficient to discouragemany producers of cots products from meeting nsa’s special needsdirectly; because of its low and shrinking influence on the market, nsaneeds to understand and work with cots technology and vendors. theshift to cots products raises questions about the scope of national security concerns and what they imply for technology strategies to meet theneeds of national security entities, the primary client of nsa.box 6.6the nsa mission:from communications security to information securitythe 1995 national security agency (nsa) corporate plan for information systems security laid out a broad mission:[nsa’s] infosec [information security] mission is to provide leadership, products, and services necessary to enable customers to protect national securityand sensitive information in information systems pursuant to federal law andnational policies, and to provide technical support to the government’s effortsto incorporate information systems security into the national information infrastructure. our customers include national security community members handling classified and sensitive information, as well as those civil governmentagencies and, when requested, private sector organizations providing vital national services. we serve our customers by assessing their needs, deliveringsolutions, and creating advanced infosec technologies. we also promotesecurity for the national information infrastructure through our policy and standards work, our efforts in public advocacy and education, and our role in shaping commercially available security technology. (p. ii)more recently, the 1996 national cryptologic strategy for the 21st century1 explicitly related military and commercial vulnerability to interconnectivity, interoperability, and increased reliance on commercial offtheshelf products and services.1using july 1996 briefing charts, john davis, ncsc director, described this program to thecommittee during its october 21, 1996, visit to nsa.trust in cyberspacecopyright national academy of sciences. all rights reserved.226trust in cyberspacepartnerships with industryincreasingly, partnering with industry is seen as an approach for lowering government research costs, ensuring the relevance of solutions, andexpediting the transfer of research into products. on the other hand,anecdotal evidence91 points to concerns about the direct and opportunitycosts of engineering efforts that respond to nsa’s concerns without generating products that see widespread use (mayfield et al., 1997). meanwhile, growing recognition of the need for trustworthiness combined withincreased dependence on niss continues to lead more organizations (e.g.,banks) with high levels of concern about information security to approachnsa for consultation and assistance. the national computer securitycenter was formed by nsa in the early 1980s as a communications conduit for information security technology. more recently, the nsa national cryptologic strategy92 described and encouraged a “zone of cooperation” among the law enforcement and national security communities,the public sector generally, and the private sector.another example of reaching out is the nsa effort in the early 1990sconcerning the multilevel information systems security initiative (missi),which was originally intended to provide a set of products and an architectural framework that would facilitate the development of multilevelsecure niss. a key aspect of missi was to promote broader use offortezza technology93 through partnerships with industry. missi embodied the view that secure hardware and software had to be developedtogether, something that the cots market eschews. for this and otherreasons, it is widely acknowledged that missi was both a technical andmarketplace failure; nevertheless, the multilevel security concerns embodied in missi—that truly secure solutions require integrated approaches—continue to shape nsa management thinking.94 an alternateway to leverage cots technology is through the development of standards, such as common application programming interfaces (apis) thatpermit the development and use of security products with differingstrength. such standards have promise in satisfying the needs of diversecommunities of security customers. the use of apis seems to the committee to be more appealing to industry than missi, although acknowledging that apis and missi are not directly comparable because apis do not91such evidence includes the experiences of committee members.92john davis, ncsc director, described this program to the committee during its october21, 1996, visit to nsa.93fortezza was originally designed for use with only unclassified data. other products,never deployed, were to provide analogous cryptographic protection for classified data.however, over time missi’s focus changed (see chapter 4, box 4.4, for additional details).94committee discussion with r2 managers, october 21, 1996.trust in cyberspacecopyright national academy of sciences. all rights reserved.the economic and public policy context227address system security or assurance issues. however, apis are consistent with the notion that successful solutions in industry are likely to beaddons, rather than integrative solutions. furthermore, some apis, notably those for cryptographic functions, can run afoul of export controlrestrictions.the u.s. trusted computer system evaluation criteria (tcsec) effort represents a further attempt by nsa to partner with the private sector. in this area, nsa insisted on specific conceptual models and corresponding technology, such as the information flow security models foraccess control at higher levels of the tcsec. the result was a differentand more costly orientation to authentication and access control thanevidenced by policy models apparent in industry. no commercially viable products emerged from this effort, and today it is regarded as essentially irrelevant to current cots information technology.the effectiveness of such outreach efforts has been limited in the pastby such factors as public mistrust of a historically secretive agency; thelack of public awareness, understanding, and support for the tcsec andevaluated product list; and the ambiguity inherent in a public outreacharm in an agency constrained by statute to national security interests(cstb, 1991). current efforts may prove more successful, but they mustovercome a legacy of suspicion originating in nsa’s traditional secrecy aswell as its role in controversies surrounding such efforts as the tcsec,clipper chip/fortezza, and its desires for controls on exports of information security devices.95other factors inhibit cooperation between nsa and the private sector. the environment in which privatesector information security needsare manifested may be different enough from the defense and foreignpolicy worlds that these technologies may not be particularly relevant inpractice to the private sector.96 furthermore, the rapid pace of commercial developments in information technology may make it difficult for theprivate sector to use technologies developed for national security purposes in a less rapidly changing environment (cstb, 1996).95this distrust and suspicion of nsa are enhanced by nsa’s history of controlorientedinteractions with industry. the technology marketplace is a worldwide marketplace. formany companies at least half of their income is derived from outside the united states.advanced technology, especially cryptography, is subject to export controls, and nsa hasplayed a significant role in advising the u.s. government on which technologies can beexported as commodities. the recent declassification of skipjack and kea is a step in theright direction; the declassification was done explicitly to allow industry to implementfortezzacompatible software, thus enabling very low cost cryptographic “soft tokens.”96for example, military users may be willing to tolerate a higher degree of inconvenienceto obtain the benefits of security.trust in cyberspacecopyright national academy of sciences. all rights reserved.228trust in cyberspacer2 programto support its mission, nsa funds and conducts research through anorganization called r, which has research subunits and staff groups thatprovide support for technology forecasting and infosec research outreach.r2 is the nsa research subunit responsible for information security research programs; it is organized into three research divisions: cryptography, engineering, and computer science. in 1997, r2 had more than 100staff members and a contracting budget in the tens of millions of dollars,a portion of which is coordinated with darpa.the major foci of r2 research are enumerated in box 6.7. the dominant areas of r2 research are secure communications technology, assurance technology, and security management infrastructure.97 althoughcryptography has been the centerpiece of nsa’s communication securityproducts and is the dominant technique for providing security withinniss, cryptography was not identified as a dominant emphasis. classified research and research performed by other nsa research elementsand other government and governmentsupported research organizationspresumably provide research support to nsa in this area.the nsa and its r2 organization have developed close working relationships with a group of companies and organizations that have acquired a significant understanding of nsa’s goals and the technologiesinvolved in satisfying those goals. a large portion of the research workfunded by r2 is conducted by selected contractors, federally funded research and development centers (ffrdcs), and researchers at nationallaboratories (e.g., work on quantum cryptography, an example of themore fundamental work supported by r2). although r2 does not, for themost part, use the same open solicitation process used by darpa, forexample, it does review and sometimes funds proposals submitted todarpa. such coordination is a goal of the jto.r2’s small university research program (urp) publishes open solicitations for research and provides modest securityrelated contracts($50,000 to $100,000) to principal investigators in a number of collegesand universities. the program is intended to encourage professors towork in computer and communications security, although published results have not been noteworthy. for example, r2 has supported operating systems (os) work that its management recognizes has not affectedmainstream os work and formal methods work that also has had limitedimpact (e.g., formal verification tools have not been developed as hopedfor).97as reflected in unclassified briefings and materials on funding and staffing levels provided to the committee.trust in cyberspacecopyright national academy of sciences. all rights reserved.the economic and public policy context229in a recent study (anderson et al., 1998), 45 nsafunded projects inthe area of information system security and survivability were identified.although the enumeration may not be comprehensive, it does indicatethe nature and scope of the research funded by nsa (see appendix j).of r2’s contract funds, a significant portion goes to supportnonresearch activities such as participation in standardssetting organizations (e.g., the internet engineering task force, where r2 contributed theisakmp protocol to the ipsec standards effort), consortia membership(e.g., the atm forum, where r2 also contributed to security protocolstandards), and support for infosec education (e.g., biometrics consortium, network security management forum, and support for infosecstudies at the naval postgraduate school and the university of maryland). numerous activities, both external and contract funded, are focused on understanding and assessing various products and technologies(e.g., hacker tools, cryptography for electronic cash). r2 also supportsseveral efforts to modify cots products to incorporate new or expandedsecurity functionality (e.g., biometrics access controls and intrusion detection for windows nt).box 6.7r2’s research activities•secure communications technology—dealing primarily with optical, wireless,digital speech encoding and compatible digital encryption technology in very highspeed communications networks.•assurance technology—including formal methods, risk management, andfault tolerance.•secure management infrastructure—significant effort in key and certificatemanagement, protocols including ipsec and isakmp, standardization efforts, andmulticast key management.•identification and authentication—with significant emphasis on biometrics.•policy invocation and enforcement—including architectures, system composition, and distributed computing.•damage detection and response—covering defensive information warfare,damage indicators, and recovery responses.•information domain definition—including boundary defenses and mappingnetwork boundaries.•cryptography—primarily classified research by its own staff (only part of thenational security agency’s cryptography research effort).source: based on program management information supplied by r2 in 1997.trust in cyberspacecopyright national academy of sciences. all rights reserved.230trust in cyberspaceissues for the futurethe committee reviewed a draft of r2’s“information system securityresearch program plan,” which was revised multiple times in 19961997.98this plan calls for greater interaction with the entire infosec communityand a more open but focused r2 research program, which would be basedon input from an infosec research council (sponsored by nsa and including participants from the relevant agencies and the military services), anational infosec technical baseline (established by nsa, doe, and doe’snational laboratories), and an infosec science and technology study group(composed of leading experts who would provide an infosec perspectivefrom the private sector). by design, the draft plan would support technology r&d “consistent with the fundamental security principles and concepts articulated in the dod goal security architecture” (burnham, 1997).to ensure a supply of knowledgeable experts in the future, the draft plancalls for the establishment of academic centers for infosec studies andresearch. the plan also emphasizes technology transfer to the infosec sideof nsa, to the military services, and to industry.the committee believes that r2 faces two related challenges. onechallenge is its research portfolio. because nsa both funds externalinfosec research and performs internal infosec research, questions arise asto the appropriate allocation of effort (internal and external) and its coordination. decisions about internal effort, like decisions about externaleffort, should recognize where the parties have comparative advantage.highly classified cryptographic research is a natural choice for internalresearch; nsa has widely recognized strength in that area and has betteraccess to mathematical talent in terms of both caliber and number orresearchers. other areas of trustworthiness, less constrained by classification requirements, seem more appropriate for r2 to pursue externally.the second critical issue is the recruitment, retention, and continuingeducation of highquality talent to pursue noncryptographic trustworthiness research areas. in these areas, especially those that depend on computer science, highly skilled researchers available in many academic andcommercial organizations can make significant contributions to infosectechnology. r2 will have to compete for that talent with other agenciesthat have established relationships with top researchers. furthermore,toptier talent with security expertise is scarce, and nongovernment em98authored by blaine burnham, nsa. this document was provided to the committee byr2 when the committee asked for insight into r2’s thinking about future directions. thecommittee examined this document not as a formal plan for nsa, but as a white paper—asa source of possibilities for the future.trust in cyberspacecopyright national academy of sciences. all rights reserved.the economic and public policy context231ployers would appear to offer more rewards, from recognition to pay(lardner, 1998). skills developed in an infosec research group, especiallythose relating to network security, cryptography, and cots software, areeasily marketable in the commercial sector—a fact that constrains bothhiring and retention in r2. finally, there is the perception that the “cloakand dagger image” that once attracted some people to nsa is no longeras strong, because of a smaller defense budget and rapidly growing privatesector alternatives (lardner, 1998).as previously indicated, senior management at nsa and nsa advisory groups have stated that it is difficult to obtain and retain highlyqualified technical research staff with computerrelated expertise for ther2 organization.99 within r2, staff is spread thinly, and loss of an individual can have a significant impact on organizational coverage. further,the ability of a technologist to do research is reportedly limited by administrative and other obligations. the adoption of a rotation program, comparable to those at the nsf and darpa for program managers, could beconsidered as a complement to hiring regular staff members. to be effective, such a program would have to be carefully designed to attract thedesired researchers to the nsa.r2 may be at a disadvantage within nsa inasmuch as its work isremoved from fielded results that constitute nsa successes and its workis not as directly linked to nsa’s mission as that of other units. thesecircumstances can constrain internal communication, and anecdotal evidence suggests that r2 may not always benefit from knowledge of relevant work done by sister units. by contrast, program managers pursuing trustworthiness topics at darpa and nsf have more visibility, andthey and the researchers they fund are free to publish their results.although r2 funds and performs unclassified work, it shares the nsaenvironment and mindset of tightly controlled information. this environment presents a real conflict with the need for access to open researchinformation. it can encourage a closed community of workers who do notcommunicate with others in the community either to seek or contributeinformation. although r2 has increased its outreach, the conferences inwhich it seems most active as an organization, the nsanistsponsorednational information system security conference and its own tech fest,tend to attract a small community of researchers with longstanding connections to nsa. these audiences have only limited interaction with thelarger community of computer science researchers with whom other hcsagency program managers have regular contact.99they note that r2 has not recruited from the academic researchers it supports.trust in cyberspacecopyright national academy of sciences. all rights reserved.232trust in cyberspacefindings1.some government customers have particularly high needs for security, and there are a handful of systems (e.g., “the president’s laptop”)that face levels of threat and require the strength of a mechanism that isnot available in commercial products and that would have insufficientdemand to support a product in the marketplace. the nsa is particularlywell situated to develop such mechanisms. classified cryptographic research is also a natural fit for the nsa internal research program.2.the r2 university research program emphasizes relatively shortterm and small projects. such projects do not tend to attract the interest ofthe best industrial and academic researchers and institutions.3.rotation of r2 researchers with researchers in industry and academia could help to broaden and invigorate the r2 program. such rotation would be most effective with institutions that have large numbers ofleading researchers.4.inadequate incentives currently exist in r2 to attract and retainhighly skilled researchers. improved incentives might be financial (e.g.,different salary scale) and/or nonfinancial (e.g., special recognition,greater public visibility). r2 faces formidable challenges in the recruitment and retention of the very best researchers.5.r2 has initiated several outreach efforts, but these efforts have notsignificantly broadened the community of researchers who work with r2.effective outreach efforts are those that are designed to be compatiblewith the interests, perspectives, and real needs of potential partners.defense advanced research projects agencydarpa’s charter is to fund research that is likely to advance themission of the dod.100 the dod has requirements, such as the need forhigh reliability, accommodation of hostile physical environments, andadaptation to varying contexts of use (e.g., whether and what kind ofwireline communications are possible; nature of wireless infrastructureavailable), that are unique to its mission, as well as requirements that arecommon to other segments of society.trustworthiness is an issue that cuts across darpa’s portfolio tovarying degrees.101 relevant work is concentrated in the informationsurvivability program (with an approximate budget of $40 million peryear) within darpa’s information technology office (ito) (with a budget of $300 million to $350 million per year), which supports research100information about darpa is available online at <http://www.darpa.mil>.101based on examination of publicly available project descriptions.trust in cyberspacecopyright national academy of sciences. all rights reserved.the economic and public policy context233directly applicable to nis trustworthiness. as noted above, this programis coordinated with nsa’s r2 program using the jto established betweenthe two agencies (and disa) for that purpose. universities and industrialresearch establishments are supported, with a program that in 1997 wasdivided into four subareas—highconfidence computing, highconfidencenetworking, survivability of largescale systems, and wrappers and composition.a reasonably broad set of topics is covered (see appendix j), withsome emphasis on fault tolerance and intrusion detection, at least as measured by the number of funded projects in these areas. research in otherareas important for nis trustworthiness, as articulated in previous chapters—containment, denialofservice attacks, cryptographic infrastructures, for instance—although present, is not treated as prominently as itshould be. to support greater use of cots products, the darpa information survivability program has sponsored research in wrappers andother technologies for retrofitting trustworthiness properties to existingcomponents.other programs within ito also support research that impinges onnis trustworthiness in areas such as software engineering, programminglanguages, computer networks, and mobile communications. for example, encryption, reliability, and various aspects of information securityare all concerns in the mobile communications (globalmobile) program.other darpa offices, including the information systems office, supportsome work in electronics and other areas related to nis trustworthiness.finally, darpa has provided funding to nsf to support smallerscaleand more theoretically oriented research projects in trustworthiness andsoftware assurance.darpa funds research based on proposals that it receives from investigators. these proposals are written in response to published broadarea announcements (baas), which outline general areas of research ofinterest based on interactions among program managers, operating unitsof the dod with specific technology needs, and members of the researchcommunity. proposals are evaluated by darpa staff as well as otherswithin the federal government, and competition for the funding is keen.funding levels are high relative to other government sources of researchsupport, reflecting the emphasis on systems that often require researchteams and significant periods of time to develop, allowing darpafunded projects to undertake nontrivial implementation efforts as well aslongrange research.the ito’s culture and its practice of organizing office and programwide principal investigator meetings have fostered contact betweendarpa program managers and the researchers that they support. thiscontact enables the research community to contribute to future darpatrust in cyberspacecopyright national academy of sciences. all rights reserved.234trust in cyberspacefunded research directions, and it helps program managers to catalyzeresearch communities. darpa principal investigator meetings also facilitate interchange among those involved in darpafunded projects.longerterm issues and planning are considered annually at a special,retreatstyle information science and technology (isat) activity organizedaround specific topics. isat enables program managers to interact intensively with small groups of researchers to better understand researchareas (potential baas) for which research funding potential is timely.darpa program managers typically are employed on temporary assignments, although there is a small cadre of longerterm staff. the ranksare populated by academics on leave from their universities, as well asscientists and developers from other branches of the government andfrom industry. limitedterm appointments mean that darpa’s direction and priorities are not static, with obvious advantages and disadvantages. most problematic is that longerterm research agendas may sufferfrom changes in personnel, as newer program managers seek funding forresearch programs they wish to create, which can be achieved only byreallocating resources at the expense of existing programs. another concern is the ability to attract top researchers for brief government stints.those academics with welldeveloped research programs are reluctant toleave them for 2 to 3 years, while those researchers who have been unableto develop such programs are probably not the candidates that darpawould like to recruit.102 on the other hand, top researchers who serve forbrief government stints bring stateoftheart thinking to darpa andmay be more willing than career employees to abandon less promisingstreams of research. because the existence of effective research programsin trustworthiness and survivability is essential, whatever challenges exist in attracting topflight academics must be overcome.the types of research undertaken have varied over the years, depending on priorities within the dod and darpa as well as outside influences (e.g., the nsa, congress). historically darpa projects have beenhigh risk, pushing the envelope of technological capabilities to achievepotentially high payoffs.for example, in the early to mid1970s, there was strong interest indarpa security research, sparked in part by a defense science boardtask force established to address the security problems of multiaccess,resourcesharing computer systems. in an effort to attain the widelyshared goal of creating a multilevel secure operating system, the dodaggressively funded an external research program that yielded many fun102interview conducted by jean e. smith for the computing research association onmarch 25, 1998. data is available online at <http://www.cra.org/crn/>.trust in cyberspacecopyright national academy of sciences. all rights reserved.the economic and public policy context235damental advances in computer security. as one view of darpa in the1970s put it:“the route to a solution—implementing a reference monitorin a security kernel—was widely agreed upon” (mackenzie and pottinger,1997). by reducing some of the research and development risks, thedarpafunded research stimulated the market to develop enhanced security capabilities (cstb, 1991) at the same time that, not coincidentally,the united states led the computer security field and agreement emergedabout the nature and role of an organization that would certify the security of actual systems.not every project was successful. some were canceled, others exceeded budgets, and yet others outlived their practicality. these experiences illustrate some of the difficulties inherent in research. some “failures” are a positive sign as indicators that challenging ideas are beingpursued (which entails some risk) and that spinoffs and learning takeplace, which may be applied to future successful projects.issues for the futurea few university computer science departments have several facultymembers who emphasize computer security research, but many departments have none who do. in any event, the number of computer securityresearchers is small compared to the number in other specialties, such asoperating systems or networks. among the consequences are a paucity ofeducational programs in security and a dearth of security experts. in recentyears, darpa funding for computer security research has been primarilyincremental and short term. longerrange research projects need to befunded, particularly those that address fundamental questions, to developthe basic research that is needed for the longterm vitality of the field.even fewer faculty conduct research programs in some other areas oftrustworthiness, such as operational vulnerabilities. increased funding isimperative to enable reasonable progress in the critical research areasneeded to improve the trustworthiness of niss.although the dodsupport mission does not seem to restrict whatresearch areas darpa pursues, pressures to demonstrate the relevanceof their research investments have generally led darpa program managers to encourage their investigators to produce shortterm results andmake rapid transitions to industry. this approach can discourage investigation of more fundamental questions and experimental efforts, and thusaffect which research topics are explored. some of the research problemsoutlined in this report require longterm efforts (e.g., achieving trustworthiness from untrustworthy components); expecting shortterm payoffmay well have the effect of diverting effort from what may be the morecritical problems or the most effective solutions.trust in cyberspacecopyright national academy of sciences. all rights reserved.236trust in cyberspacethe need for an increased emphasis in research on improving thetrustworthiness of niss in the long term is not consistent with the statedemphases of current ito direction. the current director, in a recent interview,103 articulates three main thrusts for ito: “let’s get physical” refersto moving beyond the metaphor of a human directly interacting with acomputer system to one that places greater attention on the physicalworld. the second main theme, “let’s get real” suggests an increasedfocus on realtime applications; the third theme is “let’s get mobile,”referring to mobile code research. the committee believes that whilesome part of this focus is relevant to the research agenda needed to advance the trustworthiness of niss (e.g., refer to the discussion on mobilecode in chapters 3 and 4), the three themes do not embrace the largemajority of the most important topics.the pccip calls for an increase in federal spending on informationassurance r&d from an estimated $250 million currently to $500 millionin fy 1999 and $1 billion in fy 2004 (pccip, 1997). while the studycommittee certainly endorses the need to increase federal spending ontrustworthiness r&d, the study committee has not seen any publishedrationale for this magnitude of increase. the study committee observesthat for the next several years, the population of experts who are qualifiedto conduct trustworthinessrelated research is relatively fixed, because ofthe lead time needed to recruit and educate new researchers. thus, increased activity in trustworthinessrelated research must be conducted byextant researchers who are already engaged in other work. the studycommittee believes that a quadrupling of the level of activity in the proposed time frame is therefore unnecessary. instead, a lower rate of growththat is sustained over a greater number of years would probably be moreeffective, especially if it is coupled with programs to increase the numberof university training programs in trustworthiness.findings1.darpa funds some research in important areas for nis trustworthiness. however, other critical topics—including containment, denialofservice attacks, and cryptographic infrastructures—are not emphasizedto the extent that they should be.2.the use of academics on temporary assignment as program managers has both advantages and disadvantages. this rotation of programmanagers ensures that stateoftheart thinking is constantly being in103interview conducted by jean e. smith for the computing research association onmarch 25, 1998. data is available online at <http://www.cra.org/crn/>.trust in cyberspacecopyright national academy of sciences. all rights reserved.the economic and public policy context237fused into darpa (assuming that the leading researchers in the field areappointed). on the other hand, such rotation does not promote longtermresearch agendas because a program manager’s tenure typically lasts foronly 2 to 3 years.3.darpa uses a number of mechanisms to communicate with theresearch community, which include principal investigator meetings,isats, and broad area announcements. these mechanisms seem to begenerally effective in facilitating the exchange of ideas between darpaand the research community.4.the nature and scope of major darpa projects funded in the1970s—in which security work was an integral part of a large, integratedeffort—seem to characterize darpa’s greatest successes in the securitydomain. not all of these efforts were entirely successful, as is characteristic of highrisk, highpayoff research. some level of failure is thereforeacceptable.5.the committee believes that increased funding is warranted forboth information security research in particular and nis trustworthinessresearch in general. the appropriate level of increased funding should bebased on a realistic assessment of the size and availability of the currentpopulation of researchers in relevant disciplines and on projections ofhow this population of researchers may be increased in the coming years.referencesanderson, robert h., phillip m. feldman, scott gerwehr, brian houghton, richard mesic,john d. pinder, and jeff rothenberg. 1998. a“minimum essential information infrastructure” for u.s. defense systems: meaningful? feasible? useful? santa monica, ca:rand national defense research institute, in press.board on telecommunications and computer applications, national research council.1989. the growing vulnerability of the public switched networks. washington, dc: national academy press.boehm, barry. 1981. software engineering economics. englewood cliffs, nj: prenticehall.burnham, blaine w. 1997. information system security research program plan version 4.0. ft.meade, md: national security agency (r2) infosec research and technology office, january.canadian system security centre. 1993. the canadian trusted computer product evaluationcriteria version 3.0e. ottawa, canada: the communications security establishment,government of canada, january.carpenter, brian e., and fred baker. 1996. informational cryptographic technology. rfc1984. august.clausing, jeri. 1998.“federal reserve official warns of year 2000 bug,”new york times,april 29.computer science and telecommunications board (cstb), national research council.1991.computers at risk: safe computing in the information age. washington, dc:national academy press.trust in cyberspacecopyright national academy of sciences. all rights reserved.238trust in cyberspacecomputer science and telecommunications board (cstb), national research council.1994.information technology in the service society: a twentyfirst century lever. washington, dc: national academy press.computer science and telecommunications board (cstb), national research council.1996.cryptography’s role in securing the information society, kenneth w. dam andherbert s. lin, eds. washington, dc: national academy press.cummins, arthur j. 1998.“investors are scratching their heads over details of converting to euros,”wall street journal, august 14, p. b8.de jager, peter. 1993. “doomsday 2000,”computerworld, 27(36):105.denning, dorothy e., and giovanni m. sacco. 1981. “timestamps in key distributionprotocols,”communications of the acm, 24(8):533536.diffie, whitfield, and susan landau. 1998. privacy on the line: the politics of wiretappingand encryption. cambridge, ma: mit press.edmondson, gail, stephen baker, and amy cortese. 1997. “silicon valley on the rhine,”businessweek, november 3, p. 162. available online at <http://www.businessweek.com/1997/>.electronic frontier foundation. 1998. cracking des: secrets of encryption research, wiretappolitics & chip design. sebastopol, ca: o’reilly and associates.executive office of the president, office of science and technology policy. 1997. cybernation: the american infrastructure in the information age, a technical primer on risks andreliability. washington, dc: executive office of the president.gertz, bill. 1998. “infowar game shut down u.s. power grid, disabled pacific command,”washington times, april 17, p. a1.harreld, heather. 1997. “group says few fed sites protect privacy: lack of policies andmechanisms puts web visitors at risk,”federal computer week, september 1, p. 10.hellman, martin e. 1979. “des will be totally insecure within ten years,”ieee spectrum,32(7).lardner, richard. 1998. “the secret’s out,”government executive, august. available onlineat <http://www.governmentexecutive.com/features/0898s2.htm>.lemos, robert. 1998. “lloyds to offer firms insurance against hackers,”zdnn, april 23.available online at <http://www.zdnet.com/zdnn/content/zdnn/0423/309664.htm>.mackenzie, donald, and garrel pottinger. 1997. “mathematics, technology, and trust:formal verification, computer security, and the u.s. military,”ieee annals of thehistory of computing, 19(3):4159.masters, brooke a. 1998. “laptop thefts growing: businesses losing computers, secrets,”washington post, march 30, p. b1.mayfield, william t., ron s. ross, stephen r. welke, and bill r. brykczynski. 1997. commercial perspectives on information assurance research. alexandria, va: institute fordefense analyses, october.meissner, p. 1976. report of the workshop on estimation of significant advances in computertechnology. washington, dc: national bureau of standards, december.needham, r.m., and michael d. schroeder. 1978.“using encryption for authentication inlarge networks of computers,”communications of the acm, 21(12):993999.needham, r.m., and michael d. schroeder. 1987. “authentication revisited,”operatingsystems review, 21(1):1.neumann, peter, g. 1990. “rainbows and arrows: how the security criteria addresscomputer misuse.” pp. 414422 in proceedings of the thirteenth national computer security conference. washington, dc: nist/ncsc.noll, roger g. 1996. reforming risk regulation. washington, dc: brookings institution,april.office of the president. 1997.a framework for global electronic commerce. washington, dc:the white house, july 1.trust in cyberspacecopyright national academy of sciences. all rights reserved.the economic and public policy context239president’s commission on critical infrastructure protection (pccip). 1997. critical foundations: protecting america’s infrastructures. washington, dc: pccip, october.senior officials group. 1991. information technology security evaluation criteria. london:european community information systems security, department of trade and industry.u.s. department of defense (dod). 1985. trusted computer system evaluation criteria,department of defense 5200.28std, the “orange book.” ft. meade, md: nationalcomputer security center, december.ware, willis, h. 1995. “a retrospective of the criteria movement,” pp. 582588 in proceedings of the eighteenth national information systems security conference. baltimore, md:national institute of standards and technology/national computer security center.wiener, michael j. 1994.“efficient des key search,” paper presented at the rump sessionof crypto ’93, school of computer science, carleton university, ottawa, ontario,canada, may.wilson, janet. 1998. “the ietf: laying the net’s asphalt,”computer, 31(8):116117.trust in cyberspacecopyright national academy of sciences. all rights reserved.240trust in cyberspace240the vulnerability of our nation’s critical infrastructures is attractingconsiderable attention. presidential decision directive 63, issued in may1998, called for a national effort to ensure the security of the nation’scritical infrastructures for communication, finance, energy distribution,and transportation. these infrastructures all exhibit a growing dependence on networked information systems (niss) that are not sufficientlytrustworthy, and that dependence is a source of vulnerability to the infrastructures and the nation. today’s niss are too often unable to tolerateenvironmental disturbances, human user and operator errors, and attacksby hostile parties. design and implementation errors mean that satisfactory operation would not be guaranteed even under ideal circumstances.there is a gap between the state of the art and the state of the practice.moretrustworthy niss could be built and deployed today. why arethese solutions not being implemented? the answer lies in the workingsof the market, in existing federal policies regarding cryptography, in ignorance about the real costs of trustworthiness (and of not having trustworthiness) to consumers and producers, and in the difficulty of measuring trustworthiness.there is also a gap between the needs and expectations of the public(along with parts of government) and the extant science and technologybase for building trustworthy niss. trustworthiness is a multidimensional property of an entire system, and going beyond what is knowntoday will require research breakthroughs. methods to strengthen onedimension can compromise another; building trustworthy components7conclusions and researchrecommendationstrust in cyberspacecopyright national academy of sciences. all rights reserved.conclusions and research recommendations241does not suffice, for the interconnections and interactions of componentsplay a significant role in nis trustworthiness.security is certainly important (with some data indicating that thenumber of attacks is growing exponentially and anecdotal evidence suggesting that attackers are becoming more sophisticated every day), but itis not all that is important. the substantial commercial offtheshelf(cots) makeup of an nis, the use of extensible components, the expectation of growth by accretion, and the likely absence of centralized control,trust, or authority demand a new approach to security: risk mitigationrather than risk avoidance, technologies to hinder attacks rather thanprevent them outright, addon technologies and defense in depth, andrelocation of vulnerabilities rather than their elimination. but other aspects of trustworthiness also demand progress and also will require newthinking, because the networked environment and the scale of an nisimpose novel constraints, enable new types of solutions, and change engineering tradeoffs.other studies related to critical infrastructures have successfullyraised public awareness and advocated action. this study focuses ondescribing and analyzing the technical problems and how they might besolved through research, thereby providing some direction for that action. the detailed research agenda presented in the body of this reportwas derived by surveying the state of the art, current practice, and technological trends with respect to computer networking and software. asummary of the committee’s findings, conclusions, and recommendationsfollows.protecting the evolving public telephonenetwork and the internetthe public telephone network is increasingly dependent onsoftware and databases that constitute new points of vulnerability. business decisions are also creating new points ofvulnerability. protective measures need to be developed andimplemented.the public telephone network (ptn) is evolving. valueadded services (e.g., call forwarding) rely on calltranslation databases and adjunctprocessors, which introduce new points of vulnerability. some of thenew services are themselves vulnerable. for example, caller id is increasingly used by ptn customers to provide authenticated information, butthe underlying telephone network is unable to provide this informationwith a high assurance of authenticity.management of the ptn is evolving as well. technical and markettrust in cyberspacecopyright national academy of sciences. all rights reserved.242trust in cyberspaceforces have led to reductions in reserve capacity and the number of geographically diverse redundant routings. failure of a single link can nowhave serious repercussions. crossconnects and multiplexors, which areused to route calls, are becoming dependent on complex software running in operations support systems (osss). in addition to the intrinsicvulnerabilities associated with any complex software, information aboutosss is becoming less proprietary owing to deregulation. informationabout controlling the osss will thus become more widespread, and thevulnerabilities of the osss will become known to larger numbers of attackers. similarly, the signaling system 7 (ss7) network used to managecentral office switches was designed for a small, closed community oftelephone companies; with deregulation will come increased opportunities for insider attacks. telephone companies are also increasingly sharing facilities and technology with each other and the internet, therebycreating yet another point of new vulnerability. internet telephony islikely to cause the ptn to become more vulnerable, because internetbased networks use the same channels for both user data transmissionand network management and because the end points on the internet aremuch more subject to failure than those of the ptn.attacks on the telephone network have, for the most part, been directed at perpetrating billing fraud. the frequency of those attacks isincreasing, and the potential for more disruptive attacks, with harassment and eavesdropping as goals, is growing. thus, protective measuresare needed. better protection is needed for the many numbertranslationand other databases used in the ptn. telephone companies need toenhance the firewalls that connect their osss to the internet and to enhance the physical security of their facilities.in some respects, the internet is becoming more secure as itsprotocols are improved and as security measures are more widelydeployed at higher levels of the protocol stack. however, theincreasing complexity of the internet’s infrastructure contributesto its increasing vulnerability. the end points (hosts) of theinternet continue to be vulnerable. as a consequence, theinternet is ready for some business use, but abandoning the ptnfor the internet would not be prudent for most.the internet is too susceptible to attacks and outages to be aviable basis for controlling critical infrastructures. existingtechnologies could be deployed to improve the trustworthinessof the internet, although many questions about what measureswould suffice do not currently have answers because good basicdata (e.g., on internet outages) is scant.trust in cyberspacecopyright national academy of sciences. all rights reserved.conclusions and research recommendations243the operation of the internet today depends on routing and nametoaddress translation services. the list of critical services will likely expandto include directory services and publickey certificate servers. analogous to the ptn, these services, because they depend on databases, constitute points of vulnerability. new countermeasures for nameserverattacks are thus needed. they must work well in largescale, heterogeneous environments. cryptographic mechanisms to secure the name service do exist; however, deployment to date has been limited.cryptography, while not in itself sufficient, is essential to the protection of both the internet and its end points. wider deployment of cryptography is needed. authenticationonly algorithms are largely free fromexport and usage restrictions, and they could go a long way toward helping.there is a tension between the capabilities and vulnerabilities of routing protocols. the sharing of routing information facilitates route optimization, but such cooperation also increases the risk that malicious or malfunctioning routers can compromise routing. in any event, currentinternet routing algorithms are inadequate because they do not scale well,they require central processing unit (cpu)intensive calculations, and theycannot implement diverse or flexible policies. furthermore, no effectivemeans exist to secure routing protocols, especially on backbone routers.research in these areas is urgently needed.networks formed by interconnecting extant independent subnetworks present unique challenges for controlling congestion (because localprovider optimizations may not lead to good overall behavior) and forimplementing security (because trust relationships between network components are not homogeneous). a better understanding is needed of theinternet’s current traffic profile and how it will evolve. in addition, fundamental research is needed into mechanisms for managing congestion inthe internet, especially in a way that does not conflict with network security mechanisms like encryption. attacks that result in denial of serviceare increasingly common, and little is known about defending againstthem.operational errors represent a major source of outages for theptn and the internet. some of these errors could be preventedby implementing known techniques, whereas others requireresearch to develop preventative measures.some errors could be prevented through improved operator trainingand contingency planning. however, the scale and complexity of boththe ptn and the internet (and niss in general) create the need for toolsand systems to improve an operator’s understanding of a system’s statetrust in cyberspacecopyright national academy of sciences. all rights reserved.244trust in cyberspaceand the means by which the system can be controlled. for example,research is needed into ways to meaningfully portray and display thestate of a large, complex network to a human operator. research anddevelopment are needed to develop conceptual models that will allowhuman operators to grasp the state of a network and to understand theconsequences of actions that the operator can take. improved routingmanagement tools are needed for the internet, because they will freehuman operators from an activity that is error prone.meeting the urgent need for softwarethat improves trustworthinessthe design of trustworthy networked information systems presents profound challenges for system architecture and projectplanning. little is understood, and this lack of understandingultimately compromises trustworthiness.systemlevel trustworthiness requirements are typically first characterized informally. the transformation of these informal notionsinto precise requirements that can be imposed on individual systemcomponents is difficult and often beyond the current state of the art.whereas a large software system such as an nis cannot be developeddefect free, it is possible to improve the trustworthiness of such asystem by anticipating and targeting vulnerabilities. but to determine, analyze, and, most importantly, prioritize these vulnerabilitiesrequires a good understanding for how subsystems interact with eachother and with the other elements of the larger system—obtainingsuch an understanding is not possible today. the use of some systematic development processes seems to contribute to the quality of niss.project management, a longstanding challenge in software development, is especially problematic when building niss because of thelarge and complex nature of such systems and because of the continual software changes. the challenges of software engineering,which have been formidable ones for so many years, are even moreurgent in the context of networked information systems.to develop an nis, subsystems must be integrated, but little isknown about doing this. in recent years, academic researchershave directed their focus away from largescale integrationproblems; this trend must be reversed.trust in cyberspacecopyright national academy of sciences. all rights reserved.conclusions and research recommendations245niss pose new challenges for integration because of their distributednature and the uncontrollability of most large networks. thus, testingsubsets of a system cannot adequately establish confidence in an entirenis, especially when some of the subsystems are uncontrollable or unobservable as is likely in an nis that has evolved to encompass legacy software. in addition, niss are generally developed and deployed incrementally. techniques to compose subsystems in ways that contribute directlyto trustworthiness are therefore needed.there exists a widening gap between the needs of software practitioners and the problems that are being attacked by the academic researchcommunity. in most academic computer science research today, researchers are not confronting problems related to largescale integration andstudents do not develop the skills or intuition necessary for developingsoftware that not only works but also works in the context of softwarewritten by others. a renewed emphasis on largescale development efforts is called for.it is clear that networked information systems will includecots components into the foreseeable future. however, therelationship between the use of cots components and nistrustworthiness is unclear. greater attention must be directedtoward improving our understanding of this relationship.cots software offers both advantages and disadvantages to an nisdeveloper. cots components can be less expensive, have greater functionality, and be better engineered and tested than is feasible for customized components. yet, the use of cots products could make developersdependent on outside vendors for the design and enhancement of important components. also, specifications of cots components tend to beincomplete and to compel user discovery of features by experimentation.cots software originally evolved in a standalone environment wheretrustworthiness was not a primary concern. that heritage remains visible. moreover, market pressures limit the time that can be spent ontesting before releasing a piece of cots software. the market also tendsto emphasize features that add complexity but are useful only for a minority of applications.although there are accepted processes for component designand implementation, the novel characteristics of niss raisequestions about the utility of these processes. modern programming languages include features that promote trustworthiness,and the potential may exist for further gains from research.trust in cyberspacecopyright national academy of sciences. all rights reserved.246trust in cyberspacethe performance needs of niss can be inconsistent with modulardesign, and this limits the applicability of various processes and tools. itis difficult to devise componentlevel acceptance tests that fully capturethe intent of systemslevel requirements statements. this is particularlytrue for nonfunctional and userinterface requirements. basing the development of an nis on libraries of reusable, trusted components and usingthose components in critical areas of the system can provide a costeffective way for implementing componentlevel dimensions of trustworthiness. commercial software that includes reusable components or infrastructure is now available, but it is too early to know how successful itwill be.as a practical matter, the use of higherlevel languages increases trustworthiness to a degree that outweighs any risks, although there is inadequate experimental evidence to justify the utility of any specific programming language or language feature with respect to improvingtrustworthiness. modern programming languages include features, suchas compiletime checks and support for modularity and component integration, that promote trustworthiness. the potential may exist for furthergains by developing even moreexpressive type systems and other compiletime analysis techniques.formal methods are being used with success in commercial andindustrial settings for hardware development and requirementsanalysis and with some success for software development. increased support for both fundamental research and demonstration exercises is warranted.formal methods should be regarded as an important piece of technology for eliminating design errors in hardware and software; as such, theydeserve increased attention. formal methods are particularly well suitedfor identifying errors that only become apparent in scenarios not likely tobe tested or testable. therefore, formal methods could be viewed as atechnology complementary to testing. research directed at the improvedintegration of testing and formal methods is likely to have payoffs forincreasing assurance in trustworthy niss.trust in cyberspacecopyright national academy of sciences. all rights reserved.conclusions and research recommendations247reinventing security for computersand communicationssecurity research during the past few decades has been basedon formal policy models that focus on protecting informationfrom unauthorized access by specifying which users shouldhave access to data or other system objects. it is time to challenge this paradigm of “absolute security” and move toward amodel built on three axioms of insecurity: insecurity exists;insecurity cannot be destroyed; and insecurity can be movedaround.formal policy models of the past few decades presuppose that security policies are static and have precise and succinct descriptions. theseformal policy models cannot represent the effects of some malicious orerroneous software, nor can they completely address denialofserviceattacks. finally, these formal policy models cannot account for defensivemeasures, such as virus scan software or firewalls—mechanisms thatshould not work or be needed in theory but, in practice, hinder attacks.the complex and distributed nature of niss, with their numeroussubsystems that typically have their own access controls, raises the question of whether a complete formal security model could ever be specified.even if such a model could be specified, demonstrating the correspondence between an nis and that formal model is not likely to be feasible.an alternative to this “absolute security” philosophy is to identify thevulnerabilities in an nis and make design changes to reposition the vulnerabilities in light of the threats being anticipated. further research isneeded to determine the feasibility of this new approach to the problem.cryptographic authentication and the use of hardware tokensare promising avenues for implementing authentication.networkbased authentication technology is not amenable to highassurance implementations. cryptographic authentication represents apreferred approach to authentication at the granularity that might otherwise be provided by network authentication. needs will arise for newcryptographic authentication protocols (e.g., for practical multicast communication authentication). faster encryption and authentication/integrity algorithms will be required to keep pace with rapidly increasingcommunication speeds. further research into techniques and tools shouldbe encouraged.the use of hardware tokens holds great promise for implementingauthentication. cost will be addressed by the inexorable advance of digital hardware technology. but interface commonality issues will somehowtrust in cyberspacecopyright national academy of sciences. all rights reserved.248trust in cyberspacehave to be overcome. the use of personal identification numbers (pins)to enable hardware tokens is a source of vulnerability that the use ofbiometrics might address. when tokens are being used to digitally signdata, then an interface should be provided so that a user can know what isbeing signed. biometric authentication technologies have limitationswhen employed in network contexts, because the compromise of the digital version of someone’s biometric data could allow an attacker to impersonate a legitimate user over the network.obstacles exist to more widespread deployment of keymanagement technology and there has been little experiencewith publickey infrastructures, especially largescale ones.there are many aspects of publickey infrastructure (pki) technologythat merit further research. issues related to the timely notification ofrevocation, recovery from compromise of certificate authority privatekeys, and namespace management require attention. most applicationsthat make use of certificates have poor certificatemanagement interfacesfor users and system administrators. toolkits for certificate processingcould be developed. there has been little experience with largescaledeployment of key management technologies. thus, the scale and natureof the difficulties associated with deploying this important technology isan unknown at this time.because niss are distributed systems, network access controlmechanisms play a central role in the security of niss. virtualprivate networks and firewalls have proven to be promisingtechnologies and deserve greater attention in the future.virtual private network (vpn) technology is quite promising, although proprietary protocols and simplistic keymanagement schemes inmost products have, to date, prevented adoption of vpns in largerscalesettings. the deployment of ipsec can eliminate these impediments, facilitating vpn deployment throughout the internet. much work remainsto further facilitate wholesale and flexible vpn deployments. support fordynamic location of security gateways, accommodation of complex network topologies, negotiation of traffic security policies across administratively independent domains, and support for multicast communicationare other topics deserving additional work. also, better interfaces forvpn management will be critical for avoiding vulnerabilities introducedby operational errors.firewalls, despite their limitations, will persist into the foreseeablefuture as a key defense mechanism. as support for vpns is added, firetrust in cyberspacecopyright national academy of sciences. all rights reserved.conclusions and research recommendations249wall enhancements will have to be developed for the support of sophisticated security management protocols, negotiation of traffic security policies across administratively independent domains, and managementtools. the development of increasingly sophisticated networkwide applications will create a need for applicationlayer firewalls and a betterunderstanding of how to define and enforce useful traffic policies at thislevel. guards can be thought of as special cases of firewalls, typicallyfocused at the application layer.foreign code is increasingly being used in niss. however, nistrustworthiness will deteriorate unless effective security mechanisms are developed and implemented to defend against attacksby foreign code.authenticating the author or provider of foreign code has not andlikely will not prove effective for protecting against hostile foreign code.users are unwilling and/or unable to use the source of a piece of foreigncode as a basis for denying or allowing execution. revocation of certificates is necessary should a provider be compromised, but revocation iscurrently not supported by the internet, a fact that limits the scale overwhich the approach can be deployed.access control features in commercially successful operating systemsare not adequate for supporting finegrained access control (fgac).fgac mechanisms are needed that do not significantly affect performance. operating system implementations of fgac would help supportthe construction of systems that obey the principle of least privilege, whichholds that users be accorded the minimum access that is needed to accomplish a task.fgac also has the potential to provide a means for supporting foreign code—an interpreter that implements fgac is used to provide a richaccess control model within which the foreign code is confined. that, inturn, could be an effective defense against a variety of attacks that mightbe delivered using foreign code or application programs. however, it isessential that users and administrators can correctly configure systemswith fgac structures, and that has not yet been demonstrated. (considerably simpler access control models today are often misunderstood andmisused.) enforcing application security is increasingly likely to be ashared responsibility between the application and the lower levels of asystem. research is needed to determine how to partition this responsibility and which mechanisms are best implemented at what level. inaddition, more needs to be known about the assurance limitations associated with providing applicationlayer security when employing a cotsoperating system that offers minimum assurance.trust in cyberspacecopyright national academy of sciences. all rights reserved.250trust in cyberspacea variety of opportunities seem to exist to leverage programminglanguage research in implementing system security. software fault isolation and proofcarrying code illustrate the application of programminglanguage analysis techniques to security policy enforcement. but thesetechniques are new, and their ultimate efficacy is not yet understood.defending against denialofservice attacks is often critical forthe security of an nis, because availability is often an important system property. research in this area is urgently neededto identify general schemes for defending against such attacks.no general mechanisms or systematic design methods exist for defending against denialofservice attacks. for example, each request forservice may appear legitimate in itself, but the aggregate number of requests in a short time period that are focused on a specific subsystem canoverwhelm that subsystem because the act of checking a request for legitimacy consumes resources.building trustworthy systems fromuntrustworthy componentsimproved trustworthiness may be achieved by the carefulorganization of untrustworthy components. there are a number of promising ideas, but few have been vigorously pursued.“trustworthiness from untrustworthy components” is a researcharea that deserves greater attention.replication and diversity can be employed to build systems that amplify the trustworthiness of their components, and indeed, there are successful commercial products (e.g., faulttolerant computers) in the marketplace that do exactly this. however, the potential and limits of thisapproach are not understood. for example, research is needed to determine the ways in which diversity can be added to a set of replicas, therebyimproving trustworthiness.trustworthiness functionality could reside in varying parts of an nis.little is known about the advantages and disadvantages of the differentarchitectural possibilities, so an analysis of existing niss would proveinstructive. one architecture that has been suggested is based on the ideaof a core minimum functionality—the minimum essential informationinfrastructure (meii). but building an meii for the nation would be amisguided initiative, because it presumes that the important “core minimum functionality” could be specifically defined, and that is unlikely tobe the case.trust in cyberspacecopyright national academy of sciences. all rights reserved.conclusions and research recommendations251monitoring and detection can be employed to build systems that enhance the trustworthiness of their components. but limitations in systemmonitoring technology and in technology to recognize events, like attacksand failures, impose fundamental limits on the use of monitoring anddetection for implementing trustworthiness. for example, the limits andcoverage of the various approaches to intruder and anomaly detection arenot well understood.a number of other promising research areas merit investigation. forexample, systems could be designed to respond to an attack or failure byreducing their functionality in a controlled, graceful manner. and a variety of research directions involving new types of algorithms—selfstabilization, emergent behavior, biological metaphors—may be useful in defining systems that are trustworthy. these new research directions arehighly speculative. thus, they are plausible topics for longerrange research.social and economic factors that inhibit thedeployment of trustworthy technologyimperfect information creates a disincentive to invest in trustworthiness for both consumers and producers, leading to amarket failure. initiatives to mitigate this problem are needed.decision making today about trustworthy systems occurs within thecontext of imperfect information. that increases the level of uncertaintyregarding the benefits of trustworthiness initiatives, thereby serving as adisincentive to invest in trustworthiness and distorting the market for trustworthiness. as a result, consumers prefer to purchase greater functionalityrather than to invest in improved trustworthiness. products addressingproblems that have been experienced by consumers or that are perceived toaddress wellknown or highly visible problems have been best received.the absence of standard metrics or a recognized organization to conduct assessments for trustworthiness is an important contributing factorto the imperfect information problem. useful metrics for the securitydimension of trustworthiness are unlikely to be developed because thecorresponding formal model for any particular metric would necessarilybe incomplete. therefore, useful aggregate metrics for trustworthinessare unlikely to be developed.standards may mitigate some of the difficulties that arise from imperfect information because standards can simplify the decisionmaking process for the purchasers and producers of trustworthiness by narrowingthe field of choices. the development and evolution of a standard attractscrutiny that will work toward reducing the number of remaining designtrust in cyberspacecopyright national academy of sciences. all rights reserved.252trust in cyberspaceflaws and thereby promote trustworthiness. at the same time, the existence of standards promotes the wide availability of detailed technicalinformation about a particular technology, and therefore serves as a basisfor assessing where vulnerabilities remain. standards that facilitateinteroperability increase the likelihood that successful attacks in one system might prove effective in others. the net relationship between standards and trustworthiness is therefore indeterminate. heterogeneitytends to cause niss to be more vulnerable because the scrutiny of expertsmay not take place, but the negative effects that pertain to standards arealso applicable for homogeneity.security criteria may also improve the level of information availableto both consumers and producers of components. the common criteriamay or may not prove useful for this purpose. in any case, it is doubtfulthat any criteria can keep pace with the evolving threats. however, evenif there are a sufficient number of securityevaluated components, thereis, at present, little or no rigorous methodology for assessing the securityof niss assembled from such evaluated components.consumer and producer costs for trustworthiness are difficultto assess. an improved understanding, better models, and moreand accurate data are needed.trustworthiness typically reflects systemwide characteristics of annis, so trustworthiness costs are often difficult to allocate to specific usersor uses. such costs are therefore often allocated to central units. trustworthiness also involves costs that are difficult to quantify; one exampleis the “hassle factor,” which captures the fact that trustworthy systemstend to be more cumbersome to use.it is difficult to distinguish trustworthiness costs from other directproduct costs and overhead costs. not surprisingly, there is a paucity ofdata, and what little data does exist has questionable accuracy. the production costs associated with integration and testing represent a substantial proportion of total producer costs for improving trustworthiness, andit is often difficult to separate “trustworthiness” costs from other costs.timetomarket considerations discourage the inclusion of trustworthiness features and encourage the postponement of trustworthiness to laterstages of the product life cycle.as a truly multidimensional concept, trustworthiness is dependent on all of its dimensions. however, in some sense, theproblems of security are more challenging and therefore deservespecial attention.trust in cyberspacecopyright national academy of sciences. all rights reserved.conclusions and research recommendations253security risks are more difficult to specify and manage than those thatarise from safety or reliability concerns. there is usually an absence ofmalice with respect to safety and reliability risks as well as tangible andoften severe consequences that can be easily articulated; these considerations facilitate the assessment of risk and measurement of consequencesfor safety and reliabilityrelated risks, in contrast to security. a preciseand testable definition is required to assess whether a standard has beenfulfilled or not. such definitions may often be articulated for some trustworthiness dimensions (such as reliability) but are often difficult to articulate for security.export control and keyescrow policy concerns inhibit the widespread deployment of cryptography, but there are other important inhibitory factors that deserve increased attention andaction.the public policy controversy surrounding export controls and keyrecovery does indeed inhibit the widespread deployment of cryptography. however, cryptography is not more widely deployed for other reasons, which include reduced convenience and usability, possible sacrificeof interoperability, increased computational and communications requirements, lack of a national or international key infrastructure, restrictionsresulting from patents, and the fact that most information is already secure enough relative to its value to an unauthorized party.implementing trustworthinessresearch and developmentin its necessary efforts to pursue partnerships, the federal government also needs to work to develop trust in its relationshipswith the private sector, with some emphasis on u.s.based firms.the federal government has less influence on vendors than in thepast, so cooperative arrangements are increasingly necessary. the rise ofthe marketplace for computing and communications products includesnew and/or startup firms that tend to be focused on marketplace demands generally, and not on the needs of the federal government. although the federal government is the largest single customer of computing and communications products and services, its relative market share,and therefore its market power, have declined. building trust betweenthe private and public sectors is essential to achieving increased cooperation in efforts to improve nis trustworthiness, because the cryptographytrust in cyberspacecopyright national academy of sciences. all rights reserved.254trust in cyberspacepolicy debates concerning export controls and key escrow have createdsuspicion within the private sector about government intent and plans.as trustworthinessrelated products are increasingly provided by nonu.s. companies, the influence of foreign firms and governments on thetrustworthiness marketplace is a new concern and suggests that somepriority should be placed on partnerships with u.s. firms.the nsa r2 organization must increase its efforts devoted tooutreach and recruitment and retention issues.the national security agency’s r2 organization has initiated severaloutreach efforts, but these have not significantly broadened the community of researchers that work with r2. effective outreach efforts are thosethat are designed to be compatible with the interests, perspectives, andrealities of potential partners (e.g., acknowledgment of the dominance ofcots technology).inadequate incentives currently exist within r2 to attract and retainhighly skilled researchers. improved incentives might be financial (e.g.,different salary scale) and/or nonfinancial (e.g., special recognition,greater public visibility) in nature. r2 faces formidable challenges in therecruitment and retention of the very best researchers. the rotation of r2researchers with researchers in industry and academia would help tobroaden and invigorate the r2 program. such rotation would be mosteffective if it involved institutions that have large numbers of top researchers. as currently constituted, the r2 university research programemphasizes relatively shortterm and small projects, and it does not attract the interest of the best industrial and academic researchers and institutions.darpa is generally effective in its interactions with the researchcommunity, but darpa needs to increase its focus on information security and nis trustworthiness research, especially withregard to longterm research efforts.the nature and scope of major defense advanced research projectsagency (darpa) projects that were funded in the 1970s—where securitywork was an integral part of a large, integrated effort—seem to characterize darpa’s greatest successes in the security domain. not all of theseefforts were so successful, as is characteristic of highrisk, highpayoffresearch. darpa does fund some research today in important areas fornis trustworthiness. however, other critical topics—as articulated in thisstudy—are not emphasized to the extent that they should be. these topicstrust in cyberspacecopyright national academy of sciences. all rights reserved.conclusions and research recommendations255include containment, denialofservice attacks, and cryptographic infrastructures.darpa uses a number of mechanisms to communicate with the research community, which include principal investigator meetings, information science and technology activities (isats), and board area announcements (baas). these mechanisms seem to be generally effectivein facilitating the exchange of ideas between darpa and the researchcommunity.the use of academics on temporary assignment as program managershas advantages and disadvantages. this rotation of program managersensures that stateoftheart thinking is constantly being infused intodarpa (assuming that the leading researchers in the field are appointed).on the other hand, such rotation does not promote longterm researchagendas, because the tenure of a program manager typically is only 2 to 3years.an increase in expenditures for research in information securityand nis trustworthiness is warranted.the committee believes that increased funding is warranted for bothinformation security research in particular and nis trustworthiness research in general. the appropriate level of increased funding should bebased on a realistic assessment of the size and availability of the currentpopulation of researchers in relevant disciplines and projections of howthis population of researchers may be increased in the coming years.trust in cyberspacecopyright national academy of sciences. all rights reserved.trust in cyberspacecopyright national academy of sciences. all rights reserved.appendixestrust in cyberspacecopyright national academy of sciences. all rights reserved.trust in cyberspacecopyright national academy of sciences. all rights reserved.fred b. schneider, chairfred b. schneider has been on the faculty of cornell university’scomputer science department since 1978. his research concerns concurrent systems, particularly distributed and faulttolerant ones intended formissioncritical applications. he has worked on formal methods as wellas protocols and system architectures for this setting. most recently, hisresearch has been directed at implementing faulttolerance and securityfor mobile processes (socalled agents) that might roam a network.dr. schneider is managing editor of distributed computing, comanaging editor of the springerverlag texts and monographs in computer science, and a member of the editorial boards for acm computing surveys,ieee transactions on software engineering, high integrity systems, information processing letters, and annals of software engineering. he is coauthor(with d. gries) of the introductory text, a logical approach to discretemath, and he is author of the monograph, on concurrent programming. afellow of the association for computing machinery and the americanassociation for the advancement of science, dr. schneider is also a professoratlarge at the university of tromso (norway). he was a memberof the 1995 arpa/isat study on defensive information warfare and is amember of sun microsystem’s java security advisory council.astudy committee biographies259trust in cyberspacecopyright national academy of sciences. all rights reserved.260appendix asteven m. bellovinsteven m. bellovin received a b.a. degree from columbia universityand an m.s. and ph.d. in computer science from the university of northcarolina at chapel hill. while a graduate student, he helped create netnews;for this, he and the other collaborators were awarded the 1995 usenix lifetime achievement award. he is a fellow at at&t laboratories, where hedoes research in networks and security, and why the two do not get along.he is currently focusing on cryptographic protocols and network management. bellovin is the coauthor of the recent book firewalls and internet security: repelling the wily hacker, and he is a member of the internet architectureboard.martha branstadmartha branstad is a computer security researcher and entrepreneur.she was chief operating officer of trusted information systems inc. (tis)and president of its advanced research and engineering division, directing a research program that encompassed security in networked and distributed systems, applications of cryptography, access control and confinement within operating systems, and formulation of security policyand enforcement within dynamically changing systems. before joiningtis, dr. branstad managed the software engineering program at the national science foundation (nsf), the software engineering program atthe national institute of standards and technology (nist), whose program in performance measurement for parallel processing she established,and research groups at the national security agency (nsa). she holds aph.d. in computer science from iowa state university.j. randall catoej. randall catoe is senior vicepresident of the internet engineering,solutions, operations, and suport group at cable and wireless. previously, as executive director of engineering, catoe led the engineeringportion of vinton cerf’s internet architecture and engineering group formci telecommunications inc. his responsibilities included design anddevelopment of the internetmci backbone, including applications, securityinfrastructure, and the operation of webhosting services. before joiningmci in 1994, mr. catoe served as the team leader and architect for designof data handling and control systems for nasa’s xray timing explorerspacecraft. in previous positions, mr. catoe has served as a vicepresident of engineering for the wollongong group, for which he oversaw thedevelopment of security features in the company’s tcp/ip products.trust in cyberspacecopyright national academy of sciences. all rights reserved.appendix a261earlier in his career, mr. catoe led a team of systems and network engineers in the design and development of mcimail while he was employedat digital equipment corporation.stephen d. crockerstephen d. crocker is an internet researcher and entrepreneur. hewas a founder of cybercash inc. and served as its chief technology officer. he was previously a vicepresident for trusted information systems, a senior researcher at the university of southern california information sciences institute, and a program manager in the advancedresearch projects agency (arpa). dr. crocker was part of the team thatdeveloped the original protocols for the arpanet, which paved the wayfor today’s internet. he served as the area director for security on theinternet engineering task force for 4 years and was a member of theinternet architecture board for 2 years. dr. crocker holds a ph.d. incomputer science from the university of california at los angeles.charlie kaufmancharlie kaufman works for iris associates inc. (a wholly owned subsidiary of lotus development, which is in turn a wholly owned subsidiary of ibm) as security architect for lotus notes. previously, he wasnetwork security architect for digital equipment corporation, and beforethat he worked for computer corporation of america on a researchproject designing highly survivable distributed databases. he is a coauthor of network security: private communication in a public world, published by prenticehall. he chairs the internet engineering task force(ietf) web transaction security working group, and he wrote internetrfc 1507:“dass—distributed authentication security service.” heholds more than 20 patents in the fields of computer networking andcomputer security.stephen t. kentstephen t. kent is chief scientist for information security at bbn corporation and chief technical officer for cybertrust solutions, both part ofgte internetworking. dr. kent has been engaged in network securityresearch and development activities at bbn for 20 years. his work includesthe design and development of user authentication and access control systems, network and transport layer and electronic messaging security protocols, and a multilevel secure directory system. his most recent projectsinclude publickey certification systems, mobile ip security, and securingtrust in cyberspacecopyright national academy of sciences. all rights reserved.262appendix arouting systems against denialofservice attacks. dr. kent served on theinternet architecture board, the oversight body for the internet standardsprocess, from 1983 to 1994, and chaired the privacy and security researchgroup of the internet research task force from 1985 to 1998. in the ietf,he chaired the pem working group and is currently cochair of the publickey infrastructure working group. he served on several computer andnetwork security study committees for the national research council, theoffice of technology assessment, and other government agencies. he wasa charter member of the board of directors of the international associationfor cryptologic research, served on the presidential skipjack reviewpanel for the escrowed encryption system, and chaired the acm specialpanel on cryptography and public policy and the technical advisory committee to develop a fips for key recovery.dr. kent is the author of two book chapters and numerous technicalpapers on network security and has served as a referee, panelist, andsession chair for a number of conferences. he has lectured on the topic ofnetwork security on behalf of government agencies, universities, and private companies worldwide. dr. kent received the b.s. degree in mathematics from loyola university of new orleans, and the s.m., e.e., andph.d. degrees in computer science from the massachusetts institute oftechnology. he is a member of the internet society, a fellow of the acm,and a member of sigma xi.john c. knightjohn c. knight received a b.sc. (mathematics) from the imperial college of science and technology, london, england. he also received aph.d. (computer science) from the university of newcastle upon tyne,newcastle upon tyne, england. from 1974 to 1981 he was employedwith nasa’s langley research center. he has been a member of thecomputer science department at the university of virginia since 1981.from 1987 to 1989 dr. knight was on leave from the university of virginia at the software productivity consortium. dr. knight’s researchinterests lie in software engineering for highdependability applications.the specific topic areas include formal specification, specificationcaptureprocesses, software architectures—especially involving protection shells,verification including rigorous inspections and testing, and the exploitation of reuse for dependability.steven mcgeadysteven mcgeady is vicepresident of intel corporation’s contentgroup and director of intel’s health technology initiative. upon jointrust in cyberspacecopyright national academy of sciences. all rights reserved.appendix a263ing intel in 1985, mr. mcgeady led the software development efforts forintel’s i960 32bit embedded microprocessor. in 1991, he joined intel’ssenior vicepresident ron whittier in forming the intel architecturelabs. as vicepresident and director of multimedia software, mr.mcgeady led the development of intel’s indeo video compression technology, key components of the proshare videoconferencing products,intel’s and turner broadcasting’s cnn@work networked video delivery system, the intercast technology for broadcast web pages, intel’scommon data security architecture, and numerous other advancedtechnology products.as vicepresident and director of internet technology, mr. mcgeadyled intel’s research into the internet, the world wide web, and java,intelligent information filtering and autonomous agents, and new classesof humancomputer interface. he spent the 19961997 academic year as avisiting scientist at the massachusetts institute of technology’s medialab, researching aspects of emergent behavior in networks of personalcomputers. during that time his article, titled “the digital reformation,”was published in the fall 1996 harvard journal of law and technology. mr.mcgeady chairs intel’s research council committees for applications,interface and media, charged with funding and oversight of longrangeacademic research. mr. mcgeady studied physics and philosophy at reedcollege in portland, oregon, where he became an early developer of theunix operating system, compilers, and graphics and networking software.ruth r. nelsonruth r. nelson has been involved in network and computer securityresearch since 1975. most of her career has been at gte governmentsystems, with shorter stays at bbn and digital. in 1993, she left gte andstarted information system security, a research and consulting company.she was an undergraduate and graduate student in pure mathematics atthe massachusetts institute of technology.in 1989, and again in 1992, ms. nelson was an invited participant innsa’s network security working group, which was formed to examinethe agency’s infosec approach and recommend technical and organizational improvements. she was one of the invited attendees at the conference on network evaluation criteria in 1984 and contributed her comments on several drafts of the trusted network interpretation. she hasgiven several colloquia on computer and network security at the university of massachusetts in boston and has assisted on a project to develop agraduatelevel course in network security. she has developed and refined the concept of mutual suspicion, which includes firewalls, localtrust in cyberspacecopyright national academy of sciences. all rights reserved.264appendix aresource control, and the importance of considering security as risk management.allan m. schiffmanallan m. schiffman is chief technologist of spyrus and was founderof its terisa systems subsidiary, which merged with spyrus in mid1997. mr. schiffman has more than 25 years of diverse experience in computing, heading major projects in transportation system modeling, messaging systems, software development tools, programming languageenvironments, and network protocols. he is a regular speaker at industryand academic conferences, frequently gives lectures and tutorials on security, and holds several patents. he has been a member of the worldwide web consortium’s security advisory board and netscape’s security advisory board and frequently consults on the design of communications security systems for electronic commerce. in 1996, he was part of theteam that designed the set payment card protocol commissioned bymastercard and visa.before the formation of terisa systems, mr. schiffman held the position of chief technical officer at enterprise integration technologies (eit),where he was codesigner of the wellknown secure hypertext transferprotocol (shttp). also at eit, mr. schiffman served as principal architect of commercenet, an industry consortium dedicated to promotinginternet commerce. before joining eit, mr. schiffman was the vicepresident of technical strategy at parcplace systems, where he led the development of the company’s wellknown objectworks/smalltalk product family. he has held other senior positions at schlumberger research and thefairchild laboratory for ai research. he received his m.s. in computerscience from stanford university.george a. spixas chief architect in the consumer products division, george a. spixis responsible for microsoft corporation’s endtoend solutions for consumer appliances and public networks. he also serves on the board of thedigital audio video council (davic), the information infrastructurestandards panel (iisp), and the commerce department’s computer systems’ security and privacy advisory board (csspab). mr. spix joinedmicrosoft in 1993 as the director of multimedia document architecture.he was responsible for the advanced consumer technology division’smultimedia tools efforts and early thirdparty tools acquisitions. later, asdirector of infrastructure and services, he headed the team that createdthe services and networks required for early interactive television trials.trust in cyberspacecopyright national academy of sciences. all rights reserved.appendix a265before joining microsoft, spix spent five years as director of systems andsoftware development at supercomputer systems inc. in eau claire, wisconsin. he was responsible for the delivery of systems and softwareproducts for a nextgeneration supercomputer. before that, he workedfor cray research inc. in chippewa falls, wisconsin, as a chief engineer,responsible for systems and software development for the xmp and ympline of supercomputers. a purdue university electrical engineeringgraduate, mr. spix was drawn to supercomputers, their systems, andtheir applications while at los alamos national laboratory.doug tygardoug tygar is a professor at the university of california at berkeley,with a joint appointment in the department of electrical engineering andcomputer science and the school of information management and systems. before joining berkeley, he served on the faculty of the computerscience department of carnegie mellon university.dr. tygar’s interests are in electronic commerce and computer security. he is actively working on several systems projects touching onsubjects including electronic auction technology, special electronic commerce protocols for cryptographic postal indicia to prevent forgery, secure remote execution, and user interfaces for computer security. hisprevious systems work includes netbill (a system for lowcost onlinemicrotransactions), cae tools (developed for valid logic systems, nowpart of cadence), dyad (a system for using secure coprocessors), itoss(integrated toolkit for operating system security), miro (a visual language for file system security specification), and strongbox (a system forselfsecuring programs).dr. tygar was an nsf presidential young investigator and serves onthe infosec science and technology study group. he is active in theelectronic commerce and computer security communities. he consultswidely for both industry and government, has taught a number of professional seminars on these topics, and has served as program chair forseveral conferences in these areas. dr. tygar received his bachelor’s degree from the university of california, berkeley, and his ph.d. fromharvard university.w. earl boebert, special advisorw. earl boebert is a senior scientist at sandia national laboratories.before joining sandia he was the founder and chief scientist of securecomputing technology corporation (sctc), predecessor to today’s secure computing corporation (scc). at sctc/scc he led developmenttrust in cyberspacecopyright national academy of sciences. all rights reserved.266appendix aof the lock, secure network server, and sidewinder systems. he has40 years of experience in the computer industry, with more than 25 ofthem in computer security and cryptography. he is the holder of threeand coholder of five patents in the field, the author and coauthor of abook and numerous papers, and a frequent lecturer. he has been a member of numerous government and industry working groups and panels inthe united states and canada, including the committees of the nationalresearch council that produced the reports computers at risk and for therecord.trust in cyberspacecopyright national academy of sciences. all rights reserved.267june 1011, 1996john c. davis, director, national computer security center, nationalsecurity agencyrobert v. meushaw, technical director, infosec, research and technology organization, national security agencyrichard c. schaeffer, chief, infosec, research and technology organization, national security agencyhoward shrobe, assistant director, intelligent systems and softwaretechnology, defense advanced research projects agencyjuly 1, 1996jeffrey i. schiller, director, network services, massachusetts institute oftechnologyoctober 21, 1996john c. davis, director, national computer security center, nationalsecurity agencyphil gollucci, manager, infosec research and technology, nationalsecurity agencyrobert v. meushaw, technical director, infosec, research and technology organization, national security agencybbriefers to the committeetrust in cyberspacecopyright national academy of sciences. all rights reserved.268appendix bchris mcbride, technology forecasting, national security agencydave muzzy, manager, infosec research and technology, nationalsecurity agencyrick proto, chief, research and technology, national security agencyrichard c. schaeffer, chief, infosec, research and technology organization, national security agencybill semancik, technical director, national security agencybrian snow, technical director for isso, national security agencycarol taylor, infosec, research and technology organization, nationalsecurity agencylee taylor, manager, infosec research and technology, national security agencygrant wagner, technical director, national security agencytom zlurko, infosec, research and technology organization, nationalsecurity agencyfebruary 7, 1997mark schertler, senior applications engineer, terisa systemsnovember 12, 1997john c. davis, commissioner, president’s commission on critical infrastructure protectiontrust in cyberspacecopyright national academy of sciences. all rights reserved.269cworkshop participants and agendasworkshop 1: networked infrastructureworkshop 1 participantswendell bailey, national cable television associationmichael baum, verisign inc.steven m. bellovin, at&t labs researchbarbara blaustein, national science foundationearl boebert, sandia national laboratoriesmartha branstad, computer security researcher and entrepreneurblaine burnham, national security agencywilliam e. burr, national institute of standards and technologydavid carrel, cisco systems inc.j. randall catoe, cable and wirelessstephen n. cohn, bbn corporationstephen d. crocker, steve crocker associatesdale drew, mci telecommunications inc.mary dunham, directorate of science and technology, centralintelligence agencyroch guerin, ibm t.j. watson research centermichael w. harvey, bell atlanticchrisan herrod, defense information systems agencyg. mack hicks, bank of americastephen r. katz, citibank, n.a.trust in cyberspacecopyright national academy of sciences. all rights reserved.270appendix ccharlie kaufman, iris associates inc.stephen t. kent, bbn corporationalan j. kirby, raptor systems inc.john klensin, mci communications corporationjohn c. knight, university of virginiagary m. koob, defense advanced research projects agencysteven mcgeady, intel corporationdouglas j. mcgowan, hewlettpackard companyrobert v. meushaw, national security agencyruth r. nelson, information system securitymichael d. o’dell, uunet technologies inc.hilarie orman, defense advanced research projects agencyradia perlman, novell corporationfrank perry, defense information systems agencyelaine reed, mci telecommunications inc.robert rosenthal, defense advanced research projects agencymargaret scarborough, national automated clearing house associationrichard c. schaeffer, national security agencyrichard m. schell, netscape communications corporationallan m. schiffman, spyrusfred b. schneider, cornell universityhenning schulzrinne, columbia universitybasil scott, directorate of science and technology, central intelligenceagencymark e. segal, bell communications researchgeorge a. spix, microsoft corporationdoug tygar, university of california at berkeleyabel weinrib, intel corporationrick wilder, mci telecommunications inc.john t. wroclawski, massachusetts institute of technologyworkshop 1 agendamonday, october 28, 19967:30 a.m.continental breakfast8:00welcome and overview (stephen crocker) what is trust?• what is complexity?•what are your problems composing networkedinfrastructure?8:15session 1 (george spix and steven mcgeady)trust in cyberspacecopyright national academy of sciences. all rights reserved.appendix c271how are we doing? is the nii trustworthy . . . and how dowe know it?• tell us a story: what failed and how was it fixed?• what do you believe is today’s most critical problem?what is your outlook for its resolution?• what is tomorrow’s most critical problem? what are youdoing to prepare for it?• what is your highest priority for 5 to 10 years out?• is complexity a problem and why?• is interdependence a problem and why?overviewpanelistsearl boebert, sandia national laboratoriesdale drew, mci telecommunications inc. 8:45panel 1––suppliers and toolmakers (george spix and stevenmcgeady)panelistsdavid carrel, cisco systems inc.alan kirby, raptor systems inc.douglas mcgowan, hewlettpackard companyradia perlman, novell corporation 9:45break10:00panel2––delivery vehicles (george spix and steven mcgeady)panelistswendell bailey, national cable television associationmichael harvey, bell atlanticmichael o’dell, uunet technologies inc.11:00panel 3––customers (george spix and steven mcgeady)panelistschrisan herrod, defense information systems agencymack hicks, bank of americastephen katz, citibankmargaret scarborough, national automated clearing houseassociation12:30 p.m.lunch 1:30 p.m.session 2 (steven bellovin)given increasing complexity, why should we expect theseinterconnected (telco, cableco, wireless, satellite, other) networks and supporting systems to work?trust in cyberspacecopyright national academy of sciences. all rights reserved.272appendix c•how do these systems interoperate today in differentbusinesses and organizations?•how will they interoperate tomorrow—how is the technology changing, relative to context?•do they have to interoperate or can they exist as separatedomains up to and into the customer premise?panelists (plus session 1 participants)elaine reed, mci telecommunications inc.frank perry, defense information systems agency 2:30break 2:45session 3 (allan schiffman)•what indications do we have that quality of service differentiated by cost is a workable solution?•what is the intersection of qos and trustworthiness?what are the key technical elements?•how are qos targets met today across networks andtechnologies? what are the trustworthiness tradeoffs ofmultitier, multiprice qos compared to besteffort?panelistsroch guerin, ibm t.j. watson research centerhenning schulzrinne, columbia universityabel weinrib, intel corporationrick wilder, mci telecommunications inc.john wroclawski, massachusetts institute of technology 4:00break 4:15session 4 (stephen kent)the role of publickey infrastructures in establishing trust:tackling the technical elements.•how is “success” defined in the physical world?•what are your current challenges (technical, business, social)?•how can nationalscale pkis be achieved? what technology is needed to service efficiently users who may numberfrom several hundred thousand to tens of millions?•what is your outlook? what are the hard problems?what topics should go on federal or industrial researchagendas?•if multiple, domainspecific pkis emerge, will integration or other issues call for new technology?trust in cyberspacecopyright national academy of sciences. all rights reserved.appendix c273panelistsmichael baum, verisign inc.william burr, national institute of standards andtechnologystephen cohn, bbn corporation 5:30reception and dinnertuesday, october 29, 1996 7:30 a.m.continental breakfast 8:00recap of day one (george spix) 8:45session 5 (steven mcgeady)what is the current status of software trustworthiness andhow does the increasing complexity of software affect thisissue?•tell us a story: what failed and how was it fixed?•what do you believe is today’s most critical problem?how will it be resolved?• what is tomorrow’s most critical problem? what are youdoing to prepare for it?•what happens when prophylaxis fails? how do you compare problem detection, response, and recovery alternatives?•how can we implement safety and reliability as components of trust, along with security and survivability?•is distribution of system elements and control an opportunity or a curse? what are the key technical challenges formaking distributed software systems more trustworthy?•when will all humantohuman communication be mediated by an (enduser programmable or programmableineffect) computer? do we care, from the perspective of promoting trustworthy software? should this influence researchinvestments?panelistsjohn klensin, mci telecommunications inc.richard schell, netscape communications corporationmark segal, bell communications research10:00break10:30continue discussion, session 511:30• hard problems in terms of time frame, cost, and certainty of resulttrust in cyberspacecopyright national academy of sciences. all rights reserved.274appendix c•summary of definitions—trustworthiness, complexity,compositional problems• what are our grand challenges?• discussion, revision; feedback from federal governmentobservers12:00adjournworkshop 2: endsystems infrastructureworkshop 2 participantsmartin abadi, systems research center, digital equipment corporationsteven m. bellovin, at&t labs researchmatt blaze, at&t researchw. earl boebert, sandia national laboratoriesmartha branstad, computer security researcher and entrepreneurricky w. butler, nasa langley research centershiukai chin, syracuse universitydan craigen, odyssey research associates (canada)stephen d. crocker, steve crocker associateskevin r. driscoll, honeywell technology centercynthia dwork, ibm almaden research centeredward w. felten, princeton universityli gong, javasoft inc.constance heitmeyer, u.s. naval research laboratorycharlie kaufman, iris associates inc.stephen t. kent, bbn corporationrohit khare, world wide web consortiumjohn c. knight, university of virginiapaul kocher, cryptography consultantrobert kurshan, bell laboratories inc.peter lee, carnegie mellon universitykarl n. levitt, university of california at davissteven lucco, microsoft corporationteresa lunt, sri internationalleo marcus, aerospace corporationjohn mchugh, portland state universityjohn mclean, u.s. naval research laboratorysteven mcgeady, intel corporationdejan milojicic, the open group research institutej strother moore, university of texas at austinruth r. nelson, information system securitytrust in cyberspacecopyright national academy of sciences. all rights reserved.appendix c275clifford neuman, information sciences institute, university of southerncaliforniaelaine palmer, ibm t.j. watson research centerdavid l. presotto, bell laboratories inc.joseph reagle, jr., world wide web consortiumrobert rosenthal, defense advanced research projects agencyjohn rushby, sri internationalallan m. schiffman, spyrusfred b. schneider, cornell universitymargo seltzer, harvard universitygeorge a. spix, microsoft corporationmark stefik, xerox palo alto research centervipin swarup, mitre corporationdoug tygar, university of california at berkeleybennet s. yee, university of california at san diegoworkshop 2 agendawednesday, february 5, 1997 7:30 a.m.continental breakfast available in the refectory 8:30welcome and overview (fred schneider) 8:45panel 1 (douglas tygar)mobile code: javamatt blaze, at&t researchedward w. felten, princeton universityli gong, javasoft inc.david l. presotto, bell laboratories inc.10:15break10:30panel 2 (douglas tygar)mobile code: alternative approachespeter lee, carnegie mellon universitysteven lucco, microsoft corporationdejan milojicic, the open group research institutemargo seltzer, harvard universityvipin swarup, mitre corporation12:00 p.m.lunch in refectory1:00panel 3 (allan schiffman)rights management, copy detection, access controlcynthia dwork, ibm almaden research centertrust in cyberspacecopyright national academy of sciences. all rights reserved.276appendix crohit khare (accompanied by joseph reagle, jr.), worldwide web consortiumclifford neuman, usc/information sciences institutemark stefik, xerox palo alto research center 2:30break 2:45panel 4 (stephen crocker)tamper resistant devicespaul kocher, cryptography consultantelaine palmer, ibm t.j. watson research centerbennet s. yee, university of california at san diego 4:15break 4:30continue discussion 5:30reception and dinnerthursday, february 6, 1997 7:30 a.m.continental breakfast 8:30introductory remarks (fred b. schneider) 8:45panel 5 (fred b. schneider)formal methods: state of the technologyconstance l. heitmeyer, u.s. naval research laboratoryrobert kurshan, bell laboratories inc.j strother moore, computational logic inc. anduniversity of texas at austinjohn rushby, sri international10:15break10:30panel 6 (john knight)formal methods: state of the practicericky w. butler, nasa langley research centerdan craigen, odyssey research associates (canada)kevin r. driscoll, honeywell technology centerleo marcus, aerospace corporation12:00 p.m.lunch in the refectory 1:00panel 7 (martha branstad)formal methods and securitymartin abadi, digital equipment corporation, systemsresearch centershiukai chin, syracuse universitytrust in cyberspacecopyright national academy of sciences. all rights reserved.appendix c277karl n. levitt, university of california at davisjohn mchugh, portland state universityjohn mclean, u.s. naval research laboratory 2:30concluding discussion 3:00adjournworkshop 3: open systems issuesworkshop 3 participantssteven m. bellovin, at&t labs researchearl boebert, sandia national laboratoriesdick brackney, national security agencymartha branstad, computer security researcher and entrepreneurblaine burnham, national security agencythomas buss, federal express corporationstephen d. crocker, steve crocker associatesmichaeldiaz, motorolabruce fette, motorolawilliam flanagan, perot systems corporationstephanieforrest, university of new mexicobrendas. garman, motorolaiangjeon, liberty financialcharlie kaufman, iris associates inc.stephen t. kent, bbn corporationjohn c. knight, university of virginiajimmykuo, mcafee associates inc.stevenb. lipner, mitretek systemssteven mcgeady, intel corporationjohnfrancis mergen, bbn corporationrobert v. meushaw, national security agencyruth r. nelson, information system securityallan m. schiffman, spyrusfred b. schneider, cornell universitygeorge a. spix, microsoft corporationdoug tygar, university of california at berkeleyworkshop 3 agendamonday, september 29, 19977:30 a.m.continental breakfasttrust in cyberspacecopyright national academy of sciences. all rights reserved.278appendix c8:30welcome and overview (fred schneider and stephen crocker)8:45session 1largescale open transactional systemspaneliststhomas buss, federal express corporationiang jeon, liberty financial10:45break11:00session 2antivirus technology trendspanelistjimmy kuo, mcafee associates inc.12:00lunch1:00session 3intrusion detection: approaches and trendspanelists john francis mergen, bbn corporationstephanie forrest, university of new mexico2:00break2:15session 4costing trustworthiness: process and practice as leverspanelistmichael diaz, motorolaplenary discussion––all participants and committee4:45closing remarks5:00committee caucusdiscussion and dinner with steven lipnertrust in cyberspacecopyright national academy of sciences. all rights reserved.279workshop 1earl boebert“information systems trustworthiness”roch guerin“quality of service and trustworthiness”chrisan herrod“defense information infrastructure (dii): trustworthiness, issues and enhancements”alan kirby“is the nii trustworthy?”radia perlman“information systems trustworthiness”henning schulzrinne“the impact of resource reservation for realtime internet services”mark e. segal“trustworthiness in telecommunications systems”abel weinrib“qos, multicast and information system trustworthiness”workshop 2martin abadi“formal, informal, and null methods”ricky butler“formal methods: state of the practice”shiukai chin“highly assured computer engineering”dan craigen“a perspective on formal methods”edward w. felten“research directions for java security”li gong“mobile code in java: strength and challenges”constance heitmeyer“formal methods: state of technology”dlist of position papers prepared for the workshopstrust in cyberspacecopyright national academy of sciences. all rights reserved.280appendix drohit khare“rights management, copy detection, andaccess control”paul kocher“position statement for panel 4”robert kurshan“algorithmic verification”karl n. levitt“intrusion detection for large networks”leo marcus“formal methods: state of the practice”john mchugh“formal methods for survivability”john mclean“formal methods in security”dejan s. milojicic“alternatives to mobile code”j strother moore“position statement on the state of formalmethods technology”clifford neuman“rights management, copy detection, andaccess control”elaine palmer“research on secure coprocessors”john rushby“formal methods: state of technology”margo seltzer“dealing with disaster: surviving misbehavedkernal extensions”mark stefik“security concepts for digital publishing ontrusted systems”vipin swarup“mobile code security”workshop 3thomas buss“building strong transactional systems”michaeldiaz“assessing the cost of security and trustworthiness”stephanieforrest“immunology and intrusion detection”chengi jimmykuo“free macro antivirus techniques”johnfrancis mergen“gte internetworking”trust in cyberspacecopyright national academy of sciences. all rights reserved.281software is critical for harnessing processing and communicationtechnology. but producing software is difficult, laborintensive, and timeconsuming. because of this, the trend in industry—which is expected tocontinue—has been to develop and embrace technologies that reduce theamount of new programming, hence reduce the costs, involved in developing any software system. niss, which typically involve large and complex software, are acutely affected by this trend.perhaps the most visible example of the trend to avoid programmingfunctionality from scratch is the increased use of commercial offtheshelf(cots) software (systems, subsystems, and libraries of components).through the implementation of higherlevel abstractions and services,specialized skills and knowledge of a few expert developers are leveraged across a large number of systems, with the following results:• the cots software might encapsulate complicated services thatwould be difficult, costly, or risky to build, thereby freeing programmersto work on other, perhaps easier, tasks.• the cots software might implement a user interface, therebyensuring a consistent “look and feel” over sets of independently developed applications.• the cots software might hide lowerlevel system details, therebyenabling portability of applications across platforms that differ in configuration, operating system, or hardware.further leverage can be achieved by using software tools (sometimescalled“wizards”) that allow developers to adapt and customize cotsetrends in softwaretrust in cyberspacecopyright national academy of sciences. all rights reserved.282appendix esoftware without mastering the internals of that software. and scriptinglanguages (ousterhout, 1998), by being typeless and providing programming features such as highlevel abstractions for programming graphicaluser interfaces (guis) and network inputoutput, assume increasing importance in this world where systems are built by “gluing together” existing software.middleware, infrastructure for creating clientserver applications, hasmade significant inroads into the commercial and enterprise softwaresectors. leading vendors—such as sap, oracle, baan, and peoplesoft—are now aggressively directing their efforts toward exploiting the capabilities of global computer networks, like the internet, while at the sametime shielding users of their systems from the complexity of distributedsystems. an emphasis on interoperability not only enables the interconnection of computing systems within a company, but also increasingly isfostering the interconnection of computing systems at different companies. once business partners link their computing systems, messages onnetworks can replace paper as the means by which business is transacted,and new operating modes, such as justintime manufacturing, are facilitated because transactions can be initiated automatically and completedvirtually instantaneously.another way that software developers can avoid writing code fromscratch is to exploit the growing collection of tools for transforming highlevel descriptions into actual code. tools along these lines exist today toimplement network communications software for clientserver distributed systems, databases, and spreadsheets tailored to the needs of someapplication at hand, and for windowbased or formsbased user interfaces. in some cases, the tools output program skeletons, which are thendecorated with programmerprovided applicationspecific routines. inother cases, the tools output selfcontained modules or subsystems, whichare then integrated into the application being developed.software systems in general, and niss in particular, once fielded,invariably come under pressure to evolve. needs change, bringing demands for new functionality, and technology changes, rendering obsoletehardware and software platforms. until recently, the sole solution hadbeen for software developers to periodically issue new releases of theirsystems. evolution of a software system was limited to whatever changesa developer implemented in each new release. the result was far fromideal. users had little control over whether and when their new needswould be addressed; software developers, having limited resources, haddifficulty keeping their systems attractive in an expanding market.extensible software system architectures allow program code to beincorporated into a system after it has been deployed and often even afterit is executing. with extensible architectures, new functionality that cantrust in cyberspacecopyright national academy of sciences. all rights reserved.appendix e283be coded as extensions need not await a new release. users clearly benefitfrom the approach, because extensibility empowers them to evolve asystem’s functionality in directions they desire. and the system’s developers benefit by leveraging others’ programming efforts: the market foran extensible system now expands with every extension that anyoneimplements.there is thus a strong incentive to design and deploy systems that arebroadly extensible. today, web browsers support extensibility throughtheir“helper applications,” which enable the browser to display newforms of content (e.g., video, audio, graphics); extensible operating systems, like microsoft’s windows nt, allow new types of objects and handlers for those objects to be installed in a running system. the next logicalstep, a topic of current research (e.g., bershad et al., 1995; ford et al., 1997;hawblitzel et al., 1998; kaashoek et al., 1997; seltzer et al., 1996), is placingsupport for extensibility at the very lowest levels of the operating system,as this would give the largest scope for extension. much of that work isconcerned with tradeoffs between efficiency and protection, revisitingproblems studied by the operating systems community in the 1970s.the ultimate form of software system extensibility is mobile code—programs that move from one processor to another in a network of computers. when mobile code is supported, a program—unbidden—can arrive at a host processor and start executing, thereby extending or alteringthe functionality of that host’s software. although the idea dates back tothe early days of the arpanet (rulifson, 1969), only recently has it beenattracting serious attention as a generalpurpose programming paradigm.1the now widely available java programming notation supports a restricted form of code mobility, as do microsoft’s activex controls.two technical reasons are usually offered to argue that mobile code isattractive for programming distributed systems. first, the use of mobilecode allows communications bandwidth to be conserved and specializedcomputing engines to be efficiently exploited: a computation can move toa site where data are stored or where specialized computing hardwareexists, process raw data there, and finally move on to another site, carrying only some relevant subset of what has been processed.second, with mobile code, efficient and flexible server interfaces become practical. instead of invoking a highlevel server operation across anetwork, a computation can move to the processor where that server isexecuting and invoke server operations using (local) procedure calls.since the overhead of local calls is low, there is less overhead to amortize1viruses and the postscript document description language are both instances of mobilecode developed for more specialized applications.trust in cyberspacecopyright national academy of sciences. all rights reserved.284appendix eper serveroperation invocation. it therefore becomes feasible for serverinterfaces to offer shorter, more primitive operations and for sequences ofthese operations to be invoked to accomplish a task. thus, the mobilecode dynamically defines its own highlevel server operations—highleveloperations that can be both efficient and well suited for the task at hand.besides these technical arguments, mobile code also provides an attractive architecture for the distribution of software and for system configuration management. today, for example, pc software is often installed and upgraded by customers downloading files over the internet.the logical next step is an architecture where performing an upgradedoes not require an overt action by the customer but instead can be instigated by the producer of that software. mobile code supports just thatarchitecture. push replaces pull, freeing users from a system management task. but using mobile code in this manner relinquishes control in away that affects trustworthiness. the approach also deprives the systemadministrator of control over the timing of software upgrades and configuration management changes. cautious administrators have long refrained from making system changes during crucial projects; a systemthat changes itself might be less stable at such times.referencesbershad, brian n., stefan savage, przemslaw pardyak, emin sirer, craig chambers, marce. fiuczynski, david becker, and susan eggers. 1995. “extensibility, safety and performance in the spin operating system,” pp. 267284 in proceeedings of the 15th acmsymposium on operating systems principles. new york: acm press.ford, bryan, godmar back, greg benson, jay lepreau, albert lin, and olin shivers. 1997.“the flux oskit: a substrate for kernel and language research,”pp. 3151 in  proceedings of the 16th acm symposium on operating systems principles. new york: acmpress.hawblitzel, chris, chichao chang, grzegorz czajkowski, deyu hu, and thorsten voneicken. 1998. “implementing multiple protection domains in java,” pp. 259290 inproceedings of the usenix 1998 annual technical conference,new orleans, louisiana.berkeley, ca: usenix association.kaashoek, m. frans, dawson r. engler, gregory r. ganger, hector m. briceno, russellhunt, david mazieres, thomas pinckney, robert grimm, john jannotti, and kennethmackenzie. 1997. “application performance and flexibility on exokernel systems,”pp. 5265 in proceedings of the 16th acm symposium on operating systems principles.new york: acm press.ousterhout, john k. 1998. “scripting: higherlevel programming for the 21st century,”ieee computer, 31(3):2330.rulifson, j. 1969. decodeencode language (del). rfc 5. june 2.seltzer, margo i., yasuhiro endo, christopher small, and keith a. smith. 1996. “dealingwithdisaster: surviving misbehaved kernel extensions,” pp. 213228 in proceedings ofthe second symposium on operating systems design and implementation (osdi ‘96), seattle,washington.berkeley, ca: usenix association.trust in cyberspacecopyright national academy of sciences. all rights reserved.285computers at risk: safe computing in theinformation agecomputers at risk: safe computing in the information age (cstb, 1991)focused on security—getting more and better computer and communications security into use, thereby raising the floor for all, rather than concentrating on special needs related to handling classified government information. the report responded to prevailing conditions of limited awarenessby the public, system developers, system operators, and policymakers.to help set and raise expectations about system security, the study recommended the following:•development and promulgation of a comprehensive set of generally accepted security system principles (gssp);•creation of a repository of data about incidents;•education in practice, ethics, and engineering of secure systems;and•establishment of a new institution to implement these recommendations.the report also analyzed and suggested remedies for the failure of themarketplace to substantially increase the supply of security technology;export control criteria and procedures were named as one of many contributing factors. observing that universitybased research in computerfsome related trustworthiness studiestrust in cyberspacecopyright national academy of sciences. all rights reserved.286appendix fsecurity was at a “dangerously low level,” the report mentioned broadareas where research should be pursued.report of the defense science board task force oninformation warfare defense (iwd)produced by a defense science board task force, report of the defensescience board taskforce on information warfare defense (iwd) (defensescience board, 1996) focused on defending against cyberthreats and information warfare. the task force documented an increasing militarydependence on networked information infrastructures, analyzed vulnerabilities of the current networked information infrastructure, discussedactual attacks on that infrastructure, and formulated a list of threats (boxf.1) that has been discussed broadly within the department of defense(dod) and elsewhere. the task force concluded:. . . there is a need for extraordinary action to deal with the present andemerging challenges of defending against possible information warfareattacks on facilities, information, information systems, and networks ofthe united states which [sic] would seriously affect the ability of thedepartment of defense to carry out its assigned missions and functions.some of the task force recommendations answered organizationalquestions: where might various functions in support of iwd be placedbox f.1taxonomy of threats•hackers driven by technical challenge•disgruntled employees or customers seeking revenge•crooks interested in personal financial gain or stealing services•organized crime operations interested in financial gain or covering criminalactivity•organized terrorist groups or nationstates trying to influence u.s. policy byisolated attacks•foreign espionage agents seeking to exploit information for economic, political, or military purposes•tactical countermeasures intended to disrupt specific u.s. military weaponsor command systems•multifaceted tactical information warfare applied in a broad, orchestratedmanner to disrupt a major u.s. military mission•large organized groups or major nationstates intent on overthrowing theunited statessource: defense science board (1996).trust in cyberspacecopyright national academy of sciences. all rights reserved.appendix f287and how might they be staffed and managed within the dod? howmight seniorlevel government and industry leaders be made aware ofvulnerabilities and their implications? what legislation is needed? howcan current infrastructure dependencies and vulnerabilities be determined? how can information about ongoing threats and attacks be characterized and disseminated?the other recommendations concerned both short and longertermtechnical means for repelling attacks. the task force urged greater use ofexisting security technology, certain controversial encryption technology,1and the construction of a minimum essential information infrastructure(meii). it also suggested a research program for furthering the development of the following:•system architectures that degrade gracefully and are resilient tofailures or attacks directed at single components;•methods for modeling, monitoring, and managing largescale distributed systems; and•tools and techniques for automated detection and analysis of localized or coordinated largescale attacks, and tools and methods for predicting anticipated performance of survivable distributed systems.the task force noted the low levels of activity concerning computer security and survivable systems at universities.critical foundations: protecting america’sinfrastructuresthe president’s commission on critical infrastructure protection,whose members were drawn from the private and public sector, studiedinfrastructures that are critical to the security, public welfare, and economic strength of the united states: information and communications(e.g., telecommunications), physical distribution (e.g., rail, air, and masstransport), energy (e.g., electric power generation and distribution), banking and finance, and vital human services (e.g., water supply, fire fighting, and rescue). in its report, critical foundations: protecting america’sinfrastructures (pccip, 1997), the commission concluded that all theseinfrastructures were increasingly vulnerable to physical and cyberthreats.and although the threat of cyberattacks today appears to be small, the1specifically, the task force recommended the deployment of the multilevel informationsystems security initiative (missi) and escrowed encryption. those topics are discussed inchapters 4 and 6 of the present report.trust in cyberspacecopyright national academy of sciences. all rights reserved.288appendix fprospect for such attacks in the future was found to be significant.2 alongwith the increasing threat, the commission noted an absence of any national focus for infrastructure protection. formation of a publicprivatepartnership was urged. privatesector involvement was advocated because infrastructure owners and operators, having the expertise and incentive, are best positioned to protect against and detect infrastructureattacks. federal government involvement is needed to facilitate collection and dissemination of information about tools, threats, and intent.the federal government also is ideally situated for detection of coordinated attacks, for overseeing defenseindepth and defenses across infrastructures, and for reducing the possibility that disturbances or attackscould propagate within and across critical infrastructures.broad public awareness regarding the nature and extent of cyberthreats is a necessary part of any defense that hinges on privatesectorparticipation. programs were recommended to elevate public awarenessof infrastructure threats, vulnerabilities, and interdependencies. the commission also recommended considering legislation that would enable federal and privatesector responses to infrastructure vulnerabilities and attacks. the government was also counseled by the commission to serve asa role model for the private sector in the use of standards and best practices, taking precautions that are proportionate to the threat and the valueof what is being protected. substantially increased support for researchwas recommended by the commission; the present level of funding3 wasdeemed insufficient for future needs (davis, 1997). federal support iscrucial—for sound business reasons, the private sector is not likely toinvest significant resources in longerterm research that could fuel neededadvances. the research and development vision articulated by the commission starts with $500 million for fiscal year 1999 and climbs to $1billion in 2004 for governmentsponsored basic research; and the visionhas the private sector using that basic research to create new technologyfor infrastructure protection.the commission suggests a range of research topics. those concerningnetworked computer systems and cyberthreats include the following:•information assurance: the effective protection of the communications infrastructure and the information created, stored, processed, andtransmitted on it.2the report notes that attackers’ tools are becoming more advanced and more accessible,so less skill is needed to launch ever more sophisticated attacks. moreover, the increasinginterconnectivity and complexity of critical infrastructures increase their vulnerability.3government funding was estimated at $150 million per year and industrial funding at $1billion to $1.5 billion per year.trust in cyberspacecopyright national academy of sciences. all rights reserved.appendix f289•monitoring and threat detection: reliable automated monitoring anddetection systems, timely and effective information collection technologies, and efficient data reduction and analysis tools for identifying andcharacterizing localized or coordinated largescale attacks against infrastructure.•vulnerability assessment and systems analysis: methods and tools toidentify critical nodes within infrastructures, to examine infrastructureinterdependencies, and to help understand the behavior of complex systems.•risk management and decision support: methods and tools to helpdecision makers prioritize the use of finite resources to reduce risk.•protection and mitigation: system control and containment and isolation technologies to protect systems against the spectrum of threats.•contingency planning, incident response, and recovery: methods andtools for planning for, responding to, and recovering from incidents suchas natural disasters and physical and cyberbased attacks that affect localor national infrastructures.cryptography’s role in securingthe information societya number of mechanisms for enhancing information system trustworthiness depend on the use of cryptography. cryptography, however,is a doubleedged sword. it can help legitimate businesses and lawabiding citizens keep information confidential, but it can help organizedcrime and terrorists keep information confidential. conflict between theprotection of confidential information for legitimate businesses and lawabiding citizens and the need for law enforcement and intelligence agencies to obtain information has fueled a u.s. policy debate concerning bothimport/export restrictions and domestic deployment of cryptography.the issues are subtle. they were explored during an 18month studyby the national research council’s computer science and telecommunications board (cstb)—the socalled crisis report (an acronym of thereport’s full title) edited by dam and lin (cstb, 1996)—that was completed just as the present nis trustworthiness study was getting underway. bringing together a wide range of perspectives on the subject, thecrisis report concluded that the thencurrent u.s. cryptography policy4was not adequate to support the information security requirements of aninformation society. although acknowledging that increased use of cryptography placed an increased burden on law enforcement and intelli4the report was released in may 1996.trust in cyberspacecopyright national academy of sciences. all rights reserved.290appendix fgence activities, the crisis report asserted that the interests of the nationoverall would be best served by a policy that fosters a judicious transitiontoward broad use of cryptography.crisis does not make recommendations for further research, so it isunlike the other studies just surveyed. what crisis does say is directlyrelevant to the present study in two ways. first, the existence of crisishelped delimit the scope of the present study. with crisis in hand, thepresent study was freed to concentrate on other aspects of informationsystems trustworthiness. second, crisis provides a foundation for thepresent study’s discussions about cryptography policy and its implications regarding widespread deployment of cryptography. as discussedin chapters 2, 4, and 6 of the present study, the broad availability ofcryptography can affect how nis trustworthiness problems are solved.referencescomputer science and telecommunications board (cstb), national research council.1991.computers at risk: safe computing in the information age. washington, dc:national academy press.computer science and telecommunications board (cstb), national research council.1996.cryptography’s role in securing the information society, kenneth w. dam andherbert s. lin, eds. washington, dc: national academy press.davis, john c. 1997. (draft) research and development recommendations for protecting andassuring critical national infrastructures. washington, dc: president’s commission oncritical infrastructure protection, december 7.defense science board. 1996. report of the defense science board task force on informationwarfare defense (iwd). washington, dc: office of the under secretary of defense foracquisition and technology, november 21.president’s commission on critical infrastructure protection (pccip). 1997. critical foundations: protecting america’s infrastructures. washington, dc: pccip, october.trust in cyberspacecopyright national academy of sciences. all rights reserved.291msdos is an operating system designed to operate on singleuserpersonal computers. as a consequence, it provides no identification andauthentication mechanisms and neither discretionary nor mandatory access control mechanisms. any user has access to all resources on thesystem. any access control is provided solely by controlling physicalaccess to the computer itself. if the computer is electronically connectedto any other computer, no access control is possible.unix is a multiuser operating system originally designed by kenthompson and dennis ritchie of bell laboratories. user identification issupported by passwordbased authentication. user ids are associatedwith processes. unix provides a modified version of access control listsfor files. for each file, three fields of access permissions are established,one for the file owner, one for the group in which the owner resides, andone for others (or everyone else). in each access field, permission to read,write, and execute the file is granted by the owner. for example, a filewith access permissions rw/rw/r provides the owner read/write access, the owner’s group read/write access, and all others only read accessto the file. unix provides another feature that affects access controls.each program can have the “setuid” attribute set; if set, the program runswith the access rights of the owner of the program, rather than those ofthe program’s invoker. thus, for practical purposes, the program’s invoker can establish an effective identity other than his or her own that isto be used when determining access permissions.microsoft’s windows nt operating system is designed for workstagsome operating systemsecurity examplestrust in cyberspacecopyright national academy of sciences. all rights reserved.292appendix gtions and servers. user identity is authenticated using passwords. everyactive subject in the system has an associated token that includes a uniqueidentifier, a list of group identifiers, and a set of privileges that allows asubject to override restrictions set by the system. every named object(e.g., files, directories, drivers, devices, and registry keys) in the systemhas an associated access control list (acl). acls can ascribe genericrights (e.g., read, write, and delete) and specific rights that have semanticsonly for a specific class of objects. mediation decisions are made by thesecurity reference monitor based upon the token of the subject, the aclof the object, and the requested access right. there is provision in thesystem for “impersonation,” that is, using authorization of another subject.finally, various products have been designed to provide access control mechanisms as addons for specific operating systems, to augmentthe basic operating system facilities. for example, racf, acf2, and topsecret are all products designed for use with ibm’s mvs(which has almost no intrinsic security).trust in cyberspacecopyright national academy of sciences. all rights reserved.293the four basic types of firewalls are packet filters, circuit relays, application gateways, and dynamic packet filters.packet filter firewalls operate at the network layer, with occasionalpeeks at the transportlayer headers. the information used to make apass/drop decision on a packet is contained in that packet; no state ismaintained. typical decision criteria include source address, destinationaddresses, and transport protocol port number (i.e., service) requested.not all protocols are compatible with packet filters. for example, a common security policy allows most outgoing calls but no incoming calls. apacket filter can implement this policy if and only if protocol headerscontain fields that differentiate between requests and responses. in transmission control protocol (tcp), a single bit value distinguishes the firstpacket of a conversation from all others, so it is possible to drop incomingpackets that do not have this bit properly set. by contrast, user datagramprotocol (udp) lacks such a notion, and it is impossible to enforce thedesired security policy. packet filters normally associate several ruleswith each legal conversation. not only must messages flow from theclient to the server, but replies and even protocollevel acknowledgmentpackets also must flow in the other direction. the lack of state makespossible interactions between the different rules and that can allow certain attacks—for example, a legal acknowledgment packet may also be anillegal attempt to set up a new conversation.circuit relays operate above the transport layer. they pass or dropentire conversations but have no knowledge of the semantics of what theyhtypes of firewallstrust in cyberspacecopyright national academy of sciences. all rights reserved.294appendix hrelay. circuit relays are generally considered to be more secure thanpacket filters, primarily because they terminate instantiations of the transport protocol. interactions between rules is unlikely at the transport layer,and the same mechanisms that normally separate different circuits keeptogether the inbound and outbound packets for a given connection. because connections now terminate at an intermediate point (namely, thefirewall), circuit relays generally require changes in application programs,user behavior, or both. this lack of transparency makes circuit relaysunsuitable for many environments, where transparency and compatibility are important.applications gateways are closely tied to the semantics of the trafficthey handle. typically, a separate program (proxy) is required for eachapplication. a mail gateway might rewrite header lines to eliminate references to internal machines, log senders and receivers, and so on. application gateways can handle traffic whose characteristics render packetfilters and circuit relays inappropriate enforcement mechanisms. for example, by default the file transfer protocol (ftp) (postel and reynolds,1985) uses an inbound channel for data transfers, which packet filterscannot handle safely. an ftp proxy in an application gateway can keeptrack of when a given incoming call should be accepted, and thus canallow what would otherwise be a violation of normal security policies.application gateways are also well suited for sites that require authentication of outgoing calls. since few if any protocols are designed toprovide authentication at an intermediate point like a firewall, a customdesign is necessary for each application. one might require a separatelogin, entirely inband; another might pop up a window on the user’sterminal. application gateways are generally considered to be the mostsecure type of firewall because a detailed knowledge of protocol semantics makes spoofing difficult.dynamic packet filters, the last of the four firewall types, excel attransparency. they merge the packet filter and application gateway typesof firewall. with a dynamic packet filter, most packets are accepted orrejected based solely on information in the packet. however, some packets cause additional processing that modifies the rules that will be appliedto subsequent packets. this enables the udp problem mentioned aboveto be solved: the fact that an outbound query has been sent then conditions the firewall to accept the reply packet, a message that would otherwise have been rejected. more sophisticated processing can be done aswell. for example, dynamic packet filters can be sufficiently aware of ftpto permit the incoming data channel call. some dynamic packet filters areaware of the remote procedure call (rpc) (sun microsystems, 1988) andcan mediate access to individual services. even header address translatrust in cyberspacecopyright national academy of sciences. all rights reserved.appendix h295tion can be performed (egevang and francis, 1994), further isolating internal machines.referencesegevang, k., and p. francis. 1994. the ip network address translator (nat). rfc 1631.may.postel, j., and j. reynolds. 1985. file transfer protocol (ftp). rfc 959. october.sun microsystems. 1988. rpc: remote procedure call protocol specification version 2. rfc1057.trust in cyberspacecopyright national academy of sciences. all rights reserved.296secrecy of design is often deprecated with the phrase “securitythrough obscurity,” and one often hears arguments that securitycriticalsystems or elements should be developed in an open environment thatencourages peer review by the general community. evidence is readily athand of systems that were developed in secret only to be reverse engineered and have their details published on the internet and their flawspointed out for all to see.the argument for open development rests on assumptions that generally, but not universally, hold. these assumptions are that the opencommunity will devote adequate effort to locate vulnerabilities, that theywill come forth with vulnerabilities that they find, and that vulnerabilities, once discovered, can be closed—even after the system is deployed.there are environments, such as military and diplomatic settings, inwhich these assumptions do not necessarily hold. groups interested infinding vulnerabilities here will mount longterm and wellfunded analysis efforts—efforts that are likely to dwarf those that might be launchedby individuals or organizations in the open community. further, thesewellfunded groups will take great care to ensure that any vulnerabilitiesthey discover are kept secret, so that they may be exploited (in secret) foras long as possible. finally, military systems in particular often exist inenvironments where postdeployment upgrades are difficult to achieve.special problems arise when partial public knowledge is necessaryabout the nature of the security mechanisms, such as when a militarysecurity module is designed for integration into cots equipment. rei secrecy of designtrust in cyberspacecopyright national academy of sciences. all rights reserved.appendix i297sidual vulnerabilities are inevitable, and the discovery and publication ofeven one such vulnerability may, in certain circumstances, render thesystem defenseless. it is, in general, not sufficient to protect only the exactnature of a vulnerability. the precursor information from which thevulnerability could be readily discovered must also be protected, and thatrequires an exactness of judgment not often found in group endeavors.when public knowledge of aspects of a military system is required, themost prudent course is to conduct the entire development process undercover of secrecy. only after the entire assurance and evaluation processhas been completed—and the known residual vulnerabilities identified—should a decision be made about what portions of the system descriptionare safe to release.any imposition of secrecy, about either part or all of the design, carries two risks: that a residual vulnerability could have been discoveredby a friendly peer reviewer in time to be fixed, and that the secret parts ofthe system will be reverse engineered and made public, leading to thefurther discovery, publication, and exploitation of vulnerabilities. thefirst risk has historically been mitigated by devoting substantial resourcesto analysis and assurance. (evaluation efforts that exceed the designeffort by an order of magnitude or more are not unheard of in certainenvironments.) the second risk is addressed with a combination of technology aimed at defeating reverse engineering and strict procedural controls on the storage, transport, and use of the devices in question. thesecontrols are difficult to impose in a military environment and effectivelyimpossible in a commercial or consumer one.trust in cyberspacecopyright national academy of sciences. all rights reserved.298in a recent study, anderson et al. (1998) identified a total of 104 individual research projects that were funded in fy 1998 by darpa’s information survivability program, a unit of the information technology office (ito). in addition, 45 information security projects were identifiedfrom the nsa and were included in the anderson et al. (1998) study.these projects were categorized as depicted below (some projects werecounted in two categories).heterogeneitypreferential replication/lifespan, architectural/software diversity,path diversity, randomized compilation, secure heterogeneousenvironmentsnsa r2 = 0 projects; darpa ito = 2 projectsstatic resource allocationhardware technologynsa r2 = 1 project; darpa ito = 0 projectsdynamic resource allocationdetect & respond to attacks/malfunctions, dynamic quality ofservices, active packet/node networks, dynamic securitymanagementnsa r2 = 3 projects; darpa ito = 12 projectsredundancyreplicationnsa r2 = 0 projects; darpa ito = 3 projectsjresearch in information systemsecurity and survivability funded bythe nsa and darpatrust in cyberspacecopyright national academy of sciences. all rights reserved.appendix j299resilience and robustnesscryptography/authentication, modeling and testing, fault/failuretolerant components, advanced languages & systems, wrappers, firewalls, secure protocols, advanced/secure hardwarensa r2 = 28 projects; darpa ito = 54 projectsrapid recovery and reconstitutiondetect and recover activitiesnsa r2 = 0 projects; darpa ito = 2 projectsdeceptiondecoy infection routinesnsa r2 = 0 projects; darpa ito = 0 projectssegmentation/decentralization/quarantinesecure distributed/mobile computing, enclave/shell protection,intruder detection and isolation, specialized “organs,” autonomous self contained units, damage containmentnsa r2 = 2 projects; darpa ito = 11 projectsimmunologic identificationautonomous agents, “lymphocyte” agents, detection of anomalous events, mobile code verification, self/nonself discrimination, information disseminationnsa r2 = 1 project; darpa ito = 12 projectsselforganization and collective behavioradaptive mechanisms, formal structure modeling, emergent properties & behaviors, node/software optimization, marketbasedarchitecture, scalable networks (vlsi)nsa r2 = 0 projects; darpa ito = 10 projectsother/miscellaneousmultiple approaches to network security/survivability, technologyforecastingnsa r2 = 10 projects; darpa ito = 3 projectsreferenceanderson, robert h., phillip m. feldman, scott gerwehr, brian houghton, richard mesic,john d. pinder, and jeff rothenberg. 1998. a“minimum essential information infrastructure” for u.s. defense systems: meaningful? feasible? useful? santa monica, ca:rand national defense research institute, in press.trust in cyberspacecopyright national academy of sciences. all rights reserved.300access generally refers to the right to enter or use a system and its resources; to read, write, modify, or delete data; or to use softwareprocesses or network bandwidth.access control is the granting or denying, usually according to a particular security model, of certain permissions to access a resource.access level is either the clearance level associated with a subject or theclassification level associated with an object.acl (access control list) refers to a list of subjects permitted to access anobject, and the access rights of each one.acm is the association for computing machinery.activex is a set of client and server component interfaces that enablesdevelopers to build multitier applications that use an html rendererand http and other internet protocols. activex is the technologyused to integrate the internet in windows.ada is a programming language that was developed, and subsequentlymandated, for dod software projects.adjunct processors enable the operation of many enhanced telephoneservices, such as 800 numbers and voicemenu prompts.adsl (asymmetric digital subscriber line) allows an upstream data flow(i.e., from user to server) that is a fraction of the downstream dataflow, as is appropriate to support internet services to the home andvideo on demand.ansi is the american national standards institute.kglossarytrust in cyberspacecopyright national academy of sciences. all rights reserved.appendix k301api (application programming interface) is an interface provided for anapplication to another program.arpa: see darpa.arpanet (advanced research projects agency network) was a federally funded wan that became operational in 1968 and was used forearly networking research. it evolved into the central backbone of theinternet.as (autonomous system) is an internet routing domain under the controlof one organization.assurance is confidence that a system design meets its requirements, orthat its implementation satisfies specifications, or that some specificproperty is satisfied.asymmetric (or publickey) cryptography is based on algorithms thatuse one key (typically a public key) to encrypt a message and a different, mathematically related key (typically private) to decrypt a message.atb (assessment technology branch) is part of nasa.atm (asynchronous transfer mode) enables voice, data, and video to behandled with a uniform transmission protocol. it breaks up the information to be transmitted into short packets of data and interspersesthem with data from other sources delivered over trunk networks.authenticationis the process of confirming an asserted identity with aspecified, or understood, level of confidence. the mechanism can bebased on something the user knows, such as a password, somethingthe user possesses, such as a “smart card,” something intrinsic to theperson, such as a fingerprint, or a combination of two or more ofthese.availabilityis the property asserting that a resource is usable or operational during a given time period, despite attacks or failures.baa (broad area announcement) is a form of research solicitation usedby darpa and other federal agencies.bcr (black/crypto/red) was a federally funded project that achieved fullendtoend packet encryption, with full header bypass, in workingprototype form in the mid to late 1970s.bell and la padula policy is a security policy prohibiting informationflow from one object to another with a lesser or incomparable classification.bgp(border gateway protocol) is the protocol used by internet routersto communicate with other routers across administrative boundaries.biometric authentication relies on the use of unique characteristics ofindividuals, such as a voiceprint or fingerprint, for authentication.trust in cyberspacecopyright national academy of sciences. all rights reserved.302appendix kblacker is an integrated set of network layer cryptographic devices designed to secure military data networks.blue box refers to a device used to defraud the telephone company in the1960s and 1970s. it sent network control tones over the voice path.bps (bits per second) refers to the rate at which data are generated by asource or transmitted over a communications channel. measurementsare often stated in units of 103 bits per second (kilobits or kbps) or 106bits per second (megabits or mbps).c++ is a programming language.ca (certification authority) is a trusted party that creates certificates in asecure manner.caneware is a certificatebased, military network encryption system forthe internet.cap (complex arithmetic processor) is a digital signal processor intendedfor use in a secure, multimode, programmable radio.ccitt is the consultative committee on international telephony andtelegraphy.ccf (central control function) is an air traffic management subsystem.ccv2 refers to common criteria, version 2.cdis (central control function display information system) is a component of the ccf.cdsa (common data security architecture) is an integrated softwareframework consisting of apis designed to make computer platformsmore secure for applications such as electronic commerce.cert/cc (computer emergency response team/coordination center)is an element of the networked systems survivability program of thesoftware engineering institute at carnegie mellon university. itkeeps track of attacks on the internet and issues advisories. certadvisories are available online at <http://www.cert.org>.certificate management is the overall process of issuing, storing, verifying, and generally accepting responsibility for the accuracy of publickey certificates and their secure delivery to appropriate consumers.certificationis the administrative act of approving a computer system orcomponent for use in a particular application.cgi (common gateway interface) is a script run by a world wide webserver in response to a client request.checksum consists of digits or bits calculated according to an algorithmand used to verify the integrity of accompanying data.chinese wall (or brewernash) model is a security policy concernedwith separating different organizational activities to conform withlegal and regulatory strictures in the financial world.trust in cyberspacecopyright national academy of sciences. all rights reserved.appendix k303cia is the central intelligence agency.ciao (critical infrastructure assurance office) is a unit of the u.s.government established by pdd 63.cic r&d (computing, information, and communications researchand development) refers to a committee of the national science andtechnology council that involves about 12 federal departments andagencies that coordinate computing and communications programs,budgets, and review.ciphertextis the output of any encryption process, regardless of whetherthe original digitized input was text, computer files or programs, orgraphical images.cleartext(or plaintext) is the input into an encryption process or outputof a decryption process.clefrefers to a commercially licensed evaluation facility.clipper chip is an escrowed encryption chip that implements the skipjack algorithm to encrypt communications conducted over the publicswitched network (e.g., between telephones, modems, or fax equipment).cmm (capability maturity model) is used in judging the maturity of thesoftware processes of an organization. it was developed under thestewardship of the software engineering institute.cmw (compartmented mode workstation) is a computer workstation(rated at least b1 under the tcsec) that implements both discretionary (i.e., identitybased, userdirected) and mandatory (i.e., rulebased, administratively directed) access policies.cnnis the cable news network.cocomo (constructive cost model) is a method for estimating the costof the development of a software system.com (common object model) is an open software architecture.confidentialityrefers to the protection of communications traffic orstored data against interception or receipt by unauthorized third parties.conops (concept of operations) describes the operations of a computingsystem, typically in the form of scenarios.cops (computer oracle password security) is software that checks forcracks, configuration errors, and other security flaws in a computeremploying the unix operating system.corba (common object request broker architecture) is an omg specification that provides the standard interface definition between omgcompliant objects.correctnessis the property of being consistent with a specification. thespecification may stipulate, for example, that proper outputs are produced by a system for each input.trust in cyberspacecopyright national academy of sciences. all rights reserved.304appendix kcots (commercial offtheshelf) refers to readily available commercialtechnologies and systems.countermeasureis a mechanism that reduces or eliminates a vulnerability.cpu is a central processing unit.crisisrefers to cryptography’s role in securing the information society, a1996 report by the cstb.crl (certificate revocation list) identifies unexpired certificates that areno longer valid; that is, the binding expressed by the certificates is notconsidered to be accurate.crossconnectis a component of the telephone system that shunts circuitsfrom one wire or fiber to another.cryptanalysisis the study and practice of various methods to penetrateciphertext and deduce the contents of the original cleartext message.cryptographic algorithm is a mathematical procedure, often used in conjunction with a key, that transforms input into a form that is unintelligible without knowledge of a key and the algorithm.cryptography is the science and technology of establishing or protectingthe secrecy, authenticity, or integrity of data that might be accessedby unauthorized parties by using a code or cipher.csp (communicating sequential process) is a specification and programming notation for concurrent and distributed systems.cstb (computer science and telecommunications board) is a unit ofthe national research council.ctcpecrefers to the canadian trusted computer product evaluationcriteria.cug (closed user group) is an access control concept used in x.25, framerelay, and atm networks to establish a noncryptographic vpn. acug is limited to a single network and network technology, managed by a single administration.darpa is the defense advanced research projects agency (known attimes in the past as arpa), which is part of the dod.dce (data communication equipment) refers to the devices and connections of a communications network that connect the circuit betweenthe data source and destination. a modem is the most common typeof dce.dcom (distributed common object model) refers to an infrastructurefor components that can be systematically reused.ddn (defense data network) is a global dod communications networkcomposed of milnet, other portions of the internet, and classifiednetworks.trust in cyberspacecopyright national academy of sciences. all rights reserved.appendix k305decryption is the process of transforming ciphertext into the originalmessage, or cleartext.denial of service is a form of attack that reduces the availability of aresource.des (data encryption standard) is the u.s. government standard (fips461) describing a symmetrickey cryptographic algorithm.dgsa (dod goal security architecture) is a set of specifications orgoals that support a wide range of access controls and integrity policies in an objectoriented, distributed system environment.digital signature is a digitized version of a written signature, typicallyproduced by decrypting a digest of the message being signed.disais the defense information systems agency, a unit of the dod.dms (defense messaging system) relies on the sns guard to permitelectronic mail to flow in and out of highly sensitive enclaves andfacilitate communication with lesssensitive dms subscribers.dns (domain name service) is a generalpurpose, distributed, replicated, dataquery service that is used primarily on the internet fortranslating host names into internet addresses.dod is the u.s. department of defense.doeis the u.s. department of energy.dos is disk operating system, developed by microsoft corporation andused widely on ibmcompatible personal computers. it contains noprotection against errant programs and no support for partitioningthe actions of one user from another.dsp (digital signal processor) is a specialized integrated circuit used toanalyze or alter the characteristics of communications signals.dsp (downstream service provider) is a local or regional internet provider.dte (domain and type enforcement) is a finegrained access controlmechanism.dvrp (distance vector routing protocol) enables routers to function without complete knowledge of network topology. routers broadcast alist of destinations and costs; each recipient adds its cost for traversing its link back toward the sender and rebroadcasts the updated listof destinations and costs (or a lowercost path to any of those destinations, if available).encryption is any procedure used in cryptography to convert plaintextinto ciphertext to prevent anyone but the intended recipient fromreading the data.escrowed encryption initiative is a voluntary program intended to improve the security of telephone communications while also meetingthe stated needs of law enforcement.trust in cyberspacecopyright national academy of sciences. all rights reserved.306appendix kesp (encapsulating security payload) is a protocol (part of the ietfipsec series of standards) thatprovides encryption and/or authentication for ip packets.fault tolerance is the capability of a system or component to continueoperating despite hardware or software faults. it may be expressed interms of the number of faults that can be tolerated before normaloperation is impaired.fbiis the federal bureau of investigation.fcc is the federal communications commission.fda is the food and drug administration.feal is a symmetrickey cipher developed in japan.ffrdcrefers to federally funded research and development centers.fgac (finegrained access control) enables a user or system administrator to control access to small objects, methods, and procedures.fips (federal information processing standards) are technical standardspublished by nist. u.s. government agencies are expected either topurchase computerrelated products that conform to these standardsor to obtain a formal waiver.firewallis a defensive mechanism typically deployed at the boundarybetween a trusted and a mistrusted computer network.formal language is language that has precisely defined syntax and semantics. it enables unambiguous descriptions and is often amenableto various degrees of automated analysis.formal method is a mathematically based technique for describing andanalyzing hardware, software, and computing systems.fortezzais a pcmcia cryptographic token for protecting data. it is acomponent of the missi architecture.fortezza initiative is a u.s. government initiative to promote and support escrowed encryption for data storage and communications.ftp (file transfer protocol) is a clientserver protocol that enables a useron one computer to transfer files to and from another computer overa tcp/ip network.functionalityis the functional behavior of a system. functionality requirements include confidentiality, integrity, availability, authentication, and safety.gsm (global system for mobile communications) is a standard for digital cellular communications that is being adopted by more than 60countries.gssapi (generic security services application programming interface)is an ietfstandard applicationlevel interface to cryptographic services.gui is a graphical user interface.trust in cyberspacecopyright national academy of sciences. all rights reserved.appendix k307hardware token refers to a small hardware device that contains a personal cryptographic key as well as processing capability. it is usedtypically for authentication.hash function is a form of checksum.hcs (high confidence systems) is the working group of the committeeon cic r&d that deals with trustworthiness.heisenbugrefers to a transient failure that is difficult to reproduce because it is triggered by circumstances beyond the control of a tester.hijacking refers, in the computer context, to the impersonation of a previously authenticated entity.hmois a health maintenance organization.html (hypertext markup language) is used to represent text and otherdata for posting and delivery to browsers on the world wide web.http (hypertext transfer protocol) is the clientserver tcp/ip protocolused on the world wide web for the exchange of html documents.iabis the internet architecture board.icmp (internet control message protocol) is a feature of ip that allowsfor the generation of error messages, test packets, and informationalmessages.ideis a disk interface standard.identification is an assertion about the identity of someone or something.ieee is the institute of electrical and electronics engineers.iesgis the internet engineering steering group.ietf (internet engineering task force) is a large, international community of network designers, operators, vendors, and researchers whocoordinate the evolution of the internet and resolve protocol and architectural issues.iispis the information infrastructure standards panel.iitf is the information infrastructure task force.imp (interface message processor) was a switching node for the arpanet.infosec refers to information security.integrity is the property of an object meeting an a priori established set ofexpectations. in the distributed system or communication securitycontext, integrity is more precisely defined as assurance that datahave not been undetectably modified in transit or storage.integrity check is a quantity derived by an algorithm from the runningdigital stream of a message, or the entire contents of a stored data file,and appended to it. some integrity checks are cryptographically based.ipsec (ip security) is a suite of internetworklayer security protocols developed for the internet by the ietf working group.trust in cyberspacecopyright national academy of sciences. all rights reserved.308appendix kip (internet protocol) is a connectionless, packetswitching protocol thatserves as the internetwork layer for the tcp/ip protocol suite. ipprovides packet routing, fragmentation, and reassembly.isakmp (internet security association and key management protocol)is a protocol developed by the nsa to negotiate keys for use withdata network security protocols.isat (information science and technology) refers to special activitiesheld by darpa to address longterm issues and plans.isdn (integrated services digital network) is a set of communicationsstandards that specify how different types of information (e.g., voice,data, video) can be transmitted in the public switched telephone network.iso is the information systems office of darpa.iso is the international organization for standardization.isoc (internet society) is a nonprofit, professional membership organization that facilitates and supports the technical evolution of the internet; stimulates interest in and educates the scientific and academiccommunities, industry, and the public about the technology, uses,and applications of the internet; and promotes the development ofnew applications.isp (internet service provider) is a company that provides other companies or individuals with access to, or presence on, the internet. mostisps also provide extra services, such as help with the design, creation, and administration of world wide web sites.issrjto (information systems security researchjoint technologyoffice)involves darpa, disa, and nsa.ito (information technology office) is a unit of darpa that supportsresearch in computing and communications.itsec(information technology security evaluation criteria) refers tothe harmonized criteria of france, germany, the netherlands, and theunited kingdom.iwdrefers to defensive information warfare.java is an objectoriented, distributed, architectureneutral, portable, generalpurpose programming language.javabeans is a component architecture for java that enables the development of reusable software components that can be assembled usingvisual applicationbuilder tools.jdk (java development kit) provides an environment for developingjava programs.jvm (java virtual machine) is a specification for software that interpretsjava programs compiled into byte codes.trust in cyberspacecopyright national academy of sciences. all rights reserved.appendix k309kdc (keydistribution center) is an online, automated provider of secretsymmetric keys.kernel is a small, trusted portion of a system that provides services onwhich the other portions of the system depend.keyis a value used in conjunction with a cryptographic algorithm.keyescrow encryption is an encryption system that enables exceptionalaccess to encrypted data through special datarecovery keys held (“inescrow”) by a trusted party.kpa (key process area) refers to the most important aspects of softwareprocesses.lan (local area network) is a data communications network, such as anethernet, that covers a small geographical area (typically no largerthan a 1kilometer radius), allowing easy interconnection of terminals, microprocessors, and computers within adjacent buildings.linkstate routing protocol enables routers to exchange informationabout the possibility and cost of reaching the other networks. thecost is based on number of hops, link speeds, traffic congestion, andother factors, as determined by the network designer.md4is a hash algorithm.meii (minimum essential information infrastructure) is a highly trustworthy communications subsystem originally envisioned for use byniss that control critical infrastructures.mib (management information base) is a database of objects accessed bythe internet management protocols (snmp).mic (message integrity code) is a value that is a complex function of botha set of protected data and a cryptographic key. it is computed by thesender and validated by the receiver.milnet is the military network that is part of the ddn and the internet.mime (multipurpose internet mail extension) is a standard for multipart, multimedia electronic mail messages and world wide webhypertext documents on the internet.missi (multilevel information systems security initiative) is an nsainitiative designed to provide a framework for the development ofinteroperable, complementary security products.multics is a multiuser operating system developed in the mid1960s bymit, ge, and bell laboratories that features elaborate access controls.multiplexing is the combining of several signals for transmission on ashared medium.mvs (multiple virtual storage) is an operating system for system 370and its successors that supports virtual memory.trust in cyberspacecopyright national academy of sciences. all rights reserved.310appendix knasa is the national aeronautics and space administration.ncs (national communications system) is a group of 23 federal departments and agencies that coordinates and plans systems to supportresponses to crises and disasters.ncsc (national computer security center) is part of the nsa.nes (network encryption system) is a certificatebased, packet networkencryption system certified by the nsa (cf., caneware).nis (networked information system) integrates computing and communications systems, procedures, and users and operators.nist (national institute of standards and technology) is a unit of theu.s. department of commerce that works with industry to developand apply technology, measurements, and standards.nlsp (netware linkstate protocol) is a protocol for the exchange ofrouting information in some networks.nlsp (networklayer security protocol) is a protocol (roughly comparable to ipsec) that was developed for osi networks but is rarelyused.nms (network management system) is a collection of software for managing the security of the other components in the missi architecture.noc (network operations center) is a designated site that monitors andcontrols the elements of a network.nonrepudiationis the affirmation, with extremely high confidence, ofthe identity of the signer of a digital message using a digital signatureprocedure. it is intended to protect against any subsequent attemptby the signer to deny authenticity.nprg (national partnership for reinventing government) is theadministration’s ongoing effort to make the u.s. government workbetter and cost less. it was formerly known as the national performance review.nrc (network reliability council) is the former name of the nric.nrc (national research council) is the operating arm of the nationalacademy of sciences and the national academy of engineering.nric (network reliability and interoperability council) is the newname of the former network reliability council.nsa is the national security agency, which is part of the dod.nsf is the national science foundation.nstac (national security telecommunications advisory committee)provides industry advice to the executive branch of the u.s. government.object is a hardware or software system or component (e.g., processor,file, database) that can be accessed by a subject.trust in cyberspacecopyright national academy of sciences. all rights reserved.appendix k311object code is the “executable” code of 1s and 0s that instructs a computer on the steps to be performed.oc12 (optical carrier 12) is a sonet rate communications channel of622 megabits per second.ole (object linking and embedding) is objectoriented software technology.omg (object management group) is a consortium of companies thatsupports and promotes a set of standards called corba.orange book is the common name for the dod document that providescriteria for the evaluation of different classes of trusted systems.supplementary documents extend and interpret the criteria.orcon (originator controlled) is a term used with very sensitive classified data to denote an access control policy in which the originator ofdata must approve access.os (operating system) is a computer program (e.g., msdos, windows,unix, mac os) that provides basic services for applications. suchfunctions can include screen displays, file handling, and, in the future, encryption.osi (open systems interconnection) refers to a sevenlayer model ofnetwork architecture and a suite of implementing protocols developed in 1978 as a framework for international standards for heterogeneous computer networks.ospf (open shortestpath firstinterior) is a standard interior gatewayrouting protocol for the internet. it is a linkstate routing protocol, asdistinct from a distancevector routing protocol.oss (operations support system) is a computer system involved in running the telephone network.p5is an intel processor chip known as a pentium processor.p6is an intel processor chip known as a pentium pro processor.packet switching is a networking technology that breaks up a messageinto smaller packets for transmission and switches them to their required destination. unlike circuit switching, which requires a constant pointtopoint circuit to be established, each packet in a packetswitched network contains a destination address. thus all packets ina single message do not have to travel the same path. they can bedynamically routed over the network as circuits become available orunavailable. the destination computer reassembles the packets backinto their proper sequence.passwordis a sequence of characters presented to a system for purposes ofauthentication of the user’s identity or privilege to access the system.pcis a personal computer.trust in cyberspacecopyright national academy of sciences. all rights reserved.312appendix kpcc (proofcarrying code) is a security enforcement approach in whichformal, machinecheckable proof is used to establish that a softwareprogram will not violate a particular security policy.pccipis the president’s commission on critical infrastructure protection.pcmciais the personal computer memory card interface association,an organization that specifies standards for what are now called pccards.pgp (pretty good privacy) is a publickey encryptionbased file encryption implementation. pgp enables users to exchange files or emailmessages with privacy and authentication.pinis a personal identification number and is used in much the samemanner as a password.pki (publickey infrastructure), as used in this report, refers to mechanisms, procedures, and policies that together provide a managementframework for the deployment of publickey cryptography.plaintextis a synonym for cleartext.pli (private line interface) was a networklayer encryptor designed toprotect classified data transmitted over the arpanet, developedand deployed in the mid 1970s.privacy ensures freedom from unauthorized intrusion.private key is the decryption or signature generation key associatedwith a given person’s public key for a publickey cryptographicsystem.protocols are formal rules describing how different parties cooperate toshare or exchange data, especially across a network.pseudocode is a program written using a mixture of programming language and informal statements (e.g., plain english).ptn is the public telephone network.public key is the publicly known key associated with a given subject in apublickey cryptographic system.publickey certificate is a data structure, typically transmitted electronically over an information network, that establishes the relationshipbetween a named individual or organization and a specified publickey.publickey cryptography refers to algorithms that use one key to encryptor digitally sign data and a corresponding second key to decrypt orvalidate the signature of that data.qos (quality of service) refers to performance guarantees offered by anetwork.r2 is the nsa unit that is responsible for information security research.trust in cyberspacecopyright national academy of sciences. all rights reserved.appendix k313r/3is a software product from sap for handling all major functions of acommercial enterprise.r&dis research and development.red book is the common name for the dod document containing thetrusted network interpretation of the trusted computer system evaluation criteria.reliabilityis the capability of a computer, or information or telecommunications system, to perform consistently and precisely according toits specifications and design requirements, and to do so with highconfidence.rfc (request for comments) refers to a series of numbered informationaldocuments and standards widely followed in the internet community. all internet standards are recorded in rfcs, but not all rfcs arestandards. rfcs are issued online at <http://www.rfceditor.org/rfc.html> by the rfc editor, information sciences institute, university of southern california, los angeles.rfpis a request for proposals.risk is, in the computer context, the likelihood that a vulnerability maybe exploited, or that a threat may become harmful.rpc (remote procedure call) is a protocol that allows a program running on one host to cause code to be executed on another host.rsml (requirements state machine language) is a specification notationthat has a variety of formal methods associated with it.rsvp (resource reservation protocol) is a protocol designed to provideqos guarantees on the internet.rtcais now the official name of the former radio technical commission for aeronautics.safety is a characteristic of trustworthiness asserting that a system willnot be the cause of physical harm to people or property.scc is a strongly connected component.sci (scalable coherent interface) is an ieee standard.scr (software cost reduction) is the naval research laboratory program that is developing rigorous techniques for software development. one goal is to reduce the cost of software development.scsi (small computer standard interface) is an industrystandard diskinterface.sdns (secure data network system) was the nsa project that deviseda networklayer encryption standard.sdsi (secure distributed security infrastructure) is an approach to certificate use in which all names bound to public keys are viewed ashaving only local significance.trust in cyberspacecopyright national academy of sciences. all rights reserved.314appendix ksecrecy is the habit or practice of maintaining privacy. it is an element ofsecurity.secret key is a key used in conjunction with a secretkey or symmetriccryptosystem.secretkey cryptosystem is a symmetric cryptographic process inwhich both parties use the same secret key to encrypt and decryptmessages.security refers to a collection of safeguards that ensure the confidentialityof information, protect the system(s) or network(s) used to process it,and control access to it. security typically encompasses secrecy,confidentiality, integrity, and availability and is intended to ensurethat a system resists potentially correlated attacks.security level is either the clearance level associated with a subject or aclassification level associated with an object.sei is the software engineering institute.set (secure electronic transaction) is a protocol for credit card transactions over the internet.sfi (software fault isolation) is a security enforcement approach in whichinstructions and addresses are modified so that they cannot referencememory outside the specified regions.sharewareis software that is offered publicly, free of charge, rather thansold, but shareware authors usually do request payment for the freelydistributed software.skipjack is a symmetric encryption algorithm.s/mime (secure/multipurpose internet mail extension) is a format forsecure internet email.smtp (simple mail transfer protocol) is a protocol used to transfer email over the internet.snefruis a oneway hash function.snmp (simple network management protocol) is the internet standardprotocol that manages nodes on an ip network.sns (secure network system) is a highassurance guard (a componentof the missi architecture) for separating top secret enclaves fromlesssensitive network environments.sonet (synchronous optical network) is a broadband networking standard that is generally based on ring topologies to ensure reliability.source code is the textual form in which a highlevellanguage programis entered into a computer.sp3 (security protocol at level 3) is a networklayer encryption standarddeveloped in the sdns project.specification is a precise description of the desired behavior of a system.trust in cyberspacecopyright national academy of sciences. all rights reserved.appendix k315spki (simple publickey infrastructure) is a scheme being developedby an ietf working group attempting to codify sdsi into an internetstandard.spoofingis the illicit, deliberate assumption of the characteristics of another computer system or user, for purposes of deception.ss7 (signaling system 7) is a protocol suite used for communicationwith, and control of, telephone central office switches and processors.it uses outofband signaling.ssl (secure socket layer) is a protocol designed to provide secure communications for http traffic on the internet.stateis retained information from one transaction that is used to determine how to complete a subsequent transaction, often of a relatedtype.stl (standard template library) is a component designed for systematicreuse.stuiii (secure telephone unit iii) is a standardized voice and datatelephone system capable of encryption up to topsecret level for defense and civilian government purposes. stuiii operates over standard dialup telephone lines and has been extended to cellular applications.subject refers, in this report, to an active entity (e.g., a user, or a processor device acting on the user’s behalf) that can make a request to perform an operation on an object.survivabilityis the capability to provide a level of service in adverse orhostile conditions.swat is a special weapons and tactics team.swipeis a hostbased ip encryptor that led to the ietf working group onipsec.tclis tool command language.tcp (transmission control protocol) is the most common transportlayer protocol used on the internet. it provides reliable connectionoriented fullduplex communications, flow control, and multiplexing.tcsec(trusted computer system evaluation criteria) refers to criteriafor a graded system of protection contained in the dod documentknown as the orange book.telnetis a protocol that enables a user on one machine to log onto another machine over a network and read the remote files.threatis an adversary that is both motivated and capable of exploiting avulnerability.trust in cyberspacecopyright national academy of sciences. all rights reserved.316appendix ktiger team refers to an organized group of people that tests securitymeasures by attempting to penetrate them, or, more generally, to anyofficial inspection team or special group called in to look at a computer or communications problem.tiu (trusted interface unit) is anethernet lan data encryption product.trojan horse refers to a program that, by exploiting the current user’sauthorization, provides covert access to information in an object for auser not authorized to access that object.trustworthiness is assurance that a system deserves to be trusted—that itwill perform as expected despite environmental disruptions, humanand operator error, hostile attacks, and design and implementationerrors. trustworthy systems reinforce the belief that they will continue to produce expected behavior and will not be susceptible tosubversion.udp (user datagram protocol) is an internet transport protocol that provides unreliable datagram services. it adds a checksum and additional processtoprocess addressing information on top of the basicip layer.unixis a multiuser operating system developed by bell laboratories inthe 1970s that is widely used on the internet and in the computerscience research community. it is much smaller and simpler thanmultics and has far fewer access controls and far less structure tosupport security.url (uniform resource locator) specifies an internet object, such as a fileor a newsgroup. urls are used in html documents to specify targets of hyperlinks.urp (university research program) is the program within nsa’s r2 thatawards contracts to academic investigators for securityrelated research.vdm (vienna definition method) is a formal method.verityis a tool used to design processors.vgais video graphics adapter.vlsi (very large scale integration) refers to integrated circuits composedof hundreds of thousands of logic elements, or memory cells.vpn (virtual private network) is a secure connection through an otherwise insecure network, typically the internet.vulnerabilityis an error or weakness in the design, implementation, oroperation of a system.vvslis a formal method.trust in cyberspacecopyright national academy of sciences. all rights reserved.appendix k317w3c (world wide web consortium) is an industry consortium standardssetting body for the web.wan (wide area network) is a network extending over an area greaterthan 1 kilometer in diameter.windows nt is microsoft’s multiprogramming, multitasking, and multiuser operating system. it has the ability to control users’ access to allsystem objects. windows nt is supported on several instruction setarchitectures.work factor is a measure of the difficulty of undertaking a bruteforce testof all possible keys against a given ciphertext and known algorithm.www is the world wide web.x.25 is a standard protocol suite for the dtedce interface in a packetswitched network. it was developed to describe how data passes inand out of public data communications networks.xeu (xerox encryption unit) is a functionally transparent cipher unit forprotecting information on baseband lans.y2k (year 2000) refers to the widespread problem of computers that arenot programmed to recognize correctly the years following 1999.trust in cyberspacecopyright national academy of sciences. all rights reserved.trust in cyberspacecopyright national academy of sciences. all rights reserved.index319319indexaabsolute security, philosophy of, 7, 120121, 247access control, 114, 134, 300discretionary, 114115granularity of, 134mandatory, 96, 114115mechanisms for, as addons, 292operating system, 147violations of, 44access level, 44, 300access modes, multiple, 193acl (access control list), 147, 292, 300acl2 theorem prover, 97activex, 111, 141142, 283, 300ada programming language, 8586, 300adsl (asymmetric digital subscriber lines),56, 300advanced automation systems airtrafficcontrol system, 99advanced research projects agencynetwork (arpanet), 2930, 34, 133,283, 301switching node for, 307alcatel, 220american national standards institute(ansi), 199anomaly detection, 9.see also detectionansi.see american national standardsinstituteapis.see application programminginterfacesapplicationlevel security, 139149application programming interfaces(apis), 127, 132, 226227, 301applique, 138arpa.see defense advanced researchprojects agencyarpanet.see advanced researchprojects agency networkassessment technology branch (atb) ofnasa, 222, 301assurance, 15, 204205, 301.see alsosystem assuranceasymmetric cryptography.seecryptographyasynchronous transfer mode (atm), 132133, 301atb.see assessment technology branch ofnasaatm.see asynchronous transfer modeat&t, 42, 46, 220attacks by hostile parties, 13, 22, 4755damage from, 112detecting, 160measuring, 185scripts for, 174trust in cyberspacecopyright national academy of sciences. all rights reserved.320indexauthentication, 3336, 125, 292, 301, 307biometric, 123124, 301cryptographic, 7, 122, 214need for effective, 7networkbased, 121, 124authenticode signatures, 141authority, 112. see also certificationauthorities (cas)autonomous system (as), 301routing within, 53availability, 14, 55, 149150, 250, 301bbaan (software vendor), 221, 282baas.see broad area announcementsbcr (black/crypto/red) project, 133, 301bell and la padula policy, 115, 118, 301bell laboratories, 291, 316bell operating companies, 26beta testing, 89bgp.see border gateway protocolbiometric authentication, 8, 123124, 229,248, 301blacker devices, 35, 133, 302“blue boxes,” 28, 302border gateway protocol (bgp), 30, 52, 301routers for, 38, 5355, 164bottomup integration, 93british ministry of defense, 202broad area announcements (baas), 233234, 237, 255, 301bugs, 32, 45, 99, 182protecting against, 135seriousness of, 88cc++ programming language, 85, 302standard template library (stl) for, 87cable services, 4caller id, 29, 241call forwarding, 2728canadian trusted computer productevaluation criteria (ctcpec), 204,304caneware, 133, 138, 302cap.see complex arithmetic processorcapability maturity model (cmm), 7880,303critique of, 80ccf (central control function), 99, 302ccitt.see consultative committee oninternational telephony andtelegraphyccv2.see common criteria, version 2cdis (central control function displayinformation system), 99, 302cdsa.see common data securityarchitecturecellular telephony fraud, 176central intelligence agency (cia), 223centralized namingservice architecture, 56.see also domain name servicescert/cc.see computer emergencyresponse team/coordinationcentercertificate management, 128129, 302certificate revocation list (crl), 130, 304certification, 197, 302certification authorities (cas), 8, 128132,302certification authority private keys,recovery from compromise of, 8, 129cgi.see common gateway interfacecheckpoint, 220chinese wall (brewernash) model, 116,302cia.see central intelligence agencyciao.see critical infrastructure assuranceofficecic r&d. see computing, information,and communications research anddevelopmentcircuit relays, 293294cisco routers, 46, 163clark/wilson model, 116clefs.see commercially licensedevaluation facilitiesclipper chip, 227, 303closed user groups (cugs), 111, 132133cmm.see capability maturity modelcocomo.see constructive cost modelcollective behavior, research into, 299com.see common object modelcommercially licensed evaluation facilities(clefs), 208, 303commercial offtheshelf (cots)components, 92, 118, 281, 296, 304adapting and customizing, 6, 281282benefits of, 3, 63changing role of, 8789dod use of, 10, 1315trust in cyberspacecopyright national academy of sciences. all rights reserved.index321general problems with, 8990, 103need for greater trustworthiness in, 4,70, 190191software, 22, 8790common criteria, version 2 (ccv2), 204,206207common data security architecture(cdsa), 132, 145, 302common gateway interface (cgi) scripts,32, 302common object model (com), 87, 303common object request broker architecture(corba), 87, 303, 311communications security, new approachesto, 79communications speed, 8complex arithmetic processor (cap), 97,302complexity, increased problems with, 3,16, 65components.see also commercial offtheshelf (cots) components; criticalcomponentsbuilding and acquiring, 8292design and implementation, 8485integrating, 8292computer breakins, 1718computer crime, 113computer emergency response team/coordination center (cert/cc),110, 302advisories of, 150web site of, 15, 50computer networks. see networkscomputer security, new approaches to, 79,118120computer security act, 215, 218computer security technology center, 222computing, information, andcommunications research anddevelopment (cic r&d), 216, 223,303, 307computing systems, integrating, 24, 7778,9294concurrency, 100confidentiality, 125, 303and cryptography, 214congestion, 3841, 149150conops (concept of operations), 6970, 303approval of, 74constructive cost model (cocomo), 67,194, 303consultative committee on internationaltelephony and telegraphy (ccitt),99, 302consumer risk management, 187188contingency planning, 289controlspersonnel, 109procedural, 297control theory, 16control tones, 28, 302convenience issues, 212cops (computer oracle passwordsecurity), 44, 303corba.see common object requestbroker architecturecorrectness, 14, 75, 9293, 97, 303cost pressures, 3, 13, 38costsconsumer, 181184, 252direct, 181182estimating, 67indirect, 182producer, 192194see also failure costscots components. see commercial offtheshelf componentscpu (central processing unit)increasing power of, 182intensive calculations by, 41, 243credit card transactions, 158crisis report. seecryptography’s role insecuring the information societycriteria creep, 207critical components, 7677critical foundations: protecting america’sinfrastructures, 287critical infrastructure assurance office(ciao), 13, 216, 303crl.see certificate revocation listcrossconnect components, 242, 304cryptographic authentication, 122, 135136,214, 247cryptographic protocols, 99, 124125,133cryptography.see also authenticationclassified research into, 232and confidentiality, 214defined, 304factors inhibiting widespreaddeployment of, 211213, 253increased use of, 289290promoting wider use of, 243trust in cyberspacecopyright national academy of sciences. all rights reserved.322indexand publickey infrastructures (pkis),124132and security, 7and trustworthiness, 55, 210214cryptography’s role in securing theinformation society (crisis report),211, 289290, 304csp (communicating sequential process),100, 304ctcpec.see canadian trusted computerproduct evaluation criteriacugs (closed user groups), 132133, 138,304customers, and trustworthiness, 180189cyberspace, trust in, 11, 111ddamage from attacks, 112darpa.see defense advanced researchprojects agencydatabase attacks, 4849data encryption standard (des), 199, 203,305dcom.see distributed common objectmodeldecentralization, research into, 299deception, research into, 299decision support, 289defense advanced research projectsagency (darpa), 172, 217, 223, 304,308coordinating with nsa, 228issues for the future, 235237, 254255role in trustworthiness r&d, 221, 223224, 232237sponsoring research, 5, 10, 298299defenseindepth, 127, 132, 288defense information systems agency(disa), 17, 217, 223224, 305defense messaging system (dms), 137138,305defense science board, 12, 286defensive information warfare. seeinformation warfare defensedenialofservice attacks, 44, 54, 111, 305,315defending against, 89, 149150dependency analysis, 75depreciation, 206207deregulation, today’s climate of, 3, 38, 220des.see data encryption standarddesign, top level, 6682design errors, 2, 13, 156, 251research in avoiding, 6detection, 158161, 180, 251limitations in, 158159, 161dgsa.see dod goal securityarchitecturedigital equipment corp. (dec), 198digital signal processors (dsps), 38, 305digital signatures, 126disa.see defense information systemsagencydistributed common object model(dcom), 87, 304diversity, 155158, 192, 250dms.see defense messaging systemdnss.see domain name servicesdod.see u.s. department of defensedod access control model, 115, 117dod goal security architecture (dgsa),117118, 230, 305doe.see u.s. department of energydomain name services (dnss), 3031, 46,305attacks via, 51, 175domainspecific languages, 86dos (disk operating system), 291, 305downstream service providers (dsps), 26,305dsps.see digital signal processorsdte (domain and type enforcement), 143,305dynamic packet filters, 294dynamic resource allocation, research into,298eeconomic context, 171239, 251253ecu.see european currency uniteligible receiver, 18, 19emergency systems, eliminating, 3encryptionendtoend packet, 301keyescrow, 309multiple, 154networklevel, 3435encryption technology, controversial, 287enforcement subsystems, 147environmental disruption, 13, 16, 3741trust in cyberspacecopyright national academy of sciences. all rights reserved.index323ethernets, 30, 316european currency unit (ecu),introduction of, 4, 187188evaluation processes, tension in, 210executive order 13010, 217export controls, 210211, 253extensible software, 111, 282283exterior gateway protocol, 30ffaa.see federal aviation administrationfailure costs, 183184fault isolation, 146149fault tolerance, 9, 233, 250, 306fbi.see federal bureau of investigationfcc.see federal communicationscommissionfda.see food and drug administrationfeal cipher, 203, 306federal aviation administration (faa),218, 222federal bureau of investigation (fbi), 18,112, 217federal communications commission(fcc), 37, 218federal information processing standards(fips), 199, 203, 306fips 461, 305fips 1401, 200, 203, 208federally funded research anddevelopment centers (ffrdcs), 228ffrdcs.see federally funded researchand development centersfgac.see finegrained access controlfilters.see packet filter firewallfinegrained access control (fgac), 8, 113,249, 306and application security, 143146fips.see federal information processingstandardsfirewall, 134137, 139, 242, 248249defined, 306future of, 8limitations of, 44, 113, 135137need for applicationlayer, 8thriving market for, 2, 188types of, 293295food and drug administration (fda), 184185, 218foreign code, 7, 111, 139149, 249250foreign espionage agent threat, 286formal methods, 7, 95101, 103104, 246,306formal policy models, shortcomings of,115117, 120121fortezza technology, 138, 226227, 306frame relay, 132freeh, fbi director louis, 18ftp (file transfer protocol), 294, 306proxy, 294ggateway routing protocols, 30, 135,212general accounting office (gao), 12, 18generally accepted security systemprinciples (gssp), 285government.see also individual agenciesrole in promoting trustworthiness, 34,215221graphical user interfaces (guis), 83, 282group identifiers, 292gssapi (generic security servicesapplication programming interface),145, 306gssp.see generally accepted securitysystem principlesguards, 137139, 249guis.see graphical user interfaceshhardware tokens, 8, 123124, 168, 247248,307hassle factor, 182, 189hcs.see high confidence systemshealth maintenance organization (hmo)example, 6263, 7071, 8283, 88, 90,9394heisenbug, 157, 307helper applications, 283heterogeneity, research into, 298.see alsodiversityhigh confidence systems (hcs) workinggroup, 223, 231, 307high performance computing andcommunications initiative, 216hmo.see health maintenanceorganization exampletrust in cyberspacecopyright national academy of sciences. all rights reserved.324indexhomogeneityrationale for, 191, 198risks of, 191192see also replicationhostile attacks. see attacks by hostilepartieshtml.see hypertext markup languagehttp.see hypertext transfer protocolhypertext markup language (html), 3132, 307hypertext transfer protocol (http), 3132,191, 307, 315iiab.see internet architecture boardibm, 97, 99, 220, 292icmp (internet control message protocol),307ide disk interface standard, 190, 307ieee.see institute of electrical andelectronics engineersiesg.see internet engineering steeringgroupietf.see internet engineering task forceiisp.see information infrastructurestandards paneliitf.see information infrastructure taskforceimmunological identification, researchinto, 299imperfect information, 184186implementation errors, 2, 13, 5455, 156research in avoiding, 6incident response, 289industry, partnership with, 226227information assurance, 215217, 288increasing spending on, 236237information assurance task force, 219information infrastructure standards panel(iisp), 199information infrastructure task force(iitf), 216information science and technologyactivities (isats), 234, 237, 255, 308information system security, nsa anddarpa research into, 298299information systems office (iso), 233information systems security researchjoint technology office (issrjto),224, 228, 308information technology industry council,200information technology managementreform act, 215information technology office (ito), 10,232236, 298299, 308information technology securityevaluation criteria (itsec), 204,206, 208, 308information warfare, 20, 215, 286information warfare defense (iwd), 286infosec (information security), 225infrastructure protection, 12, 1213, 2021,241, 287289insecurity, theory of, 109, 119120, 160161insiders, threat from, 112113, 135inspections, 9495institute of electrical and electronicsengineers (ieee), 97insuranceclaims data, 112demand for, 183insurance model, 178integration plans, 7778, 103104bottomup, 93costs of, 193of subsystems, 244245thread, 9394topdown, 93integrity of data, 125intel corp., 97, 145, 190interconnections, weak points in, 3, 19, 4041, 52interface message processors (imps), 35, 307interfaces, 2to facilitate intervention and control, 17server, 284international computer securityassociation, 197international organization forstandardization (iso), 31internet, 5, 21, 163164attacks on, 5055business use of, 58downloading software from, 140managing congestion on, 3940operational error on, 4243protecting, 242243readiness for business, 5657security of, 3637telephony, 5556vulnerability of, 5658trust in cyberspacecopyright national academy of sciences. all rights reserved.index325internet architecture board (iab), 201internet engineering steering group(iesg), 201internet engineering task force (ietf), 36,132, 144145, 200201, 209, 307, 315internet protocol (ip), 29, 121, 190, 308headers in, 40internet service providers (isps), 26, 38,308protection offered by, 52internet society (isoc), 201, 308interoperability issues, 191192, 212intrusion detection, 9, 113, 229, 233ip.see internet protocolipsec (ip security), 3436, 134, 200, 229, 307,315isakmp (international securityassociation and key managementprotocol), 229, 308isats.see information science andtechnology activitiesisdn (integrated services digital network),56, 308iso.see information systems office;international organization forstandardizationisps.see internet service providersissrjto.see information systems securityresearchjoint technology officeiteration, dynamic, 195ito.see information technology officeitsec.see information technologysecurity evaluation criteriaiwd.see information warfare defensejjava, 85, 142, 283, 308javabeans, 87, 308jointservice programmable radio, 97joint technology office. see informationsystems security researchjointtechnology officejvm (java virtual machine) specification,142, 308kkdcs.see keydistribution centerskernels, 164167, 309keydistribution centers (kdcs), 127128,309keyescrow encryption, 253, 309keymanagement technologies, 8, 127132,248needhamschroeder, 203key process areas (kpas), 79, 309key recovery, 211kpas.see key process areasllawrence livermore national laboratory,222lclint tool, 99legacy software. see softwareleverage, 191link failures, 3738mmaintenance practices, 44malicious attacks. see attacks by hostilepartiesmanagement information bases (mibs), 31,309marketgovernment relationship, changingrelationship, 220221meii.see minimum essential informationinfrastructuremibs.see management information basesmicrosoft corp., 190, 283, 305middleware, 282. see also sap, peoplesoftmilitary weapons, tactics intended todisrupt, 286milnet, 34, 304, 309mime (multipurpose internet mailextension) format, 200, 309minimum essential informationinfrastructure (meii), 9, 162, 164168, 287, 309building, 250taxonomy of, 166missi.see multilevel information systemssecurity initiativemitigation, risk, 23, 177178, 289mobile code, 7, 111, 283284model checking, 96, 101monitoring, 158161, 251, 289msdos, 291trust in cyberspacecopyright national academy of sciences. all rights reserved.326indexmulticasting, 8multics, 85, 309compared with unix, 316multilevel information systems securityinitiative (missi), 137138, 226, 306,309multilevel security, 96multimode jointservice programmableradio, 97multinode networks, 35mvs (multiple virtual storage), 292, 309nnamespace management, 8namingservice architecture, centralized,56nasa.see national aeronautics and spaceadministrationnational aeronautics and spaceadministration (nasa), 222, 301national communications system (ncs),217, 310national computer security center(ncsc), 226, 310national cryptologic strategy for the 21stcentury, 225national information assurancepartnership, 208, 220national information infrastructureinitiative, 216national infrastructure protection center,217national institute of standards andtechnology (nist), 199, 203, 218,220, 223, 306, 310national partnership for reinventinggovernment (nprg), 215, 310national performance review. see nationalpartnership for reinventinggovernmentnational science foundation (nsf), 201, 222national security agency (nsa), 138, 172,217218, 223, 310311issues for the future, 230232, 254mission of, 225role in trustworthiness r&d, 221232sponsoring research, 5, 1011, 298299national security telecommunicationsadvisory committee (nstac), 12,217, 219, 310national voluntary laboratoryaccreditation program, 208natural disasters, 289ncs.see national communicationssystemncsc.see national computer securitycenterneedhamschroeder keymanagementprotocol, 203nes (network encryption system), 133,310netcom, 43netware linkstate protocol (nlsp), 35,310networkbased authentication, 121networked information systems (niss),245246, 281282, 289, 310attacks on, 111building, 6466, 243definition of, 13research into vulnerabilities, 56, 1113software for, 62108trustworthiness of, 24, 1315, 154170,249250network management system (nms), 138,310network operations centers (nocs), 4243,310network reliability and interoperabilitycouncil (nric), 37, 41, 45, 218, 310networks, 282controlling access to, 8, 132139failures, 3755forming, 41layers in, 34multinode, 35our dependence on, 1network security, research into, 299network“sniffers,” 18network survivability, research into, 299niss.see networked information systemsnist.see national institute of standardsand technologynlsp.see netware linkstate protocolnlsp (networklayer security protocol),35, 310nms.see network management systemnocs.see network operations centersnodes, disparate, 94nonrepudiation, 125, 310northeast power blackout, 19trust in cyberspacecopyright national academy of sciences. all rights reserved.index327notation and style, 7072nprg.see national partnership forreinventing governmentnric.see network reliability andinteroperability councilnsf.see national science foundationnstac.see national securitytelecommunications advisorycommitteeooc12 (optical carrier 12) circuits, 40, 311office of management and budget, 215office of science and technology policy,12, 219open software foundation, 145open systems interconnection (osi)networks, 35, 310311operating systems (oss), 228, 291292. seealso individual operating systemsaddons for, 292defined, 311operational errors, 13, 16, 4145reducing, 4445, 243244operations support systems (osss), 28, 36,242, 311interconnections to the internet, 47oracle (software vendor), 282orange book, 23, 311, 315orphan products, 91osss.see operations support systemsoutofband signaling, 28, 56outsiders, threat from, 112113pp5 chip (pentium processors), 97, 311p6 chip (pentium pro processors), 97, 311packet filter firewall, 136, 293packetfiltering router, 196paperwork reduction act, 215passwords, 292, 311patents, 213pcc.see proofcarrying codepccip.seepresident’s commission oncritical infrastructure protectionpcmcia (personal computer memorycard interface association), 312cryptographic tokens, 306pdd.see presidential decision directivepeoplesoft (software vendor), 186, 282performance specifications, 7072personal computers (pcs), 88, 312personal identification numbers (pins), 8,123, 248, 312personnel controls, 109pgp (pretty good privacy), 36, 129, 312physical access, 112, 158physical threats, 50, 55, 174175, 287pki.see publickey infrastructurepli.see private line interfacepostdeployment upgrades, 104, 296postscript, 140precursor information, protecting, 297presidential decision directive (pdd) 63,13, 20, 216, 240president’s commission on criticalinfrastructure protection (pccip), 1,12, 20, 185, 217219, 223, 236, 287,312privacy, 14, 156, 312private keys, 126, 128, 312private line interface (pli), 133, 312programmable radio, 97program management, 6668programming languages, 8586, 9192. seealso individual languagespowers of, 7, 245246research in, 8proofcarrying code (pcc), 146, 148149,250, 312proofchecking, 101protocol design flaws, 5455prototyping, 8384pseudocode, 84, 312ptn.see public telephone network (ptn)publickey cryptography, 126, 301, 312313publickey infrastructure (pki), 8, 124132,248, 312defined, 130132public keys, 124126, 312public policy context, 2, 171239publicprivate partnerships, 219221, 253254public telephone network (ptn), 5, 21, 121,162163, 312attacks on, 4750backup power for, 165congestion on, 39design of, 2729trust in cyberspacecopyright national academy of sciences. all rights reserved.328indexand internet trustworthiness, 2661operational error on, 4142protecting, 241242vulnerabilities of, 5558qquality of service (qos), 30, 3233, 312guarantees of, 5, 3233quarantine, research into, 299rr2 program, 1011, 228232, 254, 312r/3.seesapradio, programmable, 97radio technical commission foraeronautics.see rtcarapid recovery, research into, 299reconfiguration, 159160reconstitution, research into, 299red book, 162, 313redundancy, 41research into, 298see also reserve capacityreliability, 14, 313amplifying, 155157replication, 154158, 250of components, 14report of the defense science board task forceon information warfare defense, 286requests for proposals (rfps), 188, 313requirements errors, 7374research and development (r&d)agenda for, 411, 21, 240255need for, 6, 13reserve capacity, reducing, 3, 3839resilience, research into, 299resource allocation, 149150, 191research into, 298resource reservation protocol (rsvp), 3233, 313response phase, 159160revenge threat, 286reverse engineering, defending against, 297reviews.see technical reviewsrevocation, 141142timely notification of, 8rfc (request for comments), 201, 313rfps.see requests for proposalsriskdefined, 173, 313measuring, 185risk assessment, 173174risk avoidance strategies, 23, 177, 180risk management, 23, 58, 172180, 289issues affecting, 186188strategies for, 176180risk mitigation strategies, 177178robustness, 88research into, 299routing attacks, 48, 5154routing protocols, 30, 305inadequacies in, 5, 4445rpc (remote procedure call) protocol,294, 313rsml (requirements state machinelanguage) notation, 98, 313rsvp.see resource reservation protocolrtca (radio technical commission foraeronautics), 202, 313ssafety, 14, 202, 313sandia national laboratories, 222sap (software vendor), 87, 186, 188, 191,220221, 282satellitebased services, 4scalable coherent interface (sci) standard,97, 313sci.see scalable coherent interfacescr.see software cost reductionscsi (small computer standard interface)interface, 190, 313sdsi.see secure distributed securityinfrastructuresecrecy, 314of design, 296297secretkey cryptography, 125, 129, 314secure distributed security infrastructure(sdsi), 132, 314315secure network system (sns), 138, 314secure socket layer (ssl) protocol, 32, 129,196, 200, 315secure telephone unit ii (stuii) systems,129secure telephone unit iii (stuiii)systems, 35, 129, 315trust in cyberspacecopyright national academy of sciences. all rights reserved.index329security, 1214, 88, 241, 285, 314. see alsonetwork securityamplifying, 157158applicationlevel, 139149demonstrating, 118120enforcing, 143examples of, 291292growing interest in, 197198languagebased, 146149reinventing, 109153, 247250through obscurity, 296security management protocols,supporting sophisticated, 8security needs, 253evolution of, 110111security reference monitor, 292segmentation, research into, 299sei.see software engineering instituteselforganization, research into, 299selfstabilization approach, 9, 168, 251sfi.see software fault isolationsiemens, 220signaling system 7 (ss7) protocol suite, 28,242, 315vulnerability of, 47, 50simple network management protocol(snmp), 31, 309, 314simple publickey infrastructure (spki),132, 315skilllifetimes, increasing, 191smart cards, 123, 200s/mime (secure/multipurpose internetmail extension) format, 36, 130, 196,314snefru function, 203snmp.see simple network managementprotocolsns.see secure network system (sns)software.see also componentsbarriers to innovative, 8182legacy, 9091needed to improve trustworthiness,244246for networked information systems(niss), 62108role of, 6466trends in, 281284software cost reduction (scr) program,70, 98, 313software developerspractices of, 21, 8990, 9596, 124132scarcity of, 85, 231software engineering, challenges of, 6, 6680, 8294software engineering institute (sei), 7880,302software fault isolation (sfi), 146, 148149,314software systems, evolution of, 282software upgrades, timing, 284software wizards, 281282sonet (synchronous optical network), 38,311, 314source code, 8485, 314specification, 17, 6869, 7475, 314spki.see simple publickey infrastructuresp3 (security protocol at level 3), 35, 314ss7.see signaling system 7ssl.see secure socket layer protocolstandards and criteria, 199210for trustworthiness, 201204, 251252standard template library (stl), 87, 315static resource allocation, research into, 298stl.see standard template librarystorage, procedural controls on, 297structured walkthrough, 84stu.see secure telephone unitstubs, 93subsystems, 116117survivability.see also networksurvivabilitydefined, 14, 315swat (special weapons and tactics) teams,100, 315switches, untrustworthy, 4041, 4647, 242symmetrickey ciphers, 119120, 122, 306secret, 309, 314system administrators, cautions for, 135,284system architectures, 6, 23, 287system assurance, 94102systematic reuse, 8687system evolution, 102103system integration, 9294system management, 284system planning, 6682requirements for, 6874systems analysis, 289trust in cyberspacecopyright national academy of sciences. all rights reserved.330indexsystem shutdown, 9systems requirements document, 6970ttactical countermeasures, 286tactical information warfare, 286tamper resistance, 123tandem systems, 157tcas ii, 98tcl (tool command language), 86, 315tcp (transmission control protocol), 29,293, 315headers in, 40tcsec.see trusted computer systemevaluation criteriatechnical reviews, 9495, 103telecommunications fraud, 183184telephone system. see public telephonenetwork (ptn)testing, 101102, 118119, 246costs of, 193research in, 104thread integration, 9394threat detection, 289threats, 316insidersversus outsiders, 112113taxonomy of, 286tiu (trusted interface unit), 133, 316tokenbased mechanisms. see hardwaretokenstopdown integration, 93top secret enclaves, 292, 315tradeoffs, managing, 176, 194traffic profile, 41transparency, 294transport layer, 293294procedural controls on, 297terminating instantiations of, 294trojan horse attacks, 18, 115, 316trust, 11erosion of, 1520warranted, 255trusted computer system evaluationcriteria (tcsec), 115, 162, 199, 206208, 227, 315trustworthiness, 313benefits of, 23costs of, 2, 189, 192196of cots components, 34and cryptography, 210214and customers, 180189defined, 1315, 240, 316enhancing, 12implementing r&d into, 253255marketing products for, 196198multidimensionality of, 252253new paradigms for, 168169of networked information systems(niss), 24placement of, 23producers of products for, 190198studies of, 195196, 285290of systems built from untrustworthycomponents, 9, 23, 154170, 250251see also software, role ofuudp.see user datagram protocolu.s. department of defense (dod), 10,234235, 286287, 311attacks against computers of, 17u.s. department of energy (doe), 221223u.s. secret service, 219united states, enemies of, 286university research program (urp), 228,316unix systems, 3, 157, 213, 291, 303, 316untrustworthy components. seetrustworthinessupgrades, 102postdeployment, 296urp.see university research programuser accounts, 44user datagram protocol (udp), 31, 34, 54,150, 293294, 316user ids, 291292uses of devices, procedural controls on,297vvalidation, 95vdm (vienna definition method), 100, 316verification, 9899verity, 97, 316veryhighlevel languages, 86vga (video graphics adapter), 190trust in cyberspacecopyright national academy of sciences. all rights reserved.index331virtual circuit data networks, 132133virtual private networks (vpns), 8, 133134, 138139, 248, 316visual basic language, 86vpns.see virtual private networks (vpns)vulnerabilities, 27, 287288, 297assessing, 120121, 289defined, 316exploiting, 173174, 313failure to validate arguments, 95intrinsic, 27residual, 297scanning for, 44vvsl method, 100, 316wwaterfall development process, 68w3c (world wide web consortium), 201,317windows nt, 157158, 229, 283, 291, 317work factor model, 178, 185, 317world wide web (www)downloading software from, 140exploding popularity of, 62xx.25 protocol suite, 35, 132133, 317xeu (xerox encryption unit), 133, 317yy2k (year 2000) problem, 4, 1213, 187188,317